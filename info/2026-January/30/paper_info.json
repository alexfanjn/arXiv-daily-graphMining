[
  {
    "id": "arXiv:2601.20870",
    "title": "STAER: Temporal Aligned Rehearsal for Continual Spiking Neural Network",
    "abstract": "           Spiking Neural Networks (SNNs) are inherently suited for continuous learning due to their event-driven temporal dynamics; however, their application to Class-Incremental Learning (CIL) has been hindered by catastrophic forgetting and the temporal misalignment of spike patterns. In this work, we introduce Spiking Temporal Alignment with Experience Replay (STAER), a novel framework that explicitly preserves temporal structure to bridge the performance gap between SNNs and ANNs. Our approach integrates a differentiable Soft-DTW alignment loss to maintain spike timing fidelity and employs a temporal expansion and contraction mechanism on output logits to enforce robust representation learning. Implemented on a deep ResNet19 spiking backbone, STAER achieves state-of-the-art performance on Sequential-MNIST and Sequential-CIFAR10. Empirical results demonstrate that our method matches or outperforms strong ANN baselines (ER, DER++) while preserving biologically plausible dynamics. Ablation studies further confirm that explicit temporal alignment is critical for representational stability, positioning STAER as a scalable solution for spike-native lifelong learning. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.20870",
    "authors": [
      "Matteo Gianferrari",
      "Omayma Moussadek",
      "Riccardo Salami",
      "Cosimo Fiorini",
      "Lorenzo Tartarini",
      "Daniela Gandolfi",
      "Simone Calderara"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.20876",
    "title": "BioNIC: Biologically Inspired Neural Network for Image Classification Using Connectomics Principles",
    "abstract": "           We present BioNIC, a multi-layer feedforward neural network for emotion classification, inspired by detailed synaptic connectivity graphs from the MICrONs dataset. At a structural level, we incorporate architectural constraints derived from a single cortical column of the mouse Primary Visual Cortex(V1): connectivity imposed via adjacency masks, laminar organization, and graded inhibition representing inhibitory neurons. At the functional level, we implement biologically inspired learning: Hebbian synaptic plasticity with homeostatic regulation, Layer Normalization, data augmentation to model exposure to natural variability in sensory input, and synaptic noise to model neural stochasticity. We also include convolutional layers for spatial processing, mimicking retinotopic mapping. The model performance is evaluated on the Facial Emotion Recognition task FER-2013 and compared with a conventional baseline. Additionally, we investigate the impacts of each biological feature through a series of ablation experiments. While connectivity was limited to a single cortical column and biologically relevant connections, BioNIC achieved performance comparable to that of conventional models, with an accuracy of 59.77 $\\pm$ 0.27% on FER-2013. Our findings demonstrate that integrating constraints derived from connectomics is a computationally plausible approach to developing biologically inspired artificial intelligence systems. This work also highlights the potential of new generation peta-scale connectomics data in advancing both neuroscience modeling and artificial intelligence.         ",
    "url": "https://arxiv.org/abs/2601.20876",
    "authors": [
      "Diya Prasanth",
      "Matthew Tivnan"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2601.20881",
    "title": "MA-LipNet: Multi-Dimensional Attention Networks for Robust Lipreading",
    "abstract": "           Lipreading, the technology of decoding spoken content from silent videos of lip movements, holds significant application value in fields such as public security. However, due to the subtle nature of articulatory gestures, existing lipreading methods often suffer from limited feature discriminability and poor generalization capabilities. To address these challenges, this paper delves into the purification of visual features from temporal, spatial, and channel dimensions. We propose a novel method named Multi-Attention Lipreading Network(MA-LipNet). The core of MA-LipNet lies in its sequential application of three dedicated attention modules. Firstly, a \\textit{Channel Attention (CA)} module is employed to adaptively recalibrate channel-wise features, thereby mitigating interference from less informative channels. Subsequently, two spatio-temporal attention modules with distinct granularities-\\textit{Joint Spatial-Temporal Attention (JSTA)} and \\textit{Separate Spatial-Temporal Attention (SSTA)}-are leveraged to suppress the influence of irrelevant pixels and video frames. The JSTA module performs a coarse-grained filtering by computing a unified weight map across the spatio-temporal dimensions, while the SSTA module conducts a more fine-grained refinement by separately modeling temporal and spatial attentions. Extensive experiments conducted on the CMLR and GRID datasets demonstrate that MA-LipNet significantly reduces the Character Error Rate (CER) and Word Error Rate (WER), validating its effectiveness and superiority over several state-of-the-art methods. Our work highlights the importance of multi-dimensional feature refinement for robust visual speech recognition.         ",
    "url": "https://arxiv.org/abs/2601.20881",
    "authors": [
      "Matteo Rossi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.20885",
    "title": "What Hard Tokens Reveal: Exploiting Low-confidence Tokens for Membership Inference Attacks against Large Language Models",
    "abstract": "           With the widespread adoption of Large Language Models (LLMs) and increasingly stringent privacy regulations, protecting data privacy in LLMs has become essential, especially for privacy-sensitive applications. Membership Inference Attacks (MIAs) attempt to determine whether a specific data sample was included in the model training/fine-tuning dataset, posing serious privacy risks. However, most existing MIA techniques against LLMs rely on sequence-level aggregated prediction statistics, which fail to distinguish prediction improvements caused by generalization from those caused by memorization, leading to low attack effectiveness. To address this limitation, we propose a novel membership inference approach that captures the token-level probabilities for low-confidence (hard) tokens, where membership signals are more pronounced. By comparing token-level probability improvements at hard tokens between a fine-tuned target model and a pre-trained reference model, HT-MIA isolates strong and robust membership signals that are obscured by prior MIA approaches. Extensive experiments on both domain-specific medical datasets and general-purpose benchmarks demonstrate that HT-MIA consistently outperforms seven state-of-the-art MIA baselines. We further investigate differentially private training as an effective defense mechanism against MIAs in LLMs. Overall, our HT-MIA framework establishes hard-token based analysis as a state-of-the-art foundation for advancing membership inference attacks and defenses for LLMs.         ",
    "url": "https://arxiv.org/abs/2601.20885",
    "authors": [
      "Md Tasnim Jawad",
      "Mingyan Xiao",
      "Yanzhao Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2601.20890",
    "title": "SW-ASR: A Context-Aware Hybrid ASR Pipeline for Robust Single Word Speech Recognition",
    "abstract": "           Single-word Automatic Speech Recognition (ASR) is a challenging task due to the lack of linguistic context and sensitivity to noise, pronunciation variation, and channel artifacts, especially in low-resource, communication-critical domains such as healthcare and emergency response. This paper reviews recent deep learning approaches and proposes a modular framework for robust single-word detection. The system combines denoising and normalization with a hybrid ASR front end (Whisper + Vosk) and a verification layer designed to handle out-of-vocabulary words and degraded audio. The verification layer supports multiple matching strategies, including embedding similarity, edit distance, and LLM-based matching with optional contextual guidance. We evaluate the framework on the Google Speech Commands dataset and a curated real-world dataset collected from telephony and messaging platforms under bandwidth-limited conditions. Results show that while the hybrid ASR front end performs well on clean audio, the verification layer significantly improves accuracy on noisy and compressed channels. Context-guided and LLM-based matching yield the largest gains, demonstrating that lightweight verification and context mechanisms can substantially improve single-word ASR robustness without sacrificing latency required for real-time telephony applications.         ",
    "url": "https://arxiv.org/abs/2601.20890",
    "authors": [
      "Manali Sharma",
      "Riya Naik",
      "Buvaneshwari G"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2601.20895",
    "title": "Faster Predictive Coding Networks via Better Initialization",
    "abstract": "           Research aimed at scaling up neuroscience inspired learning algorithms for neural networks is accelerating. Recently, a key research area has been the study of energy-based learning algorithms such as predictive coding, due to their versatility and mathematical grounding. However, the applicability of such methods is held back by the large computational requirements caused by their iterative nature. In this work, we address this problem by showing that the choice of initialization of the neurons in a predictive coding network matters significantly and can notably reduce the required training times. Consequently, we propose a new initialization technique for predictive coding networks that aims to preserve the iterative progress made on previous training samples. Our approach suggests a promising path toward reconciling the disparities between predictive coding and backpropagation in terms of computational efficiency and final performance. In fact, our experiments demonstrate substantial improvements in convergence speed and final test loss in both supervised and unsupervised settings.         ",
    "url": "https://arxiv.org/abs/2601.20895",
    "authors": [
      "Luca Pinchetti",
      "Simon Frieder",
      "Thomas Lukasiewicz",
      "Tommaso Salvatori"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.20896",
    "title": "A Study of Data Selection Strategies for Pre-training Self-Supervised Speech Models",
    "abstract": "           Self-supervised learning (SSL) has transformed speech processing, yet its reliance on massive pre-training datasets remains a bottleneck. While robustness is often attributed to scale and diversity, the role of the data distribution is less understood. We systematically examine how curated subsets of pre-training data influence Automatic Speech Recognition (ASR) performance. Surprisingly, optimizing for acoustic, speaker, or linguistic diversity yields no clear improvements over random sampling. Instead, we find that prioritizing the longest utterances achieves superior ASR results while using only half the original dataset, reducing pre-training time by 24% on a large corpora. These findings suggest that for pre-training speech SSL models, data length is a more critical factor than either data diversity or overall data quantity for performance and efficiency, offering a new perspective for data selection strategies in SSL speech processing.         ",
    "url": "https://arxiv.org/abs/2601.20896",
    "authors": [
      "Ryan Whetten",
      "Titouan Parcollet",
      "Marco Dinarelli",
      "Yannick Est\u00e8ve"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2601.20903",
    "title": "ICON: Intent-Context Coupling for Efficient Multi-Turn Jailbreak Attack",
    "abstract": "           Multi-turn jailbreak attacks have emerged as a critical threat to Large Language Models (LLMs), bypassing safety mechanisms by progressively constructing adversarial contexts from scratch and incrementally refining prompts. However, existing methods suffer from the inefficiency of incremental context construction that requires step-by-step LLM interaction, and often stagnate in suboptimal regions due to surface-level optimization. In this paper, we characterize the Intent-Context Coupling phenomenon, revealing that LLM safety constraints are significantly relaxed when a malicious intent is coupled with a semantically congruent context pattern. Driven by this insight, we propose ICON, an automated multi-turn jailbreak framework that efficiently constructs an authoritative-style context via prior-guided semantic routing. Specifically, ICON first routes the malicious intent to a congruent context pattern (e.g., Scientific Research) and instantiates it into an attack prompt sequence. This sequence progressively builds the authoritative-style context and ultimately elicits prohibited content. In addition, ICON incorporates a Hierarchical Optimization Strategy that combines local prompt refinement with global context switching, preventing the attack from stagnating in ineffective contexts. Experimental results across eight SOTA LLMs demonstrate the effectiveness of ICON, achieving a state-of-the-art average Attack Success Rate (ASR) of 97.1\\%. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.20903",
    "authors": [
      "Xingwei Lin",
      "Wenhao Lin",
      "Sicong Cao",
      "Jiahao Yu",
      "Renke Huang",
      "Lei Xue",
      "Chunming Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.20913",
    "title": "Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges",
    "abstract": "           Reliable certification of Large Language Models (LLMs)-verifying that failure rates are below a safety threshold-is critical yet challenging. While \"LLM-as-a-Judge\" offers scalability, judge imperfections, noise, and bias can invalidate statistical guarantees. We introduce a \"Noisy but Valid\" hypothesis testing framework to address this. By leveraging a small human-labelled calibration set to estimate the judge's True Positive and False Positive Rates (TPR/FPR), we derive a variance-corrected critical threshold applied to a large judge-labelled dataset. Crucially, our framework theoretically guarantees finite-sample Type-I error control (validity) despite calibration uncertainty. This distinguishes our work from Prediction-Powered Inference (PPI), positioning our method as a diagnostic tool that explicitly models judge behavior rather than a black-box estimator. Our contributions include: (1) Theoretical Guarantees: We derive the exact conditions under which noisy testing yields higher statistical power than direct evaluation; (2) Empirical Validation: Experiments on Jigsaw Comment, Hate Speech and SafeRLHF confirm our theory; (3) The Oracle Gap: We reveal a significant performance gap between practical methods and the theoretical \"Oracle\" (perfectly known judge parameters), quantifying the cost of estimation. Specifically, we provide the first systematic treatment of the imperfect-judge setting, yielding interpretable diagnostics of judge reliability and clarifying how evaluation power depends on judge quality, dataset size, and certification levels. Together, these results sharpen understanding of statistical evaluation with LLM judges, and highlight trade-offs among competing inferential tools.         ",
    "url": "https://arxiv.org/abs/2601.20913",
    "authors": [
      "Chen Feng",
      "Minghe Shen",
      "Ananth Balashankar",
      "Carsten Gerner-Beuerle",
      "Miguel R. D. Rodrigues"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.20915",
    "title": "Robust Federated Learning for Malicious Clients using Loss Trend Deviation Detection",
    "abstract": "           Federated Learning (FL) facilitates collaborative model training among distributed clients while ensuring that raw data remains on local this http URL this advantage, FL systems are still exposed to risks from malicious or unreliable participants. Such clients can interfere with the training process by sending misleading updates, which can negatively affect the performance and reliability of the global model. Many existing defense mechanisms rely on gradient inspection, complex similarity computations, or cryptographic operations, which introduce additional overhead and may become unstable under non-IID data distributions. In this paper, we propose the Federated Learning with Loss Trend Detection (FL-LTD), a lightweight and privacy-preserving defense framework that detects and mitigates malicious behavior by monitoring temporal loss dynamics rather than model gradients. The proposed approach identifies anomalous clients by detecting abnormal loss stagnation or abrupt loss fluctuations across communication rounds. To counter adaptive attackers, a short-term memory mechanism is incorporated to sustain mitigation for clients previously flagged as anomalous, while enabling trust recovery for stable participants. We evaluate FL-LTD on a non-IID federated MNIST setup under loss manipulation attacks. Experimental results demonstrate that the proposed method significantly enhances robustness, achieving a final test accuracy of 0.84, compared to 0.41 for standard FedAvg under attack. FL-LTD incurs negligible computational and communication overhead, maintains stable convergence, and avoids client exclusion or access to sensitive data, highlighting the effectiveness of loss-based monitoring for secure federated learning.         ",
    "url": "https://arxiv.org/abs/2601.20915",
    "authors": [
      "Deepthy K Bhaskar",
      "Minimol B",
      "Binu V P"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2601.20968",
    "title": "Quick Heuristic Validation of Edges in Dynamic Roadmap Graphs",
    "abstract": "           In this paper we tackle the problem of adjusting roadmap graphs for robot motion planning to non-static environments. We introduce the \"Red-Green-Gray\" paradigm, a modification of the SPITE method, capable of classifying the validity status of nodes and edges using cheap heuristic checks, allowing fast semi-lazy roadmap updates. Given a roadmap, we use simple computational geometry methods to approximate the swept volumes of robots and perform lazy collision checks, and label a subset of the edges as invalid (red), valid (green), or unknown (gray). We present preliminary experimental results comparing our method to the well-established technique of Leven and Hutchinson, and showing increased accuracy as well as the ability to correctly label edges as invalid while maintaining comparable update runtimes.         ",
    "url": "https://arxiv.org/abs/2601.20968",
    "authors": [
      "Yulie Arad",
      "Stav Ashur",
      "Nancy M. Amato"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2601.20978",
    "title": "Solution of Advection Equation with Discontinuous Initial and Boundary Conditions via Physics-Informed Neural Networks",
    "abstract": "           In this paper, we investigate several techniques for modeling the one-dimensional advection equation for a specific class of problems with discontinuous initial and boundary conditions using physics-informed neural networks (PINNs). To mitigate the spectral bias phenomenon, we employ a Fourier feature mapping layer as the input representation, adopt a two-stage training strategy in which the Fourier feature parameters and the neural network weights are optimized sequentially, and incorporate adaptive loss weighting. To further enhance the approximation accuracy, a median filter is applied to the spatial data, and the predicted solution is constrained through a bounded linear mapping. Moreover, for certain nonlinear problems, we introduce a modified loss function inspired by the upwind numerical scheme to alleviate the excessive smoothing of discontinuous solutions typically observed in neural network approximations.         ",
    "url": "https://arxiv.org/abs/2601.20978",
    "authors": [
      "Omid Khosravi",
      "Mehdi Tatari"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2601.20986",
    "title": "SMART: A Social Movement Analysis & Reasoning Tool with Case Studies on #MeToo and #BlackLivesMatter",
    "abstract": "           Social movements supporting the UN's Sustainable Development Goals (SDGs) play a vital role in improving human lives. If journalists were aware of the relationship between social movements and external events, they could provide more precise, time-sensitive reporting about movement issues and SDGs. Our SMART system achieves this goal by collecting data from multiple sources, extracting emotions on various themes, and then using a transformer-based forecasting engine (DEEP) to predict quantity and intensity of emotions in future posts. This paper demonstrates SMART's Retrospective capabilities required by journalists via case studies analyzing social media discussions of the #MeToo and #BlackLivesMatter before and after the 2024 U.S. election. We create a novel 1-year dataset which we will release upon publication. It contains over 2.7M Reddit posts and over 1M news articles. We show that SMART enables early detection of discourse shifts around key political events, providing journalists with actionable insights to inform editorial planning. SMART was developed through multiple interactions with a panel of over 20 journalists from a variety of news organizations over a 2-year period, including an author of this paper.         ",
    "url": "https://arxiv.org/abs/2601.20986",
    "authors": [
      "Valerio La Gatta",
      "Marco Postiglione",
      "Jeremy Gilbert",
      "Daniel W. Linna Jr.",
      "Morgan Manella Greenfield",
      "Aaron Shaw",
      "V.S. Subrahmanian"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2601.20995",
    "title": "Low performing pixel correction in computed tomography with unrolled network and synthetic data training",
    "abstract": "           Low performance pixels (LPP) in Computed Tomography (CT) detectors would lead to ring and streak artifacts in the reconstructed images, making them clinically unusable. In recent years, several solutions have been proposed to correct LPP artifacts, either in the image domain or in the sinogram domain using supervised deep learning methods. However, these methods require dedicated datasets for training, which are expensive to collect. Moreover, existing approaches focus solely either on image-space or sinogram-space correction, ignoring the intrinsic correlations from the forward operation of the CT geometry. In this work, we propose an unrolled dual-domain method based on synthetic data to correct LPP artifacts. Specifically, the intrinsic correlations of LPP between the sinogram and image domains are leveraged through synthetic data generated from natural images, enabling the trained model to correct artifacts without requiring any real-world clinical data. In experiments simulating 1-2% detectors defect near the isocenter, the proposed method outperformed the state-of-the-art approaches by a large margin. The results indicate that our solution can correct LPP artifacts without the cost of data collection for model training, and it is adaptable to different scanner settings for software-based applications.         ",
    "url": "https://arxiv.org/abs/2601.20995",
    "authors": [
      "Hongxu Yang",
      "Levente Lippenszky",
      "Edina Timko",
      "Lehel Ferenczi",
      "Gopal Avinash"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21012",
    "title": "Order-Aware Test-Time Adaptation: Leveraging Temporal Dynamics for Robust Streaming Inference",
    "abstract": "           Test-Time Adaptation (TTA) enables pre-trained models to adjust to distribution shift by learning from unlabeled test-time streams. However, existing methods typically treat these streams as independent samples, overlooking the supervisory signal inherent in temporal dynamics. To address this, we introduce Order-Aware Test-Time Adaptation (OATTA). We formulate test-time adaptation as a gradient-free recursive Bayesian estimation task, using a learned dynamic transition matrix as a temporal prior to refine the base model's predictions. To ensure safety in weakly structured streams, we introduce a likelihood-ratio gate (LLR) that reverts to the base predictor when temporal evidence is absent. OATTA is a lightweight, model-agnostic module that incurs negligible computational overhead. Extensive experiments across image classification, wearable and physiological signal analysis, and language sentiment analysis demonstrate its universality; OATTA consistently boosts established baselines, improving accuracy by up to 6.35%. Our findings establish that modeling temporal dynamics provides a critical, orthogonal signal beyond standard order-agnostic TTA approaches.         ",
    "url": "https://arxiv.org/abs/2601.21012",
    "authors": [
      "Young Kyung Kim",
      "Oded Schlesinger",
      "Qiangqiang Wu",
      "J. Mat\u00edas Di Martino",
      "Guillermo Sapiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21022",
    "title": "AI-based Prediction of Biochemical Recurrence from Biopsy and Prostatectomy Samples",
    "abstract": "           Biochemical recurrence (BCR) after radical prostatectomy (RP) is a surrogate marker for aggressive prostate cancer with adverse outcomes, yet current prognostic tools remain imprecise. We trained an AI-based model on diagnostic prostate biopsy slides from the STHLM3 cohort (n = 676) to predict patient-specific risk of BCR, using foundation models and attention-based multiple instance learning. Generalizability was assessed across three external RP cohorts: LEOPARD (n = 508), CHIMERA (n = 95), and TCGA-PRAD (n = 379). The image-based approach achieved 5-year time-dependent AUCs of 0.64, 0.70, and 0.70, respectively. Integrating clinical variables added complementary prognostic value and enabled statistically significant risk stratification. Compared with guideline-based CAPRA-S, AI incrementally improved postoperative prognostication. These findings suggest biopsy-trained histopathology AI can generalize across specimen types to support preoperative and postoperative decision making, but the added value of AI-based multimodal approaches over simpler predictive models should be critically scrutinized in further studies.         ",
    "url": "https://arxiv.org/abs/2601.21022",
    "authors": [
      "Andrea Camilloni",
      "Chiara Micoli",
      "Nita Mulliqi",
      "Erik Everett Palm",
      "Thorgerdur Palsdottir",
      "Kelvin Szolnoky",
      "Xiaoyi Ji",
      "Sol Erika Boman",
      "Andrea Discacciati",
      "Henrik Gr\u00f6nberg",
      "Lars Egevad",
      "Tobias Nordstr\u00f6m",
      "Kimmo Kartasalo",
      "Martin Eklund"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21045",
    "title": "Eye Feel You: A DenseNet-driven User State Prediction Approach",
    "abstract": "           Subjective self-reports, collected with eye-tracking data, reveal perceived states like fatigue, effort, and task difficulty. However, these reports are costly to collect and challenging to interpret consistently in longitudinal studies. In this work, we focus on determining whether objective gaze dynamics can reliably predict subjective reports across repeated recording rounds in the eye-tracking dataset. We formulate subjective-report prediction as a supervised regression problem and propose a DenseNet-based deep learning regressor that learns predictive representations from gaze velocity signals. We conduct two complementary experiments to clarify our aims. First, the cross-round generalization experiment tests whether models trained on earlier rounds transfer to later rounds, evaluating the models' ability to capture longitudinal changes. Second, cross-subject generalization tests models' robustness by predicting subjective outcomes for new individuals. These experiments aim to reduce reliance on hand-crafted feature designs and clarify which states of subjective experience systematically appear in oculomotor behavior over time.         ",
    "url": "https://arxiv.org/abs/2601.21045",
    "authors": [
      "Kamrul Hasan",
      "Oleg V. Komogortsev"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2601.21049",
    "title": "QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation",
    "abstract": "           User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.         ",
    "url": "https://arxiv.org/abs/2601.21049",
    "authors": [
      "Rita Qiuran Lyu",
      "Michelle Manqiao Wang",
      "Lei Shi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21050",
    "title": "SMKC: Sketch Based Kernel Correlation Images for Variable Cardinality Time Series Anomaly Detection",
    "abstract": "           Conventional anomaly detection in multivariate time series relies on the assumption that the set of observed variables remains static. In operational environments, however, monitoring systems frequently experience sensor churn. Signals may appear, disappear, or be renamed, creating data windows where the cardinality varies and may include values unseen during training. To address this challenge, we propose SMKC, a framework that decouples the dynamic input structure from the anomaly detector. We first employ permutation-invariant feature hashing to sketch raw inputs into a fixed size state sequence. We then construct a hybrid kernel image to capture global temporal structure through pairwise comparisons of the sequence and its derivatives. The model learns normal patterns using masked reconstruction and a teacher-student prediction objective. Our evaluation reveals that robust log-distance channels provide the primary discriminative signal, whereas cosine representations often fail to capture sufficient contrast. Notably, we find that a detector using random projections and nearest neighbors on the SMKC representation performs competitively with fully trained baselines without requiring gradient updates. This highlights the effectiveness of the representation itself and offers a practical cold-start solution for resource-constrained deployments.         ",
    "url": "https://arxiv.org/abs/2601.21050",
    "authors": [
      "Haokun Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21061",
    "title": "Signal from Structure: Exploiting Submodular Upper Bounds in Generative Flow Networks",
    "abstract": "           Generative Flow Networks (GFlowNets; GFNs) are a class of generative models that learn to sample compositional objects proportionally to their a priori unknown value, their reward. We focus on the case where the reward has a specified, actionable structure, namely that it is submodular. We show submodularity can be harnessed to retrieve upper bounds on the reward of compositional objects that have not yet been observed. We provide in-depth analyses of the probability of such bounds occurring, as well as how many unobserved compositional objects can be covered by a bound. Following the Optimism in the Face of Uncertainty principle, we then introduce SUBo-GFN, which uses the submodular upper bounds to train a GFN. We show that SUBo-GFN generates orders of magnitude more training data than classical GFNs for the same number of queries to the reward function. We demonstrate the effectiveness of SUBo-GFN in terms of distribution matching and high-quality candidate generation on synthetic and real-world submodular tasks.         ",
    "url": "https://arxiv.org/abs/2601.21061",
    "authors": [
      "Alexandre Larouche",
      "Audrey Durand"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2601.21066",
    "title": "BadDet+: Robust Backdoor Attacks for Object Detection",
    "abstract": "           Backdoor attacks pose a severe threat to deep learning, yet their impact on object detection remains poorly understood compared to image classification. While attacks have been proposed, we identify critical weaknesses in existing detection-based methods, specifically their reliance on unrealistic assumptions and a lack of physical validation. To bridge this gap, we introduce BadDet+, a penalty-based framework that unifies Region Misclassification Attacks (RMA) and Object Disappearance Attacks (ODA). The core mechanism utilizes a log-barrier penalty to suppress true-class predictions for triggered inputs, resulting in (i) position and scale invariance, and (ii) enhanced physical robustness. On real-world benchmarks, BadDet+ achieves superior synthetic-to-physical transfer compared to existing RMA and ODA baselines while preserving clean performance. Theoretical analysis confirms the proposed penalty acts within a trigger-specific feature subspace, reliably inducing attacks without degrading standard inference. These results highlight significant vulnerabilities in object detection and the necessity for specialized defenses.         ",
    "url": "https://arxiv.org/abs/2601.21066",
    "authors": [
      "Kealan Dunnett",
      "Reza Arablouei",
      "Dimity Miller",
      "Volkan Dedeoglu",
      "Raja Jurdak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2601.21067",
    "title": "Out-of-Distribution Generalization in Graph Foundation Models",
    "abstract": "           Graphs are a fundamental data structure for representing relational information in domains such as social networks, molecular systems, and knowledge graphs. However, graph learning models often suffer from limited generalization when applied beyond their training distributions. In practice, distribution shifts may arise from changes in graph structure, domain semantics, available modalities, or task formulations. To address these challenges, graph foundation models (GFMs) have recently emerged, aiming to learn general-purpose representations through large-scale pretraining across diverse graphs and tasks. In this survey, we review recent progress on GFMs from the perspective of out-of-distribution (OOD) generalization. We first discuss the main challenges posed by distribution shifts in graph learning and outline a unified problem setting. We then organize existing approaches based on whether they are designed to operate under a fixed task specification or to support generalization across heterogeneous task formulations, and summarize the corresponding OOD handling strategies and pretraining objectives. Finally, we review common evaluation protocols and discuss open directions for future research. To the best of our knowledge, this paper is the first survey for OOD generalization in GFMs.         ",
    "url": "https://arxiv.org/abs/2601.21067",
    "authors": [
      "Haoyang Li",
      "Haibo Chen",
      "Xin Wang",
      "Wenwu Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21083",
    "title": "OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence",
    "abstract": "           As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.21083",
    "authors": [
      "Jarrod Barnes"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21084",
    "title": "Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations",
    "abstract": "           Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MSE) loss between enhanced and clean speech. However, MSE is prone to exploiting positional embeddings in SSL models, allowing the objective to be minimised through positional correlations instead of content-related information. This work frames the problem as a general limitation of self-supervised representation fine-tuning and investigates it through representation-guided SE. Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss. Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance, underscoring the importance of position-invariant fine-tuning in SSL-based speech modelling.         ",
    "url": "https://arxiv.org/abs/2601.21084",
    "authors": [
      "Amit Meghanani",
      "Thomas Hain"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2601.21092",
    "title": "MapPFN: Learning Causal Perturbation Maps in Context",
    "abstract": "           Planning effective interventions in biological systems requires treatment-effect models that adapt to unseen biological contexts by identifying their specific underlying mechanisms. Yet single-cell perturbation datasets span only a handful of biological contexts, and existing methods cannot leverage new interventional evidence at inference time to adapt beyond their training data. To meta-learn a perturbation effect estimator, we present MapPFN, a prior-data fitted network (PFN) pretrained on synthetic data generated from a prior over causal perturbations. Given a set of experiments, MapPFN uses in-context learning to predict post-perturbation distributions, without gradient-based optimization. Despite being pretrained on in silico gene knockouts alone, MapPFN identifies differentially expressed genes, matching the performance of models trained on real single-cell data. Our code and data are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.21092",
    "authors": [
      "Marvin Sextro",
      "Weronika K\u0142os",
      "Gabriel Dernbach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21115",
    "title": "Multi-task Code LLMs: Data Mix or Model Merge?",
    "abstract": "           Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.         ",
    "url": "https://arxiv.org/abs/2601.21115",
    "authors": [
      "Mingzhi Zhu",
      "Boris Sobolev",
      "Rahul Krishna",
      "Raju Pavuluri",
      "Stacy Patterson",
      "Michele Merler"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21135",
    "title": "TRACE: Trajectory Recovery for Continuous Mechanism Evolution in Causal Representation Learning",
    "abstract": "           Temporal causal representation learning methods assume that causal mechanisms switch instantaneously between discrete domains, yet real-world systems often exhibit continuous mechanism transitions. For example, a vehicle's dynamics evolve gradually through a turning maneuver, and human gait shifts smoothly from walking to running. We formalize this setting by modeling transitional mechanisms as convex combinations of finitely many atomic mechanisms, governed by time-varying mixing coefficients. Our theoretical contributions establish that both the latent causal variables and the continuous mixing trajectory are jointly identifiable. We further propose TRACE, a Mixture-of-Experts framework where each expert learns one atomic mechanism during training, enabling recovery of mechanism trajectories at test time. This formulation generalizes to intermediate mechanism states never observed during training. Experiments on synthetic and real-world data demonstrate that TRACE recovers mixing trajectories with up to 0.99 correlation, substantially outperforming discrete-switching baselines.         ",
    "url": "https://arxiv.org/abs/2601.21135",
    "authors": [
      "Shicheng Fan",
      "Kun Zhang",
      "Lu Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21141",
    "title": "Optimization and Mobile Deployment for Anthropocene Neural Style Transfer",
    "abstract": "           This paper presents AnthropoCam, a mobile-based neural style transfer (NST) system optimized for the visual synthesis of Anthropocene environments. Unlike conventional artistic NST, which prioritizes painterly abstraction, stylizing human-altered landscapes demands a careful balance between amplifying material textures and preserving semantic legibility. Industrial infrastructures, waste accumulations, and modified ecosystems contain dense, repetitive patterns that are visually expressive yet highly susceptible to semantic erosion under aggressive style transfer. To address this challenge, we systematically investigate the impact of NST parameter configurations on the visual translation of Anthropocene textures, including feature layer selection, style and content loss weighting, training stability, and output resolution. Through controlled experiments, we identify an optimal parameter manifold that maximizes stylistic expression while preventing semantic erasure. Our results demonstrate that appropriate combinations of convolutional depth, loss ratios, and resolution scaling enable the faithful transformation of anthropogenic material properties into a coherent visual language. Building on these findings, we implement a low-latency, feed-forward NST pipeline deployed on mobile devices. The system integrates a React Native frontend with a Flask-based GPU backend, achieving high-resolution inference within 3-5 seconds on general mobile hardware. This enables real-time, in-situ visual intervention at the site of image capture, supporting participatory engagement with Anthropocene landscapes. By coupling domain-specific NST optimization with mobile deployment, AnthropoCam reframes neural style transfer as a practical and expressive tool for real-time environmental visualization in the Anthropocene.         ",
    "url": "https://arxiv.org/abs/2601.21141",
    "authors": [
      "Po-Hsun Chen",
      "Ivan C. H. Liu"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2601.21150",
    "title": "Can Neural Networks Learn Small Algebraic Worlds? An Investigation Into the Group-theoretic Structures Learned By Narrow Models Trained To Predict Group Operations",
    "abstract": "           While a real-world research program in mathematics may be guided by a motivating question, the process of mathematical discovery is typically open-ended. Ideally, exploration needed to answer the original question will reveal new structures, patterns, and insights that are valuable in their own right. This contrasts with the exam-style paradigm in which the machine learning community typically applies AI to math. To maximize progress in mathematics using AI, we will need to go beyond simple question answering. With this in mind, we explore the extent to which narrow models trained to solve a fixed mathematical task learn broader mathematical structure that can be extracted by a researcher or other AI system. As a basic test case for this, we use the task of training a neural network to predict a group operation (for example, performing modular arithmetic or composition of permutations). We describe a suite of tests designed to assess whether the model captures significant group-theoretic notions such as the identity element, commutativity, or subgroups. Through extensive experimentation we find evidence that models learn representations capable of capturing abstract algebraic properties. For example, we find hints that models capture the commutativity of modular arithmetic. We are also able to train linear classifiers that reliably distinguish between elements of certain subgroups (even though no labels for these subgroups are included in the data). On the other hand, we are unable to extract notions such as the concept of the identity element. Together, our results suggest that in some cases the representations of even small neural networks can be used to distill interesting abstract structure from new mathematical objects.         ",
    "url": "https://arxiv.org/abs/2601.21150",
    "authors": [
      "Henry Kvinge",
      "Andrew Aguilar",
      "Nayda Farnsworth",
      "Grace O'Brien",
      "Robert Jasper",
      "Sarah Scullen",
      "Helen Jenne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21151",
    "title": "Learning to Advect: A Neural Semi-Lagrangian Architecture for Weather Forecasting",
    "abstract": "           Recent machine-learning approaches to weather forecasting often employ a monolithic architecture, where distinct physical mechanisms (advection, transport), diffusion-like mixing, thermodynamic processes, and forcing are represented implicitly within a single large network. This representation is particularly problematic for advection, where long-range transport must be treated with expensive global interaction mechanisms or through deep, stacked convolutional layers. To mitigate this, we present PARADIS, a physics-inspired global weather prediction model that imposes inductive biases on network behavior through a functional decomposition into advection, diffusion, and reaction blocks acting on latent variables. We implement advection through a Neural Semi-Lagrangian operator that performs trajectory-based transport via differentiable interpolation on the sphere, enabling end-to-end learning of both the latent modes to be transported and their characteristic trajectories. Diffusion-like processes are modeled through depthwise-separable spatial mixing, while local source terms and vertical interactions are modeled via pointwise channel interactions, enabling operator-level physical structure. PARADIS provides state-of-the-art forecast skill at a fraction of the training cost. On ERA5-based benchmarks, the 1 degree PARADIS model, with a total training cost of less than a GPU month, meets or exceeds the performance of 0.25 degree traditional and machine-learning baselines, including the ECMWF HRES forecast and DeepMind's GraphCast.         ",
    "url": "https://arxiv.org/abs/2601.21151",
    "authors": [
      "Carlos A. Pereira",
      "St\u00e9phane Gaudreault",
      "Valentin Dallerit",
      "Christopher Subich",
      "Shoyon Panday",
      "Siqi Wei",
      "Sasa Zhang",
      "Siddharth Rout",
      "Eldad Haber",
      "Raymond J. Spiteri",
      "David Millard",
      "Emilia Diaconescu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21157",
    "title": "Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning",
    "abstract": "           While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from \"Arithmetic Hallucinations\" and a systemic failure mode we term \"Cognitive Collapse\". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\\% to 67.3\\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.         ",
    "url": "https://arxiv.org/abs/2601.21157",
    "authors": [
      "Boxiang Zhao",
      "Qince Li",
      "Zhonghao Wang",
      "Yi Wang",
      "Peng Cheng",
      "Bo Lin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2601.21162",
    "title": "A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning",
    "abstract": "           Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.         ",
    "url": "https://arxiv.org/abs/2601.21162",
    "authors": [
      "Jiate Liu",
      "Zebin Chen",
      "Shaobo Qiao",
      "Mingchen Ju",
      "Danting Zhang",
      "Bocheng Han",
      "Shuyue Yu",
      "Xin Shu",
      "Jingling Wu",
      "Dong Wen",
      "Xin Cao",
      "Guanfeng Liu",
      "Zhengyi Yang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2601.21170",
    "title": "The Powers of Precision: Structure-Informed Detection in Complex Systems -- From Customer Churn to Seizure Onset",
    "abstract": "           Emergent phenomena -- onset of epileptic seizures, sudden customer churn, or pandemic outbreaks -- often arise from hidden causal interactions in complex systems. We propose a machine learning method for their early detection that addresses a core challenge: unveiling and harnessing a system's latent causal structure despite the data-generating process being unknown and partially observed. The method learns an optimal feature representation from a one-parameter family of estimators -- powers of the empirical covariance or precision matrix -- offering a principled way to tune in to the underlying structure driving the emergence of critical events. A supervised learning module then classifies the learned representation. We prove structural consistency of the family and demonstrate the empirical soundness of our approach on seizure detection and churn prediction, attaining competitive results in both. Beyond prediction, and toward explainability, we ascertain that the optimal covariance power exhibits evidence of good identifiability while capturing structural signatures, thus reconciling predictive performance with interpretable statistical structure.         ",
    "url": "https://arxiv.org/abs/2601.21170",
    "authors": [
      "Augusto Santos",
      "Teresa Santos",
      "Catarina Rodrigues",
      "Jos\u00e9 M. F. Moura"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2601.21171",
    "title": "AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection",
    "abstract": "           Graph anomaly detection aims to identify abnormal patterns in networks, but faces significant challenges from label scarcity and extreme class imbalance. While graph contrastive learning offers a promising unsupervised solution, existing methods suffer from two critical limitations: random augmentations break semantic consistency in positive pairs, while naive negative sampling produces trivial, uninformative contrasts. We propose AC2L-GAD, an Active Counterfactual Contrastive Learning framework that addresses both limitations through principled counterfactual reasoning. By combining information-theoretic active selection with counterfactual generation, our approach identifies structurally complex nodes and generates anomaly-preserving positive augmentations alongside normal negative counterparts that provide hard contrasts, while restricting expensive counterfactual generation to a strategically selected subset. This design reduces computational overhead by approximately 65% compared to full-graph counterfactual generation while maintaining detection quality. Experiments on nine benchmark datasets, including real-world financial transaction graphs from GADBench, show that AC2L-GAD achieves competitive or superior performance compared to state-of-the-art baselines, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.         ",
    "url": "https://arxiv.org/abs/2601.21171",
    "authors": [
      "Kamal Berahmand",
      "Saman Forouzandeh",
      "Mehrnoush Mohammadi",
      "Parham Moradi",
      "Mahdi Jalili"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21189",
    "title": "Adaptive and Robust Cost-Aware Proof of Quality for Decentralized LLM Inference Networks",
    "abstract": "           Decentralized large language model inference networks require lightweight mechanisms to reward high quality outputs under heterogeneous latency and cost. Proof of Quality provides scalable verification by sampling evaluator nodes that score candidate outputs, then aggregating their scores into a consensus signal that determines rewards. However, evaluator heterogeneity and malicious score manipulation can distort consensus and inflate payouts, which weakens incentive alignment in open participation settings. This paper extends a cost-aware Proof of Quality mechanism by adding adversary-resilient consensus formation. We study robust aggregation rules, including median and trimmed mean, and an adaptive trust-weighted consensus that updates evaluator weights from deviation signals. Using question answering and summarization workloads with a ground truth proxy for offline analysis, we quantify evaluator reliability and show strong variance across evaluators, including task-dependent misalignment that can invert correlations. We then evaluate robustness under four adversarial strategies, including noise injection, boosting, sabotage, and intermittent manipulation, across a sweep of malicious ratios and evaluator sample sizes. Our results show that robust aggregation improves consensus alignment with the ground truth proxy and reduces sensitivity to noisy and strategic attacks compared with simple averaging. We further characterize the operational trade-off introduced by evaluator sampling, where larger evaluator sets reduce evaluator rewards and increase payoff variance while inference rewards remain relatively stable in our configuration. These findings motivate robust consensus as a default component for cost-aware Proof of Quality and provide practical guidance for selecting evaluator sampling parameters under adversarial risk and resource constraints.         ",
    "url": "https://arxiv.org/abs/2601.21189",
    "authors": [
      "Arther Tian",
      "Alex Ding",
      "Frank Chen",
      "Simon Wu",
      "Aaron Chan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21192",
    "title": "Do Reasoning Models Enhance Embedding Models?",
    "abstract": "           State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.         ",
    "url": "https://arxiv.org/abs/2601.21192",
    "authors": [
      "Wun Yu Chan",
      "Shaojin Chen",
      "Huihao Jing",
      "Kwun Hang Lau",
      "Elton Chun-Chai Li",
      "Zihao Wang",
      "Haoran Li",
      "Yangqiu Song"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2601.21207",
    "title": "A Sheaf-Theoretic and Topological Perspective on Complex Network Modeling and Attention Mechanisms in Graph Neural Models",
    "abstract": "           Combinatorial and topological structures, such as graphs, simplicial complexes, and cell complexes, form the foundation of geometric and topological deep learning (GDL and TDL) architectures. These models aggregate signals over such domains, integrate local features, and generate representations for diverse real-world applications. However, the distribution and diffusion behavior of GDL and TDL features during training remains an open and underexplored problem. Motivated by this gap, we introduce a cellular sheaf theoretic framework for modeling and analyzing the local consistency and harmonicity of node features and edge weights in graph-based architectures. By tracking local feature alignments and agreements through sheaf structures, the framework offers a topological perspective on feature diffusion and aggregation. Furthermore, a multiscale extension inspired by topological data analysis (TDA) is proposed to capture hierarchical feature interactions in graph models. This approach enables a joint characterization of GDL and TDL architectures based on their underlying geometric and topological structures and the learned signals defined on them, providing insights for future studies on conventional tasks such as node classification, substructure detection, and community detection.         ",
    "url": "https://arxiv.org/abs/2601.21207",
    "authors": [
      "Chuan-Shen Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Algebraic Topology (math.AT)"
    ]
  },
  {
    "id": "arXiv:2601.21208",
    "title": "When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning",
    "abstract": "           Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.         ",
    "url": "https://arxiv.org/abs/2601.21208",
    "authors": [
      "Wei Wen",
      "Sihang Deng",
      "Tianjun Wei",
      "Keyu Chen",
      "Ruizhi Qiao",
      "Xing Sun"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2601.21210",
    "title": "Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification",
    "abstract": "           Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.         ",
    "url": "https://arxiv.org/abs/2601.21210",
    "authors": [
      "Paul He",
      "Yinya Huang",
      "Mrinmaya Sachan",
      "Zhijing Jin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21211",
    "title": "SPOILER-GUARD: Gating Latency Effects of Memory Accesses through Randomized Dependency Prediction",
    "abstract": "           Modern microprocessors depend on speculative execution, creating vulnerabilities that enable transient execution attacks. Prior defenses target speculative data leakage but overlook false dependencies from partial address aliasing, where repeated squash and reissue events increase the load-store latency, which is exploited by the SPOILER attack. We present SPOILER-GUARD, a hardware defense that obfuscates speculative dependency resolution by dynamically randomizing the physical address bits used for load-store comparisons and tagging store entries to prevent latency-amplifying misspeculations. Implemented in gem5 and evaluated with SPEC 2017, SPOILER-GUARD reduces misspeculation to 0.0004 percent and improves integer and floating-point performance by 2.12 and 2.87 percent. HDL synthesis with Synopsys Design Compiler at 14 nm node demonstrates minimal overheads - 69 ps latency in critical path, 0.064 square millimeter in area, and 5.863 mW in power.         ",
    "url": "https://arxiv.org/abs/2601.21211",
    "authors": [
      "Gayathri Subramanian",
      "Girinath P",
      "Nitya Ranganathan",
      "Kamakoti Veezhinathan",
      "Gopalakrishnan Srinivasan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2601.21220",
    "title": "LAMP: Learning Universal Adversarial Perturbations for Multi-Image Tasks via Pre-trained Models",
    "abstract": "           Multimodal Large Language Models (MLLMs) have achieved remarkable performance across vision-language tasks. Recent advancements allow these models to process multiple images as inputs. However, the vulnerabilities of multi-image MLLMs remain unexplored. Existing adversarial attacks focus on single-image settings and often assume a white-box threat model, which is impractical in many real-world scenarios. This paper introduces LAMP, a black-box method for learning Universal Adversarial Perturbations (UAPs) targeting multi-image MLLMs. LAMP applies an attention-based constraint that prevents the model from effectively aggregating information across images. LAMP also introduces a novel cross-image contagious constraint that forces perturbed tokens to influence clean tokens, spreading adversarial effects without requiring all inputs to be modified. Additionally, an index-attention suppression loss enables a robust position-invariant attack. Experimental results show that LAMP outperforms SOTA baselines and achieves the highest attack success rates across multiple vision-language tasks and models.         ",
    "url": "https://arxiv.org/abs/2601.21220",
    "authors": [
      "Alvi Md Ishmam",
      "Najibul Haque Sarker",
      "Zaber Ibn Abdul Hakim",
      "Chris Thomas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21221",
    "title": "Causal Discovery for Explainable AI: A Dual-Encoding Approach",
    "abstract": "           Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.         ",
    "url": "https://arxiv.org/abs/2601.21221",
    "authors": [
      "Henry Salgado",
      "Meagan R. Kendall",
      "Martine Ceberio"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21222",
    "title": "FireFly-P: FPGA-Accelerated Spiking Neural Network Plasticity for Robust Adaptive Control",
    "abstract": "           Spiking Neural Networks (SNNs) offer a biologically plausible learning mechanism through synaptic plasticity, enabling unsupervised adaptation without the computational overhead of backpropagation. To harness this capability for robotics, this paper presents FireFly-P, an FPGA-based hardware accelerator that implements a novel plasticity algorithm for real-time adaptive control. By leveraging on-chip plasticity, our architecture enhances the network's generalization, ensuring robust performance in dynamic and unstructured environments. The hardware design achieves an end-to-end latency of just 8~$\\mu$s for both inference and plasticity updates, enabling rapid adaptation to unseen scenarios. Implemented on a tiny Cmod A7-35T FPGA, FireFly-P consumes only 0.713~W and $\\sim$10K~LUTs, making it ideal for power- and resource-constrained embedded robotic platforms. This work demonstrates that hardware-accelerated SNN plasticity is a viable path toward enabling adaptive, low-latency, and energy-efficient control systems.         ",
    "url": "https://arxiv.org/abs/2601.21222",
    "authors": [
      "Tenglong Li",
      "Jindong Li",
      "Guobin Shen",
      "Dongcheng Zhao",
      "Qian Zhang",
      "Yi Zeng"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2601.21225",
    "title": "MGSM-Pro: A Simple Strategy for Robust Multilingual Mathematical Reasoning Evaluation",
    "abstract": "           Large language models have made substantial progress in mathematical reasoning. However, benchmark development for multilingual evaluation has lagged behind English in both difficulty and recency. Recently, GSM-Symbolic showed a strong evidence of high variance when models are evaluated on different instantiations of the same question; however, the evaluation was conducted only in English. In this paper, we introduce MGSM-Pro, an extension of MGSM dataset with GSM-Symbolic approach. Our dataset provides five instantiations per MGSM question by varying names, digits and irrelevant context. Evaluations across nine languages reveal that many low-resource languages suffer large performance drops when tested on digit instantiations different from those in the original test set. We further find that some proprietary models, notably Gemini 2.5 Flash and GPT-4.1, are less robust to digit instantiation, whereas Claude 4.0 Sonnet is more robust. Among open models, GPT-OSS 120B and DeepSeek V3 show stronger robustness. Based on these findings, we recommend evaluating each problem using at least five digit-varying instantiations to obtain a more robust and realistic assessment of math reasoning.         ",
    "url": "https://arxiv.org/abs/2601.21225",
    "authors": [
      "Tianyi Xu",
      "Kosei Uemura",
      "Alfred Malengo Kondoro",
      "Tadesse Destaw Belay",
      "Catherine Nana Nyaah Essuman",
      "Ifeoma Okoh",
      "Ganiyat Afolabi",
      "Ayodele Awokoya",
      "David Ifeoluwa Adelani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21233",
    "title": "Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs",
    "abstract": "           Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \\textbf{\\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \\textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound--based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \\textbf{41} black-box commercial models across multiple providers, \\textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.         ",
    "url": "https://arxiv.org/abs/2601.21233",
    "authors": [
      "Xiang Zheng",
      "Yutao Wu",
      "Hanxun Huang",
      "Yige Li",
      "Xingjun Ma",
      "Bo Li",
      "Yu-Gang Jiang",
      "Cong Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21235",
    "title": "SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models",
    "abstract": "           Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.         ",
    "url": "https://arxiv.org/abs/2601.21235",
    "authors": [
      "Alok Abhishek",
      "Tushar Bandopadhyay",
      "Lisa Erickson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21236",
    "title": "Deletion-correcting codes for an adversarial nanopore channel",
    "abstract": "           We study deletion-correcting codes for an adversarial nanopore channel in which at most $t$ deletions may occur. We propose an explicit construction of $q$-ary codes of length $n$ for this channel with $2t\\log_q n+\\Theta(\\log\\log n)$ redundant symbols. We also show that the optimal redundancy is between $t\\log_q n+\\Omega(1)$ and $2t\\log_q n-\\log_q\\log_2 n+O(1)$, so our explicit construction matches the existential upper bound to first order. In contrast, for the classical adversarial $q$-ary deletion channel, the smallest redundancy achieved by known explicit constructions that correct up to $t$ deletions is $4t(1+\\epsilon)\\log_q n+o(\\log n)$.         ",
    "url": "https://arxiv.org/abs/2601.21236",
    "authors": [
      "Huiling Xie",
      "Zitan Chen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2601.21242",
    "title": "Understanding Diffusion Models via Ratio-Based Function Approximation with SignReLU Networks",
    "abstract": "           Motivated by challenges in conditional generative modeling, where the target conditional density takes the form of a ratio f1 over f2, this paper develops a theoretical framework for approximating such ratio-type functionals. Here, f1 and f2 are kernel-based marginal densities that capture structured interactions, a setting central to diffusion-based generative models. We provide a concise proof for approximating these ratio-type functionals using deep neural networks with the SignReLU activation function, leveraging the activation's piecewise structure. Under standard regularity assumptions, we establish L^p(Omega) approximation bounds and convergence rates. Specializing to Denoising Diffusion Probabilistic Models (DDPMs), we construct a SignReLU-based neural estimator for the reverse process and derive bounds on the excess Kullback-Leibler (KL) risk between the generated and true data distributions. Our analysis decomposes this excess risk into approximation and estimation error components. These results provide generalization guarantees for finite-sample training of diffusion-based generative models.         ",
    "url": "https://arxiv.org/abs/2601.21242",
    "authors": [
      "Luwei Sun",
      "Dongrui Shen",
      "Jianfe Li",
      "Yulong Zhao",
      "Han Feng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21246",
    "title": "Conditional Generative Framework with Peak-Aware Attention for Robust Chemical Detection under Interferences",
    "abstract": "           Gas chromatography-mass spectrometry (GC-MS) is a widely used analytical method for chemical substance detection, but measurement reliability tends to deteriorate in the presence of interfering substances. In particular, interfering substances cause nonspecific peaks, residence time shifts, and increased background noise, resulting in reduced sensitivity and false alarms. To overcome these challenges, in this paper, we propose an artificial intelligence discrimination framework based on a peak-aware conditional generative model to improve the reliability of GC-MS measurements under interference conditions. The framework is learned with a novel peak-aware mechanism that highlights the characteristic peaks of GC-MS data, allowing it to generate important spectral features more faithfully. In addition, chemical and solvent information is encoded in a latent vector embedded with it, allowing a conditional generative adversarial neural network (CGAN) to generate a synthetic GC-MS signal consistent with the experimental conditions. This generates an experimental dataset that assumes indirect substance situations in chemical substance data, where acquisition is limited without conducting real experiments. These data are used for the learning of AI-based GC-MS discrimination models to help in accurate chemical substance discrimination. We conduct various quantitative and qualitative evaluations of the generated simulated data to verify the validity of the proposed framework. We also verify how the generative model improves the performance of the AI discrimination framework. Representatively, the proposed method is shown to consistently achieve cosine similarity and Pearson correlation coefficient values above 0.9 while preserving peak number diversity and reducing false alarms in the discrimination model.         ",
    "url": "https://arxiv.org/abs/2601.21246",
    "authors": [
      "Namkyung Yoon",
      "Sanghong Kim",
      "Hwangnam Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21259",
    "title": "The Role of Social Identity in Shaping Biases Against Minorities in Software Organizations",
    "abstract": "           While systemic workplace bias is well-documented in non-computing fields, its specific impact on software engineers remains poorly understood. This study addresses that gap by applying Social Identity Theory (SIT) to investigate four distinct forms of bias: lack of career development, stereotyped task selection, unwelcoming environments, and identity attacks. Using a vignette-based survey, we quantified the prevalence of these biases, identified the demographics most affected, assessed their consequences, and explored the motivations behind biased actions. Our results show that career development and task selection biases are the most prevalent forms, with over two-thirds of victims experiencing them multiple times. Women were more than three times as likely as men to face career development bias, task selection bias, and an unwelcoming environment. In parallel, individuals from marginalized ethnic backgrounds were disproportionately targeted by identity attacks. Our analysis also confirms that, beyond gender and race, factors such as age, years of experience, organization size, and geographic location are significant predictors of bias victimization.         ",
    "url": "https://arxiv.org/abs/2601.21259",
    "authors": [
      "Sayma Sultana",
      "London Cavaletto",
      "Bianca Trinkenreich",
      "Amiangshu Bosu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2601.21262",
    "title": "CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding",
    "abstract": "           Although Multimodal Large Language Models (MLLMs) have shown remarkable potential in Visual Document Retrieval (VDR) through generating high-quality multi-vector embeddings, the substantial storage overhead caused by representing a page with thousands of visual tokens limits their practicality in real-world applications. To address this challenge, we propose an auto-regressive generation approach, CausalEmbed, for constructing multi-vector embeddings. By incorporating iterative margin loss during contrastive training, CausalEmbed encourages the embedding models to learn compact and well-structured representations. Our method enables efficient VDR tasks using only dozens of visual tokens, achieving a 30-155x reduction in token count while maintaining highly competitive performance across various backbones and benchmarks. Theoretical analysis and empirical results demonstrate the unique advantages of auto-regressive embedding generation in terms of training efficiency and scalability at test time. As a result, CausalEmbed introduces a flexible test-time scaling strategy for multi-vector VDR representations and sheds light on the generative paradigm within multimodal document retrieval.         ",
    "url": "https://arxiv.org/abs/2601.21262",
    "authors": [
      "Jiahao Huo",
      "Yu Huang",
      "Yibo Yan",
      "Ye Pan",
      "Yi Cao",
      "Mingdong Ou",
      "Philip S. Yu",
      "Xuming Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2601.21266",
    "title": "Model-Free Neural State Estimation in Nonlinear Dynamical Systems: A Comparative Study of Neural Architectures and Classical Filters",
    "abstract": "           Neural network models are increasingly used for state estimation in control and decision-making problems, yet it remains unclear to what extent they behave as principled filters in nonlinear dynamical systems. Unlike classical filters, which rely on explicit knowledge of system dynamics and noise models, neural estimators can be trained purely from data without access to the underlying system equations. In this work, we present a systematic empirical comparison between such model-free neural network models and classical filtering methods across multiple nonlinear scenarios. Our study evaluates Transformer-based models, state-space neural networks, and recurrent architectures alongside particle filters and nonlinear Kalman filters. The results show that neural models (in particular, state-space models (SSMs)) achieve state estimation performance that approaches strong nonlinear Kalman filters in nonlinear scenarios and outperform weaker classical baselines despite lacking access to system models, while also attaining substantially higher inference throughput.         ",
    "url": "https://arxiv.org/abs/2601.21266",
    "authors": [
      "Zhuochen Liu",
      "Hans Walker",
      "Rahul Jain"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21276",
    "title": "More Code, Less Reuse: Investigating Code Quality and Reviewer Sentiment towards AI-generated Pull Requests",
    "abstract": "           Large Language Model (LLM) Agents are advancing quickly, with the increasing leveraging of LLM Agents to assist in development tasks such as code generation. While LLM Agents accelerate code generation, studies indicate they may introduce adverse effects on development. However, existing metrics solely measure pass rates, failing to reflect impacts on long-term maintainability and readability, and failing to capture human intuitive evaluations of PR. To increase the comprehensiveness of this problem, we investigate and evaluate the characteristics of LLM to know the pull requests' characteristics beyond the pass rate. We observe the code quality and maintainability within PRs based on code metrics to evaluate objective characteristics and developers' reactions to the pull requests from both humans and LLM's generation. Evaluation results indicate that LLM Agents frequently disregard code reuse opportunities, resulting in higher levels of redundancy compared to human developers. In contrast to the quality issues, our emotions analysis reveals that reviewers tend to express more neutral or positive emotions towards AI-generated contributions than human ones. This disconnect suggests that the surface-level plausibility of AI code masks redundancy, leading to the silent accumulation of technical debt in real-world development environments. Our research provides insights for improving human-AI collaboration.         ",
    "url": "https://arxiv.org/abs/2601.21276",
    "authors": [
      "Haoming Huang",
      "Pongchai Jaisri",
      "Shota Shimizu",
      "Lingfeng Chen",
      "Sota Nakashima",
      "Gema Rodr\u00edguez-P\u00e9rez"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2601.21281",
    "title": "EGAM: Extended Graph Attention Model for Solving Routing Problems",
    "abstract": "           Neural combinatorial optimization (NCO) solvers, implemented with graph neural networks (GNNs), have introduced new approaches for solving routing problems. Trained with reinforcement learning (RL), the state-of-the-art graph attention model (GAM) achieves near-optimal solutions without requiring expert knowledge or labeled data. In this work, we generalize the existing graph attention mechanism and propose the extended graph attention model (EGAM). Our model utilizes multi-head dot-product attention to update both node and edge embeddings, addressing the limitations of the conventional GAM, which considers only node features. We employ an autoregressive encoder-decoder architecture and train it with policy gradient algorithms that incorporate a specially designed baseline. Experiments show that EGAM matches or outperforms existing methods across various routing problems. Notably, the proposed model demonstrates exceptional performance on highly constrained problems, highlighting its efficiency in handling complex graph structures.         ",
    "url": "https://arxiv.org/abs/2601.21281",
    "authors": [
      "Licheng Wang",
      "Yuzi Yan",
      "Mingtao Huang",
      "Yuan Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21287",
    "title": "Towards Zero Rotation and Beyond: Architecting Neural Networks for Fast Secure Inference with Homomorphic Encryption",
    "abstract": "           Privacy-preserving deep learning addresses privacy concerns in Machine Learning as a Service (MLaaS) by using Homomorphic Encryption (HE) for linear computations. However, the computational overhead remains a major challenge. While prior work has improved efficiency, most approaches build on models originally designed for plaintext inference. Such models incur architectural inefficiencies when adapted to HE. We argue that substantial gains require networks tailored to HE rather than retrofitting plaintext architectures. Our design has two components: the building block and the overall architecture. First, StriaBlock targets the most expensive HE operation, rotation. It integrates ExRot-Free Convolution and a novel Cross Kernel, eliminating external rotations and requiring only 19% of the internal rotations used by plaintext models. Second, our architectural principles include (i) the Focused Constraint Principle, which limits cost-sensitive factors while preserving flexibility elsewhere, and (ii) the Channel Packing-Aware Scaling Principle, which adapts bottleneck ratios to ciphertext channel capacity that varies with depth. Together, these strategies control both local and end-to-end HE cost, enabling a balanced HE-tailored network. We evaluate the resulting StriaNet across datasets of varying scales, including ImageNet, Tiny ImageNet, and CIFAR-10. At comparable accuracy, StriaNet achieves speedups of 9.78x, 6.01x, and 9.24x on ImageNet, Tiny ImageNet, and CIFAR-10, respectively.         ",
    "url": "https://arxiv.org/abs/2601.21287",
    "authors": [
      "Yifei Cai",
      "Yizhou Feng",
      "Qiao Zhang",
      "Chunsheng Xin",
      "Hongyi Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2601.21291",
    "title": "Gaussian Belief Propagation Network for Depth Completion",
    "abstract": "           Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\& parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN's superior performance, notable robustness, and generalizable capability.         ",
    "url": "https://arxiv.org/abs/2601.21291",
    "authors": [
      "Jie Tang",
      "Pingping Xie",
      "Jian Li",
      "Ping Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21298",
    "title": "Detecting Multiple Semantic Concerns in Tangled Code Commits",
    "abstract": "           Code commits in a version control system (e.g., Git) should be atomic, i.e., focused on a single goal, such as adding a feature or fixing a bug. In practice, however, developers often bundle multiple concerns into tangled commits, obscuring intent and complicating maintenance. Recent studies have used Conventional Commits Specification (CCS) and Language Models (LMs) to capture commit intent, demonstrating that Small Language Models (SLMs) can approach the performance of Large Language Models (LLMs) while maintaining efficiency and privacy. However, they do not address tangled commits involving multiple concerns, leaving the feasibility of using LMs for multi-concern detection unresolved. In this paper, we frame multi-concern detection in tangled commits as a multi-label classification problem and construct a controlled dataset of artificially tangled commits based on real-world data. We then present an empirical study using SLMs to detect multiple semantic concerns in tangled commits, examining the effects of fine-tuning, concern count, commit-message inclusion, and header-preserving truncation under practical token-budget limits. Our results show that a fine-tuned 14B-parameter SLM is competitive with a state-of-the-art LLM for single-concern commits and remains usable for up to three concerns. In particular, including commit messages improves detection accuracy by up to 44% (in terms of Hamming Loss) with negligible latency overhead, establishing them as important semantic cues.         ",
    "url": "https://arxiv.org/abs/2601.21298",
    "authors": [
      "Beomsu Koh",
      "Neil Walkinshaw",
      "Donghwan Shin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2601.21299",
    "title": "Collective Noise Filtering in Complex Networks",
    "abstract": "           Complex networks are powerful representations of complex systems across scales and domains, and the field is experiencing unprecedented growth in data availability. However, real-world network data often suffer from noise, biases, and missing data in the edge weights, which undermine the reliability of downstream network analyses. Standard noise filtering approaches, whether treating individual edges one-by-one or assuming a uniform global noise level, are suboptimal, because in reality both signal and noise can be heterogeneous and correlated across multiple edges. As a solution, we introduce the Network Wiener Filter, a principled method for collective edge-level noise filtering that leverages both network topology and noise characteristics, to reduce error in the observed edge weights and to infer missing edge weights. We demonstrate the broad practical efficacy of the Network Wiener Filter in two distinct settings, the genetic interaction network of the yeast S. cerevisiae and the Enron Corpus email network, noting compelling evidence of successful noise suppression in both applications. With the Network Wiener Filter, we advocate for a shift toward error-aware network science, one that embraces data imperfection as an inherent feature and learns to navigate it effectively.         ",
    "url": "https://arxiv.org/abs/2601.21299",
    "authors": [
      "Tingyu Zhao",
      "Istv\u00e1n A. Kov\u00e1cs"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Signal Processing (eess.SP)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2601.21309",
    "title": "Transferable Graph Condensation from the Causal Perspective",
    "abstract": "           The increasing scale of graph datasets has significantly improved the performance of graph representation learning methods, but it has also introduced substantial training challenges. Graph dataset condensation techniques have emerged to compress large datasets into smaller yet information-rich datasets, while maintaining similar test performance. However, these methods strictly require downstream applications to match the original dataset and task, which often fails in cross-task and cross-domain scenarios. To address these challenges, we propose a novel causal-invariance-based and transferable graph dataset condensation method, named \\textbf{TGCC}, providing effective and transferable condensed datasets. Specifically, to preserve domain-invariant knowledge, we first extract domain causal-invariant features from the spatial domain of the graph using causal interventions. Then, to fully capture the structural and feature information of the original graph, we perform enhanced condensation operations. Finally, through spectral-domain enhanced contrastive learning, we inject the causal-invariant features into the condensed graph, ensuring that the compressed graph retains the causal information of the original graph. Experimental results on five public datasets and our novel \\textbf{FinReport} dataset demonstrate that TGCC achieves up to a 13.41\\% improvement in cross-task and cross-domain complex scenarios compared to existing methods, and achieves state-of-the-art performance on 5 out of 6 datasets in the single dataset and task scenario.         ",
    "url": "https://arxiv.org/abs/2601.21309",
    "authors": [
      "Huaming Du",
      "Yijie Huang",
      "Su Yao",
      "Yiying Wang",
      "Yueyang Zhou",
      "Jingwen Yang",
      "Jinshi Zhang",
      "Han Ji",
      "Yu Zhao",
      "Guisong Liu",
      "Hegui Zhang",
      "Carl Yang",
      "Gang Kou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21315",
    "title": "Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation",
    "abstract": "           Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.         ",
    "url": "https://arxiv.org/abs/2601.21315",
    "authors": [
      "Seonghwi Kim",
      "Sung Ho Jo",
      "Wooseok Ha",
      "Minwoo Chae"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21316",
    "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
    "abstract": "           Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.21316",
    "authors": [
      "Aoyu Pang",
      "Maonan Wang",
      "Zifan Sha",
      "Wenwei Yue",
      "Changle Li",
      "Chung Shue Chen",
      "Man-On Pun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21323",
    "title": "Adversarial Vulnerability Transcends Computational Paradigms: Feature Engineering Provides No Defense Against Neural Adversarial Transfer",
    "abstract": "           Deep neural networks are vulnerable to adversarial examples--inputs with imperceptible perturbations causing misclassification. While adversarial transfer within neural networks is well-documented, whether classical ML pipelines using handcrafted features inherit this vulnerability when attacked via neural surrogates remains unexplored. Feature engineering creates information bottlenecks through gradient quantization and spatial binning, potentially filtering high-frequency adversarial signals. We evaluate this hypothesis through the first comprehensive study of adversarial transfer from DNNs to HOG-based classifiers. Using VGG16 as a surrogate, we generate FGSM and PGD adversarial examples and test transfer to four classical classifiers (KNN, Decision Tree, Linear SVM, Kernel SVM) and a shallow neural network across eight HOG configurations on CIFAR-10. Our results strongly refute the protective hypothesis: all classifiers suffer 16.6%-59.1% relative accuracy drops, comparable to neural-to-neural transfer. More surprisingly, we discover attack hierarchy reversal--contrary to patterns where iterative PGD dominates FGSM within neural networks, FGSM causes greater degradation than PGD in 100% of classical ML cases, suggesting iterative attacks overfit to surrogate-specific features that don't survive feature extraction. Block normalization provides partial but insufficient mitigation. These findings demonstrate that adversarial vulnerability is not an artifact of end-to-end differentiability but a fundamental property of image classification systems, with implications for security-critical deployments across computational paradigms.         ",
    "url": "https://arxiv.org/abs/2601.21323",
    "authors": [
      "Achraf Hsain",
      "Ahmed Abdelkader",
      "Emmanuel Baldwin Mbaya",
      "Hamoud Aljamaan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21331",
    "title": "An introductory Generalization of the standard SVMs loss and its applications to Shallow and Deep Neural Networks",
    "abstract": "           We propose a new convex loss for SVMs, both for the binary classification and for the regression models. Therefore, we show the mathematical derivation of the dual problems and we experiment them with several small data-sets. The minimal dimension of those data-sets is due to the difficult scalability of the SVM method to bigger instances. This preliminary study should prove that using pattern correlations inside the loss function could enhance the generalisation performances. Coherently, results show that generalisation measures are never worse than the standard losses and several times they are better. In our opinion, it should be considered a careful study of this loss, coupled with shallow and deep neural networks. In fact, we present some novel results obtained with those architectures.         ",
    "url": "https://arxiv.org/abs/2601.21331",
    "authors": [
      "Filippo Portera"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21335",
    "title": "Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation",
    "abstract": "           Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.         ",
    "url": "https://arxiv.org/abs/2601.21335",
    "authors": [
      "Yuzhe Chen",
      "Jie Cao",
      "Youquan Wang",
      "Haicheng Tao",
      "Darko B. Vukovic",
      "Jia Wu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21350",
    "title": "Factored Causal Representation Learning for Robust Reward Modeling in RLHF",
    "abstract": "           A reliable reward model is essential for aligning large language models with human preferences through reinforcement learning from human feedback. However, standard reward models are susceptible to spurious features that are not causally related to human labels. This can lead to reward hacking, where high predicted reward does not translate into better behavior. In this work, we address this problem from a causal perspective by proposing a factored representation learning framework that decomposes the model's contextual embedding into (1) causal factors that are sufficient for reward prediction and (2) non-causal factors that capture reward-irrelevant attributes such as length or sycophantic bias. The reward head is then constrained to depend only on the causal component. In addition, we introduce an adversarial head trained to predict reward from the non-causal factors, while applying gradient reversal to discourage them from encoding reward-relevant information. Experiments on both mathematical and dialogue tasks demonstrate that our method learns more robust reward models and consistently improves downstream RLHF performance over state-of-the-art baselines. Analyses on length and sycophantic bias further validate the effectiveness of our method in mitigating reward hacking behaviors.         ",
    "url": "https://arxiv.org/abs/2601.21350",
    "authors": [
      "Yupei Yang",
      "Lin Yang",
      "Wanxi Deng",
      "Lin Qu",
      "Fan Feng",
      "Biwei Huang",
      "Shikui Tu",
      "Lei Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21360",
    "title": "The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation",
    "abstract": "           The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread \"False Certification\" of functionally broken code. Our findings suggest that current alignment paradigms create a \"Trojan\" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.         ",
    "url": "https://arxiv.org/abs/2601.21360",
    "authors": [
      "Devanshu Sahoo",
      "Manish Prasad",
      "Vasudev Majhi",
      "Arjun Neekhra",
      "Yash Sinha",
      "Murari Mandal",
      "Vinay Chamola",
      "Dhruv Kumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2601.21369",
    "title": "Rethinking Federated Graph Foundation Models: A Graph-Language Alignment-based Approach",
    "abstract": "           Recent studies of federated graph foundational models (FedGFMs) break the idealized and untenable assumption of having centralized data storage to train graph foundation models, and accommodate the reality of distributed, privacy-restricted data silos. Despite their simplicity and intuition, existing studies that project aligned generalizable knowledge onto a discrete token space via vector-quantized backbones suffer from irreversible knowledge loss during the quantization process. In this context, we argue that reconciling the semantic-structural orthogonality and integrity between pre-trained language models (PLMs) and graph neural networks (GNNs) is paramount for developing effective FedGFMs while simultaneously mitigating the severe data heterogeneity and communication constraints inherent in distributed, resource-limited environments. To address these issues, we propose FedGALA (Federated Graph And Language Alignment), a framework that resolves graph-based semantic-structural orthogonality and integrity in federated settings by employing unsupervised contrastive learning to align GNNs and frozen PLMs within a continuous embedding space, thereby capturing robust, transferable general knowledge. Subsequently, FedGALA leverages a communication-efficient prompt tuning mechanism to steer these pre-aligned encoders and frozen PLMs, facilitating effective adaptation to diverse downstream tasks while circumventing the prohibitive overhead of full-parameter fine-tuning. The comprehensive experiments validate that FedGALA outperforms all competitive baselines across multi-domain datasets on multiple tasks with up to 14.37% performance improvement.         ",
    "url": "https://arxiv.org/abs/2601.21369",
    "authors": [
      "Yinlin Zhu",
      "Di Wu",
      "Xianzhi Zhang",
      "Yuming Ai",
      "Xunkai Li",
      "Miao Hu",
      "Guocong Quan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21379",
    "title": "Predicting Developer Acceptance of AI-Generated Code Suggestions",
    "abstract": "           AI-assisted programming tools are widely adopted, yet their practical utility is often undermined by undesired suggestions that interrupt developer workflows and cause frustration. While existing research has explored developer-AI interactions when programming qualitatively, a significant gap remains in quantitative analysis of developers' acceptance of AI-generated code suggestions, partly because the necessary fine-grained interaction data is often proprietary. To bridge this gap, this paper conducts an empirical study using 66,329 industrial developer-AI interactions from a large technology company. We analyze features that are significantly different between accepted code suggestions and rejected ones. We find that accepted suggestions are characterized by significantly higher historical acceptance counts and ratios for both developers and projects, longer generation intervals, shorter preceding code context in the project, and older IDE versions. Based on these findings, we introduce CSAP (Code Suggestion Acceptance Prediction) to predict whether a developer will accept the code suggestion before it is displayed. Our evaluation of CSAP shows that it achieves the accuracy of 0.973 and 0.922 on imbalanced and balanced dataset respectively. Compared to a large language model baseline and an in-production industrial filter, CSAP relatively improves the accuracy by 12.6\\% and 69.5\\% on imbalanced dataset, and improves the accuracy by 87.0\\% and 140.1\\% on balanced dataset. Our results demonstrate that targeted personalization is a powerful approach for filtering out code suggestions with predicted rejection and reduce developer interruption. To the best of our knowledge, it is the first quantitative study of code suggestion acceptance on large-scale industrial data, and this work also sheds light on an important research direction of AI-assisted programming.         ",
    "url": "https://arxiv.org/abs/2601.21379",
    "authors": [
      "Jing Jiang",
      "Liehao Li",
      "Jinyun Hou",
      "Xin Tan",
      "Li Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2601.21380",
    "title": "RerouteGuard: Understanding and Mitigating Adversarial Risks for LLM Routing",
    "abstract": "           Recent advancements in multi-model AI systems have leveraged LLM routers to reduce computational cost while maintaining response quality by assigning queries to the most appropriate model. However, as classifiers, LLM routers are vulnerable to novel adversarial attacks in the form of LLM rerouting, where adversaries prepend specially crafted triggers to user queries to manipulate routing decisions. Such attacks can lead to increased computational cost, degraded response quality, and even bypass safety guardrails, yet their security implications remain largely underexplored. In this work, we bridge this gap by systematizing LLM rerouting threats based on the adversary's objectives (i.e., cost escalation, quality hijacking, and safety bypass) and knowledge. Based on the threat taxonomy, we conduct a measurement study of real-world LLM routing systems against existing LLM rerouting attacks. The results reveal that existing routing systems are vulnerable to rerouting attacks, especially in the cost escalation scenario. We then characterize existing rerouting attacks using interpretability techniques, revealing that they exploit router decision boundaries through confounder gadgets that prepend queries to force misrouting. To mitigate these risks, we introduce RerouteGuard, a flexible and scalable guardrail framework for LLM rerouting. RerouteGuard filters adversarial rerouting prompts via dynamic embedding-based detection and adaptive thresholding. Extensive evaluations in three attack settings and four benchmarks demonstrate that RerouteGuard achieves over 99% detection accuracy against state-of-the-art rerouting attacks, while maintaining negligible impact on legitimate queries. The experimental results indicate that RerouteGuard offers a principled and practical solution for safeguarding multi-model AI systems against adversarial rerouting.         ",
    "url": "https://arxiv.org/abs/2601.21380",
    "authors": [
      "Wenhui Zhang",
      "Huiyu Xu",
      "Zhibo Wang",
      "Zhichao Li",
      "Zeqing He",
      "Xuelin Wei",
      "Kui Ren"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2601.21381",
    "title": "DA-SPS: A Dual-stage Network based on Singular Spectrum Analysis, Patching-strategy and Spearman-correlation for Multivariate Time-series Prediction",
    "abstract": "           Multivariate time-series forecasting, as a typical problem in the field of time series prediction, has a wide range of applications in weather forecasting, traffic flow prediction, and other scenarios. However, existing works do not effectively consider the impact of extraneous variables on the prediction of the target variable. On the other hand, they fail to fully extract complex sequence information based on various time patterns of the sequences. To address these drawbacks, we propose a DA-SPS model, which adopts different modules for feature extraction based on the information characteristics of different variables. DA-SPS mainly consists of two stages: the target variable processing stage (TVPS) and the extraneous variables processing stage (EVPS). In TVPS, the model first uses Singular Spectrum Analysis (SSA) to process the target variable sequence and then uses Long Short-Term Memory (LSTM) and P-Conv-LSTM which deploys a patching strategy to extract features from trend and seasonality components, respectively. In EVPS, the model filters extraneous variables that have a strong correlation with the target variate by using Spearman correlation analysis and further analyses them using the L-Attention module which consists of LSTM and attention mechanism. Finally, the results obtained by TVPS and EVPS are combined through weighted summation and linear mapping to produce the final prediction. The results on four public datasets demonstrate that the DA-SPS model outperforms existing state-of-the-art methods. Additionally, its performance in real-world scenarios is further validated using a private dataset collected by ourselves, which contains the test items' information on laptop motherboards.         ",
    "url": "https://arxiv.org/abs/2601.21381",
    "authors": [
      "Tianhao Zhang",
      "Shusen Ma",
      "Yu Kang",
      "Yun-Bo Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21384",
    "title": "Sim-MSTNet: sim2real based Multi-task SpatioTemporal Network Traffic Forecasting",
    "abstract": "           Network traffic forecasting plays a crucial role in intelligent network operations, but existing techniques often perform poorly when faced with limited data. Additionally, multi-task learning methods struggle with task imbalance and negative transfer, especially when modeling various service types. To overcome these challenges, we propose Sim-MSTNet, a multi-task spatiotemporal network traffic forecasting model based on the sim2real approach. Our method leverages a simulator to generate synthetic data, effectively addressing the issue of poor generalization caused by data scarcity. By employing a domain randomization technique, we reduce the distributional gap between synthetic and real data through bi-level optimization of both sample weighting and model training. Moreover, Sim-MSTNet incorporates attention-based mechanisms to selectively share knowledge between tasks and applies dynamic loss weighting to balance task objectives. Extensive experiments on two open-source datasets show that Sim-MSTNet consistently outperforms state-of-the-art baselines, achieving enhanced accuracy and generalization.         ",
    "url": "https://arxiv.org/abs/2601.21384",
    "authors": [
      "Hui Ma",
      "Qingzhong Li",
      "Jin Wang",
      "Jie Wu",
      "Shaoyu Dou",
      "Li Feng",
      "Xinjun Pei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21403",
    "title": "DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis",
    "abstract": "           In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and \"zombie data\" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the \"divide-and-conquer\" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented \"zombie data\" for insightful, cross-modal analysis.         ",
    "url": "https://arxiv.org/abs/2601.21403",
    "authors": [
      "Ruyi Qi",
      "Zhou Liu",
      "Wentao Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2601.21421",
    "title": "From Implicit Ambiguity to Explicit Solidity: Diagnosing Interior Geometric Degradation in Neural Radiance Fields for Dense 3D Scene Understanding",
    "abstract": "           Neural Radiance Fields (NeRFs) have emerged as a powerful paradigm for multi-view reconstruction, complementing classical photogrammetric pipelines based on Structure-from-Motion (SfM) and Multi-View Stereo (MVS). However, their reliability for quantitative 3D analysis in dense, self-occluding scenes remains poorly understood. In this study, we identify a fundamental failure mode of implicit density fields under heavy occlusion, which we term Interior Geometric Degradation (IGD). We show that transmittance-based volumetric optimization satisfies photometric supervision by reconstructing hollow or fragmented structures rather than solid interiors, leading to systematic instance undercounting. Through controlled experiments on synthetic datasets with increasing occlusion, we demonstrate that state-of-the-art mask-supervised NeRFs saturate at approximately 89% instance recovery in dense scenes, despite improved surface coherence and mask quality. To overcome this limitation, we introduce an explicit geometric pipeline based on Sparse Voxel Rasterization (SVRaster), initialized from SfM feature geometry. By projecting 2D instance masks onto an explicit voxel grid and enforcing geometric separation via recursive splitting, our approach preserves physical solidity and achieves a 95.8% recovery rate in dense clusters. A sensitivity analysis using degraded segmentation masks further shows that explicit SfM-based geometry is substantially more robust to supervision failure, recovering 43% more instances than implicit baselines. These results demonstrate that explicit geometric priors are a prerequisite for reliable quantitative analysis in highly self-occluding 3D scenes.         ",
    "url": "https://arxiv.org/abs/2601.21421",
    "authors": [
      "Jiangsan Zhao",
      "Jakob Geipel",
      "Kryzysztof Kusnierek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21424",
    "title": "Lossy Common Information in a Learnable Gray-Wyner Network",
    "abstract": "           Many computer vision tasks share substantial overlapping information, yet conventional codecs tend to ignore this, leading to redundant and inefficient representations. The Gray-Wyner network, a classical concept from information theory, offers a principled framework for separating common and task-specific information. Inspired by this idea, we develop a learnable three-channel codec that disentangles shared information from task-specific details across multiple vision tasks. We characterize the limits of this approach through the notion of lossy common information, and propose an optimization objective that balances inherent tradeoffs in learning such representations. Through comparisons of three codec architectures on two-task scenarios spanning six vision benchmarks, we demonstrate that our approach substantially reduces redundancy and consistently outperforms independent coding. These results highlight the practical value of revisiting Gray-Wyner theory in modern machine learning contexts, bridging classic information theory with task-driven representation learning.         ",
    "url": "https://arxiv.org/abs/2601.21424",
    "authors": [
      "Anderson de Andrade",
      "Alon Harell",
      "Ivan V. Baji\u0107"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2601.21437",
    "title": "Accurate Network Traffic Matrix Prediction via LEAD: an LLM-Enhanced Adapter-Based Conditional Diffusion Model",
    "abstract": "           Driven by the evolution toward 6G and AI-native edge intelligence, network operations increasingly require predictive and risk-aware adaptation under stringent computation and latency constraints. Network Traffic Matrix (TM), which characterizes flow volumes between nodes, is a fundamental signal for proactive traffic engineering. However, accurate TM forecasting remains challenging due to the stochastic, non-linear, and bursty nature of network dynamics. Existing discriminative models often suffer from over-smoothing and provide limited uncertainty awareness, leading to poor fidelity under extreme bursts. To address these limitations, we propose LEAD, a Large Language Model (LLM)-Enhanced Adapter-based conditional Diffusion model. First, LEAD adopts a \"Traffic-to-Image\" paradigm to transform traffic matrices into RGB images, enabling global dependency modeling via vision backbones. Then, we design a \"Frozen LLM with Trainable Adapter\" model, which efficiently captures temporal semantics with limited computational cost. Moreover, we propose a Dual-Conditioning Strategy to precisely guide a diffusion model to generate complex, dynamic network traffic matrices. Experiments on the Abilene and GEANT datasets demonstrate that LEAD outperforms all baselines. On the Abilene dataset, LEAD attains a remarkable 45.2% reduction in RMSE against the best baseline, with the error margin rising only marginally from 0.1098 at one-step to 0.1134 at 20-step predictions. Meanwhile, on the GEANT dataset, LEAD achieves a 0.0258 RMSE at 20-step prediction horizon which is 27.3% lower than the best baseline.         ",
    "url": "https://arxiv.org/abs/2601.21437",
    "authors": [
      "Yu Sun",
      "Yaqiong Liu",
      "Nan Cheng",
      "Jiayuan Li",
      "Zihan Jia",
      "Xialin Du",
      "Mugen Peng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21446",
    "title": "Synthetic Pattern Generation and Detection of Financial Activities using Graph Autoencoders",
    "abstract": "           Illicit financial activities such as money laundering often manifest through recurrent topological patterns in transaction networks. Detecting these patterns automatically remains challenging due to the scarcity of labeled real-world data and strict privacy constraints. To address this, we investigate whether Graph Autoencoders (GAEs) can effectively learn and distinguish topological patterns that mimic money laundering operations when trained on synthetic data. The analysis consists of two phases: (i) data generation, where synthetic samples are created for seven well-known illicit activity patterns using parametrized generators that preserve structural consistency while introducing realistic variability; and (ii) model training and validation, where separate GAEs are trained on each pattern without explicit labels, relying solely on reconstruction error as an indicator of learned structure. We compare three GAE implementations based on three distinct convolutional layers: Graph Convolutional (GAE-GCN), GraphSAGE (GAE-SAGE), and Graph Attention Network (GAE-GAT). Experimental results show that GAE-GCN achieves the most consistent reconstruction performance across patterns, while GAE-SAGE and GAE-GAT exhibit competitive results only in few specific patterns. These findings suggest that graph-based representation learning on synthetic data provides a viable path toward developing AI-driven tools for detecting illicit behaviors, overcoming the limitations of financial datasets.         ",
    "url": "https://arxiv.org/abs/2601.21446",
    "authors": [
      "Francesco Zola",
      "Lucia Mu\u00f1oz",
      "Andrea Venturi",
      "Amaia Gil"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2601.21453",
    "title": "LION: A Clifford Neural Paradigm for Multimodal-Attributed Graph Learning",
    "abstract": "           Recently, the rapid advancement of multimodal domains has driven a data-centric paradigm shift in graph ML, transitioning from text-attributed to multimodal-attributed graphs. This advancement significantly enhances data representation and expands the scope of graph downstream tasks, such as modality-oriented tasks, thereby improving the practical utility of graph ML. Despite its promise, limitations exist in the current neural paradigms: (1) Neglect Context in Modality Alignment: Most existing methods adopt topology-constrained or modality-specific operators as tokenizers. These aligners inevitably neglect graph context and inhibit modality interaction, resulting in suboptimal alignment. (2) Lack of Adaptation in Modality Fusion: Most existing methods are simple adaptations for 2-modality graphs and fail to adequately exploit aligned tokens equipped with topology priors during fusion, leading to poor generalizability and performance degradation. To address the above issues, we propose LION (c\\underline{LI}ff\\underline{O}rd \\underline{N}eural paradigm) based on the Clifford algebra and decoupled graph neural paradigm (i.e., propagation-then-aggregation) to implement alignment-then-fusion in multimodal-attributed graphs. Specifically, we first construct a modality-aware geometric manifold grounded in Clifford algebra. This geometric-induced high-order graph propagation efficiently achieves modality interaction, facilitating modality alignment. Then, based on the geometric grade properties of aligned tokens, we propose adaptive holographic aggregation. This module integrates the energy and scale of geometric grades with learnable parameters to improve modality fusion. Extensive experiments on 9 datasets demonstrate that LION significantly outperforms SOTA baselines across 3 graph and 3 modality downstream tasks.         ",
    "url": "https://arxiv.org/abs/2601.21453",
    "authors": [
      "Xunkai Li",
      "Zhengyu Wu",
      "Zekai Chen",
      "Henan Sun",
      "Daohan Su",
      "Guang Zeng",
      "Hongchao Qin",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21458",
    "title": "Mining Forgery Traces from Reconstruction Error: A Weakly Supervised Framework for Multimodal Deepfake Temporal Localization",
    "abstract": "           Modern deepfakes have evolved into localized and intermittent manipulations that require fine-grained temporal localization. The prohibitive cost of frame-level annotation makes weakly supervised methods a practical necessity, which rely only on video-level labels. To this end, we propose Reconstruction-based Temporal Deepfake Localization (RT-DeepLoc), a weakly supervised temporal forgery localization framework that identifies forgeries via reconstruction errors. Our framework uses a Masked Autoencoder (MAE) trained exclusively on authentic data to learn its intrinsic spatiotemporal patterns; this allows the model to produce significant reconstruction discrepancies for forged segments, effectively providing the missing fine-grained cues for localization. To robustly leverage these indicators, we introduce a novel Asymmetric Intra-video Contrastive Loss (AICL). By focusing on the compactness of authentic features guided by these reconstruction cues, AICL establishes a stable decision boundary that enhances local discrimination while preserving generalization to unseen forgeries. Extensive experiments on large-scale datasets, including LAV-DF, demonstrate that RT-DeepLoc achieves state-of-the-art performance in weakly-supervised temporal forgery localization.         ",
    "url": "https://arxiv.org/abs/2601.21458",
    "authors": [
      "Midou Guo",
      "Qilin Yin",
      "Wei Lu",
      "Xiangyang Luo",
      "Rui Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21463",
    "title": "Unifying Speech Editing Detection and Content Localization via Prior-Enhanced Audio LLMs",
    "abstract": "           Speech editing achieves semantic inversion by performing fine-grained segment-level manipulation on original utterances, while preserving global perceptual naturalness. Existing detection studies mainly focus on manually edited speech with explicit splicing artifacts, and therefore struggle to cope with emerging end-to-end neural speech editing techniques that generate seamless acoustic transitions. To address this challenge, we first construct a large-scale bilingual dataset, AiEdit, which leverages large language models to drive precise semantic tampering logic and employs multiple advanced neural speech editing methods for data synthesis, thereby filling the gap of high-quality speech editing datasets. Building upon this foundation, we propose PELM (Prior-Enhanced Audio Large Language Model), the first large-model framework that unifies speech editing detection and content localization by formulating them as an audio question answering task. To mitigate the inherent forgery bias and semantic-priority bias observed in existing audio large models, PELM incorporates word-level probability priors to provide explicit acoustic cues, and further designs a centroid-aggregation-based acoustic consistency perception loss to explicitly enforce the modeling of subtle local distribution anomalies. Extensive experimental results demonstrate that PELM significantly outperforms state-of-the-art methods on both the HumanEdit and AiEdit datasets, achieving equal error rates (EER) of 0.57\\% and 9.28\\% (localization), respectively.         ",
    "url": "https://arxiv.org/abs/2601.21463",
    "authors": [
      "Jun Xue",
      "Yi Chai",
      "Yanzhen Ren",
      "Jinshen He",
      "Zhiqiang Tang",
      "Zhuolin Yi",
      "Yihuan Huang",
      "Yuankun Xie",
      "Yujie Chen"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21465",
    "title": "Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance",
    "abstract": "           Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \\textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.         ",
    "url": "https://arxiv.org/abs/2601.21465",
    "authors": [
      "M\u00e1rton Kardos"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2601.21469",
    "title": "Adaptive Confidence Gating in Multi-Agent Collaboration for Efficient and Optimized Code Generation",
    "abstract": "           While Large Language Models (LLMs) have catalyzed breakthroughs in automated code generation, Small Language Models (SLMs) often encounter reasoning bottlenecks and failure loops when addressing complex logical requirements. To overcome these challenges, we propose DebateCoder, a multi-agent collaborative framework designed to improve the reasoning ability of SLMs (e.g., Pangu-1B) in resource-constrained environments. DebateCoder uses a structured role-playing protocol with three agents: User Agent (A_UA), Technical Agent (A_TA), and Quality Assurance Agent (A_QA). It also includes an Adaptive Confidence Gating mechanism with a 95% threshold to balance accuracy and inference efficiency. In addition, we introduce a multi-turn deliberation module and a reviewer-guided analytical debugging loop for orthogonal pre-generation debate and post-generation refinement. Experiments on HumanEval and MBPP show that DebateCoder achieves 70.12% Pass@1 on HumanEval, outperforming MapCoder while reducing API overhead by about 35%. These results indicate that collaborative protocols can mitigate limitations of small-parameter models and provide a scalable, efficient approach to high-quality automated software engineering.         ",
    "url": "https://arxiv.org/abs/2601.21469",
    "authors": [
      "Haoji Zhang",
      "Yuzhe Li",
      "Zhenqiang Liu",
      "Chenyang Liu",
      "Shenyang Zhang",
      "Yi Zhou"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21488",
    "title": "HADUA: Hierarchical Attention and Dynamic Uniform Alignment for Robust Cross-Subject Emotion Recognition",
    "abstract": "           Robust cross-subject emotion recognition from multimodal physiological signals remains a challenging problem, primarily due to modality heterogeneity and inter-subject distribution shift. To tackle these challenges, we propose a novel adaptive learning framework named Hierarchical Attention and Dynamic Uniform Alignment (HADUA). Our approach unifies the learning of multimodal representations with domain adaptation. First, we design a hierarchical attention module that explicitly models intra-modal temporal dynamics and inter-modal semantic interactions (e.g., between electroencephalogram(EEG) and eye movement(EM)), yielding discriminative and semantically coherent fused features. Second, to overcome the noise inherent in pseudo-labels during adaptation, we introduce a confidence-aware Gaussian weighting scheme that smooths the supervision from target-domain samples by down-weighting uncertain instances. Third, a uniform alignment loss is employed to regularize the distribution of pseudo-labels across classes, thereby mitigating imbalance and stabilizing conditional distribution matching. Extensive experiments on multiple cross-subject emotion recognition benchmarks show that HADUA consistently surpasses existing state-of-the-art methods in both accuracy and robustness, validating its effectiveness in handling modality gaps, noisy pseudo-labels, and class imbalance. Taken together, these contributions offer a practical and generalizable solution for building robust cross-subject affective computing systems.         ",
    "url": "https://arxiv.org/abs/2601.21488",
    "authors": [
      "Jiahao Tang",
      "Youjun Li",
      "Yangxuan Zheng",
      "Xiangting Fan",
      "Siyuan Lu",
      "Nuo Zhang",
      "Zi-Gang Huang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2601.21504",
    "title": "Don't double it: Efficient Agent Prediction in Occlusions",
    "abstract": "           Occluded traffic agents pose a significant challenge for autonomous vehicles, as hidden pedestrians or vehicles can appear unexpectedly, yet this problem remains understudied. Existing learning-based methods, while capable of inferring the presence of hidden agents, often produce redundant occupancy predictions where a single agent is identified multiple times. This issue complicates downstream planning and increases computational load. To address this, we introduce MatchInformer, a novel transformer-based approach that builds on the state-of-the-art SceneInformer architecture. Our method improves upon prior work by integrating Hungarian Matching, a state-of-the-art object matching algorithm from object detection, into the training process to enforce a one-to-one correspondence between predictions and ground truth, thereby reducing redundancy. We further refine trajectory forecasts by decoupling an agent's heading from its motion, a strategy that improves the accuracy and interpretability of predicted paths. To better handle class imbalances, we propose using the Matthews Correlation Coefficient (MCC) to evaluate occupancy predictions. By considering all entries in the confusion matrix, MCC provides a robust measure even in sparse or imbalanced scenarios. Experiments on the Waymo Open Motion Dataset demonstrate that our approach improves reasoning about occluded regions and produces more accurate trajectory forecasts than prior methods.         ",
    "url": "https://arxiv.org/abs/2601.21504",
    "authors": [
      "Anna Rothenh\u00e4usler",
      "Markus Mazzola",
      "Andreas Look",
      "Raghu Rajan",
      "Joschka B\u00f6decker"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2601.21523",
    "title": "Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning",
    "abstract": "           To promote cooperation in Multi-Agent Reinforcement Learning, the reward signals of all agents can be aggregated together, forming global rewards that are commonly known as the fully cooperative setting. However, global rewards are usually noisy because they contain the contributions of all agents, which have to be resolved in the credit assignment process. On the other hand, using local reward benefits from faster learning due to the separation of agents' contributions, but can be suboptimal as agents myopically optimize their own reward while disregarding the global optimality. In this work, we propose a method that combines the merits of both approaches. By using a graph of interaction between agents, our method discerns the individual agent contribution in a more fine-grained manner than a global reward, while alleviating the cooperation problem with agents' local reward. We also introduce a practical approach for approximating such a graph. Our experiments demonstrate the flexibility of the approach, enabling improvements over the traditional local and global reward settings.         ",
    "url": "https://arxiv.org/abs/2601.21523",
    "authors": [
      "Bang Giang Le",
      "Viet Cuong Ta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21529",
    "title": "Fast and Geometrically Grounded Lorentz Neural Networks",
    "abstract": "           Hyperbolic space is quickly gaining traction as a promising geometry for hierarchical and robust representation learning. A core open challenge is the development of a mathematical formulation of hyperbolic neural networks that is both efficient and captures the key properties of hyperbolic space. The Lorentz model of hyperbolic space has been shown to enable both fast forward and backward propagation. However, we prove that, with the current formulation of Lorentz linear layers, the hyperbolic norms of the outputs scale logarithmically with the number of gradient descent steps, nullifying the key advantage of hyperbolic geometry. We propose a new Lorentz linear layer grounded in the well-known ``distance-to-hyperplane\" formulation. We prove that our formulation results in the usual linear scaling of output hyperbolic norms with respect to the number of gradient descent steps. Our new formulation, together with further algorithmic efficiencies through Lorentzian activation functions and a new caching strategy results in neural networks fully abiding by hyperbolic geometry while simultaneously bridging the computation gap to Euclidean neural networks. Code available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2601.21529",
    "authors": [
      "Robert van der Klis",
      "Ricardo Ch\u00e1vez Torres",
      "Max van Spengler",
      "Yuhui Ding",
      "Thomas Hofmann",
      "Pascal Mettes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21531",
    "title": "On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression",
    "abstract": "           Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.         ",
    "url": "https://arxiv.org/abs/2601.21531",
    "authors": [
      "Xinwei Zhang",
      "Hangcheng Liu",
      "Li Bai",
      "Hao Wang",
      "Qingqing Ye",
      "Tianwei Zhang",
      "Haibo Hu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21541",
    "title": "Vision KAN: Towards an Attention-Free Backbone for Vision with Kolmogorov-Arnold Networks",
    "abstract": "           Attention mechanisms have become a key module in modern vision backbones due to their ability to model long-range dependencies. However, their quadratic complexity in sequence length and the difficulty of interpreting attention weights limit both scalability and clarity. Recent attention-free architectures demonstrate that strong performance can be achieved without pairwise attention, motivating the search for alternatives. In this work, we introduce Vision KAN (ViK), an attention-free backbone inspired by the Kolmogorov-Arnold Networks. At its core lies MultiPatch-RBFKAN, a unified token mixer that combines (a) patch-wise nonlinear transform with Radial Basis Function-based KANs, (b) axis-wise separable mixing for efficient local propagation, and (c) low-rank global mapping for long-range interaction. Employing as a drop-in replacement for attention modules, this formulation tackles the prohibitive cost of full KANs on high-resolution features by adopting a patch-wise grouping strategy with lightweight operators to restore cross-patch dependencies. Experiments on ImageNet-1K show that ViK achieves competitive accuracy with linear complexity, demonstrating the potential of KAN-based token mixing as an efficient and theoretically grounded alternative to attention.         ",
    "url": "https://arxiv.org/abs/2601.21541",
    "authors": [
      "Zhuoqin Yang",
      "Jiansong Zhang",
      "Xiaoling Luo",
      "Xu Wu",
      "Zheng Lu",
      "Linlin Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21543",
    "title": "inversedMixup: Data Augmentation via Inverting Mixed Embeddings",
    "abstract": "           Mixup generates augmented samples by linearly interpolating inputs and labels with a controllable ratio. However, since it operates in the latent embedding level, the resulting samples are not human-interpretable. In contrast, LLM-based augmentation methods produce sentences via prompts at the token level, yielding readable outputs but offering limited control over the generation process. Inspired by recent advances in LLM inversion, which reconstructs natural language from embeddings and helps bridge the gap between latent embedding space and discrete token space, we propose inversedMixup, a unified framework that combines the controllability of Mixup with the interpretability of LLM-based generation. Specifically, inversedMixup adopts a three-stage training procedure to align the output embedding space of a task-specific model with the input embedding space of an LLM. Upon successful alignment, inversedMixup can reconstruct mixed embeddings with a controllable mixing ratio into human-interpretable augmented sentences, thereby improving the augmentation performance. Additionally, inversedMixup provides the first empirical evidence of the manifold intrusion phenomenon in text Mixup and introduces a simple yet effective strategy to mitigate it. Extensive experiments demonstrate the effectiveness and generalizability of our approach in both few-shot and fully supervised scenarios.         ",
    "url": "https://arxiv.org/abs/2601.21543",
    "authors": [
      "Fanshuang Kong",
      "Richong Zhang",
      "Qiyu Sun",
      "Zhijie Nie",
      "Ting Deng",
      "Chunming Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2601.21547",
    "title": "Multi-Modal Time Series Prediction via Mixture of Modulated Experts",
    "abstract": "           Real-world time series exhibit complex and evolving dynamics, making accurate forecasting extremely challenging. Recent multi-modal forecasting methods leverage textual information such as news reports to improve prediction, but most rely on token-level fusion that mixes temporal patches with language tokens in a shared embedding space. However, such fusion can be ill-suited when high-quality time-text pairs are scarce and when time series exhibit substantial variation in scale and characteristics, thus complicating cross-modal alignment. In parallel, Mixture-of-Experts (MoE) architectures have proven effective for both time series modeling and multi-modal learning, yet many existing MoE-based modality integration methods still depend on token-level fusion. To address this, we propose Expert Modulation, a new paradigm for multi-modal time series prediction that conditions both routing and expert computation on textual signals, enabling direct and efficient cross-modal control over expert behavior. Through comprehensive theoretical analysis and experiments, our proposed method demonstrates substantial improvements in multi-modal time series prediction. The current code is available at this https URL ",
    "url": "https://arxiv.org/abs/2601.21547",
    "authors": [
      "Lige Zhang",
      "Ali Maatouk",
      "Jialin Chen",
      "Leandros Tassiulas",
      "Rex Ying"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21560",
    "title": "HistoPrism: Unlocking Functional Pathway Analysis from Pan-Cancer Histology via Gene Expression Prediction",
    "abstract": "           Predicting spatial gene expression from H&E histology offers a scalable and clinically accessible alternative to sequencing, but realizing clinical impact requires models that generalize across cancer types and capture biologically coherent signals. Prior work is often limited to per-cancer settings and variance-based evaluation, leaving functional relevance underexplored. We introduce HistoPrism, an efficient transformer-based architecture for pan-cancer prediction of gene expression from histology. To evaluate biological meaning, we introduce a pathway-level benchmark, shifting assessment from isolated gene-level variance to coherent functional pathways. HistoPrism not only surpasses prior state-of-the-art models on highly variable genes , but also more importantly, achieves substantial gains on pathway-level prediction, demonstrating its ability to recover biologically coherent transcriptomic patterns. With strong pan-cancer generalization and improved efficiency, HistoPrism establishes a new standard for clinically relevant transcriptomic modeling from routinely available histology.         ",
    "url": "https://arxiv.org/abs/2601.21560",
    "authors": [
      "Susu Hu",
      "Qinghe Zeng",
      "Nithya Bhasker",
      "Jakob Nicolas Kather",
      "Stefanie Speidel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21562",
    "title": "Decentralized Analysis Approach for Oscillation Damping in Grid-Forming and Grid-Following Heterogeneous Power Systems",
    "abstract": "           This letter proposes a decentralized local gain condition (LGC) to guarantee oscillation damping in inverter-based resource (IBR)-dominated power systems. The LGC constrains the dynamic gain between each IBR and the network at its point of connection. By satisfying the LGC locally, the closed-loop poles are confined to a desired region, thereby yielding system-wide oscillation damping without requiring global information. Notably, the LGC is agnostic to different IBR dynamics, well-suited for systems with heterogeneous IBRs, and flexible to various damping requirements. Moreover, a low-complexity algorithm is proposed to parameterize LGC, providing scalable and damping-constrained parameter tuning guidance for IBRs.         ",
    "url": "https://arxiv.org/abs/2601.21562",
    "authors": [
      "Xiang Zhu",
      "Xiuqiang He",
      "Hongyang Qing",
      "Hua Geng"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2601.21564",
    "title": "Representation Unlearning: Forgetting through Information Compression",
    "abstract": "           Machine unlearning seeks to remove the influence of specific training data from a model, a need driven by privacy regulations and robustness concerns. Existing approaches typically modify model parameters, but such updates can be unstable, computationally costly, and limited by local approximations. We introduce Representation Unlearning, a framework that performs unlearning directly in the model's representation space. Instead of modifying model parameters, we learn a transformation over representations that imposes an information bottleneck: maximizing mutual information with retained data while suppressing information about data to be forgotten. We derive variational surrogates that make this objective tractable and show how they can be instantiated in two practical regimes: when both retain and forget data are available, and in a zero-shot setting where only forget data can be accessed. Experiments across several benchmarks demonstrate that Representation Unlearning achieves more reliable forgetting, better utility retention, and greater computational efficiency than parameter-centric baselines.         ",
    "url": "https://arxiv.org/abs/2601.21564",
    "authors": [
      "Antonio Almud\u00e9var",
      "Alfonso Ortega"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21567",
    "title": "FlexCausal: Flexible Causal Disentanglement via Structural Flow Priors and Manifold-Aware Interventions",
    "abstract": "           Causal Disentangled Representation Learning(CDRL) aims to learn and disentangle low dimensional representations and their underlying causal structure from observations. However, existing disentanglement methods rely on a standard mean-field approximation with a diagonal posterior covariance, which decorrelates all latent dimensions. Additionally, these methods often assume isotropic Gaussian priors for exogenous noise, failing to capture the complex, non-Gaussian statistical properties prevalent in real-world causal factors. Therefore, we propose FlexCausal, a novel CDRL framework based on a block-diagonal covariance VAE. FlexCausal utilizes a Factorized Flow-based Prior to realistically model the complex densities of exogenous noise, effectively decoupling the learning of causal mechanisms from distributional statistics. By integrating supervised alignment objectives with counterfactual consistency constraints, our framework ensures a precise structural correspondence between the learned latent subspaces and the ground-truth causal relations. Finally, we introduce a manifold-aware relative intervention strategy to ensure high-fidelity generation. Experimental results on both synthetic and real-world datasets demonstrate that FlexCausal significantly outperforms other methods.         ",
    "url": "https://arxiv.org/abs/2601.21567",
    "authors": [
      "Yutao Jin",
      "Yuang Tao",
      "Junyong Zhai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21572",
    "title": "Signal-Adaptive Trust Regions for Gradient-Free Optimization of Recurrent Spiking Neural Networks",
    "abstract": "           Recurrent spiking neural networks (RSNNs) are a promising substrate for energy-efficient control policies, but training them for high-dimensional, long-horizon reinforcement learning remains challenging. Population-based, gradient-free optimization circumvents backpropagation through non-differentiable spike dynamics by estimating gradients. However, with finite populations, high variance of these estimates can induce harmful and overly aggressive update steps. Inspired by trust-region methods in reinforcement learning that constrain policy updates in distribution space, we propose \\textbf{Signal-Adaptive Trust Regions (SATR)}, a distributional update rule that constrains relative change by bounding KL divergence normalized by an estimated signal energy. SATR automatically expands the trust region under strong signals and contracts it when updates are noise-dominated. We instantiate SATR for Bernoulli connectivity distributions, which have shown strong empirical performance for RSNN optimization. Across a suite of high-dimensional continuous-control benchmarks, SATR improves stability under limited populations and reaches competitive returns against strong baselines including PPO-LSTM. In addition, to make SATR practical at scale, we introduce a bitset implementation for binary spiking and binary weights, substantially reducing wall-clock training time and enabling fast RSNN policy search.         ",
    "url": "https://arxiv.org/abs/2601.21572",
    "authors": [
      "Jinhao Li",
      "Yuhao Sun",
      "Zhiyuan Ma",
      "Hao He",
      "Xinche Zhang",
      "Xing Chen",
      "Jin Li",
      "Sen Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21581",
    "title": "Evaluating Prediction Uncertainty Estimates from BatchEnsemble",
    "abstract": "           Deep learning models struggle with uncertainty estimation. Many approaches are either computationally infeasible or underestimate uncertainty. We investigate \\textit{BatchEnsemble} as a general and scalable method for uncertainty estimation across both tabular and time series tasks. To extend BatchEnsemble to sequential modeling, we introduce GRUBE, a novel BatchEnsemble GRU cell. We compare the BatchEnsemble to Monte Carlo dropout and deep ensemble models. Our results show that BatchEnsemble matches the uncertainty estimation performance of deep ensembles, and clearly outperforms Monte Carlo dropout. GRUBE achieves similar or better performance in both prediction and uncertainty estimation. These findings show that BatchEnsemble and GRUBE achieve similar performance with fewer parameters and reduced training and inference time compared to traditional ensembles.         ",
    "url": "https://arxiv.org/abs/2601.21581",
    "authors": [
      "Morten Bl\u00f8rstad",
      "Herman Jangsett Mostein",
      "Nello Blaser",
      "Pekka Parviainen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21586",
    "title": "ICL-EVADER: Zero-Query Black-Box Evasion Attacks on In-Context Learning and Their Defenses",
    "abstract": "           In-context learning (ICL) has become a powerful, data-efficient paradigm for text classification using large language models. However, its robustness against realistic adversarial threats remains largely unexplored. We introduce ICL-Evader, a novel black-box evasion attack framework that operates under a highly practical zero-query threat model, requiring no access to model parameters, gradients, or query-based feedback during attack generation. We design three novel attacks, Fake Claim, Template, and Needle-in-a-Haystack, that exploit inherent limitations of LLMs in processing in-context prompts. Evaluated across sentiment analysis, toxicity, and illicit promotion tasks, our attacks significantly degrade classifier performance (e.g., achieving up to 95.3% attack success rate), drastically outperforming traditional NLP attacks which prove ineffective under the same constraints. To counter these vulnerabilities, we systematically investigate defense strategies and identify a joint defense recipe that effectively mitigates all attacks with minimal utility loss (<5% accuracy degradation). Finally, we translate our defensive insights into an automated tool that proactively fortifies standard ICL prompts against adversarial evasion. This work provides a comprehensive security assessment of ICL, revealing critical vulnerabilities and offering practical solutions for building more robust systems. Our source code and evaluation datasets are publicly available at: this https URL .         ",
    "url": "https://arxiv.org/abs/2601.21586",
    "authors": [
      "Ningyuan He",
      "Ronghong Huang",
      "Qianqian Tang",
      "Hongyu Wang",
      "Xianghang Mi",
      "Shanqing Guo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2601.21589",
    "title": "Heterogeneity-Aware Knowledge Sharing for Graph Federated Learning",
    "abstract": "           Graph Federated Learning (GFL) enables distributed graph representation learning while protecting the privacy of graph data. However, GFL suffers from heterogeneity arising from diverse node features and structural topologies across multiple clients. To address both types of heterogeneity, we propose a novel graph Federated learning method via Semantic and Structural Alignment (FedSSA), which shares the knowledge of both node features and structural topologies. For node feature heterogeneity, we propose a novel variational model to infer class-wise node distributions, so that we can cluster clients based on inferred distributions and construct cluster-level representative distributions. We then minimize the divergence between local and cluster-level distributions to facilitate semantic knowledge sharing. For structural heterogeneity, we employ spectral Graph Neural Networks (GNNs) and propose a spectral energy measure to characterize structural information, so that we can cluster clients based on spectral energy and build cluster-level spectral GNNs. We then align the spectral characteristics of local spectral GNNs with those of cluster-level spectral GNNs to enable structural knowledge sharing. Experiments on six homophilic and five heterophilic graph datasets under both non-overlapping and overlapping partitioning settings demonstrate that FedSSA consistently outperforms eleven state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2601.21589",
    "authors": [
      "Wentao Yu",
      "Sheng Wan",
      "Shuo Chen",
      "Bo Han",
      "Chen Gong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21592",
    "title": "Unifying Heterogeneous Degradations: Uncertainty-Aware Diffusion Bridge Model for All-in-One Image Restoration",
    "abstract": "           All-in-One Image Restoration (AiOIR) faces the fundamental challenge in reconciling conflicting optimization objectives across heterogeneous degradations. Existing methods are often constrained by coarse-grained control mechanisms or fixed mapping schedules, yielding suboptimal adaptation. To address this, we propose an Uncertainty-Aware Diffusion Bridge Model (UDBM), which innovatively reformulates AiOIR as a stochastic transport problem steered by pixel-wise uncertainty. By introducing a relaxed diffusion bridge formulation which replaces the strict terminal constraint with a relaxed constraint, we model the uncertainty of degradations while theoretically resolving the drift singularity inherent in standard diffusion bridges. Furthermore, we devise a dual modulation strategy: the noise schedule aligns diverse degradations into a shared high-entropy latent space, while the path schedule adaptively regulates the transport trajectory motivated by the viscous dynamics of entropy regularization. By effectively rectifying the transport geometry and dynamics, UDBM achieves state-of-the-art performance across diverse restoration tasks within a single inference step.         ",
    "url": "https://arxiv.org/abs/2601.21592",
    "authors": [
      "Luwei Tu",
      "Jiawei Wu",
      "Xing Luo",
      "Zhi Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21615",
    "title": "Beyond Parameter Finetuning: Test-Time Representation Refinement for Node Classification",
    "abstract": "           Graph Neural Networks frequently exhibit significant performance degradation in the out-of-distribution test scenario. While test-time training (TTT) offers a promising solution, existing Parameter Finetuning (PaFT) paradigm suffer from catastrophic forgetting, hindering their real-world applicability. We propose TTReFT, a novel Test-Time Representation FineTuning framework that transitions the adaptation target from model parameters to latent representations. Specifically, TTReFT achieves this through three key innovations: (1) uncertainty-guided node selection for specific interventions, (2) low-rank representation interventions that preserve pre-trained knowledge, and (3) an intervention-aware masked autoencoder that dynamically adjust masking strategy to accommodate the node selection scheme. Theoretically, we establish guarantees for TTReFT in OOD settings. Empirically, extensive experiments across five benchmark datasets demonstrate that TTReFT achieves consistent and superior performance. Our work establishes representation finetuning as a new paradigm for graph TTT, offering both theoretical grounding and immediate practical utility for real-world deployment.         ",
    "url": "https://arxiv.org/abs/2601.21615",
    "authors": [
      "Jiaxin Zhang",
      "Yiqi Wang",
      "Siwei Wang",
      "Xihong Yang",
      "Yu Shi",
      "Xinwang Liu",
      "En Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21624",
    "title": "Training Memory in Deep Neural Networks: Mechanisms, Evidence, and Measurement Gaps",
    "abstract": "           Modern deep-learning training is not memoryless. Updates depend on optimizer moments and averaging, data-order policies (random reshuffling vs with-replacement, staged augmentations and replay), the nonconvex path, and auxiliary state (teacher EMA/SWA, contrastive queues, BatchNorm statistics). This survey organizes mechanisms by source, lifetime, and visibility. It introduces seed-paired, function-space causal estimands; portable perturbation primitives (carry/reset of momentum/Adam/EMA/BN, order-window swaps, queue/teacher tweaks); and a reporting checklist with audit artifacts (order hashes, buffer/BN checksums, RNG contracts). The conclusion is a protocol for portable, causal, uncertainty-aware measurement that attributes how much training history matters across models, data, and regimes.         ",
    "url": "https://arxiv.org/abs/2601.21624",
    "authors": [
      "Vasileios Sevetlidis",
      "George Pavlidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21628",
    "title": "Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise",
    "abstract": "           Diffusion models have achieved remarkable progress in image generation, but their increasing deployment raises serious concerns about privacy. In particular, fine-tuned models are highly vulnerable, as they are often fine-tuned on small and private datasets. Membership inference attacks (MIAs) are used to assess privacy risks by determining whether a specific sample was part of a model's training data. Existing MIAs against diffusion models either assume obtaining the intermediate results or require auxiliary datasets for training the shadow model. In this work, we utilized a critical yet overlooked vulnerability: the widely used noise schedules fail to fully eliminate semantic information in the images, resulting in residual semantic signals even at the maximum noise step. We empirically demonstrate that the fine-tuned diffusion model captures hidden correlations between the residual semantics in initial noise and the original images. Building on this insight, we propose a simple yet effective membership inference attack, which injects semantic information into the initial noise and infers membership by analyzing the model's generation result. Extensive experiments demonstrate that the semantic initial noise can strongly reveal membership information, highlighting the vulnerability of diffusion models to MIAs.         ",
    "url": "https://arxiv.org/abs/2601.21628",
    "authors": [
      "Puwei Lian",
      "Yujun Cai",
      "Songze Li",
      "Bingkun Bao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21636",
    "title": "Sampling-Free Privacy Accounting for Matrix Mechanisms under Random Allocation",
    "abstract": "           We study privacy amplification for differentially private model training with matrix factorization under random allocation (also known as the balls-in-bins model). Recent work by Choquette-Choo et al. (2025) proposes a sampling-based Monte Carlo approach to compute amplification parameters in this setting. However, their guarantees either only hold with some high probability or require random abstention by the mechanism. Furthermore, the required number of samples for ensuring $(\\epsilon,\\delta)$-DP is inversely proportional to $\\delta$. In contrast, we develop sampling-free bounds based on R\u00e9nyi divergence and conditional composition. The former is facilitated by a dynamic programming formulation to efficiently compute the bounds. The latter complements it by offering stronger privacy guarantees for small $\\epsilon$, where R\u00e9nyi divergence bounds inherently lead to an over-approximation. Our framework applies to arbitrary banded and non-banded matrices. Through numerical comparisons, we demonstrate the efficacy of our approach across a broad range of matrix mechanisms used in research and practice.         ",
    "url": "https://arxiv.org/abs/2601.21636",
    "authors": [
      "Jan Schuchardt",
      "Nikita Kalinin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2601.21645",
    "title": "Identifiable Equivariant Networks are Layerwise Equivariant",
    "abstract": "           We investigate the relation between end-to-end equivariance and layerwise equivariance in deep neural networks. We prove the following: For a network whose end-to-end function is equivariant with respect to group actions on the input and output spaces, there is a parameter choice yielding the same end-to-end function such that its layers are equivariant with respect to some group actions on the latent spaces. Our result assumes that the parameters of the model are identifiable in an appropriate sense. This identifiability property has been established in the literature for a large class of networks, to which our results apply immediately, while it is conjectural for others. The theory we develop is grounded in an abstract formalism, and is therefore architecture-agnostic. Overall, our results provide a mathematical explanation for the emergence of equivariant structures in the weights of neural networks during training -- a phenomenon that is consistently observed in practice.         ",
    "url": "https://arxiv.org/abs/2601.21645",
    "authors": [
      "Vahid Shahverdi",
      "Giovanni Luca Marchetti",
      "Georg B\u00f6kman",
      "Kathl\u00e9n Kohn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Category Theory (math.CT)",
      "Representation Theory (math.RT)"
    ]
  },
  {
    "id": "arXiv:2601.21648",
    "title": "CAF-Mamba: Mamba-Based Cross-Modal Adaptive Attention Fusion for Multimodal Depression Detection",
    "abstract": "           Depression is a prevalent mental health disorder that severely impairs daily functioning and quality of life. While recent deep learning approaches for depression detection have shown promise, most rely on limited feature types, overlook explicit cross-modal interactions, and employ simple concatenation or static weighting for fusion. To overcome these limitations, we propose CAF-Mamba, a novel Mamba-based cross-modal adaptive attention fusion framework. CAF-Mamba not only captures cross-modal interactions explicitly and implicitly, but also dynamically adjusts modality contributions through a modality-wise attention mechanism, enabling more effective multimodal fusion. Experiments on two in-the-wild benchmark datasets, LMVD and D-Vlog, demonstrate that CAF-Mamba consistently outperforms existing methods and achieves state-of-the-art performance.         ",
    "url": "https://arxiv.org/abs/2601.21648",
    "authors": [
      "Bowen Zhou",
      "Marc-Andr\u00e9 Fiedler",
      "Ayoub Al-Hamadi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2601.21653",
    "title": "Gauge-invariant representation holonomy",
    "abstract": "           Deep networks learn internal representations whose geometry--how features bend, rotate, and evolve--affects both generalization and robustness. Existing similarity measures such as CKA or SVCCA capture pointwise overlap between activation sets, but miss how representations change along input paths. Two models may appear nearly identical under these metrics yet respond very differently to perturbations or adversarial stress. We introduce representation holonomy, a gauge-invariant statistic that measures this path dependence. Conceptually, holonomy quantifies the \"twist\" accumulated when features are parallel-transported around a small loop in input space: flat representations yield zero holonomy, while nonzero values reveal hidden curvature. Our estimator fixes gauge through global whitening, aligns neighborhoods using shared subspaces and rotation-only Procrustes, and embeds the result back to the full feature space. We prove invariance to orthogonal (and affine, post-whitening) transformations, establish a linear null for affine layers, and show that holonomy vanishes at small radii. Empirically, holonomy increases with loop radius, separates models that appear similar under CKA, and correlates with adversarial and corruption robustness. It also tracks training dynamics as features form and stabilize. Together, these results position representation holonomy as a practical and scalable diagnostic for probing the geometric structure of learned representations beyond pointwise similarity.         ",
    "url": "https://arxiv.org/abs/2601.21653",
    "authors": [
      "Vasileios Sevetlidis",
      "George Pavlidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21656",
    "title": "TabClustPFN: A Prior-Fitted Network for Tabular Data Clustering",
    "abstract": "           Clustering tabular data is a fundamental yet challenging problem due to heterogeneous feature types, diverse data-generating mechanisms, and the absence of transferable inductive biases across datasets. Prior-fitted networks (PFNs) have recently demonstrated strong generalization in supervised tabular learning by amortizing Bayesian inference under a broad synthetic prior. Extending this paradigm to clustering is nontrivial: clustering is unsupervised, admits a combinatorial and permutation-invariant output space, and requires inferring the number of clusters. We introduce TabClustPFN, a prior-fitted network for tabular data clustering that performs amortized Bayesian inference over both cluster assignments and cluster cardinality. Pretrained on synthetic datasets drawn from a flexible clustering prior, TabClustPFN clusters unseen datasets in a single forward pass, without dataset-specific retraining or hyperparameter tuning. The model naturally handles heterogeneous numerical and categorical features and adapts to a wide range of clustering structures. Experiments on synthetic data and curated real-world tabular benchmarks show that TabClustPFN outperforms classical, deep, and amortized clustering baselines, while exhibiting strong robustness in out-of-the-box exploratory settings. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.21656",
    "authors": [
      "Tianqi Zhao",
      "Guanyang Wang",
      "Yan Shuo Tan",
      "Qiong Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21675",
    "title": "Rethinking Fusion: Disentangled Learning of Shared and Modality-Specific Information for Stance Detection",
    "abstract": "           Multi-modal stance detection (MSD) aims to determine an author's stance toward a given target using both textual and visual content. While recent methods leverage multi-modal fusion and prompt-based learning, most fail to distinguish between modality-specific signals and cross-modal evidence, leading to suboptimal performance. We propose DiME (Disentangled Multi-modal Experts), a novel architecture that explicitly separates stance information into textual-dominant, visual-dominant, and cross-modal shared components. DiME first uses a target-aware Chain-of-Thought prompt to generate reasoning-guided textual input. Then, dual encoders extract modality features, which are processed by three expert modules with specialized loss functions: contrastive learning for modality-specific experts and cosine alignment for shared representation learning. A gating network adaptively fuses expert outputs for final prediction. Experiments on four benchmark datasets show that DiME consistently outperforms strong unimodal and multi-modal baselines under both in-target and zero-shot settings.         ",
    "url": "https://arxiv.org/abs/2601.21675",
    "authors": [
      "Zhiyu Xie",
      "Fuqiang Niu",
      "Genan Dai",
      "Qianlong Wang",
      "Li Dong",
      "Bowen Zhang",
      "Hu Huang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2601.21681",
    "title": "LLM4Fluid: Large Language Models as Generalizable Neural Solvers for Fluid Dynamics",
    "abstract": "           Deep learning has emerged as a promising paradigm for spatio-temporal modeling of fluid dynamics. However, existing approaches often suffer from limited generalization to unseen flow conditions and typically require retraining when applied to new scenarios. In this paper, we present LLM4Fluid, a spatio-temporal prediction framework that leverages Large Language Models (LLMs) as generalizable neural solvers for fluid dynamics. The framework first compresses high-dimensional flow fields into a compact latent space via reduced-order modeling enhanced with a physics-informed disentanglement mechanism, effectively mitigating spatial feature entanglement while preserving essential flow structures. A pretrained LLM then serves as a temporal processor, autoregressively predicting the dynamics of physical sequences with time series prompts. To bridge the modality gap between prompts and physical sequences, which can otherwise degrade prediction accuracy, we propose a dedicated modality alignment strategy that resolves representational mismatch and stabilizes long-term prediction. Extensive experiments across diverse flow scenarios demonstrate that LLM4Fluid functions as a robust and generalizable neural solver without retraining, achieving state-of-the-art accuracy while exhibiting powerful zero-shot and in-context learning capabilities. Code and datasets are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.21681",
    "authors": [
      "Qisong Xiao",
      "Xinhai Chen",
      "Qinglin Wang",
      "Xiaowei Guo",
      "Binglin Wang",
      "Weifeng Chen",
      "Zhichao Wang",
      "Yunfei Liu",
      "Rui Xia",
      "Hang Zou",
      "Gencheng Liu",
      "Shuai Li",
      "Jie Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2601.21683",
    "title": "Can Local Learning Match Self-Supervised Backpropagation?",
    "abstract": "           While end-to-end self-supervised learning with backpropagation (global BP-SSL) has become central for training modern AI systems, theories of local self-supervised learning (local-SSL) have struggled to build functional representations in deep neural networks. To establish a link between global and local rules, we first develop a theory for deep linear networks: we identify conditions for local-SSL algorithms (like Forward-forward or CLAPP) to implement exactly the same weight update as a global BP-SSL. Starting from the theoretical insights, we then develop novel variants of local-SSL algorithms to approximate global BP-SSL in deep non-linear convolutional neural networks. Variants that improve the similarity between gradient updates of local-SSL with those of global BP-SSL also show better performance on image datasets (CIFAR-10, STL-10, and Tiny ImageNet). The best local-SSL rule with the CLAPP loss function matches the performance of a comparable global BP-SSL with InfoNCE or CPC-like loss functions, and improves upon state-of-the-art for local SSL on these benchmarks.         ",
    "url": "https://arxiv.org/abs/2601.21683",
    "authors": [
      "Wu S. Zihan",
      "Ariane Delrocq",
      "Wulfram Gerstner",
      "Guillaume Bellec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21690",
    "title": "Understanding Model Merging: A Unified Generalization Framework for Heterogeneous Experts",
    "abstract": "           Model merging efficiently aggregates capabilities from multiple fine-tuned models into a single one, operating purely in parameter space without original data or expensive re-computation. Despite empirical successes, a unified theory for its effectiveness under heterogeneous finetuning hyperparameters (e.g., varying learning rates, batch sizes) remains missing. Moreover, the lack of hyperparameter transparency in open-source fine-tuned models makes it difficult to predict merged-model performance, leaving practitioners without guidance on how to fine-tune merge-friendly experts. To address those two challenges, we employ $L_2$-Stability theory under heterogeneous hyperparameter environments to analyze the generalization of the merged model $\\boldsymbol{x}_{avg}$. This pioneering analysis yields two key contributions: (i) \\textit{A unified theoretical framework} is provided to explain existing merging algorithms, revealing how they optimize specific terms in our bound, thus offering a strong theoretical foundation for empirical observations. (ii) \\textit{Actionable recommendations} are proposed for practitioners to strategically fine-tune expert models, enabling the construction of merge-friendly models within the pretraining-to-finetuning pipeline. Extensive experiments on the ResNet/Vit family across 20/8 visual classification tasks, involving thousands of finetuning models, robustly confirm the impact of different hyperparameters on the generalization of $\\boldsymbol{x}_{avg}$ predicted by our theoretical results.         ",
    "url": "https://arxiv.org/abs/2601.21690",
    "authors": [
      "Qinglun Li",
      "Anke Tang",
      "Miao Zhang",
      "Mengzhu Wang",
      "Quanjun Yin",
      "Li Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21692",
    "title": "TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning",
    "abstract": "           Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.         ",
    "url": "https://arxiv.org/abs/2601.21692",
    "authors": [
      "Mingzu Liu",
      "Hao Fang",
      "Runmin Cong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21722",
    "title": "Enhancing Language Models for Robust Greenwashing Detection",
    "abstract": "           Sustainability reports are critical for ESG assessment, yet greenwashing and vague claims often undermine their reliability. Existing NLP models lack robustness to these practices, typically relying on surface-level patterns that generalize poorly. We propose a parameter-efficient framework that structures LLM latent spaces by combining contrastive learning with an ordinal ranking objective to capture graded distinctions between concrete actions and ambiguous claims. Our approach incorporates gated feature modulation to filter disclosure noise and utilizes MetaGradNorm to stabilize multi-objective optimization. Experiments in cross-category settings demonstrate superior robustness over standard baselines while revealing a trade-off between representational rigidity and generalization.         ",
    "url": "https://arxiv.org/abs/2601.21722",
    "authors": [
      "Neil Heinrich Braun",
      "Keane Ong",
      "Rui Mao",
      "Erik Cambria",
      "Gianmarco Mengaldo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21726",
    "title": "DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting",
    "abstract": "           Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from \"what\" to learn to \"how much\" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.21726",
    "authors": [
      "Siru Zhong",
      "Yiqiu Liu",
      "Zhiqing Cui",
      "Zezhi Shao",
      "Fei Wang",
      "Qingsong Wen",
      "Yuxuan Liang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21731",
    "title": "Amortized Spectral Kernel Discovery via Prior-Data Fitted Network",
    "abstract": "           Prior-Data Fitted Networks (PFNs) enable efficient amortized inference but lack transparent access to their learned priors and kernels. This opacity hinders their use in downstream tasks, such as surrogate-based optimization, that require explicit covariance models. We introduce an interpretability-driven framework for amortized spectral discovery from pre-trained PFNs with decoupled attention. We perform a mechanistic analysis on a trained PFN that identifies attention latent output as the key intermediary, linking observed function data to spectral structure. Building on this insight, we propose decoder architectures that map PFN latents to explicit spectral density estimates and corresponding stationary kernels via Bochner's theorem. We study this pipeline in both single-realization and multi-realization regimes, contextualizing theoretical limits on spectral identifiability and proving consistency when multiple function samples are available. Empirically, the proposed decoders recover complex multi-peak spectral mixtures and produce explicit kernels that support Gaussian process regression with accuracy comparable to PFNs and optimization-based baselines, while requiring only a single forward pass. This yields orders-of-magnitude reductions in inference time compared to optimization-based baselines.         ",
    "url": "https://arxiv.org/abs/2601.21731",
    "authors": [
      "Kaustubh Sharma",
      "Srijan Tiwari",
      "Ojasva Nema",
      "Parikshit Pareek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21733",
    "title": "CE-GOCD: Central Entity-Guided Graph Optimization for Community Detection to Augment LLM Scientific Question Answering",
    "abstract": "           Large Language Models (LLMs) are increasingly used for question answering over scientific research papers. Existing retrieval augmentation methods often rely on isolated text chunks or concepts, but overlook deeper semantic connections between papers. This impairs the LLM's comprehension of scientific literature, hindering the comprehensiveness and specificity of its responses. To address this, we propose Central Entity-Guided Graph Optimization for Community Detection (CE-GOCD), a method that augments LLMs' scientific question answering by explicitly modeling and leveraging semantic substructures within academic knowledge graphs. Our approach operates by: (1) leveraging paper titles as central entities for targeted subgraph retrieval, (2) enhancing implicit semantic discovery via subgraph pruning and completion, and (3) applying community detection to distill coherent paper groups with shared themes. We evaluated the proposed method on three NLP literature-based question-answering datasets, and the results demonstrate its superiority over other retrieval-augmented baseline approaches, confirming the effectiveness of our framework.         ",
    "url": "https://arxiv.org/abs/2601.21733",
    "authors": [
      "Jiayin Lan",
      "Jiaqi Li",
      "Baoxin Wang",
      "Ming Liu",
      "Dayong Wu",
      "Shijin Wang",
      "Bing Qin",
      "Guoping Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2601.21772",
    "title": "Flocking behavior for dynamic and complex swarm structures",
    "abstract": "           Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a theoretical framework for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.         ",
    "url": "https://arxiv.org/abs/2601.21772",
    "authors": [
      "Carmen D. R. Pita-Romero",
      "Pedro Arias-Perez",
      "Miguel Fernandez-Cortizas",
      "Rafael Perez-Segui",
      "Pascual Campoy"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2601.21792",
    "title": "NetMamba+: A Framework of Pre-trained Models for Efficient and Accurate Network Traffic Classification",
    "abstract": "           With the rapid growth of encrypted network traffic, effective traffic classification has become essential for network security and quality of service management. Current machine learning and deep learning approaches for traffic classification face three critical challenges: computational inefficiency of Transformer architectures, inadequate traffic representations with loss of crucial byte-level features while retaining detrimental biases, and poor handling of long-tail distributions in real-world data. We propose NetMamba+, a framework that addresses these challenges through three key innovations: (1) an efficient architecture considering Mamba and Flash Attention mechanisms, (2) a multimodal traffic representation scheme that preserves essential traffic information while eliminating biases, and (3) a label distribution-aware fine-tuning strategy. Evaluation experiments on massive datasets encompassing four main classification tasks showcase NetMamba+'s superior classification performance compared to state-of-the-art baselines, with improvements of up to 6.44\\% in F1 score. Moreover, NetMamba+ demonstrates excellent efficiency, achieving 1.7x higher inference throughput than the best baseline while maintaining comparably low memory usage. Furthermore, NetMamba+ exhibits superior few-shot learning abilities, achieving better classification performance with fewer labeled data. Additionally, we implement an online traffic classification system that demonstrates robust real-world performance with a throughput of 261.87 Mb/s. As the first framework to adapt Mamba architecture for network traffic classification, NetMamba+ opens new possibilities for efficient and accurate traffic analysis in complex network environments.         ",
    "url": "https://arxiv.org/abs/2601.21792",
    "authors": [
      "Tongze Wang",
      "Xiaohui Xie",
      "Wenduo Wang",
      "Chuyi Wang",
      "Jinzhou Liu",
      "Boyan Huang",
      "Yannan Hu",
      "Youjian Zhao",
      "Yong Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21796",
    "title": "KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection",
    "abstract": "           Internet memes have become pervasive carriers of digital culture on social platforms. However, their heavy reliance on metaphors and sociocultural context also makes them subtle vehicles for harmful content, posing significant challenges for automated content moderation. Existing approaches primarily focus on intra-modal and inter-modal signal analysis, while the understanding of implicit toxicity often depends on background knowledge that is not explicitly present in the meme itself. To address this challenge, we propose KID, a Knowledge-Injected Dual-Head Learning framework for knowledge-grounded harmful meme detection. KID adopts a label-constrained distillation paradigm to decompose complex meme understanding into structured reasoning chains that explicitly link visual evidence, background knowledge, and classification labels. These chains guide the learning process by grounding external knowledge in meme-specific contexts. In addition, KID employs a dual-head architecture that jointly optimizes semantic generation and classification objectives, enabling aligned linguistic reasoning while maintaining stable decision boundaries. Extensive experiments on five multilingual datasets spanning English, Chinese, and low-resource Bengali demonstrate that KID achieves SOTA performance on both binary and multi-label harmful meme detection tasks, improving over previous best methods by 2.1%--19.7% across primary evaluation metrics. Ablation studies further confirm the effectiveness of knowledge injection and dual-head joint learning, highlighting their complementary contributions to robust and generalizable meme understanding. The code and data are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.21796",
    "authors": [
      "Yaocong Li",
      "Leihan Zhang",
      "Le Zhang",
      "Qiang Yan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21797",
    "title": "Enhancing Conversational Agents via Task-Oriented Adversarial Memory Adaptation",
    "abstract": "           Conversational agents struggle to handle long conversations due to context window limitations. Therefore, memory systems are developed to leverage essential historical information. Existing memory systems typically follow a pipeline of offline memory construction and update, and online retrieval. Despite the flexible online phase, the offline phase remains fixed and task-independent. In this phase, memory construction operates under a predefined workflow and fails to emphasize task relevant information. Meanwhile, memory updates are guided by generic metrics rather than task specific supervision. This leads to a misalignment between offline memory preparation and task requirements, which undermines downstream task performance. To this end, we propose an Adversarial Memory Adaptation mechanism (AMA) that aligns memory construction and update with task objectives by simulating task execution. Specifically, first, a challenger agent generates question answer pairs based on the original dialogues. The constructed memory is then used to answer these questions, simulating downstream inference. Subsequently, an evaluator agent assesses the responses and performs error analysis. Finally, an adapter agent analyzes the error cases and performs dual level updates on both the construction strategy and the content. Through this process, the memory system receives task aware supervision signals in advance during the offline phase, enhancing its adaptability to downstream tasks. AMA can be integrated into various existing memory systems, and extensive experiments on long dialogue benchmark LoCoMo demonstrate its effectiveness.         ",
    "url": "https://arxiv.org/abs/2601.21797",
    "authors": [
      "Yimin Deng",
      "Yuqing Fu",
      "Derong Xu",
      "Yejing Wang",
      "Wei Ni",
      "Jingtong Gao",
      "Xiaopeng Li",
      "Chengxu Liu",
      "Xiao Han",
      "Guoshuai Zhao",
      "Xiangyu Zhao",
      "Li Zhu",
      "Xueming Qian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2601.21835",
    "title": "Scalable Linearized Laplace Approximation via Surrogate Neural Kernel",
    "abstract": "           We introduce a scalable method to approximate the kernel of the Linearized Laplace Approximation (LLA). For this, we use a surrogate deep neural network (DNN) that learns a compact feature representation whose inner product replicates the Neural Tangent Kernel (NTK). This avoids the need to compute large Jacobians. Training relies solely on efficient Jacobian-vector products, allowing to compute predictive uncertainty on large-scale pre-trained DNNs. Experimental results show similar or improved uncertainty estimation and calibration compared to existing LLA approximations. Notwithstanding, biasing the learned kernel significantly enhances out-of-distribution detection. This remarks the benefits of the proposed method for finding better kernels than the NTK in the context of LLA to compute prediction uncertainty given a pre-trained DNN.         ",
    "url": "https://arxiv.org/abs/2601.21835",
    "authors": [
      "Luis A. Ortega",
      "Sim\u00f3n Rodr\u00edguez-Santana",
      "Daniel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21846",
    "title": "Optimal Energy-Aware Service Management in Future Networks with a Gamified Incentives Mechanism",
    "abstract": "           As energy demands surge across ICT infrastructures, service providers must engage users in sustainable practices while maintaining the Quality of Experience (QoE) at acceptable levels. In this paper, we introduce such an approach, leveraging gamified incentives and a model for user's acceptance on incentives, thus encouraging energy-efficient behaviors such as adaptive bitrate streaming. Each user is characterized by an environmental sensitivity factor and a private incentive threshold, shaping probabilistic responses to energy-saving offers. A serious-game mechanism based on positive behavioral reinforcement and rewards of the users, due to their inclusion in top-K and bottom-M rankings, fosters peer comparison and competition, thus transforming passive acceptance into active engagement. Moreover, within a Stackelberg game formulation, the video streaming service provider--acting as the strategic leader--optimizes both incentive levels and game parameters to achieve network-wide energy and traffic reductions, while adhering to budgetary constraints. This structured approach empowers providers with proactive, application-level control over energy consumption, offering them measurable benefits such as reduced high-bitrate traffic and increased participation in energy-saving behaviors, while also considering user satisfaction. The results of our simulations show that indeed gamification boosts significantly user participation and energy savings provided that the incentive and game parameters are chosen optimally.         ",
    "url": "https://arxiv.org/abs/2601.21846",
    "authors": [
      "Konstantinos Varsos",
      "Adamantia Stamou",
      "George D. Stamoulis",
      "Vasillios A. Siris"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2601.21859",
    "title": "Adaptive Privacy of Sequential Data Releases Under Collusion",
    "abstract": "           The fundamental trade-off between privacy and utility remains an active area of research. Our contribution is motivated by two observations. First, privacy mechanisms developed for one-time data release cannot straightforwardly be extended to sequential releases. Second, practical databases are likely to be useful to multiple distinct parties. Furthermore, we can not rule out the possibility of data sharing between parties. With utility in mind, we formulate a privacy-utility trade-off problem to adaptively tackle sequential data requests made by different, potentially colluding entities. We consider both expected distortion and mutual information as measures to quantify utility, and use mutual information to measure privacy. We assume an attack model whereby illicit data sharing, which we call collusion, can occur between data receivers. We develop an adaptive algorithm for data releases that makes use of a modified Blahut-Arimoto algorithm. We show that the resulting data releases are optimal when expected distortion quantifies utility, and locally optimal when mutual information quantifies utility. Finally, we discuss how our findings may extend to applications in machine learning.         ",
    "url": "https://arxiv.org/abs/2601.21859",
    "authors": [
      "Sophie Taylor",
      "Praneeth Kumar Vippathalla",
      "Justin Coon"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2601.21864",
    "title": "KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement",
    "abstract": "           Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \\textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.21864",
    "authors": [
      "Jinhao Pan",
      "Chahat Raj",
      "Anjishnu Mukherjee",
      "Sina Mansouri",
      "Bowen Wei",
      "Shloka Yada",
      "Ziwei Zhu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21882",
    "title": "How Expressive Are Graph Neural Networks in the Presence of Node Identifiers?",
    "abstract": "           Graph neural networks (GNNs) are a widely used class of machine learning models for graph-structured data, based on local aggregation over neighbors. GNNs have close connections to logic. In particular, their expressive power is linked to that of modal logics and bounded-variable logics with counting. In many practical scenarios, graphs processed by GNNs have node features that act as unique identifiers. In this work, we study how such identifiers affect the expressive power of GNNs. We initiate a study of the key-invariant expressive power of GNNs, inspired by the notion of order-invariant definability in finite model theory: which node queries that depend only on the underlying graph structure can GNNs express on graphs with unique node identifiers? We provide answers for various classes of GNNs with local max- or sum-aggregation.         ",
    "url": "https://arxiv.org/abs/2601.21882",
    "authors": [
      "Arie Soeteman",
      "Michael Benedikt",
      "Martin Grohe",
      "Balder ten Cate"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21893",
    "title": "WADBERT: Dual-channel Web Attack Detection Based on BERT Models",
    "abstract": "           Web attack detection is the first line of defense for securing web applications, designed to preemptively identify malicious activities. Deep learning-based approaches are increasingly popular for their advantages: automatically learning complex patterns and extracting semantic features from HTTP requests to achieve superior detection performance. However, existing methods are less effective in embedding irregular HTTP requests, even failing to model unordered parameters and achieve attack traceability. In this paper, we propose an effective web attack detection model, named WADBERT. It achieves high detection accuracy while enabling the precise identification of malicious parameters. To this end, we first employ Hybrid Granularity Embedding (HGE) to generate fine-grained embeddings for URL and payload parameters. Then, URLBERT and SecBERT are respectively utilized to extract their semantic features. Further, parameter-level features (extracted by SecBERT) are fused through a multi-head attention mechanism, resulting in a comprehensive payload feature. Finally, by feeding the concatenated URL and payload features into a linear classifier, a final detection result is obtained. The experimental results on CSIC2010 and SR-BH2020 datasets validate the efficacy of WADBERT, which respectively achieves F1-scores of 99.63% and 99.50%, and significantly outperforms state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2601.21893",
    "authors": [
      "Kangqiang Luo",
      "Yi Xie",
      "Shiqian Zhao",
      "Jing Pan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2601.21894",
    "title": "Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning",
    "abstract": "           Large Language Models (LLMs) increasingly exhibit strong reasoning abilities, often attributed to their capacity to generate chain-of-thought-style intermediate reasoning. Recent work suggests that exposure to code can further enhance these skills, but existing studies largely treat code as a generic training signal, leaving open the question of which properties of code actually contribute to improved reasoning. To address this gap, we study the structural complexity of code, which captures control flow and compositional structure that may shape how models internalise multi-step reasoning during fine-tuning. We examine two complementary settings: solution-driven complexity, where complexity varies across multiple solutions to the same problem, and problem-driven complexity, where complexity reflects variation in the underlying tasks. Using cyclomatic complexity and logical lines of code to construct controlled fine-tuning datasets, we evaluate a range of open-weight LLMs on diverse reasoning benchmarks. Our findings show that although code can improve reasoning, structural properties strongly determine its usefulness. In 83% of experiments, restricting fine-tuning data to a specific structural complexity range outperforms training on structurally diverse code, pointing to a data-centric path for improving reasoning beyond scaling.         ",
    "url": "https://arxiv.org/abs/2601.21894",
    "authors": [
      "Lukas Twist",
      "Shu Yang",
      "Hanqi Yan",
      "Jingzhi Gong",
      "Di Wang",
      "Helen Yannakoudakis",
      "Jie M. Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21900",
    "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
    "abstract": "           Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the \"locality hypothesis\", suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses.         ",
    "url": "https://arxiv.org/abs/2601.21900",
    "authors": [
      "Chuancheng Shi",
      "Shangze Li",
      "Wenjun Lu",
      "Wenhua Wu",
      "Cong Wang",
      "Zifeng Cheng",
      "Fei Shen",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2601.21935",
    "title": "Belief Propagation Converges to Gaussian Distributions in Sparsely-Connected Factor Graphs",
    "abstract": "           Belief Propagation (BP) is a powerful algorithm for distributed inference in probabilistic graphical models, however it quickly becomes infeasible for practical compute and memory budgets. Many efficient, non-parametric forms of BP have been developed, but the most popular is Gaussian Belief Propagation (GBP), a variant that assumes all distributions are locally Gaussian. GBP is widely used due to its efficiency and empirically strong performance in applications like computer vision or sensor networks - even when modelling non-Gaussian problems. In this paper, we seek to provide a theoretical guarantee for when Gaussian approximations are valid in highly non-Gaussian, sparsely-connected factor graphs performing BP (common in spatial AI). We leverage the Central Limit Theorem (CLT) to prove mathematically that variables' beliefs under BP converge to a Gaussian distribution in complex, loopy factor graphs obeying our 4 key assumptions. We then confirm experimentally that variable beliefs become increasingly Gaussian after just a few BP iterations in a stereo depth estimation task.         ",
    "url": "https://arxiv.org/abs/2601.21935",
    "authors": [
      "Tom Yates",
      "Yuzhou Cheng",
      "Ignacio Alzugaray",
      "Danyal Akarca",
      "Pedro A.M. Mediano",
      "Andrew J. Davison"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2601.21938",
    "title": "BookNet: Book Image Rectification via Cross-Page Attention Network",
    "abstract": "           Book image rectification presents unique challenges in document image processing due to complex geometric distortions from binding constraints, where left and right pages exhibit distinctly asymmetric curvature patterns. However, existing single-page document image rectification methods fail to capture the coupled geometric relationships between adjacent pages in books. In this work, we introduce BookNet, the first end-to-end deep learning framework specifically designed for dual-page book image rectification. BookNet adopts a dual-branch architecture with cross-page attention mechanisms, enabling it to estimate warping flows for both individual pages and the complete book spread, explicitly modeling how left and right pages influence each other. Moreover, to address the absence of specialized datasets, we present Book3D, a large-scale synthetic dataset for training, and Book100, a comprehensive real-world benchmark for evaluation. Extensive experiments demonstrate that BookNet outperforms existing state-of-the-art methods on book image rectification. Code and dataset will be made publicly available.         ",
    "url": "https://arxiv.org/abs/2601.21938",
    "authors": [
      "Shaokai Liu",
      "Hao Feng",
      "Bozhi Luan",
      "Min Hou",
      "Jiajun Deng",
      "Wengang Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21941",
    "title": "Robust Multimodal Representation Learning in Healthcare",
    "abstract": "           Medical multimodal representation learning aims to integrate heterogeneous data into unified patient representations to support clinical outcome prediction. However, real-world medical datasets commonly contain systematic biases from multiple sources, which poses significant challenges for medical multimodal representation learning. Existing approaches typically focus on effective multimodal fusion, neglecting inherent biased features that affect the generalization ability. To address these challenges, we propose a Dual-Stream Feature Decorrelation Framework that identifies and handles the biases through structural causal analysis introduced by latent confounders. Our method employs a causal-biased decorrelation framework with dual-stream neural networks to disentangle causal features from spurious correlations, utilizing generalized cross-entropy loss and mutual information minimization for effective decorrelation. The framework is model-agnostic and can be integrated into existing medical multimodal learning methods. Comprehensive experiments on MIMIC-IV, eICU, and ADNI datasets demonstrate consistent performance improvements.         ",
    "url": "https://arxiv.org/abs/2601.21941",
    "authors": [
      "Xiaoguang Zhu",
      "Linxiao Gong",
      "Lianlong Sun",
      "Yang Liu",
      "Haoyu Wang",
      "Jing Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.21945",
    "title": "Dependence of Equilibrium Propagation Training Success on Network Architecture",
    "abstract": "           The rapid rise of artificial intelligence has led to an unsustainable growth in energy consumption. This has motivated progress in neuromorphic computing and physics-based training of learning machines as alternatives to digital neural networks. Many theoretical studies focus on simple architectures like all-to-all or densely connected layered networks. However, these may be challenging to realize experimentally, e.g. due to connectivity constraints. In this work, we investigate the performance of the widespread physics-based training method of equilibrium propagation for more realistic architectural choices, specifically, locally connected lattices. We train an XY model and explore the influence of architecture on various benchmark tasks, tracking the evolution of spatially distributed responses and couplings during training. Our results show that sparse networks with only local connections can achieve performance comparable to dense networks. Our findings provide guidelines for further scaling up architectures based on equilibrium propagation in realistic settings.         ",
    "url": "https://arxiv.org/abs/2601.21945",
    "authors": [
      "Qingshan Wang",
      "Clara C. Wanjura",
      "Florian Marquardt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Emerging Technologies (cs.ET)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2601.21948",
    "title": "Deep Models, Shallow Alignment: Uncovering the Granularity Mismatch in Neural Decoding",
    "abstract": "           Neural visual decoding is a central problem in brain computer interface research, aiming to reconstruct human visual perception and to elucidate the structure of neural representations. However, existing approaches overlook a fundamental granularity mismatch between human and machine vision, where deep vision models emphasize semantic invariance by suppressing local texture information, whereas neural signals preserve an intricate mixture of low-level visual attributes and high-level semantic content. To address this mismatch, we propose Shallow Alignment, a novel contrastive learning strategy that aligns neural signals with intermediate representations of visual encoders rather than their final outputs, thereby striking a better balance between low-level texture details and high-level semantic features. Extensive experiments across multiple benchmarks demonstrate that Shallow Alignment significantly outperforms standard final-layer alignment, with performance gains ranging from 22% to 58% across diverse vision backbones. Notably, our approach effectively unlocks the scaling law in neural visual decoding, enabling decoding performance to scale predictably with the capacity of pre-trained vision backbones. We further conduct systematic empirical analyses to shed light on the mechanisms underlying the observed performance gains.         ",
    "url": "https://arxiv.org/abs/2601.21948",
    "authors": [
      "Yang Du",
      "Siyuan Dai",
      "Yonghao Song",
      "Paul M. Thompson",
      "Haoteng Tang",
      "Liang Zhan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21957",
    "title": "PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing",
    "abstract": "           We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: this https URL ",
    "url": "https://arxiv.org/abs/2601.21957",
    "authors": [
      "Cheng Cui",
      "Ting Sun",
      "Suyin Liang",
      "Tingquan Gao",
      "Zelun Zhang",
      "Jiaxuan Liu",
      "Xueqing Wang",
      "Changda Zhou",
      "Hongen Liu",
      "Manhui Lin",
      "Yue Zhang",
      "Yubo Zhang",
      "Yi Liu",
      "Dianhai Yu",
      "Yanjun Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.21978",
    "title": "Bridging Graph Structure and Knowledge-Guided Editing for Interpretable Temporal Knowledge Graph Reasoning",
    "abstract": "           Temporal knowledge graph reasoning (TKGR) aims to predict future events by inferring missing entities with dynamic knowledge structures. Existing LLM-based reasoning methods prioritize contextual over structural relations, struggling to extract relevant subgraphs from dynamic graphs. This limits structural information understanding, leading to unstructured, hallucination-prone inferences especially with temporal inconsistencies. To address this problem, we propose IGETR (Integration of Graph and Editing-enhanced Temporal Reasoning), a hybrid reasoning framework that combines the structured temporal modeling capabilities of Graph Neural Networks (GNNs) with the contextual understanding of LLMs. IGETR operates through a three-stage pipeline. The first stage aims to ground the reasoning process in the actual data by identifying structurally and temporally coherent candidate paths through a temporal GNN, ensuring that inference starts from reliable graph-based evidence. The second stage introduces LLM-guided path editing to address logical and semantic inconsistencies, leveraging external knowledge to refine and enhance the initial paths. The final stage focuses on integrating the refined reasoning paths to produce predictions that are both accurate and interpretable. Experiments on standard TKG benchmarks show that IGETR achieves state-of-the-art performance, outperforming strong baselines with relative improvements of up to 5.6% on Hits@1 and 8.1% on Hits@3 on the challenging ICEWS datasets. Additionally, we execute ablation studies and additional analyses confirm the effectiveness of each component.         ",
    "url": "https://arxiv.org/abs/2601.21978",
    "authors": [
      "Shiqi Fan",
      "Quanming Yao",
      "Hongyi Nie",
      "Wentao Ma",
      "Zhen Wang",
      "Wen Hua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21979",
    "title": "Investigation into using stochastic embedding representations for evaluating the trustworthiness of the Fr\u00e9chet Inception Distance",
    "abstract": "           Feature embeddings acquired from pretrained models are widely used in medical applications of deep learning to assess the characteristics of datasets; e.g. to determine the quality of synthetic, generated medical images. The Fr\u00e9chet Inception Distance (FID) is one popular synthetic image quality metric that relies on the assumption that the characteristic features of the data can be detected and encoded by an InceptionV3 model pretrained on ImageNet1K (natural images). While it is widely known that this makes it less effective for applications involving medical images, the extent to which the metric fails to capture meaningful differences in image characteristics is not obviously known. Here, we use Monte Carlo dropout to compute the predictive variance in the FID as well as a supplemental estimate of the predictive variance in the feature embedding model's latent representations. We show that the magnitudes of the predictive variances considered exhibit varying degrees of correlation with the extent to which test inputs (ImageNet1K validation set augmented at various strengths, and other external datasets) are out-of-distribution relative to its training data, providing some insight into the effectiveness of their use as indicators of the trustworthiness of the FID.         ",
    "url": "https://arxiv.org/abs/2601.21979",
    "authors": [
      "Ciaran Bench",
      "Vivek Desai",
      "Carlijn Roozemond",
      "Ruben van Engen",
      "Spencer A. Thomas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21983",
    "title": "Investigating Batch Inference in a Sequential Monte Carlo Framework for Neural Networks",
    "abstract": "           Bayesian inference allows us to define a posterior distribution over the weights of a generic neural network (NN). Exact posteriors are usually intractable, in which case approximations can be employed. One such approximation - variational inference - is computationally efficient when using mini-batch stochastic gradient descent as subsets of the data are used for likelihood and gradient evaluations, though the approach relies on the selection of a variational distribution which sufficiently matches the form of the posterior. Particle-based methods such as Markov chain Monte Carlo and Sequential Monte Carlo (SMC) do not assume a parametric family for the posterior by typically require higher computational cost. These sampling methods typically use the full-batch of data for likelihood and gradient evaluations, which contributes to this computational expense. We explore several methods of gradually introducing more mini-batches of data (data annealing) into likelihood and gradient evaluations of an SMC sampler. We find that we can achieve up to $6\\times$ faster training with minimal loss in accuracy on benchmark image classification problems using NNs.         ",
    "url": "https://arxiv.org/abs/2601.21983",
    "authors": [
      "Andrew Millard",
      "Joshua Murphy",
      "Peter Green",
      "Simon Maskell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21984",
    "title": "PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters",
    "abstract": "           Discovering superior circuit topologies requires navigating an exponentially large design space-a challenge traditionally reserved for human experts. Existing AI methods either select from predefined templates or generate novel topologies at a limited scale without rigorous verification, leaving large-scale performance-driven discovery underexplored. We present PowerGenie, a framework for automated discovery of higher-performance reconfigurable power converters at scale. PowerGenie introduces: (1) an automated analytical framework that determines converter functionality and theoretical performance limits without component sizing or SPICE simulation, and (2) an evolutionary finetuning method that co-evolves a generative model with its training distribution through fitness selection and uniqueness verification. Unlike existing methods that suffer from mode collapse and overfitting, our approach achieves higher syntax validity, function validity, novelty rate, and figure-of-merit (FoM). PowerGenie discovers a novel 8-mode reconfigurable converter with 23% higher FoM than the best training topology. SPICE simulations confirm average absolute efficiency gains of 10% across 8 modes and up to 17% at a single mode. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.21984",
    "authors": [
      "Jian Gao",
      "Yiwei Zou",
      "Abhishek Pradhan",
      "Wenhao Huang",
      "Yumin Su",
      "Kaiyuan Yang",
      "Xuan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2601.21989",
    "title": "Adaptively Robust Resettable Streaming",
    "abstract": "           We study algorithms in the resettable streaming model, where the value of each key can either be increased or reset to zero. The model is suitable for applications such as active resource monitoring with support for deletions and machine unlearning. We show that all existing sketches for this model are vulnerable to adaptive adversarial attacks that apply even when the sketch size is polynomial in the length of the stream. To overcome these vulnerabilities, we present the first adaptively robust sketches for resettable streams that maintain polylogarithmic space complexity in the stream length. Our framework supports (sub) linear statistics including $L_p$ moments for $p\\in[0,1]$ (in particular, Cardinality and Sum) and Bernstein statistics. We bypass strong impossibility results known for linear and composable sketches by designing dedicated streaming sketches robustified via Differential Privacy. Unlike standard robustification techniques, which provide limited benefits in this setting and still require polynomial space in the stream length, we leverage the Binary Tree Mechanism for continual observation to protect the sketch's internal randomness. This enables accurate prefix-max error guarantees with polylogarithmic space.         ",
    "url": "https://arxiv.org/abs/2601.21989",
    "authors": [
      "Edith Cohen",
      "Elena Gribelyuk",
      "Jelani Nelson",
      "Uri Stemmer"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2601.21998",
    "title": "Causal World Modeling for Robot Control",
    "abstract": "           This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.         ",
    "url": "https://arxiv.org/abs/2601.21998",
    "authors": [
      "Lin Li",
      "Qihang Zhang",
      "Yiming Luo",
      "Shuai Yang",
      "Ruilin Wang",
      "Fei Han",
      "Mingrui Yu",
      "Zelin Gao",
      "Nan Xue",
      "Xing Zhu",
      "Yujun Shen",
      "Yinghao Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2601.22001",
    "title": "Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference",
    "abstract": "           AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.         ",
    "url": "https://arxiv.org/abs/2601.22001",
    "authors": [
      "Yiren Zhao",
      "Junyi Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2601.22024",
    "title": "SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks",
    "abstract": "           The operation of future 6th-generation (6G) mobile networks will increasingly rely on the ability of deep reinforcement learning (DRL) to optimize network decisions in real-time. DRL yields demonstrated efficacy in various resource allocation problems, such as joint decisions on user scheduling and antenna allocation or simultaneous control of computing resources and modulation. However, trained DRL agents are closed-boxes and inherently difficult to explain, which hinders their adoption in production settings. In this paper, we make a step towards removing this critical barrier by presenting SymbXRL, a novel technique for explainable reinforcement learning (XRL) that synthesizes human-interpretable explanations for DRL agents. SymbXRL leverages symbolic AI to produce explanations where key concepts and their relationships are described via intuitive symbols and rules; coupling such a representation with logical reasoning exposes the decision process of DRL agents and offers more comprehensible descriptions of their behaviors compared to existing approaches. We validate SymbXRL in practical network management use cases supported by DRL, proving that it not only improves the semantics of the explanations but also paves the way for explicit agent control: for instance, it enables intent-based programmatic action steering that improves by 12% the median cumulative reward over a pure DRL solution.         ",
    "url": "https://arxiv.org/abs/2601.22024",
    "authors": [
      "Abhishek Duttagupta",
      "MohammadErfan Jabbari",
      "Claudio Fiandrino",
      "Marco Fiore",
      "Joerg Widmer"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.22028",
    "title": "From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning",
    "abstract": "           Most LLM unlearning methods aim to approximate retrain-from-scratch behaviors with minimal distribution shift, often via alignment-style objectives defined in the prediction space. While effective at reducing forgotten content generation, such approaches may act as suppression: forgotten concepts can persist in representations and remain entangled with retained knowledge. We introduce CLReg, a contrastive representation regularizer that identifies forget features while pushing them away from retain features, explicitly reducing forget-retain interference with minimal shifts on retain features. We provide first theoretical insights that relate representation shaping to entanglement reduction. Across unlearning benchmarks and LLMs of different sizes, CLReg decreases forget-retain representation entanglement that facilitates mainstream unlearning methods without positing extra privacy risks, inspiring future work that reshapes the representation space to remove forget concepts.         ",
    "url": "https://arxiv.org/abs/2601.22028",
    "authors": [
      "Haoran Tang",
      "Rajiv Khanna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.22031",
    "title": "Causal Autoregressive Diffusion Language Model",
    "abstract": "           In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the optimization instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 $\\times$ compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a robust paradigm for next-generation efficient LLMs.         ",
    "url": "https://arxiv.org/abs/2601.22031",
    "authors": [
      "Junhao Ruan",
      "Bei Li",
      "Yongjing Yin",
      "Pengcheng Huang",
      "Xin Chen",
      "Jingang Wang",
      "Xunliang Cai",
      "Tong Xiao",
      "JingBo Zhu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2601.22036",
    "title": "Cross-Fusion Distance: A Novel Metric for Measuring Fusion and Separability Between Data Groups in Representation Space",
    "abstract": "           Quantifying degrees of fusion and separability between data groups in representation space is a fundamental problem in representation learning, particularly under domain shift. A meaningful metric should capture fusion-altering factors like geometric displacement between representation groups, whose variations change the extent of fusion, while remaining invariant to fusion-preserving factors such as global scaling and sampling-induced layout changes, whose variations do not. Existing distributional distance metrics conflate these factors, leading to measures that are not informative of the true extent of fusion between data groups. We introduce Cross-Fusion Distance (CFD), a principled measure that isolates fusion-altering geometry while remaining robust to fusion-preserving variations, with linear computational complexity. We characterize the invariance and sensitivity properties of CFD theoretically and validate them in controlled synthetic experiments. For practical utility on real-world datasets with domain shift, CFD aligns more closely with downstream generalization degradation than commonly used alternatives. Overall, CFD provides a theoretically grounded and interpretable distance measure for representation learning.         ",
    "url": "https://arxiv.org/abs/2601.22036",
    "authors": [
      "Xiaolong Zhang",
      "Jianwei Zhang",
      "Xubo Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.22040",
    "title": "A Separable Architecture for Continuous Token Representation in Language Models",
    "abstract": "           Transformer scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), embedding matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an architecture with a continuous embedding generator to replace the discrete lookup tables of canonical models. Evaluating on the Pile dataset under isoparametric settings, Leviathan consistently outperforms a standard, LLaMA-style architecture. By means of an empirical power-law fit, Leviathan exhibits a markedly superior effective parameter capacity. Across the regime studied, Leviathan behaves as a dense model with $1.47$ to $2.11 \\times$ more parameters.         ",
    "url": "https://arxiv.org/abs/2601.22040",
    "authors": [
      "Reza T. Batley",
      "Sourav Saha"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.22044",
    "title": "SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control",
    "abstract": "           Deep reinforcement learning (DRL) promises adaptive control for future mobile networks but conventional agents remain reactive: they act on past and current measurements and cannot leverage short-term forecasts of exogenous KPIs such as bandwidth. Augmenting agents with predictions can overcome this temporal myopia, yet uptake in networking is scarce because forecast-aware agents act as closed-boxes; operators cannot tell whether predictions guide decisions or justify the added complexity. We propose SIA, the first interpreter that exposes in real time how forecast-augmented DRL agents operate. SIA fuses Symbolic AI abstractions with per-KPI Knowledge Graphs to produce explanations, and includes a new Influence Score metric. SIA achieves sub-millisecond speed, over 200x faster than existing XAI methods. We evaluate SIA on three diverse networking use cases, uncovering hidden issues, including temporal misalignment in forecast integration and reward-design biases that trigger counter-productive policies. These insights enable targeted fixes: a redesigned agent achieves a 9% higher average bitrate in video streaming, and SIA's online Action-Refinement module improves RAN-slicing reward by 25% without retraining. By making anticipatory DRL transparent and tunable, SIA lowers the barrier to proactive control in next-generation mobile networks.         ",
    "url": "https://arxiv.org/abs/2601.22044",
    "authors": [
      "MohammadErfan Jabbari",
      "Abhishek Duttagupta",
      "Claudio Fiandrino",
      "Leonardo Bonati",
      "Salvatore D'Oro",
      "Michele Polese",
      "Marco Fiore",
      "Tommaso Melodia"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.22045",
    "title": "Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion",
    "abstract": "           Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.         ",
    "url": "https://arxiv.org/abs/2601.22045",
    "authors": [
      "Da Li",
      "Chen Yao",
      "Tong Mao",
      "Jiacheng Bao",
      "Houjun Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.22052",
    "title": "Learning to Dial-a-Ride: A Deep Graph Reinforcement Learning Approach to the Electric Dial-a-Ride Problem",
    "abstract": "           Urban mobility systems are transitioning toward electric, on-demand services, creating operational challenges for fleet management under energy and service-quality constraints. The Electric Dial-a-Ride Problem (E-DARP) extends the classical dial-a-ride problem by incorporating limited battery capacity and nonlinear charging dynamics, increasing computational complexity and limiting the scalability of exact methods for real-time use. This paper proposes a deep reinforcement learning approach based on a graph neural network encoder and an attention-driven route construction policy. By operating directly on edge attributes such as travel time and energy consumption, the method captures non-Euclidean, asymmetric, and energy-dependent routing costs in real road networks. The learned policy jointly optimizes routing, charging, and service quality without relying on Euclidean assumptions or handcrafted heuristics. The approach is evaluated on two case studies using ride-sharing data from San Francisco. On benchmark instances, the method achieves solutions within 0.4% of best-known results while reducing computation times by orders of magnitude. A second case study considers large-scale instances with up to 250 request pairs, realistic energy models, and nonlinear charging. On these instances, the learned policy outperforms Adaptive Large Neighborhood Search (ALNS) by 9.5% in solution quality while achieving 100% service completion, with sub-second inference times compared to hours for the metaheuristic. Finally, sensitivity analyses quantify the impact of battery capacity, fleet size, ride-sharing capacity, and reward weights, while robustness experiments show that deterministically trained policies generalize effectively under stochastic conditions.         ",
    "url": "https://arxiv.org/abs/2601.22052",
    "authors": [
      "Sten Elling Tingstad Jacobsen",
      "Attila Lischka",
      "Bal\u00e1zs Kulcs\u00e1r",
      "Anders Lindman"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2601.22054",
    "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
    "abstract": "           Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at this http URL to support community research.         ",
    "url": "https://arxiv.org/abs/2601.22054",
    "authors": [
      "Baorui Ma",
      "Jiahui Yang",
      "Donglin Di",
      "Xuancheng Zhang",
      "Jianxun Cui",
      "Hao Li",
      "Yan Xie",
      "Wei Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.22055",
    "title": "$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA",
    "abstract": "           Retrieval-augmented generation is a practical paradigm for question answering over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieval can fail in long contexts by looping on partial evidence or drifting into irrelevant sections as noise accumulates, since each step is guided only by the current snippet without a persistent global search state. We introduce $G^2$-Reader, a dual-graph system, to address both issues. It evolves a Content Graph to preserve document-native structure and cross-modal semantics, and maintains a Planning Graph, an agentic directed acyclic graph of sub-questions, to track intermediate findings and guide stepwise navigation for evidence completion. On VisDoMBench across five multimodal domains, $G^2$-Reader with Qwen3-VL-32B-Instruct reaches 66.21\\% average accuracy, outperforming strong baselines and a standalone GPT-5 (53.08\\%).         ",
    "url": "https://arxiv.org/abs/2601.22055",
    "authors": [
      "Yaxin Du",
      "Junru Song",
      "Yifan Zhou",
      "Cheng Wang",
      "Jiahao Gu",
      "Zimeng Chen",
      "Menglan Chen",
      "Wen Yao",
      "Yang Yang",
      "Ying Wen",
      "Siheng Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2601.22061",
    "title": "BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation",
    "abstract": "           The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.         ",
    "url": "https://arxiv.org/abs/2601.22061",
    "authors": [
      "Li Zhang",
      "Pengtao Xie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.22075",
    "title": "Lens-descriptor guided evolutionary algorithm for optimization of complex optical systems with glass choice",
    "abstract": "           Designing high-performance optical lenses entails exploring a high-dimensional, tightly constrained space of surface curvatures, glass choices, element thicknesses, and spacings. In practice, standard optimizers (e.g., gradient-based local search and evolutionary strategies) often converge to a single local optimum, overlooking many comparably good alternatives that matter for downstream engineering decisions. We propose the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA), a two-stage framework for multimodal lens optimization. LDG-EA first partitions the design space into behavior descriptors defined by curvature-sign patterns and material indices, then learns a probabilistic model over descriptors to allocate evaluations toward promising regions. Within each descriptor, LDG-EA applies the Hill-Valley Evolutionary Algorithm with covariance-matrix self-adaptation to recover multiple distinct local minima, optionally followed by gradient-based refinement. On a 24-variable (18 continuous and 6 integer), six-element Double-Gauss topology, LDG-EA generates on average around 14500 candidate minima spanning 636 unique descriptors, an order of magnitude more than a CMA-ES baseline, while keeping wall-clock time at one hour scale. Although the best LDG-EA design is slightly worse than a fine-tuned reference lens, it remains in the same performance range. Overall, the proposed LDG-EA produces a diverse set of solutions while maintaining competitive quality within practical computational budgets and wall-clock time.         ",
    "url": "https://arxiv.org/abs/2601.22075",
    "authors": [
      "Kirill Antonov",
      "Teus Tukker",
      "Tiago Botari",
      "Thomas H. W. B\u00e4ck",
      "Anna V. Kononova",
      "Niki van Stein"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2601.22082",
    "title": "Auditorily Embodied Conversational Agents: Effects of Spatialization and Situated Audio Cues on Presence and Social Perception",
    "abstract": "           Embodiment can enhance conversational agents, such as increasing their perceived presence. This is typically achieved through visual representations of a virtual body; however, visual modalities are not always available, such as when users interact with agents using headphones or display-less glasses. In this work, we explore auditory embodiment. By introducing auditory cues of bodily presence - through spatially localized voice and situated Foley audio from environmental interactions - we investigate how audio alone can convey embodiment and influence perceptions of a conversational agent. We conducted a 2 (spatialization: monaural vs. spatialized) x 2 (Foley: none vs. Foley) within-subjects study, where participants (n=24) engaged in conversations with agents. Our results show that spatialization and Foley increase co-presence, but reduce users' perceptions of the agent's attention and other social attributes.         ",
    "url": "https://arxiv.org/abs/2601.22082",
    "authors": [
      "Yi Fei Cheng",
      "Jarod Bloch",
      "Alexander Wang",
      "Andrea Bianchi",
      "Anusha Withana",
      "Anhong Guo",
      "Laurie M. Heller",
      "David Lindlbauer"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2601.22083",
    "title": "Latent Adversarial Regularization for Offline Preference Optimization",
    "abstract": "           Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.         ",
    "url": "https://arxiv.org/abs/2601.22083",
    "authors": [
      "Enyi Jiang",
      "Yibo Jacky Zhang",
      "Yinglun Xu",
      "Andreas Haupt",
      "Nancy Amato",
      "Sanmi Koyejo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.22090",
    "title": "ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection",
    "abstract": "           Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.         ",
    "url": "https://arxiv.org/abs/2601.22090",
    "authors": [
      "Runsheng Wang",
      "Katelyn Lee",
      "Xinyue Zhu",
      "Lauren Winterbottom",
      "Dawn M. Nilsen",
      "Joel Stein",
      "Matei Ciocarlie"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2601.22107",
    "title": "Prior-Informed Flow Matching for Graph Reconstruction",
    "abstract": "           We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.         ",
    "url": "https://arxiv.org/abs/2601.22107",
    "authors": [
      "Harvey Chen",
      "Nicolas Zilberstein",
      "Santiago Segarra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.22134",
    "title": "Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography",
    "abstract": "           Pancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with Artificial Intelligence). It was trained on data from 1,598 patients from a single medical center. In the internal test involving 1,009 patients, ePAI achieved an area under the receiver operating characteristic curve (AUC) of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC less than 2 cm in diameter, precisely localizing PDAC as small as 2 mm. In an external test involving 7,158 patients across 6 centers, ePAI achieved an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%, precisely localizing PDAC as small as 5 mm. Importantly, ePAI detected PDACs on prediagnostic CT scans obtained 3 to 36 months before clinical diagnosis that had originally been overlooked by radiologists. It successfully detected and localized PDACs in 75 of 159 patients, with a median lead time of 347 days before clinical diagnosis. Our multi-reader study showed that ePAI significantly outperformed 30 board-certified radiologists by 50.3% (P < 0.05) in sensitivity while maintaining a comparable specificity of 95.4% in detecting PDACs early and prediagnostic. These findings suggest its potential of ePAI as an assistive tool to improve early detection of pancreatic cancer.         ",
    "url": "https://arxiv.org/abs/2601.22134",
    "authors": [
      "Wenxuan Li",
      "Pedro R. A. S. Bassi",
      "Lizhou Wu",
      "Xinze Zhou",
      "Yuxuan Zhao",
      "Qi Chen",
      "Szymon Plotka",
      "Tianyu Lin",
      "Zheren Zhu",
      "Marisa Martin",
      "Justin Caskey",
      "Shanshan Jiang",
      "Xiaoxi Chen",
      "Jaroslaw B. \u0106wikla",
      "Artur Sankowski",
      "Yaping Wu",
      "Sergio Decherchi",
      "Andrea Cavalli",
      "Chandana Lall",
      "Cristian Tomasetti",
      "Yaxing Guo",
      "Xuan Yu",
      "Yuqing Cai",
      "Hualin Qiao",
      "Jie Bao",
      "Chenhan Hu",
      "Ximing Wang",
      "Arkadiusz Sitek",
      "Kai Ding",
      "Heng Li",
      "Meiyun Wang",
      "Dexin Yu",
      "Guang Zhang",
      "Yang Yang",
      "Kang Wang",
      "Alan L. Yuille",
      "Zongwei Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.22137",
    "title": "PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training",
    "abstract": "           Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.         ",
    "url": "https://arxiv.org/abs/2601.22137",
    "authors": [
      "Shenghao Yang",
      "Zhichao Wang",
      "Oleg Balabanov",
      "N. Benjamin Erichson",
      "Michael W. Mahoney"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2601.22141",
    "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
    "abstract": "           In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.         ",
    "url": "https://arxiv.org/abs/2601.22141",
    "authors": [
      "Grzegorz Stefanski",
      "Alberto Presta",
      "Michal Byra"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.22151",
    "title": "Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing",
    "abstract": "           Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation. The code is open source at this https URL ",
    "url": "https://arxiv.org/abs/2601.22151",
    "authors": [
      "Daniel Stein",
      "Shaoyi Huang",
      "Rolf Drechsler",
      "Bing Li",
      "Grace Li Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2601.10761",
    "title": "LSR-Net: A Lightweight and Strong Robustness Network for Bearing Fault Diagnosis in Noise Environment",
    "abstract": "           Rotating bearings play an important role in modern industries, but have a high probability of occurrence of defects because they operate at high speed, high load, and poor operating environments. Therefore, if a delay time occurs when a bearing is diagnosed with a defect, this may cause economic loss and loss of life. Moreover, since the vibration sensor from which the signal is collected is highly affected by the operating environment and surrounding noise, accurate defect diagnosis in a noisy environment is also important. In this paper, we propose a lightweight and strong robustness network (LSR-Net) that is accurate in a noisy environment and enables real-time fault diagnosis. To this end, first, a denoising and feature enhancement module (DFEM) was designed to create a 3-channel 2D matrix by giving several nonlinearity to the feature-map that passed through the denoising module (DM) block composed of convolution-based denoising (CD) blocks. Moreover, adaptive pruning was applied to DM to improve denoising ability when the power of noise is strong. Second, for lightweight model design, a convolution-based efficiency shuffle (CES) block was designed using group convolution (GConv), group pointwise convolution (GPConv) and channel split that can design the model while maintaining low parameters. In addition, the trade-off between the accuracy and model computational complexity that can occur due to the lightweight design of the model was supplemented using attention mechanisms and channel shuffle. In order to verify the defect diagnosis performance of the proposed model, performance verification was conducted in a noisy environment using a vibration signal. As a result, it was confirmed that the proposed model had the best anti-noise ability compared to the benchmark models, and the computational complexity of the model was also the lowest.         ",
    "url": "https://arxiv.org/abs/2601.10761",
    "authors": [
      "Junseok Lee",
      "Jihye Shin",
      "Sangyong Lee",
      "Chang-Jae Chun"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.20869",
    "title": "Integrating Color Histogram Analysis and Convolutional Neural Network for Skin Lesion Classification",
    "abstract": "           The color of skin lesions is an important diagnostic feature for identifying malignant melanoma and other skin diseases. Typical colors associated with melanocytic lesions include tan, brown, black, red, white, and blue gray. This study introduces a novel feature: the number of colors present in a lesion, which can indicate the severity of disease and help distinguish melanomas from benign lesions. We propose a color histogram analysis method to examine lesion pixel values from three publicly available datasets: PH2, ISIC2016, and Med Node. The PH2 dataset contains ground truth annotations of lesion colors, while ISIC2016 and Med Node do not; our algorithm estimates the ground truth using color histogram analysis based on PH2. We then design and train a 19 layer Convolutional Neural Network (CNN) with residual skip connections to classify lesions into three categories based on the number of colors present. DeepDream visualization is used to interpret features learned by the network, and multiple CNN configurations are tested. The best model achieves a weighted F1 score of 75 percent. LIME is applied to identify important regions influencing model decisions. The results show that the number of colors in a lesion is a significant feature for describing skin conditions, and the proposed CNN with three skip connections demonstrates strong potential for clinical diagnostic support.         ",
    "url": "https://arxiv.org/abs/2601.20869",
    "authors": [
      "M. A. Rasel",
      "Sameem Abdul Kareem",
      "Unaizah Obaidellah"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2601.20875",
    "title": "Distributed Causality in the SDG Network: Evidence from Panel VAR and Conditional Independence Analysis",
    "abstract": "           The achievement of the 2030 Sustainable Development Goals (SDGs) is dependent upon strategic resource distribution. We propose a causal discovery framework using Panel Vector Autoregression, along with both country-specific fixed effects and PCMCI+ conditional independence testing on 168 countries (2000-2025) to develop the first complete causal architecture of SDG dependencies. Utilizing 8 strategically chosen SDGs, we identify a distributed causal network (i.e., no single 'hub' SDG), with 10 statistically significant Granger-causal relationships identified as 11 unique direct effects. Education to Inequality is identified as the most statistically significant direct relationship (r = -0.599; p < 0.05), while effect magnitude significantly varies depending on income levels (e.g., high-income: r = -0.65; lower-middle-income: r = -0.06; non-significant). We also reject the idea that there exists a single 'keystone' SDG. Additionally, we offer a proposed tiered priority framework for the SDGs namely, identifying upstream drivers (Education, Growth), enabling goals (Institutions, Energy), and downstream outcomes (Poverty, Health). Therefore, we conclude that effective SDG acceleration can be accomplished through coordinated multi-dimensional intervention(s), and that single-goal sequential strategies are insufficient.         ",
    "url": "https://arxiv.org/abs/2601.20875",
    "authors": [
      "Md Muhtasim Munif Fahim",
      "Md Jahid Hasan Imran",
      "Luknath Debnath",
      "Tonmoy Shill",
      "Md. Naim Molla",
      "Ehsanul Bashar Pranto",
      "Md Shafin Sanyan Saad",
      "Md Rezaul Karim"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2601.20904",
    "title": "ECGFlowCMR: Pretraining with ECG-Generated Cine CMR Improves Cardiac Disease Classification and Phenotype Prediction",
    "abstract": "           Cardiac Magnetic Resonance (CMR) imaging provides a comprehensive assessment of cardiac structure and function but remains constrained by high acquisition costs and reliance on expert annotations, limiting the availability of large-scale labeled datasets. In contrast, electrocardiograms (ECGs) are inexpensive, widely accessible, and offer a promising modality for conditioning the generative synthesis of cine CMR. To this end, we propose ECGFlowCMR, a novel ECG-to-CMR generative framework that integrates a Phase-Aware Masked Autoencoder (PA-MAE) and an Anatomy-Motion Disentangled Flow (AMDF) to address two fundamental challenges: (1) the cross-modal temporal mismatch between multi-beat ECG recordings and single-cycle CMR sequences, and (2) the anatomical observability gap due to the limited structural information inherent in ECGs. Extensive experiments on the UK Biobank and a proprietary clinical dataset demonstrate that ECGFlowCMR can generate realistic cine CMR sequences from ECG inputs, enabling scalable pretraining and improving performance on downstream cardiac disease classification and phenotype prediction tasks.         ",
    "url": "https://arxiv.org/abs/2601.20904",
    "authors": [
      "Xiaocheng Fang",
      "Zhengyao Ding",
      "Jieyi Cai",
      "Yujie Xiao",
      "Bo Liu",
      "Jiarui Jin",
      "Haoyu Wang",
      "Guangkun Nie",
      "Shun Huang",
      "Ting Chen",
      "Hongyan Li",
      "Shenda Hong"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21014",
    "title": "Efficient Causal Structure Learning via Modular Subgraph Integration",
    "abstract": "           Learning causal structures from observational data remains a fundamental yet computationally intensive task, particularly in high-dimensional settings where existing methods face challenges such as the super-exponential growth of the search space and increasing computational demands. To address this, we introduce VISTA (Voting-based Integration of Subgraph Topologies for Acyclicity), a modular framework that decomposes the global causal structure learning problem into local subgraphs based on Markov Blankets. The global integration is achieved through a weighted voting mechanism that penalizes low-support edges via exponential decay, filters unreliable ones with an adaptive threshold, and ensures acyclicity using a Feedback Arc Set (FAS) algorithm. The framework is model-agnostic, imposing no assumptions on the inductive biases of base learners, is compatible with arbitrary data settings without requiring specific structural forms, and fully supports parallelization. We also theoretically establish finite-sample error bounds for VISTA, and prove its asymptotic consistency under mild conditions. Extensive experiments on both synthetic and real datasets consistently demonstrate the effectiveness of VISTA, yielding notable improvements in both accuracy and efficiency over a wide range of base learners.         ",
    "url": "https://arxiv.org/abs/2601.21014",
    "authors": [
      "Haixiang Sun",
      "Pengchao Tian",
      "Zihan Zhou",
      "Jielei Zhang",
      "Peiyi Li",
      "Andrew L. Liu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2601.21152",
    "title": "Community detection in network using Szegedy quantum walk",
    "abstract": "           In a network, the vertices with similar characteristics construct communities. The vertices in a community are well-connected. Detecting the communities in a network is a challenging and important problem in the theory of complex networks. One approach to solve this problem uses the classical random walks on the graphs. In quantum computing, quantum walks are the quantum mechanical counterparts of classical random walks. In this article, we employ a variant of Szegedy's quantum walk to develop a procedure for discovering the communities in networks. The limiting probability distribution of quantum walks assists us in determining the inclusion of a vertex in a community. We apply our procedure of community detection on a number of graphs and social networks, such as the relaxed caveman graph, $l$-partition graph, Karate club graph, dolphin's social network, etc.         ",
    "url": "https://arxiv.org/abs/2601.21152",
    "authors": [
      "Md Samsur Rahaman",
      "Supriyo Dutta"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Social and Information Networks (cs.SI)",
      "Combinatorics (math.CO)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2601.21153",
    "title": "A new strategy for finite-sample valid prediction of future insurance claims in the regression setting",
    "abstract": "           The extant insurance literature demonstrates a paucity of finite-sample valid prediction intervals of future insurance claims in the regression setting. To address this challenge, this article proposes a new strategy that converts a predictive method in the unsupervised iid (independent identically distributed) setting to a predictive method in the regression setting. In particular, it enables an actuary to obtain infinitely many finite-sample valid prediction intervals in the regression setting.         ",
    "url": "https://arxiv.org/abs/2601.21153",
    "authors": [
      "Liang Hong"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.21318",
    "title": "QCL-IDS: Quantum Continual Learning for Intrusion Detection with Fidelity-Anchored Stability and Generative Replay",
    "abstract": "           Continual intrusion detection must absorb newly emerging attack stages while retaining legacy detection capability under strict operational constraints, including bounded compute and qubit budgets and privacy rules that preclude long-term storage of raw telemetry. We propose QCL-IDS, a quantum-centric continual-learning framework that co-designs stability and privacy-governed rehearsal for NISQ-era pipelines. Its core component, Q-FISH (Quantum Fisher Anchors), enforces retention using a compact anchor coreset through (i) sensitivity-weighted parameter constraints and (ii) a fidelity-based functional anchoring term that directly limits decision drift on representative historical traffic. To regain plasticity without retaining sensitive flows, QCL-IDS further introduces privacy-preserved quantum generative replay (QGR) via frozen, task-conditioned generator snapshots that synthesize bounded rehearsal samples. Across a three-stage attack stream on UNSW-NB15 and CICIDS2017, QCL-IDS consistently attains the best retention-adaptation trade-off: the gradient-anchor configuration achieves mean Attack-F1 = 0.941 with forgetting = 0.005 on UNSW-NB15 and mean Attack-F1 = 0.944 with forgetting = 0.004 on CICIDS2017, versus 0.800/0.138 and 0.803/0.128 for sequential fine-tuning, respectively.         ",
    "url": "https://arxiv.org/abs/2601.21318",
    "authors": [
      "Zirui Zhu",
      "Xiangyang Li"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2601.21347",
    "title": "Towards Robust Dysarthric Speech Recognition: LLM-Agent Post-ASR Correction Beyond WER",
    "abstract": "           While Automatic Speech Recognition (ASR) is typically benchmarked by word error rate (WER), real-world applications ultimately hinge on semantic fidelity. This mismatch is particularly problematic for dysarthric speech, where articulatory imprecision and disfluencies can cause severe semantic distortions. To bridge this gap, we introduce a Large Language Model (LLM)-based agent for post-ASR correction: a Judge-Editor over the top-k ASR hypotheses that keeps high-confidence spans, rewrites uncertain segments, and operates in both zero-shot and fine-tuned modes. In parallel, we release SAP-Hypo5, the largest benchmark for dysarthric speech correction, to enable reproducibility and future exploration. Under multi-perspective evaluation, our agent achieves a 14.51% WER reduction alongside substantial semantic gains, including a +7.59 pp improvement in MENLI and +7.66 pp in Slot Micro F1 on challenging samples. Our analysis further reveals that WER is highly sensitive to domain shift, whereas semantic metrics correlate more closely with downstream task performance.         ",
    "url": "https://arxiv.org/abs/2601.21347",
    "authors": [
      "Xiuwen Zheng",
      "Sixun Dong",
      "Bornali Phukon",
      "Mark Hasegawa-Johnson",
      "Chang D. Yoo"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2601.21612",
    "title": "Representation-Regularized Convolutional Audio Transformer for Audio Understanding",
    "abstract": "           Bootstrap-based Self-Supervised Learning (SSL) has achieved remarkable progress in audio understanding. However, existing methods typically operate at a single level of granularity, limiting their ability to model the diverse temporal and spectral structures inherent in complex audio signals. Furthermore, bootstrapping representations from scratch is computationally expensive, often requiring extensive training to converge. In this work, we propose the Convolutional Audio Transformer (CAT), a unified framework designed to address these challenges. First, to capture hierarchical audio features, CAT incorporates a Multi-resolution Block that aggregates information across varying granularities. Second, to enhance training efficiency, we introduce a Representation Regularization objective. Drawing inspiration from generative modeling, this auxiliary task guides the student model by aligning its predictions with high-quality semantic representations from frozen, pre-trained external encoders. Experimental results demonstrate that CAT significantly outperforms baselines on audio understanding benchmarks. Notably, it achieves competitive performance on the AudioSet 20k dataset with 5 times faster convergence than existing methods. Codes and checkpoints will be released soon at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.21612",
    "authors": [
      "Bing Han",
      "Chushu Zhou",
      "Yifan Yang",
      "Wei Wang",
      "Chenda Li",
      "Wangyou Zhang",
      "Yanmin Qian"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2601.21856",
    "title": "Blind Ultrasound Image Enhancement via Self-Supervised Physics-Guided Degradation Modeling",
    "abstract": "           Ultrasound (US) interpretation is hampered by multiplicative speckle, acquisition blur from the point-spread function (PSF), and scanner- and operator-dependent artifacts. Supervised enhancement methods assume access to clean targets or known degradations; conditions rarely met in practice. We present a blind, self-supervised enhancement framework that jointly deconvolves and denoises B-mode images using a Swin Convolutional U-Net trained with a \\emph{physics-guided} degradation model. From each training frame, we extract rotated/cropped patches and synthesize inputs by (i) convolving with a Gaussian PSF surrogate and (ii) injecting noise via either spatial additive Gaussian noise or complex Fourier-domain perturbations that emulate phase/magnitude distortions. For US scans, clean-like targets are obtained via non-local low-rank (NLLR) denoising, removing the need for ground truth; for natural images, the originals serve as targets. Trained and validated on UDIAT~B, JNU-IFM, and XPIE Set-P, and evaluated additionally on a 700-image PSFHS test set, the method achieves the highest PSNR/SSIM across Gaussian and speckle noise levels, with margins that widen under stronger corruption. Relative to MSANN, Restormer, and DnCNN, it typically preserves an extra $\\sim$1--4\\,dB PSNR and 0.05--0.15 SSIM in heavy Gaussian noise, and $\\sim$2--5\\,dB PSNR and 0.05--0.20 SSIM under severe speckle. Controlled PSF studies show reduced FWHM and higher peak gradients, evidence of resolution recovery without edge erosion. Used as a plug-and-play preprocessor, it consistently boosts Dice for fetal head and pubic symphysis segmentation. Overall, the approach offers a practical, assumption-light path to robust US enhancement that generalizes across datasets, scanners, and degradation types.         ",
    "url": "https://arxiv.org/abs/2601.21856",
    "authors": [
      "Shujaat Khan",
      "Syed Muhammad Atif",
      "Jaeyoung Huh",
      "Syed Saad Azhar"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2601.21887",
    "title": "VSE: Variational state estimation of complex model-free process",
    "abstract": "           We design a variational state estimation (VSE) method that provides a closed-form Gaussian posterior of an underlying complex dynamical process from (noisy) nonlinear measurements. The complex process is model-free. That is, we do not have a suitable physics-based model characterizing the temporal evolution of the process state. The closed-form Gaussian posterior is provided by a recurrent neural network (RNN). The use of RNN is computationally simple in the inference phase. For learning the RNN, an additional RNN is used in the learning phase. Both RNNs help each other learn better based on variational inference principles. The VSE is demonstrated for a tracking application - state estimation of a stochastic Lorenz system (a benchmark process) using a 2-D camera measurement model. The VSE is shown to be competitive against a particle filter that knows the Lorenz system model and a recently proposed data-driven state estimation method that does not know the Lorenz system model.         ",
    "url": "https://arxiv.org/abs/2601.21887",
    "authors": [
      "Gustav Nor\u00e9n",
      "Anubhab Ghosh",
      "Fredrik Cumlin",
      "Saikat Chatterjee"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2601.22021",
    "title": "Post-Disaster Resource Redistribution and Cooperation Evolution Based on Two-Layer Network Evolutionary Games",
    "abstract": "           In the aftermath of large-scale disasters, the scarcity of resources and the paralysis of infrastructure raise severe challenges to effective post-disaster recovery. Efficient coordination between shelters and victims plays a crucial role in building community resilience, yet the evolution of two-layer behavioral feedback between these two groups through network coupling remains insufficiently understood. Here, this study develops a two-layer network to capture the cross-layer coupling between shelters and victims. The upper layer uses a post-disaster emergency resource redistribution model within the framework of the public goods game, while the lower layer adopts a cooperative evolutionary game to describe internal victim interactions. Monte Carlo simulations on scale-free networks reveal threshold effects of incentives: moderate public goods enhancement and subsidies promote cooperation, whereas excessive incentives induce free-riding. In contrast, credible and well-executed punishment effectively suppresses defection. Targeted punishment of highly connected shelters significantly enhances cooperation under resource constraints. A comparative analysis using a network generated from the actual coordinates of Beijing shelters confirms the model's generality and practical applicability. The findings highlight the importance of calibrated incentives, enforceable sanctions, and structural targeting in fostering robust cooperation across organizational and individual levels in post-disaster environments.         ",
    "url": "https://arxiv.org/abs/2601.22021",
    "authors": [
      "Yu Chen",
      "Genjiu Xu",
      "Sinan Feng",
      "Chaoqian Wang"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2601.22080",
    "title": "Volt/VAR Optimization in Transmission Networks with Discrete-Control Devices",
    "abstract": "           Voltage (Volt) and reactive-power (VAR) control in transmission networks is critical for reliability and increasingly needs fast, implementable decisions. This paper presents a transmission Volt/VAR Optimization (VVO) framework that co-optimizes discrete control of on-load tap-changing transformers (OLTCs) and capacitor banks (CBs) with AC power flow (ACPF) physics to improve voltage stability and minimize VAR generation. The framework follows a relax-round-resolve pipeline: a continuous relaxation proposes targets, a rounding step selects feasible discrete settings, and a final solve enforces AC power flow physics. Extensive experiments on IEEE, PEGASE, and RTE systems show consistent improvements in voltage and VAR quality metrics with modest generator redispatch while preserving economic operation and achieving compatible runtimes with real-time transmission operations.         ",
    "url": "https://arxiv.org/abs/2601.22080",
    "authors": [
      "Shuaicheng Tong",
      "Michael A. Boateng",
      "Mathieu Tanneau",
      "Pascal Van Hentenryck"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2210.00310",
    "title": "Parametrized Power-Iteration Clustering for Directed Graphs",
    "abstract": "           Vertex-level clustering for directed graphs (digraphs) remains challenging as edge directionality breaks the key assumptions underlying popular spectral methods, which also incur the overhead of eigen-decomposition. This paper proposes Parametrized Power-Iteration Clustering (ParPIC), a random-walk-based clustering method for weakly connected digraphs. This builds over the Power-Iteration Clustering paradigm, which uses the rows of the iterated diffusion operator as a data embedding. ParPIC has three important features: the use of parametrized reversible random walk operators, the automatic tuning of the diffusion time, and the efficient truncation of the final embedding, which produces low-dimensional data representations and reduces complexity. Empirical results on synthetic and real-world graphs demonstrate that ParPIC achieves competitive clustering accuracy with improved scalability relative to spectral and teleportation-based methods.         ",
    "url": "https://arxiv.org/abs/2210.00310",
    "authors": [
      "Gwendal Debaussart-Joniec",
      "Harry Sevi",
      "Matthieu Jonckheere",
      "Argyris Kalogeratos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2302.11250",
    "title": "Dynamic Debt Swapping in Financial Networks",
    "abstract": "           A debt swap is an elementary edge swap in a directed, weighted graph, where two edges with the same weight swap their targets. Debt swaps are a natural and appealing operation in financial networks, in which nodes are banks and edges represent debt contracts. They can improve the clearing payments and the stability of these networks. However, their algorithmic properties are not well-understood. We analyze the computational complexity of debt swapping. Our main interest lies in semi-positive swaps, in which no creditor strictly suffers and at least one strictly profits. These swaps lead to a Pareto-improvement in the entire network. We consider network optimization via sequences of v-improving debt swaps from which a given bank v strictly profits. For ranking-based clearing, we show that every sequence of semi-positive v-improving swaps has polynomial length. In contrast, for arbitrary v-improving swaps, the problem of reaching a network configuration that allows no further swaps is PLS-complete. In global optimization, the goal is to maximize the utility of a given bank $v$ by performing a sequence of debt swaps in the network. This problem is NP-hard to approximate for multiple types of swaps. Moreover, we study reachability problems -- deciding if a sequence of swaps exists between given initial and final networks. We design a polynomial-time algorithm to decide this question for arbitrary swaps and derive hardness results for several other types of swaps. Many of our results can be extended to networks with arbitrary monotone clearing.         ",
    "url": "https://arxiv.org/abs/2302.11250",
    "authors": [
      "Henri Froese",
      "Martin Hoefer",
      "Lisa Wilhelmi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Computer Science and Game Theory (cs.GT)",
      "Risk Management (q-fin.RM)"
    ]
  },
  {
    "id": "arXiv:2311.01591",
    "title": "Fair Graph Machine Learning under Adversarial Missingness Processes",
    "abstract": "           Graph Neural Networks (GNNs) have achieved state-of-the-art results in many relevant tasks where decisions might disproportionately impact specific communities. However, existing work on fair GNNs often assumes that either sensitive attributes are fully observed or they are missing completely at random. We show that an adversarial missingness process can inadvertently disguise a fair model through the imputation, leading the model to overestimate the fairness of its predictions. We address this challenge by proposing Better Fair than Sorry (BFtS), a fair missing data imputation model for sensitive attributes. The key principle behind BFtS is that imputations should approximate the worst-case scenario for fairness -- i.e. when optimizing fairness is the hardest. We implement this idea using a 3-player adversarial scheme where two adversaries collaborate against a GNN classifier, and the classifier minimizes the maximum bias. Experiments using synthetic and real datasets show that BFtS often achieves a better fairness x accuracy trade-off than existing alternatives under an adversarial missingness process.         ",
    "url": "https://arxiv.org/abs/2311.01591",
    "authors": [
      "Debolina Halder Lina",
      "Arlei Silva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2312.08410",
    "title": "Universal approximation property of Banach space-valued random feature models including random neural networks",
    "abstract": "           We introduce a Banach space-valued extension of random feature learning, a data-driven supervised machine learning technique for large-scale kernel approximation. By randomly initializing the feature maps, only the linear readout needs to be trained, which reduces the computational complexity substantially. Viewing random feature models as Banach space-valued random variables, we prove a universal approximation result in the corresponding Bochner space. Moreover, we derive approximation rates and an explicit algorithm to learn an element of the given Banach space by such models. The framework of this paper includes random trigonometric/Fourier regression and in particular random neural networks which are single-hidden-layer feedforward neural networks whose weights and biases are randomly initialized, whence only the linear readout needs to be trained. For the latter, we can then lift the universal approximation property of deterministic neural networks to random neural networks, even within function spaces over non-compact domains, e.g., weighted spaces, $L^p$-spaces, and (weighted) Sobolev spaces, where the latter includes the approximation of the (weak) derivatives. In addition, we analyze when the training costs for approximating a given function grow polynomially in both the input/output dimension and the reciprocal of a pre-specified tolerated approximation error. Furthermore, we demonstrate in a numerical example the empirical advantages of random feature models over their deterministic counterparts.         ",
    "url": "https://arxiv.org/abs/2312.08410",
    "authors": [
      "Ariel Neufeld",
      "Philipp Schmocker"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2404.13318",
    "title": "Federated Learning for Heterogeneous Electronic Health Record Systems with Cost Effective Participant Selection",
    "abstract": "           The increasing volume of electronic health records (EHRs) presents the opportunity to improve the accuracy and robustness of models in clinical prediction tasks. Unlike traditional centralized approaches, federated learning enables training on data from multiple institutions while preserving patient privacy and complying with regulatory constraints. In practice, healthcare institutions (i.e., hosts) often need to build predictive models tailored to their specific needs (e.g., creatinine-level prediction, N-day readmission prediction) using federated learning. When building a federated learning model for a single healthcare institution, two key challenges arise: (1) ensuring compatibility across heterogeneous EHR systems, and (2) managing federated learning costs within budget constraints. Specifically, heterogeneity in EHR systems across institutions hinders compatible modeling, while the computational costs of federated learning can exceed practical budget limits for healthcare institutions. To address these challenges, we propose EHRFL, a federated learning framework designed for building a cost-effective, host-specific predictive model using patient EHR data. EHRFL consists of two components: (1) text-based EHR modeling, which facilitates cross-institution compatibility without costly data standardization, and (2) a participant selection strategy based on averaged patient embedding similarity to reduce the number of participants without degrading performance. Our participant selection strategy sharing averaged patient embeddings is differentially private, ensuring patient privacy. Experiments on multiple open-source EHR datasets demonstrate the effectiveness of both components. With our framework, healthcare institutions can build institution-specific predictive models under budgetary constraints with reduced costs and time.         ",
    "url": "https://arxiv.org/abs/2404.13318",
    "authors": [
      "Jiyoun Kim",
      "Junu Kim",
      "Kyunghoon Hur",
      "Edward Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.00005",
    "title": "Dual-pronged deep learning preprocessing on heterogeneous platforms with CPU, Accelerator and CSD",
    "abstract": "           For image-related deep learning tasks, the first step often involves reading data from external storage and performing preprocessing on the CPU. As accelerator speed increases and the number of single compute node accelerators increases, the computing and data transfer capabilities gap between accelerators and CPUs gradually increases. Data reading and preprocessing become progressively the bottleneck of these tasks. Our work, DDLP, addresses the data computing and transfer bottleneck of deep learning preprocessing using Computable Storage Devices (CSDs). DDLP allows the CPU and CSD to efficiently parallelize preprocessing from both ends of the datasets, respectively. To this end, we propose two adaptive dynamic selection strategies to make DDLP control the accelerator to automatically read data from different sources. The two strategies trade-off between consistency and efficiency. DDLP achieves sufficient computational overlap between CSD data preprocessing and CPU preprocessing, accelerator computation, and accelerator data reading. In addition, DDLP leverages direct storage technology to enable efficient SSD-to-accelerator data transfer. In addition, DDLP reduces the use of expensive CPU and DRAM resources with more energy-efficient CSDs, alleviating preprocessing bottlenecks while significantly reducing power consumption. Extensive experimental results show that DDLP can improve learning speed by up to 23.5% on ImageNet Dataset while reducing energy consumption by 19.7% and CPU and DRAM usage by 37.6%. DDLP also improves the learning speed by up to 27.6% on the Cifar-10 dataset.         ",
    "url": "https://arxiv.org/abs/2407.00005",
    "authors": [
      "Jia Wei",
      "Xingjun Zhang",
      "Witold Pedrycz",
      "Longxiang Wang",
      "Jie Zhao"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2408.06787",
    "title": "FLAME: Empowering Frozen LLMs for Knowledge Graph Completion",
    "abstract": "           Traditional knowledge graph completion (KGC) methods rely solely on structural information and struggle with sparsity, while Large Language Models (LLMs) address these limitations through rich world knowledge and strong context modeling. Fine-tuning LLMs is effective but costly, while non-fine-tuned LLMs are efficient but suboptimal. To address this trade-off, we propose \\textbf{FLAME}, a framework that extracts context-aware hidden states from intermediate layers of frozen LLMs to train data-efficient KGC classifiers. We bridge LLM-KG semantic gaps via subgraph-based entity descriptions and employ sliced mutual information (SMI) to quantify task-relevant information in representations. Experiments demonstrate that FLAME achieves 47\\% improvement over non-fine-tuned LLM baselines and, to our knowledge, is the first to achieve fine-tuned performance with $188\\times$ memory efficiency and $26.11\\times$ speedup.         ",
    "url": "https://arxiv.org/abs/2408.06787",
    "authors": [
      "Bo Xue",
      "Yi Xu",
      "Bolei Ma",
      "Yunchong Song",
      "Jiaxin Ding",
      "Luoyi Fu",
      "Xinbing Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.15578",
    "title": "FireFly-S: Exploiting Dual-Side Sparsity for Spiking Neural Networks Acceleration with Reconfigurable Spatial Architecture",
    "abstract": "           Spiking Neural Networks (SNNs), with brain-inspired structure using discrete spikes instead of continuous activations, are gaining attention for their efficient processing on neuromorphic chips. While current SNN hardware accelerators often prioritize temporal spike sparsity, exploiting sparse synaptic weights offers significant untapped potential for even greater efficiency. To address this, we propose FireFly-S, a Sparse extension of the FireFly series. This co-optimized software-hardware design focuses on leveraging dual-side sparsity for acceleration. On the software side, we propose a algorithmic optimization framework that combines gradient rewiring for pruning and modified Learned Step Size Quantization (LSQ) for SNNs, achieving a weight sparsity exceeding 85\\% and enabling efficient 4-bit quantization with negligible accuracy loss. On the hardware side, we present an efficient dual-side sparsity detector employing a Bitmap-based sparse decoding logic to pinpoint the positions of non-zero weights and input spikes. The logic allows for direct bypassing of redundant computations, thereby enhancing computational efficiency. Different from the overlay architecture adopted by previous FireFly series, we adopt a parametric spatial architecture with inter-layer pipelining that can fully exploit the fine-grained programmability and reconfigurability of Field-Programmable Gate Arrays (FPGAs), enabling fast deployment for various models. A spatial-temporal dataflow is also proposed to support such inter-layer pipelining and avoid long-term temporal dependencies. In experiments conducted on the MNIST, DVS-Gesture and CIFAR-10 datasets, the FireFly-S model achieves 85--95\\% sparsity with 4-bit quantization and the hardware accelerator effectively leverages the dual-side sparsity, delivering performance metrics of 10,047~FPS/W on MNIST, 3,683~FPS/W on DVS-Gesture, and 2,327~FPS/W on CIFAR-10.         ",
    "url": "https://arxiv.org/abs/2408.15578",
    "authors": [
      "Tenglong Li",
      "Jindong Li",
      "Guobin Shen",
      "Dongcheng Zhao",
      "Qian Zhang",
      "Yi Zeng"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2409.13959",
    "title": "One Model, Any Conjunctive Query: Graph Neural Networks for Answering Queries over Incomplete Knowledge Graphs",
    "abstract": "           Motivated by the incompleteness of modern knowledge graphs, a new setup for query answering has emerged, where the goal is to predict answers that do not necessarily appear in the knowledge graph, but are present in its completion. In this paper, we formally introduce and study two query answering problems, namely, query answer classification and query answer retrieval. To solve these problems, we propose AnyCQ, a model that can classify answers to any conjunctive query on any knowledge graph. At the core of our framework lies a graph neural network trained using a reinforcement learning objective to answer Boolean queries. Trained only on simple, small instances, AnyCQ generalizes to large queries of arbitrary structure, reliably classifying and retrieving answers to queries that existing approaches fail to handle. This is empirically validated through our newly proposed, challenging benchmarks. Finally, we empirically show that AnyCQ can effectively transfer to completely novel knowledge graphs when equipped with an appropriate link prediction model, highlighting its potential for querying incomplete data.         ",
    "url": "https://arxiv.org/abs/2409.13959",
    "authors": [
      "Krzysztof Olejniczak",
      "Xingyue Huang",
      "Mikhail Galkin",
      "\u0130smail \u0130lkan Ceylan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.01308",
    "title": "WL Tests Are Far from All We Need: Revisiting WL-Test Hardness and GNN Expressive Power from a Distributed Computation Perspective",
    "abstract": "           The expressive power of graph neural networks (GNNs) is often studied through their relationship to the Weisfeiler-Lehman (WL) tests. Despite its influence, this perspective leaves two gaps: (i) it is unclear whether WL tests are sufficiently primitive for understanding GNN expressivity, and (ii) WL-induced equivalence does not align well with characterizing the function classes that GNNs can approximate or compute. We attempt to address both gaps. First, we strengthen hardness results for the vanilla WL test, showing that in many settings it is not primitive enough to be implemented by constant-depth GNNs. Second, we propose an alternative framework for studying GNN expressivity based on an extended CONGEST model with an explicit preprocessing phase. Within this framework, we identify implicit shortcuts introduced in prior analyses and establish further results for WL tests in settings where graphs are augmented with virtual nodes and virtual edges.         ",
    "url": "https://arxiv.org/abs/2410.01308",
    "authors": [
      "Guanyu Cui",
      "Yuhe Guo",
      "Zhewei Wei",
      "Hsin-Hao Su"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.02168",
    "title": "Do graph neural network states contain graph properties?",
    "abstract": "           Deep neural networks (DNNs) achieve state-of-the-art performance on many tasks, but this often requires increasingly larger model sizes, which in turn leads to more complex internal representations. Explainability techniques (XAI) have made remarkable progress in the interpretability of ML models. However, the non-euclidean nature of Graph Neural Networks (GNNs) makes it difficult to reuse already existing XAI methods. While other works have focused on instance-based explanation methods for GNNs, very few have investigated model-based methods and, to our knowledge, none have tried to probe the embedding of the GNNs for structural graph properties. In this paper we present a model agnostic explainability pipeline for Graph Neural Networks (GNNs) employing diagnostic classifiers. We propose to consider graph-theoretic properties as the features of choice for studying the emergence of representations in GNNs. This pipeline aims to probe and interpret the learned representations in GNNs across various architectures and datasets, refining our understanding and trust in these models.         ",
    "url": "https://arxiv.org/abs/2411.02168",
    "authors": [
      "Tom Pelletreau-Duris",
      "Ruud van Bakel",
      "Michael Cochez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.03517",
    "title": "Understanding Self-Supervised Learning via Gaussian Mixture Models",
    "abstract": "           Self-supervised learning attempts to learn representations from un-labeled data; it does so via a loss function that encourages the embedding of a point to be close to that of its augmentations. This simple idea performs remarkably well, yet it is not precisely theoretically understood why this is the case. In this paper we analyze self-supervised learning in a natural context: dimensionality reduction in Gaussian Mixture Models. Crucially, we define an augmentation of a data point as being another independent draw from the same underlying mixture component. We show that vanilla contrastive learning (specifically, the InfoNCE loss) is able to find the optimal lower-dimensional subspace even when the Gaussians are not isotropic -- something that vanilla spectral techniques cannot do. We also prove a similar result for \"non-contrastive\" self-supervised learning (i.e., SimSiam loss). We further extend our analyses to multi-modal contrastive learning algorithms (e.g., CLIP). In this setting we show that contrastive learning learns the subset of fisher-optimal subspace, effectively filtering out all the noise from the learnt representations. Finally, we corroborate our theoretical finding through synthetic data experiments.         ",
    "url": "https://arxiv.org/abs/2411.03517",
    "authors": [
      "Parikshit Bansal",
      "Ali Kavis",
      "Sujay Sanghavi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.13578",
    "title": "CMOOD: Concept-based Multi-label OOD Detection",
    "abstract": "           How can models effectively detect out-of-distribution (OOD) samples in complex, multi-label settings without extensive retraining? Existing OOD detection methods struggle to capture the intricate semantic relationships and label co-occurrences inherent in multi-label settings, often requiring large amounts of training data and failing to generalize to unseen label combinations. While large language models have revolutionized zero-shot OOD detection, they primarily focus on single-label scenarios, leaving a critical gap in handling real-world tasks where samples can be associated with multiple interdependent labels. To address these challenges, we introduce COOD, a novel zero-shot multi-label OOD detection framework. COOD leverages pre-trained vision-language models, enhancing them with a concept-based label expansion strategy and a new scoring function. By enriching the semantic space with both positive and negative concepts for each label, our approach models complex label dependencies, precisely differentiating OOD samples without the need for additional training. Extensive experiments demonstrate that our method significantly outperforms existing approaches, achieving approximately 95% average AUROC on both VOC and COCO datasets, while maintaining robust performance across varying numbers of labels and different types of OOD samples.         ",
    "url": "https://arxiv.org/abs/2411.13578",
    "authors": [
      "Zhendong Liu",
      "Yi Nian",
      "Yuehan Qin",
      "Henry Peng Zou",
      "Li Li",
      "Xiyang Hu",
      "Yue Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.19092",
    "title": "Neural Window Decoder for SC-LDPC Codes",
    "abstract": "           In this paper, we propose a neural window decoder (NWD) for spatially coupled low-density parity-check (SC-LDPC) codes. The proposed NWD retains the conventional window decoder (WD) process but incorporates trainable neural weights. To train the weights of NWD, we introduce two novel training strategies. First, we restrict the loss function to target variable nodes (VNs) of the window, which prunes the neural network and accordingly enhances training efficiency. Second, we employ the active learning technique with a normalized loss term to prevent the training process from biasing toward specific training regions. Next, we develop a systematic method to derive non-uniform schedules for the NWD based on the training results. We introduce trainable damping factors that reflect the relative importance of check node (CN) updates. By skipping updates with less importance, we can omit $\\mathbf{41\\%}$ of CN updates without performance degradation compared to the conventional WD. Lastly, we address the error propagation problem inherent in SC-LDPC codes by deploying a complementary weight set, which is activated when an error is detected in the previous window. This adaptive decoding strategy effectively mitigates error propagation without requiring modifications to the code and decoder structures.         ",
    "url": "https://arxiv.org/abs/2411.19092",
    "authors": [
      "Dae-Young Yun",
      "Hee-Youl Kwak",
      "Yongjune Kim",
      "Sang-Hyo Kim",
      "Jong-Seon No"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2412.00112",
    "title": "BiPO: Bidirectional Partial Occlusion Network for Text-to-Motion Synthesis",
    "abstract": "           Generating natural and expressive human motions from textual descriptions is challenging due to the complexity of coordinating full-body dynamics and capturing nuanced motion patterns over extended sequences that accurately reflect the given text. To address this, we introduce BiPO, Bidirectional Partial Occlusion Network for Text-to-Motion Synthesis, a novel model that enhances text-to-motion synthesis by integrating part-based generation with a bidirectional autoregressive architecture. This integration allows BiPO to consider both past and future contexts during generation while enhancing detailed control over individual body parts without requiring ground-truth motion length. To relax the interdependency among body parts caused by the integration, we devise the Partial Occlusion technique, which probabilistically occludes the certain motion part information during training. In our comprehensive experiments, BiPO achieves state-of-the-art performance on the HumanML3D dataset, outperforming recent methods such as ParCo, MoMask, and BAMM in terms of FID scores and overall motion quality. Notably, BiPO excels not only in the text-to-motion generation task but also in motion editing tasks that synthesize motion based on partially generated motion sequences and textual descriptions. These results reveal the BiPO's effectiveness in advancing text-to-motion synthesis and its potential for practical applications.         ",
    "url": "https://arxiv.org/abs/2412.00112",
    "authors": [
      "Seong-Eun Hong",
      "Soobin Lim",
      "Juyeong Hwang",
      "Minwook Chang",
      "Hyeongyeop Kang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2412.00990",
    "title": "Seldom: An Anonymity Network with Selective Deanonymization",
    "abstract": "           While anonymity networks such as Tor provide invaluable privacy guarantees to society, they also enable all kinds of criminal activities. Consequently, many blameless citizens shy away from protecting their privacy using such technology for fear of being associated with criminals. To grasp the potential for alternative privacy protection for those users, we design Seldom, an anonymity network with integrated selective deanonymization that disincentivizes criminal activity. Seldom enables law enforcement agencies to selectively access otherwise anonymized identities of misbehaving users while providing technical guarantees preventing these access rights from being misused. Seldom further ensures translucency, as each access request is approved by a trustworthy consortium of impartial entities and eventually disclosed to the public (without interfering with ongoing investigations). To demonstrate Seldom's feasibility and applicability, we base our implementation on Tor, the most widely used anonymity network. Our evaluation indicates minimal latency, processing, and bandwidth overheads compared to Tor; Seldom's main costs stem from storing flow records and encrypted identities. With at most 636 TB of storage required in total to retain the encrypted identifiers of a Tor-sized network for two years, Seldom provides a practical and deployable technical solution to the inherent problem of criminal activities in anonymity networks. As such, Seldom sheds new light on the potentials and limitations when integrating selective deanonymization into anonymity networks.         ",
    "url": "https://arxiv.org/abs/2412.00990",
    "authors": [
      "Eric Wagner",
      "Roman Matzutt",
      "Martin Henze"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2412.03671",
    "title": "Tight Lower Bounds and Improved Convergence in Performative Prediction",
    "abstract": "           Performative prediction is a framework accounting for the shift in the data distribution induced by the prediction of a model deployed in the real world. Ensuring rapid convergence to a stable solution where the data distribution remains the same after the model deployment is crucial, especially in evolving environments. This paper extends the Repeated Risk Minimization (RRM) framework by utilizing historical datasets from previous retraining snapshots, yielding a class of algorithms that we call Affine Risk Minimizers and enabling convergence to a performatively stable point for a broader class of problems. We introduce a new upper bound for methods that use only the final iteration of the dataset and prove for the first time the tightness of both this new bound and the previous existing bounds within the same regime. We also prove that utilizing historical datasets can surpass the lower bound for last iterate RRM, and empirically observe faster convergence to the stable point on various performative prediction benchmarks. We offer at the same time the first lower bound analysis for RRM within the class of Affine Risk Minimizers, quantifying the potential improvements in convergence speed that could be achieved with other variants in our framework.         ",
    "url": "https://arxiv.org/abs/2412.03671",
    "authors": [
      "Pedram Khorsandi",
      "Rushil Gupta",
      "Mehrnaz Mofakhami",
      "Simon Lacoste-Julien",
      "Gauthier Gidel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.01203",
    "title": "HetGCoT: Heterogeneous Graph-Enhanced Chain-of-Thought LLM Reasoning for Academic Question Answering",
    "abstract": "           Academic question answering (QA) in heterogeneous scholarly networks presents unique challenges requiring both structural understanding and interpretable reasoning. While graph neural networks (GNNs) capture structured graph information and large language models (LLMs) demonstrate strong capabilities in semantic comprehension, current approaches lack integration at the reasoning level. We propose HetGCoT, a framework enabling LLMs to effectively leverage and learn information from graphs to reason interpretable academic QA results. Our framework introduces three technical contributions: (1) a framework that transforms heterogeneous graph structural information into LLM-processable reasoning chains, (2) an adaptive metapath selection mechanism identifying relevant subgraphs for specific queries, and (3) a multi-step reasoning strategy systematically incorporating graph contexts into the reasoning process. Experiments on OpenAlex and DBLP datasets show our approach outperforms all sota baselines. The framework demonstrates adaptability across different LLM architectures and applicability to various scholarly question answering tasks.         ",
    "url": "https://arxiv.org/abs/2501.01203",
    "authors": [
      "Runsong Jia",
      "Mengjia Wu",
      "Ying Ding",
      "Jie Lu",
      "Yi Zhang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2501.02844",
    "title": "GORAG: Graph-based Online Retrieval Augmented Generation for Dynamic Few-shot Social Media Text Classification",
    "abstract": "           Text classification is vital for Web for Good applications like hate speech and misinformation detection. However, traditional models (e.g., BERT) often fail in dynamic few-shot settings where labeled data are scarce, and target labels frequently evolve. While Large Language Models (LLMs) show promise in few-shot settings, their performance is often hindered by increased input size in dynamic evolving scenarios. To address these issues, we propose GORAG, a Graph-based Online Retrieval-Augmented Generation framework for dynamic few-shot text classification. GORAG constructs and maintains a weighted graph of keywords and text labels, representing their correlations as edges. To model these correlations, GORAG employs an edge weighting mechanism to prioritize the importance and reliability of extracted information and dynamically retrieves relevant context using a tailored minimum-cost spanning tree for each input. Empirical evaluations show GORAG outperforms existing approaches by providing more comprehensive and precise contextual information. Our code is released at: this https URL.         ",
    "url": "https://arxiv.org/abs/2501.02844",
    "authors": [
      "Yubo Wang",
      "Haoyang Li",
      "Fei Teng",
      "Lei Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.17377",
    "title": "ASAP: Exploiting the Satisficing Generalization Edge in Neural Combinatorial Optimization",
    "abstract": "           Deep Reinforcement Learning (DRL) has emerged as a promising approach for solving Combinatorial Optimization (CO) problems, such as the 3D Bin Packing Problem (3D-BPP), Traveling Salesman Problem (TSP), or Vehicle Routing Problem (VRP), but these neural solvers often exhibit brittleness when facing distribution shifts. To address this issue, we uncover the Satisficing Generalization Edge, which we validate both theoretically and experimentally: identifying a set of promising actions is inherently more generalizable than selecting the single optimal action. To exploit this property, we propose Adaptive Selection After Proposal (ASAP), a generic framework that decomposes the decision-making process into two distinct phases: a proposal policy that acts as a robust filter, and a selection policy as an adaptable decision maker. This architecture enables a highly effective online adaptation strategy where the selection policy can be rapidly fine-tuned on a new distribution. Concretely, we introduce a two-phase training framework enhanced by Model-Agnostic Meta-Learning (MAML) to prime the model for fast adaptation. Extensive experiments on 3D-BPP, TSP, and CVRP demonstrate that ASAP improves the generalization capability of state-of-the-art baselines and achieves superior online adaptation on out-of-distribution instances.         ",
    "url": "https://arxiv.org/abs/2501.17377",
    "authors": [
      "Han Fang",
      "Paul Weng",
      "Yutong Ban"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.03287",
    "title": "STEMS: Spatial-Temporal Mapping For Spiking Neural Networks",
    "abstract": "           Spiking Neural Networks (SNNs) are promising bio-inspired third-generation neural networks. Recent research has trained deep SNN models with accuracy on par with Artificial Neural Networks (ANNs). Although the event-driven and sparse nature of SNNs show potential for more energy efficient computation than ANNs, SNN neurons have internal states which evolve over time. Keeping track of SNN states can significantly increase data movement and storage requirements, potentially losing its advantages with respect to ANNs. This paper investigates the energy effects of having neuron states, and how it is influenced by the chosen mapping to realistic hardware architectures with advanced memory hierarchies. Therefore, we develop STEMS, a mapping design space exploration for SNNs. STEMS models SNN's stateful behavior and explores intra-layer and inter-layer mapping optimizations to minimize data movement, considering both spatial and temporal SNN dimensions. Using STEMS, we show up to 12x reduction in off-chip data movement and 5x reduction in energy (on top of intra-layer optimizations), on two event-based vision SNN benchmarks. Finally, neuron states may not be needed for all SNN layers. By optimizing neuron states for one of our benchmarks, we show 20x reduction in neuron states and 1.4x better performance without accuracy loss.         ",
    "url": "https://arxiv.org/abs/2502.03287",
    "authors": [
      "Sherif Eissa",
      "Sander Stuijk",
      "Floran De Putter",
      "Andrea Nardi-Dei",
      "Federico Corradi",
      "Henk Corporaal"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2502.08987",
    "title": "Neural Force Field: Few-shot Learning of Generalized Physical Reasoning",
    "abstract": "           Physical reasoning is a remarkable human ability that enables rapid learning and generalization from limited experience. Current AI models, despite extensive training, still struggle to achieve similar generalization, especially in Out-of-distribution (OOD) settings. This limitation stems from their inability to abstract core physical principles from observations. A key challenge is developing representations that can efficiently learn and generalize physical dynamics from minimal data. Here we present Neural Force Field (NFF), a framework extending Neural Ordinary Differential Equation (NODE) to learn complex object interactions through force field representations, which can be efficiently integrated through an Ordinary Differential Equation (ODE) solver to predict object trajectories. Unlike existing approaches that rely on discrete latent spaces, NFF captures fundamental physical concepts such as gravity, support, and collision in continuous explicit force fields. Experiments on three challenging physical reasoning tasks demonstrate that NFF, trained with only a few examples, achieves strong generalization to unseen scenarios. This physics-grounded representation enables efficient forward-backward planning and rapid adaptation through interactive refinement. Our work suggests that incorporating physics-inspired representations into learning systems can help bridge the gap between artificial and human physical reasoning capabilities.         ",
    "url": "https://arxiv.org/abs/2502.08987",
    "authors": [
      "Shiqian Li",
      "Ruihong Shen",
      "Yaoyu Tao",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.05600",
    "title": "Progressively Deformable 2D Gaussian Splatting for Video Representation at Arbitrary Resolutions",
    "abstract": "           Implicit neural representations (INRs) enable fast video compression and effective video processing, but a single model rarely offers scalable decoding across rates and resolutions. In practice, multi-resolution typically relies on retraining or multi-branch designs, and structured pruning failed to provide a permutation-invariant progressive transmission order. Motivated by the explicit structure and efficiency of Gaussian splatting, we propose D2GV-AR, a deformable 2D Gaussian video representation that enables \\emph{arbitrary-scale} rendering and \\emph{any-ratio} progressive coding within a single model. We partition each video into fixed-length Groups of Pictures and represent each group with a canonical set of 2D Gaussian primitives, whose temporal evolution is modeled by a neural ordinary differential equation. During training and rendering, we apply scale-aware grouping according to Nyquist sampling theorem to form a nested hierarchy across resolutions. Once trained, primitives can be pruned via a D-optimal subset objective to enable any-ratio progressive coding. Extensive experiments show that D2GV-AR renders at over 250 FPS while matching or surpassing recent INR baselines, enabling multiscale continuous rate--quality adaptation.         ",
    "url": "https://arxiv.org/abs/2503.05600",
    "authors": [
      "Mufan Liu",
      "Qi Yang",
      "Miaoran Zhao",
      "He Huang",
      "Le Yang",
      "Zhu Li",
      "Yiling Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.02194",
    "title": "FairDAG: Consensus Fairness over Multi-Proposer Causal Design",
    "abstract": "           The rise of cryptocurrencies like Bitcoin and Ethereum has driven interest in blockchain database technology, with smart contracts enabling the growth of decentralized finance (DeFi). However, research has shown that adversaries exploit transaction ordering to extract profits through attacks like front-running, sandwich attacks, and liquidation manipulation. This issue affects blockchains where block proposers have full control over transaction ordering. To address this, a more fair transaction ordering mechanism is essential. Existing fairness protocols, such as Pompe and Themis, operate on leader-based consensus protocols, which not only suffer from low throughput caused by the single-leader bottleneck, but also allow adversarial block proposers to manipulate transaction ordering. To address these limitations, we propose a new framework, FairDAG, that runs fairness protocols on top of DAG-based consensus protocols. FairDAG improves protocol performance in both throughput and fairness quality by leveraging the multi-proposer design and validity property of DAG-based consensus protocols. We conducted a comprehensive analytical and experimental evaluation of two FairDAG variants - FairDAG-AB and FairDAG-RL. Our results demonstrate that FairDAG outperforms prior fairness protocols in both throughput and fairness quality.         ",
    "url": "https://arxiv.org/abs/2504.02194",
    "authors": [
      "Dakai Kang",
      "Junchao Chen",
      "Tien Tuan Anh Dinh",
      "Mohammad Sadoghi"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2504.06160",
    "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
    "abstract": "           Large Language Models (LLMs) have been shown to demonstrate imbalanced biases against certain groups. However, the study of unprovoked targeted attacks by LLMs towards at-risk populations remains underexplored. Our paper presents three novel contributions: (1) the explicit evaluation of LLM-generated attacks on highly vulnerable mental health groups; (2) a network-based framework to study the propagation of relative biases; and (3) an assessment of the relative degree of stigmatization that emerges from these attacks. Our analysis of a recently released large-scale bias audit dataset reveals that mental health entities occupy central positions within attack narrative networks, as revealed by a significantly higher mean centrality of closeness (p-value = 4.06e-10) and dense clustering (Gini coefficient = 0.7). Drawing from an established stigmatization framework, our analysis indicates increased labeling components for mental health disorder-related targets relative to initial targets in generation chains. Taken together, these insights shed light on the structural predilections of large language models to heighten harmful discourse and highlight the need for suitable approaches for mitigation.         ",
    "url": "https://arxiv.org/abs/2504.06160",
    "authors": [
      "Rijul Magu",
      "Arka Dutta",
      "Sean Kim",
      "Ashiqur R. KhudaBukhsh",
      "Munmun De Choudhury"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2504.12588",
    "title": "Plain Transformers Can be Powerful Graph Learners",
    "abstract": "           Transformers have attained outstanding performance across various modalities, owing to their simple but powerful scaled-dot-product (SDP) attention mechanisms. Researchers have attempted to migrate Transformers to graph learning, but most advanced Graph Transformers (GTs) have strayed far from plain Transformers, exhibiting major architectural differences either by integrating message-passing or incorporating sophisticated attention mechanisms. These divergences hinder the easy adoption of training advances for Transformers developed in other domains. Contrary to previous GTs, this work demonstrates that the plain Transformer architecture can be a powerful graph learner. To achieve this, we propose to incorporate three simple, minimal, and easy-to-implement modifications to the plain Transformer architecture to construct our Powerful Plain Graph Transformers (PPGT): (1) simplified $L_2$ attention for measuring the magnitude closeness among tokens; (2) adaptive root-mean-square normalization to preserve token magnitude information; and (3) a simple MLP-based stem for graph positional encoding. Consistent with its theoretical expressivity, PPGT demonstrates noteworthy realized expressivity on the empirical graph expressivity benchmark, comparing favorably to more complicated alternatives such as subgraph GNNs and higher-order GNNs. Its empirical performance across various graph datasets also justifies the effectiveness of PPGT. This finding underscores the versatility of plain Transformer architectures and highlights their strong potential as a unified backbone for multimodal learning across language, vision, and graph domains.         ",
    "url": "https://arxiv.org/abs/2504.12588",
    "authors": [
      "Liheng Ma",
      "Soumyasundar Pal",
      "Yingxue Zhang",
      "Philip H.S. Torr",
      "Mark Coates"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.14250",
    "title": "Towards Anomaly-Aware Pre-Training and Fine-Tuning for Graph Anomaly Detection",
    "abstract": "           Graph anomaly detection (GAD) has garnered increasing attention in recent years, yet remains challenging due to two key factors: (1) label scarcity stemming from the high cost of annotations and (2) homophily disparity at node and class levels. In this paper, we introduce Anomaly-Aware Pre-Training and Fine-Tuning (APF), a targeted and effective framework to mitigate the above challenges in GAD. In the pre-training stage, APF incorporates node-specific subgraphs selected via the Rayleigh Quotient, a label-free anomaly metric, into the learning objective to enhance anomaly awareness. It further introduces two learnable spectral polynomial filters to jointly learn dual representations that capture both general semantics and subtle anomaly cues. During fine-tuning, a gated fusion mechanism adaptively integrates pre-trained representations across nodes and dimensions, while an anomaly-aware regularization loss encourages abnormal nodes to preserve more anomaly-relevant information. Furthermore, we theoretically show that APF tends to achieve linear separability under mild conditions. Comprehensive experiments on 10 benchmark datasets validate the superior performance of APF in comparison to state-of-the-art baselines.         ",
    "url": "https://arxiv.org/abs/2504.14250",
    "authors": [
      "Yunhui Liu",
      "Jiashun Cheng",
      "Yiqing Lin",
      "Qizhuo Xie",
      "Jia Li",
      "Fugee Tsung",
      "Hongzhi Yin",
      "Tao Zheng",
      "Jianhua Zhao",
      "Tieke He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.03797",
    "title": "Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks",
    "abstract": "           Partial Bayesian neural networks (pBNNs) have been shown to perform competitively with fully Bayesian neural networks while only having a subset of the parameters be stochastic. Using sequential Monte Carlo (SMC) samplers as the inference method for pBNNs gives a non-parametric probabilistic estimation of the stochastic parameters, and has shown improved performance over parametric methods. In this paper we introduce a new SMC-based training method for pBNNs by utilising a guided proposal and incorporating gradient-based Markov kernels, which gives us better scalability on high dimensional problems. We show that our new method outperforms the state-of-the-art in terms of predictive performance and optimal loss. We also show that pBNNs scale well with larger batch sizes, resulting in significantly reduced training times and often better performance.         ",
    "url": "https://arxiv.org/abs/2505.03797",
    "authors": [
      "Andrew Millard",
      "Joshua Murphy",
      "Simon Maskell",
      "Zheng Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.11766",
    "title": "Redefining Neural Operators in $d+1$ Dimensions for Embedding Evolution",
    "abstract": "           Neural Operators (NOs) have emerged as powerful tools for learning mappings between function spaces. Among them, the kernel integral operator has been widely used in universally approximating architectures. Following the original formulation, most advancements focus on designing better parameterizations for the kernel over the original physical domain (with $d$ spatial dimensions, $d\\in{1,2,3,\\ldots}$). In contrast, embedding evolution remains largely unexplored, which often drives models toward brute-force embedding lengthening to improve approximation, but at the cost of substantially increased computation. In this paper, we introduce an auxiliary dimension that explicitly models embedding evolution in operator form, thereby redefining the NO framework in $d+1$ dimensions (the original $d$ dimensions plus one auxiliary dimension). Under this formulation, we develop a Schr\u00f6dingerised Kernel Neural Operator (SKNO), which leverages Fourier-based operators to model the $d+1$ dimensional evolution. Across more than ten increasingly challenging benchmarks, ranging from the 1D heat equation to the highly nonlinear 3D Rayleigh-Taylor instability, SKNO consistently outperforms other baselines. We further validate its resolution invariance under mixed-resolution training and super-resolution inference, and evaluate zero-shot generalization to unseen temporal regimes. In addition, we present a broader set of design choices for the lifting and recovery operators, demonstrating their impact on SKNO's predictive performance.         ",
    "url": "https://arxiv.org/abs/2505.11766",
    "authors": [
      "Haoze Song",
      "Zhihao Li",
      "Xiaobo Zhang",
      "Zecheng Gan",
      "Zhilu Lai",
      "Wei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2505.12129",
    "title": "Metric Graph Kernels via the Tropical Torelli Map",
    "abstract": "           We introduce the first graph kernels for metric graphs via tropical algebraic geometry. In contrast to conventional graph kernels based on graph combinatorics such as nodes, edges, and subgraphs, our metric graph kernels are purely based on the geometry and topology of the underlying metric space. A key characterizing property of our construction is its invariance under edge subdivision, making the kernels intrinsically well-suited for comparing graphs representing different underlying metric spaces. We develop efficient algorithms to compute our kernels and analyze their complexity, which depends primarily on the genus of the input graphs rather than their size. Through experiments on synthetic data and selected real-world datasets, we demonstrate that our kernels capture complementary geometric and topological information overseen by standard combinatorial approaches, particularly in label-free settings. We further showcase their practical utility with an urban road network classification task.         ",
    "url": "https://arxiv.org/abs/2505.12129",
    "authors": [
      "Yueqi Cao",
      "Anthea Monod"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.13140",
    "title": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow",
    "abstract": "           Many density estimation techniques for 3D human motion prediction require a significant amount of inference time, often exceeding the duration of the predicted time horizon. To address the need for faster density estimation for 3D human motion prediction, we introduce a novel flow-based method for human motion prediction called CacheFlow. Unlike previous conditional generative models that suffer from poor time efficiency, CacheFlow takes advantage of an unconditional flow-based generative model that transforms a Gaussian mixture into the density of future motions. The results of the computation of the flow-based generative model can be precomputed and cached. Then, for conditional prediction, we seek a mapping from historical trajectories to samples in the Gaussian mixture. This mapping can be done by a much more lightweight model, thus saving significant computation overhead compared to a typical conditional flow model. In such a two-stage fashion and by caching results from the slow flow model computation, we build our CacheFlow without loss of prediction accuracy and model expressiveness. This inference process is completed in approximately one millisecond, making it 4 times faster than previous VAE methods and 30 times faster than previous diffusion-based methods on standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our method demonstrates improved density estimation accuracy and comparable prediction accuracy to a SOTA method on Human3.6M. Our code and models are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.13140",
    "authors": [
      "Takahiro Maeda",
      "Jinkun Cao",
      "Norimichi Ukita",
      "Kris Kitani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.15255",
    "title": "Boosting Large Language Models for Mental Manipulation Detection via Data Augmentation and Distillation",
    "abstract": "           Mental manipulation on social media poses a covert yet serious threat to individuals' psychological well-being and the integrity of online interactions. Detecting such behavior is challenging due to the difficult-to-annotate training data, its highly covert and multi-turn nature, and the lack of real-world datasets. To address these challenges, we propose MentalMAD, a framework that enhances large language models for mental manipulation detection. Our approach consists of three key components: EvoSA, an annotation-free data augmentation method that combines evolutionary operations with speech-act-aware prompting; teacher-model-generated complementary-task supervision; and Complementary-Convergent Distillation, a phase-wise strategy for transferring manipulation-specific knowledge to student models. We then constructed the ReaMent dataset, comprising 5,000 real-world-sourced dialogues. Extensive experiments show that MentalMAD improves accuracy by 14.0%, macro-F1 by 27.3%, and weighted F1 by 15.1% over the strongest baseline. The code and the dataset are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.15255",
    "authors": [
      "Yuansheng Gao",
      "Peng Gao",
      "Han Bao",
      "Bin Li",
      "Jixiang Luo",
      "Zonghui Wang",
      "Wenzhi Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.20623",
    "title": "Constructing Algorithmic Authority: How Multi-Channel Networks (MCNs) Govern Live-Streaming Labor in China",
    "abstract": "           This study examines the discursive construction of algorithms and its role in labor management in Chinese live-streaming industry by focusing on how intermediary organizations (Multi-Channel Networks, MCNs) actively construct, stabilize, and deploy particular interpretations of platform algorithms as instruments of labor management. Drawing on a nine-month ethnographic fieldwork and 44 interviews with live-streamers, former live-streamers, and MCN staff, we examine how MCNs produce and circulate structured interpretations of platform algorithms across organizational settings. We show that MCNs articulate two asymmetric yet interconnected forms of algorithmic interpretations. Internally, MCNs managers approach algorithms as volatile and uncertain systems and adopt probabilistic strategies to manage performance and risk. Externally, in interactions with streamers, MCNs circulate simplified and prescriptive algorithmic narratives that frame platform systems as transparent, fair, and responsive to individual effort. These organizationally produced algorithmic interpretations are embedded into training materials, live-streaming performance metrics, and everyday management practices. Through these mechanisms, streamers internalize responsibility for outcomes, intensify self-discipline, and increase investments in equipment, performing skills, and routines to maintain streamer-audience relationship, while accountability for unpredictable outcomes is increasingly shifted away from managers and platforms. This study contributes to CSCW and platform labor research by demonstrating how discursively constructed algorithmic knowledge can function as an intermediary infrastructure of soft control, shaping how platform labor is regulated, moralized, and governed in practice.         ",
    "url": "https://arxiv.org/abs/2505.20623",
    "authors": [
      "Qing Xiao",
      "Rongyi Chen",
      "Jingjia Xiao",
      "Tianyang Fu",
      "Alice Qian Zhang",
      "Xianzhe Fan",
      "Bingbing Zhang",
      "Zhicong Lu",
      "Hong Shen"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2505.20955",
    "title": "Enhancing Membership Inference Attacks on Diffusion Models from a Frequency-Domain Perspective",
    "abstract": "           Diffusion models have achieved tremendous success in image generation, but they also raise significant concerns regarding privacy and copyright issues. Membership Inference Attacks (MIAs) are designed to ascertain whether specific data were utilized during a model's training phase. As current MIAs for diffusion models typically exploit the model's image prediction ability, we formalize them into a unified general paradigm which computes the membership score for membership identification. Under this paradigm, we empirically find that existing attacks overlook the inherent deficiency in how diffusion models process high-frequency information. Consequently, this deficiency leads to member data with more high-frequency content being misclassified as hold-out data, and hold-out data with less high-frequency content tend to be misclassified as member data. Moreover, we theoretically demonstrate that this deficiency reduces the membership advantage of attacks, thereby interfering with the effective discrimination of member data and hold-out data. Based on this insight, we propose a plug-and-play high-frequency filter module to mitigate the adverse effects of the deficiency, which can be seamlessly integrated into any attacks within this general paradigm without additional time costs. Extensive experiments corroborate that this module significantly improves the performance of baseline attacks across different datasets and models.         ",
    "url": "https://arxiv.org/abs/2505.20955",
    "authors": [
      "Puwei Lian",
      "Yujun Cai",
      "Songze Li",
      "Bingkun Bao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.01042",
    "title": "Probing Neural Topology of Large Language Models",
    "abstract": "           Probing large language models (LLMs) has yielded valuable insights into their internal mechanisms by linking neural activations to interpretable semantics. However, the complex mechanisms that link neuron's functional co-activation with the emergent model capabilities remains largely unknown, hindering a deeper understanding and safer development of LLMs. In this work, we introduce graph probing, a method for uncovering the functional connectivity of LLM neurons and relating it to language generation performance. By probing models across diverse LLM families and scales, we discover a universal predictability of language generation and understanding performance using only neural topology, which persists even when retaining just 1% of neuron connections. Strikingly, probing on topology outperforms probing on activation by up to 130.4% and 67.7% on perplexity and space/time semantic regression respectively, suggesting that neural topology contains orders of richer information of LLM performance than neural activation, which can be easily extracted with simple linear or MLP probes. To explain the dependence between neural topology and language performance, we identify default networks and hub neurons in LLMs and provide causal evidence by interventional experiments on multiple benchmarks, showing that LLMs actually exploit these topological information. Further analyses suggest that graph probing can be effectively leveraged to improve the efficiency and reliability of LLMs through proof-of-concept applications in model pruning and hallucination detection. Codes and data for the graph probing toolbox are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.01042",
    "authors": [
      "Yu Zheng",
      "Yuan Yuan",
      "Yue Zhuo",
      "Yong Li",
      "Gabriel Kreiman",
      "Tomaso Poggio",
      "Paolo Santi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.09277",
    "title": "NeuroFaith: Evaluating LLM Self-Explanation Faithfulness via Internal Representation Alignment",
    "abstract": "           Large Language Models (LLMs) can generate plausible free text self-explanations to justify their answers. However, these natural language explanations may not accurately reflect the model's actual reasoning process, pinpointing a lack of faithfulness. Existing faithfulness evaluation methods rely primarily on behavioral tests or computational block analysis without examining the semantic content of internal neural representations. This paper proposes NeuroFaith, a flexible framework that measures the faithfulness of LLM free text self-explanation by identifying key concepts within explanations and mechanistically testing whether these concepts actually influence the model's predictions. We show the versatility of NeuroFaith across 2-hop reasoning and classification tasks. Additionally, we develop a linear faithfulness probe based on NeuroFaith to detect unfaithful self-explanations from representation space and improve faithfulness through steering. NeuroFaith provides a principled approach to evaluating and enhancing the faithfulness of LLM free text self-explanations, addressing critical needs for trustworthy AI systems.         ",
    "url": "https://arxiv.org/abs/2506.09277",
    "authors": [
      "Milan Bhan",
      "Jean-Noel Vittaut",
      "Nicolas Chesneau",
      "Sarath Chandar",
      "Marie-Jeanne Lesot"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.03779",
    "title": "FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed",
    "abstract": "           Large-scale vision foundation models such as DINOv2 boast impressive performances by leveraging massive architectures and training datasets. But numerous scenarios require practitioners to reproduce those pre-training solutions, such as on private data, new modalities, or simply for scientific questioning--which is currently extremely demanding computation-wise. We thus propose a novel pre-training strategy for DINOv2 that simultaneously accelerates convergence--and strengthens robustness to common corruptions as a by-product. Our approach involves a frequency filtering curriculum--low-frequency being seen first--and the Gaussian noise patching augmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while pre-training time and FLOPs are reduced by 1.6x and 2.25x, our method still achieves matching robustness in corruption benchmarks (ImageNet-C) and maintains competitive linear probing performance compared with baseline. This dual benefit of efficiency and robustness makes large-scale self-supervised foundation modeling more attainable, while opening the door to novel exploration around data curriculum and augmentation as means to improve self-supervised learning models robustness. The code is available at this https URL ",
    "url": "https://arxiv.org/abs/2507.03779",
    "authors": [
      "Jiaqi Zhang",
      "Juntuo Wang",
      "Zhixin Sun",
      "John Zou",
      "Randall Balestriero"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.04490",
    "title": "Dealing with Uncertainty in Contextual Anomaly Detection",
    "abstract": "           Contextual anomaly detection (CAD) aims to identify anomalies in a target (behavioral) variable conditioned on a set of contextual variables that influence the normalcy of the target variable but are not themselves indicators of anomaly. In many anomaly detection tasks, there exist contextual variables that influence the normalcy of the target variable but are not themselves indicators of anomaly. In this work, we propose a novel framework for CAD, normalcy score (NS), that explicitly models both the aleatoric and epistemic uncertainties. Built on heteroscedastic Gaussian process regression, our method regards the Z-score as a random variable, providing confidence intervals that reflect the reliability of the anomaly assessment. Through experiments on benchmark datasets and a real-world application in cardiology, we demonstrate that NS outperforms state-of-the-art CAD methods in both detection accuracy and interpretability. Moreover, confidence intervals enable an adaptive, uncertainty-driven decision-making process, which may be very important in domains such as healthcare.         ",
    "url": "https://arxiv.org/abs/2507.04490",
    "authors": [
      "Luca Bindini",
      "Lorenzo Perini",
      "Stefano Nistri",
      "Jesse Davis",
      "Paolo Frasconi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2507.06252",
    "title": "False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems",
    "abstract": "           Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach that operates in the early phases of the cyber threat lifecycle. CTI involves collecting, processing, and analyzing threat data to provide a more accurate and rapid understanding of cyber threats. Due to the large volume of data, automation through Machine Learning (ML) and Natural Language Processing (NLP) models is essential for effective CTI extraction. These automated systems leverage Open Source Intelligence (OSINT) from sources like social networks, forums, and blogs to identify Indicators of Compromise (IoCs). Although prior research has focused on adversarial attacks on specific ML models, this study expands the scope by investigating vulnerabilities within various components of the entire CTI pipeline and their susceptibility to adversarial attacks. These vulnerabilities arise because they ingest textual inputs from various open sources, including real and potentially fake content. We analyse three types of attacks against CTI pipelines, including evasion, flooding, and poisoning, and assess their impact on the system's information selection capabilities. Specifically, on fake text generation, the work demonstrates how adversarial text generation techniques can create fake cybersecurity and cybersecurity-like text that misleads classifiers, degrades performance, and disrupts system functionality. The focus is primarily on the evasion attack, as it precedes and enables flooding and poisoning attacks within the CTI pipeline.         ",
    "url": "https://arxiv.org/abs/2507.06252",
    "authors": [
      "Samaneh Shafee",
      "Alysson Bessani",
      "Pedro M. Ferreira"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.14516",
    "title": "SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning",
    "abstract": "           We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware metric function for time series self-supervised representation learning. Most Self-Supervised Learning (SSL) methods for signals commonly adopt distance-based objectives such as mean squared error (MSE), which are sensitive to amplitude, invariant to waveform polarity, and unbounded in scale. These properties hinder semantic alignment and reduce interpretability. SDSC addresses this by quantifying structural agreement between temporal signals based on the intersection of signed amplitudes, derived from the Dice Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware metric, it can be used as a loss by subtracting from 1 and applying a differentiable approximation of the Heaviside function for gradient-based optimization. A hybrid loss formulation is also proposed to combine SDSC with MSE, improving stability and preserving amplitude where necessary. Experiments on forecasting and classification benchmarks demonstrate that SDSC-based pre-training achieves comparable or improved performance over MSE, particularly in in-domain and low-resource scenarios. The results suggest that structural fidelity in signal representations enhances the semantic representation quality, supporting the consideration of structure-aware metrics as viable alternatives to conventional distance-based methods.         ",
    "url": "https://arxiv.org/abs/2507.14516",
    "authors": [
      "Jeyoung Lee",
      "Hochul Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2508.07452",
    "title": "Stackelberg Coupling of Online Representation Learning and Reinforcement Learning",
    "abstract": "           Deep Q-learning jointly learns representations and values within monolithic networks, promising beneficial co-adaptation between features and value estimates. Although this architecture has attained substantial success, the coupling between representation and value learning creates instability as representations must constantly adapt to non-stationary value targets, while value estimates depend on these shifting representations. This is compounded by high variance in bootstrapped targets, which causes bias in value estimation in off-policy methods. We introduce Stackelberg Coupled Representation and Reinforcement Learning (SCORER), a framework for value-based RL that views representation and Q-learning as two strategic agents in a hierarchical game. SCORER models the Q-function as the leader, which commits to its strategy by updating less frequently, while the perception network (encoder) acts as the follower, adapting more frequently to learn representations that minimize Bellman error variance given the leader's committed strategy. Through this division of labor, the Q-function minimizes MSBE while perception minimizes its variance, thereby reducing bias accordingly, with asymmetric updates allowing stable co-adaptation, unlike simultaneous parameter updates in monolithic solutions. Our proposed SCORER framework leads to a bi-level optimization problem whose solution is approximated by a two-timescale algorithm that creates an asymmetric learning dynamic between the two players. Extensive experiments on DQN and its variants demonstrate that gains stem from algorithmic insight rather than model complexity.         ",
    "url": "https://arxiv.org/abs/2508.07452",
    "authors": [
      "Fernando Martinez",
      "Tao Li",
      "Yingdong Lu",
      "Juntao Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.09925",
    "title": "Residual Reservoir Memory Networks",
    "abstract": "           We introduce a novel class of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear reservoir, where the latter is based on residual orthogonal connections along the temporal dimension for enhanced long-term propagation of the input. The resulting reservoir state dynamics are studied through the lens of linear stability analysis, and we investigate diverse configurations for the temporal residual connections. The proposed approach is empirically assessed on time-series and pixel-level 1-D classification tasks. Our experimental results highlight the advantages of the proposed approach over other conventional RC models.         ",
    "url": "https://arxiv.org/abs/2508.09925",
    "authors": [
      "Matteo Pinna",
      "Andrea Ceni",
      "Claudio Gallicchio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.11667",
    "title": "Guided Perturbation Sensitivity (GPS): Detecting Adversarial Text via Embedding Stability and Word Importance",
    "abstract": "           Adversarial text attacks remain a persistent threat to transformer models, yet existing defenses are typically attack-specific or require costly model retraining, leaving a gap for attack-agnostic detection. We introduce Guided Perturbation Sensitivity (GPS), a detection framework that identifies adversarial examples by measuring how embedding representations change when important words are masked. GPS first ranks words using importance heuristics, then measures embedding sensitivity to masking top-k critical words, and processes the resulting patterns with a BiLSTM detector. Experiments show that adversarially perturbed words exhibit disproportionately high masking sensitivity compared to naturally important words. Across three datasets, three attack types, and two victim models, GPS achieves over 85% detection accuracy and demonstrates competitive performance compared to existing state-of-the-art methods, often at lower computational cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure perturbation identification quality, we demonstrate that gradient-based ranking significantly outperforms attention, hybrid, and random selection approaches, with identification quality strongly correlating with detection performance for word-level attacks ($\\rho = 0.65$). GPS generalizes to unseen datasets, attacks, and models without retraining, providing a practical solution for adversarial text detection.         ",
    "url": "https://arxiv.org/abs/2508.11667",
    "authors": [
      "Bryan E. Tuck",
      "Rakesh M. Verma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.13187",
    "title": "\"Not in My Backyard\": LLMs Uncover Online and Offline Social Biases Against Homelessness",
    "abstract": "           Homelessness is a persistent social challenge, impacting millions worldwide. Over 876,000 people experienced homelessness (PEH) in the U.S. in 2025. Social bias is a significant barrier to alleviation, shaping public perception and influencing policymaking. Given that online textual media and offline city council discourse reflect and influence part of public opinion, it provides valuable insights to identify and track social biases against PEH. We present a new, manually-annotated multi-domain dataset compiled from Reddit, X (formerly Twitter), news articles, and city council meeting minutes across ten U.S. cities. Our 16-category multi-label taxonomy creates a challenging long-tail classification problem: some categories appear in less than 1% of samples, while others exceed 70%. We find that small human-annotated datasets (1,702 samples) are insufficient for training effective classifiers, whether used to fine-tune encoder models or as few-shot examples for LLMs. To address this, we use GPT-4.1 to generate pseudo-labels on a larger unlabeled corpus. Training on this expanded dataset enables even small encoder models (ModernBERT, 150M parameters) to achieve 35.23 macro-F1, approaching GPT-4.1's 41.57. This demonstrates that \\textbf{data quantity matters more than model size}, enabling low-cost, privacy-preserving deployment without relying on commercial APIs. Our results reveal that negative bias against PEH is prevalent both offline and online (especially on Reddit), with \"not in my backyard\" narratives showing the highest engagement. These findings uncover a type of ostracism that directly impacts poverty-reduction policymaking and provide actionable insights for practitioners addressing homelessness.         ",
    "url": "https://arxiv.org/abs/2508.13187",
    "authors": [
      "Jonathan A. Karr Jr.",
      "Benjamin F. Herbst",
      "Matthew L. Sisk",
      "Xueyun Li",
      "Ting Hua",
      "Matthew Hauenstein",
      "Georgina Curto",
      "Nitesh V. Chawla"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.17557",
    "title": "The Price of Uncertainty for Social Consensus",
    "abstract": "           How hard is it to achieve consensus in a social network under uncertainty? In this paper we model this problem as a social graph of agents where each vertex is initially colored red or blue. The goal of the agents is to achieve consensus, which is when the colors of all agents align. Agents attempt to do this locally through steps in which an agent changes their color to the color of the majority of their neighbors. In real life, agents may not know exactly how many of their neighbors are red or blue, which introduces uncertainty into this process. Modeling uncertainty as perturbations of relative magnitude $1+\\varepsilon$ to these color neighbor counts, we show that even small values of $\\varepsilon$ greatly hinder the ability to achieve consensus in a social network. We prove theoretically tight upper and lower bounds on the \\emph{price of uncertainty}, a metric defined in previous work by Balcan et al. to quantify the effect of uncertainty in network games.         ",
    "url": "https://arxiv.org/abs/2508.17557",
    "authors": [
      "Yunzhe Bai",
      "Alec Sun"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.19325",
    "title": "PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI",
    "abstract": "           Accurate prediction of major adverse cardiac events (MACE) remains a central challenge in cardiovascular prognosis. We present PRISM (Prompt-guided Representation Integration for Survival Modeling), a self-supervised framework that integrates visual representations from non-contrast cardiac cine magnetic resonance imaging with structured electronic health records (EHRs) for survival analysis. PRISM extracts temporally synchronized imaging features through motion-aware multi-view distillation and modulates them using medically informed textual prompts to enable fine-grained risk prediction. Across four independent clinical cohorts, PRISM consistently surpasses classical survival prediction models and state-of-the-art (SOTA) deep learning baselines under internal and external validation. Further clinical findings demonstrate that the combined imaging and EHR representations derived from PRISM provide valuable insights into cardiac risk across diverse cohorts. Three distinct imaging signatures associated with elevated MACE risk are uncovered, including lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior elevated focus during diastole. Prompt-guided attribution further identifies hypertension, diabetes, and smoking as dominant contributors among clinical and physiological EHR factors.         ",
    "url": "https://arxiv.org/abs/2508.19325",
    "authors": [
      "Haoyang Su",
      "Jin-Yi Xiang",
      "Shaohao Rui",
      "Yifan Gao",
      "Xingyu Chen",
      "Tingxuan Yin",
      "Shaoting Zhang",
      "Xiaosong Wang",
      "Lian-Ming Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.21172",
    "title": "Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks",
    "abstract": "           Echo State Networks (ESNs) are a particular type of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular for their fast and efficient learning. However, traditional ESNs often struggle with long-term information processing. In this paper, we introduce a novel class of deep untrained RNNs based on temporal residual connections, called Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a hierarchy of untrained residual recurrent layers significantly boosts memory capacity and long-term temporal modeling. For the temporal residual connections, we consider different orthogonal configurations, including randomly generated and fixed-structure configurations, and we study their effect on network dynamics. A thorough mathematical analysis outlines necessary and sufficient conditions to ensure stable dynamics within DeepResESN. Our experiments on a variety of time series tasks showcase the advantages of the proposed approach over traditional shallow and deep RC.         ",
    "url": "https://arxiv.org/abs/2508.21172",
    "authors": [
      "Matteo Pinna",
      "Andrea Ceni",
      "Claudio Gallicchio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.04154",
    "title": "Robust Filter Attention: Self-Attention as a Parallel State Estimator",
    "abstract": "           We introduce Robust Filter Attention (RFA), an attention mechanism that reformulates self-attention as parallel robust filtering under a latent stochastic differential equation (SDE) prior, where analytically propagated uncertainty defines a time-dependent precision prior over attention weights. This formulation integrates key advantages of existing positional encodings: it preserves RoPE-style rotational structure while achieving long-context stability through explicit modeling of dissipation and diffusion. By imposing isotropic constraints on the dynamics and noise, RFA matches the $O(N^2 d)$ time and $O(N^2 + Nd)$ memory complexity of standard attention. Empirically, we find that uncertainty-aware weighting induces specialization into distinct filtering regimes across heads, improving temporal consistency and extrapolation across varying context lengths.         ",
    "url": "https://arxiv.org/abs/2509.04154",
    "authors": [
      "Peter Racioppo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.17816",
    "title": "Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training",
    "abstract": "           Self-supervised learning (SSL) has emerged as a central paradigm for training foundation models by leveraging large-scale unlabeled datasets, often producing representations with strong generalization capabilities. These models are typically pre-trained on general-purpose datasets such as ImageNet and subsequently adapted to various downstream tasks through finetuning. While prior work has investigated parameter-efficient adaptation methods like adapters, LoRA, and prompt tuning, primarily targeting downstream finetuning, extending the SSL pre-training itself in a continual manner to new domains under limited data remains largely underexplored, especially for downstream dense prediction tasks like semantic segmentation. In this work, we address the challenge of adapting vision foundation models to low-data target domains through continual self-supervised pre-training, specifically targeting downstream semantic segmentation. We propose GLARE (Global Local and Regional Enforcement), a novel continual self-supervised pre-training task designed to enhance downstream semantic segmentation performance. GLARE introduces patch-level augmentations to encourage local consistency and incorporates a regional consistency constraint that leverages spatial semantics in the data. For efficient continual pre-training, we initialize Vision Transformers (ViTs) with weights from existing SSL models and update only lightweight adapter modules specifically UniAdapter - while keeping the rest of the backbone frozen. Experiments across multiple semantic segmentation benchmarks on different domains demonstrate that GLARE consistently improves downstream performance with minimal computational and parameter overhead.         ",
    "url": "https://arxiv.org/abs/2509.17816",
    "authors": [
      "Brown Ebouky",
      "Ajad Chhatkuli",
      "Cristiano Malossi",
      "Christoph Studer",
      "Roy Assaf",
      "Andrea Bartezzaghi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.18546",
    "title": "SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models",
    "abstract": "           No-Reference Image Quality Assessment (NR-IQA) models play an important role in various real-world applications. Recently, adversarial attacks against NR-IQA models have attracted increasing attention, as they provide valuable insights for revealing model vulnerabilities and guiding robust system design. Some effective attacks have been proposed against NR-IQA models in white-box settings, where the attacker has full access to the target model. However, these attacks often suffer from poor transferability to unknown target models in more realistic black-box scenarios, where the target model is inaccessible. This work makes the first attempt to address the challenge of low transferability in attacking NR-IQA models by proposing a transferable Signed Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the gradient of the target model by applying Gaussian smoothing to source models and ensembling their smoothed gradients. To ensure the imperceptibility of adversarial perturbations, SEGA further removes inappropriate perturbations using a specially designed perturbation filter mask. Experimental results on the CLIVE dataset demonstrate the superior transferability of SEGA, validating its effectiveness in enabling successful transfer-based black-box attacks against NR-IQA models.         ",
    "url": "https://arxiv.org/abs/2509.18546",
    "authors": [
      "Yujia Liu",
      "Dingquan Li",
      "Zhixuan Li",
      "Tiejun Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.18930",
    "title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning",
    "abstract": "           Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks to execute classic algorithms by supervised learning. Despite its successes, important limitations remain: inability to construct valid solutions without post-processing and to reason about multiple correct ones, poor performance on combinatorial NP-hard problems, and inapplicability to problems for which strong algorithms are not yet known. To address these limitations, we reframe the problem of learning algorithm trajectories as a Markov Decision Process, which imposes structure on the solution construction procedure and unlocks the powerful tools of imitation and reinforcement learning (RL). We propose the GNARL framework, encompassing the methodology to translate problem formulations from NAR to RL and a learning architecture suitable for a wide range of graph-based problems. We achieve very high graph accuracy results on several CLRS-30 problems, performance matching or exceeding much narrower NAR approaches for NP-hard problems and, remarkably, applicability even when lacking an expert algorithm.         ",
    "url": "https://arxiv.org/abs/2509.18930",
    "authors": [
      "Alex Schutz",
      "Victor-Alexandru Darvariu",
      "Efimia Panagiotaki",
      "Bruno Lacerda",
      "Nick Hawes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.22161",
    "title": "Pushing Toward the Simplex Vertices: A Simple Remedy for Code Collapse in Smoothed Vector Quantization",
    "abstract": "           Vector quantization, which discretizes a continuous vector space into a finite set of representative vectors (a codebook), has been widely adopted in modern machine learning. Despite its effectiveness, vector quantization poses a fundamental challenge: the non-differentiable quantization step blocks gradient backpropagation. Smoothed vector quantization addresses this issue by relaxing the hard assignment of a codebook vector into a weighted combination of codebook entries, represented as the matrix product of a simplex vector and the codebook. Effective smoothing requires two properties: (1) smoothed quantizers should remain close to a onehot vector, ensuring tight approximation, and (2) all codebook entries should be utilized, preventing code collapse. Existing methods typically address these desiderata separately. By contrast, the present study introduces a simple and intuitive regularization that promotes both simultaneously by minimizing the distance between each simplex vertex and its $K$-nearest smoothed quantizers. Experiments on representative benchmarks, including discrete image autoencoding and contrastive speech representation learning, demonstrate that the proposed method achieves more reliable codebook utilization and improves performance compared to prior approaches.         ",
    "url": "https://arxiv.org/abs/2509.22161",
    "authors": [
      "Takashi Morita"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.22259",
    "title": "Rotary Position Encodings for Graphs",
    "abstract": "           We study the extent to which rotary position encodings (RoPE), a recent transformer position encoding algorithm broadly adopted in large language models (LLMs) and vision transformers (ViTs), can be applied to graph-structured data. We find that rotating tokens depending on the spectrum of the graph Laplacian efficiently injects structural information into the attention mechanism, boosting performance in synthetic and real-world graph learning tasks. This approach, coined _Wave-Induced Rotary Encodings_ (WIRE), enjoys intriguing theoretical properties: it recovers regular RoPE on grids, and depends asymptotically on the graph effective resistance. Unlike bias-based relative position encodings, WIRE is compatible with linear attention.         ",
    "url": "https://arxiv.org/abs/2509.22259",
    "authors": [
      "Isaac Reid",
      "Arijit Sehanobish",
      "Cederik H\u00f6fs",
      "Bruno Mlodozeniec",
      "Leonhard Vulpius",
      "Federico Barbero",
      "Adrian Weller",
      "Krzysztof Choromanski",
      "Richard E. Turner",
      "Petar Veli\u010dkovi\u0107"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.23019",
    "title": "LLM Watermark Evasion via Bias Inversion",
    "abstract": "           Watermarking offers a promising solution for detecting LLM-generated content, yet its robustness under realistic query-free (black-box) evasion remains an open challenge. Existing query-free attacks often achieve limited success or severely distort semantic meaning. We bridge this gap by theoretically analyzing rewriting-based evasion, demonstrating that reducing the average conditional probability of sampling green tokens by a small margin causes the detection probability to decay exponentially. Guided by this insight, we propose the Bias-Inversion Rewriting Attack (BIRA), a practical query-free method that applies a negative logit bias to a proxy suppression set identified via token surprisal. Empirically, BIRA achieves state-of-the-art evasion rates (>99%) across diverse watermarking schemes while preserving semantic fidelity substantially better than prior baselines. Our findings reveal a fundamental vulnerability in current watermarking methods and highlight the need for rigorous stress tests.         ",
    "url": "https://arxiv.org/abs/2509.23019",
    "authors": [
      "Jeongyeon Hwang",
      "Sangdon Park",
      "Jungseul Ok"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.24472",
    "title": "FS-KAN: Permutation Equivariant Kolmogorov-Arnold Networks via Function Sharing",
    "abstract": "           Permutation equivariant neural networks employing parameter-sharing schemes have emerged as powerful models for leveraging a wide range of data symmetries, significantly enhancing the generalization and computational efficiency of the resulting models. Recently, Kolmogorov-Arnold Networks (KANs) have demonstrated promise through their improved interpretability and expressivity compared to traditional architectures based on MLPs. While equivariant KANs have been explored in recent literature for a few specific data types, a principled framework for applying them to data with permutation symmetries in a general context remains absent. This paper introduces Function Sharing KAN (FS-KAN), a principled approach to constructing equivariant and invariant KA layers for arbitrary permutation symmetry groups, unifying and significantly extending previous work in this domain. We derive the basic construction of these FS-KAN layers by generalizing parameter-sharing schemes to the Kolmogorov-Arnold setup and provide a theoretical analysis demonstrating that FS-KANs have the same expressive power as networks that use standard parameter-sharing layers, allowing us to transfer well-known and important expressivity results from parameter-sharing networks to FS-KANs. Empirical evaluations on multiple data types and symmetry groups show that FS-KANs exhibit superior data efficiency compared to standard parameter-sharing layers, by a wide margin in certain cases, while preserving the interpretability and adaptability of KANs, making them an excellent architecture choice in low-data regimes.         ",
    "url": "https://arxiv.org/abs/2509.24472",
    "authors": [
      "Ran Elbaz",
      "Guy Bar-Shalom",
      "Yam Eitan",
      "Fabrizio Frasca",
      "Haggai Maron"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.24906",
    "title": "Neural network embeddings recover value dimensions from psychometric survey items on par with human data",
    "abstract": "           We demonstrate that embeddings derived from large language models, when processed with \"Survey and Questionnaire Item Embeddings Differentials\" (SQuID), can recover the structure of human values obtained from human rater judgments on the Revised Portrait Value Questionnaire (PVQ-RR). We compare multiple embedding models across a number of evaluation metrics including internal consistency, dimension correlations and multidimensional scaling configurations. Unlike previous approaches, SQuID addresses the challenge of obtaining negative correlations between dimensions without requiring domain-specific fine-tuning or training data re-annotation. Quantitative analysis reveals that our embedding-based approach explains 55% of variance in dimension-dimension similarities compared to human data. Multidimensional scaling configurations show alignment with pooled human data from 49 different countries. Generalizability tests across three personality inventories (IPIP, BFI-2, HEXACO) demonstrate that SQuID consistently increases correlation ranges, suggesting applicability beyond value theory. These results show that semantic embeddings can effectively replicate psychometric structures previously established through extensive human surveys. The approach offers substantial advantages in cost, scalability and flexibility while maintaining comparable quality to traditional methods. Our findings have significant implications for psychometrics and social science research, providing a complementary methodology that could expand the scope of human behavior and experience represented in measurement tools.         ",
    "url": "https://arxiv.org/abs/2509.24906",
    "authors": [
      "Max Pellert",
      "Clemens M. Lechner",
      "Indira Sen",
      "Markus Strohmaier"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2510.02422",
    "title": "Dynamic Target Attack",
    "abstract": "           Existing gradient-based jailbreak attacks typically optimize an adversarial suffix to induce a fixed affirmative response, e.g., ``Sure, here is...''. However, this fixed target usually resides in an extremely low-density region of a safety-aligned LLM's output distribution. Due to the substantial discrepancy between the fixed target and the output distribution, existing attacks require numerous iterations to optimize the adversarial prompt, which might still fail to induce the low-probability target response. To address this limitation, we propose Dynamic Target Attack (DTA), which leverages the target LLM's own responses as adaptive targets. In each optimization round, DTA samples multiple candidates from the output distribution conditioned on the current prompt, and selects the most harmful one as a temporary target for prompt optimization. Extensive experiments demonstrate that, under the white-box setting, DTA achieves over 87% average attack success rate (ASR) within 200 optimization iterations on recent safety-aligned LLMs, exceeding the state-of-the-art baselines by over 15% and reducing wall-clock time by 2-26x. Under the black-box setting, DTA employs a white-box LLM as a surrogate model for gradient-based optimization, achieving an average ASR of 77.5% against black-box models, exceeding prior transfer-based attacks by over 12%.         ",
    "url": "https://arxiv.org/abs/2510.02422",
    "authors": [
      "Kedong Xiu",
      "Churui Zeng",
      "Tianhang Zheng",
      "Xinzhe Huang",
      "Xiaojun Jia",
      "Di Wang",
      "Puning Zhao",
      "Zhan Qin",
      "Kui Ren"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.02999",
    "title": "Untargeted Jailbreak Attack",
    "abstract": "           Existing gradient-based jailbreak attacks on Large Language Models (LLMs) typically optimize adversarial suffixes to align the LLM output with predefined target responses. However, restricting the objective as inducing fixed targets inherently constrains the adversarial search space, limiting the overall attack efficacy. Furthermore, existing methods typically require numerous optimization iterations to fulfill the large gap between the fixed target and the original LLM output, resulting in low attack efficiency. To overcome these limitations, we propose the first gradient-based untargeted jailbreak attack (UJA), which relies on an untargeted objective to maximize the unsafety probability of the LLM output, without enforcing any response patterns. For tractable optimization, we further decompose this objective into two differentiable sub-objectives to search the optimal harmful response and the corresponding adversarial prompt, with a theoretical analysis to validate the decomposition. In contrast to existing attacks, UJA's unrestricted objective significantly expands the search space, enabling more flexible and efficient exploration of LLM vulnerabilities. Extensive evaluations show that UJA achieves over 80\\% attack success rates against recent safety-aligned LLMs with only 100 optimization iterations, outperforming the state-of-the-art gradient-based attacks by over 30\\%.         ",
    "url": "https://arxiv.org/abs/2510.02999",
    "authors": [
      "Xinzhe Huang",
      "Wenjing Hu",
      "Tianhang Zheng",
      "Kedong Xiu",
      "Xiaojun Jia",
      "Di Wang",
      "Zhan Qin",
      "Kui Ren"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.11234",
    "title": "Neural Weight Compression for Language Models",
    "abstract": "           Efficient storage and transmission of language model weights are increasingly critical as model scale and deployment grow. Yet, most existing compression methods rely on handcrafted transforms and heuristics, reflecting the limited understanding of weights as a data modality. This motivates a shift toward learning-based paradigm, where compression schemes are optimized directly from data rather than manually designed. In this work, we take a step in this direction by formulating weight compression as a neural codec learning. We propose Neural Weight Compression (NWC), a flexible framework for training neural codecs on pretrained weight datasets. NWC addresses challenges intrinsic to weight compression, such as tensor shape heterogeneity and the misalignment between training losses and downstream performance, through components such as chunk-and-normalize preprocessing and an importance-aware training objective. Experiments show that NWC achieves state-of-the-art accuracy-compression tradeoffs, particularly at 4--6 bit regime, without relying on rigid handcrafted components such as the Hadamard transform. These gains extend across diverse architectures, e.g., vision encoders. Our analysis further supports the learning-based perspective, highlighting the roles of entropy-constrained quantization in high rate regime and learned transforms in adapting to downstream tasks.         ",
    "url": "https://arxiv.org/abs/2510.11234",
    "authors": [
      "Jegwang Ryu",
      "Minkyu Kim",
      "Seungjun Shin",
      "Hee Min Choi",
      "Dokwan Oh",
      "Jaeho Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.13244",
    "title": "MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding",
    "abstract": "           Music is both an auditory and an embodied phenomenon, closely linked to human motion and naturally expressed through dance. However, most existing audio representations neglect this embodied dimension, limiting their ability to capture rhythmic and structural cues that drive movement. We propose MotionBeat, a framework for motion-aligned music representation learning. MotionBeat is trained with two newly proposed objectives: the Embodied Contrastive Loss (ECL), an enhanced InfoNCE formulation with tempo-aware and beat-jitter negatives to achieve fine-grained rhythmic discrimination, and the Structural Rhythm Alignment Loss (SRAL), which ensures rhythm consistency by aligning music accents with corresponding motion events. Architecturally, MotionBeat introduces bar-equivariant phase rotations to capture cyclic rhythmic patterns and contact-guided attention to emphasize motion events synchronized with musical accents. Experiments show that MotionBeat outperforms state-of-the-art audio encoders in music-to-dance generation and transfers effectively to beat tracking, music tagging, genre and instrument classification, emotion recognition, and audio-visual retrieval. Our project demo page: this https URL.         ",
    "url": "https://arxiv.org/abs/2510.13244",
    "authors": [
      "Xuanchen Wang",
      "Heng Wang",
      "Weidong Cai"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2510.14466",
    "title": "Toward Robust Multilingual Adaptation of LLMs for Low-Resource Languages",
    "abstract": "           Large language models (LLMs) continue to struggle with low-resource languages, primarily due to limited training data, translation noise, and unstable cross-lingual alignment. To address these challenges, we propose LiRA (Linguistic Robust Anchoring for LLMs)-a plug-and-play framework that requires only lightweight fine-tuning on top of existing pretrained backbones. LiRA jointly optimizes representation stability and cross-lingual semantic consistency by combining two key components: Arca (Anchored Representation Composition Architecture), which aligns low-resource inputs to a shared English semantic space through anchor-based alignment and collaborative encoding; and LaSR (Language-coupled Semantic Reasoner), a lightweight, language-aware head that enforces consistency regularization for unified cross-lingual understanding, retrieval, and reasoning. We theoretically show that under controlled anchoring error and translation-induced bias, LiRA guarantees bounded representation deviation and stable downstream performance under local Lipschitz continuity. To facilitate research, we release a new multilingual product retrieval dataset covering five Southeast Asian and two South Asian languages. Extensive experiments across diverse low-resource benchmarks demonstrate consistent improvements in retrieval, ranking, question answering, and reasoning tasks. Code will be publicly available on GitHub, and the dataset will be hosted on Hugging Face.         ",
    "url": "https://arxiv.org/abs/2510.14466",
    "authors": [
      "Haolin Li",
      "Haipeng Zhang",
      "Mang Li",
      "Yaohua Wang",
      "Lijie Wen",
      "Yu Zhang",
      "Biqing Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.21296",
    "title": "An Evidence-Based Post-Hoc Adjustment Framework for Anomaly Detection Under Data Contamination",
    "abstract": "           Unsupervised anomaly detection (AD) methods typically assume clean training data, yet real-world datasets often contain undetected or mislabeled anomalies, leading to significant performance degradation. Existing solutions require access to the training pipelines, data or prior knowledge of the proportions of anomalies in the data, limiting their real-world applicability. To address this challenge, we propose EPHAD, a simple yet effective test-time adaptation framework that updates the outputs of AD models trained on contaminated datasets using evidence gathered at test time. Our approach integrates the prior knowledge captured by the AD model trained on contaminated datasets with evidence derived from multimodal foundation models like Contrastive Language-Image Pre-training (CLIP), classical AD methods like the Local Outlier Factor or domain-specific knowledge. We illustrate the intuition behind EPHAD using a synthetic toy example and validate its effectiveness through comprehensive experiments across eight visual AD datasets, twenty-six tabular AD datasets, and a real-world industrial AD dataset. Additionally, we conduct an ablation study to analyse hyperparameter influence and robustness to varying contamination levels, demonstrating the versatility and robustness of EPHAD across diverse AD models and evidence pairs. To ensure reproducibility, our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.21296",
    "authors": [
      "Sukanya Patra",
      "Souhaib Ben Taieb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.25262",
    "title": "IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning",
    "abstract": "           Normalization is fundamental to deep learning, but existing approaches such as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero mean and unit variance, stabilizing training without controlling how representations capture task-relevant information. We propose IB-Inspired Normalization (IBNorm), a simple yet powerful family of methods grounded in the Information Bottleneck principle. IBNorm introduces bounded compression operations that encourage embeddings to preserve predictive information while suppressing nuisance variability, yielding more informative representations while retaining the stability and compatibility of standard normalization. Theoretically, we prove that IBNorm achieves a higher IB value and tighter generalization bounds than variance-centric methods. Empirically, IBNorm consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual information analysis confirming superior information bottleneck behavior. Code will be released publicly.         ",
    "url": "https://arxiv.org/abs/2510.25262",
    "authors": [
      "Xiandong Zou",
      "Jia Li",
      "Xiaotong Yuan",
      "Pan Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.03068",
    "title": "Graph Homomorphism Distortion: A Metric to Distinguish Them All and in the Latent Space Bind Them",
    "abstract": "           A large driver of the complexity of graph learning is the interplay between \\emph{structure} and \\emph{features}.When analyzing the expressivity of graph neural networks, however, existing approaches ignore features in favor of structure, making it nigh-impossible to assess to what extent two graphs with close features should be considered this http URL address this by developing a new \\mbox{(pseudo-)metric} based on graph this http URL by concepts from metric geometry, our \\emph{graph homomorphism distortion} measures the minimal worst-case distortion that node features of one graph are subjected to when mapping one graph to this http URL demonstrate the utility of our novel measure by showing that (i.) it can be efficiently calculated under some additional assumptions, (ii.) it complements existing expressivity measures like \\mbox{$1$-WL}, and (iii.)it permits defining structural encodings, which improve the predictive capabilities of graph neural networks.         ",
    "url": "https://arxiv.org/abs/2511.03068",
    "authors": [
      "Martin Carrasco",
      "Olga Zaghen",
      "Kavir Sumaraj",
      "Erik Bekkers",
      "Bastian Rieck"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.05879",
    "title": "Physics-Informed Neural Networks for Real-Time Gas Crossover Prediction in PEM Electrolyzers: First Application with Multi-Membrane Validation",
    "abstract": "           Green hydrogen production via polymer electrolyte membrane (PEM) water electrolysis is pivotal for energy transition, yet hydrogen crossover through membranes threatens safety and economic viability-approaching explosive limits (4 mol% H$_2$ in O$_2$) while reducing Faradaic efficiency and accelerating membrane degradation. Current physics-based models require extensive calibration and computational resources that preclude real-time implementation, while purely data-driven approaches fail to extrapolate beyond training conditions-critical for dynamic electrolyzer operation. Here we present the first application of physics-informed neural networks (PINNs) for hydrogen crossover prediction, trained on 184 published measurements augmented to 1,114 points and constrained by a constitutive physics model (Henry's law, Fick's diffusion, and Faraday-based gas production) embedded in the loss function. Our compact architecture (17,793 parameters), validated across six membranes under industrially relevant conditions (0.05-5.0 A/cm$^2$, 1-200 bar, 25-85\u00b0C), achieves exceptional accuracy (R$^2$ = 99.84% $\\pm$ 0.15%, RMSE = 0.0932% $\\pm$ 0.0438%) based on five-fold cross-validation, with sub-millisecond inference enabling real-time control. Remarkably, the model maintains R$^2$ > 86% when predicting crossover at pressures 2.5x beyond training range-substantially outperforming pure neural networks (R$^2$ = 43.4%). The hardware-agnostic deployment, from desktop CPUs to edge devices (Raspberry Pi 4), enables distributed safety monitoring essential for gigawatt-scale installations. By bridging physical rigor and computational efficiency, this work establishes a new paradigm for real-time electrolyzer monitoring, accelerating deployment of safe, efficient green hydrogen infrastructure crucial for net-zero emissions targets.         ",
    "url": "https://arxiv.org/abs/2511.05879",
    "authors": [
      "Yong-Woon Kim",
      "Chulung Kang",
      "Yung-Cheol Byun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.16870",
    "title": "Align & Invert: Solving Inverse Problems with Diffusion and Flow-based Models via Representation Alignment",
    "abstract": "           Enforcing alignment between the internal representations of diffusion or flow-based generative models and those of pretrained self-supervised encoders has recently been shown to provide a powerful inductive bias, improving both convergence and sample quality. In this work, we extend this idea to inverse problems, where pretrained generative models are employed as priors. We propose applying representation alignment (REPA) between diffusion or flow-based models and a DINOv2 visual encoder, to guide the reconstruction process at inference time. Although ground-truth signals are unavailable in inverse problems, we empirically show that aligning model representations of approximate target features can substantially enhance reconstruction quality and perceptual realism. We provide theoretical results showing (a) that REPA regularization can be viewed as a variational approach for minimizing a divergence measure in the DINOv2 embedding space, and (b) how under certain regularity assumptions REPA updates steer the latent diffusion states toward those of the clean image. These results offer insights into the role of REPA in improving perceptual fidelity. Finally, we demonstrate the generality of our approach by We integrate REPA into multiple state-of-the-art inverse problem solvers, and provide extensive experiments on super-resolution, box inpainting, Gaussian deblurring, and motion deblurring confirming that our method consistently improves reconstruction quality, while also providing efficiency gains reducing the number of required discretization steps.         ",
    "url": "https://arxiv.org/abs/2511.16870",
    "authors": [
      "Loukas Sfountouris",
      "Giannis Daras",
      "Paris Giampouras"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.18368",
    "title": "Wireless Power Transfer and Intent-Driven Network Optimization in AAVs-assisted IoT for 6G Sustainable Connectivity",
    "abstract": "           Autonomous Aerial Vehicle (AAV)-assisted Internet of Things (IoT) represents a collaborative architecture in which AAV allocate resources over 6G links to jointly enhance user-intent interpretation and overall network performance. Owing to this mutual dependence, improvements in intent inference and policy decisions on one component reinforce the efficiency of others, making highly reliable intent prediction and low-latency action execution essential. Although numerous approaches can model intent relationships, they encounter severe obstacles when scaling to high-dimensional action sequences and managing intensive on-board computation. We propose an Intent-Driven Framework for Autonomous Network Optimization comprising prediction and decision modules. First, implicit intent modeling is adopted to mitigate inaccuracies arising from ambiguous user expressions. For prediction, we introduce Hyperdimensional Transformer (HDT), which embeds data into a Hyperdimensional space via Hyperdimensional vector encoding and replaces standard matrix and attention operations with symbolic Hyperdimensional computations. For decision-making, where AAV must respond to user intent while planning trajectories, we design Double Actions based Multi-Agent Proximal Policy Optimization (DA-MAPPO). Building upon MAPPO, it samples actions through two independently parameterized networks and cascades the user-intent network into the trajectory network to maintain action dependencies. We evaluate our framework on a real IoT action dataset with authentic wireless data. Experimental results demonstrate that HDT and DA-MAPPO achieve superior performance across diverse scenarios.         ",
    "url": "https://arxiv.org/abs/2511.18368",
    "authors": [
      "Xiaoming He",
      "Gaofeng Wang",
      "Huajun Cui",
      "Rui Yuan",
      "Haitao Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.18689",
    "title": "QuantKAN: A Unified Quantization Framework for Kolmogorov Arnold Networks",
    "abstract": "           Kolmogorov Arnold Networks (KANs) represent a new class of neural architectures that replace conventional linear transformations and node-based nonlinearities with spline-based function approximations distributed along network edges. Although KANs offer strong expressivity and interpretability, their heterogeneous spline and base branch parameters hinder efficient quantization, which remains unexamined compared to CNNs and Transformers. In this paper, we present QuantKAN, a unified framework for quantizing KANs across both quantization aware training (QAT) and post-training quantization (PTQ) regimes. QuantKAN extends modern quantization algorithms, such as LSQ, LSQ+, PACT, DoReFa, QIL, GPTQ, BRECQ, AdaRound, AWQ, and HAWQ-V2, to spline based layers with branch-specific quantizers for base, spline, and activation components. Through extensive experiments on MNIST, CIFAR 10, and CIFAR 100 across multiple KAN variants (EfficientKAN, FastKAN, PyKAN, and KAGN), we establish the first systematic benchmarks for low-bit spline networks. Our results show that KANs, particularly deeper KAGN variants, are compatible with low-bit quantization but exhibit strong method architecture interactions: LSQ, LSQ+, and PACT preserve near full precision accuracy at 4 bit for shallow KAN MLP and ConvNet models, while DoReFa provides the most stable behavior for deeper KAGN under aggressive low-bit settings. For PTQ, GPTQ and Uniform consistently deliver the strongest overall performance across datasets, with BRECQ highly competitive on simpler regimes such as MNIST. Our proposed QuantKAN framework thus unifies spline learning and quantization, and provides practical tools and guidelines for efficiently deploying KANs in real-world, resource-constrained environments.         ",
    "url": "https://arxiv.org/abs/2511.18689",
    "authors": [
      "Kazi Ahmed Asif Fuad",
      "Lizhong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.18940",
    "title": "Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery",
    "abstract": "           Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces. To mitigate strong inter-subject variability, recent work has emphasized manifold-based approaches operating on covariance representations. Yet dispersion scaling and orientation alignment remain largely unaddressed in existing methods. In this paper, we address both issues through congruence transforms and introduce three complementary geometry-aware models: (i) Discriminative Congruence Transform (DCT), (ii) Deep Linear DCT (DLDCT), and (iii) Deep DCT-UNet (DDCT-UNet). These models are evaluated both as pre-alignment modules for downstream classifiers and as end-to-end discriminative systems trained via cross-entropy backpropagation with a custom logistic-regression head. Across challenging motor-imagery benchmarks, the proposed framework improves transductive cross-subject accuracy by 2-3%, demonstrating the value of geometry-aware congruence learning.         ",
    "url": "https://arxiv.org/abs/2511.18940",
    "authors": [
      "Sanjeev Manivannan",
      "Chandrashekar Lakshminarayan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2511.20002",
    "title": "Semantic Router: On the Feasibility of Hijacking MLLMs via a Single Adversarial Perturbation",
    "abstract": "           Multimodal Large Language Models (MLLMs) are increasingly deployed in stateless systems, such as autonomous driving and robotics. This paper investigates a novel threat: Semantic-Aware Hijacking. We explore the feasibility of hijacking multiple stateless decisions simultaneously using a single universal perturbation. We introduce the Semantic-Aware Universal Perturbation (SAUP), which acts as a semantic router, \"actively\" perceiving input semantics and routing them to distinct, attacker-defined targets. To achieve this, we conduct theoretical and empirical analysis on the geometric properties in the latent space. Guided by these insights, we propose the Semantic-Oriented (SORT) optimization strategy and annotate a new dataset with fine-grained semantics to evaluate performance. Extensive experiments on three representative MLLMs demonstrate the fundamental feasibility of this attack, achieving a 66% attack success rate over five targets using a single frame against Qwen.         ",
    "url": "https://arxiv.org/abs/2511.20002",
    "authors": [
      "Changyue Li",
      "Jiaying Li",
      "Youliang Yuan",
      "Jiaming He",
      "Zhicong Huang",
      "Pinjia He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2512.00590",
    "title": "Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models",
    "abstract": "           Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.         ",
    "url": "https://arxiv.org/abs/2512.00590",
    "authors": [
      "Alla Chepurova",
      "Aydar Bulatov",
      "Mikhail Burtsev",
      "Yuri Kuratov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.02901",
    "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in $\\{\\pm 1, \\pm i\\}$",
    "abstract": "           Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware. We open-source the Fairy2i model and code at this https URL and this https URL.         ",
    "url": "https://arxiv.org/abs/2512.02901",
    "authors": [
      "Feiyu Wang",
      "Xinyu Tan",
      "Bokai Huang",
      "Yihao Zhang",
      "Guoan Wang",
      "Peizhuang Cong",
      "Tong Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.08557",
    "title": "SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds",
    "abstract": "           This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.         ",
    "url": "https://arxiv.org/abs/2512.08557",
    "authors": [
      "Alexander Dow",
      "Manduhu Manduhu",
      "Matheus Santos",
      "Ben Bartlett",
      "Gerard Dooly",
      "James Riordan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.09780",
    "title": "Physics-Aware Heterogeneous GNN Architecture for Real-Time BESS Optimization in Unbalanced Distribution Systems",
    "abstract": "           Battery energy storage systems (BESS) have become increasingly vital in three-phase unbalanced distribution grids for maintaining voltage stability and enabling optimal dispatch. However, existing deep learning approaches often lack explicit three-phase representation, making it difficult to accurately model phase-specific dynamics and enforce operational constraints--leading to infeasible dispatch solutions. This paper demonstrates that by embedding detailed three-phase grid information--including phase voltages, unbalanced loads, and BESS states--into heterogeneous graph nodes, diverse GNN architectures (GCN, GAT, GraphSAGE, GPS) can jointly predict network state variables with high accuracy. Moreover, a physics-informed loss function incorporates critical battery constraints--SoC and C-rate limits--via soft penalties during training. Experimental validation on the CIGRE 18-bus distribution system shows that this embedding-loss approach achieves low prediction errors, with bus voltage MSEs of 6.92e-07 (GCN), 1.21e-06 (GAT), 3.29e-05 (GPS), and 9.04e-07 (SAGE). Importantly, the physics-informed method ensures nearly zero SoC and C-rate constraint violations, confirming its effectiveness for reliable, constraint-compliant dispatch.         ",
    "url": "https://arxiv.org/abs/2512.09780",
    "authors": [
      "Aoxiang Ma",
      "Salah Ghamizi",
      "Jun Cao",
      "Pedro Rodriguez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.11339",
    "title": "Projected Sobolev Natural Gradient Descent for Efficient Neural Network Solution of the Gross-Pitaevskii Equation",
    "abstract": "           This paper introduces a projected Sobolev natural gradient descent (NGD) method for computing ground states of the Gross-Pitaevskii equation. By projecting a continuous Riemannian Sobolev gradient flow onto the normalized neural network tangent space, we derive a discrete NGD algorithm that preserves the normalization constraint. The numerical implementation employs variational Monte Carlo with a hybrid sampling strategy to accurately account for the normalization constant arising from nonlinear interaction terms. To enhance computational efficiency, a matrix-free Nystr\u00f6m-preconditioned conjugate gradient solver is adopted to approximate the NGD operator without explicit matrix assembly. Numerical experiments demonstrate that the proposed method converges significantly faster than physics-informed neural network approaches and exhibits linear scalability with respect to spatial dimensions. Moreover, the resulting neural-network solutions provide high-quality initial guesses that substantially accelerate subsequent refinement by traditional high-precision solvers.         ",
    "url": "https://arxiv.org/abs/2512.11339",
    "authors": [
      "Chenglong Bao",
      "Chen Cui",
      "Kai Jiang",
      "Shi Shu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2512.15605",
    "title": "Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction",
    "abstract": "           Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.         ",
    "url": "https://arxiv.org/abs/2512.15605",
    "authors": [
      "Mathieu Blondel",
      "Michael E. Sander",
      "Germain Vivier-Ardisson",
      "Tianlin Liu",
      "Vincent Roulet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2512.15824",
    "title": "State-Augmented Graphs for Circular Economy Triage",
    "abstract": "           Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.         ",
    "url": "https://arxiv.org/abs/2512.15824",
    "authors": [
      "Richard Fox",
      "Rui Li",
      "Gustav Jonsson",
      "Farzaneh Goli",
      "Miying Yang",
      "Emel Aktas",
      "Yongjing Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.18352",
    "title": "LLM-based Few-Shot Early Rumor Detection with Imitation Agent",
    "abstract": "           Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \\textit{early time point determination}, while the LLM serves as a powerful \\textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.         ",
    "url": "https://arxiv.org/abs/2512.18352",
    "authors": [
      "Fengzhu Zeng",
      "Qian Shao",
      "Ling Cheng",
      "Wei Gao",
      "Shih-Fen Cheng",
      "Jing Ma",
      "Cheng Niu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.20900",
    "title": "When Experts Speak:Sequential LLM-Bayesian Learning for Startup Success Prediction",
    "abstract": "           Evaluating startups is inherently challenging in entrepreneurial finance, where investors confront severe information asymmetry and limited quantitative data. Leveraging a novel expert network call data, we develop an LLM-Bayesian model that analyzes these conversations at the question-answer turn level, extracting semantic and evaluative signals via large language models (LLMs) and aggregating them in a sequential Bayesian architecture. The model dynamically updates beliefs as additional expert calls occur and attenuates contradictory assessments, which are absent from existing text-based screening tools. Empirically, our model outperforms state-of-the-art benchmarks by 6.691% in F1-score and increases portfolio-level Return on Investment by 15.255%. Attention and ablation analyses reveal that conversational cues are particularly informative for technologically complex startups, young firms, diverse founding teams, and firms with low public visibility. By converting expert dialogue into continually updated probabilities, our model advances research in entrepreneurial finance and information systems and offers policy implications for improving funding outcomes for informationally disadvantaged startups.         ",
    "url": "https://arxiv.org/abs/2512.20900",
    "authors": [
      "Yidong Chai",
      "Yanguang Liu",
      "Xuan Tian",
      "Jiaheng Xie",
      "Yonghang Zhou"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2512.23526",
    "title": "EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition",
    "abstract": "           Accurate recognition of human emotional states is critical for effective human-machine interaction. Electroencephalography (EEG) offers a reliable source for emotion recognition due to its high temporal resolution and its direct reflection of neural activity. Nevertheless, variations across recording sessions present a major challenge for model generalization. To address this issue, we propose EGDA, a framework that reduces cross-session discrepancies by jointly aligning the global (marginal) and class-specific (conditional) distributions, while preserving the intrinsic structure of EEG data through graph regularization. Experimental results on the SEED-IV dataset demonstrate that EGDA achieves robust cross-session performance, obtaining accuracies of 81.22%, 80.15%, and 83.27% across three transfer tasks, and surpassing several baseline methods. Furthermore, the analysis highlights the Gamma frequency band as the most discriminative and identifies the central-parietal and prefrontal brain regions as critical for reliable emotion recognition.         ",
    "url": "https://arxiv.org/abs/2512.23526",
    "authors": [
      "Maryam Mirzaei",
      "Farzaneh Shayegh",
      "Hamed Narimani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.01021",
    "title": "Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations",
    "abstract": "           Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental for modeling stochastic dynamics across the natural sciences and modern machine learning. Learning their solution operators with deep learning models promises fast solvers and new perspectives on classical learning tasks. In this work, we build on Wiener-chaos expansions (WCE) to design neural operator (NO) architectures for SDEs and SPDEs: we project driving noise paths onto orthonormal Wick-Hermite features and use NOs to parameterize the resulting chaos coefficients, enabling reconstruction of full trajectories from noise in a single forward pass. We also make the underlying WCE structure explicit for multi-dimensional SDEs and semilinear SPDEs by showing the coupled deterministic ODE/PDE systems governing these coefficients. Empirically, we achieve competitive accuracy across several tasks, including standard SPDE benchmarks and SDE-based diffusion one-step image sampling, topological graph interpolation, financial extrapolation, parameter estimation, and manifold SDE flood forecasting. These results suggest WCE-based neural operators are a practical and scalable approach to learning SDE/SPDE solution operators across domains.         ",
    "url": "https://arxiv.org/abs/2601.01021",
    "authors": [
      "Dai Shi",
      "Lequan Lin",
      "Andi Han",
      "Luke Thompson",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
      "Zhiyong Wang",
      "Junbin Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.04575",
    "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
    "abstract": "           Behavior cloning has seen a resurgence as scaling model and data sizes demonstrate strong performance. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. Empirically, we show that our best model achieves performance competitive with human players across a variety of 3D games. We use this recipe to investigate the scaling laws of behavior cloning, with a focus on causal reasoning. In a controlled toy setting, we first demonstrate that increasing training data and network depth leads to the model learning a more causal policy. We then validate these findings at scale, analyzing models up to 1.2 billion parameters. We observe that the causal improvements seen in the toy domain hold true as model size and training steps increase.         ",
    "url": "https://arxiv.org/abs/2601.04575",
    "authors": [
      "Yuguang Yue",
      "Irakli Salia",
      "Samuel Hunt",
      "Chris Green",
      "Wenzhe Shi",
      "Jonathan J Hunt"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.06758",
    "title": "A Backpropagation-Free Feedback-Hebbian Network for Continual Learning Dynamics",
    "abstract": "           Feedback-rich neural architectures can regenerate earlier representations and inject temporal context, making them a natural setting for strictly local synaptic plasticity. Existing literature raises doubt about whether a minimal, backpropagation-free feedback-Hebbian system can already express interpretable continual-learning-relevant behaviors under controlled training schedules. In this work, we introduce a compact prediction-reconstruction architecture with a dedicated feedback pathway providing lightweight, locally trainable temporal context for continual adaptation. All synapses are updated by a unified local rule combining centered Hebbian covariance, Oja-style stabilization, and a local supervised drive where targets are available. With a simple two-pair association task, learning is characterized through layer-wise activity snapshots, connectivity trajectories (row and column means of learned weights), and a normalized retention index across phases. Under sequential A to B training, forward output connectivity exhibits a long-term depression (LTD)-like suppression of the earlier association, while feedback connectivity preserves an A-related trace during acquisition of B. Under an alternating sequence, both associations are concurrently maintained rather than sequentially suppressed. Architectural controls and rule-term ablations isolate the role of dedicated feedback in regeneration and co-maintenance, alongside the role of the local supervised term in output selectivity and unlearning. Together, the results show that a compact feedback pathway trained with local plasticity can support regeneration and continual-learning-relevant dynamics in a minimal, mechanistically transparent setting.         ",
    "url": "https://arxiv.org/abs/2601.06758",
    "authors": [
      "Josh Li",
      "Fow-sen Choa"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.07118",
    "title": "Reward-Preserving Attacks For Robust Reinforcement Learning",
    "abstract": "           Adversarial training in reinforcement learning (RL) is challenging because perturbations cascade through trajectories and compound over time, making fixed-strength attacks either overly destructive or too conservative. We propose reward-preserving attacks, which adapt adversarial strength so that an $\\alpha$ fraction of the nominal-to-worst-case return gap remains achievable at each state. In deep RL, perturbation magnitudes $\\eta$ are selected dynamically, using a learned critic $Q((s,a),\\eta)$ that estimates the expected return of $\\alpha$-reward-preserving rollouts. For intermediate values of $\\alpha$, this adaptive training yields policies that are robust across a wide range of perturbation magnitudes while preserving nominal performance, outperforming fixed-radius and uniformly sampled-radius adversarial training.         ",
    "url": "https://arxiv.org/abs/2601.07118",
    "authors": [
      "Lucas Schott",
      "Elies Gherbi",
      "Hatem Hajri",
      "Sylvain Lamprier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.07288",
    "title": "Kernel Alignment-based Multi-view Unsupervised Feature Selection with Sample-level Adaptive Graph Learning",
    "abstract": "           Although multi-view unsupervised feature selection (MUFS) has demonstrated success in dimensionality reduction for unlabeled multi-view data, most existing methods reduce feature redundancy by focusing on linear correlations among features but often overlook complex nonlinear dependencies. This limits the effectiveness of feature selection. In addition, existing methods fuse similarity graphs from multiple views by employing sample-invariant weights to preserve local structure. However, this process fails to account for differences in local neighborhood clarity among samples within each view, thereby hindering accurate characterization of the intrinsic local structure of the data. In this paper, we propose a Kernel Alignment-based multi-view unsupervised FeatUre selection with Sample-level adaptive graph lEarning method (KAFUSE) to address these issues. Specifically, we first employ kernel alignment with an orthogonal constraint to reduce feature redundancy in both linear and nonlinear relationships. Then, a cross-view consistent similarity graph is learned by applying sample-level fusion to each slice of a tensor formed by stacking similarity graphs from different views, which automatically adjusts the view weights for each sample during fusion. These two steps are integrated into a unified model for feature selection, enabling mutual enhancement between them. Extensive experiments on real multi-view datasets demonstrate the superiority of KAFUSE over state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2601.07288",
    "authors": [
      "Yalan Tan",
      "Yanyong Huang",
      "Zongxin Shen",
      "Dongjie Wang",
      "Fengmao Lv",
      "Tianrui Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.10485",
    "title": "Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge",
    "abstract": "           Domain-specific knowledge graphs (DKGs) are critical yet often suffer from limited coverage compared to General Knowledge Graphs (GKGs). Existing tasks to enrich DKGs rely primarily on extracting knowledge from external unstructured data or completing KGs through internal reasoning, but the scope and quality of such integration remain limited. This highlights a critical gap: little systematic exploration has been conducted on how comprehensive, high-quality GKGs can be effectively leveraged to supplement DKGs. To address this gap, we propose a new and practical task: domain-specific knowledge graph fusion (DKGF), which aims to mine and integrate relevant facts from general knowledge graphs into domain-specific knowledge graphs to enhance their completeness and utility. Unlike previous research, this new task faces two key challenges: (1) high ambiguity of domain relevance, i.e., difficulty in determining whether knowledge from a GKG is truly relevant to the target domain , and (2) cross-domain knowledge granularity misalignment, i.e., GKG facts are typically abstract and coarse-grained, whereas DKGs frequently require more contextualized, fine-grained representations aligned with particular domain scenarios. To address these, we present ExeFuse, a neuro-symbolic framework based on a novel Fact-as-Program paradigm. ExeFuse treats fusion as an executable process, utilizing neuro-symbolic execution to infer logical relevance beyond surface similarity and employing target space grounding to calibrate granularity. We construct two new datasets to establish the first standardized evaluation suite for this task. Extensive experiments demonstrate that ExeFuse effectively overcomes domain barriers to achieve superior fusion performance.         ",
    "url": "https://arxiv.org/abs/2601.10485",
    "authors": [
      "Runhao Zhao",
      "Weixin Zeng",
      "Wentao Zhang",
      "Chong Chen",
      "Zhengpin Li",
      "Xiang Zhao",
      "Lei Chen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.11219",
    "title": "SDFLoRA: Selective Decoupled Federated LoRA for Privacy-preserving Fine-tuning with Heterogeneous Clients",
    "abstract": "           Federated learning (FL) for large language models (LLMs) has attracted increasing attention as a privacy-preserving approach for adapting models over distributed data, where parameter-efficient methods such as Low-Rank Adaptation (LoRA) are widely adopted to reduce communication and memory costs. However, practical deployments often exhibit rank and data heterogeneity: clients operate under different low-rank budgets and data distributions, making direct aggregation of LoRA updates biased and unstable. Existing approaches either enforce a unified rank or align heterogeneous updates into a single shared subspace, which tends to mix transferable and client-specific directions and consequently undermines personalization. Moreover, under differential privacy (DP), perturbing such structurally mixed updates injects noise into directions that should remain purely local, leading to unnecessary utility degradation. To address these issues, we propose Selective Decoupled Federated LoRA (SDFLoRA), a structure-aware LoRA framework that decouples each client update into a shared component for aggregation and a private component that preserves client-specific semantics. Only the shared component participates in subspace alignment, while the private component remains local and uncommunicated, making the training DP-compatible and stabilizing aggregation under rank heterogeneity. By injecting noise only into the aggregated shareable update, this approach avoids perturbations to local directions and improves the utility-privacy trade-off. Experiments on multiple benchmarks demonstrate that SDFLoRA outperforms federated LoRA baselines and achieves a strong utility-privacy trade-off.         ",
    "url": "https://arxiv.org/abs/2601.11219",
    "authors": [
      "Zhikang Shen",
      "Jianrong Lu",
      "Haiyuan Wan",
      "Jianhai Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.12448",
    "title": "Evaluating Large Language Models for Time Series Anomaly Detection in Aerospace Software",
    "abstract": "           Time series anomaly detection (TSAD) is essential for ensuring the safety and reliability of aerospace software systems. Although large language models (LLMs) provide a promising training-free alternative to unsupervised approaches, their effectiveness in aerospace settings remains under-examined because of complex telemetry, misaligned evaluation metrics, and the absence of domain knowledge. To address this gap, we introduce ATSADBench, the first benchmark for aerospace TSAD. ATSADBench comprises nine tasks that combine three pattern-wise anomaly types, univariate and multivariate signals, and both in-loop and out-of-loop feedback scenarios, yielding 108,000 data points. Using this benchmark, we systematically evaluate state-of-the-art open-source LLMs under two paradigms: Direct, which labels anomalies within sliding windows, and Prediction-Based, which detects anomalies from prediction errors. To reflect operational needs, we reformulate evaluation at the window level and propose three user-oriented metrics: Alarm Accuracy (AA), Alarm Latency (AL), and Alarm Contiguity (AC), which quantify alarm correctness, timeliness, and credibility. We further examine two enhancement strategies, few-shot learning and retrieval-augmented generation (RAG), to inject domain knowledge. The evaluation results show that (1) LLMs perform well on univariate tasks but struggle with multivariate telemetry, (2) their AA and AC on multivariate tasks approach random guessing, (3) few-shot learning provides modest gains whereas RAG offers no significant improvement, and (4) in practice LLMs can detect true anomaly onsets yet sometimes raise false alarms, which few-shot prompting mitigates but RAG exacerbates. These findings offer guidance for future LLM-based TSAD in aerospace software.         ",
    "url": "https://arxiv.org/abs/2601.12448",
    "authors": [
      "Yang Liu",
      "Yixing Luo",
      "Xiaofeng Li",
      "Xiaogang Dong",
      "Bin Gu",
      "Zhi Jin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2601.13380",
    "title": "Practical Insights into Semi-Supervised Object Detection Approaches",
    "abstract": "           Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.         ",
    "url": "https://arxiv.org/abs/2601.13380",
    "authors": [
      "Chaoxin Wang",
      "Bharaneeshwar Balasubramaniyam",
      "Anurag Sangem",
      "Nicolais Guevara",
      "Doina Caragea"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.13465",
    "title": "Graph Neural Networks are Heuristics",
    "abstract": "           We demonstrate that a single training trajectory can transform a graph neural network into an unsupervised heuristic for combinatorial optimization. Focusing on the Travelling Salesman Problem, we show that encoding global structural constraints as an inductive bias enables a non-autoregressive model to generate solutions via direct forward passes, without search, supervision, or sequential decision-making. At inference time, dropout and snapshot ensembling allow a single model to act as an implicit ensemble, reducing optimality gaps through increased solution diversity. Our results establish that graph neural networks do not require supervised training nor explicit search to be effective. Instead, they can internalize global combinatorial structure and function as strong, learned heuristics. This reframes the role of learning in combinatorial optimization: from augmenting classical algorithms to directly instantiating new heuristics.         ",
    "url": "https://arxiv.org/abs/2601.13465",
    "authors": [
      "Yimeng Min",
      "Carla P. Gomes"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.18537",
    "title": "SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction",
    "abstract": "           Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.         ",
    "url": "https://arxiv.org/abs/2601.18537",
    "authors": [
      "Linyong Gan",
      "Zimo Li",
      "Wenxin Xu",
      "Xingjian Li",
      "Jianhua Z. Huang",
      "Enmei Tu",
      "Shuhang Chen"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.18823",
    "title": "VAE with Hyperspherical Coordinates: Improving Anomaly Detection from Hypervolume-Compressed Latent Space",
    "abstract": "           Variational autoencoders (VAE) encode data into lower-dimensional latent vectors before decoding those vectors back to data. Once trained, one can hope to detect out-of-distribution (abnormal) latent vectors, but several issues arise when the latent space is high dimensional. This includes an exponential growth of the hypervolume with the dimension, which severely affects the generative capacity of the VAE. In this paper, we draw insights from high dimensional statistics: in these regimes, the latent vectors of a standard VAE are distributed on the `equators' of a hypersphere, challenging the detection of anomalies. We propose to formulate the latent variables of a VAE using hyperspherical coordinates, which allows compressing the latent vectors towards a given direction on the hypersphere, thereby allowing for a more expressive approximate posterior. We show that this improves both the fully unsupervised and OOD anomaly detection ability of the VAE, achieving the best performance on the datasets we considered, outperforming existing methods. For the unsupervised and OOD modalities, respectively, these are: i) detecting unusual landscape from the Mars Rover camera and unusual Galaxies from ground based imagery (complex, real world datasets); ii) standard benchmarks like Cifar10 and subsets of ImageNet as the in-distribution (ID) class.         ",
    "url": "https://arxiv.org/abs/2601.18823",
    "authors": [
      "Alejandro Ascarate",
      "Leo Lebrat",
      "Rodrigo Santa Cruz",
      "Clinton Fookes",
      "Olivier Salvado"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.18987",
    "title": "LLMs versus the Halting Problem: Revisiting Program Termination Prediction",
    "abstract": "           Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems.         ",
    "url": "https://arxiv.org/abs/2601.18987",
    "authors": [
      "Oren Sultan",
      "Jordi Armengol-Estape",
      "Pascal Kesseli",
      "Julien Vanegue",
      "Dafna Shahaf",
      "Yossi Adi",
      "Peter O'Hearn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2601.19120",
    "title": "RobustExplain: Evaluating Robustness of LLM-Based Explanation Agents for Recommendation",
    "abstract": "           Large Language Models (LLMs) are increasingly used to generate natural-language explanations in recommender systems, acting as explanation agents that reason over user behavior histories. While prior work has focused on explanation fluency and relevance under fixed inputs, the robustness of LLM-generated explanations to realistic user behavior noise remains largely unexplored. In real-world web platforms, interaction histories are inherently noisy due to accidental clicks, temporal inconsistencies, missing values, and evolving preferences, raising concerns about explanation stability and user trust. We present RobustExplain, the first systematic evaluation framework for measuring the robustness of LLM-generated recommendation explanations. RobustExplain introduces five realistic user behavior perturbations evaluated across multiple severity levels and a multi-dimensional robustness metric capturing semantic, keyword, structural, and length consistency. Our goal is to establish a principled, task-level evaluation framework and initial robustness baselines, rather than to provide a comprehensive leaderboard across all available LLMs. Experiments on four representative LLMs (7B--70B) show that current models exhibit only moderate robustness, with larger models achieving up to 8% higher stability. Our results establish the first robustness benchmarks for explanation agents and highlight robustness as a critical dimension for trustworthy, agent-driven recommender systems at web scale.         ",
    "url": "https://arxiv.org/abs/2601.19120",
    "authors": [
      "Guilin Zhang",
      "Kai Zhao",
      "Jeffrey Friedman",
      "Xu Chu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.19494",
    "title": "AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context",
    "abstract": "           High-quality evaluation benchmarks are pivotal for deploying Large Language Models (LLMs) in Automated Code Review (ACR). However, existing benchmarks suffer from two critical limitations: first, the lack of multi-language support in repository-level contexts, which restricts the generalizability of evaluation results; second, the reliance on noisy, incomplete ground truth derived from raw Pull Request (PR) comments, which constrains the scope of issue detection. To address these challenges, we introduce AACR-Bench a comprehensive benchmark that provides full cross-file context across multiple programming languages. Unlike traditional datasets, AACR-Bench employs an \"AI-assisted, Expert-verified\" annotation pipeline to uncover latent defects often overlooked in original PRs, resulting in a 285% increase in defect coverage. Extensive evaluations of mainstream LLMs on AACR-Bench reveal that previous assessments may have either misjudged or only partially captured model capabilities due to data limitations. Our work establishes a more rigorous standard for ACR evaluation and offers new insights on LLM based ACR, i.e., the granularity/level of context and the choice of retrieval methods significantly impact ACR performance, and this influence varies depending on the LLM, programming language, and the LLM usage paradigm e.g., whether an Agent architecture is employed. The code, data, and other artifacts of our evaluation set are available at this https URL .         ",
    "url": "https://arxiv.org/abs/2601.19494",
    "authors": [
      "Lei Zhang",
      "Yongda Yu",
      "Minghui Yu",
      "Xinxin Guo",
      "Zhengqi Zhuang",
      "Guoping Rong",
      "Dong Shao",
      "Haifeng Shen",
      "Hongyu Kuang",
      "Zhengfeng Li",
      "Boge Wang",
      "Guoan Zhang",
      "Bangyu Xiang",
      "Xiaobin Xu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.19684",
    "title": "LLM-Assisted Authentication and Fraud Detection",
    "abstract": "           User authentication and fraud detection face growing challenges as digital systems expand and adversaries adopt increasingly sophisticated tactics. Traditional knowledge-based authentication remains rigid, requiring exact word-for-word string matches that fail to accommodate natural human memory and linguistic variation. Meanwhile, fraud-detection pipelines struggle to keep pace with rapidly evolving scam behaviors, leading to high false-positive rates and frequent retraining cycles required. This work introduces two complementary LLM-enabled solutions, namely, an LLM-assisted authentication mechanism that evaluates semantic correctness rather than exact wording, supported by document segmentation and a hybrid scoring method combining LLM judgement with cosine-similarity metrics and a RAG-based fraud-detection pipeline that grounds LLM reasoning in curated evidence to reduce hallucinations and adapt to emerging scam patterns without model retraining. Experiments show that the authentication system accepts 99.5% of legitimate non-exact answers while maintaining a 0.1% false-acceptance rate, and that the RAG-enhanced fraud detection reduces false positives from 17.2% to 3.5%. Together, these findings demonstrate that LLMs can significantly improve both usability and robustness in security workflows, offering a more adaptive , explainable, and human-aligned approach to authentication and fraud detection.         ",
    "url": "https://arxiv.org/abs/2601.19684",
    "authors": [
      "Emunah S-S. Chan",
      "Aldar C-F. Chan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2601.19876",
    "title": "Real-Time Pulsatile Flow Prediction for Realistic, Diverse Intracranial Aneurysm Morphologies using a Graph Transformer and Steady-Flow Data Augmentation",
    "abstract": "           Extensive studies suggested that fluid mechanical markers of intracranial aneurysms (IAs) derived from Computational Fluid Dynamics (CFD) can indicate disease progression risks, but to date this has not been translated clinically. This is because CFD requires specialized expertise and is time-consuming and low throughput, making it difficult to support clinical trials. A deep learning model that maps IA morphology to biomechanical markers can address this, enabling physicians to obtain these markers in real time without performing CFD. Here, we show that a Graph Transformer model that incorporates temporal information, which is supervised by large CFD data, can accurately predict Wall Shear Stress (WSS) across the cardiac cycle from IA surface meshes. The model effectively captures the temporal variations of the WSS pattern, achieving a Structural Similarity Index (SSIM) of up to 0.981 and a maximum-based relative L2 error of 2.8%. Ablation studies and SOTA comparison confirmed its optimality. Further, as pulsatile CFD data is computationally expensive to generate and sample sizes are limited, we engaged a strategy of injecting a large amount of steady-state CFD data, which are extremely low-cost to generate, as augmentation. This approach enhances network performance substantially when pulsatile CFD data sample size is small. Our study provides a proof of concept that temporal sequences cardiovascular fluid mechanical parameters can be computed in real time using a deep learning model from the geometric mesh, and this is achievable even with small pulsatile CFD sample size. Our approach is likely applicable to other cardiovascular scenarios.         ",
    "url": "https://arxiv.org/abs/2601.19876",
    "authors": [
      "Yiying Sheng",
      "Wenhao Ding",
      "Dylan Roi",
      "Leonard Leong Litt Yeo",
      "Hwa Liang Leo",
      "Choon Hwai Yap"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.20138",
    "title": "Scaling Next-Brain-Token Prediction for MEG",
    "abstract": "           We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2601.20138",
    "authors": [
      "Richard Csaky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2601.20174",
    "title": "NeuraLSP: An Efficient and Rigorous Neural Left Singular Subspace Preconditioner for Conjugate Gradient Methods",
    "abstract": "           Numerical techniques for solving partial differential equations (PDEs) are integral for many fields across science and engineering. Such techniques usually involve solving large, sparse linear systems, where preconditioning methods are critical. In recent years, neural methods, particularly graph neural networks (GNNs), have demonstrated their potential through accelerated convergence. Nonetheless, to extract connective structures, existing techniques aggregate discretized system matrices into graphs, and suffer from rank inflation and a suboptimal convergence rate. In this paper, we articulate NeuraLSP, a novel neural preconditioner combined with a novel loss metric that leverages the left singular subspace of the system matrix's near-nullspace vectors. By compressing spectral information into a fixed low-rank operator, our method exhibits both theoretical guarantees and empirical robustness to rank inflation, affording up to a 53% speedup. Besides the theoretical guarantees for our newly-formulated loss function, our comprehensive experimental results across diverse families of PDEs also substantiate the aforementioned theoretical advances.         ",
    "url": "https://arxiv.org/abs/2601.20174",
    "authors": [
      "Alexander Benanti",
      "Xi Han",
      "Hong Qin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2601.20433",
    "title": "MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models",
    "abstract": "           Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.         ",
    "url": "https://arxiv.org/abs/2601.20433",
    "authors": [
      "Wenbo Xu",
      "Wei Lu",
      "Xiangyang Luo",
      "Jiantao Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2601.20615",
    "title": "DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning",
    "abstract": "           Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.20615",
    "authors": [
      "Yanlin Wang",
      "Jiadong Wu",
      "Tianyue Jiang",
      "Mingwei Liu",
      "Jiachi Chen",
      "Chong Wang",
      "Ensheng Shi",
      "Xilin Liu",
      "Yuchi Ma",
      "Zibin Zheng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2601.20847",
    "title": "A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion",
    "abstract": "           Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.         ",
    "url": "https://arxiv.org/abs/2601.20847",
    "authors": [
      "Willams de Lima Costa",
      "Thifany Ketuli Silva de Souza",
      "Jonas Ferreira Silva",
      "Carlos Gabriel Bezerra Pereira",
      "Bruno Reis Vila Nova",
      "Leonardo Silvino Brito",
      "Rafael Raider Leoni",
      "Juliano Silva Filho",
      "Valter Ferreira",
      "Sibele Miguel Soares Neto",
      "Samantha Uehara",
      "Daniel Giacometti Amaral",
      "Jo\u00e3o Marcelo Teixeira",
      "Veronica Teichrieb",
      "Cristiano Coelho de Ara\u00fajo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2307.07657",
    "title": "Machine learning for option pricing: an empirical investigation of network architectures",
    "abstract": "           We consider the supervised learning problem of learning the price of an option or the implied volatility given appropriate input data (model parameters) and corresponding output data (option prices or implied volatilities). The majority of articles in this literature considers a (plain) feed forward neural network architecture in order to connect the neurons used for learning the function mapping inputs to outputs. In this article, motivated by methods in image classification and recent advances in machine learning methods for PDEs, we investigate empirically whether and how the choice of network architecture affects the accuracy and training time of a machine learning algorithm. We find that the generalized highway network architecture achieves the best performance, when considering the mean squared error and the training time as criteria, within the considered parameter budgets for the Black-Scholes and Heston option pricing problems. Considering the transformed implied volatility problem, a simplified DGM variant achieves the lowest error among the tested architectures. We also carry out a capacity-normalised comparison for completeness, where all architectures are evaluated with an equal number of parameters. Finally, for the implied volatility problem, we additionally include experiments using real market data.         ",
    "url": "https://arxiv.org/abs/2307.07657",
    "authors": [
      "Serena Della Corte",
      "Laurens Van Mieghem",
      "Antonis Papapantoleon",
      "Jonas Papazoglou-Hennig"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2312.11232",
    "title": "Scale-Equivariant Imaging: Self-Supervised Learning for Image Super-Resolution and Deblurring",
    "abstract": "           Self-supervised methods have recently proved to be nearly as effective as supervised ones in various imaging inverse problems, paving the way for learning-based approaches in scientific and medical imaging applications where ground truth data is hard or expensive to obtain. These methods critically rely on invariance to translations and/or rotations of the image distribution to learn from incomplete measurement data alone. However, existing approaches fail to obtain competitive performances in the problems of image super-resolution and deblurring, which play a key role in most imaging systems. In this work, we show that invariance to roto-translations is insufficient to learn from measurements that only contain low-frequency information. Instead, we propose scale-equivariant imaging, a new self-supervised approach that leverages the fact that many image distributions are approximately scale-invariant, enabling the recovery of high-frequency information lost in the measurement process. We demonstrate throughout a series of experiments on real datasets that the proposed method outperforms other self-supervised approaches, and obtains performances on par with fully supervised learning.         ",
    "url": "https://arxiv.org/abs/2312.11232",
    "authors": [
      "J\u00e9r\u00e9my Scanvic",
      "Mike Davies",
      "Patrice Abry",
      "Juli\u00e1n Tachella"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19720",
    "title": "Performance bound analysis of linear consensus algorithm on strongly connected graphs using effective resistance and reversiblization",
    "abstract": "           We study the performance of the linear consensus algorithm on strongly connected directed graphs using the linear quadratic (LQ) cost as a performance measure. In particular, we derive bounds on the LQ cost by leveraging effective resistance and reversiblization. Our results extend previous analyses-which were limited to reversible cases-to the nonreversible setting. To facilitate this generalization, we introduce novel concepts, termed the back-and-forth path and the pivot node, which serve as effective alternatives to traditional techniques that require reversibility. Moreover, we apply our approach to Cayley graphs and random geometric graphs to estimate the LQ cost without the reversibility assumption. The proposed approach provides a framework that can be adapted to other contexts where reversibility is typically assumed.         ",
    "url": "https://arxiv.org/abs/2502.19720",
    "authors": [
      "Takumi Yonaiyama",
      "Kazuhiro Sato"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2505.05216",
    "title": "Do We Need EMA for Diffusion-Based Speech Enhancement? Toward a Magnitude-Preserving Network Architecture",
    "abstract": "           We study diffusion-based speech enhancement using a Schrodinger bridge formulation and extend the EDM2 framework to this setting. We employ time-dependent preconditioning of network inputs and outputs to stabilize training and explore two skip-connection configurations that allow the network to predict either environmental noise or clean speech. To control activation and weight magnitudes, we adopt a magnitude-preserving architecture and learn the contribution of the noisy input within each network block for improved conditioning. We further analyze the impact of exponential moving average (EMA) parameter smoothing by approximating different EMA profiles post training, finding that, unlike in image generation, short or absent EMA consistently yields better speech enhancement performance. Experiments on VoiceBank-DEMAND and EARS-WHAM demonstrate competitive signal-to-distortion ratios and perceptual scores, with the two skip-connection variants exhibiting complementary strengths. These findings provide new insights into EMA behavior, magnitude preservation, and skip-connection design for diffusion-based speech enhancement.         ",
    "url": "https://arxiv.org/abs/2505.05216",
    "authors": [
      "Julius Richter",
      "Danilo de Oliveira",
      "Timo Gerkmann"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2507.05303",
    "title": "The neural networks with tensor weights and emergent fermionic Wick rules in the large-width limit",
    "abstract": "           In this paper, we study complex-valued neural network (CVNNs) with tensor-valued hidden-to-output weights within the framework of neural-network quantum field theory (NN-QFT). For standard CVNNs with scalar weights, we derive the generating functional and identify the exact Gaussian process that arises in the infinite-width limit, together with its associated effective quantum state. When the last-layer weights are promoted to Clifford-algebra-valued tensors, the network output becomes complex matrix-valued, and a fermion-like sign structure in the large-width correlation functions of the network output is induced. We show that, in the infinite-width limit, correlators with equal numbers of $f^\u2020$ and $f$ obey fermionic Wick rules and can be written as determinants built from a scalar Euclidean kernel $S(x,y)=\\langle f^\u2020(x)f(y)\\rangle$. This provides a sign-structured extension of NN-QFT at the level of Euclidean correlators and Feynman rules, even though a microscopic Grassmann path integral representation for the network parameters has not yet been constructed. Our analysis thus pushes the NN-QFT correspondence beyond purely bosonic Gaussian fields and suggests a possible route to encoding fermion-like symmetries in neural architectures for QFT correspondence.         ",
    "url": "https://arxiv.org/abs/2507.05303",
    "authors": [
      "Guojun Huang",
      "Kai Zhou"
    ],
    "subjectives": [
      "High Energy Physics - Theory (hep-th)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Phenomenology (hep-ph)"
    ]
  },
  {
    "id": "arXiv:2507.23707",
    "title": "Cellular, Cell-less, and Everything in Between: A Unified Framework for Utility Region Analysis in Wireless Networks",
    "abstract": "           We introduce a unified framework for analyzing utility regions of wireless networks, with a focus on signal-to-interference-plus-noise-ratio (SINR) and achievable rate regions. The framework provides valuable insights into interference patterns of modern network architectures, including extremely large MIMO and cell-less networks. A central contribution is a simple characterization of feasible utility regions using the concept of spectral radius of nonlinear mappings. This characterization provides a powerful mathematical tool for wireless system design and analysis. For example, it allows us to generalize existing characterizations of the weak Pareto boundary using compact notation. It also allows us to derive tractable sufficient conditions for the identification of convex utility regions. This property is particularly important because, on the weak Pareto boundary, it guarantees that time sharing (or user grouping) cannot simultaneously improve the utilities of all users. Beyond geometrical insights, these sufficient conditions have two key implications. First, they identify a family of (weighted) sum-rate maximization problems that are inherently convex, thus paving the way for the development of efficient, provably optimal solvers for this family. Second, they provide justification for formulating sum-rate maximization problems directly in terms of achievable rates, rather than SINR levels. Our theoretical insights also motivate an alternative to the concept of favorable propagation in the massive MIMO literature -- one that explicitly accounts for self-interference and the beamforming strategy.         ",
    "url": "https://arxiv.org/abs/2507.23707",
    "authors": [
      "Renato Luis Garrido Cavalcante",
      "Tomasz Piotrowski",
      "Slawomir Stanczak"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2509.22740",
    "title": "Learning What To Hear: Boosting Sound-Source Association For Robust Audiovisual Instance Segmentation",
    "abstract": "           Audiovisual instance segmentation (AVIS) requires accurately localizing and tracking sounding objects throughout video sequences. Existing methods suffer from visual bias stemming from two fundamental issues: uniform additive fusion prevents queries from specializing to different sound sources, while visual-only training objectives allow queries to converge to arbitrary salient objects. We propose Audio-Centric Query Generation using cross-attention, enabling each query to selectively attend to distinct sound sources and carry sound-specific priors into visual decoding. Additionally, we introduce Sound-Aware Ordinal Counting (SAOC) loss that explicitly supervises sounding object numbers through ordinal regression with monotonic consistency constraints, preventing visual-only convergence during training. Experiments on AVISeg benchmark demonstrate consistent improvements: +1.64 mAP, +0.6 HOTA, and +2.06 FSLA, validating that query specialization and explicit counting supervision are crucial for accurate audiovisual instance segmentation.         ",
    "url": "https://arxiv.org/abs/2509.22740",
    "authors": [
      "Jinbae Seo",
      "Hyeongjun Kwon",
      "Kwonyoung Kim",
      "Jiyoung Lee",
      "Kwanghoon Sohn"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2510.00463",
    "title": "On the Adversarial Robustness of Learning-based Conformal Novelty Detection",
    "abstract": "           This paper studies the adversarial robustness of conformal novelty detection. In particular, we focus on two powerful learning-based frameworks that come with finite-sample false discovery rate (FDR) control: one is AdaDetect (by Marandon et al., 2024) that is based on the positive-unlabeled classifier, and the other is a one-class classifier-based approach (by Bates et al., 2023). While they provide rigorous statistical guarantees under benign conditions, their behavior under adversarial perturbations remains underexplored. We first formulate an oracle attack setup, under the AdaDetect formulation, that quantifies the worst-case degradation of FDR, deriving an upper bound that characterizes the statistical cost of attacks. This idealized formulation directly motivates a practical and effective attack scheme that only requires query access to the output labels of both frameworks. Coupling these formulations with two popular and complementary black-box adversarial algorithms, we systematically evaluate the vulnerability of both frameworks on synthetic and real-world datasets. Our results show that adversarial perturbations can significantly increase the FDR while maintaining high detection power, exposing fundamental limitations of current error-controlled novelty detection methods and motivating the development of more robust alternatives.         ",
    "url": "https://arxiv.org/abs/2510.00463",
    "authors": [
      "Daofu Zhang",
      "Mehrdad Pournaderi",
      "Hanne M. Clifford",
      "Yu Xiang",
      "Pramod K. Varshney"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2512.04745",
    "title": "Neural Policy Composition from Free Energy Minimization",
    "abstract": "           The ability to compose acquired skills to plan and execute behaviors is a hallmark of natural intelligence. Yet, despite remarkable cross-disciplinary efforts, a principled account of how task structure shapes gating and how such computations could be delivered in neural circuits, remains elusive. Here we introduce GateMod, an interpretable theoretically grounded computational model linking the emergence of gating to the underlying decision-making task, and to a neural circuit architecture. We first develop GateFrame, a normative framework casting policy gating into the minimization of the free energy. This framework, relating gating rules to task, applies broadly across neuroscience, cognitive and computational sciences. We then derive GateFlow, a continuous-time energy based dynamics that provably converges to GateFrame optimal solution. Convergence, exponential and global, follows from a contractivity property that also yields robustness and other desirable properties. Finally, we derive a neural circuit from GateFlow, GateNet. This is a soft-competitive recurrent circuit whose components perform local and contextual computations consistent with known dendritic and neural processing motifs. We evaluate GateMod across two different settings: collective behaviors in multi-agent systems and human decision-making in multi-armed bandits. In all settings, GateMod provides interpretable mechanistic explanations of gating and quantitatively matches or outperforms established models. GateMod offers a unifying framework for neural policy gating, linking task objectives, dynamical computation, and circuit-level mechanisms. It provides a framework to understand gating in natural agents beyond current explanations and to equip machines with this ability.         ",
    "url": "https://arxiv.org/abs/2512.04745",
    "authors": [
      "Francesca Rossi",
      "Veronica Centorrino",
      "Francesco Bullo",
      "Giovanni Russo"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ]
  },
  {
    "id": "arXiv:2601.02241",
    "title": "From Mice to Trains: Amortized Bayesian Inference on Graph Data",
    "abstract": "           Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.         ",
    "url": "https://arxiv.org/abs/2601.02241",
    "authors": [
      "Svenja Jedhoff",
      "Elizaveta Semenova",
      "Aura Raulo",
      "Anne Meyer",
      "Paul-Christian B\u00fcrkner"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2601.19946",
    "title": "MK-SGC-SC: Multiple Kernel Guided Sparse Graph Construction in Spectral Clustering for Unsupervised Speaker Diarization",
    "abstract": "           Speaker diarization aims to segment audio recordings into regions corresponding to individual speakers. Although unsupervised speaker diarization is inherently challenging, the prospect of identifying speaker regions without pretraining or weak supervision motivates research on clustering techniques. In this work, we share the notable observation that measuring multiple kernel similarities of speaker embeddings to thereafter craft a sparse graph for spectral clustering in a principled manner is sufficient to achieve state-of-the-art performances in a fully unsupervised setting. Specifically, we consider four polynomial kernels and a degree one arccosine kernel to measure similarities in speaker embeddings, using which sparse graphs are constructed in a principled manner to emphasize local similarities. Experiments show the proposed approach excels in unsupervised speaker diarization over a variety of challenging environments in the DIHARD-III, AMI, and VoxConverse corpora. To encourage further research, our implementations are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2601.19946",
    "authors": [
      "Nikhil Raghav",
      "Avisek Gupta",
      "Swagatam Das",
      "Md Sahidullah"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  }
]