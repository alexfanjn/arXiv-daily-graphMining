[
  {
    "id": "arXiv:2206.01239",
    "title": "Design and evaluation of a cognitive approach for disseminating semantic  knowledge and content in opportunistic networks",
    "abstract": "In cyber-physical convergence scenarios information flows seamlessly between the physical and the cyber worlds. Here, users' mobile devices represent a natural bridge through which users process acquired information and perform actions. The sheer amount of data available in this context calls for novel, autonomous and lightweight data-filtering solutions, where only relevant information is finally presented to users. Moreover, in many real-world scenarios data is not categorised in predefined topics, but it is generally accompanied by semantic descriptions possibly describing users' interests. In these complex conditions, user devices should autonomously become aware not only of the existence of data in the network, but also of their semantic descriptions and correlations between them. To tackle these issues, we present a set of algorithms for knowledge and data dissemination in opportunistic networks, based on simple and very effective models (called cognitive heuristics) coming from cognitive sciences. We show how to exploit them to disseminate both semantic data and the corresponding data items. We provide a thorough performance analysis, under various different conditions comparing our results against non-cognitive solutions. Simulation results demonstrate the superior performance of our solution towards a more effective semantic knowledge acquisition and representation, and a more tailored content acquisition. ",
    "url": "https://arxiv.org/abs/2206.01239",
    "authors": [
      "Matteo Mordacchini",
      "Lorenzo Valerio",
      "Marco Conti",
      "Andrea Passarella"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2206.01251",
    "title": "Expressiveness and Learnability: A Unifying View for Evaluating  Self-Supervised Learning",
    "abstract": "We propose a unifying view to analyze the representation quality of self-supervised learning (SSL) models without access to supervised labels, while being agnostic to the architecture, learning algorithm or data manipulation used during training. We argue that representations can be evaluated through the lens of expressiveness and learnability. We propose to use the Intrinsic Dimension (ID) to assess expressiveness and introduce Cluster Learnability (CL) to assess learnability. CL is measured as the learning speed of a KNN classifier trained to predict labels obtained by clustering the representations with K-means. We thus combine CL and ID into a single predictor: CLID. Through a large-scale empirical study with a diverse family of SSL algorithms, we find that CLID better correlates with in-distribution model performance than other competing recent evaluation schemes. We also benchmark CLID on out-of-domain generalization, where CLID serves as a predictor of the transfer performance of SSL models on several classification tasks, yielding improvements with respect to the competing baselines. ",
    "url": "https://arxiv.org/abs/2206.01251",
    "authors": [
      "Yuchen Lu",
      "Zhen Liu",
      "Aristide Baratin",
      "Romain Laroche",
      "Aaron Courville",
      "Alessandro Sordoni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.01266",
    "title": "Exponential Separations in Symmetric Neural Networks",
    "abstract": "In this work we demonstrate a novel separation between symmetric neural network architectures. Specifically, we consider the Relational Network~\\parencite{santoro2017simple} architecture as a natural generalization of the DeepSets~\\parencite{zaheer2017deep} architecture, and study their representational gap. Under the restriction to analytic activation functions, we construct a symmetric function acting on sets of size $N$ with elements in dimension $D$, which can be efficiently approximated by the former architecture, but provably requires width exponential in $N$ and $D$ for the latter. ",
    "url": "https://arxiv.org/abs/2206.01266",
    "authors": [
      "Aaron Zweig",
      "Joan Bruna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01272",
    "title": "Data-Driven Linear Koopman Embedding for Model-Predictive Power System  Control",
    "abstract": "This paper presents a linear Koopman embedding for model predictive emergency voltage regulation in power systems, by way of a data-driven lifting of the system dynamics into a higher dimensional linear space over which the MPC (model predictive control) is exercised, thereby scaling as well as expediting the MPC computation for its real-time implementation for practical systems. We develop a {\\em Koopman-inspired deep neural network} (KDNN) architecture for the linear embedding of the voltage dynamics subjected to reactive controls. The training of the KDNN for the purposes of linear embedding is done using the simulated voltage trajectories under a variety of applied control inputs and load conditions. The proposed framework learns the underlying system dynamics from the input/output data in the form of a triple of transforms: A Neural Network (NN)-based lifting to a higher dimension, a linear dynamics within that higher dynamics, and an NN-based projection to original space. This approach alleviates the burden of an ad-hoc selection of the basis functions for the purposes of lifting to higher dimensional linear space. The MPC is computed over the linear dynamics, making the control computation scalable and also real-time. ",
    "url": "https://arxiv.org/abs/2206.01272",
    "authors": [
      "Ramij R. Hossain",
      "Rahmat Adesunkanmi",
      "Ratnesh Kumar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2206.01278",
    "title": "Lottery Tickets on a Data Diet: Finding Initializations with Sparse  Trainable Networks",
    "abstract": "A striking observation about iterative magnitude pruning (IMP; Frankle et al. 2020) is that $\\unicode{x2014}$ after just a few hundred steps of dense training $\\unicode{x2014}$ the method can find a sparse sub-network that can be trained to the same accuracy as the dense network. However, the same does not hold at step 0, i.e. random initialization. In this work, we seek to understand how this early phase of pre-training leads to a good initialization for IMP both through the lens of the data distribution and the loss landscape geometry. Empirically we observe that, holding the number of pre-training iterations constant, training on a small fraction of (randomly chosen) data suffices to obtain an equally good initialization for IMP. We additionally observe that by pre-training only on \"easy\" training data, we can decrease the number of steps necessary to find a good initialization for IMP compared to training on the full dataset or a randomly chosen subset. Finally, we identify novel properties of the loss landscape of dense networks that are predictive of IMP performance, showing in particular that more examples being linearly mode connected in the dense network correlates well with good initializations for IMP. Combined, these results provide new insight into the role played by the early phase training in IMP. ",
    "url": "https://arxiv.org/abs/2206.01278",
    "authors": [
      "Mansheej Paul",
      "Brett W. Larsen",
      "Surya Ganguli",
      "Jonathan Frankle",
      "Gintare Karolina Dziugaite"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.01281",
    "title": "Sparx: Distributed Outlier Detection at Scale",
    "abstract": "There is no shortage of outlier detection (OD) algorithms in the literature, yet a vast body of them are designed for a single machine. With the increasing reality of already cloud-resident datasets comes the need for distributed OD techniques. This area, however, is not only understudied but also short of public-domain implementations for practical use. This paper aims to fill this gap: We design Sparx, a data-parallel OD algorithm suitable for shared-nothing infrastructures, which we specifically implement in Apache Spark. Through extensive experiments on three real-world datasets, with several billions of points and millions of features, we show that existing open-source solutions fail to scale up; either by large number of points or high dimensionality, whereas Sparx yields scalable and effective performance. To facilitate practical use of OD on modern-scale datasets, we open-source Sparx under the Apache license at https://tinyurl.com/sparx2022. ",
    "url": "https://arxiv.org/abs/2206.01281",
    "authors": [
      "Sean Zhang",
      "Varun Ursekar",
      "Leman Akoglu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2206.01288",
    "title": "Decentralized Training of Foundation Models in Heterogeneous  Environments",
    "abstract": "Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational \"tasklets\" in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8X faster than prior state-of-the-art training systems (Megatron). ",
    "url": "https://arxiv.org/abs/2206.01288",
    "authors": [
      "Binhang Yuan",
      "Yongjun He",
      "Jared Quincy Davis",
      "Tianyi Zhang",
      "Tri Dao",
      "Beidi Chen",
      "Percy Liang",
      "Christopher Re",
      "Ce Zhang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01290",
    "title": "Points2NeRF: Generating Neural Radiance Fields from 3D point cloud",
    "abstract": "Contemporary registration devices for 3D visual information, such as LIDARs and various depth cameras, capture data as 3D point clouds. In turn, such clouds are challenging to be processed due to their size and complexity. Existing methods address this problem by fitting a mesh to the point cloud and rendering it instead. This approach, however, leads to the reduced fidelity of the resulting visualization and misses color information of the objects crucial in computer graphics applications. In this work, we propose to mitigate this challenge by representing 3D objects as Neural Radiance Fields (NeRFs). We leverage a hypernetwork paradigm and train the model to take a 3D point cloud with the associated color values and return a NeRF network's weights that reconstruct 3D objects from input 2D images. Our method provides efficient 3D object representation and offers several advantages over the existing approaches, including the ability to condition NeRFs and improved generalization beyond objects seen in training. The latter we also confirmed in the results of our empirical evaluation. ",
    "url": "https://arxiv.org/abs/2206.01290",
    "authors": [
      "D. Zimny",
      "T. Trzci\u0144ski",
      "P. Spurek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.01298",
    "title": "PNODE: A memory-efficient neural ODE framework based on high-level  adjoint differentiation",
    "abstract": "Neural ordinary differential equations (neural ODEs) have emerged as a novel network architecture that bridges dynamical systems and deep learning. However, the gradient obtained with the continuous adjoint method in the vanilla neural ODE is not reverse-accurate. Other approaches suffer either from excessive memory requirement due to deep computational graphs or from limited choices for the time integration scheme, hampering their application to large-scale complex dynamical systems. To achieve accurate gradients without compromising memory efficiency and flexibility, we present a new neural ODE framework, PNODE, based on high-level discrete adjoint algorithmic differentiation. By leveraging discrete adjoint time integrators and advanced checkpointing strategies tailored for these integrators, PNODE can provide a balance between memory and computational costs, while computing the gradients consistently and accurately. We provide an open-source implementation based on PyTorch and PETSc, one of the most commonly used portable, scalable scientific computing libraries. We demonstrate the performance through extensive numerical experiments on image classification and continuous normalizing flow problems. We show that PNODE achieves the highest memory efficiency when compared with other reverse-accurate methods. On the image classification problems, PNODE is up to two times faster than the vanilla neural ODE and up to 2.3 times faster than the best existing reverse-accurate method. We also show that PNODE enables the use of the implicit time integration methods that are needed for stiff dynamical systems. ",
    "url": "https://arxiv.org/abs/2206.01298",
    "authors": [
      "Hong Zhang",
      "Wenjun Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01299",
    "title": "Fine-tuning Language Models over Slow Networks using Activation  Compression with Guarantees",
    "abstract": "Communication compression is a crucial technique for modern distributed learning systems to alleviate their communication bottlenecks over slower networks. Despite recent intensive studies of gradient compression for data parallel-style training, compressing the activations for models trained with pipeline parallelism is still an open problem. In this paper, we propose AC-SGD, a novel activation compression algorithm for communication-efficient pipeline parallelism training over slow networks. Different from previous efforts in activation compression, instead of compressing activation values directly, AC-SGD compresses the changes of the activations. This allows us to show, to the best of our knowledge for the first time, that one can still achieve $O(1/\\sqrt{T})$ convergence rate for non-convex objectives under activation compression, without making assumptions on gradient unbiasedness that do not hold for deep learning models with non-linear activation functions.We then show that AC-SGD can be optimized and implemented efficiently, without additional end-to-end runtime overhead.We evaluated AC-SGD to fine-tune language models with up to 1.5 billion parameters, compressing activations to 2-4 bits.AC-SGD provides up to 4.3X end-to-end speed-up in slower networks, without sacrificing model quality. Moreover, we also show that AC-SGD can be combined with state-of-the-art gradient compression algorithms to enable \"end-to-end communication compression: All communications between machines, including model gradients, forward activations, and backward gradients are compressed into lower precision.This provides up to 4.9X end-to-end speed-up, without sacrificing model quality. ",
    "url": "https://arxiv.org/abs/2206.01299",
    "authors": [
      "Jue Wang",
      "Binhang Yuan",
      "Luka Rimanic",
      "Yongjun He",
      "Tri Dao",
      "Beidi Chen",
      "Christopher Re",
      "Ce Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2206.01312",
    "title": "Optimization of Energy-Constrained IRS-NOMA Using a Complex Circle  Manifold Approach",
    "abstract": "This work investigates the performance of intelligent reflective surfaces (IRSs) assisted uplink non-orthogonal multiple access (NOMA) in energy-constrained networks. Specifically, we formulate and solve two optimization problems, one for minimizing the users' sum transmit power and another for maximizing the energy efficiency (EE) of the system. The two problems are solved by jointly optimizing the users' transmit powers and the passive beamforming coefficients at the IRS reflectors subject to the users' individual uplink rate constraints. A novel algorithm is developed to optimize the IRS passive beamforming coefficients by optimizing the objective function over the \\textit{complex circle manifold} (CCM), exploiting the manifold optimization technique. The proposed manifold optimization-based solution is bench-marked against the rather \\textit{standard} semi-definite relaxation method (SDR). The results show that the manifold optimization-based algorithm achieves significantly better performance for both transmit power minimization and EE maximization problems at a computational complexity lower than the SDR approach. The results also reveal that IRS-NOMA is superior to the orthogonal multiple access (OMA) counterpart only when the users' target achievable rate requirements are relatively high. ",
    "url": "https://arxiv.org/abs/2206.01312",
    "authors": [
      "Mahmoud AlaaEldin",
      "Emad Alsusa",
      "Karim G. Seddik",
      "Constantinos B. Papadias",
      "Mohammad Al-Jarrah"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2206.01314",
    "title": "A New Security Boundary of Component Differentially Challenged XOR PUFs  Against Machine Learning Modeling Attacks",
    "abstract": "Physical Unclonable Functions (PUFs) are promising security primitives for resource-constrained network nodes. The XOR Arbiter PUF (XOR PUF or XPUF) is an intensively studied PUF invented to improve the security of the Arbiter PUF, probably the most lightweight delay-based PUF. Recently, highly powerful machine learning attack methods were discovered and were able to easily break large-sized XPUFs, which were highly secure against earlier machine learning attack methods. Component-differentially-challenged XPUFs (CDC-XPUFs) are XPUFs with different component PUFs receiving different challenges. Studies showed they were much more secure against machine learning attacks than the conventional XPUFs, whose component PUFs receive the same challenge. But these studies were all based on earlier machine learning attack methods, and hence it is not clear if CDC-XPUFs can remain secure under the recently discovered powerful attack methods. In this paper, the two current most powerful two machine learning methods for attacking XPUFs are adapted by fine-tuning the parameters of the two methods for CDC-XPUFs. Attack experiments using both simulated PUF data and silicon data generated from PUFs implemented on field-programmable gate array (FPGA) were carried out, and the experimental results showed that some previously secure CDC-XPUFs of certain circuit parameter values are no longer secure under the adapted new attack methods, while many more CDC-XPUFs of other circuit parameter values remain secure. Thus, our experimental attack study has re-defined the boundary between the secure region and the insecure region of the PUF circuit parameter space, providing PUF manufacturers and IoT security application developers with valuable information in choosing PUFs with secure parameter values. ",
    "url": "https://arxiv.org/abs/2206.01314",
    "authors": [
      "Gaoxiang Li",
      "Khalid T. Mursi",
      "Ahmad O. Aseeri",
      "Mohammed S. Alkatheiri",
      "Yu Zhuang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01335",
    "title": "Code Generation Tools (Almost) for Free? A Study of Few-Shot,  Pre-Trained Language Models on Code",
    "abstract": "Few-shot learning with large-scale, pre-trained language models is a powerful way to answer questions about code, e.g., how to complete a given code example, or even generate code snippets from scratch. The success of these models raises the question whether they could serve as a basis for building a wide range code generation tools. Traditionally, such tools are built manually and separately for each task. Instead, few-shot learning may allow to obtain different tools from a single pre-trained language model by simply providing a few examples or a natural language description of the expected tool behavior. This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose. We consider three code manipulation and code generation tasks targeted by a range of traditional tools: (i) code mutation; (ii) test oracle generation from natural language documentation; and (iii) test case generation. For each task, we compare few-shot learning to a manually built tool. Our results show that the model-based tools complement (code mutation), are on par (test oracle generation), or even outperform their respective traditionally built tool (test case generation), while imposing far less effort to develop them. By comparing the effectiveness of different variants of the model-based tools, we provide insights on how to design an appropriate input (\"prompt\") to the model and what influence the size of the model has. For example, we find that providing a small natural language description of the code generation task is an easy way to improve predictions. Overall, we conclude that few-shot language models are surprisingly effective, yet there is still more work to be done, such as exploring more diverse ways of prompting and tackling even more involved tasks. ",
    "url": "https://arxiv.org/abs/2206.01335",
    "authors": [
      "Patrick Barei\u00df",
      "Beatriz Souza",
      "Marcelo d'Amorim",
      "Michael Pradel"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01349",
    "title": "On the Privacy Properties of GAN-generated Samples",
    "abstract": "The privacy implications of generative adversarial networks (GANs) are a topic of great interest, leading to several recent algorithms for training GANs with privacy guarantees. By drawing connections to the generalization properties of GANs, we prove that under some assumptions, GAN-generated samples inherently satisfy some (weak) privacy guarantees. First, we show that if a GAN is trained on m samples and used to generate n samples, the generated samples are (epsilon, delta)-differentially-private for (epsilon, delta) pairs where delta scales as O(n/m). We show that under some special conditions, this upper bound is tight. Next, we study the robustness of GAN-generated samples to membership inference attacks. We model membership inference as a hypothesis test in which the adversary must determine whether a given sample was drawn from the training dataset or from the underlying data distribution. We show that this adversary can achieve an area under the ROC curve that scales no better than O(m^{-1/4}). ",
    "url": "https://arxiv.org/abs/2206.01349",
    "authors": [
      "Zinan Lin",
      "Vyas Sekar",
      "Giulia Fanti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2206.01365",
    "title": "Adversarial Attacks on Human Vision",
    "abstract": "This article presents an introduction to visual attention retargeting, its connection to visual saliency, the challenges associated with it, and ideas for how it can be approached. The difficulty of attention retargeting as a saliency inversion problem lies in the lack of one-to-one mapping between saliency and the image domain, in addition to the possible negative impact of saliency alterations on image aesthetics. A few approaches from recent literature to solve this challenging problem are reviewed, and several suggestions for future development are presented. ",
    "url": "https://arxiv.org/abs/2206.01365",
    "authors": [
      "Victor A. Mateescu",
      "Ivan V. Baji\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2206.01367",
    "title": "Adversarial Unlearning: Reducing Confidence Along Adversarial Directions",
    "abstract": "Supervised learning methods trained with maximum likelihood objectives often overfit on training data. Most regularizers that prevent overfitting look to increase confidence on additional examples (e.g., data augmentation, adversarial training), or reduce it on training data (e.g., label smoothing). In this work we propose a complementary regularization strategy that reduces confidence on self-generated examples. The method, which we call RCAD (Reducing Confidence along Adversarial Directions), aims to reduce confidence on out-of-distribution examples lying along directions adversarially chosen to increase training loss. In contrast to adversarial training, RCAD does not try to robustify the model to output the original label, but rather regularizes it to have reduced confidence on points generated using much larger perturbations than in conventional adversarial training. RCAD can be easily integrated into training pipelines with a few lines of code. Despite its simplicity, we find on many classification benchmarks that RCAD can be added to existing techniques (e.g., label smoothing, MixUp training) to increase test accuracy by 1-3% in absolute value, with more significant gains in the low data regime. We also provide a theoretical analysis that helps to explain these benefits in simplified settings, showing that RCAD can provably help the model unlearn spurious features in the training data. ",
    "url": "https://arxiv.org/abs/2206.01367",
    "authors": [
      "Amrith Setlur",
      "Benjamin Eysenbach",
      "Virginia Smith",
      "Sergey Levine"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2206.01379",
    "title": "Instant Graph Neural Networks for Dynamic Graphs",
    "abstract": "Graph Neural Networks (GNNs) have been widely used for modeling graph-structured data. With the development of numerous GNN variants, recent years have witnessed groundbreaking results in improving the scalability of GNNs to work on static graphs with millions of nodes. However, how to instantly represent continuous changes of large-scale dynamic graphs with GNNs is still an open problem. Existing dynamic GNNs focus on modeling the periodic evolution of graphs, often on a snapshot basis. Such methods suffer from two drawbacks: first, there is a substantial delay for the changes in the graph to be reflected in the graph representations, resulting in losses on the model's accuracy; second, repeatedly calculating the representation matrix on the entire graph in each snapshot is predominantly time-consuming and severely limits the scalability. In this paper, we propose Instant Graph Neural Network (InstantGNN), an incremental computation approach for the graph representation matrix of dynamic graphs. Set to work with dynamic graphs with the edge-arrival model, our method avoids time-consuming, repetitive computations and allows instant updates on the representation and instant predictions. Graphs with dynamic structures and dynamic attributes are both supported. The upper bounds of time complexity of those updates are also provided. Furthermore, our method provides an adaptive training strategy, which guides the model to retrain at moments when it can make the greatest performance gains. We conduct extensive experiments on several real-world and synthetic datasets. Empirical results demonstrate that our model achieves state-of-the-art accuracy while having orders-of-magnitude higher efficiency than existing methods. ",
    "url": "https://arxiv.org/abs/2206.01379",
    "authors": [
      "Yanping Zheng",
      "Hanzhi Wang",
      "Zhewei Wei",
      "Jiajun Liu",
      "Sibo Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01381",
    "title": "CF-YOLO: Cross Fusion YOLO for Object Detection in Adverse Weather with  a High-quality Real Snow Dataset",
    "abstract": "Snow is one of the toughest adverse weather conditions for object detection (OD). Currently, not only there is a lack of snowy OD datasets to train cutting-edge detectors, but also these detectors have difficulties learning latent information beneficial for detection in snow. To alleviate the two above problems, we first establish a real-world snowy OD dataset, named RSOD. Besides, we develop an unsupervised training strategy with a distinctive activation function, called $Peak \\ Act$, to quantitatively evaluate the effect of snow on each object. Peak Act helps grading the images in RSOD into four-difficulty levels. To our knowledge, RSOD is the first quantitatively evaluated and graded snowy OD dataset. Then, we propose a novel Cross Fusion (CF) block to construct a lightweight OD network based on YOLOv5s (call CF-YOLO). CF is a plug-and-play feature aggregation module, which integrates the advantages of Feature Pyramid Network and Path Aggregation Network in a simpler yet more flexible form. Both RSOD and CF lead our CF-YOLO to possess an optimization ability for OD in real-world snow. That is, CF-YOLO can handle unfavorable detection problems of vagueness, distortion and covering of snow. Experiments show that our CF-YOLO achieves better detection results on RSOD, compared to SOTAs. The code and dataset are available at https://github.com/qqding77/CF-YOLO-and-RSOD. ",
    "url": "https://arxiv.org/abs/2206.01381",
    "authors": [
      "Qiqi Ding",
      "Peng Li",
      "Xuefeng Yan",
      "Ding Shi",
      "Luming Liang",
      "Weiming Wang",
      "Haoran Xie",
      "Jonathan Li",
      "Mingqiang Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.01402",
    "title": "Data Encryption based on 9D Complex Chaotic System with Quaternion for  Smart Grid",
    "abstract": "With the development of smart grid, the operation and control of power system is realized through power communication network, especially the power production and enterprise management business involve a large amount of sensitive information, and the requirements for data security and real-time transmission are gradually improved. In this paper, a new 9D complex chaotic system with quaternion is proposed for the encryption of smart grid data. Firstly, a new 9D complex chaotic system with quaternion is proposed, and its attractors, bifurcation diagram, complexity, and 0-1 test are analyzed. Secondly, the pseudo-random sequences are generated by the new chaotic system to encrypt power data. Finally, the proposed encryption algorithm is verifed with power data and images in the smart grid, which can ensure the encryption security and real-time. The verifcation results show that the proposed encryption scheme is technically feasible and available for power data and image encryption in smart grid. ",
    "url": "https://arxiv.org/abs/2206.01402",
    "authors": [
      "Fangfang Zhang",
      "Zhe Huang",
      "Lei Kou",
      "Yang Li",
      "Maoyong Cao",
      "Fengying Ma"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2206.01410",
    "title": "Fair Classification via Transformer Neural Networks: Case Study of an  Educational Domain",
    "abstract": "Educational technologies nowadays increasingly use data and Machine Learning (ML) models. This gives the students, instructors, and administrators support and insights for the optimum policy. However, it is well acknowledged that ML models are subject to bias, which raises concern about the fairness, bias, and discrimination of using these automated ML algorithms in education and its unintended and unforeseen negative consequences. The contribution of bias during the decision-making comes from datasets used for training ML models and the model architecture. This paper presents a preliminary investigation of fairness constraint in transformer neural networks on Law School and Student-Mathematics datasets. The used transformer models transform these raw datasets into a richer representation space of natural language processing (NLP) while solving fairness classification. We have employed fairness metrics for evaluation and check the trade-off between fairness and accuracy. We have reported the various metrics of F1, SPD, EOD, and accuracy for different architectures from the transformer model class. ",
    "url": "https://arxiv.org/abs/2206.01410",
    "authors": [
      "Modar Sulaiman",
      "Kallol Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.01413",
    "title": "Impact of the composition of feature extraction and class sampling in  medicare fraud detection",
    "abstract": "With healthcare being critical aspect, health insurance has become an important scheme in minimizing medical expenses. Following this, the healthcare industry has seen a significant increase in fraudulent activities owing to increased insurance, and fraud has become a significant contributor to rising medical care expenses, although its impact can be mitigated using fraud detection techniques. To detect fraud, machine learning techniques are used. The Centers for Medicaid and Medicare Services (CMS) of the United States federal government released \"Medicare Part D\" insurance claims is utilized in this study to develop fraud detection system. Employing machine learning algorithms on a class-imbalanced and high dimensional medicare dataset is a challenging task. To compact such challenges, the present work aims to perform feature extraction following data sampling, afterward applying various classification algorithms, to get better performance. Feature extraction is a dimensionality reduction approach that converts attributes into linear or non-linear combinations of the actual attributes, generating a smaller and more diversified set of attributes and thus reducing the dimensions. Data sampling is commonlya used to address the class imbalance either by expanding the frequency of minority class or reducing the frequency of majority class to obtain approximately equal numbers of occurrences for both classes. The proposed approach is evaluated through standard performance metrics. Thus, to detect fraud efficiently, this study applies autoencoder as a feature extraction technique, synthetic minority oversampling technique (SMOTE) as a data sampling technique, and various gradient boosted decision tree-based classifiers as a classification algorithm. The experimental results show the combination of autoencoders followed by SMOTE on the LightGBM classifier achieved best results. ",
    "url": "https://arxiv.org/abs/2206.01413",
    "authors": [
      "Akrity Kumari",
      "Narinder Singh Punn",
      "Sanjay Kumar Sonbhadra",
      "Sonali Agarwal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01432",
    "title": "On the Generalization of Wasserstein Robust Federated Learning",
    "abstract": "In federated learning, participating clients typically possess non-i.i.d. data, posing a significant challenge to generalization to unseen distributions. To address this, we propose a Wasserstein distributionally robust optimization scheme called WAFL. Leveraging its duality, we frame WAFL as an empirical surrogate risk minimization problem, and solve it using a local SGD-based algorithm with convergence guarantees. We show that the robustness of WAFL is more general than related approaches, and the generalization bound is robust to all adversarial distributions inside the Wasserstein ball (ambiguity set). Since the center location and radius of the Wasserstein ball can be suitably modified, WAFL shows its applicability not only in robustness but also in domain adaptation. Through empirical evaluation, we demonstrate that WAFL generalizes better than the vanilla FedAvg in non-i.i.d. settings, and is more robust than other related methods in distribution shift settings. Further, using benchmark datasets we show that WAFL is capable of generalizing to unseen target domains. ",
    "url": "https://arxiv.org/abs/2206.01432",
    "authors": [
      "Tung-Anh Nguyen",
      "Tuan Dung Nguyen",
      "Long Tan Le",
      "Canh T. Dinh",
      "Nguyen H. Tran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2206.01451",
    "title": "Learning Distributed and Fair Policies for Network Load Balancing as  Markov Potentia Game",
    "abstract": "This paper investigates the network load balancing problem in data centers (DCs) where multiple load balancers (LBs) are deployed, using the multi-agent reinforcement learning (MARL) framework. The challenges of this problem consist of the heterogeneous processing architecture and dynamic environments, as well as limited and partial observability of each LB agent in distributed networking systems, which can largely degrade the performance of in-production load balancing algorithms in real-world setups. Centralised-training-decentralised-execution (CTDE) RL scheme has been proposed to improve MARL performance, yet it incurs -- especially in distributed networking systems, which prefer distributed and plug-and-play design scheme -- additional communication and management overhead among agents. We formulate the multi-agent load balancing problem as a Markov potential game, with a carefully and properly designed workload distribution fairness as the potential function. A fully distributed MARL algorithm is proposed to approximate the Nash equilibrium of the game. Experimental evaluations involve both an event-driven simulator and real-world system, where the proposed MARL load balancing algorithm shows close-to-optimal performance in simulations, and superior results over in-production LBs in the real-world system. ",
    "url": "https://arxiv.org/abs/2206.01451",
    "authors": [
      "Zhiyuan Yao",
      "Zihan Ding"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2206.01463",
    "title": "Safety Certification for Stochastic Systems via Neural Barrier Functions",
    "abstract": "Providing non-trivial certificates of safety for non-linear stochastic systems is an important open problem that limits the wider adoption of autonomous systems in safety-critical applications. One promising solution to address this problem is barrier functions. The composition of a barrier function with a stochastic system forms a supermartingale, thus enabling the computation of the probability that the system stays in a safe set over a finite time horizon via martingale inequalities. However, existing approaches to find barrier functions for stochastic systems generally rely on convex optimization programs that restrict the search of a barrier to a small class of functions such as low degree SoS polynomials and can be computationally expensive. In this paper, we parameterize a barrier function as a neural network and show that techniques for robust training of neural networks can be successfully employed to find neural barrier functions. Specifically, we leverage bound propagation techniques to certify that a neural network satisfies the conditions to be a barrier function via linear programming and then employ the resulting bounds at training time to enforce the satisfaction of these conditions. We also present a branch-and-bound scheme that makes the certification framework scalable. We show that our approach outperforms existing methods in several case studies and often returns certificates of safety that are orders of magnitude larger. ",
    "url": "https://arxiv.org/abs/2206.01463",
    "authors": [
      "Frederik Baymler Mathiesen",
      "Simeon Calvert",
      "Luca Laurenti"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01467",
    "title": "Evaluating Transfer-based Targeted Adversarial Perturbations against  Real-World Computer Vision Systems based on Human Judgments",
    "abstract": "Computer vision systems are remarkably vulnerable to adversarial perturbations. Transfer-based adversarial images are generated on one (source) system and used to attack another (target) system. In this paper, we take the first step to investigate transfer-based targeted adversarial images in a realistic scenario where the target system is trained on some private data with its inventory of semantic labels not publicly available. Our main contributions include an extensive human-judgment-based evaluation of attack success on the Google Cloud Vision API and additional analysis of the different behaviors of Google Cloud Vision in face of original images vs. adversarial images. Resources are publicly available at \\url{https://github.com/ZhengyuZhao/Targeted-Tansfer/blob/main/google_results.zip}. ",
    "url": "https://arxiv.org/abs/2206.01467",
    "authors": [
      "Zhengyu Zhao",
      "Nga Dang",
      "Martha Larson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01473",
    "title": "Distributional loss for convolutional neural network regression and  application to GNSS multi-path estimation",
    "abstract": "Convolutional Neural Network (CNN) have been widely used in image classification. Over the years, they have also benefited from various enhancements and they are now considered as state of the art techniques for image like data. However, when they are used for regression to estimate some function value from images, fewer recommendations are available. In this study, a novel CNN regression model is proposed. It combines convolutional neural layers to extract high level features representations from images with a soft labelling technique. More specifically, as the deep regression task is challenging, the idea is to account for some uncertainty in the targets that are seen as distributions around their mean. The estimations are carried out by the model in the form of distributions. Building from earlier work, a specific histogram loss function based on the Kullback-Leibler (KL) divergence is applied during training. The model takes advantage of the CNN feature representation and is able to carry out estimation from multi-channel input images. To assess and illustrate the technique, the model is applied to Global Navigation Satellite System (GNSS) multi-path estimation where multi-path signal parameters have to be estimated from correlator output images from the I and Q channels. The multi-path signal delay, magnitude, Doppler shift frequency and phase parameters are estimated from synthetically generated datasets of satellite signals. Experiments are conducted under various receiving conditions and various input images resolutions to test the estimation performances quality and robustness. The results show that the proposed soft labelling CNN technique using distributional loss outperforms classical CNN regression under all conditions. Furthermore, the extra learning performance achieved by the model allows the reduction of input image resolution from 80x80 down to 40x40 or sometimes 20x20. ",
    "url": "https://arxiv.org/abs/2206.01473",
    "authors": [
      "Thomas Gonzalez",
      "Antoine Blais",
      "Nicolas Cou\u00ebllan",
      "Christian Ruiz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2206.01474",
    "title": "Offline Reinforcement Learning with Causal Structured World Models",
    "abstract": "Model-based methods have recently shown promising for offline reinforcement learning (RL), aiming to learn good policies from historical data without interacting with the environment. Previous model-based offline RL methods learn fully connected nets as world-models that map the states and actions to the next-step states. However, it is sensible that a world-model should adhere to the underlying causal effect such that it will support learning an effective policy generalizing well in unseen states. In this paper, We first provide theoretical results that causal world-models can outperform plain world-models for offline RL by incorporating the causal structure into the generalization error bound. We then propose a practical algorithm, oFfline mOdel-based reinforcement learning with CaUsal Structure (FOCUS), to illustrate the feasibility of learning and leveraging causal structure in offline RL. Experimental results on two benchmarks show that FOCUS reconstructs the underlying causal structure accurately and robustly. Consequently, it performs better than the plain model-based offline RL algorithms and other causal model-based RL algorithms. ",
    "url": "https://arxiv.org/abs/2206.01474",
    "authors": [
      "Zheng-Mao Zhu",
      "Xiong-Hui Chen",
      "Hong-Long Tian",
      "Kun Zhang",
      "Yang Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.01483",
    "title": "Finding Rule-Interpretable Non-Negative Data Representation",
    "abstract": "Non-negative Matrix Factorization (NMF) is an intensively used technique for obtaining parts-based, lower dimensional and non-negative representation of non-negative data. It is a popular method in different research fields. Scientists performing research in the fields of biology, medicine and pharmacy often prefer NMF over other dimensionality reduction approaches (such as PCA) because the non-negativity of the approach naturally fits the characteristics of the domain problem and its result is easier to analyze and understand. Despite these advantages, it still can be hard to get exact characterization and interpretation of the NMF's resulting latent factors due to their numerical nature. On the other hand, rule-based approaches are often considered more interpretable but lack the parts-based interpretation. In this work, we present a version of the NMF approach that merges rule-based descriptions with advantages of part-based representation offered by the NMF approach. Given the numerical input data with non-negative entries and a set of rules with high entity coverage, the approach creates the lower-dimensional non-negative representation of the input data in such a way that its factors are described by the appropriate subset of the input rules. In addition to revealing important attributes for latent factors, it allows analyzing relations between these attributes and provides the exact numerical intervals or categorical values they take. The proposed approach provides numerous advantages in tasks such as focused embedding or performing supervised multi-label NMF. ",
    "url": "https://arxiv.org/abs/2206.01483",
    "authors": [
      "Matej Mihel\u010di\u0107",
      "Pauli Miettinen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01493",
    "title": "Transferring Studies Across Embodiments: A Case Study in Confusion  Detection",
    "abstract": "Human-robot studies are expensive to conduct and difficult to control, and as such researchers sometimes turn to human-avatar interaction in the hope of faster and cheaper data collection that can be transferred to the robot domain. In terms of our work, we are particularly interested in the challenge of detecting and modelling user confusion in interaction, and as part of this research programme, we conducted situated dialogue studies to investigate users' reactions in confusing scenarios that we give in both physical and virtual environments. In this paper, we present a combined review of these studies and the results that we observed across these two embodiments. For the physical embodiment, we used a Pepper Robot, while for the virtual modality, we used a 3D avatar. Our study shows that despite attitudinal differences and technical control limitations, there were a number of similarities detected in user behaviour and self-reporting results across embodiment options. This work suggests that, while avatar interaction is no true substitute for robot interaction studies, sufficient care in study design may allow well executed human-avatar studies to supplement more challenging human-robot studies. ",
    "url": "https://arxiv.org/abs/2206.01493",
    "authors": [
      "Na Li",
      "Robert Ross"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2206.01496",
    "title": "Causality Learning With Wasserstein Generative Adversarial Networks",
    "abstract": "Conventional methods for causal structure learning from data face significant challenges due to combinatorial search space. Recently, the problem has been formulated into a continuous optimization framework with an acyclicity constraint to learn Directed Acyclic Graphs (DAGs). Such a framework allows the utilization of deep generative models for causal structure learning to better capture the relations between data sample distributions and DAGs. However, so far no study has experimented with the use of Wasserstein distance in the context of causal structure learning. Our model named DAG-WGAN combines the Wasserstein-based adversarial loss with an acyclicity constraint in an auto-encoder architecture. It simultaneously learns causal structures while improving its data generation capability. We compare the performance of DAG-WGAN with other models that do not involve the Wasserstein metric in order to identify its contribution to causal structure learning. Our model performs better with high cardinality data according to our experiments. ",
    "url": "https://arxiv.org/abs/2206.01496",
    "authors": [
      "Hristo Petkov",
      "Colin Hanley",
      "Feng Dong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2206.01498",
    "title": "YOLOv5s-GTB: light-weighted and improved YOLOv5s for bridge crack  detection",
    "abstract": "In response to the situation that the conventional bridge crack manual detection method has a large amount of human and material resources wasted, this study is aimed to propose a light-weighted, high-precision, deep learning-based bridge apparent crack recognition model that can be deployed in mobile devices' scenarios. In order to enhance the performance of YOLOv5, firstly, the data augmentation methods are supplemented, and then the YOLOv5 series algorithm is trained to select a suitable basic framework. The YOLOv5s is identified as the basic framework for the light-weighted crack detection model through experiments for comparison and validation.By replacing the traditional DarkNet backbone network of YOLOv5s with GhostNet backbone network, introducing Transformer multi-headed self-attention mechanism and bi-directional feature pyramid network (BiFPN) to replace the commonly used feature pyramid network, the improved model not only has 42% fewer parameters and faster inference response, but also significantly outperforms the original model in terms of accuracy and mAP (8.5% and 1.1% improvement, respectively). Luckily each improved part has a positive impact on the result. This paper provides a feasible idea to establish a digital operation management system in the field of highway and bridge in the future and to implement the whole life cycle structure health monitoring of civil infrastructure in China. ",
    "url": "https://arxiv.org/abs/2206.01498",
    "authors": [
      "Xiao Ruiqiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.01506",
    "title": "Can Hybrid Geometric Scattering Networks Help Solve the Maximal Clique  Problem?",
    "abstract": "We propose a geometric scattering-based graph neural network (GNN) for approximating solutions of the NP-hard maximal clique (MC) problem. We construct a loss function with two terms, one which encourages the network to find a large set of nodes and the other which acts as a surrogate for the constraint that the nodes form a clique. We then use this loss to train a novel GNN architecture that outputs a vector representing the probability for each node to be part of the MC and apply a rule-based decoder to make our final prediction. The incorporation of the scattering transform alleviates the so-called oversmoothing problem that is often encountered in GNNs and would degrade the performance of our proposed setup. Our empirical results demonstrate that our method outperforms representative GNN baselines in terms of solution accuracy and inference speed as well as conventional solvers like GUROBI with limited time budgets. ",
    "url": "https://arxiv.org/abs/2206.01506",
    "authors": [
      "Yimeng Min",
      "Frederik Wenkel",
      "Michael Perlmutter",
      "Guy Wolf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01509",
    "title": "Canonical convolutional neural networks",
    "abstract": "We introduce canonical weight normalization for convolutional neural networks. Inspired by the canonical tensor decomposition, we express the weight tensors in so-called canonical networks as scaled sums of outer vector products. In particular, we train network weights in the decomposed form, where scale weights are optimized separately for each mode. Additionally, similarly to weight normalization, we include a global scaling parameter. We study the initialization of the canonical form by running the power method and by drawing randomly from Gaussian or uniform distributions. Our results indicate that we can replace the power method with cheaper initializations drawn from standard distributions. The canonical re-parametrization leads to competitive normalization performance on the MNIST, CIFAR10, and SVHN data sets. Moreover, the formulation simplifies network compression. Once training has converged, the canonical form allows convenient model-compression by truncating the parameter sums. ",
    "url": "https://arxiv.org/abs/2206.01509",
    "authors": [
      "Lokesh Veeramacheneni",
      "Moritz Wolter",
      "Reinhard Klein",
      "Jochen Garcke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01520",
    "title": "A Survey on Surrogate-assisted Efficient Neural Architecture Search",
    "abstract": "Neural architecture search (NAS) has become increasingly popular in the deep learning community recently, mainly because it can provide an opportunity to allow interested users without rich expertise to benefit from the success of deep neural networks (DNNs). However, NAS is still laborious and time-consuming because a large number of performance estimations are required during the search process of NAS, and training DNNs is computationally intensive. To solve the major limitation of NAS, improving the efficiency of NAS is essential in the design of NAS. This paper begins with a brief introduction to the general framework of NAS. Then, the methods for evaluating network candidates under the proxy metrics are systematically discussed. This is followed by a description of surrogate-assisted NAS, which is divided into three different categories, namely Bayesian optimization for NAS, surrogate-assisted evolutionary algorithms for NAS, and MOP for NAS. Finally, remaining challenges and open research questions are discussed, and promising research topics are suggested in this emerging field. ",
    "url": "https://arxiv.org/abs/2206.01520",
    "authors": [
      "Shiqing Liu",
      "Haoyu Zhang",
      "Yaochu Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2206.01523",
    "title": "A High-Performance Customer Churn Prediction System based on  Self-Attention",
    "abstract": "Customer churn prediction is a challenging domain of research that contributes to customer retention strategy. The predictive performance of existing machine learning models, which are often adopted by churn communities, appear to be at a bottleneck, partly due to models' poor feature extraction capability. Therefore, a novel algorithm, a hybrid neural network with self-attention enhancement (HNNSAE), is proposed in this paper to improve the efficiency of feature screening and feature extraction, consequently improving the model's predictive performance. This model consists of three main blocks. The first block is the entity embedding layer, which is employed to process the categorical variables transformed into 0-1 code. The second block is the feature extractor, which extracts the significant features through the multi-head self-attention mechanism. In addition, to improve the feature extraction effect, we stack the residual connection neural network on multi-head self-attention modules. The third block is a classifier, which is a three-layer multilayer perceptron. This work conducts experiments on publicly available dataset related to commercial bank customers. The result demonstrates that HNNSAE significantly outperforms the other Individual Machine Learning (IML), Ensemble Machine Learning (EML), and Deep Learning (DL) methods tested in this paper. Furthermore, we compare the performance of the feature extractor proposed in this paper with that of other three feature extractors and find that the method proposed in this paper significantly outperforms other methods. In addition, four hypotheses about model prediction performance and overfitting risk are tested on the publicly available dataset. ",
    "url": "https://arxiv.org/abs/2206.01523",
    "authors": [
      "Haotian Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01524",
    "title": "Anomaly detection in surveillance videos using transformer based  attention model",
    "abstract": "Surveillance footage can catch a wide range of realistic anomalies. This research suggests using a weakly supervised strategy to avoid annotating anomalous segments in training videos, which is time consuming. In this approach only video level labels are used to obtain frame level anomaly scores. Weakly supervised video anomaly detection (WSVAD) suffers from the wrong identification of abnormal and normal instances during the training process. Therefore it is important to extract better quality features from the available videos. WIth this motivation, the present paper uses better quality transformer-based features named Videoswin Features followed by the attention layer based on dilated convolution and self attention to capture long and short range dependencies in temporal domain. This gives us a better understanding of available videos. The proposed framework is validated on real-world dataset i.e. ShanghaiTech Campus dataset which results in competitive performance than current state-of-the-art methods. The model and the code are available at https://github.com/kapildeshpande/Anomaly-Detection-in-Surveillance-Videos ",
    "url": "https://arxiv.org/abs/2206.01524",
    "authors": [
      "Kapil Deshpande",
      "Narinder Singh Punn",
      "Sanjay Kumar Sonbhadra",
      "Sonali Agarwal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.01535",
    "title": "Rethinking and Scaling Up Graph Contrastive Learning: An Extremely  Efficient Approach with Group Discrimination",
    "abstract": "Graph contrastive learning (GCL) alleviates the heavy reliance on label information for graph representation learning (GRL) via self-supervised learning schemes. The core idea is to learn by maximising mutual information for similar instances, which requires similarity computation between two node instances. However, this operation can be computationally expensive. For example, the time complexity of two commonly adopted contrastive loss functions (i.e., InfoNCE and JSD estimator) for a node is $O(ND)$ and $O(D)$, respectively, where $N$ is the number of nodes, and $D$ is the embedding dimension. Additionally, GCL normally requires a large number of training epochs to be well-trained on large-scale datasets. Inspired by an observation of a technical defect (i.e., inappropriate usage of Sigmoid function) commonly used in two representative GCL works, DGI and MVGRL, we revisit GCL and introduce a new learning paradigm for self-supervised GRL, namely, Group Discrimination (GD), and propose a novel GD-based method called Graph Group Discrimination (GGD). Instead of similarity computation, GGD directly discriminates two groups of summarised node instances with a simple binary cross-entropy loss. As such, GGD only requires $O(1)$ for loss computation of a node. In addition, GGD requires much fewer training epochs to obtain competitive performance compared with GCL methods on large-scale datasets. These two advantages endow GGD with the very efficient property. Extensive experiments show that GGD outperforms state-of-the-art self-supervised methods on 8 datasets. In particular, GGD can be trained in 0.18 seconds (6.44 seconds including data preprocessing) on ogbn-arxiv, which is orders of magnitude (10,000+ faster than GCL baselines} while consuming much less memory. Trained with 9 hours on ogbn-papers100M with billion edges, GGD outperforms its GCL counterparts in both accuracy and efficiency. ",
    "url": "https://arxiv.org/abs/2206.01535",
    "authors": [
      "Yizhen Zheng",
      "Shirui Pan",
      "Vincent Cs Lee",
      "Yu Zheng",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.01541",
    "title": "A robust solution strategy for the Cahn-Larch\u00e9 equations",
    "abstract": "In this paper we propose a solution strategy for the Cahn-Larch\\'e equations, which is a model for linearized elasticity in a medium with two elastic phases that evolve subject to a Ginzburg-Landau type energy functional. The system can be seen as a combination of the Cahn-Hilliard regularized interface equation and linearized elasticity, and is non-linearly coupled, has a fourth order term that comes from the Cahn-Hilliard subsystem, and is non-convex and nonlinear in both the phase-field and displacement variables. We propose a novel semi-implicit discretization in time that uses a standard convex-concave splitting method of the nonlinear double-well potential, as well as special treatment to the elastic energy. We show that the resulting discrete system is equivalent to a convex minimization problem, and propose and prove the convergence of alternating minimization applied to it. Finally, we present numerical experiments that show the robustness and effectiveness of both alternating minimization and the monolithic Newton method applied to the newly proposed discrete system of equations. We compare it to a system of equations that has been discretized with a standard convex-concave splitting of the double-well potential, and implicit evaluations of the elasticity contributions and show that the newly proposed discrete system is better conditioned for linearization techniques. ",
    "url": "https://arxiv.org/abs/2206.01541",
    "authors": [
      "Erlend Storvik",
      "Jakub Wiktor Both",
      "Jan Martin Nordbotten",
      "Florin Adrian Radu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2206.01545",
    "title": "Truly Mesh-free Physics-Informed Neural Networks",
    "abstract": "Physics-informed Neural Networks (PINNs) have recently emerged as a principled way to include prior physical knowledge in form of partial differential equations (PDEs) into neural networks. Although generally viewed as being mesh-free, current approaches still rely on collocation points obtained within a bounded region, even in settings with spatially sparse signals. Furthermore, if the boundaries are not known, the selection of such a region may be arbitrary, resulting in a large proportion of collocation points being selected in areas of low relevance. To resolve this, we present a mesh-free and adaptive approach termed particle-density PINN (pdPINN), which is inspired by the microscopic viewpoint of fluid dynamics. Instead of sampling from a bounded region, we propose to sample directly from the distribution over the (fluids) particle positions, eliminating the need to introduce boundaries while adaptively focusing on the most relevant regions. This is achieved by reformulating the modeled fluid density as an unnormalized probability distribution from which we sample with dynamic Monte Carlo methods. We further generalize pdPINNs to different settings that allow interpreting a positive scalar quantity as a particle density, such as the evolution of the temperature in the heat equation. The utility of our approach is demonstrated on experiments for modeling (non-steady) compressible fluids in up to three dimensions and a two-dimensional diffusion problem, illustrating the high flexibility and sample efficiency compared to existing refinement methods for PINNs. ",
    "url": "https://arxiv.org/abs/2206.01545",
    "authors": [
      "Fabricio Arend Torres",
      "Marcello Massimo Negri",
      "Monika Nagy-Huber",
      "Maxim Samarin",
      "Volker Roth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.01570",
    "title": "On Calibration of Graph Neural Networks for Node Classification",
    "abstract": "Graphs can model real-world, complex systems by representing entities and their interactions in terms of nodes and edges. To better exploit the graph structure, graph neural networks have been developed, which learn entity and edge embeddings for tasks such as node classification and link prediction. These models achieve good performance with respect to accuracy, but the confidence scores associated with the predictions might not be calibrated. That means that the scores might not reflect the ground-truth probabilities of the predicted events, which would be especially important for safety-critical applications. Even though graph neural networks are used for a wide range of tasks, the calibration thereof has not been sufficiently explored yet. We investigate the calibration of graph neural networks for node classification, study the effect of existing post-processing calibration methods, and analyze the influence of model capacity, graph density, and a new loss function on calibration. Further, we propose a topology-aware calibration method that takes the neighboring nodes into account and yields improved calibration compared to baseline methods. ",
    "url": "https://arxiv.org/abs/2206.01570",
    "authors": [
      "Tong Liu",
      "Yushan Liu",
      "Marcel Hildebrandt",
      "Mitchell Joblin",
      "Hang Li",
      "Volker Tresp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01583",
    "title": "Findings of the The RuATD Shared Task 2022 on Artificial Text Detection  in Russian",
    "abstract": "We present the shared task on artificial text detection in Russian, which is organized as a part of the Dialogue Evaluation initiative, held in 2022. The shared task dataset includes texts from 14 text generators, i.e., one human writer and 13 text generative models fine-tuned for one or more of the following generation tasks: machine translation, paraphrase generation, text summarization, text simplification. We also consider back-translation and zero-shot generation approaches. The human-written texts are collected from publicly available resources across multiple domains. The shared task consists of two sub-tasks: (i) to determine if a given text is automatically generated or written by a human; (ii) to identify the author of a given text. The first task is framed as a binary classification problem. The second task is a multi-class classification problem. We provide count-based and BERT-based baselines, along with the human evaluation on the first sub-task. A total of 30 and 8 systems have been submitted to the binary and multi-class sub-tasks, correspondingly. Most teams outperform the baselines by a wide margin. We publicly release our codebase, human evaluation results, and other materials in our GitHub repository (https://github.com/dialogue-evaluation/RuATD). ",
    "url": "https://arxiv.org/abs/2206.01583",
    "authors": [
      "Tatiana Shamardina",
      "Vladislav Mikhailov",
      "Daniil Chernianskii",
      "Alena Fenogenova",
      "Marat Saidov",
      "Anastasiya Valeeva",
      "Tatiana Shavrina",
      "Ivan Smurov",
      "Elena Tutubalina",
      "Ekaterina Artemova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2206.01594",
    "title": "Federating and querying heterogeneous and distributed Web APIs and  triple stores",
    "abstract": "Today's international corporations such as BASF, a leading company in the crop protection industry, produce and consume more and more data that are often fragmented and accessible through Web APIs. In addition, part of the proprietary and public data of BASF's interest are stored in triple stores and accessible with the SPARQL query language. Homogenizing the data access modes and the underlying semantics of the data without modifying or replicating the original data sources become important requirements to achieve data integration and interoperability. In this work, we propose a federated data integration architecture within an industrial setup, that relies on an ontology-based data access method. Our performance evaluation in terms of query response time showed that most queries can be answered in under 1 second. ",
    "url": "https://arxiv.org/abs/2206.01594",
    "authors": [
      "Tarcisio Mendes de Farias",
      "Christophe Dessimoz",
      "Aaron Ayllon Benitez",
      "Chen Yang",
      "Jiao Long",
      "Ana-Claudia Sima"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2206.01634",
    "title": "Reinforcement Learning with Neural Radiance Fields",
    "abstract": "It is a long-standing problem to find effective representations for training reinforcement learning (RL) agents. This paper demonstrates that learning state representations with supervision from Neural Radiance Fields (NeRFs) can improve the performance of RL compared to other learned representations or even low-dimensional, hand-engineered state information. Specifically, we propose to train an encoder that maps multiple image observations to a latent space describing the objects in the scene. The decoder built from a latent-conditioned NeRF serves as the supervision signal to learn the latent space. An RL algorithm then operates on the learned latent space as its state representation. We call this NeRF-RL. Our experiments indicate that NeRF as supervision leads to a latent space better suited for the downstream RL tasks involving robotic object manipulations like hanging mugs on hooks, pushing objects, or opening doors. Video: https://dannydriess.github.io/nerf-rl ",
    "url": "https://arxiv.org/abs/2206.01634",
    "authors": [
      "Danny Driess",
      "Ingmar Schubert",
      "Pete Florence",
      "Yunzhu Li",
      "Marc Toussaint"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2206.01640",
    "title": "PROMISSING: Pruning Missing Values in Neural Networks",
    "abstract": "While data are the primary fuel for machine learning models, they often suffer from missing values, especially when collected in real-world scenarios. However, many off-the-shelf machine learning models, including artificial neural network models, are unable to handle these missing values directly. Therefore, extra data preprocessing and curation steps, such as data imputation, are inevitable before learning and prediction processes. In this study, we propose a simple and intuitive yet effective method for pruning missing values (PROMISSING) during learning and inference steps in neural networks. In this method, there is no need to remove or impute the missing values; instead, the missing values are treated as a new source of information (representing what we do not know). Our experiments on simulated data, several classification and regression benchmarks, and a multi-modal clinical dataset show that PROMISSING results in similar prediction performance compared to various imputation techniques. In addition, our experiments show models trained using PROMISSING techniques are becoming less decisive in their predictions when facing incomplete samples with many unknowns. This finding hopefully advances machine learning models from being pure predicting machines to more realistic thinkers that can also say \"I do not know\" when facing incomplete sources of information. ",
    "url": "https://arxiv.org/abs/2206.01640",
    "authors": [
      "Seyed Mostafa Kia",
      "Nastaran Mohammadian Rad",
      "Daniel van Opstal",
      "Bart van Schie",
      "Andre F. Marquand",
      "Josien Pluim",
      "Wiepke Cahn",
      "Hugo G. Schnack"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2206.01649",
    "title": "Neural Differential Equations for Learning to Program Neural Nets  Through Continuous Learning Rules",
    "abstract": "Neural ordinary differential equations (ODEs) have attracted much attention as continuous-time counterparts of deep residual neural networks (NNs), and numerous extensions for recurrent NNs have been proposed. Since the 1980s, ODEs have also been used to derive theoretical results for NN learning rules, e.g., the famous connection between Oja's rule and principal component analysis. Such rules are typically expressed as additive iterative update processes which have straightforward ODE counterparts. Here we introduce a novel combination of learning rules and Neural ODEs to build continuous-time sequence processing nets that learn to manipulate short-term memory in rapidly changing synaptic connections of other nets. This yields continuous-time counterparts of Fast Weight Programmers and linear Transformers. Our novel models outperform the best existing Neural Controlled Differential Equation based models on various time series classification tasks, while also addressing their scalability limitations. Our code is public. ",
    "url": "https://arxiv.org/abs/2206.01649",
    "authors": [
      "Kazuki Irie",
      "Francesco Faccio",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01665",
    "title": "BaCaDI: Bayesian Causal Discovery with Unknown Interventions",
    "abstract": "Learning causal structures from observation and experimentation is a central task in many domains. For example, in biology, recent advances allow us to obtain single-cell expression data under multiple interventions such as drugs or gene knockouts. However, a key challenge is that often the targets of the interventions are uncertain or unknown. Thus, standard causal discovery methods can no longer be used. To fill this gap, we propose a Bayesian framework (BaCaDI) for discovering the causal structure that underlies data generated under various unknown experimental/interventional conditions. BaCaDI is fully differentiable and operates in the continuous space of latent probabilistic representations of both causal structures and interventions. This enables us to approximate complex posteriors via gradient-based variational inference and to reason about the epistemic uncertainty in the predicted structure. In experiments on synthetic causal discovery tasks and simulated gene-expression data, BaCaDI outperforms related methods in identifying causal structures and intervention targets. Finally, we demonstrate that, thanks to its rigorous Bayesian approach, our method provides well-calibrated uncertainty estimates. ",
    "url": "https://arxiv.org/abs/2206.01665",
    "authors": [
      "Alexander H\u00e4gele",
      "Jonas Rothfuss",
      "Lars Lorch",
      "Vignesh Ram Somnath",
      "Bernhard Sch\u00f6lkopf",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.01696",
    "title": "Deep Learning Prediction of Severe Health Risks for Pediatric COVID-19  Patients with a Large Feature Set in 2021 BARDA Data Challenge",
    "abstract": "Most children infected with COVID-19 have no or mild symptoms and can recover automatically by themselves, but some pediatric COVID-19 patients need to be hospitalized or even to receive intensive medical care (e.g., invasive mechanical ventilation or cardiovascular support) to recover from the illnesses. Therefore, it is critical to predict the severe health risk that COVID-19 infection poses to children to provide precise and timely medical care for vulnerable pediatric COVID-19 patients. However, predicting the severe health risk for COVID-19 patients including children remains a significant challenge because many underlying medical factors affecting the risk are still largely unknown. In this work, instead of searching for a small number of most useful features to make prediction, we design a novel large-scale bag-of-words like method to represent various medical conditions and measurements of COVID-19 patients. After some simple feature filtering based on logistical regression, the large set of features is used with a deep learning method to predict both the hospitalization risk for COVID-19 infected children and the severe complication risk for the hospitalized pediatric COVID-19 patients. The method was trained and tested on the datasets of the Biomedical Advanced Research and Development Authority (BARDA) Pediatric COVID-19 Data Challenge held from Sept. 15 to Dec. 17, 2021. The results show that the approach can rather accurately predict the risk of hospitalization and severe complication for pediatric COVID-19 patients and deep learning is more accurate than other machine learning methods. ",
    "url": "https://arxiv.org/abs/2206.01696",
    "authors": [
      "Sajid Mahmud",
      "Elham Soltanikazemi",
      "Frimpong Boadu",
      "Ashwin Dhakal",
      "Jianlin Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01717",
    "title": "A Theoretical Analysis on Feature Learning in Neural Networks: Emergence  from Inputs and Advantage over Fixed Features",
    "abstract": "An important characteristic of neural networks is their ability to learn representations of the input data with effective features for prediction, which is believed to be a key factor to their superior empirical performance. To better understand the source and benefit of feature learning in neural networks, we consider learning problems motivated by practical data, where the labels are determined by a set of class relevant patterns and the inputs are generated from these along with some background patterns. We prove that neural networks trained by gradient descent can succeed on these problems. The success relies on the emergence and improvement of effective features, which are learned among exponentially many candidates efficiently by exploiting the data (in particular, the structure of the input distribution). In contrast, no linear models on data-independent features of polynomial sizes can learn to as good errors. Furthermore, if the specific input structure is removed, then no polynomial algorithm in the Statistical Query model can learn even weakly. These results provide theoretical evidence showing that feature learning in neural networks depends strongly on the input structure and leads to the superior performance. Our preliminary experimental results on synthetic and real data also provide positive support. ",
    "url": "https://arxiv.org/abs/2206.01717",
    "authors": [
      "Zhenmei Shi",
      "Junyi Wei",
      "Yingyu Liang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01724",
    "title": "SNAKE: Shape-aware Neural 3D Keypoint Field",
    "abstract": "Detecting 3D keypoints from point clouds is important for shape reconstruction, while this work investigates the dual question: can shape reconstruction benefit 3D keypoint detection? Existing methods either seek salient features according to statistics of different orders or learn to predict keypoints that are invariant to transformation. Nevertheless, the idea of incorporating shape reconstruction into 3D keypoint detection is under-explored. We argue that this is restricted by former problem formulations. To this end, a novel unsupervised paradigm named SNAKE is proposed, which is short for shape-aware neural 3D keypoint field. Similar to recent coordinate-based radiance or distance field, our network takes 3D coordinates as inputs and predicts implicit shape indicators and keypoint saliency simultaneously, thus naturally entangling 3D keypoint detection and shape reconstruction. We achieve superior performance on various public benchmarks, including standalone object datasets ModelNet40, KeypointNet, SMPL meshes and scene-level datasets 3DMatch and Redwood. Intrinsic shape awareness brings several advantages as follows. (1) SNAKE generates 3D keypoints consistent with human semantic annotation, even without such supervision. (2) SNAKE outperforms counterparts in terms of repeatability, especially when the input point clouds are down-sampled. (3) the generated keypoints allow accurate geometric registration, notably in a zero-shot setting. Codes are available at https://github.com/zhongcl-thu/SNAKE ",
    "url": "https://arxiv.org/abs/2206.01724",
    "authors": [
      "Chengliang Zhong",
      "Peixing You",
      "Xiaoxue Chen",
      "Hao Zhao",
      "Fuchun Sun",
      "Guyue Zhou",
      "Xiaodong Mu",
      "Chuang Gan",
      "Wenbing Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2204.05103",
    "title": "Transformer-Based Self-Supervised Learning for Emotion Recognition",
    "abstract": "In order to exploit representations of time-series signals, such as physiological signals, it is essential that these representations capture relevant information from the whole signal. In this work, we propose to use a Transformer-based model to process electrocardiograms (ECG) for emotion recognition. Attention mechanisms of the Transformer can be used to build contextualized representations for a signal, giving more importance to relevant parts. These representations may then be processed with a fully-connected network to predict emotions. To overcome the relatively small size of datasets with emotional labels, we employ self-supervised learning. We gathered several ECG datasets with no labels of emotion to pre-train our model, which we then fine-tuned for emotion recognition on the AMIGOS dataset. We show that our approach reaches state-of-the-art performances for emotion recognition using ECG signals on AMIGOS. More generally, our experiments show that transformers and pre-training are promising strategies for emotion recognition with physiological signals. ",
    "url": "https://arxiv.org/abs/2204.05103",
    "authors": [
      "Juan Vazquez-Rodriguez",
      "Gr\u00e9goire Lefebvre",
      "Julien Cumin",
      "James L. Crowley"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2206.01344",
    "title": "Detecting Pulmonary Embolism from Computed Tomography Using  Convolutional Neural Network",
    "abstract": "The clinical symptoms of pulmonary embolism (PE) are very diverse and non-specific, which makes it difficult to diagnose. In addition, pulmonary embolism has multiple triggers and is one of the major causes of vascular death. Therefore, if it can be detected and treated quickly, it can significantly reduce the risk of death in hospitalized patients. In the detection process, the cost of computed tomography pulmonary angiography (CTPA) is high, and angiography requires the injection of contrast agents, which increase the risk of damage to the patient. Therefore, this study will use a deep learning approach to detect pulmonary embolism in all patients who take a CT image of the chest using a convolutional neural network. With the proposed pulmonary embolism detection system, we can detect the possibility of pulmonary embolism at the same time as the patient's first CT image, and schedule the CTPA test immediately, saving more than a week of CT image screening time and providing timely diagnosis and treatment to the patient. ",
    "url": "https://arxiv.org/abs/2206.01344",
    "authors": [
      "Chia-Hung Yang",
      "Yun-Chien Cheng",
      "Chin Kuo"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01385",
    "title": "Feedback Stabilization of Tank-Liquid System with Robustness to Wall  Friction",
    "abstract": "We solve the feedback stabilization problem for a tank, with friction, containing a liquid modeled by the viscous Saint-Venant system of Partial Differential Equations. A spill-free exponential stabilization is achieved, with robustness to the wall friction forces. A Control Lyapunov Functional (CLF) methodology with two different Lyapunov functionals is employed. These functionals determine specific parameterized sets which approximate the state space. The feedback law is designed based only on one of the two functionals (which is the CLF) while the other functional is used for the derivation of estimates of the sup-norm of the velocity. The feedback law does not require the knowledge of the exact relation of the friction coefficient. Two main results are provided: the first deals with the special case of a velocity-independent friction coefficient, while the second deals with the general case. The obtained results are new even in the frictionless case. ",
    "url": "https://arxiv.org/abs/2206.01385",
    "authors": [
      "Iasson Karafyllis",
      "Filippos Vokos",
      "Miroslav Krstic"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)",
      "Analysis of PDEs (math.AP)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2206.01397",
    "title": "Dynamic Structured Illumination Microscopy with a Neural Space-time  Model",
    "abstract": "Structured illumination microscopy (SIM) reconstructs a super-resolved image from multiple raw images; hence, acquisition speed is limited, making it unsuitable for dynamic scenes. We propose a new method, Speckle Flow SIM, that models sample motion during the data capture in order to reconstruct dynamic scenes with super-resolution. Speckle Flow SIM uses fixed speckle illumination and relies on sample motion to capture a sequence of raw images. Then, the spatio-temporal relationship of the dynamic scene is modeled using a neural space-time model with coordinate-based multi-layer perceptrons (MLPs), and the motion dynamics and the super-resolved scene are jointly recovered. We validated Speckle Flow SIM in simulation and built a simple, inexpensive experimental setup with off-the-shelf components. We demonstrated that Speckle Flow SIM can reconstruct a dynamic scene with deformable motion and 1.88x the diffraction-limited resolution in experiment. ",
    "url": "https://arxiv.org/abs/2206.01397",
    "authors": [
      "Ruiming Cao",
      "Fanglin Linda Liu",
      "Li-Hao Yeh",
      "Laura Waller"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2206.01475",
    "title": "Functional Connectivity Methods for EEG-based Biometrics on a Large,  Heterogeneous Dataset",
    "abstract": "This study examines the utility of functional connectivity (FC) and graph-based (GB) measures with a support vector machine classifier for use in electroencephalogram (EEG) based biometrics. Although FC-based features have been used in biometric applications, studies assessing the identification algorithms on heterogeneous and large datasets are scarce. This work investigates the performance of FC and GB metrics on a dataset of 184 subjects formed by pooling three datasets recorded under different protocols and acquisition systems. The results demonstrate the higher discriminatory power of FC than GB metrics. The identification accuracy increases with higher frequency EEG bands, indicating the enhanced uniqueness of the neural signatures in beta and gamma bands. Using all the 56 EEG channels common to the three databases, the best identification accuracy of 97.4% is obtained using phase-locking value (PLV) based measures extracted from the gamma frequency band. Further, we investigate the effect of the length of the analysis epoch to determine the data acquisition time required to obtain satisfactory identification accuracy. When the number of channels is reduced to 21 from 56, there is a marginal reduction of 2.4% only in the identification accuracy using PLV features in the gamma band. Additional experiments have been conducted to study the effect of the cognitive state of the subject and mismatched train/test conditions on the performance of the system. ",
    "url": "https://arxiv.org/abs/2206.01475",
    "authors": [
      "Pradeep Kumar G",
      "Utsav Dutta",
      "Kanishka Sharma",
      "Ramakrishnan Angarai Ganesan"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01562",
    "title": "Prescriptive maintenance with causal machine learning",
    "abstract": "Machine maintenance is a challenging operational problem, where the goal is to plan sufficient preventive maintenance to avoid machine failures and overhauls. Maintenance is often imperfect in reality and does not make the asset as good as new. Although a variety of imperfect maintenance policies have been proposed in the literature, these rely on strong assumptions regarding the effect of maintenance on the machine's condition, assuming the effect is (1) deterministic or governed by a known probability distribution, and (2) machine-independent. This work proposes to relax both assumptions by learning the effect of maintenance conditional on a machine's characteristics from observational data on similar machines using existing methodologies for causal inference. By predicting the maintenance effect, we can estimate the number of overhauls and failures for different levels of maintenance and, consequently, optimize the preventive maintenance frequency to minimize the total estimated cost. We validate our proposed approach using real-life data on more than 4,000 maintenance contracts from an industrial partner. Empirical results show that our novel, causal approach accurately predicts the maintenance effect and results in individualized maintenance schedules that are more accurate and cost-effective than supervised or non-individualized approaches. ",
    "url": "https://arxiv.org/abs/2206.01562",
    "authors": [
      "Toon Vanderschueren",
      "Robert Boute",
      "Tim Verdonck",
      "Bart Baesens",
      "Wouter Verbeke"
    ],
    "subjectives": [
      "General Economics (econ.GN)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.01685",
    "title": "Toward a realistic model of speech processing in the brain with  self-supervised learning",
    "abstract": "Several deep neural networks have recently been shown to generate activations similar to those of the brain in response to the same input. These algorithms, however, remain largely implausible: they require (1) extraordinarily large amounts of data, (2) unobtainable supervised labels, (3) textual rather than raw sensory input, and / or (4) implausibly large memory (e.g. thousands of contextual words). These elements highlight the need to identify algorithms that, under these limitations, would suffice to account for both behavioral and brain responses. Focusing on the issue of speech processing, we here hypothesize that self-supervised algorithms trained on the raw waveform constitute a promising candidate. Specifically, we compare a recent self-supervised architecture, Wav2Vec 2.0, to the brain activity of 412 English, French, and Mandarin individuals recorded with functional Magnetic Resonance Imaging (fMRI), while they listened to ~1h of audio books. Our results are four-fold. First, we show that this algorithm learns brain-like representations with as little as 600 hours of unlabelled speech -- a quantity comparable to what infants can be exposed to during language acquisition. Second, its functional hierarchy aligns with the cortical hierarchy of speech processing. Third, different training regimes reveal a functional specialization akin to the cortex: Wav2Vec 2.0 learns sound-generic, speech-specific and language-specific representations similar to those of the prefrontal and temporal cortices. Fourth, we confirm the similarity of this specialization with the behavior of 386 additional participants. These elements, resulting from the largest neuroimaging benchmark to date, show how self-supervised learning can account for a rich organization of speech processing in the brain, and thus delineate a path to identify the laws of language acquisition which shape the human brain. ",
    "url": "https://arxiv.org/abs/2206.01685",
    "authors": [
      "Juliette Millet",
      "Charlotte Caucheteux",
      "Pierre Orhan",
      "Yves Boubenec",
      "Alexandre Gramfort",
      "Ewan Dunbar",
      "Christophe Pallier",
      "Jean-Remi King"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2206.01693",
    "title": "Three-dimensional microstructure generation using generative adversarial  neural networks in the context of continuum micromechanics",
    "abstract": "Multiscale simulations are demanding in terms of computational resources. In the context of continuum micromechanics, the multiscale problem arises from the need of inferring macroscopic material parameters from the microscale. If the underlying microstructure is explicitly given by means of microCT-scans, convolutional neural networks can be used to learn the microstructure-property mapping, which is usually obtained from computational homogenization. The CNN approach provides a significant speedup, especially in the context of heterogeneous or functionally graded materials. Another application is uncertainty quantification, where many expansive evaluations are required. However, one bottleneck of this approach is the large number of training microstructures needed. This work closes this gap by proposing a generative adversarial network tailored towards three-dimensional microstructure generation. The lightweight algorithm is able to learn the underlying properties of the material from a single microCT-scan without the need of explicit descriptors. During prediction time, the network can produce unique three-dimensional microstructures with the same properties of the original data in a fraction of seconds and at consistently high quality. ",
    "url": "https://arxiv.org/abs/2206.01693",
    "authors": [
      "Alexander Henkes",
      "Henning Wessels"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:1902.10630",
    "title": "Alternating Synthetic and Real Gradients for Neural Language Modeling",
    "abstract": " Comments: renew the ideas ",
    "url": "https://arxiv.org/abs/1902.10630",
    "authors": [
      "Fangxin Shang",
      "Hao Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:1906.11713",
    "title": "GASP, a generalized framework for agglomerative clustering of signed  graphs and its application to Instance Segmentation",
    "abstract": " Comments: Published in CVPR 2022 ",
    "url": "https://arxiv.org/abs/1906.11713",
    "authors": [
      "Alberto Bailoni",
      "Constantin Pape",
      "Nathan H\u00fctsch",
      "Steffen Wolf",
      "Thorsten Beier",
      "Anna Kreshuk",
      "Fred A. Hamprecht"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2011.03633",
    "title": "Augmented Equivariant Attention Networks for Microscopy Image  Reconstruction",
    "abstract": " Comments: 13 pages, 8 figures, accepted by TMI ",
    "url": "https://arxiv.org/abs/2011.03633",
    "authors": [
      "Yaochen Xie",
      "Yu Ding",
      "Shuiwang Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2104.05097",
    "title": "Pay attention to your loss: understanding misconceptions about  1-Lipschitz neural networks",
    "abstract": " Comments: 28 pages, 10 figures ",
    "url": "https://arxiv.org/abs/2104.05097",
    "authors": [
      "Louis B\u00e9thune",
      "Thibaut Boissin",
      "Mathieu Serrurier",
      "Franck Mamalet",
      "Corentin Friedrich",
      "Alberto Gonz\u00e1lez-Sanz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2106.03228",
    "title": "Distributional Reinforcement Learning with Unconstrained Monotonic  Neural Networks",
    "abstract": " Title: Distributional Reinforcement Learning with Unconstrained Monotonic  Neural Networks ",
    "url": "https://arxiv.org/abs/2106.03228",
    "authors": [
      "Thibaut Th\u00e9ate",
      "Antoine Wehenkel",
      "Adrien Bolland",
      "Gilles Louppe",
      "Damien Ernst"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2106.12413",
    "title": "Transformer Meets Convolution: A Bilateral Awareness Network for  Semantic Segmentation of Very Fine Resolution Urban Scene Images",
    "abstract": " Comments: Accepted by Remote Sensing, see this https URL ",
    "url": "https://arxiv.org/abs/2106.12413",
    "authors": [
      "Libo Wang",
      "Rui Li",
      "Dongzhi Wang",
      "Chenxi Duan",
      "Teng Wang",
      "Xiaoliang Meng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2108.03348",
    "title": "Global Self-Attention as a Replacement for Graph Convolution",
    "abstract": " Comments: The accepted version in KDD '22 ",
    "url": "https://arxiv.org/abs/2108.03348",
    "authors": [
      "Md Shamim Hussain",
      "Mohammed J. Zaki",
      "Dharmashankar Subramanian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2109.00389",
    "title": "Optimization problems in graphs with locational uncertainty",
    "abstract": " Title: Optimization problems in graphs with locational uncertainty ",
    "url": "https://arxiv.org/abs/2109.00389",
    "authors": [
      "Marin Bougeret",
      "J\u00e9r\u00e9my Omer",
      "Michael Poss"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2109.02986",
    "title": "Instance-dependent Label-noise Learning under a Structural Causal Model",
    "abstract": " Title: Instance-dependent Label-noise Learning under a Structural Causal Model ",
    "url": "https://arxiv.org/abs/2109.02986",
    "authors": [
      "Yu Yao",
      "Tongliang Liu",
      "Mingming Gong",
      "Bo Han",
      "Gang Niu",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.00719",
    "title": "One-Bit Matrix Completion with Differential Privacy",
    "abstract": " Comments: In this updated version, we have added a new privacy-preserving perturbation method and the related experiments. In addition, we have added theoretical analysis on the recovery error bounds ",
    "url": "https://arxiv.org/abs/2110.00719",
    "authors": [
      "Zhengpin Li",
      "Zheng Wei",
      "Zengfeng Huang",
      "Xiaojun Mao",
      "Jian Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.07572",
    "title": "LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in  Semantic Parsing",
    "abstract": " Title: LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in  Semantic Parsing ",
    "url": "https://arxiv.org/abs/2110.07572",
    "authors": [
      "Dora Jambor",
      "Dzmitry Bahdanau"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2111.12981",
    "title": "Efficient Mean Estimation with Pure Differential Privacy via a  Sum-of-Squares Exponential Mechanism",
    "abstract": " Comments: 66 pages, STOC 2022 ",
    "url": "https://arxiv.org/abs/2111.12981",
    "authors": [
      "Samuel B. Hopkins",
      "Gautam Kamath",
      "Mahbod Majid"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2111.13040",
    "title": "Sketch-Guided Equality Saturation: Scaling Equality Saturation to  Complex Optimizations of Functional Programs",
    "abstract": " Comments: 23 pages excluding references, submitted to OOPLSA 2022 ",
    "url": "https://arxiv.org/abs/2111.13040",
    "authors": [
      "Thomas Koehler",
      "Phil Trinder",
      "Michel Steuwer"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2111.13336",
    "title": "MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for  Efficient Object Detection",
    "abstract": " Title: MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for  Efficient Object Detection ",
    "url": "https://arxiv.org/abs/2111.13336",
    "authors": [
      "Zhenhong Sun",
      "Ming Lin",
      "Xiuyu Sun",
      "Zhiyu Tan",
      "Hao Li",
      "Rong Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2112.00720",
    "title": "Quasi-universality of Reeb graph distances",
    "abstract": " Comments: 17 pages + 6 pages appendix, 5 figures; this version includes the appendix to the conference paper for SoCG 2022 with the same content otherwise ",
    "url": "https://arxiv.org/abs/2112.00720",
    "authors": [
      "Ulrich Bauer",
      "H\u00e5vard Bakke Bjerkevik",
      "Benedikt Fluhr"
    ],
    "subjectives": [
      "Geometric Topology (math.GT)",
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2112.01085",
    "title": "PTCT: Patches with 3D-Temporal Convolutional Transformer Network for  Precipitation Nowcasting",
    "abstract": " Comments: 9 pages, 3 figures ",
    "url": "https://arxiv.org/abs/2112.01085",
    "authors": [
      "Ziao Yang",
      "Xiangrui Yang",
      "Qifeng Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2112.07837",
    "title": "Central-Smoothing Hypergraph Neural Networks for Predicting Drug-Drug  Interactions",
    "abstract": " Title: Central-Smoothing Hypergraph Neural Networks for Predicting Drug-Drug  Interactions ",
    "url": "https://arxiv.org/abs/2112.07837",
    "authors": [
      "Duc Anh Nguyen",
      "Canh Hao Nguyen",
      "Hiroshi Mamitsuka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2112.08935",
    "title": "MVSS-Net: Multi-View Multi-Scale Supervised Networks for Image  Manipulation Detection",
    "abstract": " Comments: arXiv admin note: substantial text overlap with arXiv:2104.06832 Accepted by T-PAMI ",
    "url": "https://arxiv.org/abs/2112.08935",
    "authors": [
      "Chengbo Dong",
      "Xinru Chen",
      "Ruohan Hu",
      "Juan Cao",
      "Xirong Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2202.02394",
    "title": "JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity  Detection using Zero and One Shot Learning",
    "abstract": " Comments: Accepted at the 16th International Workshop on Semantic Evaluation (SemEval-2022), NAACL. Best Project Award for Georgia Tech CS 7650. Code available at this https URL ",
    "url": "https://arxiv.org/abs/2202.02394",
    "authors": [
      "Yash Jakhotiya",
      "Vaibhav Kumar",
      "Ashwin Pathak",
      "Raj Shah"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.05139",
    "title": "Game of Privacy: Towards Better Federated Platform Collaboration under  Privacy Restriction",
    "abstract": " Comments: Submitted to KDD 2022 ",
    "url": "https://arxiv.org/abs/2202.05139",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Yanlin Wang",
      "Yuqing Yang",
      "Yongfeng Huang",
      "Xing Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.07135",
    "title": "Compositional Scene Representation Learning via Reconstruction: A Survey",
    "abstract": " Title: Compositional Scene Representation Learning via Reconstruction: A Survey ",
    "url": "https://arxiv.org/abs/2202.07135",
    "authors": [
      "Jinyang Yuan",
      "Tonglin Chen",
      "Bin Li",
      "Xiangyang Xue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2202.07549",
    "title": "Robust Multi-Objective Bayesian Optimization Under Input Noise",
    "abstract": " Comments: To appear at ICML 2022. 36 pages. Code is available at this https URL ",
    "url": "https://arxiv.org/abs/2202.07549",
    "authors": [
      "Samuel Daulton",
      "Sait Cakmak",
      "Maximilian Balandat",
      "Michael A. Osborne",
      "Enlu Zhou",
      "Eytan Bakshy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2202.08876",
    "title": "An alternative approach to train neural networks using monotone  variational inequality",
    "abstract": " Title: An alternative approach to train neural networks using monotone  variational inequality ",
    "url": "https://arxiv.org/abs/2202.08876",
    "authors": [
      "Chen Xu",
      "Xiuyuan Cheng",
      "Yao Xie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.10847",
    "title": "UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural  Representations for Computed Tomography",
    "abstract": " Title: UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural  Representations for Computed Tomography ",
    "url": "https://arxiv.org/abs/2202.10847",
    "authors": [
      "Francisca Vasconcelos",
      "Bobby He",
      "Nalini Singh",
      "Yee Whye Teh"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.12448",
    "title": "Deep neural networks for fine-grained surveillance of overdose mortality",
    "abstract": " Comments: Minor revision under review at the American Journal of Epidemiology ",
    "url": "https://arxiv.org/abs/2202.12448",
    "authors": [
      "Patrick J. Ward",
      "April M. Young",
      "Svetla Slavova",
      "Madison Liford",
      "Lara Daniels",
      "Ripley Lucas",
      "Ramakanth Kavuluru"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2202.13164",
    "title": "Edge Augmentation for Large-Scale Sketch Recognition without Sketches",
    "abstract": " Title: Edge Augmentation for Large-Scale Sketch Recognition without Sketches ",
    "url": "https://arxiv.org/abs/2202.13164",
    "authors": [
      "Nikos Efthymiadis",
      "Giorgos Tolias",
      "Ondrej Chum"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2202.13174",
    "title": "BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves  Biomedical Machine Reading Comprehension Task",
    "abstract": " Title: BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves  Biomedical Machine Reading Comprehension Task ",
    "url": "https://arxiv.org/abs/2202.13174",
    "authors": [
      "Maria Mahbub",
      "Sudarshan Srinivasan",
      "Edmon Begoli",
      "Gregory D Peterson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2203.15143",
    "title": "Towards End-to-End Unified Scene Text Detection and Layout Analysis",
    "abstract": " Comments: To appear at CVPR 2022. Code Available: this https URL ",
    "url": "https://arxiv.org/abs/2203.15143",
    "authors": [
      "Shangbang Long",
      "Siyang Qin",
      "Dmitry Panteleev",
      "Alessandro Bissacco",
      "Yasuhisa Fujii",
      "Michalis Raptis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2204.05103",
    "title": "Transformer-Based Self-Supervised Learning for Emotion Recognition",
    "abstract": " Title: Transformer-Based Self-Supervised Learning for Emotion Recognition ",
    "url": "https://arxiv.org/abs/2204.05103",
    "authors": [
      "Juan Vazquez-Rodriguez",
      "Gr\u00e9goire Lefebvre",
      "Julien Cumin",
      "James L. Crowley"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2204.07258",
    "title": "Causal Transformer for Estimating Counterfactual Outcomes",
    "abstract": " Title: Causal Transformer for Estimating Counterfactual Outcomes ",
    "url": "https://arxiv.org/abs/2204.07258",
    "authors": [
      "Valentyn Melnychuk",
      "Dennis Frauen",
      "Stefan Feuerriegel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2205.13586",
    "title": "Comparing the Digital Annealer with Classical Evolutionary Algorithm",
    "abstract": " Comments: 16 pages, New Architectures for Search and Optimization workshop, INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE ",
    "url": "https://arxiv.org/abs/2205.13586",
    "authors": [
      "Mayowa Ayodele"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2205.13607",
    "title": "Self-supervised Pretraining and Transfer Learning Enable Flu and  COVID-19 Predictions in Small Mobile Sensing Datasets",
    "abstract": " Title: Self-supervised Pretraining and Transfer Learning Enable Flu and  COVID-19 Predictions in Small Mobile Sensing Datasets ",
    "url": "https://arxiv.org/abs/2205.13607",
    "authors": [
      "Mike A. Merrill",
      "Tim Althoff"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2205.13947",
    "title": "Spatio-Temporal Graph Few-Shot Learning with Cross-City Knowledge  Transfer",
    "abstract": " Comments: Accepted to KDD2022 ",
    "url": "https://arxiv.org/abs/2205.13947",
    "authors": [
      "Bin Lu",
      "Xiaoying Gan",
      "Weinan Zhang",
      "Huaxiu Yao",
      "Luoyi Fu",
      "Xinbing Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2205.13954",
    "title": "Geometer: Graph Few-Shot Class-Incremental Learning via Prototype  Representation",
    "abstract": " Comments: Accepted to KDD2022 ",
    "url": "https://arxiv.org/abs/2205.13954",
    "authors": [
      "Bin Lu",
      "Xiaoying Gan",
      "Lina Yang",
      "Weinan Zhang",
      "Luoyi Fu",
      "Xinbing Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2205.14109",
    "title": "Bayesian Robust Graph Contrastive Learning",
    "abstract": " Title: Bayesian Robust Graph Contrastive Learning ",
    "url": "https://arxiv.org/abs/2205.14109",
    "authors": [
      "Yancheng Wang",
      "Yingzhen Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.00518",
    "title": "Efficient Scheduling of Data Augmentation for Deep Reinforcement  Learning",
    "abstract": " Comments: I want to update my previous submitted paper arXiv:2102.08581,rather than new submission. Thus, I want to withdraw this submission, and i already replace 2102.08581 by updated paper ",
    "url": "https://arxiv.org/abs/2206.00518",
    "authors": [
      "Byungchan Ko",
      "Jungseul Ok"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.00803",
    "title": "Robust recovery of low-rank matrices and low-tubal-rank tensors from  noisy sketches",
    "abstract": " Comments: 16 pages, 2 figures ",
    "url": "https://arxiv.org/abs/2206.00803",
    "authors": [
      "Anna Ma",
      "Dominik St\u00f6ger",
      "Yizhe Zhu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2206.01041",
    "title": "End-to-End Security for Distributed Event-Driven Enclave Applications on  Heterogeneous TEEs",
    "abstract": " Comments: 35 pages, submitted to ACM Transactions on Privacy and Security, first co-authorship between Gianluca Scopelliti and Sepideh Pouyanrad, source code available at this https URL ",
    "url": "https://arxiv.org/abs/2206.01041",
    "authors": [
      "Gianluca Scopelliti",
      "Sepideh Pouyanrad",
      "Job Noorman",
      "Fritz Alder",
      "Christoph Baumann",
      "Frank Piessens",
      "Jan Tobias M\u00fchlberg"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2206.01088",
    "title": "Machine Learning-based Lung and Colon Cancer Detection using Deep  Feature Extraction and Ensemble Learning",
    "abstract": " Comments: Accepted for publication in the Special Issue of Expert Systems with Applications (IF:6.954, Cite:12.70) How to Cite: Md. Alamin Talukder, Md. Manowarul Islam, Md Ashraf Uddin, Arnisha Akhter, Khondokar Fida Hasan, Mohammad Ali Moni. \"Machine Learning-based Lung and Colon Cancer Detection using Deep Feature Extraction and Ensemble Learning\", Expert Systems with Applications. 2022 Jun 1 ",
    "url": "https://arxiv.org/abs/2206.01088",
    "authors": [
      "Md. Alamin Talukder",
      "Md. Manowarul Islam",
      "Md Ashraf Uddin",
      "Arnisha Akhter",
      "Khondokar Fida Hasan",
      "Mohammad Ali Moni"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  }
]