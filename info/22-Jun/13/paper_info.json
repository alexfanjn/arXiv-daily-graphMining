[
  {
    "id": "arXiv:2206.04675",
    "title": "Deep Convolutional Ritz Method: Parametric PDE surrogates without  labeled data",
    "abstract": "Parametric surrogate models for partial differential equations (PDEs) are a necessary component for many applications in the computational sciences, and convolutional neural networks (CNNs) have proved as an excellent tool to generate these surrogates when parametric fields are present. CNNs are commonly trained on labeled data based on one-to-one sets of parameter-input and PDE-output fields. Recently, residual-based convolutional physics-informed neural network (CPINN) solvers for parametric PDEs have been proposed to build surrogates without the need for labeled data. These allow for the generation of surrogates without an expensive offline-phase. In this work, we present an alternative formulation termed Deep Convolutional Ritz Method (DCRM) as a parametric PDE solver. The approach is based on the minimization of energy functionals, which lowers the order of the differential operators compared to residual-based methods. Based on studies involving the Poisson equation with a spatially parameterized source term and boundary conditions, we found that CNNs trained on labeled data outperform CPINNs in convergence speed and generalization ability. Surrogates generated from DCRM, however, converge significantly faster than their CPINN counterparts and prove to generalize faster and better than surrogates obtained from both CNNs trained on labeled data and CPINNs. This hints that DCRM could make PDE solution surrogates trained without labeled data possible. ",
    "url": "https://arxiv.org/abs/2206.04675",
    "authors": [
      "Jan Niklas Fuhg",
      "Arnav Karmarkar",
      "Teeratorn Kadeethum",
      "Hongkyu Yoon",
      "Nikolaos Bouklas"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2206.04677",
    "title": "Can Backdoor Attacks Survive Time-Varying Models?",
    "abstract": "Backdoors are powerful attacks against deep neural networks (DNNs). By poisoning training data, attackers can inject hidden rules (backdoors) into DNNs, which only activate on inputs containing attack-specific triggers. While existing work has studied backdoor attacks on a variety of DNN models, they only consider static models, which remain unchanged after initial deployment. In this paper, we study the impact of backdoor attacks on a more realistic scenario of time-varying DNN models, where model weights are updated periodically to handle drifts in data distribution over time. Specifically, we empirically quantify the \"survivability\" of a backdoor against model updates, and examine how attack parameters, data drift behaviors, and model update strategies affect backdoor survivability. Our results show that one-shot backdoor attacks (i.e., only poisoning training data once) do not survive past a few model updates, even when attackers aggressively increase trigger size and poison ratio. To stay unaffected by model update, attackers must continuously introduce corrupted data into the training pipeline. Together, these results indicate that when models are updated to learn new data, they also \"forget\" backdoors as hidden, malicious features. The larger the distribution shift between old and new training data, the faster backdoors are forgotten. Leveraging these insights, we apply a smart learning rate scheduler to further accelerate backdoor forgetting during model updates, which prevents one-shot backdoors from surviving past a single model update. ",
    "url": "https://arxiv.org/abs/2206.04677",
    "authors": [
      "Huiying Li",
      "Arjun Nitin Bhagoji",
      "Ben Y. Zhao",
      "Haitao Zheng"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04678",
    "title": "ReCo: A Dataset for Residential Community Layout Planning",
    "abstract": "Layout planning is centrally important in the field of architecture and urban design. Among the various basic units carrying urban functions, residential community plays a vital part for supporting human life. Therefore, the layout planning of residential community has always been of concern, and has attracted particular attention since the advent of deep learning that facilitates the automated layout generation and spatial pattern recognition. However, the research circles generally suffer from the insufficiency of residential community layout benchmark or high-quality datasets, which hampers the future exploration of data-driven methods for residential community layout planning. The lack of datasets is largely due to the difficulties of large-scale real-world residential data acquisition and long-term expert screening. In order to address the issues and advance a benchmark dataset for various intelligent spatial design and analysis applications in the development of smart city, we introduce Residential Community Layout Planning (ReCo) Dataset, which is the first and largest open-source vector dataset related to real-world community to date. ReCo Dataset is presented in multiple data formats with 37,646 residential community layout plans, covering 598,728 residential buildings with height information. ReCo can be conveniently adapted for residential community layout related urban design tasks, e.g., generative layout design, morphological pattern recognition and spatial evaluation. To validate the utility of ReCo in automated residential community layout planning, a Generative Adversarial Network (GAN) based generative model is further applied to the dataset. We expect ReCo Dataset to inspire more creative and practical work in intelligent design and beyond. The ReCo Dataset is published at: https://www.kaggle.com/fdudsde/reco-dataset. ",
    "url": "https://arxiv.org/abs/2206.04678",
    "authors": [
      "Xi Chen",
      "Yun Xiong",
      "Siqi Wang",
      "Haofen Wang",
      "Tao Sheng",
      "Yao Zhang",
      "Yu Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04685",
    "title": "Predictive Exit: Prediction of Fine-Grained Early Exits for Computation-  and Energy-Efficient Inference",
    "abstract": "By adding exiting layers to the deep learning networks, early exit can terminate the inference earlier with accurate results. The passive decision-making of whether to exit or continue the next layer has to go through every pre-placed exiting layer until it exits. In addition, it is also hard to adjust the configurations of the computing platforms alongside the inference proceeds. By incorporating a low-cost prediction engine, we propose a Predictive Exit framework for computation- and energy-efficient deep learning applications. Predictive Exit can forecast where the network will exit (i.e., establish the number of remaining layers to finish the inference), which effectively reduces the network computation cost by exiting on time without running every pre-placed exiting layer. Moreover, according to the number of remaining layers, proper computing configurations (i.e., frequency and voltage) are selected to execute the network to further save energy. Extensive experimental results demonstrate that Predictive Exit achieves up to 96.2% computation reduction and 72.9% energy-saving compared with classic deep learning networks; and 12.8% computation reduction and 37.6% energy-saving compared with the early exit under state-of-the-art exiting strategies, given the same inference accuracy and latency. ",
    "url": "https://arxiv.org/abs/2206.04685",
    "authors": [
      "Xiangjie Li",
      "Chenfei Lou",
      "Zhengping Zhu",
      "Yuchi Chen",
      "Yingtao Shen",
      "Yehan Ma",
      "An Zou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2206.04687",
    "title": "Swan: A Neural Engine for Efficient DNN Training on Smartphone SoCs",
    "abstract": "The need to train DNN models on end-user devices (e.g., smartphones) is increasing with the need to improve data privacy and reduce communication overheads. Unlike datacenter servers with powerful CPUs and GPUs, modern smartphones consist of a diverse collection of specialized cores following a system-on-a-chip (SoC) architecture that together perform a variety of tasks. We observe that training DNNs on a smartphone SoC without carefully considering its resource constraints can not only lead to suboptimal training performance but significantly affect user experience as well. In this paper, we present Swan, a neural engine to optimize DNN training on smartphone SoCs without hurting user experience. Extensive large-scale evaluations show that Swan can improve performance by 1.2 - 23.3x over the state-of-the-art. ",
    "url": "https://arxiv.org/abs/2206.04687",
    "authors": [
      "Sanjay Sri Vallabh Singapuram",
      "Fan Lai",
      "Chuheng Hu",
      "Mosharaf Chowdhury"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04723",
    "title": "On the Unreasonable Effectiveness of Federated Averaging with  Heterogeneous Data",
    "abstract": "Existing theory predicts that data heterogeneity will degrade the performance of the Federated Averaging (FedAvg) algorithm in federated learning. However, in practice, the simple FedAvg algorithm converges very well. This paper explains the seemingly unreasonable effectiveness of FedAvg that contradicts the previous theoretical predictions. We find that the key assumption of bounded gradient dissimilarity in previous theoretical analyses is too pessimistic to characterize data heterogeneity in practical applications. For a simple quadratic problem, we demonstrate there exist regimes where large gradient dissimilarity does not have any negative impact on the convergence of FedAvg. Motivated by this observation, we propose a new quantity, average drift at optimum, to measure the effects of data heterogeneity, and explicitly use it to present a new theoretical analysis of FedAvg. We show that the average drift at optimum is nearly zero across many real-world federated training tasks, whereas the gradient dissimilarity can be large. And our new analysis suggests FedAvg can have identical convergence rates in homogeneous and heterogeneous data settings, and hence, leads to better understanding of its empirical success. ",
    "url": "https://arxiv.org/abs/2206.04723",
    "authors": [
      "Jianyu Wang",
      "Rudrajit Das",
      "Gauri Joshi",
      "Satyen Kale",
      "Zheng Xu",
      "Tong Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04726",
    "title": "COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive  Learning",
    "abstract": "Graph contrastive learning (GCL) improves graph representation learning, leading to SOTA on various downstream tasks. The graph augmentation step is a vital but scarcely studied step of GCL. In this paper, we show that the node embedding obtained via the graph augmentations is highly biased, somewhat limiting contrastive models from learning discriminative features for downstream tasks.Thus, instead of investigating graph augmentation in the input space, we alternatively propose to perform augmentations on the hidden features (feature augmentation). Inspired by so-called matrix sketching, we propose COSTA, a novel COvariance-preServing feaTure space Augmentation framework for GCL, which generates augmented features by maintaining a ``good sketch'' of original features. To highlight the superiority of feature augmentation with COSTA, we investigate a single-view setting (in addition to multi-view one) which conserves memory and computations. We show that the feature augmentation with COSTA achieves comparable/better results than graph augmentation based models. ",
    "url": "https://arxiv.org/abs/2206.04726",
    "authors": [
      "Yifei Zhang",
      "Hao Zhu",
      "Zixing Song",
      "Piotr Koniusz",
      "Iriwn King"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04730",
    "title": "A Neural Network Architecture for Program Understanding Inspired by  Human Behaviors",
    "abstract": "Program understanding is a fundamental task in program language processing. Despite the success, existing works fail to take human behaviors as reference in understanding programs. In this paper, we consider human behaviors and propose the PGNN-EK model that consists of two main components. On the one hand, inspired by the \"divide-and-conquer\" reading behaviors of humans, we present a partitioning-based graph neural network model PGNN on the upgraded AST of codes. On the other hand, to characterize human behaviors of resorting to other resources to help code comprehension, we transform raw codes with external knowledge and apply pre-training techniques for information extraction. Finally, we combine the two embeddings generated from the two components to output code embeddings. We conduct extensive experiments to show the superior performance of PGNN-EK on the code summarization and code clone detection tasks. In particular, to show the generalization ability of our model, we release a new dataset that is more challenging for code clone detection and could advance the development of the community. Our codes and data are publicly available at https://github.com/RecklessRonan/PGNN-EK. ",
    "url": "https://arxiv.org/abs/2206.04730",
    "authors": [
      "Renyu Zhu",
      "Lei Yuan",
      "Xiang Li",
      "Ming Gao",
      "Wenyuan Cai"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04754",
    "title": "AFIA: ATPG-Guided Fault Injection Attack on Secure Logic Locking",
    "abstract": "The outsourcing of the design and manufacturing of integrated circuits has raised severe concerns about the piracy of Intellectual Properties and illegal overproduction. Logic locking has emerged as an obfuscation technique to protect outsourced chip designs, where the circuit netlist is locked and can only be functional once a secure key is programmed. However, Boolean Satisfiability-based attacks have shown to break logic locking, simultaneously motivating researchers to develop more secure countermeasures. In this paper, we present a novel fault injection attack to break any locking technique that relies on a stored secret key, and denote this attack as AFIA, ATPG-guided Fault Injection Attack. The proposed attack is based on sensitizing a key bit to the primary output while injecting faults at a few other key lines that block the propagation of the targeted key bit. AIFA is very effective in determining a key bit as there exists a stuck-at fault pattern that detects a stuck-at 1 (or stuck-at 0) fault at any key line. The average complexity of number of injected faults for AFIA is linear with the key size and requires only |K| test patterns to determine a secret key, K. AFIA requires a fewer number of injected faults to sensitize a bit to the primary output, compared to 2|K|-1 faults for the differential fault analysis attack [26]. ",
    "url": "https://arxiv.org/abs/2206.04754",
    "authors": [
      "Yadi Zhong",
      "Ujjwal Guin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2206.04762",
    "title": "Data-Efficient Double-Win Lottery Tickets from Robust Pre-training",
    "abstract": "Pre-training serves as a broadly adopted starting point for transfer learning on various downstream tasks. Recent investigations of lottery tickets hypothesis (LTH) demonstrate such enormous pre-trained models can be replaced by extremely sparse subnetworks (a.k.a. matching subnetworks) without sacrificing transferability. However, practical security-crucial applications usually pose more challenging requirements beyond standard transfer, which also demand these subnetworks to overcome adversarial vulnerability. In this paper, we formulate a more rigorous concept, Double-Win Lottery Tickets, in which a located subnetwork from a pre-trained model can be independently transferred on diverse downstream tasks, to reach BOTH the same standard and robust generalization, under BOTH standard and adversarial training regimes, as the full pre-trained model can do. We comprehensively examine various pre-training mechanisms and find that robust pre-training tends to craft sparser double-win lottery tickets with superior performance over the standard counterparts. For example, on downstream CIFAR-10/100 datasets, we identify double-win matching subnetworks with the standard, fast adversarial, and adversarial pre-training from ImageNet, at 89.26%/73.79%, 89.26%/79.03%, and 91.41%/83.22% sparsity, respectively. Furthermore, we observe the obtained double-win lottery tickets can be more data-efficient to transfer, under practical data-limited (e.g., 1% and 10%) downstream schemes. Our results show that the benefits from robust pre-training are amplified by the lottery ticket scheme, as well as the data-limited transfer setting. Codes are available at https://github.com/VITA-Group/Double-Win-LTH. ",
    "url": "https://arxiv.org/abs/2206.04762",
    "authors": [
      "Tianlong Chen",
      "Zhenyu Zhang",
      "Sijia Liu",
      "Yang Zhang",
      "Shiyu Chang",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04763",
    "title": "Neural Bregman Divergences for Distance Learning",
    "abstract": "Many metric learning tasks, such as triplet learning, nearest neighbor retrieval, and visualization, are treated primarily as embedding tasks where the ultimate metric is some variant of the Euclidean distance (e.g., cosine or Mahalanobis), and the algorithm must learn to embed points into the pre-chosen space. The study of non-Euclidean geometries or appropriateness is often not explored, which we believe is due to a lack of tools for learning non-Euclidean measures of distance. Under the belief that the use of asymmetric methods in particular have lacked sufficient study, we propose a new approach to learning arbitrary Bergman divergences in a differentiable manner via input convex neural networks. Over a set of both new and previously studied tasks, including asymmetric regression, ranking, and clustering, we demonstrate that our method more faithfully learns divergences than prior Bregman learning approaches. In doing so we obtain the first method for learning neural Bregman divergences and with it inherit the many nice mathematical properties of Bregman divergences, providing the foundation and tooling for better developing and studying asymmetric distance learning. ",
    "url": "https://arxiv.org/abs/2206.04763",
    "authors": [
      "Fred Lu",
      "Edward Raff",
      "Francis Ferraro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04777",
    "title": "Trimmed Maximum Likelihood Estimation for Robust Learning in Generalized  Linear Models",
    "abstract": "We study the problem of learning generalized linear models under adversarial corruptions. We analyze a classical heuristic called the iterative trimmed maximum likelihood estimator which is known to be effective against label corruptions in practice. Under label corruptions, we prove that this simple estimator achieves minimax near-optimal risk on a wide range of generalized linear models, including Gaussian regression, Poisson regression and Binomial regression. Finally, we extend the estimator to the more challenging setting of label and covariate corruptions and demonstrate its robustness and optimality in that setting as well. ",
    "url": "https://arxiv.org/abs/2206.04777",
    "authors": [
      "Weihao Kong",
      "Rajat Sen",
      "Pranjal Awasthi",
      "Abhimanyu Das"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.04783",
    "title": "ReFace: Real-time Adversarial Attacks on Face Recognition Systems",
    "abstract": "Deep neural network based face recognition models have been shown to be vulnerable to adversarial examples. However, many of the past attacks require the adversary to solve an input-dependent optimization problem using gradient descent which makes the attack impractical in real-time. These adversarial examples are also tightly coupled to the attacked model and are not as successful in transferring to different models. In this work, we propose ReFace, a real-time, highly-transferable attack on face recognition models based on Adversarial Transformation Networks (ATNs). ATNs model adversarial example generation as a feed-forward neural network. We find that the white-box attack success rate of a pure U-Net ATN falls substantially short of gradient-based attacks like PGD on large face recognition datasets. We therefore propose a new architecture for ATNs that closes this gap while maintaining a 10000x speedup over PGD. Furthermore, we find that at a given perturbation magnitude, our ATN adversarial perturbations are more effective in transferring to new face recognition models than PGD. ReFace attacks can successfully deceive commercial face recognition services in a transfer attack setting and reduce face identification accuracy from 82% to 16.4% for AWS SearchFaces API and Azure face verification accuracy from 91% to 50.1%. ",
    "url": "https://arxiv.org/abs/2206.04783",
    "authors": [
      "Shehzeen Hussain",
      "Todd Huster",
      "Chris Mesterharm",
      "Paarth Neekhara",
      "Kevin An",
      "Malhar Jere",
      "Harshvardhan Sikka",
      "Farinaz Koushanfar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04790",
    "title": "Learn2Augment: Learning to Composite Videos for Data Augmentation in  Action Recognition",
    "abstract": "We address the problem of data augmentation for video action recognition. Standard augmentation strategies in video are hand-designed and sample the space of possible augmented data points either at random, without knowing which augmented points will be better, or through heuristics. We propose to learn what makes a good video for action recognition and select only high-quality samples for augmentation. In particular, we choose video compositing of a foreground and a background video as the data augmentation process, which results in diverse and realistic new samples. We learn which pairs of videos to augment without having to actually composite them. This reduces the space of possible augmentations, which has two advantages: it saves computational cost and increases the accuracy of the final trained classifier, as the augmented pairs are of higher quality than average. We present experimental results on the entire spectrum of training settings: few-shot, semi-supervised and fully supervised. We observe consistent improvements across all of them over prior work and baselines on Kinetics, UCF101, HMDB51, and achieve a new state-of-the-art on settings with limited data. We see improvements of up to 8.6% in the semi-supervised setting. ",
    "url": "https://arxiv.org/abs/2206.04790",
    "authors": [
      "Shreyank N Gowda",
      "Marcus Rohrbach",
      "Frank Keller",
      "Laura Sevilla-Lara"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04792",
    "title": "Adaptive Model Pooling for Online Deep Anomaly Detection from a Complex  Evolving Data Stream",
    "abstract": "Online anomaly detection from a data stream is critical for the safety and security of many applications but is facing severe challenges due to complex and evolving data streams from IoT devices and cloud-based infrastructures. Unfortunately, existing approaches fall too short for these challenges; online anomaly detection methods bear the burden of handling the complexity while offline deep anomaly detection methods suffer from the evolving data distribution. This paper presents a framework for online deep anomaly detection, ARCUS, which can be instantiated with any autoencoder-based deep anomaly detection methods. It handles the complex and evolving data streams using an adaptive model pooling approach with two novel techniques: concept-driven inference and drift-aware model pool update; the former detects anomalies with a combination of models most appropriate for the complexity, and the latter adapts the model pool dynamically to fit the evolving data streams. In comprehensive experiments with ten data sets which are both high-dimensional and concept-drifted, ARCUS improved the anomaly detection accuracy of the streaming variants of state-of-the-art autoencoder-based methods and that of the state-of-the-art streaming anomaly detection methods by up to 22% and 37%, respectively. ",
    "url": "https://arxiv.org/abs/2206.04792",
    "authors": [
      "Susik Yoon",
      "Youngjun Lee",
      "Jae-Gil Lee",
      "Byung Suk Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2206.04798",
    "title": "Learning to Efficiently Propagate for Reasoning on Knowledge Graphs",
    "abstract": "Path-based methods are more appealing solutions than embedding methods for knowledge graph reasoning, due to their interpretability and generalization ability to unseen graphs. However, path-based methods usually suffer from the problem of scalability, as the time complexity grows exponentially w.r.t. the length of paths. While recent methods compute reasoning paths with the Bellman-Ford algorithm in polynomial time, the time and memory cost remains very high, as they need to propagate through all the nodes and edges in the graph. In this paper, we propose A*Net, an efficient model for path-based reasoning on knowledge graphs. Inspired by the classical A* algorithm for shortest path problems, our A*Net prioritizes important nodes and edges at each propagation step, to reduce the time and memory footprint. Unlike the classical A* algorithm that uses a heuristic function, we propose to learn the priority function for each node to capture the complex semantics in knowledge graphs. The priority function and the propagation steps are jointly optimized through backpropagation. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A*Net achieves competitive performance with existing state-of-the-art path-based methods, and meanwhile reduces the number of messages, the time and the memory cost up to 7.2$\\times$, 3.4$\\times$ and 4.9$\\times$ respectively. ",
    "url": "https://arxiv.org/abs/2206.04798",
    "authors": [
      "Zhaocheng Zhu",
      "Xinyu Yuan",
      "Louis-Pascal Xhonneux",
      "Ming Zhang",
      "Maxime Gazeau",
      "Jian Tang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04801",
    "title": "Learning Attention-based Representations from Multiple Patterns for  Relation Prediction in Knowledge Graphs",
    "abstract": "Knowledge bases, and their representations in the form of knowledge graphs (KGs), are naturally incomplete. Since scientific and industrial applications have extensively adopted them, there is a high demand for solutions that complete their information. Several recent works tackle this challenge by learning embeddings for entities and relations, then employing them to predict new relations among the entities. Despite their aggrandizement, most of those methods focus only on the local neighbors of a relation to learn the embeddings. As a result, they may fail to capture the KGs' context information by neglecting long-term dependencies and the propagation of entities' semantics. In this manuscript, we propose {\\AE}MP (Attention-based Embeddings from Multiple Patterns), a novel model for learning contextualized representations by: (i) acquiring entities' context information through an attention-enhanced message-passing scheme, which captures the entities' local semantics while focusing on different aspects of their neighborhood; and (ii) capturing the semantic context, by leveraging the paths and their relationships between entities. Our empirical findings draw insights into how attention mechanisms can improve entities' context representation and how combining entities and semantic path contexts improves the general representation of entities and the relation predictions. Experimental results on several large and small knowledge graph benchmarks show that {\\AE}MP either outperforms or competes with state-of-the-art relation prediction methods. ",
    "url": "https://arxiv.org/abs/2206.04801",
    "authors": [
      "V\u00edtor Louren\u00e7o",
      "Aline Paes"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04805",
    "title": "Motif Mining and Unsupervised Representation Learning for BirdCLEF 2022",
    "abstract": "We build a classification model for the BirdCLEF 2022 challenge using unsupervised methods. We implement an unsupervised representation of the training dataset using a triplet loss on spectrogram representation of audio motifs. Our best model performs with a score of 0.48 on the public leaderboard. ",
    "url": "https://arxiv.org/abs/2206.04805",
    "authors": [
      "Anthony Miyaguchi",
      "Jiangyue Yu",
      "Bryan Cheungvivatpant",
      "Dakota Dudley",
      "Aniketh Swain"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2206.04832",
    "title": "Transformer-Graph Neural Network with Global-Local Attention for  Multimodal Rumour Detection with Knowledge Distillation",
    "abstract": "Misinformation spreading becomes a critical issue in online conversation. Detecting rumours is an important research topic in social media analysis. Most existing methods, based on Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs), do not make use of the relationship between the global and local information of a conversation for detection. In this paper, we propose a Transformer-Graph Neural Network (TGNN), to fuse the local information with the global representation, through an attention mechanism. Then, we extend the proposed TGNN for multimodal rumour detection, by considering the latent relationship between the multimodal feature and node feature to form a more comprehensive graph representation. To verify the effectiveness of our proposed method for multimodal rumour detection, we extend the existing PHEME-2016, PHEME-2018, and Weibo data sets, by collecting available and relevant images for training the proposal framework. To improve the performance of single-modal rumour detection, i.e., based on text input only, a teacher-student framework is employed to distil the knowledge from the multimodal model to the single-modal model. Experimental results show that our proposed TGNN can achieve state-of-the-art performance and generalization ability evaluated on the PHEME-2016, PHEME-2018, and Weibo data sets. ",
    "url": "https://arxiv.org/abs/2206.04832",
    "authors": [
      "Tsun-hin Cheung",
      "Kin-man Lam"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2206.04833",
    "title": "Training Neural Networks using SAT solvers",
    "abstract": "We propose an algorithm to explore the global optimization method, using SAT solvers, for training a neural net. Deep Neural Networks have achieved great feats in tasks like-image recognition, speech recognition, etc. Much of their success can be attributed to the gradient-based optimisation methods, which scale well to huge datasets while still giving solutions, better than any other existing methods. However, there exist learning problems like the parity function and the Fast Fourier Transform, where a neural network using gradient-based optimisation algorithm can not capture the underlying structure of the learning task properly. Thus, exploring global optimisation methods is of utmost interest as the gradient-based methods get stuck in local optima. In the experiments, we demonstrate the effectiveness of our algorithm against the ADAM optimiser in certain tasks like parity learning. However, in the case of image classification on the MNIST Dataset, the performance of our algorithm was less than satisfactory. We further discuss the role of the size of the training dataset and the hyper-parameter settings in keeping things scalable for a SAT solver. ",
    "url": "https://arxiv.org/abs/2206.04833",
    "authors": [
      "Subham S. Sahoo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04838",
    "title": "In Defense of Core-set: A Density-aware Core-set Selection for Active  Learning",
    "abstract": "Active learning enables the efficient construction of a labeled dataset by labeling informative samples from an unlabeled dataset. In a real-world active learning scenario, considering the diversity of the selected samples is crucial because many redundant or highly similar samples exist. Core-set approach is the promising diversity-based method selecting diverse samples based on the distance between samples. However, the approach poorly performs compared to the uncertainty-based approaches that select the most difficult samples where neural models reveal low confidence. In this work, we analyze the feature space through the lens of the density and, interestingly, observe that locally sparse regions tend to have more informative samples than dense regions. Motivated by our analysis, we empower the core-set approach with the density-awareness and propose a density-aware core-set (DACS). The strategy is to estimate the density of the unlabeled samples and select diverse samples mainly from sparse regions. To reduce the computational bottlenecks in estimating the density, we also introduce a new density approximation based on locality-sensitive hashing. Experimental results clearly demonstrate the efficacy of DACS in both classification and regression tasks and specifically show that DACS can produce state-of-the-art performance in a practical scenario. Since DACS is weakly dependent on neural architectures, we present a simple yet effective combination method to show that the existing methods can be beneficially combined with DACS. ",
    "url": "https://arxiv.org/abs/2206.04838",
    "authors": [
      "Yeachan Kim",
      "Bonggun Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04843",
    "title": "Neural Laplace: Learning diverse classes of differential equations in  the Laplace domain",
    "abstract": "Neural Ordinary Differential Equations model dynamical systems with \\textit{ODE}s learned by neural networks. However, ODEs are fundamentally inadequate to model systems with long-range dependencies or discontinuities, which are common in engineering and biological systems. Broader classes of differential equations (DE) have been proposed as remedies, including delay differential equations and integro-differential equations. Furthermore, Neural ODE suffers from numerical instability when modelling stiff ODEs and ODEs with piecewise forcing functions. In this work, we propose \\textit{Neural Laplace}, a unified framework for learning diverse classes of DEs including all the aforementioned ones. Instead of modelling the dynamics in the time domain, we model it in the Laplace domain, where the history-dependencies and discontinuities in time can be represented as summations of complex exponentials. To make learning more efficient, we use the geometrical stereographic map of a Riemann sphere to induce more smoothness in the Laplace domain. In the experiments, Neural Laplace shows superior performance in modelling and extrapolating the trajectories of diverse classes of DEs, including the ones with complex history dependency and abrupt changes. ",
    "url": "https://arxiv.org/abs/2206.04843",
    "authors": [
      "Samuel Holt",
      "Zhaozhi Qian",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.04846",
    "title": "Masked Autoencoders are Robust Data Augmentors",
    "abstract": "Deep neural networks are capable of learning powerful representations to tackle complex vision tasks but expose undesirable properties like the over-fitting issue. To this end, regularization techniques like image augmentation are necessary for deep neural networks to generalize well. Nevertheless, most prevalent image augmentation recipes confine themselves to off-the-shelf linear transformations like scale, flip, and colorjitter. Due to their hand-crafted property, these augmentations are insufficient to generate truly hard augmented examples. In this paper, we propose a novel perspective of augmentation to regularize the training process. Inspired by the recent success of applying masked image modeling to self-supervised learning, we adopt the self-supervised masked autoencoder to generate the distorted view of the input images. We show that utilizing such model-based nonlinear transformation as data augmentation can improve high-level recognition tasks. We term the proposed method as \\textbf{M}ask-\\textbf{R}econstruct \\textbf{A}ugmentation (MRA). The extensive experiments on various image classification benchmarks verify the effectiveness of the proposed augmentation. Specifically, MRA consistently enhances the performance on supervised, semi-supervised as well as few-shot classification. The code will be available at \\url{https://github.com/haohang96/MRA}. ",
    "url": "https://arxiv.org/abs/2206.04846",
    "authors": [
      "Haohang Xu",
      "Shuangrui Ding",
      "Xiaopeng Zhang",
      "Hongkai Xiong",
      "Qi Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04854",
    "title": "Heterogeneous Face Recognition via Face Synthesis with  Identity-Attribute Disentanglement",
    "abstract": "Heterogeneous Face Recognition (HFR) aims to match faces across different domains (e.g., visible to near-infrared images), which has been widely applied in authentication and forensics scenarios. However, HFR is a challenging problem because of the large cross-domain discrepancy, limited heterogeneous data pairs, and large variation of facial attributes. To address these challenges, we propose a new HFR method from the perspective of heterogeneous data augmentation, named Face Synthesis with Identity-Attribute Disentanglement (FSIAD). Firstly, the identity-attribute disentanglement (IAD) decouples face images into identity-related representations and identity-unrelated representations (called attributes), and then decreases the correlation between identities and attributes. Secondly, we devise a face synthesis module (FSM) to generate a large number of images with stochastic combinations of disentangled identities and attributes for enriching the attribute diversity of synthetic images. Both the original images and the synthetic ones are utilized to train the HFR network for tackling the challenges and improving the performance of HFR. Extensive experiments on five HFR databases validate that FSIAD obtains superior performance than previous HFR approaches. Particularly, FSIAD obtains 4.8% improvement over state of the art in terms of VR@FAR=0.01% on LAMP-HQ, the largest HFR database so far. ",
    "url": "https://arxiv.org/abs/2206.04854",
    "authors": [
      "Ziming Yang",
      "Jian Liang",
      "Chaoyou Fu",
      "Mandi Luo",
      "Xiao-Yu Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04855",
    "title": "Beyond the Gates of Euclidean Space: Temporal-Discrimination-Fusions and  Attention-based Graph Neural Network for Human Activity Recognition",
    "abstract": "Human activity recognition (HAR) through wearable devices has received much interest due to its numerous applications in fitness tracking, wellness screening, and supported living. As a result, we have seen a great deal of work in this field. Traditional deep learning (DL) has set a state of the art performance for HAR domain. However, it ignores the data's structure and the association between consecutive time stamps. To address this constraint, we offer an approach based on Graph Neural Networks (GNNs) for structuring the input representation and exploiting the relations among the samples. However, even when using a simple graph convolution network to eliminate this shortage, there are still several limiting factors, such as inter-class activities issues, skewed class distribution, and a lack of consideration for sensor data priority, all of which harm the HAR model's performance. To improve the current HAR model's performance, we investigate novel possibilities within the framework of graph structure to achieve highly discriminated and rich activity features. We propose a model for (1) time-series-graph module that converts raw data from HAR dataset into graphs; (2) Graph Convolutional Neural Networks (GCNs) to discover local dependencies and correlations between neighboring nodes; and (3) self-attention GNN encoder to identify sensors interactions and data priorities. To the best of our knowledge, this is the first work for HAR, which introduces a GNN-based approach that incorporates both the GCN and the attention mechanism. By employing a uniform evaluation method, our framework significantly improves the performance on hospital patient's activities dataset comparatively considered other state of the art baseline methods. ",
    "url": "https://arxiv.org/abs/2206.04855",
    "authors": [
      "Nafees Ahmad",
      "Savio Ho-Chit Chow",
      "Ho-fung Leung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2206.04860",
    "title": "Conformal Prediction Intervals for Markov Decision Process Trajectories",
    "abstract": "Before delegating a task to an autonomous system, a human operator may want a guarantee about the behavior of the system. This paper extends previous work on conformal prediction for functional data and conformalized quantile regression to provide conformal prediction intervals over the future behavior of an autonomous system executing a fixed control policy on a Markov Decision Process (MDP). The prediction intervals are constructed by applying conformal corrections to prediction intervals computed by quantile regression. The resulting intervals guarantee that with probability $1-\\delta$ the observed trajectory will lie inside the prediction interval, where the probability is computed with respect to the starting state distribution and the stochasticity of the MDP. The method is illustrated on MDPs for invasive species management and StarCraft2 battles. ",
    "url": "https://arxiv.org/abs/2206.04860",
    "authors": [
      "Thomas G. Dietterich",
      "Jesse Hostetler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.04863",
    "title": "Symbolic image detection using scene and knowledge graphs",
    "abstract": "Sometimes the meaning conveyed by images goes beyond the list of objects they contain; instead, images may express a powerful message to affect the viewers' minds. Inferring this message requires reasoning about the relationships between the objects, and general common-sense knowledge about the components. In this paper, we use a scene graph, a graph representation of an image, to capture visual components. In addition, we generate a knowledge graph using facts extracted from ConceptNet to reason about objects and attributes. To detect the symbols, we propose a neural network framework named SKG-Sym. The framework first generates the representations of the scene graph of the image and its knowledge graph using Graph Convolution Network. The framework then fuses the representations and uses an MLP to classify them. We extend the network further to use an attention mechanism which learn the importance of the graph representations. We evaluate our methods on a dataset of advertisements, and compare it with baseline symbolism classification methods (ResNet and VGG). Results show that our methods outperform ResNet in terms of F-score and the attention-based mechanism is competitive with VGG while it has much lower model complexity. ",
    "url": "https://arxiv.org/abs/2206.04863",
    "authors": [
      "Nasrin Kalanat",
      "Adriana Kovashka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04864",
    "title": "Binarizing Split Learning for Data Privacy Enhancement and Computation  Reduction",
    "abstract": "Split learning (SL) enables data privacy preservation by allowing clients to collaboratively train a deep learning model with the server without sharing raw data. However, SL still has limitations such as potential data privacy leakage and high computation at clients. In this study, we propose to binarize the SL local layers for faster computation (up to 17.5 times less forward-propagation time in both training and inference phases on mobile devices) and reduced memory usage (up to 32 times less memory and bandwidth requirements). More importantly, the binarized SL (B-SL) model can reduce privacy leakage from SL smashed data with merely a small degradation in model accuracy. To further enhance the privacy preservation, we also propose two novel approaches: 1) training with additional local leak loss and 2) applying differential privacy, which could be integrated separately or concurrently into the B-SL model. Experimental results with different datasets have affirmed the advantages of the B-SL models compared with several benchmark models. The effectiveness of B-SL models against feature-space hijacking attack (FSHA) is also illustrated. Our results have demonstrated B-SL models are promising for lightweight IoT/mobile applications with high privacy-preservation requirements such as mobile healthcare applications. ",
    "url": "https://arxiv.org/abs/2206.04864",
    "authors": [
      "Ngoc Duy Pham",
      "Alsharif Abuadbba",
      "Yansong Gao",
      "Tran Khoa Phan",
      "Naveen Chilamkurti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2206.04865",
    "title": "An alternative approach to the exact network reliability assessment  through the quickest path",
    "abstract": "Extending the quickest path problem to the network reliability, a new problem emerged which aims to assess the network reliability for transmitting at least d units of data from a source node to a sink node through one minimal path (MP) within given T units of time. Many of the proposed approaches in the literature check all the MPs of the network for doing the job and then construct desired system state vectors based on the accepted MPs. Hence, they need to have all the MPs of the network in advance. Here, we propose a simple approach that does not need any MP in advance. The algorithm is shown to be corrected and is illustrated through an example. ",
    "url": "https://arxiv.org/abs/2206.04865",
    "authors": [
      "Majid Forghani-elahabad"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2206.04872",
    "title": "Multi-fidelity Hierarchical Neural Processes",
    "abstract": "Science and engineering fields use computer simulation extensively. These simulations are often run at multiple levels of sophistication to balance accuracy and efficiency. Multi-fidelity surrogate modeling reduces the computational cost by fusing different simulation outputs. Cheap data generated from low-fidelity simulators can be combined with limited high-quality data generated by an expensive high-fidelity simulator. Existing methods based on Gaussian processes rely on strong assumptions of the kernel functions and can hardly scale to high-dimensional settings. We propose Multi-fidelity Hierarchical Neural Processes (MF-HNP), a unified neural latent variable model for multi-fidelity surrogate modeling. MF-HNP inherits the flexibility and scalability of Neural Processes. The latent variables transform the correlations among different fidelity levels from observations to latent space. The predictions across fidelities are conditionally independent given the latent states. It helps alleviate the error propagation issue in existing methods. MF-HNP is flexible enough to handle non-nested high dimensional data at different fidelity levels with varying input and output dimensions. We evaluate MF-HNP on epidemiology and climate modeling tasks, achieving competitive performance in terms of accuracy and uncertainty estimation. In contrast to deep Gaussian Processes with only low-dimensional (< 10) tasks, our method shows great promise for speeding up high-dimensional complex simulations (over 7000 for epidemiology modeling and 45000 for climate modeling). ",
    "url": "https://arxiv.org/abs/2206.04872",
    "authors": [
      "Dongxia Wu",
      "Matteo Chinazzi",
      "Alessandro Vespignani",
      "Yi-An Ma",
      "Rose Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04875",
    "title": "Smallset Timelines: A Visual Representation of Data Preprocessing  Decisions",
    "abstract": "Data preprocessing is a crucial stage in the data analysis pipeline, with both technical and social aspects to consider. Yet, the attention it receives is often lacking in research practice and dissemination. We present the Smallset Timeline, a visualisation to help reflect on and communicate data preprocessing decisions. A \"Smallset\" is a small selection of rows from the original dataset containing instances of dataset alterations. The Timeline is comprised of Smallset snapshots representing different points in the preprocessing stage and captions to describe the alterations visualised at each point. Edits, additions, and deletions to the dataset are highlighted with colour. We develop the R software package, smallsets, that can create Smallset Timelines from R and Python data preprocessing scripts. Constructing the figure asks practitioners to reflect on and revise decisions as necessary, while sharing it aims to make the process accessible to a diverse range of audiences. We present two case studies to illustrate use of the Smallset Timeline for visualising preprocessing decisions. Case studies include software defect data and income survey benchmark data, in which preprocessing affects levels of data loss and group fairness in prediction tasks, respectively. We envision Smallset Timelines as a go-to data provenance tool, enabling better documentation and communication of preprocessing tasks at large. ",
    "url": "https://arxiv.org/abs/2206.04875",
    "authors": [
      "Lydia R. Lucchesi",
      "Petra M. Kuhnert",
      "Jenny L. Davis",
      "Lexing Xie"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2206.04881",
    "title": "Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers",
    "abstract": "Backdoor attacks threaten Deep Neural Networks (DNNs). Towards stealthiness, researchers propose clean-label backdoor attacks, which require the adversaries not to alter the labels of the poisoned training datasets. Clean-label settings make the attack more stealthy due to the correct image-label pairs, but some problems still exist: first, traditional methods for poisoning training data are ineffective; second, traditional triggers are not stealthy which are still perceptible. To solve these problems, we propose a two-phase and image-specific triggers generation method to enhance clean-label backdoor attacks. Our methods are (1) powerful: our triggers can both promote the two phases (i.e., the backdoor implantation and activation phase) in backdoor attacks simultaneously; (2) stealthy: our triggers are generated from each image. They are image-specific instead of fixed triggers. Extensive experiments demonstrate that our approach can achieve a fantastic attack success rate~(98.98%) with low poisoning rate~(5%), high stealthiness under many evaluation metrics and is resistant to backdoor defense methods. ",
    "url": "https://arxiv.org/abs/2206.04881",
    "authors": [
      "Nan Luo",
      "Yuanzhang Li",
      "Yajie Wang",
      "Shangbo Wu",
      "Yu-an Tan",
      "Quanxin Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04882",
    "title": "$\\mathsf{G^2Retro}$: Two-Step Graph Generative Models for Retrosynthesis  Prediction",
    "abstract": "Retrosynthesis is a procedure where a molecule is transformed into potential reactants and thus the synthesis routes are identified. We propose a novel generative framework, denoted as $\\mathsf{G^2Retro}$, for one-step retrosynthesis prediction. $\\mathsf{G^2Retro}$ imitates the reversed logic of synthetic reactions, that is, first predicting the reaction centers to convert the target molecule into fragments named synthons, and then transforming synthons into reactants, following previous semi-template-based methods. In predicting reaction centers, $\\mathsf{G^2Retro}$ defines a comprehensive set of reaction center types, and enables diversity in the predicted reactions by considering multiple reaction center candidates. In completing synthons, $\\mathsf{G^2Retro}$ deploys a sequence of substructure attachments to transform synthons into reactants, which utilize a holistic view of the most updated structures of the synthons to be completed, as well as all the involved synthon and product structures. Here we show that $\\mathsf{G^2Retro}$ is able to better prioritize the most possible reactants in the benchmark dataset than the state-of-the-art methods, and discover novel and highly likely reactions that are not included in the benchmark dataset. ",
    "url": "https://arxiv.org/abs/2206.04882",
    "authors": [
      "Ziqi Chen",
      "Oluwatosin R. Ayinde",
      "James R. Fuchs",
      "Huan Sun",
      "Xia Ning"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2206.04888",
    "title": "AntPivot: Livestream Highlight Detection via Hierarchical Attention  Mechanism",
    "abstract": "In recent days, streaming technology has greatly promoted the development in the field of livestream. Due to the excessive length of livestream records, it's quite essential to extract highlight segments with the aim of effective reproduction and redistribution. Although there are lots of approaches proven to be effective in the highlight detection for other modals, the challenges existing in livestream processing, such as the extreme durations, large topic shifts, much irrelevant information and so forth, heavily hamper the adaptation and compatibility of these methods. In this paper, we formulate a new task Livestream Highlight Detection, discuss and analyze the difficulties listed above and propose a novel architecture AntPivot to solve this problem. Concretely, we first encode the original data into multiple views and model their temporal relations to capture clues in a hierarchical attention mechanism. Afterwards, we try to convert the detection of highlight clips into the search for optimal decision sequences and use the fully integrated representations to predict the final results in a dynamic-programming mechanism. Furthermore, we construct a fully-annotated dataset AntHighlight to instantiate this task and evaluate the performance of our model. The extensive experiments indicate the effectiveness and validity of our proposed method. ",
    "url": "https://arxiv.org/abs/2206.04888",
    "authors": [
      "Yang Zhao",
      "Xuan Lin",
      "Wenqiang Xu",
      "Maozong Zheng",
      "Zhengyong Liu",
      "Zhou Zhao"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04890",
    "title": "Adversarial Counterfactual Environment Model Learning",
    "abstract": "A good model for action-effect prediction, named environment model, is important to achieve sample-efficient decision-making policy learning in many domains like robot control, recommender systems, and patients' treatment selection. We can take unlimited trials with such a model to identify the appropriate actions so that the costs of queries in the real world can be saved. It requires the model to handle unseen data correctly, also called counterfactual data. However, standard data fitting techniques do not automatically achieve such generalization ability and commonly result in unreliable models. In this work, we introduce counterfactual-query risk minimization (CQRM) in model learning for generalizing to a counterfactual dataset queried by a specific target policy. Since the target policies can be various and unknown in policy learning, we propose an adversarial CQRM objective in which the model learns on counterfactual data queried by adversarial policies, and finally derive a tractable solution GALILEO. We also discover that adversarial CQRM is closely related to the adversarial model learning, explaining the effectiveness of the latter. We apply GALILEO in synthetic tasks and a real-world application. The results show that GALILEO makes accurate predictions on counterfactual data and thus significantly improves policies in real-world testing. ",
    "url": "https://arxiv.org/abs/2206.04890",
    "authors": [
      "Xiong-Hui Chen",
      "Yang Yu",
      "Zheng-Mao Zhu",
      "Zhihua Yu",
      "Zhenjun Chen",
      "Chenghe Wang",
      "Yinan Wu",
      "Hongqiu Wu",
      "Rong-Jun Qin",
      "Ruijin Ding",
      "Fangsheng Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04891",
    "title": "Explaining Neural Networks without Access to Training Data",
    "abstract": "We consider generating explanations for neural networks in cases where the network's training data is not accessible, for instance due to privacy or safety issues. Recently, $\\mathcal{I}$-Nets have been proposed as a sample-free approach to post-hoc, global model interpretability that does not require access to training data. They formulate interpretation as a machine learning task that maps network representations (parameters) to a representation of an interpretable function. In this paper, we extend the $\\mathcal{I}$-Net framework to the cases of standard and soft decision trees as surrogate models. We propose a suitable decision tree representation and design of the corresponding $\\mathcal{I}$-Net output layers. Furthermore, we make $\\mathcal{I}$-Nets applicable to real-world tasks by considering more realistic distributions when generating the $\\mathcal{I}$-Net's training data. We empirically evaluate our approach against traditional global, post-hoc interpretability approaches and show that it achieves superior results when the training data is not accessible. ",
    "url": "https://arxiv.org/abs/2206.04891",
    "authors": [
      "Sascha Marton",
      "Stefan L\u00fcdtke",
      "Christian Bartelt",
      "Andrej Tschalzev",
      "Heiner Stuckenschmidt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04907",
    "title": "Efficient Heterogeneous Treatment Effect Estimation With Multiple  Experiments and Multiple Outcomes",
    "abstract": "Learning heterogeneous treatment effects (HTEs) is an important problem across many fields. Most existing methods consider the setting with a single treatment arm and a single outcome metric. However, in many real world domains, experiments are run consistently - for example, in internet companies, A/B tests are run every day to measure the impacts of potential changes across many different metrics of interest. We show that even if an analyst cares only about the HTEs in one experiment for one metric, precision can be improved greatly by analyzing all of the data together to take advantage of cross-experiment and cross-outcome metric correlations. We formalize this idea in a tensor factorization framework and propose a simple and scalable model which we refer to as the low rank or LR-learner. Experiments in both synthetic and real data suggest that the LR-learner can be much more precise than independent HTE estimation. ",
    "url": "https://arxiv.org/abs/2206.04907",
    "authors": [
      "Leon Yao",
      "Caroline Lo",
      "Israel Nir",
      "Sarah Tan",
      "Ariel Evnine",
      "Adam Lerer",
      "Alex Peysakhovich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2206.04910",
    "title": "NAGphormer: Neighborhood Aggregation Graph Transformer for Node  Classification in Large Graphs",
    "abstract": "Graph Transformers have demonstrated superiority on various graph learning tasks in recent years. However, the complexity of existing Graph Transformers scales quadratically with the number of nodes, making it hard to scale to graphs with thousands of nodes. To this end, we propose a Neighborhood Aggregation Graph Transformer (NAGphormer) that is scalable to large graphs with millions of nodes. Before feeding the node features into the Transformer model, NAGphormer constructs tokens for each node by a neighborhood aggregation module called Hop2Token. For each node, Hop2Token aggregates neighborhood features from each hop into a representation, and thereby produces a sequence of token vectors. Subsequently, the resulting sequence of different hop information serves as input to the Transformer model. By considering each node as a sequence, NAGphormer could be trained in a mini-batch manner and thus could scale to large graphs. NAGphormer further develops an attention-based readout function so as to learn the importance of each hop adaptively. We conduct extensive experiments on various popular benchmarks, including six small datasets and three large datasets. The results demonstrate that NAGphormer consistently outperforms existing Graph Transformers and mainstream Graph Neural Networks. ",
    "url": "https://arxiv.org/abs/2206.04910",
    "authors": [
      "Jinsong Chen",
      "Kaiyuan Gao",
      "Gaichao Li",
      "Kun He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04922",
    "title": "A Novel Chinese Dialect TTS Frontend with Non-Autoregressive Neural  Machine Translation",
    "abstract": "Chinese dialect text-to-speech(TTS) system usually can only be utilized by native linguists, because the written form of Chinese dialects has different characters, idioms, grammar and usage from Mandarin, and even the local speaker cannot input a correct sentence. For Mandarin text inputs, Chinese dialect TTS can only generate partly-meaningful speech with relatively poor prosody and naturalness. To lower the bar of use and make it more practical in commercial, we propose a novel Chinese dialect TTS frontend with a translation module. It helps to convert Mandarin text into idiomatic expressions with correct orthography and grammar, so that the intelligibility and naturalness of the synthesized speech can be improved. A non-autoregressive neural machine translation model with a glancing sampling strategy is proposed for the translation task. It is the first known work to incorporate translation with TTS frontend. Our experiments on Cantonese approve that the proposed frontend can help Cantonese TTS system achieve a 0.27 improvement in MOS with Mandarin inputs. ",
    "url": "https://arxiv.org/abs/2206.04922",
    "authors": [
      "Wudi Bao",
      "Junhui Zhang",
      "Junjie Pan",
      "Xiang Yin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2206.04942",
    "title": "Neural Template: Topology-aware Reconstruction and Disentangled  Generation of 3D Meshes",
    "abstract": "This paper introduces a novel framework called DTNet for 3D mesh reconstruction and generation via Disentangled Topology. Beyond previous works, we learn a topology-aware neural template specific to each input then deform the template to reconstruct a detailed mesh while preserving the learned topology. One key insight is to decouple the complex mesh reconstruction into two sub-tasks: topology formulation and shape deformation. Thanks to the decoupling, DT-Net implicitly learns a disentangled representation for the topology and shape in the latent space. Hence, it can enable novel disentangled controls for supporting various shape generation applications, e.g., remix the topologies of 3D objects, that are not achievable by previous reconstruction works. Extensive experimental results demonstrate that our method is able to produce high-quality meshes, particularly with diverse topologies, as compared with the state-of-the-art methods. ",
    "url": "https://arxiv.org/abs/2206.04942",
    "authors": [
      "Ka-Hei Hui",
      "Ruihui Li",
      "Jingyu Hu",
      "Chi-Wing Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04948",
    "title": "A Holistic Robust Motion Controller Framework for Autonomous Platooning",
    "abstract": "Safety is the foremost concern for autonomous platooning. The vehicle-to-vehicle (V2V) communication delay and the sudden appearance of obstacles will trigger the safety of the intended functionality (SOTIF) issues for autonomous platooning. This research proposes a holistic robust motion controller framework (MCF) for an intelligent and connected vehicle platoon system. The MCF utilizes a hierarchical structure to resolve the longitudinal string stability and the lateral control problem under the complex driving environment and time-varying communication delay. Firstly, the H-infinity feedback controller is developed to ensure the robustness of the platoon under time-varying communication delay in the upper-level coordination layer (UCL). The output from UCL will be delivered to the lower-level motion-planning layer (LML) as reference signals. Secondly, the model predictive control (MPC) algorithm is implemented in the LML to achieve multi-objective control, which comprehensively considers the reference signals, the artificial potential field, and multiple vehicle dynamics constraints. Furthermore, three critical scenarios are co-simulated for case studies, including platooning under time-varying communication delay, merging, and obstacle avoidance scenarios. The simulation results indicate that, compared with single-structure MPC, the proposed MCF can offer a better suppression on position error propagation, and get improvements on maximum position error in the three scenarios by $19.2\\%$, $59.8\\%$, and $15.3\\%$, respectively. Last, the practicability and effectiveness of the proposed MCF are verified via hardware-in-the-loop experiment. The average conducting time of the proposed method on Speedgoat real-time target machine is 1.1 milliseconds, which meets the real-time requirements. ",
    "url": "https://arxiv.org/abs/2206.04948",
    "authors": [
      "Hong Wang",
      "Li-Ming Peng",
      "Zi-Chun Wei",
      "Kai Yang",
      "Xian-Xu Bai",
      "Luo Jiang",
      "Ehsan Hashemi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2206.04951",
    "title": "Evolutionary Echo State Network: evolving reservoirs in the Fourier  space",
    "abstract": "The Echo State Network (ESN) is a class of Recurrent Neural Network with a large number of hidden-hidden weights (in the so-called reservoir). Canonical ESN and its variations have recently received significant attention due to their remarkable success in the modeling of non-linear dynamical systems. The reservoir is randomly connected with fixed weights that don't change in the learning process. Only the weights from reservoir to output are trained. Since the reservoir is fixed during the training procedure, we may wonder if the computational power of the recurrent structure is fully harnessed. In this article, we propose a new computational model of the ESN type, that represents the reservoir weights in the Fourier space and performs a fine-tuning of these weights applying genetic algorithms in the frequency domain. The main interest is that this procedure will work in a much smaller space compared to the classical ESN, thus providing a dimensionality reduction transformation of the initial method. The proposed technique allows us to exploit the benefits of the large recurrent structure avoiding the training problems of gradient-based method. We provide a detailed experimental study that demonstrates the good performances of our approach with well-known chaotic systems and real-world data. ",
    "url": "https://arxiv.org/abs/2206.04951",
    "authors": [
      "Sebastian Basterrech",
      "Gerardo Rubino"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04958",
    "title": "Self-Supervised Deep Subspace Clustering with Entropy-norm",
    "abstract": "Auto-Encoder based deep subspace clustering (DSC) is widely used in computer vision, motion segmentation and image processing. However, it suffers from the following three issues in the self-expressive matrix learning process: the first one is less useful information for learning self-expressive weights due to the simple reconstruction loss; the second one is that the construction of the self-expression layer associated with the sample size requires high-computational cost; and the last one is the limited connectivity of the existing regularization terms. In order to address these issues, in this paper we propose a novel model named Self-Supervised deep Subspace Clustering with Entropy-norm (S$^{3}$CE). Specifically, S$^{3}$CE exploits a self-supervised contrastive network to gain a more effetive feature vector. The local structure and dense connectivity of the original data benefit from the self-expressive layer and additional entropy-norm constraint. Moreover, a new module with data enhancement is designed to help S$^{3}$CE focus on the key information of data, and improve the clustering performance of positive and negative instances through spectral clustering. Extensive experimental results demonstrate the superior performance of S$^{3}$CE in comparison to the state-of-the-art approaches. ",
    "url": "https://arxiv.org/abs/2206.04958",
    "authors": [
      "Guangyi Zhao",
      "Simin Kou",
      "Xuesong Yin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04962",
    "title": "Feature Learning and Ensemble Pre-Tasks Based Self-Supervised Speech  Denoising and Dereverberation",
    "abstract": "Self-supervised learning (SSL) achieves great success in monaural speech enhancement, while the accuracy of the target speech estimation, particularly for unseen speakers, remains inadequate with existing pre-tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, and spoken content, the latent representation for speech enhancement becomes a tough task. In this paper, we study the effectiveness of each feature which is commonly used in speech enhancement and exploit the feature combination in the SSL case. Besides, we propose an ensemble training strategy. The latent representation of the clean speech signal is learned, meanwhile, the dereverberated mask and the estimated ratio mask are exploited to denoise and dereverberate the mixture. The latent representation learning and the masks estimation are considered as two pre-tasks in the training stage. In addition, to study the effectiveness between the pre-tasks, we compare different training routines to train the model and further refine the performance. The NOISEX and DAPS corpora are used to evaluate the efficacy of the proposed method, which also outperforms the state-of-the-art methods. ",
    "url": "https://arxiv.org/abs/2206.04962",
    "authors": [
      "Yi Li",
      "ShuangLin Li",
      "Yang Sun",
      "Syed Mohsen Naqvi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2206.04975",
    "title": "NR-DFERNet: Noise-Robust Network for Dynamic Facial Expression  Recognition",
    "abstract": "Dynamic facial expression recognition (DFER) in the wild is an extremely challenging task, due to a large number of noisy frames in the video sequences. Previous works focus on extracting more discriminative features, but ignore distinguishing the key frames from the noisy frames. To tackle this problem, we propose a noise-robust dynamic facial expression recognition network (NR-DFERNet), which can effectively reduce the interference of noisy frames on the DFER task. Specifically, at the spatial stage, we devise a dynamic-static fusion module (DSF) that introduces dynamic features to static features for learning more discriminative spatial features. To suppress the impact of target irrelevant frames, we introduce a novel dynamic class token (DCT) for the transformer at the temporal stage. Moreover, we design a snippet-based filter (SF) at the decision stage to reduce the effect of too many neutral frames on non-neutral sequence classification. Extensive experimental results demonstrate that our NR-DFERNet outperforms the state-of-the-art methods on both the DFEW and AFEW benchmarks. ",
    "url": "https://arxiv.org/abs/2206.04975",
    "authors": [
      "Hanting Li",
      "Mingzhe Sui",
      "Zhaoqing Zhu",
      "Feng zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04976",
    "title": "Refining neural network predictions using background knowledge",
    "abstract": "Recent work has showed we can use logical background knowledge in learning system to compensate for a lack of labeled training data. Many such methods work by creating a loss function that encodes this knowledge. However, often the logic is discarded after training, even if it is still useful at test-time. Instead, we ensure neural network predictions satisfy the knowledge by refining the predictions with an extra computation step. We introduce differentiable refinement functions that find a corrected prediction close to the original prediction. We study how to effectively and efficiently compute these refinement functions. Using a new algorithm, we combine refinement functions to find refined predictions for logical formulas of any complexity. This algorithm finds optimal refinements on complex SAT formulas in significantly fewer iterations and frequently finds solutions where gradient descent can not. ",
    "url": "https://arxiv.org/abs/2206.04976",
    "authors": [
      "Alessandro Daniele",
      "Emile van Krieken",
      "Luciano Serafini",
      "Frank van Harmelen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.04979",
    "title": "Convolutional Layers Are Not Translation Equivariant",
    "abstract": "The purpose of this paper is to correct a misconception about convolutional neural networks (CNNs). CNNs are made up of convolutional layers which are shift equivariant due to weight sharing. However, contrary to popular belief, convolutional layers are not translation equivariant, even when boundary effects are ignored and when pooling and subsampling are absent. This is because shift equivariance is a discrete symmetry while translation equivariance is a continuous symmetry. That discrete systems do not in general inherit continuous equivariances is a fundamental limitation of equivariant deep learning. We discuss two implications of this fact. First, CNNs have achieved success in image processing despite not inheriting the translation equivariance of the physical systems they model. Second, using CNNs to solve partial differential equations (PDEs) will not result in translation equivariant solvers. ",
    "url": "https://arxiv.org/abs/2206.04979",
    "authors": [
      "Nick McGreivy",
      "Ammar Hakim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04981",
    "title": "Position Labels for Self-Supervised Vision Transformer",
    "abstract": "Position encoding is important for vision transformer (ViT) to capture the spatial structure of the input image. General efficacy has been proven in ViT. In our work we propose to train ViT to recognize the 2D position encoding of patches of the input image, this apparently simple task actually yields a meaningful self-supervisory task. Based on previous work on ViT position encoding, we propose two position labels dedicated to 2D images including absolute position and relative position. Our position labels can be easily plugged into transformer, combined with the various current ViT variants. It can work in two ways: 1.As an auxiliary training target for vanilla ViT (e.g., ViT-B and Swin-B) to improve model performance. 2. Combine the self-supervised ViT (e.g., MAE) to provide a more powerful self-supervised signal for semantic feature learning. Experiments demonstrate that solely due to the proposed self-supervised methods, Swin-B and ViT-B obtained improvements of 1.9% (top-1 Acc) and 5.6% (top-1 Acc) on Mini-ImageNet, respectively. ",
    "url": "https://arxiv.org/abs/2206.04981",
    "authors": [
      "Zhemin Zhang",
      "Xun Gong",
      "Jinyi Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.05015",
    "title": "A Simple Yet Efficient Method for Adversarial Word-Substitute Attack",
    "abstract": "NLP researchers propose different word-substitute black-box attacks that can fool text classification models. In such attack, an adversary keeps sending crafted adversarial queries to the target model until it can successfully achieve the intended outcome. State-of-the-art attack methods usually require hundreds or thousands of queries to find one adversarial example. In this paper, we study whether a sophisticated adversary can attack the system with much less queries. We propose a simple yet efficient method that can reduce the average number of adversarial queries by 3-30 times and maintain the attack effectiveness. This research highlights that an adversary can fool a deep NLP model with much less cost. ",
    "url": "https://arxiv.org/abs/2206.05015",
    "authors": [
      "Tianle Li",
      "Yi Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.05028",
    "title": "Spatial Cross-Attention Improves Self-Supervised Visual Representation  Learning",
    "abstract": "Unsupervised representation learning methods like SwAV are proved to be effective in learning visual semantics of a target dataset. The main idea behind these methods is that different views of a same image represent the same semantics. In this paper, we further introduce an add-on module to facilitate the injection of the knowledge accounting for spatial cross correlations among the samples. This in turn results in distilling intra-class information including feature level locations and cross similarities between same-class instances. The proposed add-on can be added to existing methods such as the SwAV. We can later remove the add-on module for inference without any modification of the learned weights. Through an extensive set of empirical evaluations, we verify that our method yields an improved performance in detecting the class activation maps, top-1 classification accuracy, and down-stream tasks such as object detection, with different configuration settings. ",
    "url": "https://arxiv.org/abs/2206.05028",
    "authors": [
      "Mehdi Seyfi",
      "Amin Banitalebi-Dehkordi",
      "Yong Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.05056",
    "title": "On Neural Architecture Inductive Biases for Relational Tasks",
    "abstract": "Current deep learning approaches have shown good in-distribution generalization performance, but struggle with out-of-distribution generalization. This is especially true in the case of tasks involving abstract relations like recognizing rules in sequences, as we find in many intelligence tests. Recent work has explored how forcing relational representations to remain distinct from sensory representations, as it seems to be the case in the brain, can help artificial systems. Building on this work, we further explore and formalize the advantages afforded by 'partitioned' representations of relations and sensory details, and how this inductive bias can help recompose learned relational structure in newly encountered settings. We introduce a simple architecture based on similarity scores which we name Compositional Relational Network (CoRelNet). Using this model, we investigate a series of inductive biases that ensure abstract relations are learned and represented distinctly from sensory data, and explore their effects on out-of-distribution generalization for a series of relational psychophysics tasks. We find that simple architectural choices can outperform existing models in out-of-distribution generalization. Together, these results show that partitioning relational representations from other information streams may be a simple way to augment existing network architectures' robustness when performing out-of-distribution relational computations. ",
    "url": "https://arxiv.org/abs/2206.05056",
    "authors": [
      "Giancarlo Kerg",
      "Sarthak Mittal",
      "David Rolnick",
      "Yoshua Bengio",
      "Blake Richards",
      "Guillaume Lajoie"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.05060",
    "title": "Social Network Structure Shapes Innovation: Experience-sharing in RL  with SAPIENS",
    "abstract": "The human cultural repertoire relies on innovation: our ability to continuously and hierarchically explore how existing elements can be combined to create new ones. Innovation is not solitary, it relies on collective accumulation and merging of previous solutions. Machine learning approaches commonly assume that fully connected multi-agent networks are best suited for innovation. However, human laboratory and field studies have shown that hierarchical innovation is more robustly achieved by dynamic communication topologies. In dynamic topologies, humans oscillate between innovating individually or in small clusters, and then sharing outcomes with others. To our knowledge, the role of multi-agent topology on innovation has not been systematically studied in machine learning. It remains unclear a) which communication topologies are optimal for which innovation tasks, and b) which properties of experience sharing improve multi-level innovation. Here we use a multi-level hierarchical problem setting (WordCraft), with three different innovation tasks. We systematically design networks of DQNs sharing experiences from their replay buffers in varying topologies (fully connected, small world, dynamic, ring). Comparing the level of innovation achieved by different experience-sharing topologies across different tasks shows that, first, consistent with human findings, experience sharing within a dynamic topology achieves the highest level of innovation across tasks. Second, experience sharing is not as helpful when there is a single clear path to innovation. Third, two metrics we propose, conformity and diversity of shared experience, can explain the success of different topologies on different tasks. These contributions can advance our understanding of optimal AI-AI, human-human, and human-AI collaborative networks, inspiring future tools for fostering collective innovation in large organizations. ",
    "url": "https://arxiv.org/abs/2206.05060",
    "authors": [
      "Eleni Nisioti",
      "Mateo Mahaut",
      "Pierre-Yves Oudeyer",
      "Ida Momennejad",
      "Cl\u00e9ment Moulin-Frier"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2206.05070",
    "title": "We Cannot Guarantee Safety: The Undecidability of Graph Neural Network  Verification",
    "abstract": "Graph Neural Networks (GNN) are commonly used for two tasks: (whole) graph classification and node classification. We formally introduce generically formulated decision problems for both tasks, corresponding to the following pattern: given a GNN, some specification of valid inputs, and some specification of valid outputs, decide whether there is a valid input satisfying the output specification. We then prove that graph classifier verification is undecidable in general, implying that there cannot be an algorithm surely guaranteeing the absence of misclassification of any kind. Additionally, we show that verification in the node classification case becomes decidable as soon as we restrict the degree of the considered graphs. Furthermore, we discuss possible changes to these results depending on the considered GNN model and specifications. ",
    "url": "https://arxiv.org/abs/2206.05070",
    "authors": [
      "Marco S\u00e4lzer",
      "Martin Lange"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2206.05086",
    "title": "Finite Model Theory and Proof Complexity revisited: Distinguishing  graphs in Choiceless Polynomial Time and the Extended Polynomial Calculus",
    "abstract": "This paper extends prior work on the connections between logics from finite model theory and propositional/algebraic proof systems. We show that if all non-isomorphic graphs in a given graph class can be distinguished in the logic Choiceless Polynomial Time with counting (CPT), then they can also be distinguished in the bounded-degree extended polynomial calculus (EPC), and the refutations have roughly the same size as the resource consumption of the CPT-sentence. This allows to transfer lower bounds for EPC to CPT and thus constitutes a new potential approach towards better understanding the limits of CPT. A super-polynomial EPC lower bound for a PTIME-instance of the graph isomorphism problem would separate CPT from PTIME and thus solve a major open question in finite model theory. Further, using our result, we provide a model theoretic proof for the separation of bounded-degree polynomial calculus and bounded-degree extended polynomial calculus. ",
    "url": "https://arxiv.org/abs/2206.05086",
    "authors": [
      "Benedikt Pago"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2206.05091",
    "title": "Muffliato: Peer-to-Peer Privacy Amplification for Decentralized  Optimization and Averaging",
    "abstract": "Decentralized optimization is increasingly popular in machine learning for its scalability and efficiency. Intuitively, it should also provide better privacy guarantees, as nodes only observe the messages sent by their neighbors in the network graph. But formalizing and quantifying this gain is challenging: existing results are typically limited to Local Differential Privacy (LDP) guarantees that overlook the advantages of decentralization. In this work, we introduce pairwise network differential privacy, a relaxation of LDP that captures the fact that the privacy leakage from a node $u$ to a node $v$ may depend on their relative position in the graph. We then analyze the combination of local noise injection with (simple or randomized) gossip averaging protocols on fixed and random communication graphs. We also derive a differentially private decentralized optimization algorithm that alternates between local gradient descent steps and gossip averaging. Our results show that our algorithms amplify privacy guarantees as a function of the distance between nodes in the graph, matching the privacy-utility trade-off of the trusted curator, up to factors that explicitly depend on the graph topology. Finally, we illustrate our privacy gains with experiments on synthetic and real-world datasets. ",
    "url": "https://arxiv.org/abs/2206.05091",
    "authors": [
      "Edwige Cyffers",
      "Mathieu Even",
      "Aur\u00e9lien Bellet",
      "Laurent Massouli\u00e9"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.05099",
    "title": "SimVP: Simpler yet Better Video Prediction",
    "abstract": "From CNN, RNN, to ViT, we have witnessed remarkable advancements in video prediction, incorporating auxiliary inputs, elaborate neural architectures, and sophisticated training strategies. We admire these progresses but are confused about the necessity: is there a simple method that can perform comparably well? This paper proposes SimVP, a simple video prediction model that is completely built upon CNN and trained by MSE loss in an end-to-end fashion. Without introducing any additional tricks and complicated strategies, we can achieve state-of-the-art performance on five benchmark datasets. Through extended experiments, we demonstrate that SimVP has strong generalization and extensibility on real-world datasets. The significant reduction of training cost makes it easier to scale to complex scenarios. We believe SimVP can serve as a solid baseline to stimulate the further development of video prediction. The code is available at \\href{https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction}{Github}. ",
    "url": "https://arxiv.org/abs/2206.05099",
    "authors": [
      "Zhangyang Gao",
      "Cheng Tan",
      "Lirong Wu",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.05102",
    "title": "Saccade Mechanisms for Image Classification, Object Detection and  Tracking",
    "abstract": "We examine how the saccade mechanism from biological vision can be used to make deep neural networks more efficient for classification and object detection problems. Our proposed approach is based on the ideas of attention-driven visual processing and saccades, miniature eye movements influenced by attention. We conduct experiments by analyzing: i) the robustness of different deep neural network (DNN) feature extractors to partially-sensed images for image classification and object detection, and ii) the utility of saccades in masking image patches for image classification and object tracking. Experiments with convolutional nets (ResNet-18) and transformer-based models (ViT, DETR, TransTrack) are conducted on several datasets (CIFAR-10, DAVSOD, MSCOCO, and MOT17). Our experiments show intelligent data reduction via learning to mimic human saccades when used in conjunction with state-of-the-art DNNs for classification, detection, and tracking tasks. We observed minimal drop in performance for the classification and detection tasks while only using about 30\\% of the original sensor data. We discuss how the saccade mechanism can inform hardware design via ``in-pixel'' processing. ",
    "url": "https://arxiv.org/abs/2206.05102",
    "authors": [
      "Saurabh Farkya",
      "Zachary Daniels",
      "Aswin Nadamuni Raghavan",
      "David Zhang",
      "Michael Piacentino"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2206.05139",
    "title": "Finite electro-elasticity with physics-augmented neural networks",
    "abstract": "In the present work, a machine learning based constitutive model for electro-mechanically coupled material behavior at finite deformations is proposed. Using different sets of invariants as inputs, an internal energy density is formulated as a convex neural network. In this way, the model fulfills the polyconvexity condition which ensures material stability, as well as thermodynamic consistency, objectivity, material symmetry, and growth conditions. Depending on the considered invariants, this physics-augmented machine learning model can either be applied for compressible or nearly incompressible material behavior, as well as for arbitrary material symmetry classes. The applicability and versatility of the approach is demonstrated by calibrating it on transversely isotropic data generated with an analytical potential, as well as for the effective constitutive modeling of an analytically homogenized, transversely isotropic rank-one laminate composite and a numerically homogenized cubic metamaterial. These examinations show the excellent generalization properties that physics-augmented neural networks offer also for multi-physical material modeling such as nonlinear electro-elasticity. ",
    "url": "https://arxiv.org/abs/2206.05139",
    "authors": [
      "Dominik K. Klein",
      "Rogelio Ortigosa",
      "Jes\u00fas Mart\u00ednez-Frutos",
      "Oliver Weeger"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2206.05146",
    "title": "Global Internet public peering capacity of interconnection: a complex  network analysis",
    "abstract": "A massive and growing part of Autonomous System (AS)-level traffic exchanges takes place at Internet Exchange Points (IXPs). This paper leverages PeeringDB, a database providing a partial but reasonable view of the global interconnection of ASes at IXPs, to model a complex graph enabling the characterization of the key Internet peering players and their interactions over time. We model a PeeringDB snapshot as a weighted directed bipartite graph, called the pDB c-graph, that captures the port size ASes possess at IXPs using available metadata. This novel model of the Internet is shown to picture relevant features of a complex network that groups ASes and IXPs in geographical areas of influence. From this model, we extract central players of public peering such as hypergiant AS content providers and major regional traffic receivers. Most importantly, this graph model opens the way to apply spectral analysis using reduced Google matrix in order to retrieve the intensity of possible interactions between ASes on the basis of pure connectivity information. As an illustration, we retrieve the timely evolution of the peering network to show how the central content and cloud providers have increased their reach to eyeball networks during Covid-19 pandemic. ",
    "url": "https://arxiv.org/abs/2206.05146",
    "authors": [
      "Justin Loye",
      "Sandrine Mouysset",
      "Marc Bruy\u00e8re",
      "Katia Jaffr\u00e8s-Runser"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2206.05174",
    "title": "Near-Optimal Distributed Dominating Set in Bounded Arboricity Graphs",
    "abstract": "We describe a simple deterministic $O( \\varepsilon^{-1} \\log \\Delta)$ round distributed algorithm for $(2\\alpha+1)(1 + \\varepsilon)$ approximation of minimum weighted dominating set on graphs with arboricity at most $\\alpha$. Here $\\Delta$ denotes the maximum degree. We also show a lower bound proving that this round complexity is nearly optimal even for the unweighted case, via a reduction from the celebrated KMW lower bound on distributed vertex cover approximation [Kuhn, Moscibroda, and Wattenhofer JACM'16]. Our algorithm improves on all the previous results (that work only for unweighted graphs) including a randomized $O(\\alpha^2)$ approximation in $O(\\log n)$ rounds [Lenzen and Wattenhofer DISC'10], a deterministic $O(\\alpha \\log \\Delta)$ approximation in $O(\\log \\Delta)$ rounds [Lenzen and Wattenhofer DISC'10], a deterministic $O(\\alpha)$ approximation in $O(\\log^2 \\Delta)$ rounds [implicit in Bansal and Umboh IPL'17 and Kuhn, Moscibroda, and Wattenhofer SODA'06], and a randomized $O(\\alpha)$ approximation in $O(\\alpha\\log n)$ rounds [Morgan, Solomon and Wein DISC'21]. We also provide a randomized $O(\\alpha \\log\\Delta)$ round distributed algorithm that sharpens the approximation factor to $\\alpha(1+o(1))$. If each node is restricted to do polynomial-time computations, our approximation factor is tight in the first order as it is NP-hard to achieve $\\alpha - 1 - \\varepsilon$ approximation [Bansal and Umboh IPL'17]. ",
    "url": "https://arxiv.org/abs/2206.05174",
    "authors": [
      "Michal Dory",
      "Mohsen Ghaffari",
      "Saeed Ilchi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2206.05184",
    "title": "Exploring Feature Self-relation for Self-supervised Transformer",
    "abstract": "Learning representations with self-supervision for convolutional networks (CNN) has proven effective for vision tasks. As an alternative for CNN, vision transformers (ViTs) emerge strong representation ability with the pixel-level self-attention and channel-level feed-forward networks. Recent works reveal that self-supervised learning helps unleash the great potential of ViTs. Still, most works follow self-supervised strategy designed for CNNs, e.g., instance-level discrimination of samples, but they ignore the unique properties of ViTs. We observe that modeling relations among pixels and channels distinguishes ViTs from other networks. To enforce this property, we explore the feature self-relations for training self-supervised ViTs. Specifically, instead of conducting self-supervised learning solely on feature embeddings from multiple views, we utilize the feature self-relations, i.e., pixel/channel-level self-relations, for self-supervised learning. Self-relation based learning further enhance the relation modeling ability of ViTs, resulting in strong representations that stably improve performance on multiple downstream tasks. Our source code will be made publicly available. ",
    "url": "https://arxiv.org/abs/2206.05184",
    "authors": [
      "Zhong-Yu Li",
      "Shanghua Gao",
      "Ming-Ming Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.05199",
    "title": "Bayesian Estimation of Differential Privacy",
    "abstract": "Algorithms such as Differentially Private SGD enable training machine learning models with formal privacy guarantees. However, there is a discrepancy between the protection that such algorithms guarantee in theory and the protection they afford in practice. An emerging strand of work empirically estimates the protection afforded by differentially private training as a confidence interval for the privacy budget $\\varepsilon$ spent on training a model. Existing approaches derive confidence intervals for $\\varepsilon$ from confidence intervals for the false positive and false negative rates of membership inference attacks. Unfortunately, obtaining narrow high-confidence intervals for $\\epsilon$ using this method requires an impractically large sample size and training as many models as samples. We propose a novel Bayesian method that greatly reduces sample size, and adapt and validate a heuristic to draw more than one sample per trained model. Our Bayesian method exploits the hypothesis testing interpretation of differential privacy to obtain a posterior for $\\varepsilon$ (not just a confidence interval) from the joint posterior of the false positive and false negative rates of membership inference attacks. For the same sample size and confidence, we derive confidence intervals for $\\varepsilon$ around 40% narrower than prior work. The heuristic, which we adapt from label-only DP, can be used to further reduce the number of trained models needed to get enough samples by up to 2 orders of magnitude. ",
    "url": "https://arxiv.org/abs/2206.05199",
    "authors": [
      "Santiago Zanella-B\u00e9guelin",
      "Lukas Wutschitz",
      "Shruti Tople",
      "Ahmed Salem",
      "Victor R\u00fchle",
      "Andrew Paverd",
      "Mohammad Naseri",
      "Boris K\u00f6pf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2206.05209",
    "title": "Hierarchical Federated Learning with Privacy",
    "abstract": "Federated learning (FL), where data remains at the federated clients, and where only gradient updates are shared with a central aggregator, was assumed to be private. Recent work demonstrates that adversaries with gradient-level access can mount successful inference and reconstruction attacks. In such settings, differentially private (DP) learning is known to provide resilience. However, approaches used in the status quo (\\ie central and local DP) introduce disparate utility vs. privacy trade-offs. In this work, we take the first step towards mitigating such trade-offs through {\\em hierarchical FL (HFL)}. We demonstrate that by the introduction of a new intermediary level where calibrated DP noise can be added, better privacy vs. utility trade-offs can be obtained; we term this {\\em hierarchical DP (HDP)}. Our experiments with 3 different datasets (commonly used as benchmarks for FL) suggest that HDP produces models as accurate as those obtained using central DP, where noise is added at a central aggregator. Such an approach also provides comparable benefit against inference adversaries as in the local DP case, where noise is added at the federated clients. ",
    "url": "https://arxiv.org/abs/2206.05209",
    "authors": [
      "Varun Chandrasekaran",
      "Suman Banerjee",
      "Diego Perino",
      "Nicolas Kourtellis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2206.05224",
    "title": "A Multi-Task Benchmark for Korean Legal Language Understanding and  Judgement Prediction",
    "abstract": "The recent advances of deep learning have dramatically changed how machine learning, especially in the domain of natural language processing, can be applied to legal domain. However, this shift to the data-driven approaches calls for larger and more diverse datasets, which are nevertheless still small in number, especially in non-English languages. Here we present the first large-scale benchmark of Korean legal AI datasets, LBox Open, that consists of one legal corpus, two classification tasks, two legal judgement prediction (LJP) tasks, and one summarization task. The legal corpus consists of 150k Korean precedents (264M tokens), of which 63k are sentenced in last 4 years and 96k are from the first and the second level courts in which factual issues are reviewed. The two classification tasks are case names (10k) and statutes (3k) prediction from the factual description of individual cases. The LJP tasks consist of (1) 11k criminal examples where the model is asked to predict fine amount, imprisonment with labor, and imprisonment without labor ranges for the given facts, and (2) 5k civil examples where the inputs are facts and claim for relief and outputs are the degrees of claim acceptance. The summarization task consists of the Supreme Court precedents and the corresponding summaries. We also release LCube, the first Korean legal language model trained on the legal corpus from this study. Given the uniqueness of the Law of South Korea and the diversity of the legal tasks covered in this work, we believe that LBox Open contributes to the multilinguality of global legal research. LBox Open and LCube will be publicly available. ",
    "url": "https://arxiv.org/abs/2206.05224",
    "authors": [
      "Wonseok Hwang",
      "Dongjun Lee",
      "Kyoungyeon Cho",
      "Hanuhl Lee",
      "Minjoon Seo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.05238",
    "title": "Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus  Creation, Annotation Reliability, and Prediction",
    "abstract": "The most prominent tasks in emotion analysis are to assign emotions to texts and to understand how emotions manifest in language. An important observation for natural language processing is that emotions can be communicated implicitly by referring to events alone, appealing to an empathetic, intersubjective understanding of events, even without explicitly mentioning an emotion name. In psychology, the class of emotion theories known as appraisal theories aims at explaining the link between events and emotions. Appraisals can be formalized as variables that measure a cognitive evaluation by people living through an event that they consider relevant. They include the assessment if an event is novel, if the person considers themselves to be responsible, if it is in line with the own goals, and many others. Such appraisals explain which emotions are developed based on an event, e.g., that a novel situation can induce surprise or one with uncertain consequences could evoke fear. We analyze the suitability of appraisal theories for emotion analysis in text with the goal of understanding if appraisal concepts can reliably be reconstructed by annotators, if they can be predicted by text classifiers, and if appraisal concepts help to identify emotion categories. To achieve that, we compile a corpus by asking people to textually describe events that triggered particular emotions and to disclose their appraisals. Then, we ask readers to reconstruct emotions and appraisals from the text. This setup allows us to measure if emotions and appraisals can be recovered purely from text and provides a human baseline to judge model's performance measures. Our comparison of text classification methods to human annotators shows that both can reliably detect emotions and appraisals with similar performance. We further show that appraisal concepts improve the categorization of emotions in text. ",
    "url": "https://arxiv.org/abs/2206.05238",
    "authors": [
      "Enrica Troiano",
      "Laura Oberl\u00e4nder",
      "Roman Klinger"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2206.05239",
    "title": "StructCoder: Structure-Aware Transformer for Code Generation",
    "abstract": "There has been a recent surge of interest in automating software engineering tasks using deep learning. This work addresses the problem of code generation where the goal is to generate target code given source code in a different language or a natural language description. Most of the state-of-the-art deep learning models for code generation use training strategies that are primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model where both the encoder and decoder are trained to recognize the syntax and data flow in the source and target codes, respectively. We not only make the encoder structure-aware by leveraging the source code's syntax tree and data flow graph, but we also ensure that our decoder preserves the syntax and data flow of the target code by introducing two auxiliary tasks: AST (Abstract Syntax Tree) paths prediction and data flow prediction. To the best of our knowledge, this is the first work to introduce a structure-aware Transformer decoder to enhance the quality of generated code by modeling target syntax and data flow. The proposed StructCoder model achieves state-of-the-art performance on code translation and text-to-code generation tasks in the CodeXGLUE benchmark. ",
    "url": "https://arxiv.org/abs/2206.05239",
    "authors": [
      "Sindhu Tipirneni",
      "Ming Zhu",
      "Chandan K. Reddy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2206.05253",
    "title": "Rethinking Spatial Invariance of Convolutional Networks for Object  Counting",
    "abstract": "Previous work generally believes that improving the spatial invariance of convolutional networks is the key to object counting. However, after verifying several mainstream counting networks, we surprisingly found too strict pixel-level spatial invariance would cause overfit noise in the density map generation. In this paper, we try to use locally connected Gaussian kernels to replace the original convolution filter to estimate the spatial position in the density map. The purpose of this is to allow the feature extraction process to potentially stimulate the density map generation process to overcome the annotation noise. Inspired by previous work, we propose a low-rank approximation accompanied with translation invariance to favorably implement the approximation of massive Gaussian convolution. Our work points a new direction for follow-up research, which should investigate how to properly relax the overly strict pixel-level spatial invariance for object counting. We evaluate our methods on 4 mainstream object counting networks (i.e., MCNN, CSRNet, SANet, and ResNet-50). Extensive experiments were conducted on 7 popular benchmarks for 3 applications (i.e., crowd, vehicle, and plant counting). Experimental results show that our methods significantly outperform other state-of-the-art methods and achieve promising learning of the spatial position of objects. ",
    "url": "https://arxiv.org/abs/2206.05253",
    "authors": [
      "Zhi-Qi Cheng",
      "Qi Dai",
      "Hong Li",
      "JingKuan Song",
      "Xiao Wu",
      "Alexander G. Hauptmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2206.05259",
    "title": "Is Self-Supervised Learning More Robust Than Supervised Learning?",
    "abstract": "Self-supervised contrastive learning is a powerful tool to learn visual representation without labels. Prior work has primarily focused on evaluating the recognition accuracy of various pre-training algorithms, but has overlooked other behavioral aspects. In addition to accuracy, distributional robustness plays a critical role in the reliability of machine learning models. We design and conduct a series of robustness tests to quantify the behavioral differences between contrastive learning and supervised learning to downstream or pre-training data distribution changes. These tests leverage data corruptions at multiple levels, ranging from pixel-level gamma distortion to patch-level shuffling and to dataset-level distribution shift. Our tests unveil intriguing robustness behaviors of contrastive and supervised learning. On the one hand, under downstream corruptions, we generally observe that contrastive learning is surprisingly more robust than supervised learning. On the other hand, under pre-training corruptions, we find contrastive learning vulnerable to patch shuffling and pixel intensity change, yet less sensitive to dataset-level distribution change. We attempt to explain these results through the role of data augmentation and feature space properties. Our insight has implications in improving the downstream robustness of supervised learning. ",
    "url": "https://arxiv.org/abs/2206.05259",
    "authors": [
      "Yuanyi Zhong",
      "Haoran Tang",
      "Junkun Chen",
      "Jian Peng",
      "Yu-Xiong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.05263",
    "title": "Causal Balancing for Domain Generalization",
    "abstract": "While machine learning models rapidly advance the state-of-the-art on various real-world tasks, out-of-domain (OOD) generalization remains a challenging problem given the vulnerability of these models to spurious correlations. While current domain generalization methods usually focus on enforcing certain invariance properties across different domains by new loss function designs, we propose a balanced mini-batch sampling strategy to reduce the domain-specific spurious correlations in the observed training distributions. More specifically, we propose a two-phased method that 1) identifies the source of spurious correlations, and 2) builds balanced mini-batches free from spurious correlations by matching on the identified source. We provide an identifiability guarantee of the source of spuriousness and show that our proposed approach provably samples from a balanced, spurious-free distribution over all training environments. Experiments are conducted on three computer vision datasets with documented spurious correlations, demonstrating empirically that our balanced mini-batch sampling strategy improves the performance of four different established domain generalization model baselines compared to the random mini-batch sampling strategy. ",
    "url": "https://arxiv.org/abs/2206.05263",
    "authors": [
      "Xinyi Wang",
      "Michael Saxon",
      "Jiachen Li",
      "Hongyang Zhang",
      "Kun Zhang",
      "William Yang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.05266",
    "title": "Does Self-supervised Learning Really Improve Reinforcement Learning from  Pixels?",
    "abstract": "We investigate whether self-supervised learning (SSL) can improve online reinforcement learning (RL) from pixels. We extend the contrastive reinforcement learning framework (e.g., CURL) that jointly optimizes SSL and RL losses and conduct an extensive amount of experiments with various self-supervised losses. Our observations suggest that the existing SSL framework for RL fails to bring meaningful improvement over the baselines only taking advantage of image augmentation when the same amount of data and augmentation is used. We further perform an evolutionary search to find the optimal combination of multiple self-supervised losses for RL, but find that even such a loss combination fails to meaningfully outperform the methods that only utilize carefully designed image augmentations. Often, the use of self-supervised losses under the existing framework lowered RL performances. We evaluate the approach in multiple different environments including a real-world robot environment and confirm that no single self-supervised loss or image augmentation method can dominate all environments and that the current framework for joint optimization of SSL and RL is limited. Finally, we empirically investigate the pretraining framework for SSL + RL and the properties of representations learned with different approaches. ",
    "url": "https://arxiv.org/abs/2206.05266",
    "authors": [
      "Xiang Li",
      "Jinghuan Shang",
      "Srijan Das",
      "Michael S. Ryoo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2206.04682",
    "title": "RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search  for 3D Cardiac Cine MRI Segmentation",
    "abstract": "Accurately segmenting temporal frames of cine magnetic resonance imaging (MRI) is a crucial step in various real-time MRI guided cardiac interventions. To achieve fast and accurate visual assistance, there are strict requirements on the maximum latency and minimum throughput of the segmentation framework. State-of-the-art neural networks on this task are mostly hand-crafted to satisfy these constraints while achieving high accuracy. On the other hand, while existing literature have demonstrated the power of neural architecture search (NAS) in automatically identifying the best neural architectures for various medical applications, they are mostly guided by accuracy, sometimes with computation complexity, and the importance of real-time constraints are overlooked. A major challenge is that such constraints are non-differentiable and are thus not compatible with the widely used differentiable NAS frameworks. In this paper, we present a strategy that directly handles real-time constraints in a differentiable NAS framework named RT-DNAS. Experiments on extended 2017 MICCAI ACDC dataset show that compared with state-of-the-art manually and automatically designed architectures, RT-DNAS is able to identify ones with better accuracy while satisfying the real-time constraints. ",
    "url": "https://arxiv.org/abs/2206.04682",
    "authors": [
      "Qing Lu",
      "Xiaowei Xu",
      "Shunjie Dong",
      "Callie Hao",
      "Lei Yang",
      "Cheng Zhuo",
      "Yiyu Shi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04683",
    "title": "Molecular dynamics without molecules: searching the conformational space  of proteins with generative neural networks",
    "abstract": "All-atom and coarse-grained molecular dynamics are two widely used computational tools to study the conformational states of proteins. Yet, these two simulation methods suffer from the fact that without access to supercomputing resources, the time and length scales at which these states become detectable are difficult to achieve. One alternative to such methods is based on encoding the atomistic trajectory of molecular dynamics as a shorthand version devoid of physical particles, and then learning to propagate the encoded trajectory through the use of artificial intelligence. Here we show that a simple textual representation of the frames of molecular dynamics trajectories as vectors of Ramachandran basin classes retains most of the structural information of the full atomistic representation of a protein in each frame, and can be used to generate equivalent atom-less trajectories suitable to train different types of generative neural networks. In turn, the trained generative models can be used to extend indefinitely the atom-less dynamics or to sample the conformational space of proteins from their representation in the models latent space. We define intuitively this methodology as molecular dynamics without molecules, and show that it enables to cover physically relevant states of proteins that are difficult to access with traditional molecular dynamics. ",
    "url": "https://arxiv.org/abs/2206.04683",
    "authors": [
      "Gregory Schwing",
      "Luigi L. Palese",
      "Ariel Fern\u00e1ndez",
      "Loren Schwiebert",
      "Domenico L. Gatti"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2206.04684",
    "title": "Structure-consistent Restoration Network for Cataract Fundus Image  Enhancement",
    "abstract": "Fundus photography is a routine examination in clinics to diagnose and monitor ocular diseases. However, for cataract patients, the fundus image always suffers quality degradation caused by the clouding lens. The degradation prevents reliable diagnosis by ophthalmologists or computer-aided systems. To improve the certainty in clinical diagnosis, restoration algorithms have been proposed to enhance the quality of fundus images. Unfortunately, challenges remain in the deployment of these algorithms, such as collecting sufficient training data and preserving retinal structures. In this paper, to circumvent the strict deployment requirement, a structure-consistent restoration network (SCR-Net) for cataract fundus images is developed from synthesized data that shares an identical structure. A cataract simulation model is firstly designed to collect synthesized cataract sets (SCS) formed by cataract fundus images sharing identical structures. Then high-frequency components (HFCs) are extracted from the SCS to constrain structure consistency such that the structure preservation in SCR-Net is enforced. The experiments demonstrate the effectiveness of SCR-Net in the comparison with state-of-the-art methods and the follow-up clinical applications. The code is available at https://github.com/liamheng/ArcNet-Medical-Image-Enhancement. ",
    "url": "https://arxiv.org/abs/2206.04684",
    "authors": [
      "Heng Li",
      "Haofeng Liu",
      "Huazhu Fu",
      "Hai Shu",
      "Yitian Zhao",
      "Xiaoling Luo",
      "Yan Hu",
      "Jiang Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04689",
    "title": "AI-based Clinical Assessment of Optic Nerve Head Robustness Superseding  Biomechanical Testing",
    "abstract": "$\\mathbf{Purpose}$: To use artificial intelligence (AI) to: (1) exploit biomechanical knowledge of the optic nerve head (ONH) from a relatively large population; (2) assess ONH robustness from a single optical coherence tomography (OCT) scan of the ONH; (3) identify what critical three-dimensional (3D) structural features make a given ONH robust. $\\mathbf{Design}$: Retrospective cross-sectional study. $\\mathbf{Methods}$: 316 subjects had their ONHs imaged with OCT before and after acute intraocular pressure (IOP) elevation through ophthalmo-dynamometry. IOP-induced lamina-cribrosa deformations were then mapped in 3D and used to classify ONHs. Those with LC deformations superior to 4% were considered fragile, while those with deformations inferior to 4% robust. Learning from these data, we compared three AI algorithms to predict ONH robustness strictly from a baseline (undeformed) OCT volume: (1) a random forest classifier; (2) an autoencoder; and (3) a dynamic graph CNN (DGCNN). The latter algorithm also allowed us to identify what critical 3D structural features make a given ONH robust. $\\mathbf{Results}$: All 3 methods were able to predict ONH robustness from 3D structural information alone and without the need to perform biomechanical testing. The DGCNN (area under the receiver operating curve [AUC]: 0.76 $\\pm$ 0.08) outperformed the autoencoder (AUC: 0.70 $\\pm$ 0.07) and the random forest classifier (AUC: 0.69 $\\pm$ 0.05). Interestingly, to assess ONH robustness, the DGCNN mainly used information from the scleral canal and the LC insertion sites. $\\mathbf{Conclusions}$: We propose an AI-driven approach that can assess the robustness of a given ONH solely from a single OCT scan of the ONH, and without the need to perform biomechanical testing. Longitudinal studies should establish whether ONH robustness could help us identify fast visual field loss progressors. ",
    "url": "https://arxiv.org/abs/2206.04689",
    "authors": [
      "Fabian A. Braeu",
      "Thanadet Chuangsuwanich",
      "Tin A. Tun",
      "Alexandre H. Thiery",
      "Tin Aung",
      "George Barbastathis",
      "Micha\u00ebl J.A. Girard"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04727",
    "title": "STNDT: Modeling Neural Population Activity with a Spatiotemporal  Transformer",
    "abstract": "Modeling neural population dynamics underlying noisy single-trial spiking activities is essential for relating neural observation and behavior. A recent non-recurrent method - Neural Data Transformers (NDT) - has shown great success in capturing neural dynamics with low inference latency without an explicit dynamical model. However, NDT focuses on modeling the temporal evolution of the population activity while neglecting the rich covariation between individual neurons. In this paper we introduce SpatioTemporal Neural Data Transformer (STNDT), an NDT-based architecture that explicitly models responses of individual neurons in the population across time and space to uncover their underlying firing rates. In addition, we propose a contrastive learning loss that works in accordance with mask modeling objective to further improve the predictive performance. We show that our model achieves state-of-the-art performance on ensemble level in estimating neural activities across four neural datasets, demonstrating its capability to capture autonomous and non-autonomous dynamics spanning different cortical regions while being completely agnostic to the specific behaviors at hand. Furthermore, STNDT spatial attention mechanism reveals consistently important subsets of neurons that play a vital role in driving the response of the entire population, providing interpretability and key insights into how the population of neurons performs computation. ",
    "url": "https://arxiv.org/abs/2206.04727",
    "authors": [
      "Trung Le",
      "Eli Shlizerman"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04732",
    "title": "AI-MIA: COVID-19 Detection & Severity Analysis through Medical Imaging",
    "abstract": "This paper presents the baseline approach for the organized 2nd Covid-19 Competition, occurring in the framework of the AIMIA Workshop in the European Conference on Computer Vision (ECCV 2022). It presents the COV19-CT-DB database which is annotated for COVID-19 detction, consisting of about 7,700 3-D CT scans. Part of the database consisting of Covid-19 cases is further annotated in terms of four Covid-19 severity conditions. We have split the database and the latter part of it in training, validation and test datasets. The former two datasets are used for training and validation of machine learning models, while the latter will be used for evaluation of the developed models. The baseline approach consists of a deep learning approach, based on a CNN-RNN network and report its performance on the COVID19-CT-DB database. ",
    "url": "https://arxiv.org/abs/2206.04732",
    "authors": [
      "Dimitrios Kollias",
      "Anastasios Arsenos",
      "Stefanos Kollias"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04815",
    "title": "Connections between graphs and matrix spaces",
    "abstract": "Given a bipartite graph $G$, the graphical matrix space $\\mathcal{S}_G$ consists of matrices whose non-zero entries can only be at those positions corresponding to edges in $G$. Tutte (J. London Math. Soc., 1947), Edmonds (J. Res. Nat. Bur. Standards Sect. B, 1967) and Lov\\'asz (FCT, 1979) observed connections between perfect matchings in $G$ and full-rank matrices in $\\mathcal{S}_G$. Dieudonn\\'e ({Arch. Math., 1948) proved a tight upper bound on the dimensions of those matrix spaces containing only singular matrices. The starting point of this paper is a simultaneous generalization of these two classical results: we show that the largest dimension over subspaces of $\\mathcal{S}_G$ containing only singular matrices is equal to the maximum size over subgraphs of $G$ without perfect matchings, based on Meshulam's proof of Dieudonn\\'e's result (Quart. J. Math., 1985). Starting from this result, we go on to establish more connections between properties of graphs and matrix spaces. For example, we establish connections between acyclicity and nilpotency, between strong connectivity and irreducibility, and between isomorphism and conjugacy/congruence. For each connection, we study three types of correspondences, namely the basic correspondence, the inherited correspondence (for subgraphs and subspaces), and the induced correspondence (for induced subgraphs and restrictions). Some correspondences lead to intriguing generalizations of classical results, such as for Dieudonn\\'e's result mentioned above, and for a celebrated theorem of Gerstenhaber regarding the largest dimension of nil matrix spaces (Amer. J. Math., 1958). Finally, we show some implications of our results to quantum information and present open problems in computational complexity motivated by these results. ",
    "url": "https://arxiv.org/abs/2206.04815",
    "authors": [
      "Yinan Li",
      "Youming Qiao",
      "Avi Wigderson",
      "Yuval Wigderson",
      "Chuanqi Zhang"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Complexity (cs.CC)",
      "Rings and Algebras (math.RA)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2206.04850",
    "title": "Feature-informed Embedding Space Regularization For Audio Classification",
    "abstract": "Feature representations derived from models pre-trained on large-scale datasets have shown their generalizability on a variety of audio analysis tasks. Despite this generalizability, however, task-specific features can outperform if sufficient training data is available, as specific task-relevant properties can be learned. Furthermore, the complex pre-trained models bring considerable computational burdens during inference. We propose to leverage both detailed task-specific features from spectrogram input and generic pre-trained features by introducing two regularization methods that integrate the information of both feature classes. The workload is kept low during inference as the pre-trained features are only necessary for training. In experiments with the pre-trained features VGGish, OpenL3, and a combination of both, we show that the proposed methods not only outperform baseline methods, but also can improve state-of-the-art models on several audio classification tasks. The results also suggest that using the mixture of features performs better than using individual features. ",
    "url": "https://arxiv.org/abs/2206.04850",
    "authors": [
      "Yun-Ning Hung",
      "Alexander Lerch"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2206.04877",
    "title": "Efficient Per-Shot Convex Hull Prediction By Recurrent Learning",
    "abstract": "Adaptive video streaming relies on the construction of efficient bitrate ladders to deliver the best possible visual quality to viewers under bandwidth constraints. The traditional method of content dependent bitrate ladder selection requires a video shot to be pre-encoded with multiple encoding parameters to find the optimal operating points given by the convex hull of the resulting rate-quality curves. However, this pre-encoding step is equivalent to an exhaustive search process over the space of possible encoding parameters, which causes significant overhead in terms of both computation and time expenditure. To reduce this overhead, we propose a deep learning based method of content aware convex hull prediction. We employ a recurrent convolutional network (RCN) to implicitly analyze the spatiotemporal complexity of video shots in order to predict their convex hulls. A two-step transfer learning scheme is adopted to train our proposed RCN-Hull model, which ensures sufficient content diversity to analyze scene complexity, while also making it possible capture the scene statistics of pristine source videos. Our experimental results reveal that our proposed model yields better approximations of the optimal convex hulls, and offers competitive time savings as compared to existing approaches. On average, the pre-encoding time was reduced by 58.0% by our method, while the average Bjontegaard delta bitrate (BD-rate) of the predicted convex hulls against ground truth was 0.08%, while the mean absolute deviation of the BD-rate distribution was 0.44% ",
    "url": "https://arxiv.org/abs/2206.04877",
    "authors": [
      "Somdyuti Paul",
      "Andrey Norkin",
      "Alan C. Bovik"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.05032",
    "title": "Scalable Deep Gaussian Markov Random Fields for General Graphs",
    "abstract": "Machine learning methods on graphs have proven useful in many applications due to their ability to handle generally structured data. The framework of Gaussian Markov Random Fields (GMRFs) provides a principled way to define Gaussian models on graphs by utilizing their sparsity structure. We propose a flexible GMRF model for general graphs built on the multi-layer structure of Deep GMRFs, originally proposed for lattice graphs only. By designing a new type of layer we enable the model to scale to large graphs. The layer is constructed to allow for efficient training using variational inference and existing software frameworks for Graph Neural Networks. For a Gaussian likelihood, close to exact Bayesian inference is available for the latent field. This allows for making predictions with accompanying uncertainty estimates. The usefulness of the proposed model is verified by experiments on a number of synthetic and real world datasets, where it compares favorably to other both Bayesian and deep learning methods. ",
    "url": "https://arxiv.org/abs/2206.05032",
    "authors": [
      "Joel Oskarsson",
      "Per Sid\u00e9n",
      "Fredrik Lindsten"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Computation (stat.CO)"
    ]
  },
  {
    "id": "arXiv:2206.05134",
    "title": "Distributionally Robust End-to-End Portfolio Construction",
    "abstract": "We propose an end-to-end distributionally robust system for portfolio construction that integrates the asset return prediction model with a distributionally robust portfolio optimization model. We also show how to learn the risk-tolerance parameter and the degree of robustness directly from data. End-to-end systems have an advantage in that information can be communicated between the prediction and decision layers during training, allowing the parameters to be trained for the final task rather than solely for predictive performance. However, existing end-to-end systems are not able to quantify and correct for the impact of model risk on the decision layer. Our proposed distributionally robust end-to-end portfolio selection system explicitly accounts for the impact of model risk. The decision layer chooses portfolios by solving a minimax problem where the distribution of the asset returns is assumed to belong to an ambiguity set centered around a nominal distribution. Using convex duality, we recast the minimax problem in a form that allows for efficient training of the end-to-end system. ",
    "url": "https://arxiv.org/abs/2206.05134",
    "authors": [
      "Giorgio Costa",
      "Garud N. Iyengar"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Portfolio Management (q-fin.PM)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.05236",
    "title": "Optical Diffraction Tomography based on 3D Physics-Inspired Neural  Network (PINN)",
    "abstract": "Optical diffraction tomography (ODT) is an emerging 3D imaging technique that is used for the 3D reconstruction of the refractive index (RI) for semi-transparent samples. Various inverse models have been proposed to reconstruct the 3D RI based on the holographic detection of different samples such as the Born and the Rytov approximations. However, such approximations usually suffer from the so-called missing-cone problem that results in an elongation of the final reconstruction along the optical axis. Different iterative schemes have been proposed to solve the missing cone problem relying on physical forward models and an error function that aims at filling in the k-space and thus eliminating the missing-cone problem and reaching better reconstruction accuracy. In this paper, we propose a different approach where a 3D neural network (NN) is employed. The NN is trained with a cost function derived from a physical model based on the physics of optical wave propagation. The 3D NN starts with an initial guess for the 3D RI reconstruction (i.e. Born, or Rytov) and aims at reconstructing better 3D reconstruction based on an error function. With this technique, the NN can be trained without any examples of the relation between the ill-posed reconstruction (Born or Rytov) and the ground truth (true shape). ",
    "url": "https://arxiv.org/abs/2206.05236",
    "authors": [
      "Ahmed B. Ayoub",
      "Amirhossein Saba",
      "Carlo Gigli",
      "Demetri Psaltis"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:1908.04902",
    "title": "Planar graphs without normally adjacent short cycles",
    "abstract": " Comments: 17 pages, 3 figures ",
    "url": "https://arxiv.org/abs/1908.04902",
    "authors": [
      "Fangyao Lu",
      "Mengjiao Rao",
      "Qianqian Wang",
      "Tao Wang"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2005.12458",
    "title": "Trainability of Dissipative Perceptron-Based Quantum Neural Networks",
    "abstract": " Comments: 5 + 21 pages, 3+2 figures, final version accepted for publication in Physical Review Letters ",
    "url": "https://arxiv.org/abs/2005.12458",
    "authors": [
      "Kunal Sharma",
      "M. Cerezo",
      "Lukasz Cincio",
      "Patrick J. Coles"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2006.03535",
    "title": "CoCon: A Self-Supervised Approach for Controlled Text Generation",
    "abstract": " Comments: ICLR 2021 Camera-Ready ",
    "url": "https://arxiv.org/abs/2006.03535",
    "authors": [
      "Alvin Chan",
      "Yew-Soon Ong",
      "Bill Pung",
      "Aston Zhang",
      "Jie Fu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2006.16516",
    "title": "Mixed Logit Models and Network Formation",
    "abstract": " Comments: revision; 33 pages ",
    "url": "https://arxiv.org/abs/2006.16516",
    "authors": [
      "Harsh Gupta",
      "Mason A. Porter"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Theoretical Economics (econ.TH)",
      "Physics and Society (physics.soc-ph)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2010.14982",
    "title": "Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity  Detection",
    "abstract": " Comments: Toyota Smarthome Untrimmed dataset, project page: this https URL ",
    "url": "https://arxiv.org/abs/2010.14982",
    "authors": [
      "Rui Dai",
      "Srijan Das",
      "Saurav Sharma",
      "Luca Minciullo",
      "Lorenzo Garattoni",
      "Francois Bremond",
      "Gianpiero Francesca"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2102.08466",
    "title": "Robust Factorization of Real-world Tensor Streams with Patterns, Missing  Values, and Outliers",
    "abstract": " Comments: Published at ICDE 2021 ",
    "url": "https://arxiv.org/abs/2102.08466",
    "authors": [
      "Dongjin Lee",
      "Kijung Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2104.07060",
    "title": "Membership-Mappings for Data Representation Learning: Measure Theoretic  Conceptualization",
    "abstract": " Title: Membership-Mappings for Data Representation Learning: Measure Theoretic  Conceptualization ",
    "url": "https://arxiv.org/abs/2104.07060",
    "authors": [
      "Mohit Kumar",
      "Bernhard A. Moser",
      "Lukas Fischer",
      "Bernhard Freudenthaler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)"
    ]
  },
  {
    "id": "arXiv:2104.08101",
    "title": "Embedding Dependencies Between Wind Farms in Uncertainty-Aware Optimal  Power Flow",
    "abstract": " Title: Embedding Dependencies Between Wind Farms in Uncertainty-Aware Optimal  Power Flow ",
    "url": "https://arxiv.org/abs/2104.08101",
    "authors": [
      "Adriano Arrigo",
      "Jalal Kazempour",
      "Zacharie De Gr\u00e8ve",
      "Jean-Fran\u00e7ois Toubeau",
      "Fran\u00e7ois Vall\u00e9e"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2104.14811",
    "title": "Why scholars are diagramming neural network models",
    "abstract": " Comments: 16 pages, 4 figures ",
    "url": "https://arxiv.org/abs/2104.14811",
    "authors": [
      "Guy Clarke Marshall",
      "Caroline Jay",
      "Andre Freitas"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2106.11180",
    "title": "Generalization Bounds with Minimal Dependency on Hypothesis Class via  Distributionally Robust Optimization",
    "abstract": " Title: Generalization Bounds with Minimal Dependency on Hypothesis Class via  Distributionally Robust Optimization ",
    "url": "https://arxiv.org/abs/2106.11180",
    "authors": [
      "Yibo Zeng",
      "Henry Lam"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2107.11445",
    "title": "Self-Correcting Neural Networks For Safe Classification",
    "abstract": " Title: Self-Correcting Neural Networks For Safe Classification ",
    "url": "https://arxiv.org/abs/2107.11445",
    "authors": [
      "Klas Leino",
      "Aymeric Fromherz",
      "Ravi Mangal",
      "Matt Fredrikson",
      "Bryan Parno",
      "Corina P\u0103s\u0103reanu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2108.07644",
    "title": "Wireless Federated Langevin Monte Carlo: Repurposing Channel Noise for  Bayesian Sampling and Privacy",
    "abstract": " Comments: submitted ",
    "url": "https://arxiv.org/abs/2108.07644",
    "authors": [
      "Dongzhu Liu",
      "Osvaldo Simeone"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2109.04010",
    "title": "Popularity Adjusted Block Models are Generalized Random Dot Product  Graphs",
    "abstract": " Comments: 36 pages, 9 figures ",
    "url": "https://arxiv.org/abs/2109.04010",
    "authors": [
      "John Koo",
      "Minh Tang",
      "Michael W. Trosset"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2109.12372",
    "title": "Know it to Defeat it: Exploring Health Rumor Characteristics and  Debunking Efforts on Chinese Social Media during COVID-19 Crisis",
    "abstract": " Comments: ICWSM 2022 ",
    "url": "https://arxiv.org/abs/2109.12372",
    "authors": [
      "Wenjie Yang",
      "Sitong Wang",
      "Zhenhui Peng",
      "Chuhan Shi",
      "Xiaojuan Ma",
      "Diyi Yang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2110.11084",
    "title": "Grafting Transformer on Automatically Designed Convolutional Neural  Network for Hyperspectral Image Classification",
    "abstract": " Comments: 15 pages, 10 figures ",
    "url": "https://arxiv.org/abs/2110.11084",
    "authors": [
      "Xizhe Xue",
      "Haokui Zhang",
      "Bei Fang",
      "Zongwen Bai",
      "Ying Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2110.11584",
    "title": "Multiwave COVID-19 Prediction from Social Awareness using Web Search and  Mobility Data",
    "abstract": " Comments: 11 pages, 8 figures. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA ",
    "url": "https://arxiv.org/abs/2110.11584",
    "authors": [
      "J. Xue",
      "T. Yabe",
      "K. Tsubouchi",
      "J. Ma",
      "S. V. Ukkusuri"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2110.14053",
    "title": "NeuroComb: Improving SAT Solving with Graph Neural Networks",
    "abstract": " Title: NeuroComb: Improving SAT Solving with Graph Neural Networks ",
    "url": "https://arxiv.org/abs/2110.14053",
    "authors": [
      "Wenxi Wang",
      "Yang Hu",
      "Mohit Tiwari",
      "Sarfraz Khurshid",
      "Kenneth McMillan",
      "Risto Miikkulainen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2111.02673",
    "title": "Recurrent Neural Network Training with Convex Loss and Regularization  Functions by Extended Kalman Filtering",
    "abstract": " Comments: 22 pages, 4 figures, submitted for publication ",
    "url": "https://arxiv.org/abs/2111.02673",
    "authors": [
      "Alberto Bemporad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2111.13336",
    "title": "MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for  Efficient Object Detection",
    "abstract": " Comments: Accepted by ICML 2022 ",
    "url": "https://arxiv.org/abs/2111.13336",
    "authors": [
      "Zhenhong Sun",
      "Ming Lin",
      "Xiuyu Sun",
      "Zhiyu Tan",
      "Hao Li",
      "Rong Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2111.15309",
    "title": "Deep Auto-encoder with Neural Response",
    "abstract": " Title: Deep Auto-encoder with Neural Response ",
    "url": "https://arxiv.org/abs/2111.15309",
    "authors": [
      "Xuming Ran",
      "Jie Zhang",
      "Ziyuan Ye",
      "Haiyan Wu",
      "Qi Xu",
      "Huihui Zhou",
      "Quanying Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2112.10374",
    "title": "CGIBNet: Bandwidth-constrained Communication with Graph Information  Bottleneck in Multi-Agent Reinforcement Learning",
    "abstract": " Title: CGIBNet: Bandwidth-constrained Communication with Graph Information  Bottleneck in Multi-Agent Reinforcement Learning ",
    "url": "https://arxiv.org/abs/2112.10374",
    "authors": [
      "Qi Tian",
      "Kun Kuang",
      "Baoxiang Wang",
      "Furui Liu",
      "Fei Wu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2112.12033",
    "title": "Encoding protein dynamic information in graph representation for  functional residue identification",
    "abstract": " Title: Encoding protein dynamic information in graph representation for  functional residue identification ",
    "url": "https://arxiv.org/abs/2112.12033",
    "authors": [
      "Yuan Chiang",
      "Wei-Han Hui",
      "Shu-Wei Chang"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Machine Learning (cs.LG)",
      "Atomic and Molecular Clusters (physics.atm-clus)",
      "Biological Physics (physics.bio-ph)"
    ]
  },
  {
    "id": "arXiv:2201.12250",
    "title": "Gradient Descent on Neurons and its Link to Approximate Second-Order  Optimization",
    "abstract": " Comments: ICML 2022, camera ready ",
    "url": "https://arxiv.org/abs/2201.12250",
    "authors": [
      "Frederik Benzing"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.02363",
    "title": "Meta-Reinforcement Learning with Self-Modifying Networks",
    "abstract": " Title: Meta-Reinforcement Learning with Self-Modifying Networks ",
    "url": "https://arxiv.org/abs/2202.02363",
    "authors": [
      "Mathieu Chalvidal",
      "Thomas Serre",
      "Rufin VanRullen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2202.09096",
    "title": "A Free Lunch with Influence Functions? Improving Neural Network  Estimates with Concepts from Semiparametric Statistics",
    "abstract": " Title: A Free Lunch with Influence Functions? Improving Neural Network  Estimates with Concepts from Semiparametric Statistics ",
    "url": "https://arxiv.org/abs/2202.09096",
    "authors": [
      "Matthew J. Vowels",
      "Sina Akbari",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2202.14036",
    "title": "How Well Do My Results Generalize Now? The External Validity of Online  Privacy and Security Surveys",
    "abstract": " Title: How Well Do My Results Generalize Now? The External Validity of Online  Privacy and Security Surveys ",
    "url": "https://arxiv.org/abs/2202.14036",
    "authors": [
      "Jenny Tang",
      "Eleanor Birrell",
      "Ada Lerner"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2203.00026",
    "title": "Predicting the Thermal Sunyaev-Zel'dovich Field using Modular and  Equivariant Set-Based Neural Networks",
    "abstract": " Comments: 11 pages, 5 figures; condensed version accepted at the Fourth Workshop on Machine Learning and the Physical Sciences (NeurIPS 2021) as \"Equivariant and Modular DeepSets with Applications in Cluster Cosmology\"; revised version accepted by MLST includes numerous clarifications and a new appendix comparing to CNN ",
    "url": "https://arxiv.org/abs/2203.00026",
    "authors": [
      "Leander Thiele",
      "Miles Cranmer",
      "William Coulton",
      "Shirley Ho",
      "David N. Spergel"
    ],
    "subjectives": [
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2203.16527",
    "title": "Exploring Plain Vision Transformer Backbones for Object Detection",
    "abstract": " Comments: Tech report. arXiv v2: add RetinaNet results ",
    "url": "https://arxiv.org/abs/2203.16527",
    "authors": [
      "Yanghao Li",
      "Hanzi Mao",
      "Ross Girshick",
      "Kaiming He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2204.00943",
    "title": "Efficient Convolutional Neural Networks on Raspberry Pi for Image  Classification",
    "abstract": " Comments: 12 pages, 2 figures ",
    "url": "https://arxiv.org/abs/2204.00943",
    "authors": [
      "Rui-Yang Ju",
      "Ting-Yu Lin",
      "Jia-Hao Jian",
      "Jen-Shiun Chiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2204.04452",
    "title": "Refined Convergence and Topology Learning for Decentralized Optimization  with Heterogeneous Data",
    "abstract": " Title: Refined Convergence and Topology Learning for Decentralized Optimization  with Heterogeneous Data ",
    "url": "https://arxiv.org/abs/2204.04452",
    "authors": [
      "B. Le Bars",
      "A. Bellet",
      "M. Tommasi",
      "E. Lavoie",
      "AM. Kermarrec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2205.05476",
    "title": "Contrastive Supervised Distillation for Continual Representation  Learning",
    "abstract": " Comments: Paper published as Oral and awarded as Best Student Paper at ICIAP21 ",
    "url": "https://arxiv.org/abs/2205.05476",
    "authors": [
      "Tommaso Barletti",
      "Niccolo' Biondi",
      "Federico Pernici",
      "Matteo Bruni",
      "Alberto Del Bimbo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2205.11508",
    "title": "Contrastive and Non-Contrastive Self-Supervised Learning Recover Global  and Local Spectral Embedding Methods",
    "abstract": " Title: Contrastive and Non-Contrastive Self-Supervised Learning Recover Global  and Local Spectral Embedding Methods ",
    "url": "https://arxiv.org/abs/2205.11508",
    "authors": [
      "Randall Balestriero",
      "Yann LeCun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Spectral Theory (math.SP)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2205.11664",
    "title": "Towards Model Generalization for Monocular 3D Object Detection",
    "abstract": " Comments: Some mistakes are raised up and we need to re-write the paper and re-order the paper structure ",
    "url": "https://arxiv.org/abs/2205.11664",
    "authors": [
      "Zhenyu Li",
      "Zehui Chen",
      "Ang Li",
      "Liangji Fang",
      "Qinhong Jiang",
      "Xianming Liu",
      "Junjun Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2205.13220",
    "title": "DGSVis: Visual Analysis of Hierarchical Snapshots in Dynamic Graph",
    "abstract": " Comments: 11 pages, 9 figures ",
    "url": "https://arxiv.org/abs/2205.13220",
    "authors": [
      "Baofeng Chang",
      "Sujia Zhu",
      "Qi Jiang",
      "Wang Xia",
      "Jingwei Tang",
      "Lvhan Pan",
      "Ronghua Liang",
      "Guodao Sun"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2205.13635",
    "title": "RIGID: Robust Linear Regression with Missing Data",
    "abstract": " Title: RIGID: Robust Linear Regression with Missing Data ",
    "url": "https://arxiv.org/abs/2205.13635",
    "authors": [
      "Alireza Aghasi",
      "MohammadJavad Feizollahi",
      "Saeed Ghadimi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2205.15952",
    "title": "Knowledge Graph - Deep Learning: A Case Study in Question Answering in  Aviation Safety Domain",
    "abstract": " Comments: LREC 2022 Main Conference Accepted Paper ",
    "url": "https://arxiv.org/abs/2205.15952",
    "authors": [
      "Ankush Agarwal",
      "Raj Gite",
      "Shreya Laddha",
      "Pushpak Bhattacharyya",
      "Satyanarayan Kar",
      "Asif Ekbal",
      "Prabhjit Thind",
      "Rajesh Zele",
      "Ravi Shankar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.00858",
    "title": "Bayesian Inference of Stochastic Dynamical Networks",
    "abstract": " Comments: 12 pages, 2 figures, and 7 tables ",
    "url": "https://arxiv.org/abs/2206.00858",
    "authors": [
      "Yasen Wang",
      "Junyang Jin",
      "Jorge Goncalves"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01288",
    "title": "Decentralized Training of Foundation Models in Heterogeneous  Environments",
    "abstract": " Title: Decentralized Training of Foundation Models in Heterogeneous  Environments ",
    "url": "https://arxiv.org/abs/2206.01288",
    "authors": [
      "Binhang Yuan",
      "Yongjun He",
      "Jared Quincy Davis",
      "Tianyi Zhang",
      "Tri Dao",
      "Beidi Chen",
      "Percy Liang",
      "Christopher Re",
      "Ce Zhang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01299",
    "title": "Fine-tuning Language Models over Slow Networks using Activation  Compression with Guarantees",
    "abstract": " Title: Fine-tuning Language Models over Slow Networks using Activation  Compression with Guarantees ",
    "url": "https://arxiv.org/abs/2206.01299",
    "authors": [
      "Jue Wang",
      "Binhang Yuan",
      "Luka Rimanic",
      "Yongjun He",
      "Tri Dao",
      "Beidi Chen",
      "Christopher Re",
      "Ce Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2206.02066",
    "title": "PIDNet: A Real-time Semantic Segmentation Network Inspired from PID  Controller",
    "abstract": " Comments: 11 pages, 10 figures ",
    "url": "https://arxiv.org/abs/2206.02066",
    "authors": [
      "Jiacong Xu",
      "Zixiang Xiong",
      "Shankar P. Bhattacharyya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.02144",
    "title": "Product safety idioms: a method for building causal Bayesian networks  for product safety and risk assessment",
    "abstract": " Title: Product safety idioms: a method for building causal Bayesian networks  for product safety and risk assessment ",
    "url": "https://arxiv.org/abs/2206.02144",
    "authors": [
      "Joshua Hunte",
      "Martin Neil",
      "Norman Fenton"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.02290",
    "title": "A knowledge graph representation learning approach to predict novel  kinase-substrate interactions",
    "abstract": " Title: A knowledge graph representation learning approach to predict novel  kinase-substrate interactions ",
    "url": "https://arxiv.org/abs/2206.02290",
    "authors": [
      "Sachin Gavali",
      "Karen Ross",
      "Chuming Chen",
      "Julie Cowart",
      "Cathy H. Wu"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Molecular Networks (q-bio.MN)"
    ]
  },
  {
    "id": "arXiv:2206.02435",
    "title": "Tackling covariate shift with node-based Bayesian neural networks",
    "abstract": " Comments: Published at ICML 2022 (long oral presentation). Code is available at this https URL ",
    "url": "https://arxiv.org/abs/2206.02435",
    "authors": [
      "Trung Trinh",
      "Markus Heinonen",
      "Luigi Acerbi",
      "Samuel Kaski"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.03426",
    "title": "Improving Fairness in Graph Neural Networks via Mitigating Sensitive  Attribute Leakage",
    "abstract": " Title: Improving Fairness in Graph Neural Networks via Mitigating Sensitive  Attribute Leakage ",
    "url": "https://arxiv.org/abs/2206.03426",
    "authors": [
      "Yu Wang",
      "Yuying Zhao",
      "Yushun Dong",
      "Huiyuan Chen",
      "Jundong Li",
      "Tyler Derr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2206.03644",
    "title": "Neural Bandit with Arm Group Graph",
    "abstract": " Comments: Accepted to SIGKDD 2022 ",
    "url": "https://arxiv.org/abs/2206.03644",
    "authors": [
      "Yunzhe Qi",
      "Yikun Ban",
      "Jingrui He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.03970",
    "title": "Narrowing the Coordinate-frame Gap in Behavior Prediction Models:  Distillation for Efficient and Accurate Scene-centric Motion Forecasting",
    "abstract": " Comments: Accepted at ICRA 2022 ",
    "url": "https://arxiv.org/abs/2206.03970",
    "authors": [
      "DiJia Su",
      "Bertrand Douillard",
      "Rami Al-Rfou",
      "Cheolho Park",
      "Benjamin Sapp"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2206.04460",
    "title": "Open ERP System Data For Occupational Fraud Detection",
    "abstract": " Title: Open ERP System Data For Occupational Fraud Detection ",
    "url": "https://arxiv.org/abs/2206.04460",
    "authors": [
      "Julian Tritscher",
      "Fabian Gwinner",
      "Daniel Schl\u00f6r",
      "Anna Krause",
      "Andreas Hotho"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  }
]