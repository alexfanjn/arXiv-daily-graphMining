[
  {
    "id": "arXiv:2206.04124",
    "title": "DRHDR: A Dual branch Residual Network for Multi-Bracket High Dynamic  Range Imaging",
    "abstract": "We introduce DRHDR, a Dual branch Residual Convolutional Neural Network for Multi-Bracket HDR Imaging. To address the challenges of fusing multiple brackets from dynamic scenes, we propose an efficient dual branch network that operates on two different resolutions. The full resolution branch uses a Deformable Convolutional Block to align features and retain high-frequency details. A low resolution branch with a Spatial Attention Block aims to attend wanted areas from the non-reference brackets, and suppress displaced features that could incur on ghosting artifacts. By using a dual branch approach we are able to achieve high quality results while constraining the computational resources required to estimate the HDR results. ",
    "url": "https://arxiv.org/abs/2206.04124",
    "authors": [
      "Juan Mar\u00edn-Vega",
      "Michael Sloth",
      "Peter Schneider-Kamp",
      "Richard R\u00f6ttger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2206.04125",
    "title": "Towards Self-supervised and Weight-preserving Neural Architecture Search",
    "abstract": "Neural architecture search (NAS) algorithms save tremendous labor from human experts. Recent advancements further reduce the computational overhead to an affordable level. However, it is still cumbersome to deploy the NAS techniques in real-world applications due to the fussy procedures and the supervised learning paradigm. In this work, we propose the self-supervised and weight-preserving neural architecture search (SSWP-NAS) as an extension of the current NAS framework by allowing the self-supervision and retaining the concomitant weights discovered during the search stage. As such, we simplify the workflow of NAS to a one-stage and proxy-free procedure. Experiments show that the architectures searched by the proposed framework achieve state-of-the-art accuracy on CIFAR-10, CIFAR-100, and ImageNet datasets without using manual labels. Moreover, we show that employing the concomitant weights as initialization consistently outperforms the random initialization and the two-stage weight pre-training method by a clear margin under semi-supervised learning scenarios. Codes are publicly available at https://github.com/LzVv123456/SSWP-NAS. ",
    "url": "https://arxiv.org/abs/2206.04125",
    "authors": [
      "Zhuowei Li",
      "Yibo Gao",
      "Zhenzhou Zha",
      "Zhiqiang HU",
      "Qing Xia",
      "Shaoting Zhang",
      "Dimitris N. Metaxas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04137",
    "title": "Adversarial Text Normalization",
    "abstract": "Text-based adversarial attacks are becoming more commonplace and accessible to general internet users. As these attacks proliferate, the need to address the gap in model robustness becomes imminent. While retraining on adversarial data may increase performance, there remains an additional class of character-level attacks on which these models falter. Additionally, the process to retrain a model is time and resource intensive, creating a need for a lightweight, reusable defense. In this work, we propose the Adversarial Text Normalizer, a novel method that restores baseline performance on attacked content with low computational overhead. We evaluate the efficacy of the normalizer on two problem areas prone to adversarial attacks, i.e. Hate Speech and Natural Language Inference. We find that text normalization provides a task-agnostic defense against character-level attacks that can be implemented supplementary to adversarial retraining solutions, which are more suited for semantic alterations. ",
    "url": "https://arxiv.org/abs/2206.04137",
    "authors": [
      "Joanna Bitton",
      "Maya Pavlova",
      "Ivan Evtimov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2206.04149",
    "title": "A Comprehensive Survey of Graph-based Deep Learning Approaches for  Anomaly Detection in Complex Distributed Systems",
    "abstract": "Anomaly detection is an important problem for complex distributed systems consisting of hardware and software components. A thorough understanding of the requirements and challenges of anomaly detection for such systems is pivotal to the security of a system, especially for real-world deployment. While there have been many diverse research areas and application domains that deal with the problem, few have attempted to provide an in-depth look at such systems. Most anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. In this survey, we explore the significant potential of graph-based algorithms to identify and mitigate different types of anomalies in complex distributed heterogeneous systems. Our main focus is to provide an in-depth look at graphs when applied on heterogeneous computing devices spread across complex distributed systems. This study analyzes, compares, and contrasts the state-of-the-art research articles in the field. First, we describe the characteristics of the real-world distributed systems and their specific challenges of anomaly detection in such complex networks, such as data and evaluation, nature of the anomalies, and real-world requirements. Later, we discuss why graphs can be leveraged in such systems and the benefits of utilizing graphs. Then we will aptly delve into the state-of-the-art approaches and highlight their strength and weaknesses. Finally, we evaluate and compare these approaches and point out the areas for possible improvements. ",
    "url": "https://arxiv.org/abs/2206.04149",
    "authors": [
      "Armin Danesh Pazho",
      "Ghazal Alinezhad Noghre",
      "Arnab A Purkayastha",
      "Jagannadh Vempati",
      "Otto Martin",
      "Hamed Tabkhi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04153",
    "title": "Unsupervised Key Event Detection from Massive Text Corpora",
    "abstract": "Automated event detection from news corpora is a crucial task towards mining fast-evolving structured knowledge. As real-world events have different granularities, from the top-level themes to key events and then to event mentions corresponding to concrete actions, there are generally two lines of research: (1) theme detection identifies from a news corpus major themes (e.g., \"2019 Hong Kong Protests\" vs. \"2020 U.S. Presidential Election\") that have very distinct semantics; and (2) action extraction extracts from one document mention-level actions (e.g., \"the police hit the left arm of the protester\") that are too fine-grained for comprehending the event. In this paper, we propose a new task, key event detection at the intermediate level, aiming to detect from a news corpus key events (e.g., \"HK Airport Protest on Aug. 12-14\"), each happening at a particular time/location and focusing on the same topic. This task can bridge event understanding and structuring and is inherently challenging because of the thematic and temporal closeness of key events and the scarcity of labeled data due to the fast-evolving nature of news articles. To address these challenges, we develop an unsupervised key event detection framework, EvMine, that (1) extracts temporally frequent peak phrases using a novel ttf-itf score, (2) merges peak phrases into event-indicative feature sets by detecting communities from our designed peak phrase graph that captures document co-occurrences, semantic similarities, and temporal closeness signals, and (3) iteratively retrieves documents related to each key event by training a classifier with automatically generated pseudo labels from the event-indicative feature sets and refining the detected key events using the retrieved documents. Extensive experiments and case studies show EvMine outperforms all the baseline methods and its ablations on two real-world news corpora. ",
    "url": "https://arxiv.org/abs/2206.04153",
    "authors": [
      "Yunyi Zhang",
      "Fang Guo",
      "Jiaming Shen",
      "Jiawei Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2206.04162",
    "title": "Improved two-stage hate speech classification for twitter based on Deep  Neural Networks",
    "abstract": "Hate speech is a form of online harassment that involves the use of abusive language, and it is commonly seen in social media posts. This sort of harassment mainly focuses on specific group characteristics such as religion, gender, ethnicity, etc and it has both societal and economic consequences nowadays. The automatic detection of abusive language in text postings has always been a difficult task, but it is lately receiving much interest from the scientific community. This paper addresses the important problem of discerning hateful content in social media. The model we propose in this work is an extension of an existing approach based on LSTM neural network architectures, which we appropriately enhanced and fine-tuned to detect certain forms of hatred language, such as racism or sexism, in a short text. The most significant enhancement is the conversion to a two-stage scheme consisting of Recurrent Neural Network (RNN) classifiers. The output of all One-vs-Rest (OvR) classifiers from the first stage are combined and used to train the second stage classifier, which finally determines the type of harassment. Our study includes a performance comparison of several proposed alternative methods for the second stage evaluated on a public corpus of 16k tweets, followed by a generalization study on another dataset. The reported results show the superior classification quality of the proposed scheme in the task of hate speech detection as compared to the current state-of-the-art. ",
    "url": "https://arxiv.org/abs/2206.04162",
    "authors": [
      "Georgios K. Pitsilis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2206.04192",
    "title": "ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion",
    "abstract": "Knowledge graphs are inherently incomplete. Therefore substantial research has been directed towards knowledge graph completion (KGC), i.e., predicting missing triples from the information represented in the knowledge graph (KG). Embedding models have yielded promising results for KGC, yet any current KGC embedding model is incapable of: (1) fully capturing vital inference patterns (e.g., composition), (2) capturing prominent logical rules jointly (e.g., hierarchy and composition), and (3) providing an intuitive interpretation of captured patterns. In this work, we propose ExpressivE, a fully expressive spatio-functional embedding model that solves all these challenges simultaneously. ExpressivE embeds pairs of entities as points and relations as hyper-parallelograms in the virtual triple space $\\mathbb{R}^{2d}$. This model design allows ExpressivE not only to capture a rich set of inference patterns jointly but additionally to display any supported inference pattern through the spatial relation of hyper-parallelograms, offering an intuitive and consistent geometric interpretation of ExpressivE embeddings and their captured patterns. Experimental results on standard KGC benchmarks reveal that ExpressivE is competitive with state-of-the-art models and even significantly outperforms them on WN18RR. ",
    "url": "https://arxiv.org/abs/2206.04192",
    "authors": [
      "Aleksandar Pavlovi\u0107",
      "Emanuel Sallinger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04215",
    "title": "It's a super deal -- train recurrent network on noisy data and get  smooth prediction free",
    "abstract": "Recent research demonstrate that prediction of time series by predictive recurrent neural networks based on the noisy input generates a {\\it smooth} anticipated trajectory. We examine influence of the noise component in both the training data sets and the input sequences on network prediction quality. We propose and discuss an explanation of the observed noise compression in the predictive process. We also discuss importance of this property of recurrent networks in the neuroscience context for the evolution of living organisms. ",
    "url": "https://arxiv.org/abs/2206.04215",
    "authors": [
      "Boris Rubinstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2206.04216",
    "title": "Neo-GNNs: Neighborhood Overlap-aware Graph Neural Networks for Link  Prediction",
    "abstract": "Graph Neural Networks (GNNs) have been widely applied to various fields for learning over graph-structured data. They have shown significant improvements over traditional heuristic methods in various tasks such as node classification and graph classification. However, since GNNs heavily rely on smoothed node features rather than graph structure, they often show poor performance than simple heuristic methods in link prediction where the structural information, e.g., overlapped neighborhoods, degrees, and shortest paths, is crucial. To address this limitation, we propose Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs) that learn useful structural features from an adjacency matrix and estimate overlapped neighborhoods for link prediction. Our Neo-GNNs generalize neighborhood overlap-based heuristic methods and handle overlapped multi-hop neighborhoods. Our extensive experiments on Open Graph Benchmark datasets (OGB) demonstrate that Neo-GNNs consistently achieve state-of-the-art performance in link prediction. Our code is publicly available at https://github.com/seongjunyun/Neo_GNNs. ",
    "url": "https://arxiv.org/abs/2206.04216",
    "authors": [
      "Seongjun Yun",
      "Seoyoon Kim",
      "Junhyun Lee",
      "Jaewoo Kang",
      "Hyunwoo J. Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04236",
    "title": "Analytical Composition of Differential Privacy via the Edgeworth  Accountant",
    "abstract": "Many modern machine learning algorithms are composed of simple private algorithms; thus, an increasingly important problem is to efficiently compute the overall privacy loss under composition. In this study, we introduce the Edgeworth Accountant, an analytical approach to composing differential privacy guarantees of private algorithms. The Edgeworth Accountant starts by losslessly tracking the privacy loss under composition using the $f$-differential privacy framework, which allows us to express the privacy guarantees using privacy-loss log-likelihood ratios (PLLRs). As the name suggests, this accountant next uses the Edgeworth expansion to the upper and lower bounds the probability distribution of the sum of the PLLRs. Moreover, by relying on a technique for approximating complex distributions using simple ones, we demonstrate that the Edgeworth Accountant can be applied to the composition of any noise-addition mechanism. Owing to certain appealing features of the Edgeworth expansion, the $(\\epsilon, \\delta)$-differential privacy bounds offered by this accountant are non-asymptotic, with essentially no extra computational cost, as opposed to the prior approaches in, wherein the running times increase with the number of compositions. Finally, we demonstrate that our upper and lower $(\\epsilon, \\delta)$-differential privacy bounds are tight in federated analytics and certain regimes of training private deep learning models. ",
    "url": "https://arxiv.org/abs/2206.04236",
    "authors": [
      "Hua Wang",
      "Sheng Gao",
      "Huanyu Zhang",
      "Milan Shen",
      "Weijie J. Su"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.04239",
    "title": "Crosslinguistic word order variation reflects evolutionary pressures of  dependency and information locality",
    "abstract": "Languages vary considerably in syntactic structure. About 40% of the world's languages have subject-verb-object order, and about 40% have subject-object-verb order. Extensive work has sought to explain this word order variation across languages. However, the existing approaches are not able to explain coherently the frequency distribution and evolution of word order in individual languages. We propose that variation in word order reflects different ways of balancing competing pressures of dependency locality and information locality, whereby languages favor placing elements together when they are syntactically related or contextually informative about each other. Using data from 80 languages in 17 language families and phylogenetic modeling, we demonstrate that languages evolve to balance these pressures, such that word order change is accompanied by change in the frequency distribution of the syntactic structures which speakers communicate to maintain overall efficiency. Variability in word order thus reflects different ways in which languages resolve these evolutionary pressures. We identify relevant characteristics that result from this joint optimization, particularly the frequency with which subjects and objects are expressed together for the same verb. Our findings suggest that syntactic structure and usage across languages co-adapt to support efficient communication under limited cognitive resources. ",
    "url": "https://arxiv.org/abs/2206.04239",
    "authors": [
      "Michael Hahn",
      "Yang Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2206.04242",
    "title": "OOD Augmentation May Be at Odds with Open-Set Recognition",
    "abstract": "Despite advances in image classification methods, detecting the samples not belonging to the training classes is still a challenging problem. There has been a burst of interest in this subject recently, which is called Open-Set Recognition (OSR). In OSR, the goal is to achieve both the classification and detecting out-of-distribution (OOD) samples. Several ideas have been proposed to push the empirical result further through complicated techniques. We believe that such complication is indeed not necessary. To this end, we have shown that Maximum Softmax Probability (MSP), as the simplest baseline for OSR, applied on Vision Transformers (ViTs) as the base classifier that is trained with non-OOD augmentations can surprisingly outperform many recent methods. Non-OOD augmentations are the ones that do not alter the data distribution by much. Our results outperform state-of-the-art in CIFAR-10 datasets, and is also better than most of the current methods in SVHN and MNIST. We show that training augmentation has a significant effect on the performance of ViTs in the OSR tasks, and while they should produce significant diversity in the augmented samples, the generated sample OOD-ness must remain limited. ",
    "url": "https://arxiv.org/abs/2206.04242",
    "authors": [
      "Mohammad Azizmalayeri",
      "Mohammad Hossein Rohban"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04255",
    "title": "ScatterSample: Diversified Label Sampling for Data Efficient Graph  Neural Network Learning",
    "abstract": "What target labels are most effective for graph neural network (GNN) training? In some applications where GNNs excel-like drug design or fraud detection, labeling new instances is expensive. We develop a data-efficient active sampling framework, ScatterSample, to train GNNs under an active learning setting. ScatterSample employs a sampling module termed DiverseUncertainty to collect instances with large uncertainty from different regions of the sample space for labeling. To ensure diversification of the selected nodes, DiverseUncertainty clusters the high uncertainty nodes and selects the representative nodes from each cluster. Our ScatterSample algorithm is further supported by rigorous theoretical analysis demonstrating its advantage compared to standard active sampling methods that aim to simply maximize the uncertainty and not diversify the samples. In particular, we show that ScatterSample is able to efficiently reduce the model uncertainty over the whole sample space. Our experiments on five datasets show that ScatterSample significantly outperforms the other GNN active learning baselines, specifically it reduces the sampling cost by up to 50% while achieving the same test accuracy. ",
    "url": "https://arxiv.org/abs/2206.04255",
    "authors": [
      "Zhenwei Dai",
      "Vasileios Ioannidis",
      "Soji Adeshina",
      "Zak Jost",
      "Christos Faloutsos",
      "George Karypis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04281",
    "title": "Local Spatiotemporal Representation Learning for  Longitudinally-consistent Neuroimage Analysis",
    "abstract": "Recent self-supervised advances in medical computer vision exploit global and local anatomical self-similarity for pretraining prior to downstream tasks such as segmentation. However, current methods assume i.i.d. image acquisition, which is invalid in clinical study designs where follow-up longitudinal scans track subject-specific temporal changes. Further, existing self-supervised methods for medically-relevant image-to-image architectures exploit only spatial or temporal self-similarity and only do so via a loss applied at a single image-scale, with naive multi-scale spatiotemporal extensions collapsing to degenerate solutions. To these ends, this paper makes two contributions: (1) It presents a local and multi-scale spatiotemporal representation learning method for image-to-image architectures trained on longitudinal images. It exploits the spatiotemporal self-similarity of learned multi-scale intra-subject features for pretraining and develops several feature-wise regularizations that avoid collapsed identity representations; (2) During finetuning, it proposes a surprisingly simple self-supervised segmentation consistency regularization to exploit intra-subject correlation. Benchmarked in the one-shot segmentation setting, the proposed framework outperforms both well-tuned randomly-initialized baselines and current self-supervised techniques designed for both i.i.d. and longitudinal datasets. These improvements are demonstrated across both longitudinal neurodegenerative adult MRI and developing infant brain MRI and yield both higher performance and longitudinal consistency. ",
    "url": "https://arxiv.org/abs/2206.04281",
    "authors": [
      "Mengwei Ren",
      "Neel Dey",
      "Martin A. Styner",
      "Kelly Botteron",
      "Guido Gerig"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04285",
    "title": "Pseudo-Poincar\u00e9: A Unification Framework for Euclidean and Hyperbolic  Graph Neural Networks",
    "abstract": "Hyperbolic neural networks have recently gained significant attention due to their promising results on several graph problems including node classification and link prediction. The primary reason for this success is the effectiveness of the hyperbolic space in capturing the inherent hierarchy of graph datasets. However, they are limited in terms of generalization, scalability, and have inferior performance when it comes to non-hierarchical datasets. In this paper, we take a completely orthogonal perspective for modeling hyperbolic networks. We use Poincar\\'e disk to model the hyperbolic geometry and also treat it as if the disk itself is a tangent space at origin. This enables us to replace non-scalable M\\\"obius gyrovector operations with an Euclidean approximation, and thus simplifying the entire hyperbolic model to a Euclidean model cascaded with a hyperbolic normalization function. Our approach does not adhere to M\\\"obius math, yet it still works in the Riemannian manifold, hence we call it Pseudo-Poincar\\'e framework. We applied our non-linear hyperbolic normalization to the current state-of-the-art homogeneous and multi-relational graph networks and demonstrate significant improvements in performance compared to both Euclidean and hyperbolic counterparts. The primary impact of this work lies in its ability to capture hierarchical features in the Euclidean space, and thus, can replace hyperbolic networks without loss in performance metrics while simultaneously leveraging the power of Euclidean networks such as interpretability and efficient execution of various model components. ",
    "url": "https://arxiv.org/abs/2206.04285",
    "authors": [
      "Mehrdad Khatir",
      "Nurendra Choudhary",
      "Sutanay Choudhury",
      "Khushbu Agarwal",
      "Chandan K. Reddy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2206.04310",
    "title": "GSmooth: Certified Robustness against Semantic Transformations via  Generalized Randomized Smoothing",
    "abstract": "Certified defenses such as randomized smoothing have shown promise towards building reliable machine learning systems against $\\ell_p$-norm bounded attacks. However, existing methods are insufficient or unable to provably defend against semantic transformations, especially those without closed-form expressions (such as defocus blur and pixelate), which are more common in practice and often unrestricted. To fill up this gap, we propose generalized randomized smoothing (GSmooth), a unified theoretical framework for certifying robustness against general semantic transformations via a novel dimension augmentation strategy. Under the GSmooth framework, we present a scalable algorithm that uses a surrogate image-to-image network to approximate the complex transformation. The surrogate model provides a powerful tool for studying the properties of semantic transformations and certifying robustness. Experimental results on several datasets demonstrate the effectiveness of our approach for robustness certification against multiple kinds of semantic transformations and corruptions, which is not achievable by the alternative baselines. ",
    "url": "https://arxiv.org/abs/2206.04310",
    "authors": [
      "Zhongkai Hao",
      "Chengyang Ying",
      "Yinpeng Dong",
      "Hang Su",
      "Jun Zhu",
      "Jian Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04316",
    "title": "Adversarial Noises Are Linearly Separable for (Nearly) Random Neural  Networks",
    "abstract": "Adversarial examples, which are usually generated for specific inputs with a specific model, are ubiquitous for neural networks. In this paper we unveil a surprising property of adversarial noises when they are put together, i.e., adversarial noises crafted by one-step gradient methods are linearly separable if equipped with the corresponding labels. We theoretically prove this property for a two-layer network with randomly initialized entries and the neural tangent kernel setup where the parameters are not far from initialization. The proof idea is to show the label information can be efficiently backpropagated to the input while keeping the linear separability. Our theory and experimental evidence further show that the linear classifier trained with the adversarial noises of the training data can well classify the adversarial noises of the test data, indicating that adversarial noises actually inject a distributional perturbation to the original data distribution. Furthermore, we empirically demonstrate that the adversarial noises may become less linearly separable when the above conditions are compromised while they are still much easier to classify than original features. ",
    "url": "https://arxiv.org/abs/2206.04316",
    "authors": [
      "Huishuai Zhang",
      "Da Yu",
      "Yiping Lu",
      "Di He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.04318",
    "title": "Blind Surveillance Image Quality Assessment via Deep Neural Network  Combined with the Visual Saliency",
    "abstract": "The intelligent video surveillance system (IVSS) can automatically analyze the content of the surveillance image (SI) and reduce the burden of the manual labour. However, the SIs may suffer quality degradations in the procedure of acquisition, compression, and transmission, which makes IVSS hard to understand the content of SIs. In this paper, we first conduct an example experiment (i.e. the face detection task) to demonstrate that the quality of the SIs has a crucial impact on the performance of the IVSS, and then propose a saliency-based deep neural network for the blind quality assessment of the SIs, which helps IVSS to filter the low-quality SIs and improve the detection and recognition performance. Specifically, we first compute the saliency map of the SI to select the most salient local region since the salient regions usually contain rich semantic information for machine vision and thus have a great impact on the overall quality of the SIs. Next, the convolutional neural network (CNN) is adopted to extract quality-aware features for the whole image and local region, which are then mapped into the global and local quality scores through the fully connected (FC) network respectively. Finally, the overall quality score is computed as the weighted sum of the global and local quality scores. Experimental results on the SI quality database (SIQD) show that the proposed method outperforms all compared state-of-the-art BIQA methods. ",
    "url": "https://arxiv.org/abs/2206.04318",
    "authors": [
      "Wei Lu",
      "Wei Sun",
      "Wenhan Zhu",
      "Xiongkuo Min",
      "Zicheng Zhang",
      "Tao Wang",
      "Guangtao Zhai"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04330",
    "title": "Predicting Embedding Reliability in Low-Resource Settings Using Corpus  Similarity Measures",
    "abstract": "This paper simulates a low-resource setting across 17 languages in order to evaluate embedding similarity, stability, and reliability under different conditions. The goal is to use corpus similarity measures before training to predict properties of embeddings after training. The main contribution of the paper is to show that it is possible to predict downstream embedding similarity using upstream corpus similarity measures. This finding is then applied to low-resource settings by modelling the reliability of embeddings created from very limited training data. Results show that it is possible to estimate the reliability of low-resource embeddings using corpus similarity measures that remain robust on small amounts of data. These findings have significant implications for the evaluation of truly low-resource languages in which such systematic downstream validation methods are not possible because of data limitations. ",
    "url": "https://arxiv.org/abs/2206.04330",
    "authors": [
      "Jonathan Dunn",
      "Haipeng Li",
      "Damian Sastre"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2206.04332",
    "title": "Corpus Similarity Measures Remain Robust Across Diverse Languages",
    "abstract": "This paper experiments with frequency-based corpus similarity measures across 39 languages using a register prediction task. The goal is to quantify (i) the distance between different corpora from the same language and (ii) the homogeneity of individual corpora. Both of these goals are essential for measuring how well corpus-based linguistic analysis generalizes from one dataset to another. The problem is that previous work has focused on Indo-European languages, raising the question of whether these measures are able to provide robust generalizations across diverse languages. This paper uses a register prediction task to evaluate competing measures across 39 languages: how well are they able to distinguish between corpora representing different contexts of production? Each experiment compares three corpora from a single language, with the same three digital registers shared across all languages: social media, web pages, and Wikipedia. Results show that measures of corpus similarity retain their validity across different language families, writing systems, and types of morphology. Further, the measures remain robust when evaluated on out-of-domain corpora, when applied to low-resource languages, and when applied to different sets of registers. These findings are significant given our need to make generalizations across the rapidly increasing number of corpora available for analysis. ",
    "url": "https://arxiv.org/abs/2206.04332",
    "authors": [
      "Haipeng Li",
      "Jonathan Dunn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2206.04355",
    "title": "Graph Attention Multi-Layer Perceptron",
    "abstract": "Graph neural networks (GNNs) have achieved great success in many graph-based applications. However, the enormous size and high sparsity level of graphs hinder their applications under industrial scenarios. Although some scalable GNNs are proposed for large-scale graphs, they adopt a fixed $K$-hop neighborhood for each node, thus facing the over-smoothing issue when adopting large propagation depths for nodes within sparse regions. To tackle the above issue, we propose a new GNN architecture -- Graph Attention Multi-Layer Perceptron (GAMLP), which can capture the underlying correlations between different scales of graph knowledge. We have deployed GAMLP in Tencent with the Angel platform, and we further evaluate GAMLP on both real-world datasets and large-scale industrial datasets. Extensive experiments on these 14 graph datasets demonstrate that GAMLP achieves state-of-the-art performance while enjoying high scalability and efficiency. Specifically, it outperforms GAT by 1.3\\% regarding predictive accuracy on our large-scale Tencent Video dataset while achieving up to $50\\times$ training speedup. Besides, it ranks top-1 on both the leaderboards of the largest homogeneous and heterogeneous graph (i.e., ogbn-papers100M and ogbn-mag) of Open Graph Benchmark. ",
    "url": "https://arxiv.org/abs/2206.04355",
    "authors": [
      "Wentao Zhang",
      "Ziqi Yin",
      "Zeang Sheng",
      "Yang Li",
      "Wen Ouyang",
      "Xiaosen Li",
      "Yangyu Tao",
      "Zhi Yang",
      "Bin Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04359",
    "title": "Trajectory-dependent Generalization Bounds for Deep Neural Networks via  Fractional Brownian Motion",
    "abstract": "Despite being tremendously overparameterized, it is appreciated that deep neural networks trained by stochastic gradient descent (SGD) generalize surprisingly well. Based on the Rademacher complexity of a pre-specified hypothesis set, different norm-based generalization bounds have been developed to explain this phenomenon. However, recent studies suggest these bounds might be problematic as they increase with the training set size, which is contrary to empirical evidence. In this study, we argue that the hypothesis set SGD explores is trajectory-dependent and thus may provide a tighter bound over its Rademacher complexity. To this end, we characterize the SGD recursion via a stochastic differential equation by assuming the incurred stochastic gradient noise follows the fractional Brownian motion. We then identify the Rademacher complexity in terms of the covering numbers and relate it to the Hausdorff dimension of the optimization trajectory. By invoking the hypothesis set stability, we derive a novel generalization bound for deep neural networks. Extensive experiments demonstrate that it predicts well the generalization gap over several common experimental interventions. We further show that the Hurst parameter of the fractional Brownian motion is more informative than existing generalization indicators such as the power-law index and the upper Blumenthal-Getoor index. ",
    "url": "https://arxiv.org/abs/2206.04359",
    "authors": [
      "Chengli Tan",
      "Jiangshe Zhang",
      "Junmin Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04360",
    "title": "A general approximation lower bound in $L^p$ norm, with applications to  feed-forward neural networks",
    "abstract": "We study the fundamental limits to the expressive power of neural networks. Given two sets $F$, $G$ of real-valued functions, we first prove a general lower bound on how well functions in $F$ can be approximated in $L^p(\\mu)$ norm by functions in $G$, for any $p \\geq 1$ and any probability measure $\\mu$. The lower bound depends on the packing number of $F$, the range of $F$, and the fat-shattering dimension of $G$. We then instantiate this bound to the case where $G$ corresponds to a piecewise-polynomial feed-forward neural network, and describe in details the application to two sets $F$: H{\\\"o}lder balls and multivariate monotonic functions. Beside matching (known or new) upper bounds up to log factors, our lower bounds shed some light on the similarities or differences between approximation in $L^p$ norm or in sup norm, solving an open question by DeVore et al. (2021). Our proof strategy differs from the sup norm case and uses a key probability result of Mendelson (2002). ",
    "url": "https://arxiv.org/abs/2206.04360",
    "authors": [
      "El Mehdi Achour",
      "Armand Foucault",
      "S\u00e9bastien Gerchinovitz",
      "Fran\u00e7ois Malgouyres"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.04361",
    "title": "Model Degradation Hinders Deep Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have achieved great success in various graph mining tasks.However, drastic performance degradation is always observed when a GNN is stacked with many layers. As a result, most GNNs only have shallow architectures, which limits their expressive power and exploitation of deep neighborhoods.Most recent studies attribute the performance degradation of deep GNNs to the \\textit{over-smoothing} issue. In this paper, we disentangle the conventional graph convolution operation into two independent operations: \\textit{Propagation} (\\textbf{P}) and \\textit{Transformation} (\\textbf{T}).Following this, the depth of a GNN can be split into the propagation depth ($D_p$) and the transformation depth ($D_t$). Through extensive experiments, we find that the major cause for the performance degradation of deep GNNs is the \\textit{model degradation} issue caused by large $D_t$ rather than the \\textit{over-smoothing} issue mainly caused by large $D_p$. Further, we present \\textit{Adaptive Initial Residual} (AIR), a plug-and-play module compatible with all kinds of GNN architectures, to alleviate the \\textit{model degradation} issue and the \\textit{over-smoothing} issue simultaneously. Experimental results on six real-world datasets demonstrate that GNNs equipped with AIR outperform most GNNs with shallow architectures owing to the benefits of both large $D_p$ and $D_t$, while the time costs associated with AIR can be ignored. ",
    "url": "https://arxiv.org/abs/2206.04361",
    "authors": [
      "Wentao Zhang",
      "Zeang Sheng",
      "Ziqi Yin",
      "Yuezihan Jiang",
      "Yikuan Xia",
      "Jun Gao",
      "Zhi Yang",
      "Bin Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04363",
    "title": "Deep Neural Network for Blind Visual Quality Assessment of 4K Content",
    "abstract": "The 4K content can deliver a more immersive visual experience to consumers due to the huge improvement of spatial resolution. However, existing blind image quality assessment (BIQA) methods are not suitable for the original and upscaled 4K contents due to the expanded resolution and specific distortions. In this paper, we propose a deep learning-based BIQA model for 4K content, which on one hand can recognize true and pseudo 4K content and on the other hand can evaluate their perceptual visual quality. Considering the characteristic that high spatial resolution can represent more abundant high-frequency information, we first propose a Grey-level Co-occurrence Matrix (GLCM) based texture complexity measure to select three representative image patches from a 4K image, which can reduce the computational complexity and is proven to be very effective for the overall quality prediction through experiments. Then we extract different kinds of visual features from the intermediate layers of the convolutional neural network (CNN) and integrate them into the quality-aware feature representation. Finally, two multilayer perception (MLP) networks are utilized to map the quality-aware features into the class probability and the quality score for each patch respectively. The overall quality index is obtained through the average pooling of patch results. The proposed model is trained through the multi-task learning manner and we introduce an uncertainty principle to balance the losses of the classification and regression tasks. The experimental results show that the proposed model outperforms all compared BIQA metrics on four 4K content quality assessment databases. ",
    "url": "https://arxiv.org/abs/2206.04363",
    "authors": [
      "Wei Lu",
      "Wei Sun",
      "Xiongkuo Min",
      "Wenhan Zhu",
      "Quan Zhou",
      "Jun He",
      "Qiyuan Wang",
      "Zicheng Zhang",
      "Tao Wang",
      "Guangtao Zhai"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04365",
    "title": "CARLA-GeAR: a Dataset Generator for a Systematic Evaluation of  Adversarial Robustness of Vision Models",
    "abstract": "Adversarial examples represent a serious threat for deep neural networks in several application domains and a huge amount of work has been produced to investigate them and mitigate their effects. Nevertheless, no much work has been devoted to the generation of datasets specifically designed to evaluate the adversarial robustness of neural models. This paper presents CARLA-GeAR, a tool for the automatic generation of photo-realistic synthetic datasets that can be used for a systematic evaluation of the adversarial robustness of neural models against physical adversarial patches, as well as for comparing the performance of different adversarial defense/detection methods. The tool is built on the CARLA simulator, using its Python API, and allows the generation of datasets for several vision tasks in the context of autonomous driving. The adversarial patches included in the generated datasets are attached to billboards or the back of a truck and are crafted by using state-of-the-art white-box attack strategies to maximize the prediction error of the model under test. Finally, the paper presents an experimental study to evaluate the performance of some defense methods against such attacks, showing how the datasets generated with CARLA-GeAR might be used in future work as a benchmark for adversarial defense in the real world. All the code and datasets used in this paper are available at this http URL ",
    "url": "https://arxiv.org/abs/2206.04365",
    "authors": [
      "Federico Nesti",
      "Giulio Rossolini",
      "Gianluca D'Amico",
      "Alessandro Biondi",
      "Giorgio Buttazzo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04381",
    "title": "STIP: A SpatioTemporal Information-Preserving and Perception-Augmented  Model for High-Resolution Video Prediction",
    "abstract": "Although significant achievements have been achieved by recurrent neural network (RNN) based video prediction methods, their performance in datasets with high resolutions is still far from satisfactory because of the information loss problem and the perception-insensitive mean square error (MSE) based loss functions. In this paper, we propose a Spatiotemporal Information-Preserving and Perception-Augmented Model (STIP) to solve the above two problems. To solve the information loss problem, the proposed model aims to preserve the spatiotemporal information for videos during the feature extraction and the state transitions, respectively. Firstly, a Multi-Grained Spatiotemporal Auto-Encoder (MGST-AE) is designed based on the X-Net structure. The proposed MGST-AE can help the decoders recall multi-grained information from the encoders in both the temporal and spatial domains. In this way, more spatiotemporal information can be preserved during the feature extraction for high-resolution videos. Secondly, a Spatiotemporal Gated Recurrent Unit (STGRU) is designed based on the standard Gated Recurrent Unit (GRU) structure, which can efficiently preserve spatiotemporal information during the state transitions. The proposed STGRU can achieve more satisfactory performance with a much lower computation load compared with the popular Long Short-Term (LSTM) based predictive memories. Furthermore, to improve the traditional MSE loss functions, a Learned Perceptual Loss (LP-loss) is further designed based on the Generative Adversarial Networks (GANs), which can help obtain a satisfactory trade-off between the objective quality and the perceptual quality. Experimental results show that the proposed STIP can predict videos with more satisfactory visual quality compared with a variety of state-of-the-art methods. Source code has been available at \\url{https://github.com/ZhengChang467/STIPHR}. ",
    "url": "https://arxiv.org/abs/2206.04381",
    "authors": [
      "Zheng Chang",
      "Xinfeng Zhang",
      "Shanshe Wang",
      "Siwei Ma",
      "Wen Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04397",
    "title": "ESBMC-Jimple: Verifying Kotlin Programs via Jimple Intermediate  Representation",
    "abstract": "In this work, we describe and evaluate the first model checker for verifying Kotlin programs through the Jimple intermediate representation. The verifier, named ESBMC-Jimple, is built on top of the Efficient SMT-based Context-Bounded Model Checker (ESBMC). It uses the Soot framework to obtain the Jimple IR, representing a simplified version of the Kotlin source code, containing a maximum of three operands per instruction. ESBMC-Jimple processes Kotlin source code together with a model of the standard Kotlin libraries and checks a set of safety properties. Experimental results show that ESBMC-Jimple can correctly verify a set of Kotlin benchmarks from the literature and that it is competitive with state-of-the-art Java bytecode verifiers. A demonstration is available at https://youtu.be/J6WhNfXvJNc. ",
    "url": "https://arxiv.org/abs/2206.04397",
    "authors": [
      "Rafael Menezes",
      "Daniel Moura",
      "Helena Cavalcante",
      "Rosiane Freitas",
      "Lucas Cordeiro"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2206.04426",
    "title": "Biologically Inspired Dynamic Thresholds for Spiking Neural Networks",
    "abstract": "The dynamic membrane potential threshold, as one of the essential properties of a biological neuron, is a spontaneous regulation mechanism that maintains neuronal homeostasis, i.e., the constant overall spiking firing rate of a neuron. As such, the neuron firing rate is regulated by a dynamic spiking threshold, which has been extensively studied in biology. Existing work in the machine learning community does not employ bioplausible spiking threshold schemes. This work aims at bridging this gap by introducing a novel bioinspired dynamic energy-temporal threshold (BDETT) scheme for spiking neural networks (SNNs). The proposed BDETT scheme mirrors two bioplausible observations: a dynamic threshold has 1) a positive correlation with the average membrane potential and 2) a negative correlation with the preceding rate of depolarization. We validate the effectiveness of the proposed BDETT on robot obstacle avoidance and continuous control tasks under both normal conditions and various degraded conditions, including noisy observations, weights, and dynamic environments. We find that the BDETT outperforms existing static and heuristic threshold approaches by significant margins in all tested conditions, and we confirm that the proposed bioinspired dynamic threshold scheme offers bioplausible homeostasis to SNNs in complex real-world tasks. ",
    "url": "https://arxiv.org/abs/2206.04426",
    "authors": [
      "Jianchuan Ding",
      "Bo Dong",
      "Felix Heide",
      "Yufei Ding",
      "Yunduo Zhou",
      "Baocai Yin",
      "Xin Yang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2206.04428",
    "title": "Security-Reliability Trade-Off Analysis for SWIPT- and AF-Based IoT  Networks with Friendly Jammers",
    "abstract": "Radio-frequency (RF) energy harvesting (EH) in wireless relaying networks has attracted considerable recent interest, especially for supplying energy to relay nodes in Internet-of-Things (IoT) systems to assist the information exchange between a source and a destination. Moreover, limited hardware, computational resources, and energy availability of IoT devices have raised various security challenges. To this end, physical layer security (PLS) has been proposed as an effective alternative to cryptographic methods for providing information security. In this study, we propose a PLS approach for simultaneous wireless information and power transfer (SWIPT)-based half-duplex (HD) amplify-and-forward (AF) relaying systems in the presence of an eavesdropper. Furthermore, we take into account both static power splitting relaying (SPSR) and dynamic power splitting relaying (DPSR) to thoroughly investigate the benefits of each one. To further enhance secure communication, we consider multiple friendly jammers to help prevent wiretapping attacks from the eavesdropper. More specifically, we provide a reliability and security analysis by deriving closed-form expressions of outage probability (OP) and intercept probability (IP), respectively, for both the SPSR and DPSR schemes. Then, simulations are also performed to validate our analysis and the effectiveness of the proposed schemes. Specifically, numerical results illustrate the non-trivial trade-off between reliability and security of the proposed system. In addition, we conclude from the simulation results that the proposed DPSR scheme outperforms the SPSR-based scheme in terms of OP and IP under the influences of different parameters on system performance. ",
    "url": "https://arxiv.org/abs/2206.04428",
    "authors": [
      "Tan N. Nguyen",
      "Dinh-Hieu Tran",
      "Trinh Van Chien",
      "Van-Duc Phan",
      "Miroslav Voznak",
      "Phu Tran Tin",
      "Symeon Chatzinotas",
      "Derrick Wing Kwan Ng",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2206.04449",
    "title": "Segmentation Enhanced Lameness Detection in Dairy Cows from RGB and  Depth Video",
    "abstract": "Cow lameness is a severe condition that affects the life cycle and life quality of dairy cows and results in considerable economic losses. Early lameness detection helps farmers address illnesses early and avoid negative effects caused by the degeneration of cows' condition. We collected a dataset of short clips of cows passing through a hallway exiting a milking station and annotated the degree of lameness of the cows. This paper explores the resulting dataset and provides a detailed description of the data collection process. Additionally, we proposed a lameness detection method that leverages pre-trained neural networks to extract discriminative features from videos and assign a binary score to each cow indicating its condition: \"healthy\" or \"lame.\" We improve this approach by forcing the model to focus on the structure of the cow, which we achieve by substituting the RGB videos with binary segmentation masks predicted with a trained segmentation model. This work aims to encourage research and provide insights into the applicability of computer vision models for cow lameness detection on farms. ",
    "url": "https://arxiv.org/abs/2206.04449",
    "authors": [
      "Eric Arazo",
      "Robin Aly",
      "Kevin McGuinness"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04460",
    "title": "Open ERP System Data For Occupational Fraud Detection",
    "abstract": "Recent estimates report that companies lose 5% of their revenue to occupational fraud. Since most medium-sized and large companies employ Enterprise Resource Planning (ERP) systems to track vast amounts of information regarding their business process, researchers have in the past shown interest in automatically detecting fraud through ERP system data. Current research in this area, however, is hindered by the fact that ERP system data is not publicly available for the development and comparison of fraud detection methods. We therefore endeavour to generate public ERP system data that includes both normal business operation and fraud. We propose a strategy for generating ERP system data through a serious game, model a variety of fraud scenarios in cooperation with auditing experts, and generate data from a simulated make-to-stock production company with multiple research participants. We aggregate the generated data into ready to used datasets for fraud detection in ERP systems, and supply both the raw and aggregated data to the general public to allow for open development and comparison of fraud detection approaches on ERP system data. ",
    "url": "https://arxiv.org/abs/2206.04460",
    "authors": [
      "Julian Tritscher",
      "Fabian Gwinner",
      "Daniel Schl\u00f6r",
      "Anna Krause",
      "Andreas Hotho"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04465",
    "title": "Joint Encoder-Decoder Self-Supervised Pre-training for ASR",
    "abstract": "Self-supervised learning (SSL) has shown tremendous success in various speech-related downstream tasks, including Automatic Speech Recognition (ASR). The output embeddings of the SSL model are treated as powerful short-time representations of the speech signal. However, in the ASR task, the main objective is to get the correct sequence of acoustic units, characters, or byte-pair encodings (BPEs). Usually, encoder-decoder architecture works exceptionally well for a sequence-to-sequence task like ASR. Therefore, in this paper, we propose a new paradigm that exploits the power of a decoder during self-supervised learning. We use Hidden Unit BERT (HuBERT) SSL framework to compute the conventional masked prediction loss for the encoder. In addition, we have introduced a decoder in the SSL framework and proposed a target preparation strategy for the decoder. Finally, we use a multitask SSL setup wherein we jointly optimize both the encoder and decoder losses. We hypothesize that the presence of a decoder in the SSL model helps it learn an acoustic unit-based language model, which might improve the performance of an ASR downstream task. We compare our proposed SSL model with HuBERT and show up to 25% relative improvement in performance on ASR by finetuning on various LibriSpeech subsets. ",
    "url": "https://arxiv.org/abs/2206.04465",
    "authors": [
      "Arunkumar A",
      "Umesh S"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2206.04471",
    "title": "Towards Understanding Graph Neural Networks: An Algorithm Unrolling  Perspective",
    "abstract": "The graph neural network (GNN) has demonstrated its superior performance in various applications. The working mechanism behind it, however, remains mysterious. GNN models are designed to learn effective representations for graph-structured data, which intrinsically coincides with the principle of graph signal denoising (GSD). Algorithm unrolling, a \"learning to optimize\" technique, has gained increasing attention due to its prospects in building efficient and interpretable neural network architectures. In this paper, we introduce a class of unrolled networks built based on truncated optimization algorithms (e.g., gradient descent and proximal gradient descent) for GSD problems. They are shown to be tightly connected to many popular GNN models in that the forward propagations in these GNNs are in fact unrolled networks serving specific GSDs. Besides, the training process of a GNN model can be seen as solving a bilevel optimization problem with a GSD problem at the lower level. Such a connection brings a fresh view of GNNs, as we could try to understand their practical capabilities from their GSD counterparts, and it can also motivate designing new GNN models. Based on the algorithm unrolling perspective, an expressive model named UGDGNN, i.e., unrolled gradient descent GNN, is further proposed which inherits appealing theoretical properties. Extensive numerical simulations on seven benchmark datasets demonstrate that UGDGNN can achieve superior or competitive performance over the state-of-the-art models. ",
    "url": "https://arxiv.org/abs/2206.04471",
    "authors": [
      "Zepeng Zhang",
      "Ziping Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2206.04472",
    "title": "Early Transferability of Adversarial Examples in Deep Neural Networks",
    "abstract": "This paper will describe and analyze a new phenomenon that was not known before, which we call \"Early Transferability\". Its essence is that the adversarial perturbations transfer among different networks even at extremely early stages in their training. In fact, one can initialize two networks with two different independent choices of random weights and measure the angle between their adversarial perturbations after each step of the training. What we discovered was that these two adversarial directions started to align with each other already after the first few training steps (which typically use only a small fraction of the available training data), even though the accuracy of the two networks hadn't started to improve from their initial bad values due to the early stage of the training. The purpose of this paper is to present this phenomenon experimentally and propose plausible explanations for some of its properties. ",
    "url": "https://arxiv.org/abs/2206.04472",
    "authors": [
      "Oriel BenShmuel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2206.04480",
    "title": "Comparison Study of Inertial Sensor Signal Combination for Human  Activity Recognition based on Convolutional Neural Networks",
    "abstract": "Human Activity Recognition (HAR) is one of the essential building blocks of so many applications like security, monitoring, the internet of things and human-robot interaction. The research community has developed various methodologies to detect human activity based on various input types. However, most of the research in the field has been focused on applications other than human-in-the-centre applications. This paper focused on optimising the input signals to maximise the HAR performance from wearable sensors. A model based on Convolutional Neural Networks (CNN) has been proposed and trained on different signal combinations of three Inertial Measurement Units (IMU) that exhibit the movements of the dominant hand, leg and chest of the subject. The results demonstrate k-fold cross-validation accuracy between 99.77 and 99.98% for signals with the modality of 12 or higher. The performance of lower dimension signals, except signals containing information from both chest and ankle, was far inferior, showing between 73 and 85% accuracy. ",
    "url": "https://arxiv.org/abs/2206.04480",
    "authors": [
      "Farhad Nazari",
      "Navid Mohajer",
      "Darius Nahavandi",
      "Abbas Khosravi",
      "Saeid Nahavandi"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2206.04490",
    "title": "Redundancy in Deep Linear Neural Networks",
    "abstract": "Conventional wisdom states that deep linear neural networks benefit from expressiveness and optimization advantages over a single linear layer. This paper suggests that, in practice, the training process of deep linear fully-connected networks using conventional optimizers is convex in the same manner as a single linear fully-connected layer. This paper aims to explain this claim and demonstrate it. Even though convolutional networks are not aligned with this description, this work aims to attain a new conceptual understanding of fully-connected linear networks that might shed light on the possible constraints of convolutional settings and non-linear architectures. ",
    "url": "https://arxiv.org/abs/2206.04490",
    "authors": [
      "Oriel BenShmuel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2206.04500",
    "title": "Unlearning Protected User Attributes in Recommendations with Adversarial  Training",
    "abstract": "Collaborative filtering algorithms capture underlying consumption patterns, including the ones specific to particular demographics or protected information of users, e.g. gender, race, and location. These encoded biases can influence the decision of a recommendation system (RS) towards further separation of the contents provided to various demographic subgroups, and raise privacy concerns regarding the disclosure of users' protected attributes. In this work, we investigate the possibility and challenges of removing specific protected information of users from the learned interaction representations of a RS algorithm, while maintaining its effectiveness. Specifically, we incorporate adversarial training into the state-of-the-art MultVAE architecture, resulting in a novel model, Adversarial Variational Auto-Encoder with Multinomial Likelihood (Adv-MultVAE), which aims at removing the implicit information of protected attributes while preserving recommendation performance. We conduct experiments on the MovieLens-1M and LFM-2b-DemoBias datasets, and evaluate the effectiveness of the bias mitigation method based on the inability of external attackers in revealing the users' gender information from the model. Comparing with baseline MultVAE, the results show that Adv-MultVAE, with marginal deterioration in performance (w.r.t. NDCG and recall), largely mitigates inherent biases in the model on both datasets. ",
    "url": "https://arxiv.org/abs/2206.04500",
    "authors": [
      "Christian Ganh\u00f6r",
      "David Penz",
      "Navid Rekabsaz",
      "Oleg Lesota",
      "Markus Schedl"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04507",
    "title": "Software Mitigation of RISC-V Spectre Attacks",
    "abstract": "Speculative attacks are still an active threat today that, even if initially focused on the x86 platform, reach across all modern hardware architectures. RISC-V is a newly proposed open instruction set architecture that has seen traction from both the industry and academia in recent years. In this paper we focus on the RISC-V cores where speculation is enabled and, as we show, where Spectre attacks are as effective as on x86. Even though RISC-V hardware mitigations were proposed in the past, they have not yet passed the prototype phase. Instead, we propose low-overhead software mitigations for Spectre-BTI, inspired from those used on the x86 architecture, and for Spectre-RSB, to our knowledge the first such mitigation to be proposed. We show that these mitigations work in practice and that they can be integrated in the LLVM toolchain. For transparency and reproducibility, all our programs and data are made publicly available online. ",
    "url": "https://arxiv.org/abs/2206.04507",
    "authors": [
      "Ruxandra B\u0103lucea",
      "Paul Irofti"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Operating Systems (cs.OS)"
    ]
  },
  {
    "id": "arXiv:2206.04510",
    "title": "SsciBERT: A Pre-trained Language Model for Social Science Texts",
    "abstract": "The academic literature of social sciences is the literature that records human civilization and studies human social problems. With the large-scale growth of this literature, ways to quickly find existing research on relevant issues have become an urgent demand for researchers. Previous studies, such as SciBERT, have shown that pre-training using domain-specific texts can improve the performance of natural language processing tasks in those fields. However, there is no pre-trained language model for social sciences, so this paper proposes a pre-trained model on many abstracts published in the Social Science Citation Index (SSCI) journals. The models, which are available on Github (https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT), show excellent performance on discipline classification and abstract structure-function recognition tasks with the social sciences literature. ",
    "url": "https://arxiv.org/abs/2206.04510",
    "authors": [
      "Si Shen",
      "Jiangfeng Liu",
      "Litao Lin",
      "Ying Huang",
      "Lin Zhang",
      "Chang Liu",
      "Yutong Feng",
      "Dongbo Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2206.04516",
    "title": "Accurate Node Feature Estimation with Structured Variational Graph  Autoencoder",
    "abstract": "Given a graph with partial observations of node features, how can we estimate the missing features accurately? Feature estimation is a crucial problem for analyzing real-world graphs whose features are commonly missing during the data collection process. Accurate estimation not only provides diverse information of nodes but also supports the inference of graph neural networks that require the full observation of node features. However, designing an effective approach for estimating high-dimensional features is challenging, since it requires an estimator to have large representation power, increasing the risk of overfitting. In this work, we propose SVGA (Structured Variational Graph Autoencoder), an accurate method for feature estimation. SVGA applies strong regularization to the distribution of latent variables by structured variational inference, which models the prior of variables as Gaussian Markov random field based on the graph structure. As a result, SVGA combines the advantages of probabilistic inference and graph neural networks, achieving state-of-the-art performance in real datasets. ",
    "url": "https://arxiv.org/abs/2206.04516",
    "authors": [
      "Jaemin Yoo",
      "Hyunsik Jeon",
      "Jinhong Jung",
      "U Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04530",
    "title": "DORA: Exploring outlier representations in Deep Neural Networks",
    "abstract": "Deep Neural Networks (DNNs) draw their power from the representations they learn. In recent years, however, researchers have found that DNNs, while being incredibly effective in learning complex abstractions, also tend to be infected with artifacts, such as biases, Clever Hanses (CH), or Backdoors, due to spurious correlations inherent in the training data. So far, existing methods for uncovering such artifactual and malicious behavior in trained models focus on finding artifacts in the input data, which requires both availabilities of a data set and human intervention. In this paper, we introduce DORA (Data-agnOstic Representation Analysis): the first automatic data-agnostic method for the detection of potentially infected representations in Deep Neural Networks. We further show that contaminated representations found by DORA can be used to detect infected samples in any given dataset. We qualitatively and quantitatively evaluate the performance of our proposed method in both, controlled toy scenarios, and in real-world settings, where we demonstrate the benefit of DORA in safety-critical applications. ",
    "url": "https://arxiv.org/abs/2206.04530",
    "authors": [
      "Kirill Bykov",
      "Mayukh Deb",
      "Dennis Grinwald",
      "Klaus-Robert M\u00fcller",
      "Marina M.-C. H\u00f6hne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.04557",
    "title": "SparseFormer: Attention-based Depth Completion Network",
    "abstract": "Most pipelines for Augmented and Virtual Reality estimate the ego-motion of the camera by creating a map of sparse 3D landmarks. In this paper, we tackle the problem of depth completion, that is, densifying this sparse 3D map using RGB images as guidance. This remains a challenging problem due to the low density, non-uniform and outlier-prone 3D landmarks produced by SfM and SLAM pipelines. We introduce a transformer block, SparseFormer, that fuses 3D landmarks with deep visual features to produce dense depth. The SparseFormer has a global receptive field, making the module especially effective for depth completion with low-density and non-uniform landmarks. To address the issue of depth outliers among the 3D landmarks, we introduce a trainable refinement module that filters outliers through attention between the sparse landmarks. ",
    "url": "https://arxiv.org/abs/2206.04557",
    "authors": [
      "Frederik Warburg",
      "Michael Ramamonjisoa",
      "Manuel L\u00f3pez-Antequera"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04561",
    "title": "Functional Code Building Genetic Programming",
    "abstract": "General program synthesis has become an important application area for genetic programming (GP), and for artificial intelligence more generally. Code Building Genetic Programming (CBGP) is a recently introduced GP method for general program synthesis that leverages reflection and first class specifications to support the evolution of programs that may use arbitrary data types, polymorphism, and functions drawn from existing codebases. However, neither a formal description nor a thorough benchmarking of CBGP have yet been reported. In this work, we formalize the method of CBGP using algorithms from type theory. Specially, we show that a functional programming language and a Hindley-Milner type system can be used to evolve type-safe programs using the process abstractly described in the original CBGP paper. Furthermore, we perform a comprehensive analysis of the search performance of this functional variant of CBGP compared to other contemporary GP program synthesis methods. ",
    "url": "https://arxiv.org/abs/2206.04561",
    "authors": [
      "Edward Pantridge",
      "Thomas Helmuth",
      "Lee Spector"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2206.04564",
    "title": "TwiBot-22: Towards Graph-Based Twitter Bot Detection",
    "abstract": "Twitter bot detection has become an increasingly important task to combat misinformation, facilitate social media moderation, and preserve the integrity of the online discourse. State-of-the-art bot detection methods generally leverage the graph structure of the Twitter network, and they exhibit promising performance when confronting novel Twitter bots that traditional methods fail to detect. However, very few of the existing Twitter bot detection datasets are graph-based, and even these few graph-based datasets suffer from limited dataset scale, incomplete graph structure, as well as low annotation quality. In fact, the lack of a large-scale graph-based Twitter bot detection benchmark that addresses these issues has seriously hindered the development and evaluation of novel graph-based bot detection approaches. In this paper, we propose TwiBot-22, a comprehensive graph-based Twitter bot detection benchmark that presents the largest dataset to date, provides diversified entities and relations on the Twitter network, and has considerably better annotation quality than existing datasets. In addition, we re-implement 35 representative Twitter bot detection baselines and evaluate them on 9 datasets, including TwiBot-22, to promote a fair comparison of model performance and a holistic understanding of research progress. To facilitate further research, we consolidate all implemented codes and datasets into the TwiBot-22 evaluation framework, where researchers could consistently evaluate new models and datasets. The TwiBot-22 Twitter bot detection benchmark and evaluation framework are publicly available at https://twibot22.github.io/ ",
    "url": "https://arxiv.org/abs/2206.04564",
    "authors": [
      "Shangbin Feng",
      "Zhaoxuan Tan",
      "Herun Wan",
      "Ningnan Wang",
      "Zilong Chen",
      "Binchi Zhang",
      "Qinghua Zheng",
      "Wenqian Zhang",
      "Zhenyu Lei",
      "Shujie Yang",
      "Xinshun Feng",
      "Qingyue Zhang",
      "Hongrui Wang",
      "Yuhan Liu",
      "Yuyang Bai",
      "Heng Wang",
      "Zijian Cai",
      "Yanbo Wang",
      "Lijing Zheng",
      "Zihan Ma",
      "Jundong Li",
      "Minnan Luo"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04568",
    "title": "Byzantine-Resilient Decentralized Stochastic Optimization with Robust  Aggregation Rules",
    "abstract": "This work focuses on decentralized stochastic optimization in the presence of Byzantine attacks. During the optimization process, an unknown number of malfunctioning or malicious nodes, which we term as Byzantine workers, disobey the algorithmic protocol and send wrong messages to their neighbors. Even though various Byzantine-resilient algorithms have been developed for distributed stochastic optimization, we show that there are still two major challenges during the designation of robust aggregation rules suitable for decentralized stochastic optimization: disagreement and non-doubly stochastic mixing matrix. This paper provides a comprehensive analysis disclosing the negative effects of these two issues, and gives guidelines of designing favorable Byzantine-resilient decentralized stochastic optimization algorithms. Following the guidelines, we propose an iterative filtering-based robust aggregation rule termed iterative outlier scissor (IOS), which has provable Byzantine-resilience. Numerical experiments demonstrate the effectiveness of IOS. ",
    "url": "https://arxiv.org/abs/2206.04568",
    "authors": [
      "Zhaoxian Wu",
      "Tianyi Chen",
      "Qing Ling"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2206.04572",
    "title": "Log-Concave and Multivariate Canonical Noise Distributions for  Differential Privacy",
    "abstract": "A canonical noise distribution (CND) is an additive mechanism designed to satisfy $f$-differential privacy ($f$-DP), without any wasted privacy budget. $f$-DP is a hypothesis testing-based formulation of privacy phrased in terms of tradeoff functions, which captures the difficulty of a hypothesis test. In this paper, we consider the existence and construction of log-concave CNDs as well as multivariate CNDs. Log-concave distributions are important to ensure that higher outputs of the mechanism correspond to higher input values, whereas multivariate noise distributions are important to ensure that a joint release of multiple outputs has a tight privacy characterization. We show that the existence and construction of CNDs for both types of problems is related to whether the tradeoff function can be decomposed by functional composition (related to group privacy) or mechanism composition. In particular, we show that pure $\\epsilon$-DP cannot be decomposed in either way and that there is neither a log-concave CND nor any multivariate CND for $\\epsilon$-DP. On the other hand, we show that Gaussian-DP, $(0,\\delta)$-DP, and Laplace-DP each have both log-concave and multivariate CNDs. ",
    "url": "https://arxiv.org/abs/2206.04572",
    "authors": [
      "Jordan Awan",
      "Jinshuo Dong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2206.04584",
    "title": "Efficient and Robust 2D-to-BEV Representation Learning via  Geometry-guided Kernel Transformer",
    "abstract": "Learning Bird's Eye View (BEV) representation from surrounding-view cameras is of great importance for autonomous driving. In this work, we propose a Geometry-guided Kernel Transformer (GKT), a novel 2D-to-BEV representation learning mechanism. GKT leverages the geometric priors to guide the transformer to focus on discriminative regions and unfolds kernel features to generate BEV representation. For fast inference, we further introduce a look-up table (LUT) indexing method to get rid of the camera's calibrated parameters at runtime. GKT can run at $72.3$ FPS on 3090 GPU / $45.6$ FPS on 2080ti GPU and is robust to the camera deviation and the predefined BEV height. And GKT achieves the state-of-the-art real-time segmentation results, i.e., 38.0 mIoU (100m$\\times$100m perception range at a 0.5m resolution) on the nuScenes val set. Given the efficiency, effectiveness, and robustness, GKT has great practical values in autopilot scenarios, especially for real-time running systems. Code and models will be available at \\url{https://github.com/hustvl/GKT}. ",
    "url": "https://arxiv.org/abs/2206.04584",
    "authors": [
      "Shaoyu Chen",
      "Tianheng Cheng",
      "Xinggang Wang",
      "Wenming Meng",
      "Qian Zhang",
      "Wenyu Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04590",
    "title": "GASP: Gated Attention For Saliency Prediction",
    "abstract": "Saliency prediction refers to the computational task of modeling overt attention. Social cues greatly influence our attention, consequently altering our eye movements and behavior. To emphasize the efficacy of such features, we present a neural model for integrating social cues and weighting their influences. Our model consists of two stages. During the first stage, we detect two social cues by following gaze, estimating gaze direction, and recognizing affect. These features are then transformed into spatiotemporal maps through image processing operations. The transformed representations are propagated to the second stage (GASP) where we explore various techniques of late fusion for integrating social cues and introduce two sub-networks for directing attention to relevant stimuli. Our experiments indicate that fusion approaches achieve better results for static integration methods, whereas non-fusion approaches for which the influence of each modality is unknown, result in better outcomes when coupled with recurrent models for dynamic saliency prediction. We show that gaze direction and affective representations contribute a prediction to ground-truth correspondence improvement of at least 5% compared to dynamic saliency models without social cues. Furthermore, affective representations improve GASP, supporting the necessity of considering affect-biased attention in predicting saliency. ",
    "url": "https://arxiv.org/abs/2206.04590",
    "authors": [
      "Fares Abawi",
      "Tom Weber",
      "Stefan Wermter"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.04591",
    "title": "Privacy Leakage in Text Classification: A Data Extraction Approach",
    "abstract": "Recent work has demonstrated the successful extraction of training data from generative language models. However, it is not evident whether such extraction is feasible in text classification models since the training objective is to predict the class label as opposed to next-word prediction. This poses an interesting challenge and raises an important question regarding the privacy of training data in text classification settings. Therefore, we study the potential privacy leakage in the text classification domain by investigating the problem of unintended memorization of training data that is not pertinent to the learning task. We propose an algorithm to extract missing tokens of a partial text by exploiting the likelihood of the class label provided by the model. We test the effectiveness of our algorithm by inserting canaries into the training set and attempting to extract tokens in these canaries post-training. In our experiments, we demonstrate that successful extraction is possible to some extent. This can also be used as an auditing strategy to assess any potential unauthorized use of personal data without consent. ",
    "url": "https://arxiv.org/abs/2206.04591",
    "authors": [
      "Adel Elmahdy",
      "Huseyin A. Inan",
      "Robert Sim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04620",
    "title": "On the Generalization and Adaption Performance of Causal Models",
    "abstract": "Learning models that offer robust out-of-distribution generalization and fast adaptation is a key challenge in modern machine learning. Modelling causal structure into neural networks holds the promise to accomplish robust zero and few-shot adaptation. Recent advances in differentiable causal discovery have proposed to factorize the data generating process into a set of modules, i.e. one module for the conditional distribution of every variable where only causal parents are used as predictors. Such a modular decomposition of knowledge enables adaptation to distributions shifts by only updating a subset of parameters. In this work, we systematically study the generalization and adaption performance of such modular neural causal models by comparing it to monolithic models and structured models where the set of predictors is not constrained to causal parents. Our analysis shows that the modular neural causal models outperform other models on both zero and few-shot adaptation in low data regimes and offer robust generalization. We also found that the effects are more significant for sparser graphs as compared to denser graphs. ",
    "url": "https://arxiv.org/abs/2206.04620",
    "authors": [
      "Nino Scherrer",
      "Anirudh Goyal",
      "Stefan Bauer",
      "Yoshua Bengio",
      "Nan Rosemary Ke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.04621",
    "title": "A Critical Review on the Use (and Misuse) of Differential Privacy in  Machine Learning",
    "abstract": "We review the use of differential privacy (DP) for privacy protection in machine learning (ML). We show that, driven by the aim of preserving the accuracy of the learned models, DP-based ML implementations are so loose that they do not offer the ex ante privacy guarantees of DP. Instead, what they deliver is basically noise addition similar to the traditional (and often criticized) statistical disclosure control approach. Due to the lack of formal privacy guarantees, the actual level of privacy offered must be experimentally assessed ex post, which is done very seldom. In this respect, we present empirical results showing that standard anti-overfitting techniques in ML can achieve a better utility/privacy/efficiency trade-off than DP. ",
    "url": "https://arxiv.org/abs/2206.04621",
    "authors": [
      "Alberto Blanco-Justicia",
      "David Sanchez",
      "Josep Domingo-Ferrer",
      "Krishnamurty Muralidhar"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04635",
    "title": "Optimal Design of Energy-Harvesting Hybrid VLC-RF Networks",
    "abstract": "In this extended abstract, we consider a dual-hop hybrid visible light communication (VLC)/radio frequency (RF) scenario where energy is harvested during the VLC transmission and used to power the relay. We formulate the optimization problem in the sense of maximizing the data rate under the assumption of decode-and-forward (DF) relaying. As the design parameters, the direct current (DC) bias and the assigned time duration for energy harvesting are taken into account. In particular, the joint optimization is split into two subproblems, which are then cyclically solved. Additional details and numerical results are left to be presented in the full paper. ",
    "url": "https://arxiv.org/abs/2206.04635",
    "authors": [
      "Amir Hossein Fahim Raouf",
      "Chethan Kumar Anjinappa",
      "Ismail Guvenc"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2206.04658",
    "title": "BigVGAN: A Universal Neural Vocoder with Large-Scale Training",
    "abstract": "Despite recent progress in generative adversarial network(GAN)-based vocoders, where the model generates raw waveform conditioned on mel spectrogram, it is still challenging to synthesize high-fidelity audio for numerous speakers across varied recording environments. In this work, we present BigVGAN, a universal vocoder that generalizes well under various unseen conditions in zero-shot setting. We introduce periodic nonlinearities and anti-aliased representation into the generator, which brings the desired inductive bias for waveform synthesis and significantly improves audio quality. Based on our improved generator and the state-of-the-art discriminators, we train our GAN vocoder at the largest scale up to 112M parameters, which is unprecedented in the literature. In particular, we identify and address the training instabilities specific to such scale, while maintaining high-fidelity output without over-regularization. Our BigVGAN achieves the state-of-the-art zero-shot performance for various out-of-distribution scenarios, including new speakers, novel languages, singing voices, music and instrumental audio in unseen (even noisy) recording environments. We will release our code and model at: https://github.com/NVIDIA/BigVGAN ",
    "url": "https://arxiv.org/abs/2206.04658",
    "authors": [
      "Sang-gil Lee",
      "Wei Ping",
      "Boris Ginsburg",
      "Bryan Catanzaro",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2206.04665",
    "title": "AGConv: Adaptive Graph Convolution on 3D Point Clouds",
    "abstract": "Convolution on 3D point clouds is widely researched yet far from perfect in geometric deep learning. The traditional wisdom of convolution characterises feature correspondences indistinguishably among 3D points, arising an intrinsic limitation of poor distinctive feature learning. In this paper, we propose Adaptive Graph Convolution (AGConv) for wide applications of point cloud analysis. AGConv generates adaptive kernels for points according to their dynamically learned features. Compared with the solution of using fixed/isotropic kernels, AGConv improves the flexibility of point cloud convolutions, effectively and precisely capturing the diverse relations between points from different semantic parts. Unlike the popular attentional weight schemes, AGConv implements the adaptiveness inside the convolution operation instead of simply assigning different weights to the neighboring points. Extensive evaluations clearly show that our method outperforms state-of-the-arts of point cloud classification and segmentation on various benchmark datasets.Meanwhile, AGConv can flexibly serve more point cloud analysis approaches to boost their performance. To validate its flexibility and effectiveness, we explore AGConv-based paradigms of completion, denoising, upsampling, registration and circle extraction, which are comparable or even superior to their competitors. Our code is available at https://github.com/hrzhou2/AdaptConv-master. ",
    "url": "https://arxiv.org/abs/2206.04665",
    "authors": [
      "Mingqiang Wei",
      "Zeyong Wei",
      "Haoran Zhou",
      "Fei Hu",
      "Huajian Si",
      "Zhilei Chen",
      "Zhe Zhu",
      "Jingbo Qiu",
      "Xuefeng Yan",
      "Yanwen Guo",
      "Jun Wang",
      "Jing Qin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04668",
    "title": "GateHUB: Gated History Unit with Background Suppression for Online  Action Detection",
    "abstract": "Online action detection is the task of predicting the action as soon as it happens in a streaming video. A major challenge is that the model does not have access to the future and has to solely rely on the history, i.e., the frames observed so far, to make predictions. It is therefore important to accentuate parts of the history that are more informative to the prediction of the current frame. We present GateHUB, Gated History Unit with Background Suppression, that comprises a novel position-guided gated cross-attention mechanism to enhance or suppress parts of the history as per how informative they are for current frame prediction. GateHUB further proposes Future-augmented History (FaH) to make history features more informative by using subsequently observed frames when available. In a single unified framework, GateHUB integrates the transformer's ability of long-range temporal modeling and the recurrent model's capacity to selectively encode relevant information. GateHUB also introduces a background suppression objective to further mitigate false positive background frames that closely resemble the action frames. Extensive validation on three benchmark datasets, THUMOS, TVSeries, and HDD, demonstrates that GateHUB significantly outperforms all existing methods and is also more efficient than the existing best work. Furthermore, a flow-free version of GateHUB is able to achieve higher or close accuracy at 2.8x higher frame rate compared to all existing methods that require both RGB and optical flow information for prediction. ",
    "url": "https://arxiv.org/abs/2206.04668",
    "authors": [
      "Junwen Chen",
      "Gaurav Mittal",
      "Ye Yu",
      "Yu Kong",
      "Mei Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04669",
    "title": "Beyond RGB: Scene-Property Synthesis with Neural Radiance Fields",
    "abstract": "Comprehensive 3D scene understanding, both geometrically and semantically, is important for real-world applications such as robot perception. Most of the existing work has focused on developing data-driven discriminative models for scene understanding. This paper provides a new approach to scene understanding, from a synthesis model perspective, by leveraging the recent progress on implicit 3D representation and neural rendering. Building upon the great success of Neural Radiance Fields (NeRFs), we introduce Scene-Property Synthesis with NeRF (SS-NeRF) that is able to not only render photo-realistic RGB images from novel viewpoints, but also render various accurate scene properties (e.g., appearance, geometry, and semantics). By doing so, we facilitate addressing a variety of scene understanding tasks under a unified framework, including semantic segmentation, surface normal estimation, reshading, keypoint detection, and edge detection. Our SS-NeRF framework can be a powerful tool for bridging generative learning and discriminative learning, and thus be beneficial to the investigation of a wide range of interesting problems, such as studying task relationships within a synthesis paradigm, transferring knowledge to novel tasks, facilitating downstream discriminative tasks as ways of data augmentation, and serving as auto-labeller for data creation. ",
    "url": "https://arxiv.org/abs/2206.04669",
    "authors": [
      "Mingtong Zhang",
      "Shuhong Zheng",
      "Zhipeng Bao",
      "Martial Hebert",
      "Yu-Xiong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04672",
    "title": "Overcoming the Spectral Bias of Neural Value Approximation",
    "abstract": "Value approximation using deep neural networks is at the heart of off-policy deep reinforcement learning, and is often the primary module that provides learning signals to the rest of the algorithm. While multi-layer perceptron networks are universal function approximators, recent works in neural kernel regression suggest the presence of a spectral bias, where fitting high-frequency components of the value function requires exponentially more gradient update steps than the low-frequency ones. In this work, we re-examine off-policy reinforcement learning through the lens of kernel regression and propose to overcome such bias via a composite neural tangent kernel. With just a single line-change, our approach, the Fourier feature networks (FFN) produce state-of-the-art performance on challenging continuous control domains with only a fraction of the compute. Faster convergence and better off-policy stability also make it possible to remove the target network without suffering catastrophic divergences, which further reduces TD}(0)'s estimation bias on a few tasks. ",
    "url": "https://arxiv.org/abs/2206.04672",
    "authors": [
      "Ge Yang",
      "Anurag Ajay",
      "Pulkit Agrawal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.04673",
    "title": "Neural Prompt Search",
    "abstract": "The size of vision models has grown exponentially over the last few years, especially after the emergence of Vision Transformer. This has motivated the development of parameter-efficient tuning methods, such as learning adapter layers or visual prompt tokens, which allow a tiny portion of model parameters to be trained whereas the vast majority obtained from pre-training are frozen. However, designing a proper tuning method is non-trivial: one might need to try out a lengthy list of design choices, not to mention that each downstream dataset often requires custom designs. In this paper, we view the existing parameter-efficient tuning methods as \"prompt modules\" and propose Neural prOmpt seArcH (NOAH), a novel approach that learns, for large vision models, the optimal design of prompt modules through a neural architecture search algorithm, specifically for each downstream dataset. By conducting extensive experiments on over 20 vision datasets, we demonstrate that NOAH (i) is superior to individual prompt modules, (ii) has a good few-shot learning ability, and (iii) is domain-generalizable. The code and models are available at https://github.com/Davidzhangyuanhan/NOAH. ",
    "url": "https://arxiv.org/abs/2206.04673",
    "authors": [
      "Yuanhan Zhang",
      "Kaiyang Zhou",
      "Ziwei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04056",
    "title": "An Improved Deep Convolutional Neural Network by Using Hybrid  Optimization Algorithms to Detect and Classify Brain Tumor Using Augmented  MRI Images",
    "abstract": "Automated brain tumor detection is becoming a highly considerable medical diagnosis research. In recent medical diagnoses, detection and classification are highly considered to employ machine learning and deep learning techniques. Nevertheless, the accuracy and performance of current models need to be improved for suitable treatments. In this paper, an improvement in deep convolutional learning is ensured by adopting enhanced optimization algorithms, Thus, Deep Convolutional Neural Network (DCNN) based on improved Harris Hawks Optimization (HHO), called G-HHO has been considered. This hybridization features Grey Wolf Optimization (GWO) and HHO to give better results, limiting the convergence rate and enhancing performance. Moreover, Otsu thresholding is adopted to segment the tumor portion that emphasizes brain tumor detection. Experimental studies are conducted to validate the performance of the suggested method on a total number of 2073 augmented MRI images. The technique's performance was ensured by comparing it with the nine existing algorithms on huge augmented MRI images in terms of accuracy, precision, recall, f-measure, execution time, and memory usage. The performance comparison shows that the DCNN-G-HHO is much more successful than existing methods, especially on a scoring accuracy of 97%. Additionally, the statistical performance analysis indicates that the suggested approach is faster and utilizes less memory at identifying and categorizing brain tumor cancers on the MR images. The implementation of this validation is conducted on the Python platform. The relevant codes for the proposed approach are available at: https://github.com/bryarahassan/DCNN-G-HHO. ",
    "url": "https://arxiv.org/abs/2206.04056",
    "authors": [
      "Shko M. Qader",
      "Bryar A. Hassan",
      "Tarik A. Rashid"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.04262",
    "title": "The Classical Capacity of Quantum Jackson Networks with Waiting  Time-Dependent Erasures",
    "abstract": "We study the fundamental limits of classical communication using quantum states that decohere as they traverse through a network of queues. We consider a network of Markovian queues, known as a Jackson network, with a single source or multiple sources and a single destination. Qubits are communicated through this network with inevitable buffering at intermediate nodes. We model each node as a `queue-channel,' wherein as the qubits wait in buffer, they continue to interact with the environment and suffer a waiting time-dependent noise. Focusing on erasures, we first obtain explicit classical capacity expressions for simple topologies such as tandem queue-channel and parallel queue-channel. Using these as building blocks, we characterize the classical capacity of a general quantum Jackson network with waiting time-dependent erasures. Throughout, we study two types of quantum networks, namely, (i) Repeater-assisted and (ii) Repeater-less. We also obtain optimal pumping rates and routing probabilities to maximize capacity in simple topologies. More broadly, our work quantifies the impact of delay-induced decoherence on the fundamental limits of classical communication over quantum networks. ",
    "url": "https://arxiv.org/abs/2206.04262",
    "authors": [
      "Jaswanthi Mandalapu",
      "Krishna Jagannathan"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2206.04276",
    "title": "Robust Matrix Completion with Heavy-tailed Noise",
    "abstract": "This paper studies low-rank matrix completion in the presence of heavy-tailed and possibly asymmetric noise, where we aim to estimate an underlying low-rank matrix given a set of highly incomplete noisy entries. Though the matrix completion problem has attracted much attention in the past decade, there is still lack of theoretical understanding when the observations are contaminated by heavy-tailed noises. Prior theory falls short of explaining the empirical results and is unable to capture the optimal dependence of the estimation error on the noise level. In this paper, we adopt an adaptive Huber loss to accommodate heavy-tailed noise, which is robust against large and possibly asymmetric errors when the parameter in the loss function is carefully designed to balance the Huberization biases and robustness to outliers. Then, we propose an efficient nonconvex algorithm via a balanced low-rank Burer-Monteiro matrix factorization and gradient decent with robust spectral initialization. We prove that under merely bounded second moment condition on the error distributions, rather than the sub-Gaussian assumption, the Euclidean error of the iterates generated by the proposed algorithm decrease geometrically fast until achieving a minimax-optimal statistical estimation error, which has the same order as that in the sub-Gaussian case. The key technique behind this significant advancement is a powerful leave-one-out analysis framework. The theoretical results are corroborated by our simulation studies. ",
    "url": "https://arxiv.org/abs/2206.04276",
    "authors": [
      "Bingyan Wang",
      "Jianqing Fan"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.04320",
    "title": "Negative Shannon Information Hides Networks",
    "abstract": "Negative numbers are essential in mathematics. They are not needed to describe statistical experiments, as those are expressed in terms of positive probabilities. Shannon information was firstly defined for characterizing informational uncertainty of classical probabilistic distributions. However, it is unknown why there is negative information for more than two random variables on finite sample spaces. We first show the negative Shannon mutual information of three random variables implies Bayesian network representations of its joint distribution. We then show the intrinsic compatibility with negative Shannon information is generic for Bayesian networks with quantum realizations. This further suggests a new kind of space-dependent nonlocality. The present result provides a device-independent witness of negative Shannon information. ",
    "url": "https://arxiv.org/abs/2206.04320",
    "authors": [
      "Ming-Xing Luo"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2206.04405",
    "title": "Conformal Off-Policy Prediction in Contextual Bandits",
    "abstract": "Most off-policy evaluation methods for contextual bandits have focused on the expected outcome of a policy, which is estimated via methods that at best provide only asymptotic guarantees. However, in many applications, the expectation may not be the best measure of performance as it does not capture the variability of the outcome. In addition, particularly in safety-critical settings, stronger guarantees than asymptotic correctness may be required. To address these limitations, we consider a novel application of conformal prediction to contextual bandits. Given data collected under a behavioral policy, we propose \\emph{conformal off-policy prediction} (COPP), which can output reliable predictive intervals for the outcome under a new target policy. We provide theoretical finite-sample guarantees without making any additional assumptions beyond the standard contextual bandit setup, and empirically demonstrate the utility of COPP compared with existing methods on synthetic and real-world data. ",
    "url": "https://arxiv.org/abs/2206.04405",
    "authors": [
      "Muhammad Faaiz Taufiq",
      "Jean-Francois Ton",
      "Rob Cornish",
      "Yee Whye Teh",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04447",
    "title": "Convolutional Dictionary Learning by End-To-End Training of Iterative  Neural Networks",
    "abstract": "Sparsity-based methods have a long history in the field of signal processing and have been successfully applied to various image reconstruction problems. The involved sparsifying transformations or dictionaries are typically either pre-trained using a model which reflects the assumed properties of the signals or adaptively learned during the reconstruction - yielding so-called blind Compressed Sensing approaches. However, by doing so, the transforms are never explicitly trained in conjunction with the physical model which generates the signals. In addition, properly choosing the involved regularization parameters remains a challenging task. Another recently emerged training-paradigm for regularization methods is to use iterative neural networks (INNs) - also known as unrolled networks - which contain the physical model. In this work, we construct an INN which can be used as a supervised and physics-informed online convolutional dictionary learning algorithm. We evaluated the proposed approach by applying it to a realistic large-scale dynamic MR reconstruction problem and compared it to several other recently published works. We show that the proposed INN improves over two conventional model-agnostic training methods and yields competitive results also compared to a deep INN. Further, it does not require to choose the regularization parameters and - in contrast to deep INNs - each network component is entirely interpretable. ",
    "url": "https://arxiv.org/abs/2206.04447",
    "authors": [
      "Andreas Kofler",
      "Christian Wald",
      "Tobias Schaeffter",
      "Markus Haltmeier",
      "Christoph Kolbitsch"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2206.04498",
    "title": "Abstract message passing and distributed graph signal processing",
    "abstract": "Graph signal processing is a framework to handle graph structured data. The fundamental concept is graph shift operator, giving rise to the graph Fourier transform. While the graph Fourier transform is a centralized procedure, distributed graph signal processing algorithms are needed to address challenges such as scalability and privacy. In this paper, we develop a theory of distributed graph signal processing based on the classical notion of message passing. However, we generalize the definition of a message to permit more abstract mathematical objects. The framework provides an alternative point of view that avoids the iterative nature of existing approaches to distributed graph signal processing. Moreover, our framework facilitates investigating theoretical questions such as solubility of distributed problems. ",
    "url": "https://arxiv.org/abs/2206.04498",
    "authors": [
      "Feng Ji",
      "Yiqi Lu",
      "Wee Peng Tay",
      "Edwin Chong"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2206.04569",
    "title": "Benefits of Overparameterized Convolutional Residual Networks: Function  Approximation under Smoothness Constraint",
    "abstract": "Overparameterized neural networks enjoy great representation power on complex data, and more importantly yield sufficiently smooth output, which is crucial to their generalization and robustness. Most existing function approximation theories suggest that with sufficiently many parameters, neural networks can well approximate certain classes of functions in terms of the function value. The neural network themselves, however, can be highly nonsmooth. To bridge this gap, we take convolutional residual networks (ConvResNets) as an example, and prove that large ConvResNets can not only approximate a target function in terms of function value, but also exhibit sufficient first-order smoothness. Moreover, we extend our theory to approximating functions supported on a low-dimensional manifold. Our theory partially justifies the benefits of using deep and wide networks in practice. Numerical experiments on adversarial robust image classification are provided to support our theory. ",
    "url": "https://arxiv.org/abs/2206.04569",
    "authors": [
      "Hao Liu",
      "Minshuo Chen",
      "Siawpeng Er",
      "Wenjing Liao",
      "Tong Zhang",
      "Tuo Zhao"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04573",
    "title": "Simple lessons from complex learning: what a neural network model learns  about cosmic structure formation",
    "abstract": "We train a neural network model to predict the full phase space evolution of cosmological N-body simulations. Its success implies that the neural network model is accurately approximating the Green's function expansion that relates the initial conditions of the simulations to its outcome at later times in the deeply nonlinear regime. We test the accuracy of this approximation by assessing its performance on well understood simple cases that have either known exact solutions or well understood expansions. These scenarios include spherical configurations, isolated plane waves, and two interacting plane waves: initial conditions that are very different from the Gaussian random fields used for training. We find our model generalizes well to these well understood scenarios, demonstrating that the networks have inferred general physical principles and learned the nonlinear mode couplings from the complex, random Gaussian training data. These tests also provide a useful diagnostic for finding the model's strengths and weaknesses, and identifying strategies for model improvement. We also test the model on initial conditions that contain only transverse modes, a family of modes that differ not only in their phases but also in their evolution from the longitudinal growing modes used in the training set. When the network encounters these initial conditions that are orthogonal to the training set, the model fails completely. In addition to these simple configurations, we evaluate the model's predictions for the density, displacement, and momentum power spectra with standard initial conditions for N-body simulations. We compare these summary statistics against N-body results and an approximate, fast simulation method called COLA. Our model achieves percent level accuracy at nonlinear scales of $k\\sim 1\\ \\mathrm{Mpc}^{-1}\\, h$, representing a significant improvement over COLA. ",
    "url": "https://arxiv.org/abs/2206.04573",
    "authors": [
      "Drew Jamieson",
      "Yin Li",
      "Siyu He",
      "Francisco Villaescusa-Navarro",
      "Shirley Ho",
      "Renan Alves de Oliveira",
      "David N. Spergel"
    ],
    "subjectives": [
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04594",
    "title": "Field Level Neural Network Emulator for Cosmological N-body Simulations",
    "abstract": "We build a field level emulator for cosmic structure formation that is accurate in the nonlinear regime. Our emulator consists of two convolutional neural networks trained to output the nonlinear displacements and velocities of N-body simulation particles based on their linear inputs. Cosmology dependence is encoded in the form of style parameters at each layer of the neural network, enabling the emulator to effectively interpolate the outcomes of structure formation between different flat $\\Lambda$CDM cosmologies over a wide range of background matter densities. The neural network architecture makes the model differentiable by construction, providing a powerful tool for fast field level inference. We test the accuracy of our method by considering several summary statistics, including the density power spectrum with and without redshift space distortions, the displacement power spectrum, the momentum power spectrum, the density bispectrum, halo abundances, and halo profiles with and without redshift space distortions. We compare these statistics from our emulator with the full N-body results, the COLA method, and a fiducial neural network with no cosmological dependence. We find our emulator gives accurate results down to scales of $k \\sim 1\\ \\mathrm{Mpc}^{-1}\\, h$, representing a considerable improvement over both COLA and the fiducial neural network. We also demonstrate that our emulator generalizes well to initial conditions containing primordial non-Gaussianity, without the need for any additional style parameters or retraining. ",
    "url": "https://arxiv.org/abs/2206.04594",
    "authors": [
      "Drew Jamieson",
      "Yin Li",
      "Renan Alves de Oliveira",
      "Francisco Villaescusa-Navarro",
      "Shirley Ho",
      "David N. Spergel"
    ],
    "subjectives": [
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.04647",
    "title": "VideoINR: Learning Video Implicit Neural Representation for Continuous  Space-Time Super-Resolution",
    "abstract": "Videos typically record the streaming and continuous visual data as discrete consecutive frames. Since the storage cost is expensive for videos of high fidelity, most of them are stored in a relatively low resolution and frame rate. Recent works of Space-Time Video Super-Resolution (STVSR) are developed to incorporate temporal interpolation and spatial super-resolution in a unified framework. However, most of them only support a fixed up-sampling scale, which limits their flexibility and applications. In this work, instead of following the discrete representations, we propose Video Implicit Neural Representation (VideoINR), and we show its applications for STVSR. The learned implicit neural representation can be decoded to videos of arbitrary spatial resolution and frame rate. We show that VideoINR achieves competitive performances with state-of-the-art STVSR methods on common up-sampling scales and significantly outperforms prior works on continuous and out-of-training-distribution scales. Our project page is at this http URL . ",
    "url": "https://arxiv.org/abs/2206.04647",
    "authors": [
      "Zeyuan Chen",
      "Yinbo Chen",
      "Jingwen Liu",
      "Xingqian Xu",
      "Vidit Goel",
      "Zhangyang Wang",
      "Humphrey Shi",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2002.04861",
    "title": "Training Two-Layer ReLU Networks with Gradient Descent is Inconsistent",
    "abstract": " Comments: To appear in Journal of Machine Learning Research (JMLR). Changes in v3: Added new Section 10 with extensive experimental evaluation. Code available at this https URL ",
    "url": "https://arxiv.org/abs/2002.04861",
    "authors": [
      "David Holzm\u00fcller",
      "Ingo Steinwart"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2006.14042",
    "title": "Blacklight: Scalable Defense for Neural Networks against Query-Based  Black-Box Attacks",
    "abstract": " Title: Blacklight: Scalable Defense for Neural Networks against Query-Based  Black-Box Attacks ",
    "url": "https://arxiv.org/abs/2006.14042",
    "authors": [
      "Huiying Li",
      "Shawn Shan",
      "Emily Wenger",
      "Jiayun Zhang",
      "Haitao Zheng",
      "Ben Y. Zhao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2007.12826",
    "title": "The Interpolation Phase Transition in Neural Networks: Memorization and  Generalization under Lazy Training",
    "abstract": " Comments: 83 pages, 5 figures ",
    "url": "https://arxiv.org/abs/2007.12826",
    "authors": [
      "Andrea Montanari",
      "Yiqiao Zhong"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2008.06029",
    "title": "Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks  in Highly Accelerated MRI",
    "abstract": " Title: Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks  in Highly Accelerated MRI ",
    "url": "https://arxiv.org/abs/2008.06029",
    "authors": [
      "Burhaneddin Yaman",
      "Hongyi Gu",
      "Seyed Amir Hossein Hosseini",
      "Omer Burak Demirel",
      "Steen Moeller",
      "Jutta Ellermann",
      "K\u00e2mil U\u011furbil",
      "Mehmet Ak\u00e7akaya"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2012.08466",
    "title": "Objective-Based Hierarchical Clustering of Deep Embedding Vectors",
    "abstract": " Title: Objective-Based Hierarchical Clustering of Deep Embedding Vectors ",
    "url": "https://arxiv.org/abs/2012.08466",
    "authors": [
      "Stanislav Naumov",
      "Grigory Yaroslavtsev",
      "Dmitrii Avdiukhin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2103.04053",
    "title": "NVUM: Non-Volatile Unbiased Memory for Robust Medical Image  Classification",
    "abstract": " Comments: MICCAI 2022 Early Accept ",
    "url": "https://arxiv.org/abs/2103.04053",
    "authors": [
      "Fengbei Liu",
      "Yuanhong Chen",
      "Yu Tian",
      "Yuyuan Liu",
      "Chong Wang",
      "Vasileios Belagiannis",
      "Gustavo Carneiro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2104.10039",
    "title": "GraphGuess: Approximate Graph Processing System with Adaptive Correction",
    "abstract": " Title: GraphGuess: Approximate Graph Processing System with Adaptive Correction ",
    "url": "https://arxiv.org/abs/2104.10039",
    "authors": [
      "Morteza Ramezani",
      "Mahmut T. Kandemir",
      "Anand Sivasubramaniam"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2105.10123",
    "title": "Backdoor Attacks on Self-Supervised Learning",
    "abstract": " Comments: CVPR 2022 (Oral) ",
    "url": "https://arxiv.org/abs/2105.10123",
    "authors": [
      "Aniruddha Saha",
      "Ajinkya Tejankar",
      "Soroush Abbasi Koohpayegani",
      "Hamed Pirsiavash"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2106.05009",
    "title": "Network insensitivity to parameter noise via adversarial regularization",
    "abstract": " Title: Network insensitivity to parameter noise via adversarial regularization ",
    "url": "https://arxiv.org/abs/2106.05009",
    "authors": [
      "Julian B\u00fcchel",
      "Fynn Faber",
      "Dylan R. Muir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.00637",
    "title": "Generalization and Robustness Implications in Object-Centric Learning",
    "abstract": " Comments: Published at ICML 2022 ",
    "url": "https://arxiv.org/abs/2107.00637",
    "authors": [
      "Andrea Dittadi",
      "Samuele Papa",
      "Michele De Vita",
      "Bernhard Sch\u00f6lkopf",
      "Ole Winther",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2107.05762",
    "title": "Strategic Instrumental Variable Regression: Recovering Causal  Relationships From Strategic Responses",
    "abstract": " Comments: In the 39th International Conference on Machine Learning (ICML 2022) ",
    "url": "https://arxiv.org/abs/2107.05762",
    "authors": [
      "Keegan Harris",
      "Daniel Ngo",
      "Logan Stapleton",
      "Hoda Heidari",
      "Zhiwei Steven Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.08543",
    "title": "Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back  Projection Augmentation",
    "abstract": " Comments: table fixed ",
    "url": "https://arxiv.org/abs/2107.08543",
    "authors": [
      "Talgat Saparov",
      "Anvar Kurmukov",
      "Boris Shirokikh",
      "Mikhail Belyaev"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2107.13137",
    "title": "Unsupervised Monocular Depth Estimation in Highly Complex Environments",
    "abstract": " Comments: Accepted by IEEE Transactions on Emerging Topics in Computational Intelligence ",
    "url": "https://arxiv.org/abs/2107.13137",
    "authors": [
      "Chaoqiang Zhao",
      "Yang Tang",
      "Qiyu Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2108.10097",
    "title": "Graph Attention MLP with Reliable Label Utilization",
    "abstract": " Comments: 12 pages, 4 figures ",
    "url": "https://arxiv.org/abs/2108.10097",
    "authors": [
      "Wentao Zhang",
      "Ziqi Yin",
      "Zeang Sheng",
      "Wen Ouyang",
      "Xiaosen Li",
      "Yangyu Tao",
      "Zhi Yang",
      "Bin Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2109.02035",
    "title": "Variational Physics Informed Neural Networks: the role of quadratures  and test functions",
    "abstract": " Comments: 20 pages, 22 figures ",
    "url": "https://arxiv.org/abs/2109.02035",
    "authors": [
      "Stefano Berrone",
      "Claudio Canuto",
      "Moreno Pintore"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2109.06429",
    "title": "Robust Inverse Framework using Knowledge-guided Self-Supervised  Learning: An application to Hydrology",
    "abstract": " Comments: Accepted at KDD 2022 ",
    "url": "https://arxiv.org/abs/2109.06429",
    "authors": [
      "Rahul Ghosh",
      "Arvind Renganathan",
      "Kshitij Tayal",
      "Xiang Li",
      "Ankush Khandelwal",
      "Xiaowei Jia",
      "Chris Duffy",
      "John Neiber",
      "Vipin Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2109.08026",
    "title": "EVAGAN: Evasion Generative Adversarial Network for Low Data Regimes",
    "abstract": " Comments: 12 pages, 10 figures ",
    "url": "https://arxiv.org/abs/2109.08026",
    "authors": [
      "Rizwan Hamid Randhawa",
      "Nauman Aslam",
      "Mohammad Alauthman",
      "Husnain Rafiq"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2109.09461",
    "title": "Machine-Learning Side-Channel Attacks on the GALACTICS Constant-Time  Implementation of BLISS",
    "abstract": " Comments: 23 pages, 4 Figures, 7 algorithms ",
    "url": "https://arxiv.org/abs/2109.09461",
    "authors": [
      "Soundes Marzougui",
      "Nils Wisiol",
      "Patrick Gersch",
      "Juliane Kr\u00e4mer",
      "Jean-Pierre Seifert"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2110.05007",
    "title": "Boosting Fast Adversarial Training with Learnable Adversarial  Initialization",
    "abstract": " Comments: Accepted TIP ",
    "url": "https://arxiv.org/abs/2110.05007",
    "authors": [
      "Xiaojun Jia",
      "Yong Zhang",
      "Baoyuan Wu",
      "Jue Wang",
      "Xiaochun Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.06892",
    "title": "TAG: Toward Accurate Social Media Content Tagging with a Concept Graph",
    "abstract": " Comments: Accepted by ACM SIGKDD 2022 ",
    "url": "https://arxiv.org/abs/2110.06892",
    "authors": [
      "Jiuding Yang",
      "Weidong Guo",
      "Bang Liu",
      "Yakun Yu",
      "Chaoyue Wang",
      "Jinwen Luo",
      "Linglong Kong",
      "Di Niu",
      "Zhen Wen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2110.11088",
    "title": "RoMA: a Method for Neural Network Robustness Measurement and Assessment",
    "abstract": " Title: RoMA: a Method for Neural Network Robustness Measurement and Assessment ",
    "url": "https://arxiv.org/abs/2110.11088",
    "authors": [
      "Natan Levy",
      "Guy Katz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2111.12284",
    "title": "A Self-Supervised Automatic Post-Editing Data Generation Tool",
    "abstract": " Comments: Accepted for DataPerf workshop at ICML 2022 ",
    "url": "https://arxiv.org/abs/2111.12284",
    "authors": [
      "Hyeonseok Moon",
      "Chanjun Park",
      "Sugyeong Eo",
      "Jaehyung Seo",
      "SeungJun Lee",
      "Heuiseok Lim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2112.13593",
    "title": "Multi-modal Attention Network for Stock Movements Prediction",
    "abstract": " Title: Multi-modal Attention Network for Stock Movements Prediction ",
    "url": "https://arxiv.org/abs/2112.13593",
    "authors": [
      "Shwai He",
      "Shi Gu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Trading and Market Microstructure (q-fin.TR)"
    ]
  },
  {
    "id": "arXiv:2201.02588",
    "title": "FogAdapt: Self-Supervised Domain Adaptation for Semantic Segmentation of  Foggy Images",
    "abstract": " Comments: Accepted at Elsevier Journal of Neurocomputing ",
    "url": "https://arxiv.org/abs/2201.02588",
    "authors": [
      "Javed Iqbal",
      "Rehan Hafiz",
      "Mohsen Ali"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.08443",
    "title": "Diversifying the Genomic Data Science Research Community",
    "abstract": " Comments: 42 pages, 3 figures ",
    "url": "https://arxiv.org/abs/2201.08443",
    "authors": [
      "Genomic Data Science Community Network",
      "Rosa Alcazar",
      "Maria Alvarez",
      "Rachel Arnold",
      "Mentewab Ayalew",
      "Lyle G. Best",
      "Michael C. Campbell",
      "Kamal Chowdhury",
      "Katherine E. L. Cox",
      "Christina Daulton",
      "Youping Deng",
      "Carla Easter",
      "Karla Fuller",
      "Shazia Tabassum Hakim",
      "Ava M. Hoffman",
      "Natalie Kucher",
      "Andrew Lee",
      "Joslynn Lee",
      "Jeffrey T. Leek",
      "Robert Meller",
      "Loyda B. M\u00e9ndez",
      "Miguel P. M\u00e9ndez-Gonz\u00e1lez",
      "Stephen Mosher",
      "Michele Nishiguchi",
      "Siddharth Pratap",
      "Tiffany Rolle",
      "Sourav Roy",
      "Rachel Saidi",
      "Michael C. Schatz",
      "Shurjo Sen",
      "James Sniezek",
      "Edu Suarez Martinez",
      "Frederick Tan",
      "Jennifer Vessio",
      "Karriem Watson",
      "Wendy Westbroek"
    ],
    "subjectives": [
      "Other Quantitative Biology (q-bio.OT)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2201.12179",
    "title": "Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks",
    "abstract": " Comments: Accepted by ICML 2022 ",
    "url": "https://arxiv.org/abs/2201.12179",
    "authors": [
      "Lukas Struppek",
      "Dominik Hintersdorf",
      "Antonio De Almeida Correia",
      "Antonia Adler",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2202.01361",
    "title": "Generative Flow Networks for Discrete Probabilistic Modeling",
    "abstract": " Comments: Accepted by ICML 2022 ",
    "url": "https://arxiv.org/abs/2202.01361",
    "authors": [
      "Dinghuai Zhang",
      "Nikolay Malkin",
      "Zhen Liu",
      "Alexandra Volokhova",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2202.08312",
    "title": "Improved Differential Privacy for SGD via Optimal Private Linear  Operators on Adaptive Streams",
    "abstract": " Comments: 32 pages, 6 figures. Associated code at this https URL ",
    "url": "https://arxiv.org/abs/2202.08312",
    "authors": [
      "Sergey Denisov",
      "Brendan McMahan",
      "Keith Rush",
      "Adam Smith",
      "Abhradeep Guha Thakurta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2202.11425",
    "title": "Multi-view Intent Disentangle Graph Networks for Bundle Recommendation",
    "abstract": " Title: Multi-view Intent Disentangle Graph Networks for Bundle Recommendation ",
    "url": "https://arxiv.org/abs/2202.11425",
    "authors": [
      "Sen Zhao",
      "Wei Wei",
      "Ding Zou",
      "Xianling Mao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2203.00418",
    "title": "Recovery of Missing Sensor Data by Reconstructing Time-varying Graph  Signals",
    "abstract": " Comments: Five pages, two figures, Accepted at EUSIPCO 2022 ",
    "url": "https://arxiv.org/abs/2203.00418",
    "authors": [
      "Anindya Mondal",
      "Mayukhmali Das",
      "Aditi Chatterjee",
      "Palaniandavar Venkateswaran"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2203.01827",
    "title": "Democratic Governance and International Research Collaboration: A  Longitudinal Analysis of the Global Science Network",
    "abstract": " Title: Democratic Governance and International Research Collaboration: A  Longitudinal Analysis of the Global Science Network ",
    "url": "https://arxiv.org/abs/2203.01827",
    "authors": [
      "Travis A. Whetsell"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Digital Libraries (cs.DL)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2203.12616",
    "title": "Unsupervised Pre-Training on Patient Population Graphs for Patient-Level  Predictions",
    "abstract": " Comments: 10 pages, 1 figure, 3 tables ",
    "url": "https://arxiv.org/abs/2203.12616",
    "authors": [
      "Chantal Pellegrini",
      "Anees Kazi",
      "Nassir Navab"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2203.15041",
    "title": "Socially Compliant Navigation Dataset (SCAND): A Large-Scale Dataset of  Demonstrations for Social Navigation",
    "abstract": " Title: Socially Compliant Navigation Dataset (SCAND): A Large-Scale Dataset of  Demonstrations for Social Navigation ",
    "url": "https://arxiv.org/abs/2203.15041",
    "authors": [
      "Haresh Karnan",
      "Anirudh Nair",
      "Xuesu Xiao",
      "Garrett Warnell",
      "Soeren Pirk",
      "Alexander Toshev",
      "Justin Hart",
      "Joydeep Biswas",
      "Peter Stone"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2203.16616",
    "title": "Knowledge-based Entity Prediction for Improved Machine Perception in  Autonomous Systems",
    "abstract": " Comments: 6 pages, 4 figures, in IEEE Intelligent Systems, 2022 ",
    "url": "https://arxiv.org/abs/2203.16616",
    "authors": [
      "Ruwan Wickramarachchi",
      "Cory Henson",
      "Amit Sheth"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2204.08663",
    "title": "Pre-training of Equivariant Graph Matching Networks with Conformation  Flexibility for Drug Binding",
    "abstract": " Title: Pre-training of Equivariant Graph Matching Networks with Conformation  Flexibility for Drug Binding ",
    "url": "https://arxiv.org/abs/2204.08663",
    "authors": [
      "Fang Wu",
      "Yinghui Jiang",
      "Shuting Jin",
      "Xurui Jin",
      "Xiangrong Liu",
      "Zhangming Niu",
      "Qiang Zhang",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2205.02293",
    "title": "Original or Translated? A Causal Analysis of the Impact of  Translationese on Machine Translation Performance",
    "abstract": " Comments: NAACL 2022 (Oral) ",
    "url": "https://arxiv.org/abs/2205.02293",
    "authors": [
      "Jingwei Ni",
      "Zhijing Jin",
      "Markus Freitag",
      "Mrinmaya Sachan",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2205.05343",
    "title": "Learning Multitask Gaussian Bayesian Networks",
    "abstract": " Title: Learning Multitask Gaussian Bayesian Networks ",
    "url": "https://arxiv.org/abs/2205.05343",
    "authors": [
      "Shuai Liu",
      "Yixuan Qiu",
      "Baojuan Li",
      "Huaning Wang",
      "Xiangyu Chang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2205.07547",
    "title": "SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed  Stochastic Quantization",
    "abstract": " Comments: 25 pages with 10 figures, accepted for publication in ICML 2022 (Our code is available at this https URL) ",
    "url": "https://arxiv.org/abs/2205.07547",
    "authors": [
      "Yuhta Takida",
      "Takashi Shibuya",
      "WeiHsiang Liao",
      "Chieh-Hsin Lai",
      "Junki Ohmura",
      "Toshimitsu Uesaka",
      "Naoki Murata",
      "Shusuke Takahashi",
      "Toshiyuki Kumakura",
      "Yuki Mitsufuji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2205.09351",
    "title": "Mip-NeRF RGB-D: Depth Assisted Fast Neural Radiance Fields",
    "abstract": " Title: Mip-NeRF RGB-D: Depth Assisted Fast Neural Radiance Fields ",
    "url": "https://arxiv.org/abs/2205.09351",
    "authors": [
      "Arnab Dey",
      "Yassine Ahmine",
      "Andrew I. Comport"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2205.14014",
    "title": "What Dense Graph Do You Need for Self-Attention?",
    "abstract": " Comments: Accepted by ICML 2022. Code is available at this https URL ",
    "url": "https://arxiv.org/abs/2205.14014",
    "authors": [
      "Yuxing Wang",
      "Chu-Tak Lee",
      "Qipeng Guo",
      "Zhangyue Yin",
      "Yunhua Zhou",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2206.00065",
    "title": "FELARE: Fair Scheduling of Machine Learning Applications on  Heterogeneous Edge Systems",
    "abstract": " Title: FELARE: Fair Scheduling of Machine Learning Applications on  Heterogeneous Edge Systems ",
    "url": "https://arxiv.org/abs/2206.00065",
    "authors": [
      "Ali Mokhtari",
      "Pooyan Jamshidi",
      "Mohsen Amini Salehi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2206.01775",
    "title": "Seamless Interaction Design with Coexistence and Cooperation Modes for  Robust Human-Robot Collaboration",
    "abstract": " Comments: Accepted by CASE 2022 Special Session on Adaptive and Resilient Cyber-Physical Manufacturing Networks ",
    "url": "https://arxiv.org/abs/2206.01775",
    "authors": [
      "Zhe Huang",
      "Ye-Ji Mun",
      "Xiang Li",
      "Yiqing Xie",
      "Ninghan Zhong",
      "Weihang Liang",
      "Junyi Geng",
      "Tan Chen",
      "Katherine Driggs-Campbell"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2206.02284",
    "title": "Tagged-MRI Sequence to Audio Synthesis via Self Residual Attention  Guided Heterogeneous Translator",
    "abstract": " Comments: MICCAI 2022 (early accept) ",
    "url": "https://arxiv.org/abs/2206.02284",
    "authors": [
      "Xiaofeng Liu",
      "Fangxu Xing",
      "Jerry L. Prince",
      "Jiachen Zhuo",
      "Maureen Stone",
      "Georges El Fakhri",
      "Jonghye Woo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2206.02976",
    "title": "Recall Distortion in Neural Network Pruning and the Undecayed Pruning  Algorithm",
    "abstract": " Comments: (Under review.) ",
    "url": "https://arxiv.org/abs/2206.02976",
    "authors": [
      "Aidan Good",
      "Jiaqi Lin",
      "Hannah Sieg",
      "Mikey Ferguson",
      "Xin Yu",
      "Shandian Zhe",
      "Jerzy Wieczorek",
      "Thiago Serra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.03538",
    "title": "WIDESim: A toolkit for simulating resource management techniques of  scientific Workflows In Distributed Environments with graph topology",
    "abstract": " Title: WIDESim: A toolkit for simulating resource management techniques of  scientific Workflows In Distributed Environments with graph topology ",
    "url": "https://arxiv.org/abs/2206.03538",
    "authors": [
      "Mohammad Amin Rayej",
      "Hajar Siar",
      "Mohammad Izadi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2206.03544",
    "title": "A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of  Natural Movies from Brain Activity",
    "abstract": " Title: A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of  Natural Movies from Brain Activity ",
    "url": "https://arxiv.org/abs/2206.03544",
    "authors": [
      "Ganit Kupershmidt",
      "Roman Beliy",
      "Guy Gaziv",
      "Michal Irani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.03581",
    "title": "Compromised account detection using authorship verification: a novel  approach",
    "abstract": " Title: Compromised account detection using authorship verification: a novel  approach ",
    "url": "https://arxiv.org/abs/2206.03581",
    "authors": [
      "Forough Farazmanesh",
      "Fateme Foroutan",
      "Amir Jalaly Bidgoly"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2206.03669",
    "title": "Toward Certified Robustness Against Real-World Distribution Shifts",
    "abstract": " Comments: Under submission ",
    "url": "https://arxiv.org/abs/2206.03669",
    "authors": [
      "Haoze Wu",
      "Teruhiro Tagomori",
      "Alexander Robey",
      "Fengjun Yang",
      "Nikolai Matni",
      "George Pappas",
      "Hamed Hassani",
      "Corina Pasareanu",
      "Clark Barrett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2206.03935",
    "title": "Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays",
    "abstract": " Comments: Early Accepted to MICCAI 2022 ",
    "url": "https://arxiv.org/abs/2206.03935",
    "authors": [
      "Yu Cai",
      "Hao Chen",
      "Xin Yang",
      "Yu Zhou",
      "Kwang-Ting Cheng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  }
]