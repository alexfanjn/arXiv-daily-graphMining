[
  {
    "id": "arXiv:2407.18917",
    "title": "Spatio-temporal Structure of Excitation and Inhibition Emerges in Spiking Neural Networks with and without Biologically Plausible Constraints",
    "abstract": "           We present a Spiking Neural Network (SNN) model that incorporates learnable synaptic delays using Dilated Convolution with Learnable Spacings (DCLS). We train this model on the Raw Heidelberg Digits keyword spotting benchmark using Backpropagation Through Time with surrogate gradients. Analysing the spatio-temporal structure of synaptic interactions in the network we observe that after training excitation and inhibition are grouped together both in space and time. To further enhance the efficiency and biological realism of our model, we implemented a dynamic pruning strategy that combines DEEP R for connection removal and RigL for connection reintroduction, ensuring that the network maintains optimal connectivity throughout training. Additionally, we incorporated Dale's Principle, enforcing each neuron to be exclusively excitatory or inhibitory -- aligning our model closer to biological neural networks. We observed that, after training, the spatio-temporal patterns of excitation and inhibition appeared in the more biologically plausible model as well. Our research demonstrates the potential of integrating learnable delays, dynamic pruning, and biological constraints to develop efficient SNN models for temporal data processing. Furthermore, our results enhance the understanding of spatio-temporal dynamics in SNNs -- suggesting that the spatio-temporal features which emerge from training are robust to both pruning and rewiring processes -- providing a solid foundation for future work in neuromorphic computing applications.         ",
    "url": "https://arxiv.org/abs/2407.18917",
    "authors": [
      "Bal\u00e1zs M\u00e9sz\u00e1ros",
      "James Knight",
      "Thomas Nowotny"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2407.18929",
    "title": "THEA-Code: an Autoencoder-Based IDS-correcting Code for DNA Storage",
    "abstract": "           The insertion, deletion, substitution (IDS) correcting code has garnered increased attention due to significant advancements in DNA storage that emerged recently. Despite this, the pursuit of optimal solutions in IDS-correcting codes remains an open challenge, drawing interest from both theoretical and engineering perspectives. This work introduces a pioneering approach named THEA-code. The proposed method follows a heuristic idea of employing an end-to-end autoencoder for the integrated encoding and decoding processes. To address the challenges associated with deploying an autoencoder as an IDS-correcting code, we propose innovative techniques, including the differentiable IDS channel, the entropy constraint on the codeword, and the auxiliary reconstruction of the source sequence. These strategies contribute to the successful convergence of the autoencoder, resulting in a deep learning-based IDS-correcting code with commendable performance. Notably, THEA-Code represents the first instance of a deep learning-based code that is independent of conventional coding frameworks in the IDS-correcting domain. Comprehensive experiments, including an ablation study, provide a detailed analysis and affirm the effectiveness of THEA-Code.         ",
    "url": "https://arxiv.org/abs/2407.18929",
    "authors": [
      "Alan J.X. Guo",
      "Mengyi Wei",
      "Yufan Dai",
      "Yali Wei",
      "Pengchen Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18931",
    "title": "Multi-dimensional Graph Linear Canonical Transform",
    "abstract": "           Many multi-dimensional (M-D) graph signals appear in the real world, such as digital images, sensor network measurements and temperature records from weather observation stations. It is a key challenge to design a transform method for processing these graph M-D signals in the linear canonical transform domain. This paper proposes the two-dimensional graph linear canonical transform based on the central discrete dilated Hermite function (2-D CDDHFs-GLCT) and the two-dimensional graph linear canonical transform based on chirp multiplication-chirp convolution-chirp multiplication decomposition (2-D CM-CC-CM-GLCT). Then, extending 2-D CDDHFs-GLCT and 2-D CM-CC-CM-GLCT to M-D CDDHFs-GLCT and M-D CM-CC-CM-GLCT. In terms of the computational complexity, additivity and reversibility, M-D CDDHFs-GLCT and M-D CM-CC-CM-GLCT are compared. Theoretical analysis shows that the computational complexity of M-D CM-CC-CM-GLCT algorithm is obviously reduced. Simulation results indicate that M-D CM-CC-CM-GLCT achieves comparable additivity to M-D CDDHFs-GLCT, while M-D CM-CC-CM-GLCT exhibits better reversibility. Finally, M-D GLCT is applied to data compression to show its application advantages. The experimental results reflect the superiority of M-D GLCT in the algorithm design and implementation of data compression.         ",
    "url": "https://arxiv.org/abs/2407.18931",
    "authors": [
      "Na Li",
      "Zhichao Zhang",
      "Jie Han",
      "Yunjie Chen",
      "Chunzheng Cao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.18941",
    "title": "LEMoN: Label Error Detection using Multimodal Neighbors",
    "abstract": "           Large repositories of image-caption pairs are essential for the development of vision-language models. However, these datasets are often extracted from noisy data scraped from the web, and contain many mislabeled examples. In order to improve the reliability of downstream models, it is important to identify and filter images with incorrect captions. However, beyond filtering based on image-caption embedding similarity, no prior works have proposed other methods to filter noisy multimodal data, or concretely assessed the impact of noisy captioning data on downstream training. In this work, we propose LEMoN, a method to automatically identify label errors in multimodal datasets. Our method leverages the multimodal neighborhood of image-caption pairs in the latent space of contrastively pretrained multimodal models. We find that our method outperforms the baselines in label error identification, and that training on datasets filtered using our method improves downstream classification and captioning performance.         ",
    "url": "https://arxiv.org/abs/2407.18941",
    "authors": [
      "Haoran Zhang",
      "Aparna Balagopalan",
      "Nassim Oufattole",
      "Hyewon Jeong",
      "Yan Wu",
      "Jiacheng Zhu",
      "Marzyeh Ghassemi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18970",
    "title": "Region Guided Attention Network for Retinal Vessel Segmentation",
    "abstract": "           Retinal imaging has emerged as a promising method of addressing this challenge, taking advantage of the unique structure of the retina. The retina is an embryonic extension of the central nervous system, providing a direct in vivo window into neurological health. Recent studies have shown that specific structural changes in retinal vessels can not only serve as early indicators of various diseases but also help to understand disease progression. In this work, we present a lightweight retinal vessel segmentation network based on the encoder-decoder mechanism with region-guided attention. We introduce inverse addition attention blocks with region guided attention to focus on the foreground regions and improve the segmentation of regions of interest. To further boost the model's performance on retinal vessel segmentation, we employ a weighted dice loss. This choice is particularly effective in addressing the class imbalance issues frequently encountered in retinal vessel segmentation tasks. Dice loss penalises false positives and false negatives equally, encouraging the model to generate more accurate segmentation with improved object boundary delineation and reduced fragmentation. Extensive experiments on a benchmark dataset show better performance (0.8285, 0.8098, 0.9677, and 0.8166 recall, precision, accuracy and F1 score respectively) compared to state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2407.18970",
    "authors": [
      "Syed Javed",
      "Tariq M. Khan",
      "Abdul Qayyum",
      "Arcot Sowmya",
      "Imran Razzak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.18981",
    "title": "Prompt Injection Attacks on Large Language Models in Oncology",
    "abstract": "           Vision-language artificial intelligence models (VLMs) possess medical knowledge and can be employed in healthcare in numerous ways, including as image interpreters, virtual scribes, and general decision support systems. However, here, we demonstrate that current VLMs applied to medical tasks exhibit a fundamental security flaw: they can be attacked by prompt injection attacks, which can be used to output harmful information just by interacting with the VLM, without any access to its parameters. We performed a quantitative study to evaluate the vulnerabilities to these attacks in four state of the art VLMs which have been proposed to be of utility in healthcare: Claude 3 Opus, Claude 3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N=297 attacks, we show that all of these models are susceptible. Specifically, we show that embedding sub-visual prompts in medical imaging data can cause the model to provide harmful output, and that these prompts are non-obvious to human observers. Thus, our study demonstrates a key vulnerability in medical VLMs which should be mitigated before widespread clinical adoption.         ",
    "url": "https://arxiv.org/abs/2407.18981",
    "authors": [
      "Jan Clusmann",
      "Dyke Ferber",
      "Isabella C. Wiest",
      "Carolin V. Schneider",
      "Titus J. Brinker",
      "Sebastian Foersch",
      "Daniel Truhn",
      "Jakob N. Kather"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18986",
    "title": "TERIME: An improved RIME algorithm with enhanced exploration and exploitation for robust parameter extraction of photovoltaic models",
    "abstract": "           Parameter extraction of photovoltaic (PV) models is crucial for the planning, optimization, and control of PV systems. Although some methods using meta-heuristic algorithms have been proposed to determine these parameters, the robustness of solutions obtained by these methods faces great challenges when the complexity of the PV model increases. The unstable results will affect the reliable operation and maintenance strategies of PV systems. In response to this challenge, an improved RIME algorithm with enhanced exploration and exploitation is proposed for robust and accurate parameter identification for various PV models. Specifically, the differential evolution mutation operator is integrated in the exploration phase to enhance the population diversity. Meanwhile, a new exploitation strategy incorporating randomization and neighborhood strategies simultaneously is developed to maintain the balance of exploitation width and depth. The improved RIME algorithm is applied to estimate the optimal parameters of the single-diode model (SDM), double-diode model (DDM), and triple-diode model (TDM) combined with the Lambert-W function for three PV cell and module types including RTC France, Photo Watt-PWP 201 and S75. According to the statistical analysis in 100 runs, the TEIMRE achieves more accurate and robust parameter estimations than other techniques to various PV models in varying environmental conditions. All of our source codes are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.18986",
    "authors": [
      "Shi-Shun Chen",
      "Yu-Tong Jiang",
      "Wen-Bin Chen",
      "Xiao-Yang Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2407.18999",
    "title": "Graph-based Unsupervised Disentangled Representation Learning via Multimodal Large Language Models",
    "abstract": "           Disentangled representation learning (DRL) aims to identify and decompose underlying factors behind observations, thus facilitating data perception and generation. However, current DRL approaches often rely on the unrealistic assumption that semantic factors are statistically independent. In reality, these factors may exhibit correlations, which off-the-shelf solutions have yet to properly address. To tackle this challenge, we introduce a bidirectional weighted graph-based framework, to learn factorized attributes and their interrelations within complex data. Specifically, we propose a $\\beta$-VAE based module to extract factors as the initial nodes of the graph, and leverage the multimodal large language model (MLLM) to discover and rank latent correlations, thereby updating the weighted edges. By integrating these complementary modules, our model successfully achieves fine-grained, practical and unsupervised disentanglement. Experiments demonstrate our method's superior performance in disentanglement and reconstruction. Furthermore, the model inherits enhanced interpretability and generalizability from MLLMs.         ",
    "url": "https://arxiv.org/abs/2407.18999",
    "authors": [
      "Baao Xie",
      "Qiuyu Chen",
      "Yunnan Wang",
      "Zequn Zhang",
      "Xin Jin",
      "Wenjun Zeng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19031",
    "title": "Artificial neural networks on graded vector spaces",
    "abstract": "           We develop new artificial neural network models for graded vector spaces, which are suitable when different features in the data have different significance (weights). This is the first time that such models are designed mathematically and they are expected to perform better than neural networks over usual vector spaces, which are the special case when the gradings are all 1s.         ",
    "url": "https://arxiv.org/abs/2407.19031",
    "authors": [
      "T. Shaska"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19039",
    "title": "GraphBPE: Molecular Graphs Meet Byte-Pair Encoding",
    "abstract": "           With the increasing attention to molecular machine learning, various innovations have been made in designing better models or proposing more comprehensive benchmarks. However, less is studied on the data preprocessing schedule for molecular graphs, where a different view of the molecular graph could potentially boost the model's performance. Inspired by the Byte-Pair Encoding (BPE) algorithm, a subword tokenization method popularly adopted in Natural Language Processing, we propose GraphBPE, which tokenizes a molecular graph into different substructures and acts as a preprocessing schedule independent of the model architectures. Our experiments on 3 graph-level classification and 3 graph-level regression datasets show that data preprocessing could boost the performance of models for molecular graphs, and GraphBPE is effective for small classification datasets and it performs on par with other tokenization methods across different model architectures.         ",
    "url": "https://arxiv.org/abs/2407.19039",
    "authors": [
      "Yuchen Shen",
      "Barnab\u00e1s P\u00f3czos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Chemical Physics (physics.chem-ph)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2407.19044",
    "title": "Advancing Neural Network Performance through Emergence-Promoting Initialization Scheme",
    "abstract": "           We introduce a novel yet straightforward neural network initialization scheme that modifies conventional methods like Xavier and Kaiming initialization. Inspired by the concept of emergence and leveraging the emergence measures proposed by Li (2023), our method adjusts the layer-wise weight scaling factors to achieve higher emergence values. This enhancement is easy to implement, requiring no additional optimization steps for initialization compared to GradInit. We evaluate our approach across various architectures, including MLP and convolutional architectures for image recognition, and transformers for machine translation. We demonstrate substantial improvements in both model accuracy and training speed, with and without batch normalization. The simplicity, theoretical innovation, and demonstrable empirical advantages of our method make it a potent enhancement to neural network initialization practices. These results suggest a promising direction for leveraging emergence to improve neural network training methodologies. Code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2407.19044",
    "authors": [
      "Johnny Jingze Li",
      "Vivek Kurien George",
      "Gabriel A. Silva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19053",
    "title": "A Study of Using Multimodal LLMs for Non-Crash Functional Bug Detection in Android Apps",
    "abstract": "           Numerous approaches employing various strategies have been developed to test the graphical user interfaces (GUIs) of mobile apps. However, traditional GUI testing techniques, such as random and model-based testing, primarily focus on generating test sequences that excel in achieving high code coverage but often fail to act as effective test oracles for non-crash functional (NCF) bug detection. To tackle these limitations, this study empirically investigates the capability of leveraging large language models (LLMs) to be test oracles to detect NCF bugs in Android apps. Our intuition is that the training corpora of LLMs, encompassing extensive mobile app usage and bug report descriptions, enable them with the domain knowledge relevant to NCF bug detection. We conducted a comprehensive empirical study to explore the effectiveness of LLMs as test oracles for detecting NCF bugs in Android apps on 71 well-documented NCF bugs. The results demonstrated that LLMs achieve a 49% bug detection rate, outperforming existing tools for detecting NCF bugs in Android apps. Additionally, by leveraging LLMs to be test oracles, we successfully detected 24 previously unknown NCF bugs in 64 Android apps, with four of these bugs being confirmed or fixed. However, we also identified limitations of LLMs, primarily related to performance degradation, inherent randomness, and false positives. Our study highlights the potential of leveraging LLMs as test oracles for Android NCF bug detection and suggests directions for future research.         ",
    "url": "https://arxiv.org/abs/2407.19053",
    "authors": [
      "Bangyan Ju",
      "Jin Yang",
      "Tingting Yu",
      "Tamerlan Abdullayev",
      "Yuanyuan Wu",
      "Dingbang Wang",
      "Yu Zhao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2407.19072",
    "title": "Configural processing as an optimized strategy for robust object recognition in neural networks",
    "abstract": "           Configural processing, the perception of spatial relationships among an object's components, is crucial for object recognition. However, the teleology and underlying neurocomputational mechanisms of such processing are still elusive, notwithstanding decades of research. We hypothesized that processing objects via configural cues provides a more robust means to recognizing them relative to local featural cues. We evaluated this hypothesis by devising identification tasks with composite letter stimuli and comparing different neural network models trained with either only local or configural cues available. We found that configural cues yielded more robust performance to geometric transformations such as rotation or scaling. Furthermore, when both features were simultaneously available, configural cues were favored over local featural cues. Layerwise analysis revealed that the sensitivity to configural cues emerged later relative to local feature cues, possibly contributing to the robustness to pixel-level transformations. Notably, this configural processing occurred in a purely feedforward manner, without the need for recurrent computations. Our findings with letter stimuli were successfully extended to naturalistic face images. Thus, our study provides neurocomputational evidence that configural processing emerges in a na\u00efve network based on task contingencies, and is beneficial for robust object processing under varying viewing conditions.         ",
    "url": "https://arxiv.org/abs/2407.19072",
    "authors": [
      "Hojin Jang",
      "Pawan Sinha",
      "Xavier Boix"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19077",
    "title": "Flexible graph convolutional network for 3D human pose estimation",
    "abstract": "           Although graph convolutional networks exhibit promising performance in 3D human pose estimation, their reliance on one-hop neighbors limits their ability to capture high-order dependencies among body joints, crucial for mitigating uncertainty arising from occlusion or depth ambiguity. To tackle this limitation, we introduce Flex-GCN, a flexible graph convolutional network designed to learn graph representations that capture broader global information and dependencies. At its core is the flexible graph convolution, which aggregates features from both immediate and second-order neighbors of each node, while maintaining the same time and memory complexity as the standard convolution. Our network architecture comprises residual blocks of flexible graph convolutional layers, as well as a global response normalization layer for global feature aggregation, normalization and calibration. Quantitative and qualitative results demonstrate the effectiveness of our model, achieving competitive performance on benchmark datasets.         ",
    "url": "https://arxiv.org/abs/2407.19077",
    "authors": [
      "Abu Taib Mohammed Shahjahan",
      "A. Ben Hamza"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19079",
    "title": "UniForensics: Face Forgery Detection via General Facial Representation",
    "abstract": "           Previous deepfake detection methods mostly depend on low-level textural features vulnerable to perturbations and fall short of detecting unseen forgery methods. In contrast, high-level semantic features are less susceptible to perturbations and not limited to forgery-specific artifacts, thus having stronger generalization. Motivated by this, we propose a detection method that utilizes high-level semantic features of faces to identify inconsistencies in temporal domain. We introduce UniForensics, a novel deepfake detection framework that leverages a transformer-based video classification network, initialized with a meta-functional face encoder for enriched facial representation. In this way, we can take advantage of both the powerful spatio-temporal model and the high-level semantic information of faces. Furthermore, to leverage easily accessible real face data and guide the model in focusing on spatio-temporal features, we design a Dynamic Video Self-Blending (DVSB) method to efficiently generate training samples with diverse spatio-temporal forgery traces using real facial videos. Based on this, we advance our framework with a two-stage training approach: The first stage employs a novel self-supervised contrastive learning, where we encourage the network to focus on forgery traces by impelling videos generated by the same forgery process to have similar representations. On the basis of the representation learned in the first stage, the second stage involves fine-tuning on face forgery detection dataset to build a deepfake detector. Extensive experiments validates that UniForensics outperforms existing face forgery methods in generalization ability and robustness. In particular, our method achieves 95.3\\% and 77.2\\% cross dataset AUC on the challenging Celeb-DFv2 and DFDC respectively.         ",
    "url": "https://arxiv.org/abs/2407.19079",
    "authors": [
      "Ziyuan Fang",
      "Hanqing Zhao",
      "Tianyi Wei",
      "Wenbo Zhou",
      "Ming Wan",
      "Zhanyi Wang",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19082",
    "title": "Regularized Multi-Decoder Ensemble for an Error-Aware Scene Representation Network",
    "abstract": "           Feature grid Scene Representation Networks (SRNs) have been applied to scientific data as compact functional surrogates for analysis and visualization. As SRNs are black-box lossy data representations, assessing the prediction quality is critical for scientific visualization applications to ensure that scientists can trust the information being visualized. Currently, existing architectures do not support inference time reconstruction quality assessment, as coordinate-level errors cannot be evaluated in the absence of ground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN) ensemble architecture consisting of a shared feature grid with multiple lightweight multi-layer perceptron decoders. MDSRN can generate a set of plausible predictions for a given input coordinate to compute the mean as the prediction of the multi-decoder ensemble and the variance as a confidence score. The coordinate-level variance can be rendered along with the data to inform the reconstruction quality, or be integrated into uncertainty-aware volume visualization algorithms. To prevent the misalignment between the quantified variance and the prediction quality, we propose a novel variance regularization loss for ensemble learning that promotes the Regularized multi-decoder SRN (RMDSRN) to obtain a more reliable variance that correlates closely to the true model error. We comprehensively evaluate the quality of variance quantification and data reconstruction of Monte Carlo Dropout, Mean Field Variational Inference, Deep Ensemble, and Predicting Variance compared to the proposed MDSRN and RMDSRN across diverse scalar field datasets. We demonstrate that RMDSRN attains the most accurate data reconstruction and competitive variance-error correlation among uncertain SRNs under the same neural network parameter budgets.         ",
    "url": "https://arxiv.org/abs/2407.19082",
    "authors": [
      "Tianyu Xiong",
      "Skylar W. Wurster",
      "Hanqi Guo",
      "Tom Peterka",
      "Han-Wei Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2407.19090",
    "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous Key-Value Stores",
    "abstract": "           Cloud key-value (KV) stores provide businesses with a cost-effective and adaptive alternative to traditional on-premise data management solutions. KV stores frequently consist of heterogeneous clusters, characterized by varying hardware specifications of the deployment nodes, with each node potentially running a distinct version of the KV store software. This heterogeneity is accompanied by the diverse metadata that they need to manage. In this study, we introduce MetaHive, a cache-optimized approach to managing metadata in heterogeneous KV store clusters. MetaHive disaggregates the original data from its associated metadata to promote independence between them, while maintaining their interconnection during usage. This makes the metadata opaque from the downstream processes and the other KV stores in the cluster. MetaHive also ensures that the KV and metadata entries are stored in the vicinity of each other in memory and storage. This allows MetaHive to optimally utilize the caching mechanism without extra storage read overhead for metadata retrieval. We deploy MetaHive to ensure data integrity in RocksDB and demonstrate its rapid data validation with minimal effect on performance.         ",
    "url": "https://arxiv.org/abs/2407.19090",
    "authors": [
      "Alireza Heidari",
      "Amirhossein Ahmadi",
      "Zefeng Zhi",
      "Wei Zhang"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.19097",
    "title": "NARVis: Neural Accelerated Rendering for Real-Time Scientific Point Cloud Visualization",
    "abstract": "           Exploring scientific datasets with billions of samples in real-time visualization presents a challenge - balancing high-fidelity rendering with speed. This work introduces a novel renderer - Neural Accelerated Renderer (NAR), that uses the neural deferred rendering framework to visualize large-scale scientific point cloud data. NAR augments a real-time point cloud rendering pipeline with high-quality neural post-processing, making the approach ideal for interactive visualization at scale. Specifically, we train a neural network to learn the point cloud geometry from a high-performance multi-stream rasterizer and capture the desired postprocessing effects from a conventional high-quality renderer. We demonstrate the effectiveness of NAR by visualizing complex multidimensional Lagrangian flow fields and photometric scans of a large terrain and compare the renderings against the state-of-the-art high-quality renderers. Through extensive evaluation, we demonstrate that NAR prioritizes speed and scalability while retaining high visual fidelity. We achieve competitive frame rates of $>$ 126 fps for interactive rendering of $>$ 350M points (i.e., an effective throughput of $>$ 44 billion points per second) using $\\sim$12 GB of memory on RTX 2080 Ti GPU. Furthermore, we show that NAR is generalizable across different point clouds with similar visualization needs and the desired post-processing effects could be obtained with substantial high quality even at lower resolutions of the original point cloud, further reducing the memory requirements.         ",
    "url": "https://arxiv.org/abs/2407.19097",
    "authors": [
      "Srinidhi Hegde",
      "Kaur Kullman",
      "Thomas Grubb",
      "Leslie Lait",
      "Stephen Guimond",
      "Matthias Zwicker"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19102",
    "title": "The Computational Complexity of Factored Graphs",
    "abstract": "           Computational complexity is traditionally measured with respect to input size. For graphs, this is typically the number of vertices (or edges) of the graph. However, for large graphs even explicitly representing the graph could be prohibitively expensive. Instead, graphs with enough structure could admit more succinct representations. A number of previous works have considered various succinct representations of graphs, such as small circuits [Galperin, Wigderson '83]. We initiate the study of the computational complexity of problems on factored graphs: graphs that are given as a formula of products and union on smaller graphs. For any graph problem, we define a parameterized version by the number of operations used to construct the graph. For different graph problems, we show that the corresponding parameterized problems have a wide range of complexities that are also quite different from most parameterized problems. We give a natural example of a parameterized problem that is unconditionally not fixed parameter tractable (FPT). On the other hand, we show that subgraph counting is FPT. Finally, we show that reachability for factored graphs is FPT if and only if $\\mathbf{NL}$ is in some fixed polynomial time.         ",
    "url": "https://arxiv.org/abs/2407.19102",
    "authors": [
      "Shreya Gupta",
      "Boyang Huang",
      "Russell Impagliazzo",
      "Stanley Woo",
      "Christopher Ye"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2407.19118",
    "title": "Large Language Models as Co-Pilots for Causal Inference in Medical Studies",
    "abstract": "           The validity of medical studies based on real-world clinical data, such as observational studies, depends on critical assumptions necessary for drawing causal conclusions about medical interventions. Many published studies are flawed because they violate these assumptions and entail biases such as residual confounding, selection bias, and misalignment between treatment and measurement times. Although researchers are aware of these pitfalls, they continue to occur because anticipating and addressing them in the context of a specific study can be challenging without a large, often unwieldy, interdisciplinary team with extensive expertise. To address this expertise gap, we explore the use of large language models (LLMs) as co-pilot tools to assist researchers in identifying study design flaws that undermine the validity of causal inferences. We propose a conceptual framework for LLMs as causal co-pilots that encode domain knowledge across various fields, engaging with researchers in natural language interactions to provide contextualized assistance in study design. We provide illustrative examples of how LLMs can function as causal co-pilots, propose a structured framework for their grounding in existing causal inference frameworks, and highlight the unique challenges and opportunities in adapting LLMs for reliable use in epidemiological research.         ",
    "url": "https://arxiv.org/abs/2407.19118",
    "authors": [
      "Ahmed Alaa",
      "Rachael V. Phillips",
      "Emre K\u0131c\u0131man",
      "Laura B. Balzer",
      "Mark van der Laan",
      "Maya Petersen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19119",
    "title": "Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning",
    "abstract": "           Over the last few years, federated learning (FL) has emerged as a prominent method in machine learning, emphasizing privacy preservation by allowing multiple clients to collaboratively build a model while keeping their training data private. Despite this focus on privacy, FL models are susceptible to various attacks, including membership inference attacks (MIAs), posing a serious threat to data confidentiality. In a recent study, Rezaei \\textit{et al.} revealed the existence of an accuracy-privacy trade-off in deep ensembles and proposed a few fusion strategies to overcome it. In this paper, we aim to explore the relationship between deep ensembles and FL. Specifically, we investigate whether confidence-based metrics derived from deep ensembles apply to FL and whether there is a trade-off between accuracy and privacy in FL with respect to MIA. Empirical investigations illustrate a lack of a non-monotonic correlation between the number of clients and the accuracy-privacy trade-off. By experimenting with different numbers of federated clients, datasets, and confidence-metric-based fusion strategies, we identify and analytically justify the clear existence of the accuracy-privacy trade-off.         ",
    "url": "https://arxiv.org/abs/2407.19119",
    "authors": [
      "Sayyed Farid Ahamed",
      "Soumya Banerjee",
      "Sandip Roy",
      "Devin Quinn",
      "Marc Vucovich",
      "Kevin Choi",
      "Abdul Rahman",
      "Alison Hu",
      "Edward Bowen",
      "Sachin Shetty"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.19150",
    "title": "RoSE-Opt: Robust and Efficient Analog Circuit Parameter Optimization with Knowledge-infused Reinforcement Learning",
    "abstract": "           This paper proposes a learning framework, RoSE-Opt, to achieve robust and efficient analog circuit parameter optimization. RoSE-Opt has two important features. First, it incorporates key domain knowledge of analog circuit design, such as circuit topology, couplings between circuit specifications, and variations of process, supply voltage, and temperature, into the learning loop. This strategy facilitates the training of an artificial agent capable of achieving design goals by identifying device parameters that are optimal and robust. Second, it exploits a two-level optimization method, that is, integrating Bayesian optimization (BO) with reinforcement learning (RL) to improve sample efficiency. In particular, BO is used for a coarse yet quick search of an initial starting point for optimization. This sets a solid foundation to efficiently train the RL agent with fewer samples. Experimental evaluations on benchmarking circuits show promising sample efficiency, extraordinary figure-of-merit in terms of design efficiency and design success rate, and Pareto optimality in circuit performance of our framework, compared to previous methods. Furthermore, this work thoroughly studies the performance of different RL optimization algorithms, such as Deep Deterministic Policy Gradients (DDPG) with an off-policy learning mechanism and Proximal Policy Optimization (PPO) with an on-policy learning mechanism. This investigation provides users with guidance on choosing the appropriate RL algorithms to optimize the device parameters of analog circuits. Finally, our study also demonstrates RoSE-Opt's promise in parasitic-aware device optimization for analog circuits. In summary, our work reports a knowledge-infused BO-RL design automation framework for reliable and efficient optimization of analog circuits' device parameters.         ",
    "url": "https://arxiv.org/abs/2407.19150",
    "authors": [
      "Weidong Cao",
      "Jian Gao",
      "Tianrui Ma",
      "Rui Ma",
      "Mouhacine Benosman",
      "Xuan Zhang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2407.19153",
    "title": "A Survey of Malware Detection Using Deep Learning",
    "abstract": "           The problem of malicious software (malware) detection and classification is a complex task, and there is no perfect approach. There is still a lot of work to be done. Unlike most other research areas, standard benchmarks are difficult to find for malware detection. This paper aims to investigate recent advances in malware detection on MacOS, Windows, iOS, Android, and Linux using deep learning (DL) by investigating DL in text and image classification, the use of pre-trained and multi-task learning models for malware detection approaches to obtain high accuracy and which the best approach if we have a standard benchmark dataset. We discuss the issues and the challenges in malware detection using DL classifiers by reviewing the effectiveness of these DL classifiers and their inability to explain their decisions and actions to DL developers presenting the need to use Explainable Machine Learning (XAI) or Interpretable Machine Learning (IML) programs. Additionally, we discuss the impact of adversarial attacks on deep learning models, negatively affecting their generalization capabilities and resulting in poor performance on unseen data. We believe there is a need to train and test the effectiveness and efficiency of the current state-of-the-art deep learning models on different malware datasets. We examine eight popular DL approaches on various datasets. This survey will help researchers develop a general understanding of malware recognition using deep learning.         ",
    "url": "https://arxiv.org/abs/2407.19153",
    "authors": [
      "Ahmed Bensaoud",
      "Jugal Kalita",
      "Mahmoud Bensaoud"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19155",
    "title": "Debiased Graph Poisoning Attack via Contrastive Surrogate Objective",
    "abstract": "           Graph neural networks (GNN) are vulnerable to adversarial attacks, which aim to degrade the performance of GNNs through imperceptible changes on the graph. However, we find that in fact the prevalent meta-gradient-based attacks, which utilizes the gradient of the loss w.r.t the adjacency matrix, are biased towards training nodes. That is, their meta-gradient is determined by a training procedure of the surrogate model, which is solely trained on the training nodes. This bias manifests as an uneven perturbation, connecting two nodes when at least one of them is a labeled node, i.e., training node, while it is unlikely to connect two unlabeled nodes. However, these biased attack approaches are sub-optimal as they do not consider flipping edges between two unlabeled nodes at all. This means that they miss the potential attacked edges between unlabeled nodes that significantly alter the representation of a node. In this paper, we investigate the meta-gradients to uncover the root cause of the uneven perturbations of existing attacks. Based on our analysis, we propose a Meta-gradient-based attack method using contrastive surrogate objective (Metacon), which alleviates the bias in meta-gradient using a new surrogate loss. We conduct extensive experiments to show that Metacon outperforms existing meta gradient-based attack methods through benchmark datasets, while showing that alleviating the bias towards training nodes is effective in attacking the graph structure.         ",
    "url": "https://arxiv.org/abs/2407.19155",
    "authors": [
      "Kanghoon Yoon",
      "Yeonjun In",
      "Namkyeong Lee",
      "Kibum Kim",
      "Chanyoung Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.19156",
    "title": "Robust Multimodal 3D Object Detection via Modality-Agnostic Decoding and Proximity-based Modality Ensemble",
    "abstract": "           Recent advancements in 3D object detection have benefited from multi-modal information from the multi-view cameras and LiDAR sensors. However, the inherent disparities between the modalities pose substantial challenges. We observe that existing multi-modal 3D object detection methods heavily rely on the LiDAR sensor, treating the camera as an auxiliary modality for augmenting semantic details. This often leads to not only underutilization of camera data but also significant performance degradation in scenarios where LiDAR data is unavailable. Additionally, existing fusion methods overlook the detrimental impact of sensor noise induced by environmental changes, on detection performance. In this paper, we propose MEFormer to address the LiDAR over-reliance problem by harnessing critical information for 3D object detection from every available modality while concurrently safeguarding against corrupted signals during the fusion process. Specifically, we introduce Modality Agnostic Decoding (MOAD) that extracts geometric and semantic features with a shared transformer decoder regardless of input modalities and provides promising improvement with a single modality as well as multi-modality. Additionally, our Proximity-based Modality Ensemble (PME) module adaptively utilizes the strengths of each modality depending on the environment while mitigating the effects of a noisy sensor. Our MEFormer achieves state-of-the-art performance of 73.9% NDS and 71.5% mAP in the nuScenes validation set. Extensive analyses validate that our MEFormer improves robustness against challenging conditions such as sensor malfunctions or environmental changes. The source code is available at this https URL ",
    "url": "https://arxiv.org/abs/2407.19156",
    "authors": [
      "Juhan Cha",
      "Minseok Joo",
      "Jihwan Park",
      "Sanghyeok Lee",
      "Injae Kim",
      "Hyunwoo J. Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19160",
    "title": "Decomposing heterogeneous dynamical systems with graph neural networks",
    "abstract": "           Natural physical, chemical, and biological dynamical systems are often complex, with heterogeneous components interacting in diverse ways. We show that graph neural networks can be designed to jointly learn the interaction rules and the structure of the heterogeneity from data alone. The learned latent structure and dynamics can be used to virtually decompose the complex system which is necessary to parameterize and infer the underlying governing equations. We tested the approach with simulation experiments of moving particles and vector fields that interact with each other. While our current aim is to better understand and validate the approach with simulated data, we anticipate it to become a generally applicable tool to uncover the governing rules underlying complex dynamics observed in nature.         ",
    "url": "https://arxiv.org/abs/2407.19160",
    "authors": [
      "C\u00e9dric Allier",
      "Magdalena C. Schneider",
      "Michael Innerberger",
      "Larissa Heinrich",
      "John A. Bogovic",
      "Stephan Saalfeld"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2407.19165",
    "title": "HENNC: Hardware Engine for Artificial Neural Network-based Chaotic Oscillators",
    "abstract": "           This letter introduces a framework for the automatic generation of hardware cores for Artificial Neural Network (ANN)-based chaotic oscillators. The framework trains the model to approximate a chaotic system, then performs design space exploration yielding potential hardware architectures for its implementation. The framework then generates the corresponding synthesizable High-Level Synthesis code and a validation testbench from a selected solution. The hardware design primarily targets FPGAs. The proposed framework offers a rapid hardware design process of candidate architectures superior to manually designed works in terms of hardware cost and throughput. The source code is available on GitHub.         ",
    "url": "https://arxiv.org/abs/2407.19165",
    "authors": [
      "Mobin Vaziri",
      "Shervin Vakili",
      "M. Mehdi Rahimifar",
      "J.M. Pierre Langlois"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2407.19166",
    "title": "Revisit Self-supervised Depth Estimation with Local Structure-from-Motion",
    "abstract": "           Both self-supervised depth estimation and Structure-from-Motion (SfM) recover scene depth from RGB videos. Despite sharing a similar objective, the two approaches are disconnected. Prior works of self-supervision backpropagate losses defined within immediate neighboring frames. Instead of learning-through-loss, this work proposes an alternative scheme by performing local SfM. First, with calibrated RGB or RGB-D images, we employ a depth and correspondence estimator to infer depthmaps and pair-wise correspondence maps. Then, a novel bundle-RANSAC-adjustment algorithm jointly optimizes camera poses and one depth adjustment for each depthmap. Finally, we fix camera poses and employ a NeRF, however, without a neural network, for dense triangulation and geometric verification. Poses, depth adjustments, and triangulated sparse depths are our outputs. For the first time, we show self-supervision within $5$ frames already benefits SoTA supervised depth and correspondence models.         ",
    "url": "https://arxiv.org/abs/2407.19166",
    "authors": [
      "Shengjie Zhu",
      "Xiaoming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19173",
    "title": "FarSSiBERT: A Novel Transformer-based Model for Semantic Similarity Measurement of Persian Social Networks Informal Texts",
    "abstract": "           One fundamental task for NLP is to determine the similarity between two texts and evaluate the extent of their likeness. The previous methods for the Persian language have low accuracy and are unable to comprehend the structure and meaning of texts effectively. Additionally, these methods primarily focus on formal texts, but in real-world applications of text processing, there is a need for robust methods that can handle colloquial texts. This requires algorithms that consider the structure and significance of words based on context, rather than just the frequency of words. The lack of a proper dataset for this task in the Persian language makes it important to develop such algorithms and construct a dataset for Persian text. This paper introduces a new transformer-based model to measure semantic similarity between Persian informal short texts from social networks. In addition, a Persian dataset named FarSSiM has been constructed for this purpose, using real data from social networks and manually annotated and verified by a linguistic expert team. The proposed model involves training a large language model using the BERT architecture from scratch. This model, called FarSSiBERT, is pre-trained on approximately 104 million Persian informal short texts from social networks, making it one of a kind in the Persian language. Moreover, a novel specialized informal language tokenizer is provided that not only performs tokenization on formal texts well but also accurately identifies tokens that other Persian tokenizers are unable to recognize. It has been demonstrated that our proposed model outperforms ParsBERT, laBSE, and multilingual BERT in the Pearson and Spearman's coefficient criteria. Additionally, the pre-trained large language model has great potential for use in other NLP tasks on colloquial text and as a tokenizer for less-known informal words.         ",
    "url": "https://arxiv.org/abs/2407.19173",
    "authors": [
      "Seyed Mojtaba Sadjadi",
      "Zeinab Rajabi",
      "Leila Rabiei",
      "Mohammad-Shahram Moin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19183",
    "title": "Graph Memory Learning: Imitating Lifelong Remembering and Forgetting of Brain Networks",
    "abstract": "           Graph data in real-world scenarios undergo rapid and frequent changes, making it challenging for existing graph models to effectively handle the continuous influx of new data and accommodate data withdrawal requests. The approach to frequently retraining graph models is resource intensive and impractical. To address this pressing challenge, this paper introduces a new concept of graph memory learning. Its core idea is to enable a graph model to selectively remember new knowledge but forget old knowledge. Building on this approach, the paper presents a novel graph memory learning framework - Brain-inspired Graph Memory Learning (BGML), inspired by brain network dynamics and function-structure coupling strategies. BGML incorporates a multi-granular hierarchical progressive learning mechanism rooted in feature graph grain learning to mitigate potential conflict between memorization and forgetting in graph memory learning. This mechanism allows for a comprehensive and multi-level perception of local details within evolving graphs. In addition, to tackle the issue of unreliable structures in newly added incremental information, the paper introduces an information self-assessment ownership mechanism. This mechanism not only facilitates the propagation of incremental information within the model but also effectively preserves the integrity of past experiences. We design five types of graph memory learning tasks: regular, memory, unlearning, data-incremental, and class-incremental to evaluate BGML. Its excellent performance is confirmed through extensive experiments on multiple real-world node classification datasets.         ",
    "url": "https://arxiv.org/abs/2407.19183",
    "authors": [
      "Jiaxing Miao",
      "Liang Hu",
      "Qi Zhang",
      "Longbing Cao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2407.19184",
    "title": "Enhancing Tree Type Detection in Forest Fire Risk Assessment: Multi-Stage Approach and Color Encoding with Forest Fire Risk Evaluation Framework for UAV Imagery",
    "abstract": "           Forest fires pose a significant threat to ecosystems, economies, and human health worldwide. Early detection and assessment of forest fires are crucial for effective management and conservation efforts. Unmanned Aerial Vehicles (UAVs) equipped with advanced computer vision algorithms offer a promising solution for forest fire detection and assessment. In this paper, we optimize an integrated forest fire risk assessment framework using UAVs and multi-stage object detection algorithms. We introduce improvements to our previous framework, including the adoption of Faster R-CNN, Grid R-CNN, Sparse R-CNN, Cascade R-CNN, Dynamic R-CNN, and Libra R-CNN detectors, and explore optimizations such as CBAM for attention enhancement, random erasing for preprocessing, and different color space representations. We evaluate these enhancements through extensive experimentation using aerial image footage from various regions in British Columbia, Canada. Our findings demonstrate the effectiveness of multi-stage detectors and optimizations in improving the accuracy of forest fire risk assessment. This research contributes to the advancement of UAV-based forest fire detection and assessment systems, enhancing their efficiency and effectiveness in supporting sustainable forest management and conservation efforts.         ",
    "url": "https://arxiv.org/abs/2407.19184",
    "authors": [
      "Jinda Zhang",
      "Michal Aibin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19187",
    "title": "Efficiently improving key weather variables forecasting by performing the guided iterative prediction in latent space",
    "abstract": "           Weather forecasting refers to learning evolutionary patterns of some key upper-air and surface variables which is of great significance. Recently, deep learning-based methods have been increasingly applied in the field of weather forecasting due to their powerful feature learning capabilities. However, prediction methods based on the original space iteration struggle to effectively and efficiently utilize large number of weather variables. Therefore, we propose an 'encoding-prediction-decoding' prediction network. This network can efficiently benefit to more related input variables with key variables, that is, it can adaptively extract key variable-related low-dimensional latent feature from much more input atmospheric variables for iterative prediction. And we construct a loss function to guide the iteration of latent feature by utilizing multiple atmospheric variables in corresponding lead times. The obtained latent features through iterative prediction are then decoded to obtain the predicted values of key variables in multiple lead times. In addition, we improve the HTA algorithm in \\cite{bi2023accurate} by inputting more time steps to enhance the temporal correlation between the prediction results and input variables. Both qualitative and quantitative prediction results on ERA5 dataset validate the superiority of our method over other methods. (The code will be available at this https URL)         ",
    "url": "https://arxiv.org/abs/2407.19187",
    "authors": [
      "Shuangliang Li",
      "Siwei Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2407.19192",
    "title": "Harmfully Manipulated Images Matter in Multimodal Misinformation Detection",
    "abstract": "           Nowadays, misinformation is widely spreading over various social media platforms and causes extremely negative impacts on society. To combat this issue, automatically identifying misinformation, especially those containing multimodal content, has attracted growing attention from the academic and industrial communities, and induced an active research topic named Multimodal Misinformation Detection (MMD). Typically, existing MMD methods capture the semantic correlation and inconsistency between multiple modalities, but neglect some potential clues in multimodal content. Recent studies suggest that manipulated traces of the images in articles are non-trivial clues for detecting misinformation. Meanwhile, we find that the underlying intentions behind the manipulation, e.g., harmful and harmless, also matter in MMD. Accordingly, in this work, we propose to detect misinformation by learning manipulation features that indicate whether the image has been manipulated, as well as intention features regarding the harmful and harmless intentions of the manipulation. Unfortunately, the manipulation and intention labels that make these features discriminative are unknown. To overcome the problem, we propose two weakly supervised signals as alternatives by introducing additional datasets on image manipulation detection and formulating two classification tasks as positive and unlabeled learning problems. Based on these ideas, we propose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD (HAMI-M3D). Extensive experiments across three benchmark datasets can demonstrate that HAMI-M3D can consistently improve the performance of any MMD baselines.         ",
    "url": "https://arxiv.org/abs/2407.19192",
    "authors": [
      "Bing Wang",
      "Shengsheng Wang",
      "Changchun Li",
      "Renchu Guan",
      "Ximing Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2407.19201",
    "title": "Long Range Switching Time Series Prediction via State Space Model",
    "abstract": "           In this study, we delve into the Structured State Space Model (S4), Change Point Detection methodologies, and the Switching Non-linear Dynamics System (SNLDS). Our central proposition is an enhanced inference technique and long-range dependency method for SNLDS. The cornerstone of our approach is the fusion of S4 and SNLDS, leveraging the strengths of both models to effectively address the intricacies of long-range dependencies in switching time series. Through rigorous testing, we demonstrate that our proposed methodology adeptly segments and reproduces long-range dependencies in both the 1-D Lorenz dataset and the 2-D bouncing ball dataset. Notably, our integrated approach outperforms the standalone SNLDS in these tasks.         ",
    "url": "https://arxiv.org/abs/2407.19201",
    "authors": [
      "Jiaming Zhang",
      "Yang Ding",
      "Yunfeng Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19203",
    "title": "Towards Clean-Label Backdoor Attacks in the Physical World",
    "abstract": "           Deep Neural Networks (DNNs) are vulnerable to backdoor poisoning attacks, with most research focusing on digital triggers, special patterns digitally added to test-time inputs to induce targeted misclassification. In contrast, physical triggers, which are natural objects within a physical scene, have emerged as a desirable alternative since they enable real-time backdoor activations without digital manipulation. However, current physical attacks require that poisoned inputs have incorrect labels, making them easily detectable upon human inspection. In this paper, we collect a facial dataset of 21,238 images with 7 common accessories as triggers and use it to study the threat of clean-label backdoor attacks in the physical world. Our study reveals two findings. First, the success of physical attacks depends on the poisoning algorithm, physical trigger, and the pair of source-target classes. Second, although clean-label poisoned samples preserve ground-truth labels, their perceptual quality could be seriously degraded due to conspicuous artifacts in the images. Such samples are also vulnerable to statistical filtering methods because they deviate from the distribution of clean samples in the feature space. To address these issues, we propose replacing the standard $\\ell_\\infty$ regularization with a novel pixel regularization and feature regularization that could enhance the imperceptibility of poisoned samples without compromising attack performance. Our study highlights accidental backdoor activations as a key limitation of clean-label physical backdoor attacks. This happens when unintended objects or classes accidentally cause the model to misclassify as the target class.         ",
    "url": "https://arxiv.org/abs/2407.19203",
    "authors": [
      "Thinh Dao",
      "Cuong Chi Le",
      "Khoa D Doan",
      "Kok-Seng Wong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19216",
    "title": "EaTVul: ChatGPT-based Evasion Attack Against Software Vulnerability Detection",
    "abstract": "           Recently, deep learning has demonstrated promising results in enhancing the accuracy of vulnerability detection and identifying vulnerabilities in software. However, these techniques are still vulnerable to attacks. Adversarial examples can exploit vulnerabilities within deep neural networks, posing a significant threat to system security. This study showcases the susceptibility of deep learning models to adversarial attacks, which can achieve 100% attack success rate (refer to Table 5). The proposed method, EaTVul, encompasses six stages: identification of important samples using support vector machines, identification of important features using the attention mechanism, generation of adversarial data based on these features using ChatGPT, preparation of an adversarial attack pool, selection of seed data using a fuzzy genetic algorithm, and the execution of an evasion attack. Extensive experiments demonstrate the effectiveness of EaTVul, achieving an attack success rate of more than 83% when the snippet size is greater than 2. Furthermore, in most cases with a snippet size of 4, EaTVul achieves a 100% attack success rate. The findings of this research emphasize the necessity of robust defenses against adversarial attacks in software vulnerability detection.         ",
    "url": "https://arxiv.org/abs/2407.19216",
    "authors": [
      "Shigang Liu",
      "Di Cao",
      "Junae Kim",
      "Tamas Abraham",
      "Paul Montague",
      "Seyit Camtepe",
      "Jun Zhang",
      "Yang Xiang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2407.19224",
    "title": "RAVSS: Robust Audio-Visual Speech Separation in Multi-Speaker Scenarios with Missing Visual Cues",
    "abstract": "           While existing Audio-Visual Speech Separation (AVSS) methods primarily concentrate on the audio-visual fusion strategy for two-speaker separation, they demonstrate a severe performance drop in the multi-speaker separation scenarios. Typically, AVSS methods employ guiding videos to sequentially isolate individual speakers from the given audio mixture, resulting in notable missing and noisy parts across various segments of the separated speech. In this study, we propose a simultaneous multi-speaker separation framework that can facilitate the concurrent separation of multiple speakers within a singular process. We introduce speaker-wise interactions to establish distinctions and correlations among speakers. Experimental results on the VoxCeleb2 and LRS3 datasets demonstrate that our method achieves state-of-the-art performance in separating mixtures with 2, 3, 4, and 5 speakers, respectively. Additionally, our model can utilize speakers with complete audio-visual information to mitigate other visual-deficient speakers, thereby enhancing its resilience to missing visual cues. We also conduct experiments where visual information for specific speakers is entirely absent or visual frames are partially missing. The results demonstrate that our model consistently outperforms others, exhibiting the smallest performance drop across all settings involving 2, 3, 4, and 5 speakers.         ",
    "url": "https://arxiv.org/abs/2407.19224",
    "authors": [
      "Tianrui Pan",
      "Jie Liu",
      "Bohan Wang",
      "Jie Tang",
      "Gangshan Wu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2407.19253",
    "title": "Taylor-Expansion-Based Robust Power Flow in Unbalanced Distribution Systems: A Hybrid Data-Aided Method",
    "abstract": "           Traditional power flow methods often adopt certain assumptions designed for passive balanced distribution systems, thus lacking practicality for unbalanced operation. Moreover, their computation accuracy and efficiency are heavily subject to unknown errors and bad data in measurements or prediction data of distributed energy resources (DERs). To address these issues, this paper proposes a hybrid data-aided robust power flow algorithm in unbalanced distribution systems, which combines Taylor series expansion knowledge with a data-driven regression technique. The proposed method initiates a linearization power flow model to derive an explicitly analytical solution by modified Taylor expansion. To mitigate the approximation loss that surges due to the DER integration and bad data, we further develop a data-aided robust support vector regression approach to estimate the errors efficiently. Comparative analysis in the 13-bus and 123-bus IEEE unbalanced feeders shows that the proposed algorithm achieves superior computational efficiency, with guaranteed accuracy and robustness against outliers.         ",
    "url": "https://arxiv.org/abs/2407.19253",
    "authors": [
      "Sungjoo Chung",
      "Ying Zhang",
      "Zhaoyu Wang",
      "Fei Ding"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2407.19258",
    "title": "Comprehensive Survey of Complex-Valued Neural Networks: Insights into Backpropagation and Activation Functions",
    "abstract": "           Artificial neural networks (ANNs), particularly those employing deep learning models, have found widespread application in fields such as computer vision, signal processing, and wireless communications, where complex numbers are crucial. Despite the prevailing use of real-number implementations in current ANN frameworks, there is a growing interest in developing ANNs that utilize complex numbers. This paper presents a comprehensive survey of recent advancements in complex-valued neural networks (CVNNs), focusing on their activation functions (AFs) and learning algorithms. We delve into the extension of the backpropagation algorithm to the complex domain, which enables the training of neural networks with complex-valued inputs, weights, AFs, and outputs. This survey considers three complex backpropagation algorithms: the complex derivative approach, the partial derivatives approach, and algorithms incorporating the Cauchy-Riemann equations. A significant challenge in CVNN design is the identification of suitable nonlinear Complex Valued Activation Functions (CVAFs), due to the conflict between boundedness and differentiability over the entire complex plane as stated by Liouville theorem. We examine both fully complex AFs, which strive for boundedness and differentiability, and split AFs, which offer a practical compromise despite not preserving analyticity. This review provides an in-depth analysis of various CVAFs essential for constructing effective CVNNs. Moreover, this survey not only offers a comprehensive overview of the current state of CVNNs but also contributes to ongoing research and development by introducing a new set of CVAFs (fully complex, split and complex amplitude-phase AFs).         ",
    "url": "https://arxiv.org/abs/2407.19258",
    "authors": [
      "M. M. Hammad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19259",
    "title": "Fine-Grained Scene Graph Generation via Sample-Level Bias Prediction",
    "abstract": "           Scene Graph Generation (SGG) aims to explore the relationships between objects in images and obtain scene summary graphs, thereby better serving downstream tasks. However, the long-tailed problem has adversely affected the scene graph's quality. The predictions are dominated by coarse-grained relationships, lacking more informative fine-grained ones. The union region of one object pair (i.e., one sample) contains rich and dedicated contextual information, enabling the prediction of the sample-specific bias for refining the original relationship prediction. Therefore, we propose a novel Sample-Level Bias Prediction (SBP) method for fine-grained SGG (SBG). Firstly, we train a classic SGG model and construct a correction bias set by calculating the margin between the ground truth label and the predicted label with one classic SGG model. Then, we devise a Bias-Oriented Generative Adversarial Network (BGAN) that learns to predict the constructed correction biases, which can be utilized to correct the original predictions from coarse-grained relationships to fine-grained ones. The extensive experimental results on VG, GQA, and VG-1800 datasets demonstrate that our SBG outperforms the state-of-the-art methods in terms of Average@K across three mainstream SGG models: Motif, VCtree, and Transformer. Compared to dataset-level correction methods on VG, SBG shows a significant average improvement of 5.6%, 3.9%, and 3.2% on Average@K for tasks PredCls, SGCls, and SGDet, respectively. The code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.19259",
    "authors": [
      "Yansheng Li",
      "Tingzhu Wang",
      "Kang Wu",
      "Linlin Wang",
      "Xin Guo",
      "Wenbin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19265",
    "title": "Towards Robust Few-shot Class Incremental Learning in Audio Classification using Contrastive Representation",
    "abstract": "           In machine learning applications, gradual data ingress is common, especially in audio processing where incremental learning is vital for real-time analytics. Few-shot class-incremental learning addresses challenges arising from limited incoming data. Existing methods often integrate additional trainable components or rely on a fixed embedding extractor post-training on base sessions to mitigate concerns related to catastrophic forgetting and the dangers of model overfitting. However, using cross-entropy loss alone during base session training is suboptimal for audio data. To address this, we propose incorporating supervised contrastive learning to refine the representation space, enhancing discriminative power and leading to better generalization since it facilitates seamless integration of incremental classes, upon arrival. Experimental results on NSynth and LibriSpeech datasets with 100 classes, as well as ESC dataset with 50 and 10 classes, demonstrate state-of-the-art performance.         ",
    "url": "https://arxiv.org/abs/2407.19265",
    "authors": [
      "Riyansha Singh",
      "Parinita Nema",
      "Vinod K Kurmi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2407.19271",
    "title": "Sewer Image Super-Resolution with Depth Priors and Its Lightweight Network",
    "abstract": "           The Quick-view (QV) technique serves as a primary method for detecting defects within sewerage systems. However, the effectiveness of QV is impeded by the limited visual range of its hardware, resulting in suboptimal image quality for distant portions of the sewer network. Image super-resolution is an effective way to improve image quality and has been applied in a variety of scenes. However, research on super-resolution for sewer images remains considerably unexplored. In response, this study leverages the inherent depth relationships present within QV images and introduces a novel Depth-guided, Reference-based Super-Resolution framework denoted as DSRNet. It comprises two core components: a depth extraction module and a depth information matching module (DMM). DSRNet utilizes the adjacent frames of the low-resolution image as reference images and helps them recover texture information based on the correlation. By combining these modules, the integration of depth priors significantly enhances both visual quality and performance benchmarks. Besides, in pursuit of computational efficiency and compactness, our paper introduces a super-resolution knowledge distillation model based on an attention mechanism. This mechanism facilitates the acquisition of feature similarity between a more complex teacher model and a streamlined student model, the latter being a lightweight version of DSRNet. Experimental results demonstrate that DSRNet significantly improves PSNR and SSIM compared with other methods. This study also conducts experiments on sewer defect semantic segmentation, object detection, and classification on the Pipe dataset and Sewer-ML dataset. Experiments show that the method can improve the performance of low-resolution sewer images in these tasks.         ",
    "url": "https://arxiv.org/abs/2407.19271",
    "authors": [
      "Gang Pan",
      "Chen Wang",
      "Zhijie Sui",
      "Shuai Guo",
      "Yaozhi Lv",
      "Honglie Li",
      "Di Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2407.19275",
    "title": "Forms of representation of interpolation trigonometric splines",
    "abstract": "           Three forms of representation of trigonometric interpolation splines are considered, in particular, the representation by the coefficients of the interpolation trigonometric polynomial, the representation by trigonometric B-splines, which are considered in more detail, and the representation by fundamental trigonometric splines. The first and third forms of representation are generalized to the case of non-periodic functions.         ",
    "url": "https://arxiv.org/abs/2407.19275",
    "authors": [
      "Volodymyr Denysiuk",
      "Lyudmila Rybachuk"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2407.19288",
    "title": "SignedLouvain: Louvain for signed networks",
    "abstract": "           In this article, we consider the problem of community detection in signed networks. We propose SignedLouvain, an adaptation of the Louvain method to maximise signed modularity, efficiently taking advantage of the structure induced by signed relations. We begin by identifying the inherent limitations of applying the standard Louvain algorithm to signed networks, before introducing a novel variant specifically engineered to overcome these challenges. Through extensive experiments on real-world datasets, we demonstrate that the proposed method not only maintains the speed and scalability of its predecessor but also significantly enhances accuracy in detecting communities within signed networks.         ",
    "url": "https://arxiv.org/abs/2407.19288",
    "authors": [
      "John N. Pougu\u00e9-Biyong",
      "Renaud Lambiotte"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2407.19310",
    "title": "Ensembling convolutional neural networks for human skin segmentation",
    "abstract": "           Detecting and segmenting human skin regions in digital images is an intensively explored topic of computer vision with a variety of approaches proposed over the years that have been found useful in numerous practical applications. The first methods were based on pixel-wise skin color modeling and they were later enhanced with context-based analysis to include the textural and geometrical features, recently extracted using deep convolutional neural networks. It has been also demonstrated that skin regions can be segmented from grayscale images without using color information at all. However, the possibility to combine these two sources of information has not been explored so far and we address this research gap with the contribution reported in this paper. We propose to train a convolutional network using the datasets focused on different features to create an ensemble whose individual outcomes are effectively combined using yet another convolutional network trained to produce the final segmentation map. The experimental results clearly indicate that the proposed approach outperforms the basic classifiers, as well as an ensemble based on the voting scheme. We expect that this study will help in developing new ensemble-based techniques that will improve the performance of semantic segmentation systems, reaching beyond the problem of detecting human skin.         ",
    "url": "https://arxiv.org/abs/2407.19310",
    "authors": [
      "Patryk Kuban",
      "Michal Kawulok"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19311",
    "title": "Can Modifying Data Address Graph Domain Adaptation?",
    "abstract": "           Graph neural networks (GNNs) have demonstrated remarkable success in numerous graph analytical tasks. Yet, their effectiveness is often compromised in real-world scenarios due to distribution shifts, limiting their capacity for knowledge transfer across changing environments or domains. Recently, Unsupervised Graph Domain Adaptation (UGDA) has been introduced to resolve this issue. UGDA aims to facilitate knowledge transfer from a labeled source graph to an unlabeled target graph. Current UGDA efforts primarily focus on model-centric methods, such as employing domain invariant learning strategies and designing model architectures. However, our critical examination reveals the limitations inherent to these model-centric methods, while a data-centric method allowed to modify the source graph provably demonstrates considerable potential. This insight motivates us to explore UGDA from a data-centric perspective. By revisiting the theoretical generalization bound for UGDA, we identify two data-centric principles for UGDA: alignment principle and rescaling principle. Guided by these principles, we propose GraphAlign, a novel UGDA method that generates a small yet transferable graph. By exclusively training a GNN on this new graph with classic Empirical Risk Minimization (ERM), GraphAlign attains exceptional performance on the target graph. Extensive experiments under various transfer scenarios demonstrate the GraphAlign outperforms the best baselines by an average of 2.16%, training on the generated graph as small as 0.25~1% of the original training graph.         ",
    "url": "https://arxiv.org/abs/2407.19311",
    "authors": [
      "Renhong Huang",
      "Jiarong Xu",
      "Xin Jiang",
      "Ruichuan An",
      "Yang Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2407.19324",
    "title": "Deep Learning Based Crime Prediction Models: Experiments and Analysis",
    "abstract": "           Crime prediction is a widely studied research problem due to its importance in ensuring safety of city dwellers. Starting from statistical and classical machine learning based crime prediction methods, in recent years researchers have focused on exploiting deep learning based models for crime prediction. Deep learning based crime prediction models use complex architectures to capture the latent features in the crime data, and outperform the statistical and classical machine learning based crime prediction methods. However, there is a significant research gap in existing research on the applicability of different models in different real-life scenarios as no longitudinal study exists comparing all these approaches in a unified setting. In this paper, we conduct a comprehensive experimental evaluation of all major state-of-the-art deep learning based crime prediction models. Our evaluation provides several key insights on the pros and cons of these models, which enables us to select the most suitable models for different application scenarios. Based on the findings, we further recommend certain design practices that should be taken into account while building future deep learning based crime prediction models.         ",
    "url": "https://arxiv.org/abs/2407.19324",
    "authors": [
      "Rittik Basak Utsha",
      "Muhtasim Noor Alif",
      "Yeasir Rayhan",
      "Tanzima Hashem",
      "Mohammad Eunus Ali"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2407.19326",
    "title": "Accounting for plasticity: An extension of inelastic Constitutive Artificial Neural Networks",
    "abstract": "           The class of Constitutive Artificial Neural Networks (CANNs) represents a new approach of neural networks in the field of constitutive modeling. So far, CANNs have proven to be a powerful tool in predicting elastic and inelastic material behavior. However, the specification of inelastic constitutive artificial neural networks (iCANNs) to capture plasticity remains to be discussed. We present the extension and application of an iCANN to the inelastic phenomena of plasticity. This includes the prediction of a formulation for the elastic and plastic Helmholtz free energies, the inelastic flow rule, and the yield condition that defines the onset of plasticity. Thus, we learn four feed-forward networks in combination with a recurrent neural network and use the second Piola-Kirchhoff stress measure for training. The presented formulation captures both, associative and non-associative plasticity. In addition, the formulation includes kinematic hardening effects by introducing the plastic Helmholtz free energy. This opens the range of application to a wider class of materials. The capabilities of the presented framework are demonstrated by training on artificially generated data of models for perfect plasticity of von-Mises type, tension-compression asymmetry, and kinematic hardening. We observe already satisfactory results for training on one load case only while extremely precise agreement is found for an increase in load cases. In addition, the performance of the specified iCANN was validated using experimental data of X10CrMoVNb9-1 steel. Training has been performed on both, uniaxial tension and cyclic loading, separately and the predicted results are then validated on the opposing set. The results underline that the autonomously discovered material model is capable to describe and predict the underlying experimental data.         ",
    "url": "https://arxiv.org/abs/2407.19326",
    "authors": [
      "Birte Boes",
      "Jaan-Willem Simon",
      "Hagen Holthusen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19332",
    "title": "A Semi-supervised Fake News Detection using Sentiment Encoding and LSTM with Self-Attention",
    "abstract": "           Micro-blogs and cyber-space social networks are the main communication mediums to receive and share news nowadays. As a side effect, however, the networks can disseminate fake news that harms individuals and the society. Several methods have been developed to detect fake news, but the majority require large sets of manually labeled data to attain the application-level accuracy. Due to the strict privacy policies, the required data are often inaccessible or limited to some specific topics. On the other side, quite diverse and abundant unlabeled data on social media suggests that with a few labeled data, the problem of detecting fake news could be tackled via semi-supervised learning. Here, we propose a semi-supervised self-learning method in which a sentiment analysis is acquired by some state-of-the-art pretrained models. Our learning model is trained in a semi-supervised fashion and incorporates LSTM with self-attention layers. We benchmark our model on a dataset with 20,000 news content along with their feedback, which shows better performance in precision, recall, and measures compared to competitive methods in fake news detection.         ",
    "url": "https://arxiv.org/abs/2407.19332",
    "authors": [
      "Pouya Shaeri",
      "Ali Katanforoush"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19338",
    "title": "Semantic Communication Enhanced by Knowledge Graph Representation Learning",
    "abstract": "           This paper investigates the advantages of representing and processing semantic knowledge extracted into graphs within the emerging paradigm of semantic communications. The proposed approach leverages semantic and pragmatic aspects, incorporating recent advances on large language models (LLMs) to achieve compact representations of knowledge to be processed and exchanged between intelligent agents. This is accomplished by using the cascade of LLMs and graph neural networks (GNNs) as semantic encoders, where information to be shared is selected to be meaningful at the receiver. The embedding vectors produced by the proposed semantic encoder represent information in the form of triplets: nodes (semantic concepts entities), edges(relations between concepts), nodes. Thus, semantic information is associated with the representation of relationships among elements in the space of semantic concept abstractions. In this paper, we investigate the potential of achieving high compression rates in communication by incorporating relations that link elements within graph embeddings. We propose sending semantic symbols solely equivalent to node embeddings through the wireless channel and inferring the complete knowledge graph at the receiver. Numerical simulations illustrate the effectiveness of leveraging knowledge graphs to semantically compress and transmit information.         ",
    "url": "https://arxiv.org/abs/2407.19338",
    "authors": [
      "Nour Hello",
      "Paolo Di Lorenzo",
      "Emilio Calvanese Strinati"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19354",
    "title": "The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies",
    "abstract": "           Inspired by the rapid development of Large Language Models (LLMs), LLM agents have evolved to perform complex tasks. LLM agents are now extensively applied across various domains, handling vast amounts of data to interact with humans and execute tasks. The widespread applications of LLM agents demonstrate their significant commercial value; however, they also expose security and privacy vulnerabilities. At the current stage, comprehensive research on the security and privacy of LLM agents is highly needed. This survey aims to provide a comprehensive overview of the newly emerged privacy and security issues faced by LLM agents. We begin by introducing the fundamental knowledge of LLM agents, followed by a categorization and analysis of the threats. We then discuss the impacts of these threats on humans, environment, and other agents. Subsequently, we review existing defensive strategies, and finally explore future trends. Additionally, the survey incorporates diverse case studies to facilitate a more accessible understanding. By highlighting these critical security and privacy issues, the survey seeks to stimulate future research towards enhancing the security and privacy of LLM agents, thereby increasing their reliability and trustworthiness in future applications.         ",
    "url": "https://arxiv.org/abs/2407.19354",
    "authors": [
      "Feng He",
      "Tianqing Zhu",
      "Dayong Ye",
      "Bo Liu",
      "Wanlei Zhou",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.19359",
    "title": "Learning to Select the Best Forecasting Tasks for Clinical Outcome Prediction",
    "abstract": "           We propose to meta-learn an a self-supervised patient trajectory forecast learning rule by meta-training on a meta-objective that directly optimizes the utility of the patient representation over the subsequent clinical outcome prediction. This meta-objective directly targets the usefulness of a representation generated from unlabeled clinical measurement forecast for later supervised tasks. The meta-learned can then be directly used in target risk prediction, and the limited available samples can be used for further fine-tuning the model performance. The effectiveness of our approach is tested on a real open source patient EHR dataset MIMIC-III. We are able to demonstrate that our attention-based patient state representation approach can achieve much better performance for predicting target risk with low resources comparing with both direct supervised learning and pretraining with all-observation trajectory forecast.         ",
    "url": "https://arxiv.org/abs/2407.19359",
    "authors": [
      "Yuan Xue",
      "Nan Du",
      "Anne Mottram",
      "Martin Seneviratne",
      "Andrew M. Dai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19364",
    "title": "Defogger: A Visual Analysis Approach for Data Exploration of Sensitive Data Protected by Differential Privacy",
    "abstract": "           Differential privacy ensures the security of individual privacy but poses challenges to data exploration processes because the limited privacy budget incapacitates the flexibility of exploration and the noisy feedback of data requests leads to confusing uncertainty. In this study, we take the lead in describing corresponding exploration scenarios, including underlying requirements and available exploration strategies. To facilitate practical applications, we propose a visual analysis approach to the formulation of exploration strategies. Our approach applies a reinforcement learning model to provide diverse suggestions for exploration strategies according to the exploration intent of users. A novel visual design for representing uncertainty in correlation patterns is integrated into our prototype system to support the proposed approach. Finally, we implemented a user study and two case studies. The results of these studies verified that our approach can help develop strategies that satisfy the exploration intent of users.         ",
    "url": "https://arxiv.org/abs/2407.19364",
    "authors": [
      "Xumeng Wang",
      "Shuangcheng Jiao",
      "Chris Bryan"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.19376",
    "title": "CIDER: Counterfactual-Invariant Diffusion-based GNN Explainer for Causal Subgraph Inference",
    "abstract": "           Inferring causal links or subgraphs corresponding to a specific phenotype or label based solely on measured data is an important yet challenging task, which is also different from inferring causal nodes. While Graph Neural Network (GNN) Explainers have shown potential in subgraph identification, existing methods with GNN often offer associative rather than causal insights. This lack of transparency and explainability hinders our understanding of their results and also underlying mechanisms. To address this issue, we propose a novel method of causal link/subgraph inference, called CIDER: Counterfactual-Invariant Diffusion-based GNN ExplaineR, by implementing both counterfactual and diffusion implementations. In other words, it is a model-agnostic and task-agnostic framework for generating causal explanations based on a counterfactual-invariant and diffusion process, which provides not only causal subgraphs due to counterfactual implementation but reliable causal links due to the diffusion process. Specifically, CIDER is first formulated as an inference task that generatively provides the two distributions of one causal subgraph and another spurious subgraph. Then, to enhance the reliability, we further model the CIDER framework as a diffusion process. Thus, using the causal subgraph distribution, we can explicitly quantify the contribution of each subgraph to a phenotype/label in a counterfactual manner, representing each subgraph's causal strength. From a causality perspective, CIDER is an interventional causal method, different from traditional association studies or observational causal approaches, and can also reduce the effects of unobserved confounders. We evaluate CIDER on both synthetic and real-world datasets, which all demonstrate the superiority of CIDER over state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2407.19376",
    "authors": [
      "Qibin Zhang",
      "Chengshang Lyu",
      "Lingxi Chen",
      "Qiqi Jin",
      "Luonan Chen"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2407.19397",
    "title": "Domain Adaptive Lung Nodule Detection in X-ray Image",
    "abstract": "           Medical images from different healthcare centers exhibit varied data distributions, posing significant challenges for adapting lung nodule detection due to the domain shift between training and application phases. Traditional unsupervised domain adaptive detection methods often struggle with this shift, leading to suboptimal outcomes. To overcome these challenges, we introduce a novel domain adaptive approach for lung nodule detection that leverages mean teacher self-training and contrastive learning. First, we propose a hierarchical contrastive learning strategy to refine nodule representations and enhance the distinction between nodules and background. Second, we introduce a nodule-level domain-invariant feature learning (NDL) module to capture domain-invariant features through adversarial learning across different domains. Additionally, we propose a new annotated dataset of X-ray images to aid in advancing lung nodule detection research. Extensive experiments conducted on multiple X-ray datasets demonstrate the efficacy of our approach in mitigating domain shift impacts.         ",
    "url": "https://arxiv.org/abs/2407.19397",
    "authors": [
      "Haifeng Zhao",
      "Lixiang Jiang",
      "Leilei Ma",
      "Dengdi Sun",
      "Yanping Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19398",
    "title": "IDEA: A Flexible Framework of Certified Unlearning for Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNNs) have been increasingly deployed in a plethora of applications. However, the graph data used for training may contain sensitive personal information of the involved individuals. Once trained, GNNs typically encode such information in their learnable parameters. As a consequence, privacy leakage may happen when the trained GNNs are deployed and exposed to potential attackers. Facing such a threat, machine unlearning for GNNs has become an emerging technique that aims to remove certain personal information from a trained GNN. Among these techniques, certified unlearning stands out, as it provides a solid theoretical guarantee of the information removal effectiveness. Nevertheless, most of the existing certified unlearning methods for GNNs are only designed to handle node and edge unlearning requests. Meanwhile, these approaches are usually tailored for either a specific design of GNN or a specially designed training objective. These disadvantages significantly jeopardize their flexibility. In this paper, we propose a principled framework named IDEA to achieve flexible and certified unlearning for GNNs. Specifically, we first instantiate four types of unlearning requests on graphs, and then we propose an approximation approach to flexibly handle these unlearning requests over diverse GNNs. We further provide theoretical guarantee of the effectiveness for the proposed approach as a certification. Different from existing alternatives, IDEA is not designed for any specific GNNs or optimization objectives to perform certified unlearning, and thus can be easily generalized. Extensive experiments on real-world datasets demonstrate the superiority of IDEA in multiple key perspectives.         ",
    "url": "https://arxiv.org/abs/2407.19398",
    "authors": [
      "Yushun Dong",
      "Binchi Zhang",
      "Zhenyu Lei",
      "Na Zou",
      "Jundong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19401",
    "title": "Complete Security and Privacy for AI Inference in Decentralized Systems",
    "abstract": "           The need for data security and model integrity has been accentuated by the rapid adoption of AI and ML in data-driven domains including healthcare, finance, and security. Large models are crucial for tasks like diagnosing diseases and forecasting finances but tend to be delicate and not very scalable. Decentralized systems solve this issue by distributing the workload and reducing central points of failure. Yet, data and processes spread across different nodes can be at risk of unauthorized access, especially when they involve sensitive information. Nesa solves these challenges with a comprehensive framework using multiple techniques to protect data and model outputs. This includes zero-knowledge proofs for secure model verification. The framework also introduces consensus-based verification checks for consistent outputs across nodes and confirms model integrity. Split Learning divides models into segments processed by different nodes for data privacy by preventing full data access at any single point. For hardware-based security, trusted execution environments are used to protect data and computations within secure zones. Nesa's state-of-the-art proofs and principles demonstrate the framework's effectiveness, making it a promising approach for securely democratizing artificial intelligence.         ",
    "url": "https://arxiv.org/abs/2407.19401",
    "authors": [
      "Hongyang Zhang",
      "Yue Zhao",
      "Claudio Angione",
      "Harry Yang",
      "James Buban",
      "Ahmad Farhan",
      "Fielding Johnston",
      "Patrick Colangelo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19402",
    "title": "NVC-1B: A Large Neural Video Coding Model",
    "abstract": "           The emerging large models have achieved notable progress in the fields of natural language processing and computer vision. However, large models for neural video coding are still unexplored. In this paper, we try to explore how to build a large neural video coding model. Based on a small baseline model, we gradually scale up the model sizes of its different coding parts, including the motion encoder-decoder, motion entropy model, contextual encoder-decoder, contextual entropy model, and temporal context mining module, and analyze the influence of model sizes on video compression performance. Then, we explore to use different architectures, including CNN, mixed CNN-Transformer, and Transformer architectures, to implement the neural video coding model and analyze the influence of model architectures on video compression performance. Based on our exploration results, we design the first neural video coding model with more than 1 billion parameters -- NVC-1B. Experimental results show that our proposed large model achieves a significant video compression performance improvement over the small baseline model, and represents the state-of-the-art compression efficiency. We anticipate large models may bring up the video coding technologies to the next level.         ",
    "url": "https://arxiv.org/abs/2407.19402",
    "authors": [
      "Xihua Sheng",
      "Chuanbo Tang",
      "Li Li",
      "Dong Liu",
      "Feng Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2407.19405",
    "title": "Logic Distillation: Learning from Code Function by Function for Planning and Decision-making",
    "abstract": "           Large language models (LLMs) have garnered increasing attention owing to their powerful logical reasoning capabilities. Generally, larger LLMs (L-LLMs) that require paid interfaces exhibit significantly superior performance compared to smaller LLMs (S-LLMs) that can be deployed on a variety of devices. Knowledge distillation (KD) aims to empower S-LLMs with the capabilities of L-LLMs, while S-LLMs merely mimic the outputs of L-LLMs, failing to get the powerful logical reasoning capabilities. Consequently, S-LLMs are helpless when it comes to planning and decision-making tasks that require logical reasoning capabilities. To tackle the identified challenges, we propose a novel framework called Logic Distillation (LD). Initially, LD employs L-LLMs to instantiate complex instructions into discrete functions and illustrates their usage to establish a function base. Subsequently, based on the function base, LD fine-tunes S-LLMs to learn the logic employed by L-LLMs in planning and decision-making. During testing, LD utilizes a retriever to identify the top-$K$ relevant functions based on instructions and current states, which will be selected and invoked by S-LLMs. Ultimately, S-LLMs yield planning and decision-making outcomes, function by function. Relevant experiments demonstrate that with the assistance of LD, S-LLMs can achieve outstanding results in planning and decision-making tasks, comparable to, or even surpassing, those of L-LLMs.         ",
    "url": "https://arxiv.org/abs/2407.19405",
    "authors": [
      "Dong Chen",
      "Shilin Zhang",
      "Fei Gao",
      "Yueting Zhuang",
      "Siliang Tang",
      "Qidong Liu",
      "Mingliang Xu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19406",
    "title": "Moral and emotional influences on attitude stability towards COVID-19 vaccines on social media",
    "abstract": "           Effective public health messaging benefits from understanding antecedents to unstable attitudes that are more likely to be influenced. This work investigates the relationship between moral and emotional bases for attitudes towards COVID-19 vaccines and variance in stance. Evaluating nearly 1 million X users over a two month period, we find that emotional language in tweets about COVID-19 vaccines is largely associated with more variation in stance of the posting user, except anger and surprise. The strength of COVID-19 vaccine attitudes associated with moral values varies across foundations. Most notably, liberty is consistently used by users with no or less variation in stance, while fairness and sanctity are used by users with more variation. Our work has implications for designing constructive pro-vaccine messaging and identifying receptive audiences.         ",
    "url": "https://arxiv.org/abs/2407.19406",
    "authors": [
      "Samantha C. Phillips",
      "Lynnette Hui Xian Ng",
      "Wenqi Zhou",
      "Kathleen M. Carley"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2407.19414",
    "title": "Appformer: A Novel Framework for Mobile App Usage Prediction Leveraging Progressive Multi-Modal Data Fusion and Feature Extraction",
    "abstract": "           This article presents Appformer, a novel mobile application prediction framework inspired by the efficiency of Transformer-like architectures in processing sequential data through self-attention mechanisms. Combining a Multi-Modal Data Progressive Fusion Module with a sophisticated Feature Extraction Module, Appformer leverages the synergies of multi-modal data fusion and data mining techniques while maintaining user privacy. The framework employs Points of Interest (POIs) associated with base stations, optimizing them through comprehensive comparative experiments to identify the most effective clustering method. These refined inputs are seamlessly integrated into the initial phases of cross-modal data fusion, where temporal units are encoded via word embeddings and subsequently merged in later stages. The Feature Extraction Module, employing Transformer-like architectures specialized for time series analysis, adeptly distils comprehensive features. It meticulously fine-tunes the outputs from the fusion module, facilitating the extraction of high-calibre, multi-modal features, thus guaranteeing a robust and efficient extraction process. Extensive experimental validation confirms Appformer's effectiveness, attaining state-of-the-art (SOTA) metrics in mobile app usage prediction, thereby signifying a notable progression in this field.         ",
    "url": "https://arxiv.org/abs/2407.19414",
    "authors": [
      "Chuike Sun",
      "Junzhou Chen",
      "Yue Zhao",
      "Hao Han",
      "Ruihai Jing",
      "Guang Tan",
      "Di Wu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19420",
    "title": "UniGAP: A Universal and Adaptive Graph Upsampling Approach to Mitigate Over-Smoothing in Node Classification Tasks",
    "abstract": "           In the graph domain, deep graph networks based on Message Passing Neural Networks (MPNNs) or Graph Transformers often cause over-smoothing of node features, limiting their expressive capacity. Many upsampling techniques involving node and edge manipulation have been proposed to mitigate this issue. However, these methods often require extensive manual labor, resulting in suboptimal performance and lacking a universal integration strategy. In this study, we introduce UniGAP, a universal and adaptive graph upsampling technique for graph data. It provides a universal framework for graph upsampling, encompassing most current methods as variants. Moreover, UniGAP serves as a plug-in component that can be seamlessly and adaptively integrated with existing GNNs to enhance performance and mitigate the over-smoothing problem. Through extensive experiments, UniGAP demonstrates significant improvements over heuristic data augmentation methods across various datasets and metrics. We analyze how graph structure evolves with UniGAP, identifying key bottlenecks where over-smoothing occurs, and providing insights into how UniGAP addresses this issue. Lastly, we show the potential of combining UniGAP with large language models (LLMs) to further improve downstream performance. Our code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2407.19420",
    "authors": [
      "Xiaotang Wang",
      "Yun Zhu",
      "Haizhou Shi",
      "Yongchao Liu",
      "Chuntao Hong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19421",
    "title": "Improved physics-informed neural network in mitigating gradient related failures",
    "abstract": "           Physics-informed neural networks (PINNs) integrate fundamental physical principles with advanced data-driven techniques, driving significant advancements in scientific computing. However, PINNs face persistent challenges with stiffness in gradient flow, which limits their predictive capabilities. This paper presents an improved PINN (I-PINN) to mitigate gradient-related failures. The core of I-PINN is to combine the respective strengths of neural networks with an improved architecture and adaptive weights containingupper bounds. The capability to enhance accuracy by at least one order of magnitude and accelerate convergence, without introducing extra computational complexity relative to the baseline model, is achieved by I-PINN. Numerical experiments with a variety of benchmarks illustrate the improved accuracy and generalization of I-PINN. The supporting data and code are accessible at this https URL, enabling broader research engagement.         ",
    "url": "https://arxiv.org/abs/2407.19421",
    "authors": [
      "Pancheng Niu",
      "Yongming Chen",
      "Jun Guo",
      "Yuqian Zhou",
      "Minfu Feng",
      "Yanchao Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19426",
    "title": "Causal Discovery in Linear Models with Unobserved Variables and Measurement Error",
    "abstract": "           The presence of unobserved common causes and the presence of measurement error are two of the most limiting challenges in the task of causal structure learning. Ignoring either of the two challenges can lead to detecting spurious causal links among variables of interest. In this paper, we study the problem of causal discovery in systems where these two challenges can be present simultaneously. We consider linear models which include four types of variables: variables that are directly observed, variables that are not directly observed but are measured with error, the corresponding measurements, and variables that are neither observed nor measured. We characterize the extent of identifiability of such model under separability condition (i.e., the matrix indicating the independent exogenous noise terms pertaining to the observed variables is identifiable) together with two versions of faithfulness assumptions and propose a notion of observational equivalence. We provide graphical characterization of the models that are equivalent and present a recovery algorithm that could return models equivalent to the ground truth.         ",
    "url": "https://arxiv.org/abs/2407.19426",
    "authors": [
      "Yuqin Yang",
      "Mohamed Nafea",
      "Negar Kiyavash",
      "Kun Zhang",
      "AmirEmad Ghassami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.19428",
    "title": "Reputation-Driven Asynchronous Federated Learning for Enhanced Trajectory Prediction with Blockchain",
    "abstract": "           Federated learning combined with blockchain empowers secure data sharing in autonomous driving applications. Nevertheless, with the increasing granularity and complexity of vehicle-generated data, the lack of data quality audits raises concerns about multi-party mistrust in trajectory prediction tasks. In response, this paper proposes an asynchronous federated learning data sharing method based on an interpretable reputation quantization mechanism utilizing graph neural network tools. Data providers share data structures under differential privacy constraints to ensure security while reducing redundant data. We implement deep reinforcement learning to categorize vehicles by reputation level, which optimizes the aggregation efficiency of federated learning. Experimental results demonstrate that the proposed data sharing scheme not only reinforces the security of the trajectory prediction task but also enhances prediction accuracy.         ",
    "url": "https://arxiv.org/abs/2407.19428",
    "authors": [
      "Weiliang Chen",
      "Li Jia",
      "Yang Zhou",
      "Qianqian Ren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19429",
    "title": "FTF-ER: Feature-Topology Fusion-Based Experience Replay Method for Continual Graph Learning",
    "abstract": "           Continual graph learning (CGL) is an important and challenging task that aims to extend static GNNs to dynamic task flow scenarios. As one of the mainstream CGL methods, the experience replay (ER) method receives widespread attention due to its superior performance. However, existing ER methods focus on identifying samples by feature significance or topological relevance, which limits their utilization of comprehensive graph data. In addition, the topology-based ER methods only consider local topological information and add neighboring nodes to the buffer, which ignores the global topological information and increases memory overhead. To bridge these gaps, we propose a novel method called Feature-Topology Fusion-based Experience Replay (FTF-ER) to effectively mitigate the catastrophic forgetting issue with enhanced efficiency. Specifically, from an overall perspective to maximize the utilization of the entire graph data, we propose a highly complementary approach including both feature and global topological information, which can significantly improve the effectiveness of the sampled nodes. Moreover, to further utilize global topological information, we propose Hodge Potential Score (HPS) as a novel module to calculate the topological importance of nodes. HPS derives a global node ranking via Hodge decomposition on graphs, providing more accurate global topological information compared to neighbor sampling. By excluding neighbor sampling, HPS significantly reduces buffer storage costs for acquiring topological information and simultaneously decreases training time. Compared with state-of-the-art methods, FTF-ER achieves a significant improvement of 3.6% in AA and 7.1% in AF on the OGB-Arxiv dataset, demonstrating its superior performance in the class-incremental learning setting.         ",
    "url": "https://arxiv.org/abs/2407.19429",
    "authors": [
      "Jinhui Pang",
      "Changqing Lin",
      "Xiaoshuai Hao",
      "Rong Yin",
      "Zixuan Wang",
      "Zhihui Zhang",
      "Jinglin He",
      "Huang Tai Sheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2407.19434",
    "title": "FINER++: Building a Family of Variable-periodic Functions for Activating Implicit Neural Representation",
    "abstract": "           Implicit Neural Representation (INR), which utilizes a neural network to map coordinate inputs to corresponding attributes, is causing a revolution in the field of signal processing. However, current INR techniques suffer from the \"frequency\"-specified spectral bias and capacity-convergence gap, resulting in imperfect performance when representing complex signals with multiple \"frequencies\". We have identified that both of these two characteristics could be handled by increasing the utilization of definition domain in current activation functions, for which we propose the FINER++ framework by extending existing periodic/non-periodic activation functions to variable-periodic ones. By initializing the bias of the neural network with different ranges, sub-functions with various frequencies in the variable-periodic function are selected for activation. Consequently, the supported frequency set can be flexibly tuned, leading to improved performance in signal representation. We demonstrate the generalization and capabilities of FINER++ with different activation function backbones (Sine, Gauss. and Wavelet) and various tasks (2D image fitting, 3D signed distance field representation, 5D neural radiance fields optimization and streamable INR transmission), and we show that it improves existing INRs. Project page: {this https URL}         ",
    "url": "https://arxiv.org/abs/2407.19434",
    "authors": [
      "Hao Zhu",
      "Zhen Liu",
      "Qi Zhang",
      "Jingde Fu",
      "Weibing Deng",
      "Zhan Ma",
      "Yanwen Guo",
      "Xun Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19441",
    "title": "Competition-based Adaptive ReLU for Deep Neural Networks",
    "abstract": "           Activation functions introduce nonlinearity into deep neural networks. Most popular activation functions allow positive values to pass through while blocking or suppressing negative values. From the idea that positive values and negative values are equally important, and they must compete for activation, we proposed a new Competition-based Adaptive ReLU (CAReLU). CAReLU scales the input values based on the competition results between positive values and negative values. It defines two parameters to adjust the scaling strategy and can be trained uniformly with other network parameters. We verify the effectiveness of CAReLU on image classification, super-resolution, and natural language processing tasks. In the experiment, our method performs better than other widely used activation functions. In the case of replacing ReLU in ResNet-18 with our proposed activation function, it improves the classification accuracy on the CIFAR-100 dataset. The effectiveness and the new perspective on the utilization of competition results between positive values and negative values make CAReLU a promising activation function.         ",
    "url": "https://arxiv.org/abs/2407.19441",
    "authors": [
      "Junjia Chen",
      "Zhibin Pan"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2407.19446",
    "title": "Leave-One-Out Analysis for Nonconvex Robust Matrix Completion with General Thresholding Functions",
    "abstract": "           We study the problem of robust matrix completion (RMC), where the partially observed entries of an underlying low-rank matrix is corrupted by sparse noise. Existing analysis of the non-convex methods for this problem either requires the explicit but empirically redundant regularization in the algorithm or requires sample splitting in the analysis. In this paper, we consider a simple yet efficient nonconvex method which alternates between a projected gradient step for the low-rank part and a thresholding step for the sparse noise part. Inspired by leave-one out analysis for low rank matrix completion, it is established that the method can achieve linear convergence for a general class of thresholding functions, including for example soft-thresholding and SCAD. To the best of our knowledge, this is the first leave-one-out analysis on a nonconvex method for RMC. Additionally, when applying our result to low rank matrix completion, it improves the sampling complexity of existing result for the singular value projection method.         ",
    "url": "https://arxiv.org/abs/2407.19446",
    "authors": [
      "Tianming Wang",
      "Ke Wei"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.19451",
    "title": "\\textsc{Perm}: A Parametric Representation for Multi-Style 3D Hair Modeling",
    "abstract": "           We present \\textsc{Perm}, a learned parametric model of human 3D hair designed to facilitate various hair-related applications. Unlike previous work that jointly models the global hair shape and local strand details, we propose to disentangle them using a PCA-based strand representation in the frequency domain, thereby allowing more precise editing and output control. Specifically, we leverage our strand representation to fit and decompose hair geometry textures into low- to high-frequency hair structures. These decomposed textures are later parameterized with different generative models, emulating common stages in the hair modeling process. We conduct extensive experiments to validate the architecture design of \\textsc{Perm}, and finally deploy the trained model as a generic prior to solve task-agnostic problems, further showcasing its flexibility and superiority in tasks such as 3D hair parameterization, hairstyle interpolation, single-view hair reconstruction, and hair-conditioned image generation. Our code and data will be available at: \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2407.19451",
    "authors": [
      "Chengan He",
      "Xin Sun",
      "Zhixin Shu",
      "Fujun Luan",
      "S\u00f6ren Pirk",
      "Jorge Alejandro Amador Herrera",
      "Dominik L. Michels",
      "Tuanfeng Y. Wang",
      "Meng Zhang",
      "Holly Rushmeier",
      "Yi Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2407.19475",
    "title": "Multi-task Neural Networks for Pain Intensity Estimation using Electrocardiogram and Demographic Factors",
    "abstract": "           Pain is a complex phenomenon which is manifested and expressed by patients in various forms. The immediate and objective recognition of it is a great of importance in order to attain a reliable and unbiased healthcare system. In this work, we elaborate electrocardiography signals revealing the existence of variations in pain perception among different demographic groups. We exploit this insight by introducing a novel multi-task neural network for automatic pain estimation utilizing the age and the gender information of each individual, and show its advantages compared to other approaches.         ",
    "url": "https://arxiv.org/abs/2407.19475",
    "authors": [
      "Stefanos Gkikas",
      "Chariklia Chatzaki",
      "Manolis Tsiknakis"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19479",
    "title": "Breaking the Balance of Power: Commitment Attacks on Ethereum's Reward Mechanism",
    "abstract": "           Validators in permissionless, large-scale blockchains (e.g., Ethereum) are typically payoff-maximizing, rational actors. Ethereum relies on in-protocol incentives, like rewards for validators delivering correct and timely votes, to induce honest behavior and secure the blockchain. However, external incentives, such as the block proposer's opportunity to capture maximal extractable value (MEV), may tempt validators to deviate from honest protocol participation. We show a series of commitment attacks on LMD GHOST, a core part of Ethereum's consensus mechanism. We demonstrate how a single adversarial block proposer can orchestrate long-range chain reorganizations by manipulating Ethereum's reward system for timely votes. These attacks disrupt the intended balance of power between proposers and voters: by leveraging credible threats, the adversarial proposer can coerce voters from previous slots into supporting blocks that conflict with the honest chain, enabling a chain reorganization at no cost to the adversary. In response, we introduce a novel reward mechanism that restores the voters' role as a check against proposer power. Our proposed mitigation is fairer and more decentralized -- not only in the context of these attacks -- but also practical for implementation in Ethereum.         ",
    "url": "https://arxiv.org/abs/2407.19479",
    "authors": [
      "Roozbeh Sarenche",
      "Ertem Nusret Tas",
      "Barnabe Monnot",
      "Caspar Schwarz-Schilling",
      "Bart Preneel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.19487",
    "title": "RLCoder: Reinforcement Learning for Repository-Level Code Completion",
    "abstract": "           Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrieval-augmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.19487",
    "authors": [
      "Yanlin Wang",
      "Yanli Wang",
      "Daya Guo",
      "Jiachi Chen",
      "Ruikai Zhang",
      "Yuchi Ma",
      "Zibin Zheng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2407.19493",
    "title": "Official-NV: A News Video Dataset for Multimodal Fake News Detection",
    "abstract": "           News media, especially video news media, have penetrated into every aspect of daily life, which also brings the risk of fake news. Therefore, multimodal fake news detection has recently received more attention. However, the number of fake news detection data sets for video modal is small, and these data sets are composed of unofficial videos uploaded by users, so there is too much useless data. To solve this problem, we present in this paper a dataset named Official-NV, which consists of officially published news videos on Xinhua. We crawled videos on Xinhua, and then extended the data set using LLM generation and manual modification. In addition, we benchmarked the data set presented in this paper using a baseline model to demonstrate the advantage of Official-NV in multimodal fake news detection.         ",
    "url": "https://arxiv.org/abs/2407.19493",
    "authors": [
      "Yihao Wang",
      "Lizhi Chen",
      "Zhong Qian",
      "Peifeng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2407.19497",
    "title": "Skeleton-based Group Activity Recognition via Spatial-Temporal Panoramic Graph",
    "abstract": "           Group Activity Recognition aims to understand collective activities from videos. Existing solutions primarily rely on the RGB modality, which encounters challenges such as background variations, occlusions, motion blurs, and significant computational overhead. Meanwhile, current keypoint-based methods offer a lightweight and informative representation of human motions but necessitate accurate individual annotations and specialized interaction reasoning modules. To address these limitations, we design a panoramic graph that incorporates multi-person skeletons and objects to encapsulate group activity, offering an effective alternative to RGB video. This panoramic graph enables Graph Convolutional Network (GCN) to unify intra-person, inter-person, and person-object interactive modeling through spatial-temporal graph convolutions. In practice, we develop a novel pipeline that extracts skeleton coordinates using pose estimation and tracking algorithms and employ Multi-person Panoramic GCN (MP-GCN) to predict group activities. Extensive experiments on Volleyball and NBA datasets demonstrate that the MP-GCN achieves state-of-the-art performance in both accuracy and efficiency. Notably, our method outperforms RGB-based approaches by using only estimated 2D keypoints as input. Code is available at this https URL ",
    "url": "https://arxiv.org/abs/2407.19497",
    "authors": [
      "Zhengcen Li",
      "Xinle Chang",
      "Yueran Li",
      "Jingyong Su"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19517",
    "title": "Evaluating LLMs for Text-to-SQL Generation With Complex SQL Workload",
    "abstract": "           This study presents a comparative analysis of the a complex SQL benchmark, TPC-DS, with two existing text-to-SQL benchmarks, BIRD and Spider. Our findings reveal that TPC-DS queries exhibit a significantly higher level of structural complexity compared to the other two benchmarks. This underscores the need for more intricate benchmarks to simulate realistic scenarios effectively. To facilitate this comparison, we devised several measures of structural complexity and applied them across all three benchmarks. The results of this study can guide future research in the development of more sophisticated text-to-SQL benchmarks. We utilized 11 distinct Language Models (LLMs) to generate SQL queries based on the query descriptions provided by the TPC-DS benchmark. The prompt engineering process incorporated both the query description as outlined in the TPC-DS specification and the database schema of TPC-DS. Our findings indicate that the current state-of-the-art generative AI models fall short in generating accurate decision-making queries. We conducted a comparison of the generated queries with the TPC-DS gold standard queries using a series of fuzzy structure matching techniques based on query features. The results demonstrated that the accuracy of the generated queries is insufficient for practical real-world application.         ",
    "url": "https://arxiv.org/abs/2407.19517",
    "authors": [
      "Limin Ma",
      "Ken Pu",
      "Ying Zhu"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19523",
    "title": "Robust Fast Adaptation from Adversarially Explicit Task Distribution Generation",
    "abstract": "           Meta-learning is a practical learning paradigm to transfer skills across tasks from a few examples. Nevertheless, the existence of task distribution shifts tends to weaken meta-learners' generalization capability, particularly when the task distribution is naively hand-crafted or based on simple priors that fail to cover typical scenarios sufficiently. Here, we consider explicitly generative modeling task distributions placed over task identifiers and propose robustifying fast adaptation from adversarial training. Our approach, which can be interpreted as a model of a Stackelberg game, not only uncovers the task structure during problem-solving from an explicit generative model but also theoretically increases the adaptation robustness in worst cases. This work has practical implications, particularly in dealing with task distribution shifts in meta-learning, and contributes to theoretical insights in the field. Our method demonstrates its robustness in the presence of task subpopulation shifts and improved performance over SOTA baselines in extensive experiments. The project is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.19523",
    "authors": [
      "Cheems Wang",
      "Yiqin Lv",
      "Yixiu Mao",
      "Yun Qu",
      "Yi Xu",
      "Xiangyang Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19529",
    "title": "From Obstacle Problems to Neural Insights: Feed Forward Neural Network Modeling of Ice Thickness",
    "abstract": "           In this study, we integrate the established obstacle problem formulation from ice sheet modeling with cutting-edge deep learning methodologies to enhance ice thickness predictions, specifically targeting the Greenland ice sheet. By harmonizing the mathematical structure with an energy minimization framework tailored for neural network approximations, our method's efficacy is confirmed through both 1D and 2D numerical simulations. Utilizing the NSIDC-0092 dataset for Greenland and incorporating bedrock topography for model pre-training, we register notable advances in prediction accuracy. Our research underscores the potent combination of traditional mathematical models and advanced computational techniques in delivering precise ice thickness estimations.         ",
    "url": "https://arxiv.org/abs/2407.19529",
    "authors": [
      "Kapil Chawla",
      "William Holmes",
      "Roger Temam"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2407.19533",
    "title": "FreeShell: A Context-Free 4D Printing Technique for Fabricating Complex 3D Triangle Mesh Shells",
    "abstract": "           Freeform thin-shell surfaces are critical in various fields, but their fabrication is complex and costly. Traditional methods are wasteful and require custom molds, while 3D printing needs extensive support structures and post-processing. Thermoshrinkage actuated 4D printing is an effective method through flat structures fabricating 3D shell. However, existing research faces issues related to precise deformation and limited robustness. Addressing these issues is challenging due to three key factors: (1) Difficulty in finding a universal method to control deformation across different materials; (2) Variability in deformation influenced by factors such as printing speed, layer thickness, and heating temperature; (3) Environmental factors affecting the deformation process. To overcome these challenges, we introduce FreeShell, a robust 4D printing technique that uses thermoshrinkage to create precise 3D shells. This method prints triangular tiles connected by shrinkable connectors from a single material. Upon heating, the connectors shrink, moving the tiles to form the desired 3D shape, simplifying fabrication and reducing material and environment dependency. An optimized algorithm for flattening 3D meshes ensures precision in printing. FreeShell demonstrates its effectiveness through various examples and experiments, showcasing accuracy, robustness, and strength, representing advancement in fabricating complex freeform surfaces.         ",
    "url": "https://arxiv.org/abs/2407.19533",
    "authors": [
      "Chao Yuan",
      "Nan Cao",
      "Xuejiao Ma",
      "Shengqi Dang"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2407.19540",
    "title": "Overcoming Uncertain Incompleteness for Robust Multimodal Sequential Diagnosis Prediction via Knowledge Distillation and Random Data Erasing",
    "abstract": "           In this paper, we present NECHO v2, a novel framework designed to enhance the predictive accuracy of multimodal sequential patient diagnoses under uncertain missing visit sequences, a common challenge in clinical settings. Firstly, we modify NECHO to handle uncertain modality representation dominance under the imperfect data. Next, we develop a systematic knowledge distillation by employing the modified NECHO as both teacher and student. It encompasses a modality-wise contrastive and hierarchical distillation, transformer representation random distillation, along with other distillations to align representations tightly and effectively. We also utilise random erasing on individual data points within sequences during both training and distillation of teacher to lightly simulate scenario with missing visit information to foster effective knowledge transfer. As a result, NECHO v2 verifies itself by showing superiority in multimodal sequential diagnosis prediction on both balanced and imbalanced incomplete settings on multimodal healthcare data.         ",
    "url": "https://arxiv.org/abs/2407.19540",
    "authors": [
      "Heejoon Koo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.19542",
    "title": "UniVoxel: Fast Inverse Rendering by Unified Voxelization of Scene Representation",
    "abstract": "           Typical inverse rendering methods focus on learning implicit neural scene representations by modeling the geometry, materials and illumination separately, which entails significant computations for optimization. In this work we design a Unified Voxelization framework for explicit learning of scene representations, dubbed UniVoxel, which allows for efficient modeling of the geometry, materials and illumination jointly, thereby accelerating the inverse rendering significantly. To be specific, we propose to encode a scene into a latent volumetric representation, based on which the geometry, materials and illumination can be readily learned via lightweight neural networks in a unified manner. Particularly, an essential design of UniVoxel is that we leverage local Spherical Gaussians to represent the incident light radiance, which enables the seamless integration of modeling illumination into the unified voxelization framework. Such novel design enables our UniVoxel to model the joint effects of direct lighting, indirect lighting and light visibility efficiently without expensive multi-bounce ray tracing. Extensive experiments on multiple benchmarks covering diverse scenes demonstrate that UniVoxel boosts the optimization efficiency significantly compared to other methods, reducing the per-scene training time from hours to 18 minutes, while achieving favorable reconstruction quality. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.19542",
    "authors": [
      "Shuang Wu",
      "Songlin Tang",
      "Guangming Lu",
      "Jianzhuang Liu",
      "Wenjie Pei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19553",
    "title": "Exploring the Adversarial Robustness of CLIP for AI-generated Image Detection",
    "abstract": "           In recent years, many forensic detectors have been proposed to detect AI-generated images and prevent their use for malicious purposes. Convolutional neural networks (CNNs) have long been the dominant architecture in this field and have been the subject of intense study. However, recently proposed Transformer-based detectors have been shown to match or even outperform CNN-based detectors, especially in terms of generalization. In this paper, we study the adversarial robustness of AI-generated image detectors, focusing on Contrastive Language-Image Pretraining (CLIP)-based methods that rely on Visual Transformer backbones and comparing their performance with CNN-based methods. We study the robustness to different adversarial attacks under a variety of conditions and analyze both numerical results and frequency-domain patterns. CLIP-based detectors are found to be vulnerable to white-box attacks just like CNN-based detectors. However, attacks do not easily transfer between CNN-based and CLIP-based methods. This is also confirmed by the different distribution of the adversarial noise patterns in the frequency domain. Overall, this analysis provides new insights into the properties of forensic detectors that can help to develop more effective strategies.         ",
    "url": "https://arxiv.org/abs/2407.19553",
    "authors": [
      "Vincenzo De Rosa",
      "Fabrizio Guillaro",
      "Giovanni Poggi",
      "Davide Cozzolino",
      "Luisa Verdoliva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19557",
    "title": "Neural stochastic Volterra equations: learning path-dependent dynamics",
    "abstract": "           Stochastic Volterra equations (SVEs) serve as mathematical models for the time evolutions of random systems with memory effects and irregular behaviour. We introduce neural stochastic Volterra equations as a physics-inspired architecture, generalizing the class of neural stochastic differential equations, and provide some theoretical foundation. Numerical experiments on various SVEs, like the disturbed pendulum equation, the generalized Ornstein--Uhlenbeck process and the rough Heston model are presented, comparing the performance of neural SVEs, neural SDEs and Deep Operator Networks (DeepONets).         ",
    "url": "https://arxiv.org/abs/2407.19557",
    "authors": [
      "David J. Pr\u00f6mel",
      "David Scheffels"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.19562",
    "title": "Lock-Free Computation of PageRank in Dynamic Graphs",
    "abstract": "           PageRank is a metric that assigns importance to the vertices of a graph based on its neighbors and their scores. Recently, there has been increasing interest in computing PageRank on dynamic graphs, where the graph structure evolves due to edge insertions and deletions. However, traditional barrier-based approaches for updating PageRanks encounter significant wait times on certain graph structures, leading to high overall runtimes. Additionally, the growing trend of multicore architectures with increased core counts has raised concerns about random thread delays and failures. In this study, we propose a lock-free algorithm for updating PageRank scores on dynamic graphs. First, we introduce our Dynamic Frontier (DF) approach, which identifies and processes vertices likely to change PageRanks with minimal overhead. Subsequently, we integrate DF with our lock-free and fault-tolerant PageRank ($DF_{LF}$), incorporating a helping mechanism among threads between its two phases. Experimental results demonstrate that $DF_{LF}$ not only eliminates waiting times at iteration barriers but also withstands random thread delays and crashes. On average, it is 4.6x faster than lock-free Naive-dynamic PageRank ($ND_{LF}$).         ",
    "url": "https://arxiv.org/abs/2407.19562",
    "authors": [
      "Subhajit Sahu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2407.19566",
    "title": "Rouser: Robust SNN training using adaptive threshold learning",
    "abstract": "           In Spiking Neural Networks (SNNs), learning rules are based on neuron spiking behavior, that is, if and when spikes are generated due to a neuron's membrane potential exceeding that neuron's firing threshold, and this spike timing encodes vital information. However, the threshold is generally treated as a hyperparameter, and incorrect selection can lead to neurons that do not spike for large portions of the training process, hindering the effective rate of learning. Inspired by homeostatic mechanisms in biological neurons, this work (Rouser) presents a study to rouse training-inactive neurons and improve the SNN training by using an in-loop adaptive threshold learning mechanism. Rouser's adaptive threshold allows for dynamic adjustments based on input data and network hyperparameters, influencing spike timing and improving training. This study focuses primarily on investigating the significance of learning neuron thresholds alongside weights in SNNs. We evaluate the performance of Rouser on the spatiotemporal datasets NMNIST, DVS128 and Spiking Heidelberg Digits (SHD), compare our results with state-of-the-art SNN training techniques, and discuss the strengths and limitations of our approach. Our results suggest that promoting threshold from a hyperparameter to a parameter can effectively address the issue of dead neurons during training, resulting in a more robust training algorithm that leads to improved training convergence, increased test accuracy, and substantial reductions in the number of training epochs needed to achieve viable accuracy. Rouser achieves up to 70% lower training latency while providing up to 2% higher accuracy over state-of-the-art SNNs with similar network architecture on the neuromorphic datasets NMNIST, DVS128 and SHD.         ",
    "url": "https://arxiv.org/abs/2407.19566",
    "authors": [
      "Sanaz Mahmoodi Takaghaj",
      "Jack Sampson"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2407.19567",
    "title": "Sharp Bounds for Poly-GNNs and the Effect of Graph Noise",
    "abstract": "           We investigate the classification performance of graph neural networks with graph-polynomial features, poly-GNNs, on the problem of semi-supervised node classification. We analyze poly-GNNs under a general contextual stochastic block model (CSBM) by providing a sharp characterization of the rate of separation between classes in their output node representations. A question of interest is whether this rate depends on the depth of the network $k$, i.e., whether deeper networks can achieve a faster separation? We provide a negative answer to this question: for a sufficiently large graph, a depth $k > 1$ poly-GNN exhibits the same rate of separation as a depth $k=1$ counterpart. Our analysis highlights and quantifies the impact of ``graph noise'' in deep GNNs and shows how noise in the graph structure can dominate other sources of signal in the graph, negating any benefit further aggregation provides. Our analysis also reveals subtle differences between even and odd-layered GNNs in how the feature noise propagates.         ",
    "url": "https://arxiv.org/abs/2407.19567",
    "authors": [
      "Luciano Vinas",
      "Arash A. Amini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.19569",
    "title": "Detection of Unknown Errors in Human-Centered Systems",
    "abstract": "           Artificial Intelligence-enabled systems are increasingly being deployed in real-world safety-critical settings involving human participants. It is vital to ensure the safety of such systems and stop the evolution of the system with error before causing harm to human participants. We propose a model-agnostic approach to detecting unknown errors in such human-centered systems without requiring any knowledge about the error signatures. Our approach employs dynamics-induced hybrid recurrent neural networks (DiH-RNN) for constructing physics-based models from operational data, coupled with conformal inference for assessing errors in the underlying model caused by violations of physical laws, thereby facilitating early detection of unknown errors before unsafe shifts in operational data distribution occur. We evaluate our framework on multiple real-world safety critical systems and show that our technique outperforms the existing state-of-the-art in detecting unknown errors.         ",
    "url": "https://arxiv.org/abs/2407.19569",
    "authors": [
      "Aranyak Maity",
      "Ayan Banerjee",
      "Sandeep Gupta"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2407.19605",
    "title": "Look Hear: Gaze Prediction for Speech-directed Human Attention",
    "abstract": "           For computer systems to effectively interact with humans using spoken language, they need to understand how the words being generated affect the users' moment-by-moment attention. Our study focuses on the incremental prediction of attention as a person is seeing an image and hearing a referring expression defining the object in the scene that should be fixated by gaze. To predict the gaze scanpaths in this incremental object referral task, we developed the Attention in Referral Transformer model or ART, which predicts the human fixations spurred by each word in a referring expression. ART uses a multimodal transformer encoder to jointly learn gaze behavior and its underlying grounding tasks, and an autoregressive transformer decoder to predict, for each word, a variable number of fixations based on fixation history. To train ART, we created RefCOCO-Gaze, a large-scale dataset of 19,738 human gaze scanpaths, corresponding to 2,094 unique image-expression pairs, from 220 participants performing our referral task. In our quantitative and qualitative analyses, ART not only outperforms existing methods in scanpath prediction, but also appears to capture several human attention patterns, such as waiting, scanning, and verification.         ",
    "url": "https://arxiv.org/abs/2407.19605",
    "authors": [
      "Sounak Mondal",
      "Seoyoung Ahn",
      "Zhibo Yang",
      "Niranjan Balasubramanian",
      "Dimitris Samaras",
      "Gregory Zelinsky",
      "Minh Hoai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19619",
    "title": "Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation",
    "abstract": "           The advent of large language models (LLMs) has significantly advanced the field of code translation, enabling automated translation between programming languages. However, these models often struggle with complex translation tasks due to inadequate contextual understanding. This paper introduces a novel approach that enhances code translation through Few-Shot Learning, augmented with retrieval-based techniques. By leveraging a repository of existing code translations, we dynamically retrieve the most relevant examples to guide the model in translating new code segments. Our method, based on Retrieval-Augmented Generation (RAG), substantially improves translation quality by providing contextual examples from which the model can learn in real-time. We selected RAG over traditional fine-tuning methods due to its ability to utilize existing codebases or a locally stored corpus of code, which allows for dynamic adaptation to diverse translation tasks without extensive retraining. Extensive experiments on diverse datasets with open LLM models such as Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code Instruct, and Mixtral-8x22B, as well as commercial LLM models like GPT-3.5 Turbo and GPT-4o, demonstrate our approach's superiority over traditional zero-shot methods, especially in translating between Fortran and CPP. We also explored varying numbers of shots i.e. examples provided during inference, specifically 1, 2, and 3 shots and different embedding models for RAG, including Nomic-Embed, Starencoder, and CodeBERT, to assess the robustness and effectiveness of our approach.         ",
    "url": "https://arxiv.org/abs/2407.19619",
    "authors": [
      "Manish Bhattarai",
      "Javier E. Santos",
      "Shawn Jones",
      "Ayan Biswas",
      "Boian Alexandrov",
      "Daniel O'Malley"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2407.19625",
    "title": "LoginMEA: Local-to-Global Interaction Network for Multi-modal Entity Alignment",
    "abstract": "           Multi-modal entity alignment (MMEA) aims to identify equivalent entities between two multi-modal knowledge graphs (MMKGs), whose entities can be associated with relational triples and related images. Most previous studies treat the graph structure as a special modality, and fuse different modality information with separate uni-modal encoders, neglecting valuable relational associations in modalities. Other studies refine each uni-modal information with graph structures, but may introduce unnecessary relations in specific modalities. To this end, we propose a novel local-to-global interaction network for MMEA, termed as LoginMEA. Particularly, we first fuse local multi-modal interactions to generate holistic entity semantics and then refine them with global relational interactions of entity neighbors. In this design, the uni-modal information is fused adaptively, and can be refined with relations accordingly. To enrich local interactions of multi-modal entity information, we device modality weights and low-rank interactive fusion, allowing diverse impacts and element-level interactions among modalities. To capture global interactions of graph structures, we adopt relation reflection graph attention networks, which fully capture relational associations between entities. Extensive experiments demonstrate superior results of our method over 5 cross-KG or bilingual benchmark datasets, indicating the effectiveness of capturing local and global interactions.         ",
    "url": "https://arxiv.org/abs/2407.19625",
    "authors": [
      "Taoyu Su",
      "Xinghua Zhang",
      "Jiawei Sheng",
      "Zhenyu Zhang",
      "Tingwen Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2407.19638",
    "title": "From Pre-training Corpora to Large Language Models: What Factors Influence LLM Performance in Causal Discovery Tasks?",
    "abstract": "           Recent advances in artificial intelligence have seen Large Language Models (LLMs) demonstrate notable proficiency in causal discovery tasks. This study explores the factors influencing the performance of LLMs in causal discovery tasks. Utilizing open-source LLMs, we examine how the frequency of causal relations within their pre-training corpora affects their ability to accurately respond to causal discovery queries. Our findings reveal that a higher frequency of causal mentions correlates with better model performance, suggesting that extensive exposure to causal information during training enhances the models' causal discovery capabilities. Additionally, we investigate the impact of context on the validity of causal relations. Our results indicate that LLMs might exhibit divergent predictions for identical causal relations when presented in different contexts. This paper provides the first comprehensive analysis of how different factors contribute to LLM performance in causal discovery tasks.         ",
    "url": "https://arxiv.org/abs/2407.19638",
    "authors": [
      "Tao Feng",
      "Lizhen Qu",
      "Niket Tandon",
      "Zhuang Li",
      "Xiaoxi Kang",
      "Gholamreza Haffari"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.19643",
    "title": "Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model for Computer Components Recommendation",
    "abstract": "           Knowledge graphs (KGs) are essential in applications such as network alignment, question-answering, and recommender systems (RSs) since they offer structured relational data that facilitate the inference of indirect relationships. However, the development of KG-based RSs capable of processing user inputs in natural language faces significant challenges. Firstly, natural language processing units must effectively handle the ambiguity and variability in human language to interpret user intents accurately. Secondly, the system must precisely identify and link entities, like product names, to their corresponding nodes in KGs. To overcome these challenges, supported by Lenovo, we developed a novel chatbot called \"Prometheus,\" which integrates a KG with a large language model (LLM), specifically designed for recommending computer components. This chatbot can accurately decode user requests and deliver personalized recommendations derived from KGs, ensuring precise comprehension and response to their computer setup needs.         ",
    "url": "https://arxiv.org/abs/2407.19643",
    "authors": [
      "Yunsheng Wang",
      "Songhao Chen",
      "Kevin Jin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19645",
    "title": "Non-iterative complex variable solution on sequential shallow tunnelling in gravitational geomaterial with reasonable far-field displacement",
    "abstract": "           Sequential excavation is common in shallow tunnel engineering, especially for large-span tunnels. However, existing complex variable solutions can not handle sequential shallow tunnelling effectively. This paper proposes a new complex variable solution on sequential shallow tunnelling in gravitational geomaterial with reasonable far-field displacement in a non-iterative manner by incorporating a bidirectional stepwise conformal mapping combining Charge Simulation Method and Complex Dipole Simulation Method. The non-iterative manner ensures that the mechanical models of sequential excavation stages share similar mathematical formation with non-successive mixed boundary conditions, which are respectively transformed into corresponding homogenerous Riemann-Hilbert problems, which are solved to obtain stress and displacement fields of sequential shallow tunnelling. The proposed solution is subsequently validated by sufficient comparisons with equivalent finite element solution with good agreements. The comparisons also suggest that the proposed solution should be more accurate than the finite element one. A parametric investigation is finally conducted to illustrate possible practical applications of the proposed solution with several engineering recommendations. Additionally, the theoretical improvements and defects of the proposed solution are discussed for objectivity.         ",
    "url": "https://arxiv.org/abs/2407.19645",
    "authors": [
      "Luo-bin Lin",
      "Fu-quan Chen",
      "Change-jie Zheng",
      "Shang-shun Lin"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Complex Variables (math.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19646",
    "title": "Foundations for Unfairness in Anomaly Detection -- Case Studies in Facial Imaging Data",
    "abstract": "           Deep anomaly detection (AD) is perhaps the most controversial of data analytic tasks as it identifies entities that are then specifically targeted for further investigation or exclusion. Also controversial is the application of AI to facial imaging data. This work explores the intersection of these two areas to understand two core questions: \"Who\" these algorithms are being unfair to and equally important \"Why\". Recent work has shown that deep AD can be unfair to different groups despite being unsupervised with a recent study showing that for portraits of people: men of color are far more likely to be chosen to be outliers. We study the two main categories of AD algorithms: autoencoder-based and single-class-based which effectively try to compress all the instances with those that can not be easily compressed being deemed to be outliers. We experimentally verify sources of unfairness such as the under-representation of a group (e.g. people of color are relatively rare), spurious group features (e.g. men are often photographed with hats), and group labeling noise (e.g. race is subjective). We conjecture that lack of compressibility is the main foundation and the others cause it but experimental results show otherwise and we present a natural hierarchy amongst them.         ",
    "url": "https://arxiv.org/abs/2407.19646",
    "authors": [
      "Michael Livanos",
      "Ian Davidson"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19650",
    "title": "Practical Video Object Detection via Feature Selection and Aggregation",
    "abstract": "           Compared with still image object detection, video object detection (VOD) needs to particularly concern the high across-frame variation in object appearance, and the diverse deterioration in some frames. In principle, the detection in a certain frame of a video can benefit from information in other frames. Thus, how to effectively aggregate features across different frames is key to the target problem. Most of contemporary aggregation methods are tailored for two-stage detectors, suffering from high computational costs due to the dual-stage nature. On the other hand, although one-stage detectors have made continuous progress in handling static images, their applicability to VOD lacks sufficient exploration. To tackle the above issues, this study invents a very simple yet potent strategy of feature selection and aggregation, gaining significant accuracy at marginal computational expense. Concretely, for cutting the massive computation and memory consumption from the dense prediction characteristic of one-stage object detectors, we first condense candidate features from dense prediction maps. Then, the relationship between a target frame and its reference frames is evaluated to guide the aggregation. Comprehensive experiments and ablation studies are conducted to validate the efficacy of our design, and showcase its advantage over other cutting-edge VOD methods in both effectiveness and efficiency. Notably, our model reaches \\emph{a new record performance, i.e., 92.9\\% AP50 at over 30 FPS on the ImageNet VID dataset on a single 3090 GPU}, making it a compelling option for large-scale or real-time applications. The implementation is simple, and accessible at \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2407.19650",
    "authors": [
      "Yuheng Shi",
      "Tong Zhang",
      "Xiaojie Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19658",
    "title": "Enhancing CTR Prediction through Sequential Recommendation Pre-training: Introducing the SRP4CTR Framework",
    "abstract": "           Understanding user interests is crucial for Click-Through Rate (CTR) prediction tasks. In sequential recommendation, pre-training from user historical behaviors through self-supervised learning can better comprehend user dynamic preferences, presenting the potential for direct integration with CTR tasks. Previous methods have integrated pre-trained models into downstream tasks with the sole purpose of extracting semantic information or well-represented user features, which are then incorporated as new features. However, these approaches tend to ignore the additional inference costs to the downstream tasks, and they do not consider how to transfer the effective information from the pre-trained models for specific estimated items in CTR prediction. In this paper, we propose a Sequential Recommendation Pre-training framework for CTR prediction (SRP4CTR) to tackle the above problems. Initially, we discuss the impact of introducing pre-trained models on inference costs. Subsequently, we introduced a pre-trained method to encode sequence side information concurrently.During the fine-tuning process, we incorporate a cross-attention block to establish a bridge between estimated items and the pre-trained model at a low cost. Moreover, we develop a querying transformer technique to facilitate the knowledge transfer from the pre-trained model to industrial CTR models. Offline and online experiments show that our method outperforms previous baseline models.         ",
    "url": "https://arxiv.org/abs/2407.19658",
    "authors": [
      "Ruidong Han",
      "Qianzhong Li",
      "He Jiang",
      "Rui Li",
      "Yurou Zhao",
      "Xiang Li",
      "Wei Lin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.19662",
    "title": "Towards Detecting IoT Event Spoofing Attacks Using Time-Series Classification",
    "abstract": "           Internet of Things (IoT) devices have grown in popularity since they can directly interact with the real world. Home automation systems automate these interactions. IoT events are crucial to these systems' decision-making but are often unreliable. Security vulnerabilities allow attackers to impersonate events. Using statistical machine learning, IoT event fingerprints from deployed sensors have been used to detect spoofed events. Multivariate temporal data from these sensors has structural and temporal properties that statistical machine learning cannot learn. These schemes' accuracy depends on the knowledge base; the larger, the more accurate. However, the lack of huge datasets with enough samples of each IoT event in the nascent field of IoT can be a bottleneck. In this work, we deployed advanced machine learning to detect event-spoofing assaults. The temporal nature of sensor data lets us discover important patterns with fewer events. Our rigorous investigation of a publicly available real-world dataset indicates that our time-series-based solution technique learns temporal features from sensor data faster than earlier work, even with a 100- or 500-fold smaller training sample, making it a realistic IoT solution.         ",
    "url": "https://arxiv.org/abs/2407.19662",
    "authors": [
      "Uzma Maroof",
      "Gustavo Batista",
      "Arash Shaghaghi",
      "Sanjay Jha"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.19668",
    "title": "Urban Traffic Accident Risk Prediction Revisited: Regionality, Proximity, Similarity and Sparsity",
    "abstract": "           Traffic accidents pose a significant risk to human health and property safety. Therefore, to prevent traffic accidents, predicting their risks has garnered growing interest. We argue that a desired prediction solution should demonstrate resilience to the complexity of traffic accidents. In particular, it should adequately consider the regional background, accurately capture both spatial proximity and semantic similarity, and effectively address the sparsity of traffic accidents. However, these factors are often overlooked or difficult to incorporate. In this paper, we propose a novel multi-granularity hierarchical spatio-temporal network. Initially, we innovate by incorporating remote sensing data, facilitating the creation of hierarchical multi-granularity structure and the comprehension of regional background. We construct multiple high-level risk prediction tasks to enhance model's ability to cope with sparsity. Subsequently, to capture both spatial proximity and semantic similarity, region feature and multi-view graph undergo encoding processes to distill effective representations. Additionally, we propose message passing and adaptive temporal attention module that bridges different granularities and dynamically captures time correlations inherent in traffic accident patterns. At last, a multivariate hierarchical loss function is devised considering the complexity of the prediction purpose. Extensive experiments on two real datasets verify the superiority of our model against the state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2407.19668",
    "authors": [
      "Minxiao Chen",
      "Haitao Yuan",
      "Nan Jiang",
      "Zhifeng Bao",
      "Shangguang Wang"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19669",
    "title": "mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval",
    "abstract": "           We present systematic efforts in building long-context multilingual text representation model (TRM) and reranker from scratch for text retrieval. We first introduce a text encoder (base size) enhanced with RoPE and unpadding, pre-trained in a native 8192-token context (longer than 512 of previous multilingual encoders). Then we construct a hybrid TRM and a cross-encoder reranker by contrastive learning. Evaluations show that our text encoder outperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM and reranker match the performance of large-sized state-of-the-art BGE-M3 models and achieve better results on long-context retrieval benchmarks. Further analysis demonstrate that our proposed models exhibit higher efficiency during both training and inference. We believe their efficiency and effectiveness could benefit various researches and industrial applications.         ",
    "url": "https://arxiv.org/abs/2407.19669",
    "authors": [
      "Xin Zhang",
      "Yanzhao Zhang",
      "Dingkun Long",
      "Wen Xie",
      "Ziqi Dai",
      "Jialong Tang",
      "Huan Lin",
      "Baosong Yang",
      "Pengjun Xie",
      "Fei Huang",
      "Meishan Zhang",
      "Wenjie Li",
      "Min Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.19683",
    "title": "Revisiting the robustness of post-hoc interpretability methods",
    "abstract": "           Post-hoc interpretability methods play a critical role in explainable artificial intelligence (XAI), as they pinpoint portions of data that a trained deep learning model deemed important to make a decision. However, different post-hoc interpretability methods often provide different results, casting doubts on their accuracy. For this reason, several evaluation strategies have been proposed to understand the accuracy of post-hoc interpretability. Many of these evaluation strategies provide a coarse-grained assessment -- i.e., they evaluate how the performance of the model degrades on average by corrupting different data points across multiple samples. While these strategies are effective in selecting the post-hoc interpretability method that is most reliable on average, they fail to provide a sample-level, also referred to as fine-grained, assessment. In other words, they do not measure the robustness of post-hoc interpretability methods. We propose an approach and two new metrics to provide a fine-grained assessment of post-hoc interpretability methods. We show that the robustness is generally linked to its coarse-grained performance.         ",
    "url": "https://arxiv.org/abs/2407.19683",
    "authors": [
      "Jiawen Wei",
      "Hugues Turb\u00e9",
      "Gianmarco Mengaldo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19688",
    "title": "Causal Interventional Prediction System for Robust and Explainable Effect Forecasting",
    "abstract": "           Although the widespread use of AI systems in today's world is growing, many current AI systems are found vulnerable due to hidden bias and missing information, especially in the most commonly used forecasting system. In this work, we explore the robustness and explainability of AI-based forecasting systems. We provide an in-depth analysis of the underlying causality involved in the effect prediction task and further establish a causal graph based on treatment, adjustment variable, confounder, and outcome. Correspondingly, we design a causal interventional prediction system (CIPS) based on a variational autoencoder and fully conditional specification of multiple imputations. Extensive results demonstrate the superiority of our system over state-of-the-art methods and show remarkable versatility and extensibility in practice.         ",
    "url": "https://arxiv.org/abs/2407.19688",
    "authors": [
      "Zhixuan Chu",
      "Hui Ding",
      "Guang Zeng",
      "Shiyu Wang",
      "Yiming Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2407.19692",
    "title": "High-Order Fusion Graph Contrastive Learning for Recommendation",
    "abstract": "           Self-supervised learning (SSL) has recently attracted significant attention in the field of recommender systems. Contrastive learning (CL) stands out as a major SSL paradigm due to its robust ability to generate self-supervised signals. Mainstream graph contrastive learning (GCL)-based methods typically implement CL by creating contrastive views through various data augmentation techniques. Despite these methods are effective, we argue that there still exist several challenges: i) Data augmentation (e.g., discarding edges or adding noise) necessitates additional graph convolution (GCN) or modeling operations, which are highly time-consuming and potentially harm the embedding quality. ii) Existing CL-based methods use traditional CL objectives to capture self-supervised signals. However, few studies have explored obtaining CL objectives from more perspectives and have attempted to fuse the varying signals from these CL objectives to enhance recommendation performance. To overcome these challenges, we propose a High-Order Fusion Graph Contrastive Learning (HFGCL) framework for recommendation. Specifically, we discards the data augmentations and instead high-order information from GCN process to create contrastive views. Additionally, to integrate self-supervised signals from various CL objectives, we propose an advanced CL objective. By ensuring that positive pairs are distanced from negative samples derived from both contrastive views, we effectively fuse self-supervised signals from distinct CL objectives, thereby enhancing the mutual information between positive pairs. Experimental results on three public datasets demonstrate the superior effectiveness of HFGCL compared to the state-of-the-art baselines.         ",
    "url": "https://arxiv.org/abs/2407.19692",
    "authors": [
      "Yu Zhang",
      "Lei Sang",
      "Yi Zhang",
      "Yiwen Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.19694",
    "title": "Structural damage detection via hierarchical damage information with volumetric assessment",
    "abstract": "           Image environments and noisy labels hinder deep learning-based inference models in structural damage detection. Post-detection, there is the challenge of reliance on manual assessments of detected damages. As a result, Guided-DetNet, characterized by Generative Attention Module (GAM), Hierarchical Elimination Algorithm (HEA), and Volumetric Contour Visual Assessment (VCVA), is proposed to mitigate complex image environments, noisy labeling, and post-detection manual assessment of structural damages. GAM leverages cross-horizontal and cross-vertical patch merging and cross foreground-background feature fusion to generate varied features to mitigate complex image environments. HEA addresses noisy labeling using hierarchical relationships among classes to refine instances given an image by eliminating unlikely class categories. VCVA assesses the severity of detected damages via volumetric representation and quantification leveraging the Dirac delta distribution. A comprehensive quantitative study, two robustness tests, and an application scenario based on the PEER Hub Image-Net dataset substantiate Guided-DetNet's promising performances. Guided-DetNet outperformed the best-compared models in a triple classification task by a difference of not less than 3% and not less than 2% in a dual detection task under varying metrics.         ",
    "url": "https://arxiv.org/abs/2407.19694",
    "authors": [
      "Isaac Osei Agyemang",
      "Jianwen Chen",
      "Liaoyuan Zeng",
      "Isaac Adjei-Mensah",
      "Daniel Acheampong",
      "Gordon Owusu Boateng",
      "Adu Asare Baffour"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19696",
    "title": "Cross-Layer Feature Pyramid Transformer for Small Object Detection in Aerial Images",
    "abstract": "           Object detection in aerial images has always been a challenging task due to the generally small size of the objects. Most current detectors prioritize novel detection frameworks, often overlooking research on fundamental components such as feature pyramid networks. In this paper, we introduce the Cross-Layer Feature Pyramid Transformer (CFPT), a novel upsampler-free feature pyramid network designed specifically for small object detection in aerial images. CFPT incorporates two meticulously designed attention blocks with linear computational complexity: the Cross-Layer Channel-Wise Attention (CCA) and the Cross-Layer Spatial-Wise Attention (CSA). CCA achieves cross-layer interaction by dividing channel-wise token groups to perceive cross-layer global information along the spatial dimension, while CSA completes cross-layer interaction by dividing spatial-wise token groups to perceive cross-layer global information along the channel dimension. By integrating these modules, CFPT enables cross-layer interaction in one step, thereby avoiding the semantic gap and information loss associated with element-wise summation and layer-by-layer transmission. Furthermore, CFPT incorporates global contextual information, which enhances detection performance for small objects. To further enhance location awareness during cross-layer interaction, we propose the Cross-Layer Consistent Relative Positional Encoding (CCPE) based on inter-layer mutual receptive fields. We evaluate the effectiveness of CFPT on two challenging object detection datasets in aerial images, namely VisDrone2019-DET and TinyPerson. Extensive experiments demonstrate the effectiveness of CFPT, which outperforms state-of-the-art feature pyramid networks while incurring lower computational costs. The code will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.19696",
    "authors": [
      "Zewen Du",
      "Zhenjiang Hu",
      "Guiyu Zhao",
      "Ying Jin",
      "Hongbin Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19697",
    "title": "Multiscale Representation Enhanced Temporal Flow Fusion Model for Long-Term Workload Forecasting",
    "abstract": "           Accurate workload forecasting is critical for efficient resource management in cloud computing systems, enabling effective scheduling and autoscaling. Despite recent advances with transformer-based forecasting models, challenges remain due to the non-stationary, nonlinear characteristics of workload time series and the long-term dependencies. In particular, inconsistent performance between long-term history and near-term forecasts hinders long-range predictions. This paper proposes a novel framework leveraging self-supervised multiscale representation learning to capture both long-term and near-term workload patterns. The long-term history is encoded through multiscale representations while the near-term observations are modeled via temporal flow fusion. These representations of different scales are fused using an attention mechanism and characterized with normalizing flows to handle non-Gaussian/non-linear distributions of time series. Extensive experiments on 9 benchmarks demonstrate superiority over existing methods.         ",
    "url": "https://arxiv.org/abs/2407.19697",
    "authors": [
      "Shiyu Wang",
      "Zhixuan Chu",
      "Yinbo Sun",
      "Yu Liu",
      "Yuliang Guo",
      "Yang Chen",
      "Huiyang Jian",
      "Lintao Ma",
      "Xingyu Lu",
      "Jun Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19698",
    "title": "Classification Matters: Improving Video Action Detection with Class-Specific Attention",
    "abstract": "           Video action detection (VAD) aims to detect actors and classify their actions in a video. We figure that VAD suffers more from classification rather than localization of actors. Hence, we analyze how prevailing methods form features for classification and find that they prioritize actor regions, yet often overlooking the essential contextual information necessary for accurate classification. Accordingly, we propose to reduce the bias toward actor and encourage paying attention to the context that is relevant to each action class. By assigning a class-dedicated query to each action class, our model can dynamically determine where to focus for effective classification. The proposed model demonstrates superior performance on three challenging benchmarks with significantly fewer parameters and less computation.         ",
    "url": "https://arxiv.org/abs/2407.19698",
    "authors": [
      "Jinsung Lee",
      "Taeoh Kim",
      "Inwoong Lee",
      "Minho Shim",
      "Dongyoon Wee",
      "Minsu Cho",
      "Suha Kwak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19707",
    "title": "Neural networks for bifurcation and linear stability analysis of steady states in partial differential equations",
    "abstract": "           This research introduces an extended application of neural networks for solving nonlinear partial differential equations (PDEs). A neural network, combined with a pseudo-arclength continuation, is proposed to construct bifurcation diagrams from parameterized nonlinear PDEs. Additionally, a neural network approach is also presented for solving eigenvalue problems to analyze solution linear stability, focusing on identifying the largest eigenvalue. The effectiveness of the proposed neural network is examined through experiments on the Bratu equation and the Burgers equation. Results from a finite difference method are also presented as comparison. Varying numbers of grid points are employed in each case to assess the behavior and accuracy of both the neural network and the finite difference method. The experimental results demonstrate that the proposed neural network produces better solutions, generates more accurate bifurcation diagrams, has reasonable computational times, and proves effective for linear stability analysis.         ",
    "url": "https://arxiv.org/abs/2407.19707",
    "authors": [
      "Muhammad Luthfi Shahab",
      "Hadi Susanto"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19718",
    "title": "Robust Beamforming Design for Integrated Satellite-Terrestrial Maritime Communications in the Presence of Wave Fluctuation",
    "abstract": "           In order to provide wireless services for wide sea area, this paper designs an integrated satellite-terrestrial maritime communication framework. Specifically, the terrestrial base station (TBS) serves near-shore users, while the low earth orbit (LEO) satellite communicates with off-shore users. We aim to improve the overall performance of integrated satellite-terrestrial maritime communication system. Thus, it makes sense to jointly optimize transmit beamforming at the TBS and LEO satellite. Due to sea wave fluctuation, the obtained channel state information (CSI) is often imperfect. In this context, a robust beamforming design algorithm is proposed with the goal of minimizing the total power consumption of integrated satellite-terrestrial maritime communication system while satisfying quality of service (QoS) requirements. Both theoretical analysis and simulation results confirm the effectiveness of proposed algorithm in maritime communications.         ",
    "url": "https://arxiv.org/abs/2407.19718",
    "authors": [
      "Kaiwei Xiong",
      "Xiaoming Chen",
      "Ming Ying"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.19739",
    "title": "Nomadic Non-Public Networks for 6G: Use Cases and Key Performance Indicators",
    "abstract": "           The landscape of wireless communication systems is evolving rapidly, with a pivotal role envisioned for dynamic network structures and self-organizing networks in upcoming technologies like the 6G mobile communications standard. This evolution is fueled by the growing demand from diverse sectors, including industry, manufacturing, agriculture, and the public sector, each with increasingly specific requirements. The establishment of non-public networks in the current 5G standard has laid a foundation, enabling independent operation within certain frequencies and local limitations, notably for Internet of Things applications. This paper explores the progression from non-public networks to nomadic non-public networks and their significance in the context of the forthcoming 6G era. Building on existing work in dynamic network structures, non-public networks regulations, and alternative technological solutions, this paper introduces specific use cases enhanced by nomadic networks. In addition, relevant Key Performance Indicators are discussed on the basis of the presented use cases. These serve as a starting point for the definition of requirement clusters and thus for a evaluation metric of nomadic non-public networks. This work lays the groundwork for understanding the potential of nomadic non-public networks in the dynamic landscape of 6G wireless communication systems.         ",
    "url": "https://arxiv.org/abs/2407.19739",
    "authors": [
      "Daniel Lindenschmitt",
      "Benedikt Veith",
      "Khurshid Alam",
      "Ainur Aurembekova",
      "Michael Gundall",
      "Mohammad Asif Habibi",
      "Bin Han",
      "Dennis Krummacker",
      "Philipp Rosemann",
      "Hans D. Schotten"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2407.19740",
    "title": "KNOWCOMP POKEMON Team at DialAM-2024: A Two-Stage Pipeline for Detecting Relations in Dialogical Argument Mining",
    "abstract": "           Dialogical Argument Mining(DialAM) is an important branch of Argument Mining(AM). DialAM-2024 is a shared task focusing on dialogical argument mining, which requires us to identify argumentative relations and illocutionary relations among proposition nodes and locution nodes. To accomplish this, we propose a two-stage pipeline, which includes the Two-Step S-Node Prediction Model in Stage 1 and the YA-Node Prediction Model in Stage 2. We also augment the training data in both stages and introduce context in Stage 2. We successfully completed the task and achieved good results. Our team Pokemon ranked 1st in the ARI Focused score and 4th in the Global Focused score.         ",
    "url": "https://arxiv.org/abs/2407.19740",
    "authors": [
      "Zihao Zheng",
      "Zhaowei Wang",
      "Qing Zong",
      "Yangqiu Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19746",
    "title": "Octave-YOLO: Cross frequency detection network with octave convolution",
    "abstract": "           Despite the rapid advancement of object detection algorithms, processing high-resolution images on embedded devices remains a significant challenge. Theoretically, the fully convolutional network architecture used in current real-time object detectors can handle all input resolutions. However, the substantial computational demands required to process high-resolution images render them impractical for real-time applications. To address this issue, real-time object detection models typically downsample the input image for inference, leading to a loss of detail and decreased accuracy. In response, we developed Octave-YOLO, designed to process high-resolution images in real-time within the constraints of embedded systems. We achieved this through the introduction of the cross frequency partial network (CFPNet), which divides the input feature map into low-resolution, low-frequency, and high-resolution, high-frequency sections. This configuration enables complex operations such as convolution bottlenecks and self-attention to be conducted exclusively on low-resolution feature maps while simultaneously preserving the details in high-resolution maps. Notably, this approach not only dramatically reduces the computational demands of convolution tasks but also allows for the integration of attention modules, which are typically challenging to implement in real-time applications, with minimal additional cost. Additionally, we have incorporated depthwise separable convolution into the core building blocks and downsampling layers to further decrease latency. Experimental results have shown that Octave-YOLO matches the performance of YOLOv8 while significantly reducing computational demands. For example, in 1080x1080 resolution, Octave-YOLO-N is 1.56 times faster than YOLOv8, achieving nearly the same accuracy on the COCO dataset with approximately 40 percent fewer parameters and FLOPs.         ",
    "url": "https://arxiv.org/abs/2407.19746",
    "authors": [
      "Sangjune Shin",
      "Dongkun Shin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19752",
    "title": "Contextuality Helps Representation Learning for Generalized Category Discovery",
    "abstract": "           This paper introduces a novel approach to Generalized Category Discovery (GCD) by leveraging the concept of contextuality to enhance the identification and classification of categories in unlabeled datasets. Drawing inspiration from human cognition's ability to recognize objects within their context, we propose a dual-context based method. Our model integrates two levels of contextuality: instance-level, where nearest-neighbor contexts are utilized for contrastive learning, and cluster-level, employing prototypical contrastive learning based on category prototypes. The integration of the contextual information effectively improves the feature learning and thereby the classification accuracy of all categories, which better deals with the real-world datasets. Different from the traditional semi-supervised and novel category discovery techniques, our model focuses on a more realistic and challenging scenario where both known and novel categories are present in the unlabeled data. Extensive experimental results on several benchmark data sets demonstrate that the proposed model outperforms the state-of-the-art. Code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2407.19752",
    "authors": [
      "Tingzhang Luo",
      "Mingxuan Du",
      "Jiatao Shi",
      "Xinxiang Chen",
      "Bingchen Zhao",
      "Shaoguang Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19753",
    "title": "PredIN: Towards Open-Set Gesture Recognition via Prediction Inconsistency",
    "abstract": "           Gesture recognition based on surface electromyography (sEMG) has achieved significant progress in human-machine interaction (HMI). However, accurately recognizing predefined gestures within a closed set is still inadequate in practice; a robust open-set system needs to effectively reject unknown gestures while correctly classifying known ones. To handle this challenge, we first report prediction inconsistency discovered for unknown classes due to ensemble diversity, which can significantly facilitate the detection of unknown classes. Based on this insight, we propose an ensemble learning approach, PredIN, to explicitly magnify the prediction inconsistency by enhancing ensemble diversity. Specifically, PredIN maximizes the class feature distribution inconsistency among ensemble members to enhance diversity. Meanwhile, it optimizes inter-class separability within an individual ensemble member to maintain individual performance. Comprehensive experiments on various benchmark datasets demonstrate that the PredIN outperforms state-of-the-art methods by a clear margin.Our proposed method simultaneously achieves accurate closed-set classification for predefined gestures and effective rejection for unknown gestures, exhibiting its efficacy and superiority in open-set gesture recognition based on sEMG.         ",
    "url": "https://arxiv.org/abs/2407.19753",
    "authors": [
      "Chen Liu",
      "Can Han",
      "Chengfeng Zhou",
      "Crystal Cai",
      "Dahong Qian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.19760",
    "title": "Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional Principles in Complex Scenarios",
    "abstract": "           In this paper, we conduct an empirical analysis of how large language models (LLMs), specifically GPT-4, interpret constitutional principles in complex decision-making scenarios. We examine rulings from the Italian Constitutional Court on bioethics issues that involve trade-offs between competing values and compare model-generated legal arguments on these issues to those presented by the State, the Court, and the applicants. Our results indicate that GPT-4 consistently aligns more closely with progressive interpretations of the Constitution, often overlooking competing values and mirroring the applicants' views rather than the more conservative perspectives of the State or the Court's moderate positions. Our experiments reveal a distinct tendency of GPT-4 to favor progressive legal interpretations, underscoring the influence of underlying data biases. We thus underscore the importance of testing alignment in real-world scenarios and considering the implications of deploying LLMs in decision-making processes.         ",
    "url": "https://arxiv.org/abs/2407.19760",
    "authors": [
      "Camilla Bignotti",
      "Carolina Camassa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2407.19768",
    "title": "Efficient Face Super-Resolution via Wavelet-based Feature Enhancement Network",
    "abstract": "           Face super-resolution aims to reconstruct a high-resolution face image from a low-resolution face image. Previous methods typically employ an encoder-decoder structure to extract facial structural features, where the direct downsampling inevitably introduces distortions, especially to high-frequency features such as edges. To address this issue, we propose a wavelet-based feature enhancement network, which mitigates feature distortion by losslessly decomposing the input feature into high and low-frequency components using the wavelet transform and processing them separately. To improve the efficiency of facial feature extraction, a full domain Transformer is further proposed to enhance local, regional, and global facial features. Such designs allow our method to perform better without stacking many modules as previous methods did. Experiments show that our method effectively balances performance, model size, and speed. Code link: this https URL.         ",
    "url": "https://arxiv.org/abs/2407.19768",
    "authors": [
      "Wenjie Li",
      "Heng Guo",
      "Xuannan Liu",
      "Kongming Liang",
      "Jiani Hu",
      "Zhanyu Ma",
      "Jun Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19772",
    "title": "Generating Unseen Code Tests In Infinitum",
    "abstract": "           Large Language Models (LLMs) are used for many tasks, including those related to coding. An important aspect of being able to utilize LLMs is the ability to assess their fitness for specific usages. The common practice is to evaluate LLMs against a set of benchmarks. While benchmarks provide a sound foundation for evaluation and comparison of alternatives, they suffer from the well-known weakness of leaking into the training data \\cite{Xu2024Benchmarking}. We present a method for creating benchmark variations that generalize across coding tasks and programming languages, and may also be applied to in-house code bases. Our approach enables ongoing generation of test-data thus mitigating the leaking into the training data issue. We implement one benchmark, called \\textit{auto-regression}, for the task of text-to-code generation in Python. Auto-regression is specifically created to aid in debugging and in tracking model generation changes as part of the LLM regression testing process.         ",
    "url": "https://arxiv.org/abs/2407.19772",
    "authors": [
      "Marcel Zalmanovici",
      "Orna Raz",
      "Eitan Farchi",
      "Iftach Freund"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19775",
    "title": "Model Agnostic Hybrid Sharding For Heterogeneous Distributed Inference",
    "abstract": "           The rapid growth of large-scale AI models, particularly large language models has brought significant challenges in data privacy, computational resources, and accessibility. Traditional centralized architectures often struggle to meet required data security and scalability needs which hinders the democratization of AI systems. Nesa introduces a model-agnostic sharding framework designed for decentralized AI inference. Our framework uses blockchain-based sequential deep neural network sharding to distribute computational tasks across a diverse network of nodes based on a personalised heuristic and routing mechanism. This enables efficient distributed training and inference for recent large-scale models even on consumer-grade hardware. We use compression techniques like dynamic blockwise quantization and mixed matrix decomposition to reduce data transfer and memory needs. We also integrate robust security measures, including hardware-based trusted execution environments to ensure data integrity and confidentiality. Evaluating our system across various natural language processing and vision tasks shows that these compression strategies do not compromise model accuracy. Our results highlight the potential to democratize access to cutting-edge AI technologies by enabling secure and efficient inference on a decentralized network.         ",
    "url": "https://arxiv.org/abs/2407.19775",
    "authors": [
      "Claudio Angione",
      "Yue Zhao",
      "Harry Yang",
      "Ahmad Farhan",
      "Fielding Johnston",
      "James Buban",
      "Patrick Colangelo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2407.19789",
    "title": "Interpreting Low-level Vision Models with Causal Effect Maps",
    "abstract": "           Deep neural networks have significantly improved the performance of low-level vision tasks but also increased the difficulty of interpretability. A deep understanding of deep models is beneficial for both network design and practical reliability. To take up this challenge, we introduce causality theory to interpret low-level vision models and propose a model-/task-agnostic method called Causal Effect Map (CEM). With CEM, we can visualize and quantify the input-output relationships on either positive or negative effects. After analyzing various low-level vision tasks with CEM, we have reached several interesting insights, such as: (1) Using more information of input images (e.g., larger receptive field) does NOT always yield positive outcomes. (2) Attempting to incorporate mechanisms with a global receptive field (e.g., channel attention) into image denoising may prove futile. (3) Integrating multiple tasks to train a general model could encourage the network to prioritize local information over global context. Based on the causal effect theory, the proposed diagnostic tool can refresh our common knowledge and bring a deeper understanding of low-level vision models. Codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.19789",
    "authors": [
      "Jinfan Hu",
      "Jinjin Gu",
      "Shiyao Yu",
      "Fanghua Yu",
      "Zheyuan Li",
      "Zhiyuan You",
      "Chaochao Lu",
      "Chao Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19802",
    "title": "A novel Taguchi-based approach for optimizing neural network architectures: application to elastic short fiber composites",
    "abstract": "           This study presents an innovative application of the Taguchi design of experiment method to optimize the structure of an Artificial Neural Network (ANN) model for the prediction of elastic properties of short fiber reinforced composites. The main goal is to minimize the required computational effort for hyperparameter optimization while enhancing the prediction accuracy. Utilizing a robust design of experiment framework, the structure of an ANN model is optimized. This essentially is the identification of a combination of hyperparameters that yields an optimal predictive accuracy with the fewest algorithmic runs, thereby achieving a significant reduction of the required computational effort. Our findings demonstrate that the Taguchi method not only streamlines the hyperparameter tuning process but also could substantially improve the algorithm's performance. These results underscore the potential of the Taguchi method as a powerful tool for optimizing machine learning algorithms, particularly in scenarios where computational resources are limited. The implications of this study are far-reaching, offering insights for future research in the optimization of different algorithms for improved accuracies and computational efficiencies.         ",
    "url": "https://arxiv.org/abs/2407.19802",
    "authors": [
      "Mohammad Hossein Nikzad",
      "Mohammad Heidari-Rarani",
      "Mohsen Mirkhalaf"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2407.19819",
    "title": "Detecting Unsafe Behavior in Neural Network Imitation Policies for Caregiving Robotics",
    "abstract": "           In this paper, the application of imitation learning in caregiving robotics is explored, aiming at addressing the increasing demand for automated assistance in caring for the elderly and disabled. Leveraging advancements in deep learning and control algorithms, the study focuses on training neural network policies using offline demonstrations. A key challenge addressed is the \"Policy Stopping\" problem, crucial for enhancing safety in imitation learning-based policies, particularly diffusion policies. Novel solutions proposed include ensemble predictors and adaptations of the normalizing flow-based algorithm for early anomaly detection. Comparative evaluations against anomaly detection methods like VAE and Tran-AD demonstrate superior performance on assistive robotics benchmarks. The paper concludes by discussing the further research in integrating safety models into policy training, crucial for the reliable deployment of neural network policies in caregiving robotics.         ",
    "url": "https://arxiv.org/abs/2407.19819",
    "authors": [
      "Andrii Tytarenko"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2407.19820",
    "title": "ActivityCLIP: Enhancing Group Activity Recognition by Mining Complementary Information from Text to Supplement Image Modality",
    "abstract": "           Previous methods usually only extract the image modality's information to recognize group activity. However, mining image information is approaching saturation, making it difficult to extract richer information. Therefore, extracting complementary information from other modalities to supplement image information has become increasingly important. In fact, action labels provide clear text information to express the action's semantics, which existing methods often overlook. Thus, we propose ActivityCLIP, a plug-and-play method for mining the text information contained in the action labels to supplement the image information for enhancing group activity recognition. ActivityCLIP consists of text and image branches, where the text branch is plugged into the image branch (The off-the-shelf image-based method). The text branch includes Image2Text and relation modeling modules. Specifically, we propose the knowledge transfer module, Image2Text, which adapts image information into text information extracted by CLIP via knowledge distillation. Further, to keep our method convenient, we add fewer trainable parameters based on the relation module of the image branch to model interaction relation in the text branch. To show our method's generality, we replicate three representative methods by ActivityCLIP, which adds only limited trainable parameters, achieving favorable performance improvements for each method. We also conduct extensive ablation studies and compare our method with state-of-the-art methods to demonstrate the effectiveness of ActivityCLIP.         ",
    "url": "https://arxiv.org/abs/2407.19820",
    "authors": [
      "Guoliang Xu",
      "Jianqin Yin",
      "Feng Zhou",
      "Yonghao Dang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19828",
    "title": "Federated Learning based Latent Factorization of Tensors for Privacy-Preserving QoS Prediction",
    "abstract": "           In applications related to big data and service computing, dynamic connections tend to be encountered, especially the dynamic data of user-perspective quality of service (QoS) in Web services. They are transformed into high-dimensional and incomplete (HDI) tensors which include abundant temporal pattern information. Latent factorization of tensors (LFT) is an extremely efficient and typical approach for extracting such patterns from an HDI tensor. However, current LFT models require the QoS data to be maintained in a central place (e.g., a central server), which is impossible for increasingly privacy-sensitive users. To address this problem, this article creatively designs a federated learning based on latent factorization of tensors (FL-LFT). It builds a data-density -oriented federated learning model to enable isolated users to collaboratively train a global LFT model while protecting user's privacy. Extensive experiments on a QoS dataset collected from the real world verify that FL-LFT shows a remarkable increase in prediction accuracy when compared to state-of-the-art federated learning (FL) approaches.         ",
    "url": "https://arxiv.org/abs/2407.19828",
    "authors": [
      "Shuai Zhong",
      "Zengtong Tang",
      "Di Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.19845",
    "title": "BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning",
    "abstract": "           As an emerging approach to explore the vulnerability of deep neural networks (DNNs), backdoor learning has attracted increasing interest in recent years, and many seminal backdoor attack and defense algorithms are being developed successively or concurrently, in the status of a rapid arms race. However, mainly due to the diverse settings, and the difficulties of implementation and reproducibility of existing works, there is a lack of a unified and standardized benchmark of backdoor learning, causing unfair comparisons or unreliable conclusions (e.g., misleading, biased or even false conclusions). Consequently, it is difficult to evaluate the current progress and design the future development roadmap of this literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. Our benchmark makes three valuable contributions to the research community. 1) We provide an integrated implementation of state-of-the-art (SOTA) backdoor learning algorithms (currently including 20 attack and 32 defense algorithms), based on an extensible modular-based codebase. 2) We conduct comprehensive evaluations with 5 poisoning ratios, based on 4 models and 4 datasets, leading to 11,492 pairs of attack-against-defense evaluations in total. 3) Based on above evaluations, we present abundant analysis from 10 perspectives via 18 useful analysis tools, and provide several inspiring insights about backdoor learning. We hope that our efforts could build a solid foundation of backdoor learning to facilitate researchers to investigate existing algorithms, develop more innovative algorithms, and explore the intrinsic mechanism of backdoor learning. Finally, we have created a user-friendly website at this http URL, which collects all important information of BackdoorBench, including codebase, docs, leaderboard, and model Zoo.         ",
    "url": "https://arxiv.org/abs/2407.19845",
    "authors": [
      "Baoyuan Wu",
      "Hongrui Chen",
      "Mingda Zhang",
      "Zihao Zhu",
      "Shaokui Wei",
      "Danni Yuan",
      "Mingli Zhu",
      "Ruotong Wang",
      "Li Liu",
      "Chao Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.19847",
    "title": "Brain-inspired polymer dendrite networks for morphology-dependent computing hardware",
    "abstract": "           Variability has always been a challenge to mitigate in electronics. This especially holds true for organic semiconductors, where reproducibility and long-term stability concerns hinder industrialization. By relying on a bio-inspired computing paradigm, we show that AC-electropolymerization is a powerful platform for the development of morphology-dependent computing hardware. Our findings reveal that electropolymerized polymer dendrite networks exhibit a complex relationship between structure and operation that allows them to implement nearly linear to nonlinear functions depending on the complexity of their structure. Moreover, dendritic networks can integrate a limitless number of inputs from their environment, for which their unique morphologies induce specific patterns in the dynamic encoding of the network's output. We demonstrate that this property can be used to our advantage in the context of in materio computing to discriminate between different spatiotemporal inputs. These results show how, due to its inherent stochasticity, electropolymerization is a pivotal technique for the bottom-up implementation of computationally powerful objects. We anticipate this study will help shifting the negative perception of variability in the material science community and promote the electropolymerization framework as a foundation for the development of a new generation of hardware defined by its topological richness.         ",
    "url": "https://arxiv.org/abs/2407.19847",
    "authors": [
      "Scholaert Corentin",
      "Coffinier Yannick",
      "Pecqueur S\u00e9bastien",
      "Alibart Fabien"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Applied Physics (physics.app-ph)"
    ]
  },
  {
    "id": "arXiv:2407.19849",
    "title": "Normality Addition via Normality Detection in Industrial Image Anomaly Detection Models",
    "abstract": "           The task of image anomaly detection (IAD) aims to identify deviations from normality in image data. These anomalies are patterns that deviate significantly from what the IAD model has learned from the data during training. However, in real-world scenarios, the criteria for what constitutes normality often change, necessitating the reclassification of previously anomalous instances as normal. To address this challenge, we propose a new scenario termed \"normality addition,\" involving the post-training adjustment of decision boundaries to incorporate new normalities. To address this challenge, we propose a method called Normality Addition via Normality Detection (NAND), leveraging a vision-language model. NAND performs normality detection which detect patterns related to the intended normality within images based on textual descriptions. We then modify the results of a pre-trained IAD model to implement this normality addition. Using the benchmark dataset in IAD, MVTec AD, we establish an evaluation protocol for the normality addition task and empirically demonstrate the effectiveness of the NAND method.         ",
    "url": "https://arxiv.org/abs/2407.19849",
    "authors": [
      "Jihun Yi",
      "Dahuin Jung",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19875",
    "title": "Exploring Robust Face-Voice Matching in Multilingual Environments",
    "abstract": "           This paper presents Team Xaiofei's innovative approach to exploring Face-Voice Association in Multilingual Environments (FAME) at ACM Multimedia 2024. We focus on the impact of different languages in face-voice matching by building upon Fusion and Orthogonal Projection (FOP), introducing four key components: a dual-branch structure, dynamic sample pair weighting, robust data augmentation, and score polarization strategy. Our dual-branch structure serves as an auxiliary mechanism to better integrate and provide more comprehensive information. We also introduce a dynamic weighting mechanism for various sample pairs to optimize learning. Data augmentation techniques are employed to enhance the model's generalization across diverse conditions. Additionally, score polarization strategy based on age and gender matching confidence clarifies and accentuates the final results. Our methods demonstrate significant effectiveness, achieving an equal error rate (EER) of 20.07 on the V2-EH dataset and 21.76 on the V1-EU dataset.         ",
    "url": "https://arxiv.org/abs/2407.19875",
    "authors": [
      "Jiehui Tang",
      "Xiaofei Wang",
      "Zhen Xiao",
      "Jiayi Liu",
      "Xueliang Liu",
      "Richang Hong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19877",
    "title": "Language-driven Grasp Detection with Mask-guided Attention",
    "abstract": "           Grasp detection is an essential task in robotics with various industrial applications. However, traditional methods often struggle with occlusions and do not utilize language for grasping. Incorporating natural language into grasp detection remains a challenging task and largely unexplored. To address this gap, we propose a new method for language-driven grasp detection with mask-guided attention by utilizing the transformer attention mechanism with semantic segmentation features. Our approach integrates visual data, segmentation mask features, and natural language instructions, significantly improving grasp detection accuracy. Our work introduces a new framework for language-driven grasp detection, paving the way for language-driven robotic applications. Intensive experiments show that our method outperforms other recent baselines by a clear margin, with a 10.0% success score improvement. We further validate our method in real-world robotic experiments, confirming the effectiveness of our approach.         ",
    "url": "https://arxiv.org/abs/2407.19877",
    "authors": [
      "Tuan Van Vo",
      "Minh Nhat Vu",
      "Baoru Huang",
      "An Vuong",
      "Ngan Le",
      "Thieu Vo",
      "Anh Nguyen"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19886",
    "title": "A Unified Graph Transformer for Overcoming Isolations in Multi-modal Recommendation",
    "abstract": "           With the rapid development of online multimedia services, especially in e-commerce platforms, there is a pressing need for personalised recommendation systems that can effectively encode the diverse multi-modal content associated with each item. However, we argue that existing multi-modal recommender systems typically use isolated processes for both feature extraction and modality modelling. Such isolated processes can harm the recommendation performance. Firstly, an isolated extraction process underestimates the importance of effective feature extraction in multi-modal recommendations, potentially incorporating non-relevant information, which is harmful to item representations. Second, an isolated modality modelling process produces disjointed embeddings for item modalities due to the individual processing of each modality, which leads to a suboptimal fusion of user/item representations for effective user preferences prediction. We hypothesise that the use of a unified model for addressing both aforementioned isolated processes will enable the consistent extraction and cohesive fusion of joint multi-modal features, thereby enhancing the effectiveness of multi-modal recommender systems. In this paper, we propose a novel model, called Unified Multi-modal Graph Transformer (UGT), which firstly leverages a multi-way transformer to extract aligned multi-modal features from raw data for top-k recommendation. Subsequently, we build a unified graph neural network in our UGT model to jointly fuse the user/item representations with their corresponding multi-modal features. Using the graph transformer architecture of our UGT model, we show that the UGT model can achieve significant effectiveness gains, especially when jointly optimised with the commonly-used multi-modal recommendation losses.         ",
    "url": "https://arxiv.org/abs/2407.19886",
    "authors": [
      "Zixuan Yi",
      "Iadh Ounis"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19889",
    "title": "Self-Supervised Learning for Text Recognition: A Critical Survey",
    "abstract": "           Text Recognition (TR) refers to the research area that focuses on retrieving textual information from images, a topic that has seen significant advancements in the last decade due to the use of Deep Neural Networks (DNN). However, these solutions often necessitate vast amounts of manually labeled or synthetic data. Addressing this challenge, Self-Supervised Learning (SSL) has gained attention by utilizing large datasets of unlabeled data to train DNN, thereby generating meaningful and robust representations. Although SSL was initially overlooked in TR because of its unique characteristics, recent years have witnessed a surge in the development of SSL methods specifically for this field. This rapid development, however, has led to many methods being explored independently, without taking previous efforts in methodology or comparison into account, thereby hindering progress in the field of research. This paper, therefore, seeks to consolidate the use of SSL in the field of TR, offering a critical and comprehensive overview of the current state of the art. We will review and analyze the existing methods, compare their results, and highlight inconsistencies in the current literature. This thorough analysis aims to provide general insights into the field, propose standardizations, identify new research directions, and foster its proper development.         ",
    "url": "https://arxiv.org/abs/2407.19889",
    "authors": [
      "Carlos Penarrubia",
      "Jose J. Valero-Mas",
      "Jorge Calvo-Zaragoza"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19907",
    "title": "Neural Control Barrier Functions for Safe Navigation",
    "abstract": "           Autonomous robot navigation can be particularly demanding, especially when the surrounding environment is not known and safety of the robot is crucial. This work relates to the synthesis of Control Barrier Functions (CBFs) through data for safe navigation in unknown environments. A novel methodology to jointly learn CBFs and corresponding safe controllers, in simulation, inspired by the State Dependent Riccati Equation (SDRE) is proposed. The CBF is used to obtain admissible commands from any nominal, possibly unsafe controller. An approach to apply the CBF inside a safety filter without the need for a consistent map or position estimate is developed. Subsequently, the resulting reactive safety filter is deployed on a multirotor platform integrating a LiDAR sensor both in simulation and real-world experiments.         ",
    "url": "https://arxiv.org/abs/2407.19907",
    "authors": [
      "Marvin Harms",
      "Mihir Kulkarni",
      "Nikhil Khedekar",
      "Martin Jacquet",
      "Kostas Alexis"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2407.19916",
    "title": "Aero-Nef: Neural Fields for Rapid Aircraft Aerodynamics Simulations",
    "abstract": "           This paper presents a methodology to learn surrogate models of steady state fluid dynamics simulations on meshed domains, based on Implicit Neural Representations (INRs). The proposed models can be applied directly to unstructured domains for different flow conditions, handle non-parametric 3D geometric variations, and generalize to unseen shapes at test time. The coordinate-based formulation naturally leads to robustness with respect to discretization, allowing an excellent trade-off between computational cost (memory footprint and training time) and accuracy. The method is demonstrated on two industrially relevant applications: a RANS dataset of the two-dimensional compressible flow over a transonic airfoil and a dataset of the surface pressure distribution over 3D wings, including shape, inflow condition, and control surface deflection variations. On the considered test cases, our approach achieves a more than three times lower test error and significantly improves generalization error on unseen geometries compared to state-of-the-art Graph Neural Network architectures. Remarkably, the method can perform inference five order of magnitude faster than the high fidelity solver on the RANS transonic airfoil dataset. Code is available at this https URL ",
    "url": "https://arxiv.org/abs/2407.19916",
    "authors": [
      "Giovanni Catalani",
      "Siddhant Agarwal",
      "Xavier Bertrand",
      "Frederic Tost",
      "Michael Bauerheim",
      "Joseph Morlier"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2407.19934",
    "title": "Graph Fourier Transform Enhancement through Envelope Extensions",
    "abstract": "           Many real-world networks are characterized by directionality; however, the absence of an appropriate Fourier basis hinders the effective implementation of graph signal processing techniques. Inspired by discrete signal processing, where embedding a line digraph into a cycle digraph facilitates the powerful Discrete Fourier Transform for signal analysis, addressing the structural complexities of general digraphs can help overcome the limitations of the Graph Fourier Transform (GFT) and unlock its potential. The Discrete Fourier Transform (DFT) serves as a Graph Fourier Transform for both cycle graphs and Cayley digraphs on the finite cyclic groups $\\mathbb{Z}_N$. We propose a systematic method to identify a class of such Cayley digraphs that can encompass a given directed graph. By embedding the directed graph into these Cayley digraphs and opting for envelope extensions that naturally support the Graph Fourier Transform, the GFT functionalities of these extensions can be harnessed for signal analysis. Among the potential envelopes, optimal performance is achieved by selecting one that meets key properties. This envelope's structure closely aligns with the characteristics of the original digraph. The Graph Fourier Transform of this envelope is reliable in terms of numerical stability, and its columns approximately form an eigenbasis for the adjacency matrix associated with the original digraph. It is shown that the envelope extensions possess a convolution product, with their GFT fulfilling the convolution theorem. Additionally, shift-invariant graph filters (systems) are described as the convolution operator, analogous to the classical case. This allows the utilization of systems for signal analysis.         ",
    "url": "https://arxiv.org/abs/2407.19934",
    "authors": [
      "Ali Bagheri Bardi",
      "Taher Yazdanpanah",
      "Milos Dakovic",
      "Ljubisa Stankovic"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.19938",
    "title": "Robust Conformal Volume Estimation in 3D Medical Images",
    "abstract": "           Volumetry is one of the principal downstream applications of 3D medical image segmentation, for example, to detect abnormal tissue growth or for surgery planning. Conformal Prediction is a promising framework for uncertainty quantification, providing calibrated predictive intervals associated with automatic volume measurements. However, this methodology is based on the hypothesis that calibration and test samples are exchangeable, an assumption that is in practice often violated in medical image applications. A weighted formulation of Conformal Prediction can be framed to mitigate this issue, but its empirical investigation in the medical domain is still lacking. A potential reason is that it relies on the estimation of the density ratio between the calibration and test distributions, which is likely to be intractable in scenarios involving high-dimensional data. To circumvent this, we propose an efficient approach for density ratio estimation relying on the compressed latent representations generated by the segmentation model. Our experiments demonstrate the efficiency of our approach to reduce the coverage error in the presence of covariate shifts, in both synthetic and real-world settings. Our implementation is available at this https URL ",
    "url": "https://arxiv.org/abs/2407.19938",
    "authors": [
      "Benjamin Lambert",
      "Florence Forbes",
      "Senan Doyle",
      "Michel Dojat"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19941",
    "title": "Boosting Graph Foundation Model from Structural Perspective",
    "abstract": "           Graph foundation models have recently attracted significant attention due to its strong generalizability. Although existing methods resort to language models to learn unified semantic representations across domains, they disregard the unique structural characteristics of graphs from different domains. To address the problem, in this paper, we boost graph foundation model from structural perspective and propose BooG. The model constructs virtual super nodes to unify structural characteristics of graph data from different domains. Specifically, the super nodes fuse the information of anchor nodes and class labels, where each anchor node captures the information of a node or a graph instance to be classified. Instead of using the raw graph structure, we connect super nodes to all nodes within their neighborhood by virtual edges. This new structure allows for effective information aggregation while unifying cross-domain structural characteristics. Additionally, we propose a novel pre-training objective based on contrastive learning, which learns more expressive representations for graph data and generalizes effectively to different domains and downstream tasks. Experimental results on various datasets and tasks demonstrate the superior performance of BooG. We provide our code and data here: https://anonymous.4open.science/r/BooG-EE42/.         ",
    "url": "https://arxiv.org/abs/2407.19941",
    "authors": [
      "Yao Cheng",
      "Yige Zhao",
      "Jianxiang Yu",
      "Xiang Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19943",
    "title": "Practical and Robust Safety Guarantees for Advanced Counterfactual Learning to Rank",
    "abstract": "           Counterfactual learning to rank (CLTR ) can be risky; various circumstances can cause it to produce sub-optimal models that hurt performance when deployed. Safe CLTR was introduced to mitigate these risks when using inverse propensity scoring to correct for position bias. However, the existing safety measure for CLTR is not applicable to state-of-the-art CLTR, it cannot handle trust bias, and its guarantees rely on specific assumptions about user behavior. Our contributions are two-fold. First, we generalize the existing safe CLTR approach to make it applicable to state-of-the-art doubly robust (DR) CLTR and trust bias. Second, we propose a novel approach, proximal ranking policy optimization (PRPO ), that provides safety in deployment without assumptions about user behavior. PRPO removes incentives for learning ranking behavior that is too dissimilar to a safe ranking model. Thereby, PRPO imposes a limit on how much learned models can degrade performance metrics, without relying on any specific user assumptions. Our experiments show that both our novel safe doubly robust method and PRPO provide higher performance than the existing safe inverse propensity scoring approach. However, when circumstances are unexpected, the safe doubly robust approach can become unsafe and bring detrimental performance. In contrast, PRPO always maintains safety, even in maximally adversarial situations. By avoiding assumptions, PRPO is the first method with unconditional safety in deployment that translates to robust safety for real-world applications.         ",
    "url": "https://arxiv.org/abs/2407.19943",
    "authors": [
      "Shashank Gupta",
      "Harrie Oosterhuis",
      "Maarten de Rijke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.19944",
    "title": "Noise-Resilient Unsupervised Graph Representation Learning via Multi-Hop Feature Quality Estimation",
    "abstract": "           Unsupervised graph representation learning (UGRL) based on graph neural networks (GNNs), has received increasing attention owing to its efficacy in handling graph-structured data. However, existing UGRL methods ideally assume that the node features are noise-free, which makes them fail to distinguish between useful information and noise when applied to real data with noisy features, thus affecting the quality of learned representations. This urges us to take node noisy features into account in real-world UGRL. With empirical analysis, we reveal that feature propagation, the essential operation in GNNs, acts as a \"double-edged sword\" in handling noisy features - it can both denoise and diffuse noise, leading to varying feature quality across nodes, even within the same node at different hops. Building on this insight, we propose a novel UGRL method based on Multi-hop feature Quality Estimation (MQE for short). Unlike most UGRL models that directly utilize propagation-based GNNs to generate representations, our approach aims to learn representations through estimating the quality of propagated features at different hops. Specifically, we introduce a Gaussian model that utilizes a learnable \"meta-representation\" as a condition to estimate the expectation and variance of multi-hop propagated features via neural networks. In this way, the \"meta representation\" captures the semantic and structural information underlying multiple propagated features but is naturally less susceptible to interference by noise, thereby serving as high-quality node representations beneficial for downstream tasks. Extensive experiments on multiple real-world datasets demonstrate that MQE in learning reliable node representations in scenarios with diverse types of feature noise.         ",
    "url": "https://arxiv.org/abs/2407.19944",
    "authors": [
      "Shiyuan Li",
      "Yixin Liu",
      "Qingfeng Chen",
      "Geoffrey I. Webb",
      "Shirui Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19951",
    "title": "Can I trust my anomaly detection system? A case study based on explainable AI",
    "abstract": "           Generative models based on variational autoencoders are a popular technique for detecting anomalies in images in a semi-supervised context. A common approach employs the anomaly score to detect the presence of anomalies, and it is known to reach high level of accuracy on benchmark datasets. However, since anomaly scores are computed from reconstruction disparities, they often obscure the detection of various spurious features, raising concerns regarding their actual efficacy. This case study explores the robustness of an anomaly detection system based on variational autoencoder generative models through the use of eXplainable AI methods. The goal is to get a different perspective on the real performances of anomaly detectors that use reconstruction differences. In our case study we discovered that, in many cases, samples are detected as anomalous for the wrong or misleading factors.         ",
    "url": "https://arxiv.org/abs/2407.19951",
    "authors": [
      "Muhammad Rashid",
      "Elvio Amparore",
      "Enrico Ferrari",
      "Damiano Verda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.19967",
    "title": "A Temporal Psycholinguistics Approach to Identity Resolution of Social Media Users",
    "abstract": "           In this thesis, we propose an approach to identity resolution across social media platforms using the topics, sentiments, and timings of the posts on the platforms. After collecting the public posts of around 5000 profiles from Disqus and Twitter, we analyze their posts to match their profiles across the two platforms. We pursue both temporal and non-temporal methods in our analysis. While neither approach proves definitively superior, the temporal approach generally performs better. We found that the temporal window size influences results more than the shifting amount. On the other hand, our sentiment analysis shows that the inclusion of sentiment makes little difference, probably due to flawed data extraction methods. We also experimented with a distance-based reward-and-punishment-focused scoring model, which achieved an accuracy of 24.198% and an average rank of 158.217 out of 2525 in our collected corpus. Future work includes refining sentiment analysis by evaluating sentiments per topic, extending temporal analysis with additional phases, and improving the scoring model through weight adjustments and modified rewards.         ",
    "url": "https://arxiv.org/abs/2407.19967",
    "authors": [
      "Md Touhidul Islam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.19970",
    "title": "From Flat to Spatial: Comparison of 4 methods constructing 3D, 2 and 1/2D Models from 2D Plans with neural networks",
    "abstract": "           In the field of architecture, the conversion of single images into 2 and 1/2D and 3D meshes is a promising technology that enhances design visualization and efficiency. This paper evaluates four innovative methods: \"One-2-3-45,\" \"CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model,\" \"Instant Mesh,\" and \"Image-to-Mesh.\" These methods are at the forefront of this technology, focusing on their applicability in architectural design and visualization. They streamline the creation of 3D architectural models, enabling rapid prototyping and detailed visualization from minimal initial inputs, such as photographs or simple sketches.One-2-3-45 leverages a diffusion-based approach to generate multi-view reconstructions, ensuring high geometric fidelity and texture quality. CRM utilizes a convolutional network to integrate geometric priors into its architecture, producing detailed and textured meshes quickly and efficiently. Instant Mesh combines the strengths of multi-view diffusion and sparse-view models to offer speed and scalability, suitable for diverse architectural projects. Image-to-Mesh leverages a generative adversarial network (GAN) to produce 3D meshes from single images, focusing on maintaining high texture fidelity and geometric accuracy by incorporating image and depth map data into its training process. It uses a hybrid approach that combines voxel-based representations with surface reconstruction techniques to ensure detailed and realistic 3D models.This comparative study highlights each method's contribution to reducing design cycle times, improving accuracy, and enabling flexible adaptations to various architectural styles and requirements. By providing architects with powerful tools for rapid visualization and iteration, these advancements in 3D mesh generation are set to revolutionize architectural practices.         ",
    "url": "https://arxiv.org/abs/2407.19970",
    "authors": [
      "Jacob Sam",
      "Karan Patel",
      "Mike Saad"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2407.19981",
    "title": "Adversarial Robustness in RGB-Skeleton Action Recognition: Leveraging Attention Modality Reweighter",
    "abstract": "           Deep neural networks (DNNs) have been applied in many computer vision tasks and achieved state-of-the-art (SOTA) performance. However, misclassification will occur when DNNs predict adversarial examples which are created by adding human-imperceptible adversarial noise to natural examples. This limits the application of DNN in security-critical fields. In order to enhance the robustness of models, previous research has primarily focused on the unimodal domain, such as image recognition and video understanding. Although multi-modal learning has achieved advanced performance in various tasks, such as action recognition, research on the robustness of RGB-skeleton action recognition models is scarce. In this paper, we systematically investigate how to improve the robustness of RGB-skeleton action recognition models. We initially conducted empirical analysis on the robustness of different modalities and observed that the skeleton modality is more robust than the RGB modality. Motivated by this observation, we propose the \\formatword{A}ttention-based \\formatword{M}odality \\formatword{R}eweighter (\\formatword{AMR}), which utilizes an attention layer to re-weight the two modalities, enabling the model to learn more robust features. Our AMR is plug-and-play, allowing easy integration with multimodal models. To demonstrate the effectiveness of AMR, we conducted extensive experiments on various datasets. For example, compared to the SOTA methods, AMR exhibits a 43.77\\% improvement against PGD20 attacks on the NTU-RGB+D 60 dataset. Furthermore, it effectively balances the differences in robustness between different modalities.         ",
    "url": "https://arxiv.org/abs/2407.19981",
    "authors": [
      "Chao Liu",
      "Xin Liu",
      "Zitong Yu",
      "Yonghong Hou",
      "Huanjing Yue",
      "Jingyu Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19984",
    "title": "Confidence Estimation for Automatic Detection of Depression and Alzheimer's Disease Based on Clinical Interviews",
    "abstract": "           Speech-based automatic detection of Alzheimer's disease (AD) and depression has attracted increased attention. Confidence estimation is crucial for a trust-worthy automatic diagnostic system which informs the clinician about the confidence of model predictions and helps reduce the risk of misdiagnosis. This paper investigates confidence estimation for automatic detection of AD and depression based on clinical interviews. A novel Bayesian approach is proposed which uses a dynamic Dirichlet prior distribution to model the second-order probability of the predictive distribution. Experimental results on the publicly available ADReSS and DAIC-WOZ datasets demonstrate that the proposed method outperforms a range of baselines for both classification accuracy and confidence estimation.         ",
    "url": "https://arxiv.org/abs/2407.19984",
    "authors": [
      "Wen Wu",
      "Chao Zhang",
      "Philip C. Woodland"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.19987",
    "title": "HOBOTAN: Efficient Higher Order Binary Optimization Solver with Tensor Networks and PyTorch",
    "abstract": "           In this study, we introduce HOBOTAN, a new solver designed for Higher Order Binary Optimization (HOBO). HOBOTAN supports both CPU and GPU, with the GPU version developed based on PyTorch, offering a fast and scalable system. This solver utilizes tensor networks to solve combinatorial optimization problems, employing a HOBO tensor that maps the problem and performs tensor contractions as needed. Additionally, by combining techniques such as batch processing for tensor optimization and binary-based integer encoding, we significantly enhance the efficiency of combinatorial optimization. In the future, the utilization of increased GPU numbers is expected to harness greater computational power, enabling efficient collaboration between multiple GPUs for high scalability. Moreover, HOBOTAN is designed within the framework of quantum computing, thus providing insights for future quantum computer applications. This paper details the design, implementation, performance evaluation, and scalability of HOBOTAN, demonstrating its effectiveness.         ",
    "url": "https://arxiv.org/abs/2407.19987",
    "authors": [
      "Shoya Yasuda",
      "Shunsuke Sotobayashi",
      "Yuichiro Minato"
    ],
    "subjectives": [
      "Mathematical Software (cs.MS)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2407.19994",
    "title": "A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph",
    "abstract": "           This study aims to improve knowledge-based question-answering (QA) systems by overcoming the limitations of existing Retrieval-Augmented Generation (RAG) models and implementing an advanced RAG system based on Graph technology to develop high-quality generative AI services. While existing RAG models demonstrate high accuracy and fluency by utilizing retrieved information, they may suffer from accuracy degradation as they generate responses using pre-loaded knowledge without reprocessing. Additionally, they cannot incorporate real-time data after the RAG configuration stage, leading to issues with contextual understanding and biased information. To address these limitations, this study implemented an enhanced RAG system utilizing Graph technology. This system is designed to efficiently search and utilize information. Specifically, it employs LangGraph to evaluate the reliability of retrieved information and synthesizes diverse data to generate more accurate and enhanced responses. Furthermore, the study provides a detailed explanation of the system's operation, key implementation steps, and examples through implementation code and validation results, thereby enhancing the understanding of advanced RAG technology. This approach offers practical guidelines for implementing advanced RAG systems in corporate services, making it a valuable resource for practical application.         ",
    "url": "https://arxiv.org/abs/2407.19994",
    "authors": [
      "Cheonsu Jeong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.20007",
    "title": "Rosetta Statements: Lowering the Barrier for Semantic Parsing and Increasing the Cognitive Interoperability of Knowledge Graphs",
    "abstract": "           Machines need data and metadata to be machine-actionable and FAIR (findable, accessible, interoperable, reusable) to manage increasing data volumes. Knowledge graphs and ontologies are key to this, but their use is hampered by high access barriers due to required prior knowledge in semantics and data modelling. The Rosetta Statement approach proposes modeling English natural language statements instead of a mind-independent reality. We propose a metamodel for creating semantic schema patterns for simple statement types. The approach supports versioning of statements and provides a detailed editing history. Each Rosetta Statement pattern has a dynamic label for displaying statements as natural language sentences. Implemented in the Open Research Knowledge Graph (ORKG) as a use case, this approach allows domain experts to define data schema patterns without needing semantic knowledge. Future plans include combining Rosetta Statements with semantic units to organize ORKG into meaningful subgraphs, improving usability. A search interface for querying statements without needing SPARQL or Cypher knowledge is also planned, along with tools for data entry and display using Large Language Models and NLP. The Rosetta Statement metamodel supports a two-step knowledge graph construction procedure. Domain experts can model semantic content without support from ontology engineers, lowering entry barriers and increasing cognitive interoperability. The second level involves developing semantic graph patterns for reasoning, requiring collaboration with ontology engineers.         ",
    "url": "https://arxiv.org/abs/2407.20007",
    "authors": [
      "Lars Vogt",
      "Marcel Konrad",
      "Kheir Eddine Farfar",
      "Manuel Prinz",
      "Allard Oelen"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2407.20013",
    "title": "Classification of freshwater snails of the genus \\emph{Radomaniola} with multimodal triplet networks",
    "abstract": "           In this paper, we present our first proposal of a machine learning system for the classification of freshwater snails of the genus \\emph{Radomaniola}. We elaborate on the specific challenges encountered during system design, and how we tackled them; namely a small, very imbalanced dataset with a high number of classes and high visual similarity between classes. We then show how we employed triplet networks and the multiple input modalities of images, measurements, and genetic information to overcome these challenges and reach a performance comparable to that of a trained domain expert.         ",
    "url": "https://arxiv.org/abs/2407.20013",
    "authors": [
      "Dennis Vetter",
      "Muhammad Ahsan",
      "Diana Delicado",
      "Thomas A. Neubauer",
      "Thomas Wilke",
      "Gemma Roig"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.20020",
    "title": "ImagiNet: A Multi-Content Dataset for Generalizable Synthetic Image Detection via Contrastive Learning",
    "abstract": "           Generative models, such as diffusion models (DMs), variational autoencoders (VAEs), and generative adversarial networks (GANs), produce images with a level of authenticity that makes them nearly indistinguishable from real photos and artwork. While this capability is beneficial for many industries, the difficulty of identifying synthetic images leaves online media platforms vulnerable to impersonation and misinformation attempts. To support the development of defensive methods, we introduce ImagiNet, a high-resolution and balanced dataset for synthetic image detection, designed to mitigate potential biases in existing resources. It contains 200K examples, spanning four content categories: photos, paintings, faces, and uncategorized. Synthetic images are produced with open-source and proprietary generators, whereas real counterparts of the same content type are collected from public datasets. The structure of ImagiNet allows for a two-track evaluation system: i) classification as real or synthetic and ii) identification of the generative model. To establish a baseline, we train a ResNet-50 model using a self-supervised contrastive objective (SelfCon) for each track. The model demonstrates state-of-the-art performance and high inference speed across established benchmarks, achieving an AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%, even under social network conditions that involve compression and resizing. Our data and code are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.20020",
    "authors": [
      "Delyan Boychev",
      "Radostin Cholakov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.20026",
    "title": "JAX-SSO: Differentiable Finite Element Analysis Solver for Structural Optimization and Seamless Integration with Neural Networks",
    "abstract": "           Differentiable numerical simulations of physical systems have gained rising attention in the past few years with the development of automatic differentiation tools. This paper presents JAX-SSO, a differentiable finite element analysis solver built with JAX, Google's high-performance computing library, to assist efficient structural design in the built environment. With the adjoint method and automatic differentiation feature, JAX-SSO can efficiently evaluate gradients of physical quantities in an automatic way, enabling accurate sensitivity calculation in structural optimization problems. Written in Python and JAX, JAX-SSO is naturally within the machine learning ecosystem so it can be seamlessly integrated with neural networks to train machine learning models with inclusion of physics. Moreover, JAX-SSO supports GPU acceleration to further boost finite element analysis. Several examples are presented to showcase the capabilities and efficiency of JAX-SSO: i) shape optimization of grid-shells and continuous shells; ii) size (thickness) optimization of continuous shells; iii) simultaneous shape and topology optimization of continuous shells; and iv) training of physics-informed neural networks for structural optimization. We believe that JAX-SSO can facilitate research related to differentiable physics and machine learning to further address problems in structural and architectural design.         ",
    "url": "https://arxiv.org/abs/2407.20026",
    "authors": [
      "Gaoyuan Wu"
    ],
    "subjectives": [
      "Mathematical Software (cs.MS)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2407.20028",
    "title": "Aircraft Trajectory Segmentation-based Contrastive Coding: A Framework for Self-supervised Trajectory Representation",
    "abstract": "           Air traffic trajectory recognition has gained significant interest within the air traffic management community, particularly for fundamental tasks such as classification and clustering. This paper introduces Aircraft Trajectory Segmentation-based Contrastive Coding (ATSCC), a novel self-supervised time series representation learning framework designed to capture semantic information in air traffic trajectory data. The framework leverages the segmentable characteristic of trajectories and ensures consistency within the self-assigned segments. Intensive experiments were conducted on datasets from three different airports, totaling four datasets, comparing the learned representation's performance of downstream classification and clustering with other state-of-the-art representation learning techniques. The results show that ATSCC outperforms these methods by aligning with the labels defined by aeronautical procedures. ATSCC is adaptable to various airport configurations and scalable to incomplete trajectories. This research has expanded upon existing capabilities, achieving these improvements independently without predefined inputs such as airport configurations, maneuvering procedures, or labeled data.         ",
    "url": "https://arxiv.org/abs/2407.20028",
    "authors": [
      "Thaweerath Phisannupawong",
      "Joshua Julian Damanik",
      "Han-Lim Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.20036",
    "title": "Planning For Edge Failure in Fixed-Charge Flow Networks",
    "abstract": "           The Fixed-Charge Network Flow problem is a well-studied NP-hard problem that has the goal of finding a flow in a network where fixed edge costs are incurred, regardless of the amount of flow hosted by the edge. In this paper, we consider scenarios where a designated edge in the network has the potential to fail after edges have already been purchased. If the edge does fail, procurement of additional edges may be required to repair the flow and compensate for the failed edge so as to maintain the original flow amount. We formulate a multi-objective optimization problem that aims to minimize the costs of both the initial flow as well as the repaired flow. We introduce an algorithm that finds the Pareto front between these two objectives, thereby providing decision makers with a sequence of solutions that trade off initial flow cost with repaired flow cost. We demonstrate the algorithm's efficacy with an evaluation using real-world CO2 capture and storage infrastructure data.         ",
    "url": "https://arxiv.org/abs/2407.20036",
    "authors": [
      "Daniel Olson",
      "Caleb Eardley",
      "Sean Yaw"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2407.20042",
    "title": "When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention",
    "abstract": "           Code generation aims to automatically generate code snippets that meet given natural language requirements and plays an important role in software development. Although Code LLMs have shown excellent performance in this domain, their long generation time poses a signification limitation in practice use. In this paper, we first conduct an in-depth preliminary study with different Code LLMs on code generation tasks and identify a significant efficiency issue, i.e., continual generation of excess tokens. It harms the developer productivity and leads to huge computational wastes. To address it, we introduce CodeFast, an inference acceleration approach for Code LLMs on code generation. The key idea of CodeFast is to terminate the inference process in time when unnecessary excess tokens are detected. First, we propose an automatic data construction framework to obtain training data. Then, we train a unified lightweight model GenGuard applicable to multiple programming languages to predict whether to terminate inference at the current step. Finally, we enhance Code LLM with GenGuard to accelerate its inference in code generation tasks. We conduct extensive experiments with CodeFast on five representative Code LLMs across four widely used code generation datasets. Experimental results show that (1) CodeFast can significantly improve the inference speed of various Code LLMs in code generation, ranging form 34% to 452%, without compromising the quality of generated code. (2) CodeFast is stable across different parameter settings and can generalize to untrained datasets. Our code and data are available at this https URL ",
    "url": "https://arxiv.org/abs/2407.20042",
    "authors": [
      "Lianghong Guo",
      "Yanlin Wang",
      "Ensheng Shi",
      "Wanjun Zhong",
      "Hongyu Zhang",
      "Jiachi Chen",
      "Ruikai Zhang",
      "Yuchi Ma",
      "Zibin Zheng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2407.20045",
    "title": "NegotiaToR: Towards A Simple Yet Effective On-demand Reconfigurable Datacenter Network",
    "abstract": "           Recent advances in fast optical switching technology show promise in meeting the high goodput and low latency requirements of datacenter networks (DCN). We present NegotiaToR, a simple network architecture for optical reconfigurable DCNs that utilizes on-demand scheduling to handle dynamic traffic. In NegotiaToR, racks exchange scheduling messages through an in-band control plane and distributedly calculate non-conflicting paths from binary traffic demand information. Optimized for incasts, it also provides opportunities to bypass scheduling delays. NegotiaToR is compatible with prevalent flat topologies, and is tailored towards a minimalist design for on-demand reconfigurable DCNs, enhancing practicality. Through large-scale simulations, we show that NegotiaToR achieves both small mice flow completion time (FCT) and high goodput on two representative flat topologies, especially under heavy loads. Particularly, the FCT of mice flows is one to two orders of magnitude better than the state-of-the-art traffic-oblivious reconfigurable DCN design.         ",
    "url": "https://arxiv.org/abs/2407.20045",
    "authors": [
      "Cong Liang",
      "Xiangli Song",
      "Jing Cheng",
      "Mowei Wang",
      "Yashe Liu",
      "Zhenhua Liu",
      "Shizhen Zhao",
      "Yong Cui"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2407.20047",
    "title": "Denoising ESG: quantifying data uncertainty from missing data with Machine Learning and prediction intervals",
    "abstract": "           Environmental, Social, and Governance (ESG) datasets are frequently plagued by significant data gaps, leading to inconsistencies in ESG ratings due to varying imputation methods. This paper explores the application of established machine learning techniques for imputing missing data in a real-world ESG dataset, emphasizing the quantification of uncertainty through prediction intervals. By employing multiple imputation strategies, this study assesses the robustness of imputation methods and quantifies the uncertainty associated with missing data. The findings highlight the importance of probabilistic machine learning models in providing better understanding of ESG scores, thereby addressing the inherent risks of wrong ratings due to incomplete data. This approach improves imputation practices to enhance the reliability of ESG ratings.         ",
    "url": "https://arxiv.org/abs/2407.20047",
    "authors": [
      "Sergio Caprioli",
      "Jacopo Foschi",
      "Riccardo Crupi",
      "Alessandro Sabatino"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.20062",
    "title": "SalNAS: Efficient Saliency-prediction Neural Architecture Search with self-knowledge distillation",
    "abstract": "           Recent advancements in deep convolutional neural networks have significantly improved the performance of saliency prediction. However, the manual configuration of the neural network architectures requires domain knowledge expertise and can still be time-consuming and error-prone. To solve this, we propose a new Neural Architecture Search (NAS) framework for saliency prediction with two contributions. Firstly, a supernet for saliency prediction is built with a weight-sharing network containing all candidate architectures, by integrating a dynamic convolution into the encoder-decoder in the supernet, termed SalNAS. Secondly, despite the fact that SalNAS is highly efficient (20.98 million parameters), it can suffer from the lack of generalization. To solve this, we propose a self-knowledge distillation approach, termed Self-KD, that trains the student SalNAS with the weighted average information between the ground truth and the prediction from the teacher model. The teacher model, while sharing the same architecture, contains the best-performing weights chosen by cross-validation. Self-KD can generalize well without the need to compute the gradient in the teacher model, enabling an efficient training system. By utilizing Self-KD, SalNAS outperforms other state-of-the-art saliency prediction models in most evaluation rubrics across seven benchmark datasets while being a lightweight model. The code will be available at this https URL ",
    "url": "https://arxiv.org/abs/2407.20062",
    "authors": [
      "Chakkrit Termritthikun",
      "Ayaz Umer",
      "Suwichaya Suwanwimolkul",
      "Feng Xia",
      "Ivan Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.20076",
    "title": "Investigating the Impact of Semi-Supervised Methods with Data Augmentation on Offensive Language Detection in Romanian Language",
    "abstract": "           Offensive language detection is a crucial task in today's digital landscape, where online platforms grapple with maintaining a respectful and inclusive environment. However, building robust offensive language detection models requires large amounts of labeled data, which can be expensive and time-consuming to obtain. Semi-supervised learning offers a feasible solution by utilizing labeled and unlabeled data to create more accurate and robust models. In this paper, we explore a few different semi-supervised methods, as well as data augmentation techniques. Concretely, we implemented eight semi-supervised methods and ran experiments for them using only the available data in the RO-Offense dataset and applying five augmentation techniques before feeding the data to the models. Experimental results demonstrate that some of them benefit more from augmentations than others.         ",
    "url": "https://arxiv.org/abs/2407.20076",
    "authors": [
      "Elena Beatrice Nicola",
      "Dumitru Clementin Cercel",
      "Florin Pop"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.20078",
    "title": "Background Semantics Matter: Cross-Task Feature Exchange Network for Clustered Infrared Small Target Detection With Sky-Annotated Dataset",
    "abstract": "           Infrared small target detection poses unique challenges due to the scarcity of intrinsic target features and the abundance of similar background distractors. We argue that background semantics play a pivotal role in distinguishing visually similar objects for this task. To address this, we introduce a new task -- clustered infrared small target detection, and present DenseSIRST, a novel benchmark dataset that provides per-pixel semantic annotations for background regions, enabling the transition from sparse to dense target detection. Leveraging this dataset, we propose the Background-Aware Feature Exchange Network (BAFE-Net), which transforms the detection paradigm from a single task focused on the foreground to a multi-task architecture that jointly performs target detection and background semantic segmentation. BAFE-Net introduces a cross-task feature hard-exchange mechanism to embed target and background semantics between the two tasks. Furthermore, we propose the Background-Aware Gaussian Copy-Paste (BAG-CP) method, which selectively pastes small targets into sky regions during training, avoiding the creation of false alarm targets in complex non-sky backgrounds. Extensive experiments validate the effectiveness of BAG-CP and BAFE-Net in improving target detection accuracy while reducing false alarms. The DenseSIRST dataset, code, and trained models are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.20078",
    "authors": [
      "Yimian Dai",
      "Mengxuan Xiao",
      "Yiming Zhu",
      "Huan Wang",
      "Kehua Guo",
      "Jian Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.20088",
    "title": "As Transparent as Possible, System Implementation Governance Using Process Mining",
    "abstract": "           This paper advocates for guiding an effective system implementation approach at a business process level. It details a case study of a food product manufacturer that transitioned to a new local information system. 41 units' data (10160 cases) over the pre-maturity phase of the system were then structured into event logs and analyzed. This analysis identified deviant process paths, questioning whether the new system efficiently supports procurement operations immediately post-implementation. The findings reveal critical implementation risks with conformance-checking of the as-is process with the to-be process model; this includes incomplete cases, unauthorized activities, irregular payment practices, stemming from organizational bottlenecks, or violating internal control regulations. Challenges are attributed to technical shortcomings in system design and cultural misalignments, necessitating immediate interventions or longer-term cultural and training solutions. This study's contribution is its demonstration of a transparent, process-driven approach to system governance, highlighting the strategic benefits of this integration for organizational management.         ",
    "url": "https://arxiv.org/abs/2407.20088",
    "authors": [
      "Negin Maddah"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2407.20089",
    "title": "Performance Study of Various Relay Nodes in 5G Wireless Network",
    "abstract": "           This paper studies performance of various types of relay nodes in a 5G wireless network: conventional amplify-forward repeaters, (semi-)smart/smart amplify-forward repeaters with different levels of side information, and half-duplex/full-duplex decode-forward relay nodes with and without spatial reuse. End-to-end effective signal to interference and noise ratios (SINRs) and achievable rates are derived for these different types of relay nodes. Performance and complexity tradeoffs are discussed with a simulation over a Manhattan topology setting. Over-the-air (OTA) test results corroborates the findings in this paper.         ",
    "url": "https://arxiv.org/abs/2407.20089",
    "authors": [
      "Jianghong Luo",
      "Ashwin Sampath",
      "Navid Abedini",
      "Tao Luo"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2407.20090",
    "title": "Infrared Small Target Detection based on Adjustable Sensitivity Strategy and Multi-Scale Fusion",
    "abstract": "           Recently, deep learning-based single-frame infrared small target (SIRST) detection technology has made significant progress. However, existing infrared small target detection methods are often optimized for a fixed image resolution, a single wavelength, or a specific imaging system, limiting their breadth and flexibility in practical applications. Therefore, we propose a refined infrared small target detection scheme based on an adjustable sensitivity (AS) strategy and multi-scale fusion. Specifically, a multi-scale model fusion framework based on multi-scale direction-aware network (MSDA-Net) is constructed, which uses input images of multiple scales to train multiple models and fuses them. Multi-scale fusion helps characterize the shape, edge, and texture features of the target from different scales, making the model more accurate and reliable in locating the target. At the same time, we fully consider the characteristics of the infrared small target detection task and construct an edge enhancement difficulty mining (EEDM) loss. The EEDM loss helps alleviate the problem of category imbalance and guides the network to pay more attention to difficult target areas and edge features during training. In addition, we propose an adjustable sensitivity strategy for post-processing. This strategy significantly improves the detection rate of infrared small targets while ensuring segmentation accuracy. Extensive experimental results show that the proposed scheme achieves the best performance. Notably, this scheme won the first prize in the PRCV 2024 wide-area infrared small target detection competition.         ",
    "url": "https://arxiv.org/abs/2407.20090",
    "authors": [
      "Jinmiao Zhao",
      "Zelin Shi",
      "Chuang Yu",
      "Yunpeng Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.20099",
    "title": "RSC-SNN: Exploring the Trade-off Between Adversarial Robustness and Accuracy in Spiking Neural Networks via Randomized Smoothing Coding",
    "abstract": "           Spiking Neural Networks (SNNs) have received widespread attention due to their unique neuronal dynamics and low-power nature. Previous research empirically shows that SNNs with Poisson coding are more robust than Artificial Neural Networks (ANNs) on small-scale datasets. However, it is still unclear in theory how the adversarial robustness of SNNs is derived, and whether SNNs can still maintain its adversarial robustness advantage on large-scale dataset tasks. This work theoretically demonstrates that SNN's inherent adversarial robustness stems from its Poisson coding. We reveal the conceptual equivalence of Poisson coding and randomized smoothing in defense strategies, and analyze in depth the trade-off between accuracy and adversarial robustness in SNNs via the proposed Randomized Smoothing Coding (RSC) method. Experiments demonstrate that the proposed RSC-SNNs show remarkable adversarial robustness, surpassing ANNs and achieving state-of-the-art robustness results on large-scale dataset ImageNet. Our open-source implementation code is available at this https URL: this https URL.         ",
    "url": "https://arxiv.org/abs/2407.20099",
    "authors": [
      "Keming Wu",
      "Man Yao",
      "Yuhong Chou",
      "Xuerui Qiu",
      "Rui Yang",
      "Bo Xu",
      "Guoqi Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.20100",
    "title": "F-KANs: Federated Kolmogorov-Arnold Networks",
    "abstract": "           In this paper, we present an innovative federated learning (FL) approach that utilizes Kolmogorov-Arnold Networks (KANs) for classification tasks. By utilizing the adaptive activation capabilities of KANs in a federated framework, we aim to improve classification capabilities while preserving privacy. The study evaluates the performance of federated KANs (F- KANs) compared to traditional Multi-Layer Perceptrons (MLPs) on classification task. The results show that the F-KANs model significantly outperforms the federated MLP model in terms of accuracy, precision, recall, F1 score and stability, and achieves better performance, paving the way for more efficient and privacy-preserving predictive analytics.         ",
    "url": "https://arxiv.org/abs/2407.20100",
    "authors": [
      "Engin Zeydan",
      "Cristian J. Vaca-Rubio",
      "Luis Blanco",
      "Roberto Pereira",
      "Marius Caus",
      "Abdullah Aydeger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2407.20111",
    "title": "Enhancing Anti-spoofing Countermeasures Robustness through Joint Optimization and Transfer Learning",
    "abstract": "           Current research in synthesized speech detection primarily focuses on the generalization of detection systems to unknown spoofing methods of noise-free speech. However, the performance of anti-spoofing countermeasures (CM) system is often don't work as well in more challenging scenarios, such as those involving noise and reverberation. To address the problem of enhancing the robustness of CM systems, we propose a transfer learning-based speech enhancement front-end joint optimization (TL-SEJ) method, investigating its effectiveness in improving robustness against noise and reverberation. We evaluated the proposed method's performance through a series of comparative and ablation experiments. The experimental results show that, across different signal-to-noise ratio test conditions, the proposed TL-SEJ method improves recognition accuracy by 2.7% to 15.8% compared to the baseline. Compared to conventional data augmentation methods, our system achieves an accuracy improvement ranging from 0.7% to 5.8% in various noisy conditions and from 1.7% to 2.8% under different RT60 reverberation scenarios. These experiments demonstrate that the proposed method effectively enhances system robustness in noisy and reverberant conditions.         ",
    "url": "https://arxiv.org/abs/2407.20111",
    "authors": [
      "Yikang Wang",
      "Xingming Wang",
      "Hiromitsu Nishizaki",
      "Ming Li"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.20119",
    "title": "Adaptive Self-supervised Robust Clustering for Unstructured Data with Unknown Cluster Number",
    "abstract": "           We introduce a novel self-supervised deep clustering approach tailored for unstructured data without requiring prior knowledge of the number of clusters, termed Adaptive Self-supervised Robust Clustering (ASRC). In particular, ASRC adaptively learns the graph structure and edge weights to capture both local and global structural information. The obtained graph enables us to learn clustering-friendly feature representations by an enhanced graph auto-encoder with contrastive learning technique. It further leverages the clustering results adaptively obtained by robust continuous clustering (RCC) to generate prototypes for negative sampling, which can further contribute to promoting consistency among positive pairs and enlarging the gap between positive and negative samples. ASRC obtains the final clustering results by applying RCC to the learned feature representations with their consistent graph structure and edge weights. Extensive experiments conducted on seven benchmark datasets demonstrate the efficacy of ASRC, demonstrating its superior performance over other popular clustering models. Notably, ASRC even outperforms methods that rely on prior knowledge of the number of clusters, highlighting its effectiveness in addressing the challenges of clustering unstructured data.         ",
    "url": "https://arxiv.org/abs/2407.20119",
    "authors": [
      "Chen-Lu Ding",
      "Jiancan Wu",
      "Wei Lin",
      "Shiyang Shen",
      "Xiang Wang",
      "Yancheng Yuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.20152",
    "title": "Hierarchically Disentangled Recurrent Network for Factorizing System Dynamics of Multi-scale Systems",
    "abstract": "           We present a knowledge-guided machine learning (KGML) framework for modeling multi-scale processes, and study its performance in the context of streamflow forecasting in hydrology. Specifically, we propose a novel hierarchical recurrent neural architecture that factorizes the system dynamics at multiple temporal scales and captures their interactions. This framework consists of an inverse and a forward model. The inverse model is used to empirically resolve the system's temporal modes from data (physical model simulations, observed data, or a combination of them from the past), and these states are then used in the forward model to predict streamflow. In a hydrological system, these modes can represent different processes, evolving at different temporal scales (e.g., slow: groundwater recharge and baseflow vs. fast: surface runoff due to extreme rainfall). A key advantage of our framework is that once trained, it can incorporate new observations into the model's context (internal state) without expensive optimization approaches (e.g., EnKF) that are traditionally used in physical sciences for data assimilation. Experiments with several river catchments from the NWS NCRFC region show the efficacy of this ML-based data assimilation framework compared to standard baselines, especially for basins that have a long history of observations. Even for basins that have a shorter observation history, we present two orthogonal strategies of training our FHNN framework: (a) using simulation data from imperfect simulations and (b) using observation data from multiple basins to build a global model. We show that both of these strategies (that can be used individually or together) are highly effective in mitigating the lack of training data. The improvement in forecast accuracy is particularly noteworthy for basins where local models perform poorly because of data sparsity.         ",
    "url": "https://arxiv.org/abs/2407.20152",
    "authors": [
      "Rahul Ghosh",
      "Zac McEachran",
      "Arvind Renganathan",
      "Kelly Lindsay",
      "Somya Sharma",
      "Michael Steinbach",
      "John Nieber",
      "Christopher Duffy",
      "Vipin Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.20155",
    "title": "GsPINN: A novel fast Green kernel solver based on symmetric Physics-Informed neural networks",
    "abstract": "           Ever since deep learning was introduced in the calculation of partial differential equation (PDE), there has been a lot of interests on real time response of system where the kernel function plays an important role. As a popular tool in recent years, physics-informed neural networks (PINNs) was proposed to perform a mesh-free, semi-supervised learning with high flexibility. This paper explores the integration of Lie symmetry groups with deep learning techniques to enhance the numerical solutions of fundamental solution in PDE. We propose a novel approach that combines the strengths of PINN and Lie group theory to address the computational inefficiencies in traditional methods. By incorporating the linearized symmetric condition (LSC) derived from Lie symmetries into PINNs, we introduce a new type of residual loss with lower order of derivative needed to calculate. This integration allows for significant reductions in computational costs and improvements in solution precision. Numerical simulation shows that our method can achieve up to a 50\\% reduction in training time while maintaining good accuracy. Additionally, we provide a general theoretical framework to identify invariant infinitesimal generators for arbitrary Cauchy problems. This unsupervised algorithm does not require prior numerical solutions, making it both practical and efficient for various applications. Our contributions demonstrate the potential of combining symmetry analysis with deep learning to advance the field of scientific machine learning.         ",
    "url": "https://arxiv.org/abs/2407.20155",
    "authors": [
      "Xiaopei Jiao",
      "Fansheng Xiong"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2407.20176",
    "title": "Emotion-Driven Melody Harmonization via Melodic Variation and Functional Representation",
    "abstract": "           Emotion-driven melody harmonization aims to generate diverse harmonies for a single melody to convey desired emotions. Previous research found it hard to alter the perceived emotional valence of lead sheets only by harmonizing the same melody with different chords, which may be attributed to the constraints imposed by the melody itself and the limitation of existing music representation. In this paper, we propose a novel functional representation for symbolic music. This new method takes musical keys into account, recognizing their significant role in shaping music's emotional character through major-minor tonality. It also allows for melodic variation with respect to keys and addresses the problem of data scarcity for better emotion modeling. A Transformer is employed to harmonize key-adaptable melodies, allowing for keys determined in rule-based or model-based manner. Experimental results confirm the effectiveness of our new representation in generating key-aware harmonies, with objective and subjective evaluations affirming the potential of our approach to convey specific valence for versatile melody.         ",
    "url": "https://arxiv.org/abs/2407.20176",
    "authors": [
      "Jingyue Huang",
      "Yi-Hsuan Yang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2407.20177",
    "title": "AutoScale: Automatic Prediction of Compute-optimal Data Composition for Training LLMs",
    "abstract": "           To ensure performance on a diverse set of downstream tasks, LLMs are pretrained via data mixtures over different domains. In this work, we demonstrate that the optimal data composition for a fixed compute budget varies depending on the scale of the training data, suggesting that the common practice of empirically determining an optimal composition using small-scale experiments will not yield the optimal data mixtures when scaling up to the final model. To address this challenge, we propose *AutoScale*, an automated tool that finds a compute-optimal data composition for training at any desired target scale. AutoScale first determines the optimal composition at a small scale using a novel bilevel optimization framework, Direct Data Optimization (*DDO*), and then fits a predictor to estimate the optimal composition at larger scales. The predictor's design is inspired by our theoretical analysis of scaling laws related to data composition, which could be of independent interest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2 Large) on RedPajama dataset, AutoScale decreases validation perplexity at least 25% faster than any baseline with up to 38% speed up compared to without reweighting, achieving the best overall performance across downstream tasks. On pre-training Encoder-only LMs (BERT) with masked language modeling, DDO is shown to decrease loss on all domains while visibly improving average task performance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by 5.9% compared with without reweighting. AutoScale speeds up training by up to 28%. Our codes are open-sourced.         ",
    "url": "https://arxiv.org/abs/2407.20177",
    "authors": [
      "Feiyang Kang",
      "Yifan Sun",
      "Bingbing Wen",
      "Si Chen",
      "Dawn Song",
      "Rafid Mahmood",
      "Ruoxi Jia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.20189",
    "title": "Aligning Query Representation with Rewritten Query and Relevance Judgments in Conversational Search",
    "abstract": "           Conversational search supports multi-turn user-system interactions to solve complex information needs. Different from the traditional single-turn ad-hoc search, conversational search encounters a more challenging problem of context-dependent query understanding with the lengthy and long-tail conversational history context. While conversational query rewriting methods leverage explicit rewritten queries to train a rewriting model to transform the context-dependent query into a stand-stone search query, this is usually done without considering the quality of search results. Conversational dense retrieval methods use fine-tuning to improve a pre-trained ad-hoc query encoder, but they are limited by the conversational search data available for training. In this paper, we leverage both rewritten queries and relevance judgments in the conversational search data to train a better query representation model. The key idea is to align the query representation with those of rewritten queries and relevant documents. The proposed model -- Query Representation Alignment Conversational Dense Retriever, QRACDR, is tested on eight datasets, including various settings in conversational search and ad-hoc search. The results demonstrate the strong performance of QRACDR compared with state-of-the-art methods, and confirm the effectiveness of representation alignment.         ",
    "url": "https://arxiv.org/abs/2407.20189",
    "authors": [
      "Fengran Mo",
      "Chen Qu",
      "Kelong Mao",
      "Yihong Wu",
      "Zhan Su",
      "Kaiyu Huang",
      "Jian-Yun Nie"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.20207",
    "title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval",
    "abstract": "           In dense retrieval, embedding long texts into dense vectors can result in information loss, leading to inaccurate query-text matching. Additionally, low-quality texts with excessive noise or sparse key information are unlikely to align well with relevant queries. Recent studies mainly focus on improving the sentence embedding model or retrieval process. In this work, we introduce a novel text augmentation framework for dense retrieval. This framework transforms raw documents into information-dense text formats, which supplement the original texts to effectively address the aforementioned issues without modifying embedding or retrieval methodologies. Two text representations are generated via large language models (LLMs) zero-shot prompting: question-answer pairs and element-driven events. We term this approach QAEA-DR: unifying question-answer generation and event extraction in a text augmentation framework for dense retrieval. To further enhance the quality of generated texts, a scoring-based evaluation and regeneration mechanism is introduced in LLM prompting. Our QAEA-DR model has a positive impact on dense retrieval, supported by both theoretical analysis and empirical experiments.         ",
    "url": "https://arxiv.org/abs/2407.20207",
    "authors": [
      "Hongming Tan",
      "Shaoxiong Zhan",
      "Hai Lin",
      "Hai-Tao Zheng",
      "Wai Kin",
      "Chan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.20213",
    "title": "Registering Neural 4D Gaussians for Endoscopic Surgery",
    "abstract": "           The recent advance in neural rendering has enabled the ability to reconstruct high-quality 4D scenes using neural networks. Although 4D neural reconstruction is popular, registration for such representations remains a challenging task, especially for dynamic scene registration in surgical planning and simulation. In this paper, we propose a novel strategy for dynamic surgical neural scene registration. We first utilize 4D Gaussian Splatting to represent the surgical scene and capture both static and dynamic scenes effectively. Then, a spatial aware feature aggregation method, Spatially Weight Cluttering (SWC) is proposed to accurately align the feature between surgical scenes, enabling precise and realistic surgical simulations. Lastly, we present a novel strategy of deformable scene registration to register two dynamic scenes. By incorporating both spatial and temporal information for correspondence matching, our approach achieves superior performance compared to existing registration methods for implicit neural representation. The proposed method has the potential to improve surgical planning and training, ultimately leading to better patient outcomes.         ",
    "url": "https://arxiv.org/abs/2407.20213",
    "authors": [
      "Yiming Huang",
      "Beilei Cui",
      "Ikemura Kei",
      "Jiekai Zhang",
      "Long Bai",
      "Hongliang Ren"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.20214",
    "title": "SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction",
    "abstract": "           Graph-based holistic scene representations facilitate surgical workflow understanding and have recently demonstrated significant success. However, this task is often hindered by the limited availability of densely annotated surgical scene data. In this work, we introduce an end-to-end framework for the generation and optimization of surgical scene graphs on a downstream task. Our approach leverages the flexibility of graph-based spectral clustering and the generalization capability of foundation models to generate unsupervised scene graphs with learnable properties. We reinforce the initial spatial graph with sparse temporal connections using local matches between consecutive frames to predict temporally consistent clusters across a temporal neighborhood. By jointly optimizing the spatiotemporal relations and node features of the dynamic scene graph with the downstream task of phase segmentation, we address the costly and annotation-burdensome task of semantic scene comprehension and scene graph generation in surgical videos using only weak surgical phase labels. Further, by incorporating effective intermediate scene representation disentanglement steps within the pipeline, our solution outperforms the SOTA on the CATARACTS dataset by 8% accuracy and 10% F1 score in surgical workflow recognition         ",
    "url": "https://arxiv.org/abs/2407.20214",
    "authors": [
      "\u00c7a\u011fhan K\u00f6ksal",
      "Ghazal Ghazaei",
      "Felix Holm",
      "Azade Farshad",
      "Nassir Navab"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18915",
    "title": "Learning-Based WiFi Fingerprint Inpainting via Generative Adversarial Networks",
    "abstract": "           WiFi-based indoor positioning has been extensively studied. A fundamental issue in such solutions is the collection of WiFi fingerprints. However, due to real-world constraints, collecting complete fingerprints at all intended locations is sometimes prohibited. This work considers the WiFi fingerprint inpainting problem. This problem differs from typical image/video inpainting problems in several aspects. Unlike RGB images, WiFi field maps come in any shape, and signal data may follow certain distributions. Therefore, it is difficult to forcefully fit them into a fixed-dimensional matrix, as done with processing images in RGB format. As soon as a map is changed, it also becomes difficult to adapt it to the same model due to scale issues. Furthermore, such models are significantly constrained in situations requiring outward inpainting. Fortunately, the spatial relationships of WiFi signals and the rich information provided among channels offer ample opportunities for this generative model to accomplish inpainting. Therefore, we designed this model to not only retain the characteristic of regression models in generating fingerprints of arbitrary shapes but also to accommodate the observational outcomes from densely deployed APs. This work makes two major contributions. Firstly, we delineate the distinctions between this problem and image inpainting, highlighting potential avenues for research. Secondly, we introduce novel generative inpainting models aimed at capturing both inter-AP and intra-AP correlations while preserving latent information. Additionally, we incorporate a specially designed adversarial discriminator to enhance the quality of inpainting outcomes.         ",
    "url": "https://arxiv.org/abs/2407.18915",
    "authors": [
      "Yu Chan",
      "Pin-Yu Lin",
      "Yu-Yun Tseng",
      "Jen-Jee Chen",
      "Yu-Chee Tseng"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19073",
    "title": "On Machine Learning Approaches for Protein-Ligand Binding Affinity Prediction",
    "abstract": "           Binding affinity optimization is crucial in early-stage drug discovery. While numerous machine learning methods exist for predicting ligand potency, their comparative efficacy remains unclear. This study evaluates the performance of classical tree-based models and advanced neural networks in protein-ligand binding affinity prediction. Our comprehensive benchmarking encompasses 2D models utilizing ligand-only RDKit embeddings and Large Language Model (LLM) ligand representations, as well as 3D neural networks incorporating bound protein-ligand conformations. We assess these models across multiple standard datasets, examining various predictive scenarios including classification, ranking, regression, and active learning. Results indicate that simpler models can surpass more complex ones in specific tasks, while 3D models leveraging structural information become increasingly competitive with larger training datasets containing compounds with labelled affinity data against multiple targets. Pre-trained 3D models, by incorporating protein pocket environments, demonstrate significant advantages in data-scarce scenarios for specific binding pockets. Additionally, LLM pretraining on 2D ligand data enhances complex model performance, providing versatile embeddings that outperform traditional RDKit features in computational efficiency. Finally, we show that combining 2D and 3D model strengths improves active learning outcomes beyond current state-of-the-art approaches. These findings offer valuable insights for optimizing machine learning strategies in drug discovery pipelines.         ",
    "url": "https://arxiv.org/abs/2407.19073",
    "authors": [
      "Nikolai Schapin",
      "Carles Navarro",
      "Albert Bou",
      "Gianni De Fabritiis"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2407.19147",
    "title": "Reexamination of the realtime protection for user privacy in practical quantum private query",
    "abstract": "           Quantum private query (QPQ) is the quantum version for symmetrically private retrieval. However, the user privacy in QPQ is generally guarded in the non-realtime and cheat sensitive way. That is, the dishonest database holder's cheating to elicit user privacy can only be discovered after the protocol is finished (when the user finds some errors in the retrieved database item). Such delayed detection may cause very unpleasant results for the user in real-life applications. Current efforts to protect user privacy in realtime in existing QPQ protocols mainly use two techniques, i.e., adding an honesty checking on the database or allowing the user to reorder the qubits. We reexamine these two kinds of QPQ protocols and find neither of them can work well. We give concrete cheating strategies for both participants and show that honesty checking of inner participant should be dealt more carefully in for example the choosing of checking qubits. We hope such discussion can supply new concerns when detection of dishonest participant is considered in quantum multi-party secure computations.         ",
    "url": "https://arxiv.org/abs/2407.19147",
    "authors": [
      "Chun-Yan Wei",
      "Xiao-Qiu Cai",
      "Tian-Yin Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.19269",
    "title": "A Bayesian Approach Toward Robust Multidimensional Ellipsoid-Specific Fitting",
    "abstract": "           This work presents a novel and effective method for fitting multidimensional ellipsoids to scattered data in the contamination of noise and outliers. We approach the problem as a Bayesian parameter estimate process and maximize the posterior probability of a certain ellipsoidal solution given the data. We establish a more robust correlation between these points based on the predictive distribution within the Bayesian framework. We incorporate a uniform prior distribution to constrain the search for primitive parameters within an ellipsoidal domain, ensuring ellipsoid-specific results regardless of inputs. We then establish the connection between measurement point and model data via Bayes' rule to enhance the method's robustness against noise. Due to independent of spatial dimensions, the proposed method not only delivers high-quality fittings to challenging elongated ellipsoids but also generalizes well to multidimensional spaces. To address outlier disturbances, often overlooked by previous approaches, we further introduce a uniform distribution on top of the predictive distribution to significantly enhance the algorithm's robustness against outliers. We introduce an {\\epsilon}-accelerated technique to expedite the convergence of EM considerably. To the best of our knowledge, this is the first comprehensive method capable of performing multidimensional ellipsoid specific fitting within the Bayesian optimization paradigm under diverse disturbances. We evaluate it across lower and higher dimensional spaces in the presence of heavy noise, outliers, and substantial variations in axis ratios. Also, we apply it to a wide range of practical applications such as microscopy cell counting, 3D reconstruction, geometric shape approximation, and magnetometer calibration tasks.         ",
    "url": "https://arxiv.org/abs/2407.19269",
    "authors": [
      "Zhao Mingyang",
      "Jia Xiaohong",
      "Ma Lei",
      "Shi Yuke",
      "Jiang Jingen",
      "Li Qizhai",
      "Yan Dong-Ming",
      "Huang Tiejun"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19282",
    "title": "A self-supervised and adversarial approach to hyperspectral demosaicking and RGB reconstruction in surgical imaging",
    "abstract": "           Hyperspectral imaging holds promises in surgical imaging by offering biological tissue differentiation capabilities with detailed information that is invisible to the naked eye. For intra-operative guidance, real-time spectral data capture and display is mandated. Snapshot mosaic hyperspectral cameras are currently seen as the most suitable technology given this requirement. However, snapshot mosaic imaging requires a demosaicking algorithm to fully restore the spatial and spectral details in the images. Modern demosaicking approaches typically rely on synthetic datasets to develop supervised learning methods, as it is practically impossible to simultaneously capture both snapshot and high-resolution spectral images of the exact same surgical scene. In this work, we present a self-supervised demosaicking and RGB reconstruction method that does not depend on paired high-resolution data as ground truth. We leverage unpaired standard high-resolution surgical microscopy images, which only provide RGB data but can be collected during routine surgeries. Adversarial learning complemented by self-supervised approaches are used to drive our hyperspectral-based RGB reconstruction into resembling surgical microscopy images and increasing the spatial resolution of our demosaicking. The spatial and spectral fidelity of the reconstructed hyperspectral images have been evaluated quantitatively. Moreover, a user study was conducted to evaluate the RGB visualisation generated from these spectral images. Both spatial detail and colour accuracy were assessed by neurosurgical experts. Our proposed self-supervised demosaicking method demonstrates improved results compared to existing methods, demonstrating its potential for seamless integration into intra-operative workflows.         ",
    "url": "https://arxiv.org/abs/2407.19282",
    "authors": [
      "Peichao Li",
      "Oscar MacCormac",
      "Jonathan Shapey",
      "Tom Vercauteren"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19316",
    "title": "AResNet-ViT: A Hybrid CNN-Transformer Network for Benign and Malignant Breast Nodule Classification in Ultrasound Images",
    "abstract": "           To address the challenges of similarity between lesions and surrounding tissues, overlapping appearances of partially benign and malignant nodules, and difficulty in classification, a deep learning network that integrates CNN and Transformer is proposed for the classification of benign and malignant breast lesions in ultrasound images. This network adopts a dual-branch architecture for local-global feature extraction, making full use of the advantages of CNN in extracting local features and the ability of ViT to extract global features to enhance the network's feature extraction capabilities for breast nodules. The local feature extraction branch employs a residual network with multiple attention-guided modules, which can effectively capture the local details and texture features of breast nodules, enhance sensitivity to subtle changes within the nodules, and thus can aid in accurate classification of their benign and malignancy. The global feature extraction branch utilizes the multi-head self-attention ViT network, which can capture the overall shape, boundary, and relationship with surrounding tissues, and thereby enhancing the understanding and modeling of both nodule and global image features. Experimental results on a public ultrasound breast nodule data set show that the proposed method is better than other comparison networks, This indicates that the fusion of CNN and Transformer networks can effectively improve the performance of the classification model and provide a powerful solution for the benign-malignant classification of ultrasound breast.         ",
    "url": "https://arxiv.org/abs/2407.19316",
    "authors": [
      "Xin Zhao",
      "Qianqian Zhu",
      "Jialing Wu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.19353",
    "title": "A spring-block theory of feature learning in deep neural networks",
    "abstract": "           A central question in deep learning is how deep neural networks (DNNs) learn features. DNN layers progressively collapse data into a regular low-dimensional geometry. This collective effect of non-linearity, noise, learning rate, width, depth, and numerous other parameters, has eluded first-principles theories which are built from microscopic neuronal dynamics. Here we present a noise-non-linearity phase diagram that highlights where shallow or deep layers learn features more effectively. We then propose a macroscopic mechanical theory of feature learning that accurately reproduces this phase diagram, offering a clear intuition for why and how some DNNs are ``lazy'' and some are ``active'', and relating the distribution of feature learning over layers with test accuracy.         ",
    "url": "https://arxiv.org/abs/2407.19353",
    "authors": [
      "Cheng Shi",
      "Liming Pan",
      "Ivan Dokmani\u0107"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.19834",
    "title": "Frequency & Channel Attention Network for Small Footprint Noisy Spoken Keyword Spotting",
    "abstract": "           In this paper, we aim to improve the robustness of Keyword Spotting (KWS) systems in noisy environments while keeping a small memory footprint. We propose a new convolutional neural network (CNN) called FCA-Net, which combines mixer unit-based feature interaction with a two-dimensional convolution-based attention module. First, we introduce and compare lightweight attention methods to enhance noise robustness in CNN. Then, we propose an attention module that creates fine-grained attention weights to capture channel and frequency-specific information, boosting the model's ability to handle noisy conditions. By combining the mixer unit-based feature interaction with the attention module, we enhance performance. Additionally, we use a curriculum-based multi-condition training strategy. Our experiments show that our system outperforms current state-of-the-art solutions for small-footprint KWS in noisy environments, making it reliable for real-world use.         ",
    "url": "https://arxiv.org/abs/2407.19834",
    "authors": [
      "Yuanxi Lin",
      "Yuriy Evgenyevich Gapanyuk"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2407.19841",
    "title": "RRAM-Based Bio-Inspired Circuits for Mobile Epileptic Correlation Extraction and Seizure Prediction",
    "abstract": "           Non-invasive mobile electroencephalography (EEG) acquisition systems have been utilized for long-term monitoring of seizures, yet they suffer from limited battery life. Resistive random access memory (RRAM) is widely used in computing-in-memory(CIM) systems, which offers an ideal platform for reducing the computational energy consumption of seizure prediction algorithms, potentially solving the endurance issues of mobile EEG systems. To address this challenge, inspired by neuronal mechanisms, we propose a RRAM-based bio-inspired circuit system for correlation feature extraction and seizure prediction. This system achieves a high average sensitivity of 91.2% and a low false positive rate per hour (FPR/h) of 0.11 on the CHB-MIT seizure dataset. The chip under simulation demonstrates an area of approximately 0.83 mm2 and a latency of 62.2 {\\mu}s. Power consumption is recorded at 24.4 mW during the feature extraction phase and 19.01 mW in the seizure prediction phase, with a cumulative energy consumption of 1.515 {\\mu}J for a 3-second window data processing, predicting 29.2 minutes ahead. This method exhibits an 81.3% reduction in computational energy relative to the most efficient existing seizure prediction approaches, establishing a new benchmark for energy efficiency.         ",
    "url": "https://arxiv.org/abs/2407.19841",
    "authors": [
      "Hao Wang",
      "Lingfeng Zhang",
      "Erjia Xiao",
      "Xin Wang",
      "Zhongrui Wang",
      "Renjing Xu"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2407.19858",
    "title": "AI-Powered Energy algorithmic Trading: Integrating Hidden Markov Models with Neural Networks",
    "abstract": "           In the field of quantitative finance, machine learning methods have become essential for alpha generation. This paper presents a pioneering method that uniquely combines Hidden Markov Models (HMM) and neural networks, creating a dual-model alpha generation system integrated with Black-Litterman portfolio optimization. The methodology, implemented on the QuantConnect platform, aims to predict future price movements and optimize trading strategies. Specifically, it filters for highly liquid, top-cap energy stocks to ensure stable and predictable performance while also accounting for broker payments. QuantConnect was selected because of its robust framework and to guarantee experimental reproducibility. The algorithm achieved a 31% return between June 1, 2023, and January 1, 2024, with a Sharpe ratio of 1.669, demonstrating its potential. The findings suggest significant improvements in trading strategy performance through the combined use of the HMM and neural networks. This study explores the architecture of the algorithm, data pre-processing techniques, model training procedures, and performance evaluation, highlighting its practical applicability and effectiveness in real-world trading environments. The full code and backtesting data are available under the MIT license.         ",
    "url": "https://arxiv.org/abs/2407.19858",
    "authors": [
      "Tiago Monteiro"
    ],
    "subjectives": [
      "Portfolio Management (q-fin.PM)",
      "Machine Learning (cs.LG)",
      "General Finance (q-fin.GN)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2407.20126",
    "title": "Extreme time extrapolation capabilities and thermodynamic consistency of physics-inspired Neural Networks for the 3D microstructure evolution of materials",
    "abstract": "           A Convolutional Recurrent Neural Network (CRNN) is trained to reproduce the evolution of the spinodal decomposition process in three dimensions as described by the Cahn-Hilliard equation. A specialized, physics-inspired architecture is proven to provide close accordance between the predicted evolutions and the ground truth ones obtained via conventional integration schemes. The method can closely reproduce the evolution of microstructures not represented in the training set at a fraction of the computational costs. Extremely long-time extrapolation capabilities are achieved, up to reaching the theoretically expected equilibrium state of the system, despite the training set containing only relatively-short, initial phases of the evolution. Quantitative accordance with the decay rate of the Free energy is also demonstrated up to late coarsening stages, providing an example of a data-driven, physically consistent and high-accuracy Machine Learning method for the long timescale simulation of materials.         ",
    "url": "https://arxiv.org/abs/2407.20126",
    "authors": [
      "Daniele Lanzoni",
      "Andrea Fantasia",
      "Roberto Bergamaschini",
      "Olivier Pierre-Louis",
      "Francesco Montalenti"
    ],
    "subjectives": [
      "Mesoscale and Nanoscale Physics (cond-mat.mes-hall)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:1903.02758",
    "title": "A face cover perspective to $\\ell_1$ embeddings of planar graphs",
    "abstract": "           It was conjectured by Gupta et al. [Combinatorica04] that every planar graph can be embedded into $\\ell_1$ with constant distortion. However, given an $n$-vertex weighted planar graph, the best upper bound on the distortion is only $O(\\sqrt{\\log n})$, by Rao [SoCG99]. In this paper we study the case where there is a set $K$ of terminals, and the goal is to embed only the terminals into $\\ell_1$ with low distortion. In a seminal paper, Okamura and Seymour [J.Comb.Theory81] showed that if all the terminals lie on a single face, they can be embedded isometrically into $\\ell_1$. The more general case, where the set of terminals can be covered by $\\gamma$ faces, was studied by Lee and Sidiropoulos [STOC09] and Chekuri et al. [J.Comb.Theory13]. The state of the art is an upper bound of $O(\\log \\gamma)$ by Krauthgamer, Lee and Rika [SODA19]. Our contribution is a further improvement on the upper bound to $O(\\sqrt{\\log\\gamma})$. Since every planar graph has at most $O(n)$ faces, any further improvement on this result, will be a major breakthrough, directly improving upon Rao's long standing upper bound. Moreover, it is well known that the flow-cut gap equals to the distortion of the best embedding into $\\ell_1$. Therefore, our result provides a polynomial time $O(\\sqrt{\\log \\gamma})$-approximation to the sparsest cut problem on planar graphs, for the case where all the demand pairs can be covered by $\\gamma$ faces.         ",
    "url": "https://arxiv.org/abs/1903.02758",
    "authors": [
      "Arnold Filtser"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2104.05031",
    "title": "Deformable Capsules for Object Detection",
    "abstract": "           Capsule networks promise significant benefits over convolutional networks by storing stronger internal representations, and routing information based on the agreement between intermediate representations' projections. Despite this, their success has been limited to small-scale classification datasets due to their computationally expensive nature. Though memory efficient, convolutional capsules impose geometric constraints that fundamentally limit the ability of capsules to model the pose/deformation of objects. Further, they do not address the bigger memory concern of class-capsules scaling up to bigger tasks such as detection or large-scale classification. In this study, we introduce a new family of capsule networks, deformable capsules (\\textit{DeformCaps}), to address a very important problem in computer vision: object detection. We propose two new algorithms associated with our \\textit{DeformCaps}: a novel capsule structure (\\textit{SplitCaps}), and a novel dynamic routing algorithm (\\textit{SE-Routing}), which balance computational efficiency with the need for modeling a large number of objects and classes, which have never been achieved with capsule networks before. We demonstrate that the proposed methods efficiently scale up to create the first-ever capsule network for object detection in the literature. Our proposed architecture is a one-stage detection framework and it obtains results on MS COCO which are on par with state-of-the-art one-stage CNN-based methods, while producing fewer false positive detection, generalizing to unusual poses/viewpoints of objects.         ",
    "url": "https://arxiv.org/abs/2104.05031",
    "authors": [
      "Rodney Lalonde",
      "Naji Khosravan",
      "Ulas Bagci"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.03019",
    "title": "Robust and Resource-Efficient Data-Free Knowledge Distillation by Generative Pseudo Replay",
    "abstract": "           Data-Free Knowledge Distillation (KD) allows knowledge transfer from a trained neural network (teacher) to a more compact one (student) in the absence of original training data. Existing works use a validation set to monitor the accuracy of the student over real data and report the highest performance throughout the entire process. However, validation data may not be available at distillation time either, making it infeasible to record the student snapshot that achieved the peak accuracy. Therefore, a practical data-free KD method should be robust and ideally provide monotonically increasing student accuracy during distillation. This is challenging because the student experiences knowledge degradation due to the distribution shift of the synthetic data. A straightforward approach to overcome this issue is to store and rehearse the generated samples periodically, which increases the memory footprint and creates privacy concerns. We propose to model the distribution of the previously observed synthetic samples with a generative network. In particular, we design a Variational Autoencoder (VAE) with a training objective that is customized to learn the synthetic data representations optimally. The student is rehearsed by the generative pseudo replay technique, with samples produced by the VAE. Hence knowledge degradation can be prevented without storing any samples. Experiments on image classification benchmarks show that our method optimizes the expected value of the distilled model accuracy while eliminating the large memory overhead incurred by the sample-storing methods.         ",
    "url": "https://arxiv.org/abs/2201.03019",
    "authors": [
      "Kuluhan Binici",
      "Shivam Aggarwal",
      "Nam Trung Pham",
      "Karianto Leman",
      "Tulika Mitra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.04728",
    "title": "Quasi-Framelets: Robust Graph Neural Networks via Adaptive Framelet Convolution",
    "abstract": "           This paper aims to provide a novel design of a multiscale framelet convolution for spectral graph neural networks (GNNs). While current spectral methods excel in various graph learning tasks, they often lack the flexibility to adapt to noisy, incomplete, or perturbed graph signals, making them fragile in such conditions. Our newly proposed framelet convolution addresses these limitations by decomposing graph data into low-pass and high-pass spectra through a finely-tuned multiscale approach. Our approach directly designs filtering functions within the spectral domain, allowing for precise control over the spectral components. The proposed design excels in filtering out unwanted spectral information and significantly reduces the adverse effects of noisy graph signals. Our approach not only enhances the robustness of GNNs but also preserves crucial graph features and structures. Through extensive experiments on diverse, real-world graph datasets, we demonstrate that our framelet convolution achieves superior performance in node classification tasks. It exhibits remarkable resilience to noisy data and adversarial attacks, highlighting its potential as a robust solution for real-world graph applications. This advancement opens new avenues for more adaptive and reliable spectral GNN architectures.         ",
    "url": "https://arxiv.org/abs/2201.04728",
    "authors": [
      "Mengxi Yang",
      "Dai Shi",
      "Xuebin Zheng",
      "Jie Yin",
      "Junbin Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2202.06834",
    "title": "Memory-Efficient Sequential Pattern Mining with Hybrid Tries",
    "abstract": "           This paper develops a memory-efficient approach for Sequential Pattern Mining (SPM), a fundamental topic in knowledge discovery that faces a well-known memory bottleneck for large data sets. Our methodology involves a novel hybrid trie data structure that exploits recurring patterns to compactly store the data set in memory; and a corresponding mining algorithm designed to effectively extract patterns from this compact representation. Numerical results on small to medium-sized real-life test instances show an average improvement of 85% in memory consumption and 49% in computation time compared to the state of the art. For large data sets, our algorithm stands out as the only capable SPM approach within 256GB of system memory, potentially saving 1.7TB in memory consumption.         ",
    "url": "https://arxiv.org/abs/2202.06834",
    "authors": [
      "Amin Hosseininasab",
      "Willem-Jan van Hoeve",
      "Andre A. Cire"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2205.06771",
    "title": "Empowered Neural Cellular Automata",
    "abstract": "           Information-theoretic fitness functions are becoming increasingly popular to produce generally useful, task-independent behaviors. One such universal function, dubbed empowerment, measures the amount of control an agent exerts on its environment via its sensorimotor system. Specifically, empowerment attempts to maximize the mutual information between an agent's actions and its received sensor states at a later point in time. Traditionally, empowerment has been applied to a conventional sensorimotor apparatus, such as a robot. Here, we expand the approach to a distributed, multi-agent sensorimotor system embodied by a neural cellular automaton (NCA). We show that the addition of empowerment as a secondary objective in the evolution of NCA to perform the task of morphogenesis, growing and maintaining a pre-specified shape, results in higher fitness compared to evolving for morphogenesis alone. Results suggest there may be a synergistic relationship between morphogenesis and empowerment. That is, indirectly selecting for coordination between neighboring cells over the duration of development is beneficial to the developmental process itself. Such a finding may have applications in developmental biology by providing potential mechanisms of communication between cells during growth from a single cell to a multicellular, target morphology. Source code for the experiments in this paper can be found at: \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2205.06771",
    "authors": [
      "Caitlin Grasso",
      "Josh Bongard"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2209.10082",
    "title": "Generalized Groves of Neural Additive Models: Pursuing transparent and accurate machine learning models in finance",
    "abstract": "           While machine learning methods have significantly improved model performance over traditional methods, their black-box structure makes it difficult for researchers to interpret results. For highly regulated financial industries, model transparency is equally important to accuracy. Without understanding how models work, even highly accurate machine learning methods are unlikely to be accepted. We address this issue by introducing a novel class of transparent machine learning models known as generalized groves of neural additive models. The generalized groves of neural additive models separate features into three categories: linear features, individual nonlinear features, and interacted nonlinear features. Additionally, interactions in the last category are only local. A stepwise selection algorithm distinguishes the linear and nonlinear components, and interacted groups are carefully verified by applying additive separation criteria. Through some empirical examples in finance, we demonstrate that generalized grove of neural additive models exhibit high accuracy and transparency with predominantly linear terms and only sparse nonlinear ones.         ",
    "url": "https://arxiv.org/abs/2209.10082",
    "authors": [
      "Dangxing Chen",
      "Weicheng Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Finance (q-fin.CP)"
    ]
  },
  {
    "id": "arXiv:2209.14699",
    "title": "ARQ-based Average Consensus over Directed Network Topologies with Unreliable Communication Links",
    "abstract": "           In this paper, we address the discrete-time average consensus problem in strongly connected directed graphs, where nodes exchange information over unreliable error-prone communication links. We enhance the Robustified Ratio Consensus algorithm by exploiting features of the (Hybrid) Automatic Repeat ReQuest - (H)ARQ protocol used for error control of data transmissions, in order to allow the nodes to reach asymptotic average consensus even when information is exchanged over error-prone directional networks. This strategy, apart from handling time-varying information delays induced by retransmissions of erroneous packets, can also handle packet drops that occur when exceeding a predefined packet retransmission limit. Invoking the (H)ARQ protocol allows nodes to: (a) exploit the incoming error-free acknowledgement feedback to initially acquire or later update their out-degree, (b) know whether a packet has arrived or not, and (c) determine a local upper-bound on the delays imposed by the retransmission limit. By augmenting the network's corresponding weight matrix, we show that nodes utilizing our proposed (H)ARQ Ratio Consensus algorithm can reach asymptotic average consensus over unreliable networks, while improving their convergence speed and maintaining low values in their local buffers compared to the current state-of-the-art.         ",
    "url": "https://arxiv.org/abs/2209.14699",
    "authors": [
      "Evagoras Makridis",
      "Themistoklis Charalambous",
      "Christoforos N. Hadjicostis"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2209.15525",
    "title": "Slimmable Networks for Contrastive Self-supervised Learning",
    "abstract": "           Self-supervised learning makes significant progress in pre-training large models, but struggles with small models. Mainstream solutions to this problem rely mainly on knowledge distillation, which involves a two-stage procedure: first training a large teacher model and then distilling it to improve the generalization ability of smaller ones. In this work, we introduce another one-stage solution to obtain pre-trained small models without the need for extra teachers, namely, slimmable networks for contrastive self-supervised learning (SlimCLR). A slimmable network consists of a full network and several weight-sharing sub-networks, which can be pre-trained once to obtain various networks, including small ones with low computation costs. However, interference between weight-sharing networks leads to severe performance degradation in self-supervised cases, as evidenced by gradient magnitude imbalance and gradient direction divergence. The former indicates that a small proportion of parameters produce dominant gradients during backpropagation, while the main parameters may not be fully optimized. The latter shows that the gradient direction is disordered, and the optimization process is unstable. To address these issues, we introduce three techniques to make the main parameters produce dominant gradients and sub-networks have consistent outputs. These techniques include slow start training of sub-networks, online distillation, and loss re-weighting according to model sizes. Furthermore, theoretical results are presented to demonstrate that a single slimmable linear layer is sub-optimal during linear evaluation. Thus a switchable linear probe layer is applied during linear evaluation. We instantiate SlimCLR with typical contrastive learning frameworks and achieve better performance than previous arts with fewer parameters and FLOPs. The code is at this https URL.         ",
    "url": "https://arxiv.org/abs/2209.15525",
    "authors": [
      "Shuai Zhao",
      "Linchao Zhu",
      "Xiaohan Wang",
      "Yi Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2210.11996",
    "title": "Unbalanced Triangle Detection and Enumeration Hardness for Unions of Conjunctive Queries",
    "abstract": "           We study the enumeration of answers to Unions of Conjunctive Queries (UCQs) with optimal time guarantees. More precisely, we wish to identify the queries that can be solved with linear preprocessing time and constant delay. Despite the basic nature of this problem, it was shown only recently that UCQs can be solved within these time bounds if they admit free-connex union extensions, even if all individual CQs in the union are intractable with respect to the same complexity measure. Our goal is to understand whether there exist additional tractable UCQs, not covered by the currently known algorithms. As a first step, we show that some previously unclassified UCQs are hard using the classic 3SUM hypothesis, via a known reduction from 3SUM to triangle listing in graphs. As a second step, we identify a question about a variant of this graph task that is unavoidable if we want to classify all self-join-free UCQs: is it possible to decide the existence of a triangle in a vertex-unbalanced tripartite graph in linear time? We prove that this task is equivalent in hardness to some family of UCQs. Finally, we show a dichotomy for unions of two self-join-free CQs if we assume the answer to this question is negative. In conclusion, this paper pinpoints a computational barrier in the form of a single decision problem that is key to advancing our understanding of the enumeration complexity of many UCQs. Without a breakthrough for unbalanced triangle detection, we have no hope of finding an efficient algorithm for additional unions of two self-join-free CQs. On the other hand, a sufficiently efficient unbalanced triangle detection algorithm can be turned into an efficient algorithm for a family of UCQs currently not known to be tractable.         ",
    "url": "https://arxiv.org/abs/2210.11996",
    "authors": [
      "Karl Bringmann",
      "Nofar Carmeli"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2210.13900",
    "title": "Deep NURBS -- Admissible Physics-informed Neural Networks",
    "abstract": "           In this study, we propose a new numerical scheme for physics-informed neural networks (PINNs) that enables precise and inexpensive solution for partial differential equations (PDEs) in case of arbitrary geometries while strictly enforcing Dirichlet boundary conditions. The proposed approach combines admissible NURBS parametrizations required to define the physical domain and the Dirichlet boundary conditions with a PINN solver. The fundamental boundary conditions are automatically satisfied in this novel Deep NURBS framework. We verified our new approach using two-dimensional elliptic PDEs when considering arbitrary geometries, including non-Lipschitz domains. Compared to the classical PINN solver, the Deep NURBS estimator has a remarkably high convergence rate for all the studied problems. Moreover, a desirable accuracy was realized for most of the studied PDEs using only one hidden layer of neural networks. This novel approach is considered to pave the way for more effective solutions for high-dimensional problems by allowing for more realistic physics-informed statistical learning to solve PDE-based variational problems.         ",
    "url": "https://arxiv.org/abs/2210.13900",
    "authors": [
      "Hamed Saidaoui",
      "Luis Espath",
      "R\u00e1ul Tempone"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2212.10950",
    "title": "UNIKD: UNcertainty-filtered Incremental Knowledge Distillation for Neural Implicit Representation",
    "abstract": "           Recent neural implicit representations (NIRs) have achieved great success in the tasks of 3D reconstruction and novel view synthesis. However, they require the images of a scene from different camera views to be available for one-time training. This is expensive especially for scenarios with large-scale scenes and limited data storage. In view of this, we explore the task of incremental learning for NIRs in this work. We design a student-teacher framework to mitigate the catastrophic forgetting problem. Specifically, we iterate the process of using the student as the teacher at the end of each time step and let the teacher guide the training of the student in the next step. As a result, the student network is able to learn new information from the streaming data and retain old knowledge from the teacher network simultaneously. Although intuitive, naively applying the student-teacher pipeline does not work well in our task. Not all information from the teacher network is helpful since it is only trained with the old data. To alleviate this problem, we further introduce a random inquirer and an uncertainty-based filter to filter useful information. Our proposed method is general and thus can be adapted to different implicit representations such as neural radiance field (NeRF) and neural surface field. Extensive experimental results for both 3D reconstruction and novel view synthesis demonstrate the effectiveness of our approach compared to different baselines.         ",
    "url": "https://arxiv.org/abs/2212.10950",
    "authors": [
      "Mengqi Guo",
      "Chen Li",
      "Hanlin Chen",
      "Gim Hee Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2301.07995",
    "title": "Sequential learning and control: Targeted exploration for robust performance",
    "abstract": "           We present a novel dual control strategy for uncertain linear systems based on targeted harmonic exploration and gain-scheduling with performance and excitation guarantees. In the proposed sequential approach, robust control is implemented after exploration with the main feature that the exploration is optimized with respect to the robust control performance. Specifically, we leverage recent results on finite excitation using spectral lines to determine a high probability lower bound on the resultant finite excitation of the exploration data. This provides an a priori upper bound on the remaining model uncertainty after exploration, which can further be leveraged in a gain-scheduling controller design that guarantees robust performance. This leads to a semidefinite program-based design which computes an exploration strategy with finite excitation bounds and minimal energy, and a gain-scheduled controller with probabilistic performance bounds that can be implemented after exploration. The effectiveness of our approach and its benefits over common random exploration strategies are demonstrated with an example of a system which is 'hard to learn'.         ",
    "url": "https://arxiv.org/abs/2301.07995",
    "authors": [
      "Janani Venkatasubramanian",
      "Johannes K\u00f6hler",
      "Julian Berberich",
      "Frank Allg\u00f6wer"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2302.00833",
    "title": "RobustNeRF: Ignoring Distractors with Robust Losses",
    "abstract": "           Neural radiance fields (NeRF) excel at synthesizing new views given multi-view, calibrated images of a static scene. When scenes include distractors, which are not persistent during image capture (moving objects, lighting variations, shadows), artifacts appear as view-dependent effects or 'floaters'. To cope with distractors, we advocate a form of robust estimation for NeRF training, modeling distractors in training data as outliers of an optimization problem. Our method successfully removes outliers from a scene and improves upon our baselines, on synthetic and real-world scenes. Our technique is simple to incorporate in modern NeRF frameworks, with few hyper-parameters. It does not assume a priori knowledge of the types of distractors, and is instead focused on the optimization problem rather than pre-processing or modeling transient objects. More results on our page this https URL.         ",
    "url": "https://arxiv.org/abs/2302.00833",
    "authors": [
      "Sara Sabour",
      "Suhani Vora",
      "Daniel Duckworth",
      "Ivan Krasin",
      "David J. Fleet",
      "Andrea Tagliasacchi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2304.05007",
    "title": "Privacy Amplification via Shuffling: Unified, Simplified, and Tightened",
    "abstract": "           The shuffle model of differential privacy provides promising privacy-utility balances in decentralized, privacy-preserving data analysis. However, the current analyses of privacy amplification via shuffling lack both tightness and generality. To address this issue, we propose the \\emph{variation-ratio reduction} as a comprehensive framework for privacy amplification in both single-message and multi-message shuffle protocols. It leverages two new parameterizations: the total variation bounds of local messages and the probability ratio bounds of blanket messages, to determine indistinguishability levels. Our theoretical results demonstrate that our framework provides tighter bounds, especially for local randomizers with extremal probability design, where our bounds are exactly tight. Additionally, variation-ratio reduction complements parallel composition in the shuffle model, yielding enhanced privacy accounting for popular sampling-based randomizers employed in statistical queries (e.g., range queries, marginal queries, and frequent itemset mining). Empirical findings demonstrate that our numerical amplification bounds surpass existing ones, conserving up to $30\\%$ of the budget for single-message protocols, $75\\%$ for multi-message ones, and a striking $75\\%$-$95\\%$ for parallel composition. Our bounds also result in a remarkably efficient $\\tilde{O}(n)$ algorithm that numerically amplifies privacy in less than $10$ seconds for $n=10^8$ users.         ",
    "url": "https://arxiv.org/abs/2304.05007",
    "authors": [
      "Shaowei Wang",
      "Yun Peng",
      "Jin Li",
      "Zikai Wen",
      "Zhipeng Li",
      "Shiyu Yu",
      "Di Wang",
      "Wei Yang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2305.06683",
    "title": "Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation",
    "abstract": "           This paper introduces a novel crowdsourcing worker selection algorithm, enhancing annotation quality and reducing costs. Unlike previous studies targeting simpler tasks, this study contends with the complexities of label interdependencies in sequence labeling. The proposed algorithm utilizes a Combinatorial Multi-Armed Bandit (CMAB) approach for worker selection, and a cost-effective human feedback mechanism. The challenge of dealing with imbalanced and small-scale datasets, which hinders offline simulation of worker selection, is tackled using an innovative data augmentation method termed shifting, expanding, and shrinking (SES). Rigorous testing on CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm's efficiency, with an increase in F1 score up to 100.04% of the expert-only baseline, alongside cost savings up to 65.97%. The paper also encompasses a dataset-independent test emulating annotation evaluation through a Bernoulli distribution, which still led to an impressive 97.56% F1 score of the expert baseline and 59.88% cost savings. Furthermore, our approach can be seamlessly integrated into Reinforcement Learning from Human Feedback (RLHF) systems, offering a cost-effective solution for obtaining human feedback.         ",
    "url": "https://arxiv.org/abs/2305.06683",
    "authors": [
      "Yujie Wang",
      "Chao Huang",
      "Liner Yang",
      "Zhixuan Fang",
      "Yaping Huang",
      "Yang Liu",
      "Jingsi Yu",
      "Erhong Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.15220",
    "title": "Selection for short-term empowerment accelerates the evolution of homeostatic neural cellular automata",
    "abstract": "           Empowerment -- a domain independent, information-theoretic metric -- has previously been shown to assist in the evolutionary search for neural cellular automata (NCA) capable of homeostasis when employed as a fitness function. In our previous study, we successfully extended empowerment, defined as maximum time-lagged mutual information between agents' actions and future sensations, to a distributed sensorimotor system embodied as an NCA. However, the time-delay between actions and their corresponding sensations was arbitrarily chosen. Here, we expand upon previous work by exploring how the time scale at which empowerment operates impacts its efficacy as an auxiliary objective to accelerate the discovery of homeostatic NCAs. We show that shorter time delays result in marked improvements over empowerment with longer delays, when compared to evolutionary selection only for homeostasis. Moreover, we evaluate stability and adaptability of evolved NCAs, both hallmarks of living systems that are of interest to replicate in artificial ones. We find that short-term empowered NCA are more stable and are capable of generalizing better to unseen homeostatic challenges. Taken together, these findings motivate the use of empowerment during the evolution of other artifacts, and suggest how it should be incorporated to accelerate evolution of desired behaviors for them. Source code for the experiments in this paper can be found at: this https URL.         ",
    "url": "https://arxiv.org/abs/2305.15220",
    "authors": [
      "Caitlin Grasso",
      "Josh Bongard"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2306.01812",
    "title": "SAPI: Surroundings-Aware Vehicle Trajectory Prediction at Intersections",
    "abstract": "           In this work we propose a deep learning model, i.e., SAPI, to predict vehicle trajectories at intersections. SAPI uses an abstract way to represent and encode surrounding environment by utilizing information from real-time map, right-of-way, and surrounding traffic. The proposed model consists of two convolutional network (CNN) and recurrent neural network (RNN)-based encoders and one decoder. A refiner is proposed to conduct a look-back operation inside the model, in order to make full use of raw history trajectory information. We evaluate SAPI on a proprietary dataset collected in real-world intersections through autonomous vehicles. It is demonstrated that SAPI shows promising performance when predicting vehicle trajectories at intersection, and outperforms benchmark methods. The average displacement error(ADE) and final displacement error(FDE) for 6-second prediction are 1.84m and 4.32m respectively. We also show that the proposed model can accurately predict vehicle trajectories in different scenarios.         ",
    "url": "https://arxiv.org/abs/2306.01812",
    "authors": [
      "Ethan Zhang",
      "Hao Xiao",
      "Yiqian Gan",
      "Lei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.04979",
    "title": "CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification",
    "abstract": "           Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose Coupled Contrastive Graph Representation Learning (CoCo), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. CoCo contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which not only incorporates graph representations learned from complementary views for enhanced understanding, but also encourages the similarity between cross-domain example pairs with the same semantics for domain alignment. Extensive experiments on popular datasets show that our CoCo outperforms these competing baselines in different settings generally.         ",
    "url": "https://arxiv.org/abs/2306.04979",
    "authors": [
      "Nan Yin",
      "Li Shen",
      "Mengzhu Wang",
      "Long Lan",
      "Zeyu Ma",
      "Chong Chen",
      "Xian-Sheng Hua",
      "Xiao Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.06048",
    "title": "How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?",
    "abstract": "           Recent large vision-language models such as CLIP have shown remarkable out-of-distribution (OOD) detection and generalization performance. However, their zero-shot in-distribution (ID) accuracy is often limited for downstream datasets. Recent CLIP-based fine-tuning methods such as prompt learning have demonstrated significant improvements in ID classification and OOD generalization where OOD labels are available. Nonetheless, it remains unclear whether the model is reliable to semantic shifts without OOD labels. In this paper, we aim to bridge the gap and present a comprehensive study to understand how fine-tuning impact OOD detection for few-shot downstream tasks. By framing OOD detection as multi-modal concept matching, we establish a connection between fine-tuning methods and various OOD scores. Our results suggest that a proper choice of OOD scores is essential for CLIP-based fine-tuning. In particular, the maximum concept matching (MCM) score provides a promising solution consistently. We also show that prompt learning demonstrates the state-of-the-art OOD detection performance over the zero-shot counterpart.         ",
    "url": "https://arxiv.org/abs/2306.06048",
    "authors": [
      "Yifei Ming",
      "Yixuan Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.11617",
    "title": "Robust Fully-Asynchronous Methods for Distributed Training over General Architecture",
    "abstract": "           Perfect synchronization in distributed machine learning problems is inefficient and even impossible due to the existence of latency, package losses and stragglers. We propose a Robust Fully-Asynchronous Stochastic Gradient Tracking method (R-FAST), where each device performs local computation and communication at its own pace without any form of synchronization. Different from existing asynchronous distributed algorithms, R-FAST can eliminate the impact of data heterogeneity across devices and allow for packet losses by employing a robust gradient tracking strategy that relies on properly designed auxiliary variables for tracking and buffering the overall gradient vector. More importantly, the proposed method utilizes two spanning-tree graphs for communication so long as both share at least one common root, enabling flexible designs in communication architectures. We show that R-FAST converges in expectation to a neighborhood of the optimum with a geometric rate for smooth and strongly convex objectives; and to a stationary point with a sublinear rate for general non-convex settings. Extensive experiments demonstrate that R-FAST runs 1.5-2 times faster than synchronous benchmark algorithms, such as Ring-AllReduce and D-PSGD, while still achieving comparable accuracy, and outperforms existing asynchronous SOTA algorithms, such as AD-PSGD and OSGP, especially in the presence of stragglers.         ",
    "url": "https://arxiv.org/abs/2307.11617",
    "authors": [
      "Zehan Zhu",
      "Ye Tian",
      "Yan Huang",
      "Jinming Xu",
      "Shibo He"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.12169",
    "title": "Rail-only: A Low-Cost High-Performance Network for Training LLMs with Trillion Parameters",
    "abstract": "           This paper presents a low-cost network architecture for training large language models (LLMs) at hyperscale. We study the optimal parallelization strategy of LLMs and propose a novel datacenter network design tailored to LLM's unique communication pattern. We show that LLM training generates sparse communication patterns in the network and, therefore, does not require any-to-any full-bisection network to complete efficiently. As a result, our design eliminates the spine layer in traditional GPU clusters. We name this design a Rail-only network and demonstrate that it achieves the same training performance while reducing the network cost by 38% to 77% and network power consumption by 37% to 75% compared to a conventional GPU datacenter. Our architecture also supports Mixture-of-Expert (MoE) models with all-to-all communication through forwarding, with only 4.1% to 5.6% completion time overhead for all-to-all traffic. We study the failure robustness of Rail-only networks and provide insights into the performance impact of different network and training parameters.         ",
    "url": "https://arxiv.org/abs/2307.12169",
    "authors": [
      "Weiyang Wang",
      "Manya Ghobadi",
      "Kayvon Shakeri",
      "Ying Zhang",
      "Naader Hasani"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.12488",
    "title": "How Does Naming Affect LLMs on Code Analysis Tasks?",
    "abstract": "           The Large Language Models (LLMs), such as GPT and BERT, were proposed for natural language processing (NLP) and have shown promising results as general-purpose language models. An increasing number of industry professionals and researchers are adopting LLMs for program analysis tasks. However, one significant difference between programming languages and natural languages is that a programmer has the flexibility to assign any names to variables, methods, and functions in the program, whereas a natural language writer does not. Intuitively, the quality of naming in a program affects the performance of LLMs in program analysis tasks. This paper investigates how naming affects LLMs on code analysis tasks. Specifically, we create a set of datasets with code containing nonsense or misleading names for variables, methods, and functions, respectively. We then use well-trained models (CodeBERT) to perform code analysis tasks on these datasets. The experimental results show that naming has a significant impact on the performance of code analysis tasks based on LLMs, indicating that code representation learning based on LLMs heavily relies on well-defined names in code. Additionally, we conduct a case study on some special code analysis tasks using GPT, providing further insights.         ",
    "url": "https://arxiv.org/abs/2307.12488",
    "authors": [
      "Zhilong Wang",
      "Lan Zhang",
      "Chen Cao",
      "Nanqing Luo",
      "Xinzhi Luo",
      "Peng Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.03366",
    "title": "POSIT: Promotion of Semantic Item Tail via Adversarial Learning",
    "abstract": "           In many recommendations, a handful of popular items (e.g., movies / television shows, news, etc.) can be dominant in recommendations for many users. However, we know that in a large catalog of items, users are likely interested in more than what is popular. The dominance of popular items may mean that users will not see items that they would probably enjoy. In this paper, we propose a technique to overcome this problem using adversarial machine learning. We define a metric to translate the user-level utility metric in terms of an advantage/disadvantage over items. We subsequently used that metric in an adversarial learning framework to systematically promote disadvantaged items. Distinctly, our method integrates a small-capacity model to produce semantically meaningful weights, leading to an algorithm that identifies and promotes a semantically similar item within the learning process. In the empirical study, we evaluated the proposed technique on three publicly available datasets and seven competitive baselines. The result shows that our proposed method not only improves the coverage, but also, surprisingly, improves the overall performance.         ",
    "url": "https://arxiv.org/abs/2308.03366",
    "authors": [
      "Qiuling Xu",
      "Pannaga Shivaswamy",
      "Xiangyu Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2308.08206",
    "title": "Explainable Multi-View Deep Networks Methodology for Experimental Physics",
    "abstract": "           Physical experiments often involve multiple imaging representations, such as X-ray scans and microscopic images. Deep learning models have been widely used for supervised analysis in these experiments. Combining different image representations is frequently required to analyze and make a decision properly. Consequently, multi-view data has emerged - datasets where each sample is described by views from different angles, sources, or modalities. These problems are addressed with the concept of multi-view learning. Understanding the decision-making process of deep learning models is essential for reliable and credible analysis. Hence, many explainability methods have been devised recently. Nonetheless, there is a lack of proper explainability in multi-view models, which are challenging to explain due to their architectures. In this paper, we suggest different multi-view architectures for the vision domain, each suited to another problem, and we also present a methodology for explaining these models. To demonstrate the effectiveness of our methodology, we focus on the domain of High Energy Density Physics (HEDP) experiments, where multiple imaging representations are used to assess the quality of foam samples. We apply our methodology to classify the foam samples quality using the suggested multi-view architectures. Through experimental results, we showcase the improvement of accurate architecture choice on both accuracy - 78% to 84% and AUC - 83% to 93% and present a trade-off between performance and explainability. Specifically, we demonstrate that our approach enables the explanation of individual one-view models, providing insights into the decision-making process of each view. This understanding enhances the interpretability of the overall multi-view model. The sources of this work are available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2308.08206",
    "authors": [
      "Nadav Schneider",
      "Muriel Tzdaka",
      "Galit Sturm",
      "Guy Lazovski",
      "Galit Bar",
      "Gilad Oren",
      "Raz Gvishi",
      "Gal Oren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.08538",
    "title": "Proprioceptive Learning with Soft Polyhedral Networks",
    "abstract": "           Proprioception is the \"sixth sense\" that detects limb postures with motor neurons. It requires a natural integration between the musculoskeletal systems and sensory receptors, which is challenging among modern robots that aim for lightweight, adaptive, and sensitive designs at a low cost. Here, we present the Soft Polyhedral Network with an embedded vision for physical interactions, capable of adaptive kinesthesia and viscoelastic proprioception by learning kinetic features. This design enables passive adaptations to omni-directional interactions, visually captured by a miniature high-speed motion tracking system embedded inside for proprioceptive learning. The results show that the soft network can infer real-time 6D forces and torques with accuracies of 0.25/0.24/0.35 N and 0.025/0.034/0.006 Nm in dynamic interactions. We also incorporate viscoelasticity in proprioception during static adaptation by adding a creep and relaxation modifier to refine the predicted results. The proposed soft network combines simplicity in design, omni-adaptation, and proprioceptive sensing with high accuracy, making it a versatile solution for robotics at a low cost with more than 1 million use cycles for tasks such as sensitive and competitive grasping, and touch-based geometry reconstruction. This study offers new insights into vision-based proprioception for soft robots in adaptive grasping, soft manipulation, and human-robot interaction.         ",
    "url": "https://arxiv.org/abs/2308.08538",
    "authors": [
      "Xiaobo Liu",
      "Xudong Han",
      "Wei Hong",
      "Fang Wan",
      "Chaoyang Song"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.16891",
    "title": "GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields",
    "abstract": "           It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present $\\textbf{GNFactor}$, a visual behavior cloning agent for multi-task robotic manipulation with $\\textbf{G}$eneralizable $\\textbf{N}$eural feature $\\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model ($\\textit{e.g.}$, Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of demonstrations. We observe a substantial improvement of GNFactor over current state-of-the-art methods in seen and unseen tasks, demonstrating the strong generalization ability of GNFactor. Our project website is this https URL .         ",
    "url": "https://arxiv.org/abs/2308.16891",
    "authors": [
      "Yanjie Ze",
      "Ge Yan",
      "Yueh-Hua Wu",
      "Annabella Macaluso",
      "Yuying Ge",
      "Jianglong Ye",
      "Nicklas Hansen",
      "Li Erran Li",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2309.02876",
    "title": "Non-Clashing Teaching Maps for Balls in Graphs",
    "abstract": "           Recently, Kirkpatrick et al. [ALT 2019] and Fallat et al. [JMLR 2023] introduced non-clashing teaching and showed it is the most efficient machine teaching model satisfying the Goldman-Mathias collusion-avoidance criterion. A teaching map $T$ for a concept class $\\mathcal{C}$ assigns a (teaching) set $T(C)$ of examples to each concept $C \\in \\mathcal{C}$. A teaching map is non-clashing if no pair of concepts are consistent with the union of their teaching sets. The size of a non-clashing teaching map (NCTM) $T$ is the maximum size of a teaching set $T(C)$, $C \\in \\mathcal{C}$. The non-clashing teaching dimension NCTD$(\\mathcal{C})$ of $\\mathcal{C}$ is the minimum size of an NCTM for $\\mathcal{C}$. NCTM$^+$ and NCTD$^+(\\mathcal{C})$ are defined analogously, except the teacher may only use positive examples. We study NCTMs and NCTM$^+$s for the concept class $\\mathcal{B}(G)$ consisting of all balls of a graph $G$. We show that the associated decision problem B-NCTD$^+$ for NCTD$^+$ is NP-complete in split, co-bipartite, and bipartite graphs. Surprisingly, we even prove that, unless the ETH fails, B-NCTD$^+$ does not admit an algorithm running in time $2^{2^{o(\\text{vc})}}\\cdot n^{O(1)}$, nor a kernelization algorithm outputting a kernel with $2^{o(\\text{vc})}$ vertices, where vc is the vertex cover number of $G$. We complement these lower bounds with matching upper bounds. These are extremely rare results: it is only the second problem in NP to admit such a tight double-exponential lower bound parameterized by vc, and only one of very few problems to admit such an ETH-based conditional lower bound on the number of vertices in a kernel. For trees, interval graphs, cycles, and trees of cycles, we derive NCTM$^+$s or NCTMs for $\\mathcal{B}(G)$ of size proportional to its VC-dimension, and for Gromov-hyperbolic graphs, we design an approximate NCTM$^+$ of size 2.         ",
    "url": "https://arxiv.org/abs/2309.02876",
    "authors": [
      "J\u00e9r\u00e9mie Chalopin",
      "Victor Chepoi",
      "Fionn Mc Inerney",
      "S\u00e9bastien Ratel"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2309.05072",
    "title": "Uncertainty-Aware Probabilistic Graph Neural Networks for Road-Level Traffic Accident Prediction",
    "abstract": "           Traffic accidents present substantial challenges to human safety and socio-economic development in urban areas. Developing a reliable and responsible traffic accident prediction model is crucial to addressing growing public safety concerns and enhancing the safety of urban mobility systems. Traditional methods face limitations at fine spatiotemporal scales due to the sporadic nature of highrisk accidents and the predominance of non-accident characteristics. Furthermore, while most current models show promising occurrence prediction, they overlook the uncertainties arising from the inherent nature of accidents, and then fail to adequately map the hierarchical ranking of accident risk values for more precise insights. To address these issues, we introduce the Spatiotemporal Zero-Inflated Tweedie Graph Neural Network STZITDGNN -- the first uncertainty-aware probabilistic graph deep learning model in roadlevel traffic accident prediction for multisteps. This model integrates the interpretability of the statistical Tweedie family model and the expressive power of graph neural networks. Its decoder innovatively employs a compound Tweedie model,a Poisson distribution to model the frequency of accident occurrences and a Gamma distribution to assess injury severity, supplemented by a zeroinflated component to effectively identify exessive nonincident instances. Empirical tests using realworld traffic data from London, UK, demonstrate that the STZITDGNN surpasses other baseline models across multiple benchmarks and metrics, including accident risk value prediction, uncertainty minimisation, non-accident road identification and accident occurrence accuracy. Our study demonstrates that STZTIDGNN can effectively inform targeted road monitoring, thereby improving urban road safety strategies.         ",
    "url": "https://arxiv.org/abs/2309.05072",
    "authors": [
      "Xiaowei Gao",
      "Xinke Jiang",
      "Dingyi Zhuang",
      "Huanfa Chen",
      "Shenhao Wang",
      "Stephen Law",
      "James Haworth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2310.00607",
    "title": "Understanding Robust Overfitting from the Feature Generalization Perspective",
    "abstract": "           Adversarial training (AT) constructs robust neural networks by incorporating adversarial perturbations into natural data. However, it is plagued by the issue of robust overfitting (RO), which severely damages the model's robustness. In this paper, we investigate RO from a novel feature generalization perspective. Specifically, we design factor ablation experiments to assess the respective impacts of natural data and adversarial perturbations on RO, identifying that the inducing factor of RO stems from natural data. Given that the only difference between adversarial and natural training lies in the inclusion of adversarial perturbations, we further hypothesize that adversarial perturbations degrade the generalization of features in natural data and verify this hypothesis through extensive experiments. Based on these findings, we provide a holistic view of RO from the feature generalization perspective and explain various empirical behaviors associated with RO. To examine our feature generalization perspective, we devise two representative methods, attack strength and data augmentation, to prevent the feature generalization degradation during AT. Extensive experiments conducted on benchmark datasets demonstrate that the proposed methods can effectively mitigate RO and enhance adversarial robustness.         ",
    "url": "https://arxiv.org/abs/2310.00607",
    "authors": [
      "Chaojian Yu",
      "Xiaolong Shi",
      "Jun Yu",
      "Bo Han",
      "Tongliang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.03986",
    "title": "Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation",
    "abstract": "           Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose a simple and parameter-efficient adaptation procedure for pretrained multimodal networks. In particular, we exploit modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 1% of the total parameters) and applicable to a wide range of modality combinations and tasks. We conduct a series of experiments to highlight the missing modality robustness of our proposed method on five different multimodal tasks across seven datasets. Our proposed method demonstrates versatility across various tasks and datasets, and outperforms existing methods for robust multimodal learning with missing modalities.         ",
    "url": "https://arxiv.org/abs/2310.03986",
    "authors": [
      "Md Kaykobad Reza",
      "Ashley Prater-Bennette",
      "M. Salman Asif"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.04328",
    "title": "Robust Losses for Decision-Focused Learning",
    "abstract": "           Optimization models used to make discrete decisions often contain uncertain parameters that are context-dependent and estimated through prediction. To account for the quality of the decision made based on the prediction, decision-focused learning (end-to-end predict-then-optimize) aims at training the predictive model to minimize regret, i.e., the loss incurred by making a suboptimal decision. Despite the challenge of the gradient of this loss w.r.t. the predictive model parameters being zero almost everywhere for optimization problems with a linear objective, effective gradient-based learning approaches have been proposed to minimize the expected loss, using the empirical loss as a surrogate. However, empirical regret can be an ineffective surrogate because empirical optimal decisions can vary substantially from expected optimal decisions. To understand the impact of this deficiency, we evaluate the effect of aleatoric and epistemic uncertainty on the accuracy of empirical regret as a surrogate. Next, we propose three novel loss functions that approximate expected regret more robustly. Experimental results show that training two state-of-the-art decision-focused learning approaches using robust regret losses improves test-sample empirical regret in general while keeping computational time equivalent relative to the number of training epochs.         ",
    "url": "https://arxiv.org/abs/2310.04328",
    "authors": [
      "Noah Schutte",
      "Krzysztof Postek",
      "Neil Yorke-Smith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2310.15581",
    "title": "Deep ReLU neural networks overcome the curse of dimensionality when approximating semilinear partial integro-differential equations",
    "abstract": "           In this paper we consider PIDEs with gradient-independent Lipschitz continuous nonlinearities and prove that deep neural networks with ReLU activation function can approximate solutions of such semilinear PIDEs without curse of dimensionality in the sense that the required number of parameters in the deep neural networks increases at most polynomially in both the dimension $ d $ of the corresponding PIDE and the reciprocal of the prescribed accuracy $\\epsilon $.         ",
    "url": "https://arxiv.org/abs/2310.15581",
    "authors": [
      "Ariel Neufeld",
      "Tuan Anh Nguyen",
      "Sizhou Wu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2310.15772",
    "title": "Analyzing User Characteristics of Hate Speech Spreaders on Social Media",
    "abstract": "           Hate speech on social media threatens the mental and physical well-being of individuals and contributes to real-world violence. Resharing is an important driver behind the spread of hate speech on social media. Yet, little is known about who reshares hate speech and what their characteristics are. In this paper, we analyze the role of user characteristics in hate speech resharing across different types of hate speech (e.g., political hate). For this, we proceed as follows: First, we cluster hate speech posts using large language models to identify different types of hate speech. Then we model the effects of user attributes on users' probability to reshare hate speech using an explainable machine learning model. To do so, we apply debiasing to control for selection bias in our observational social media data and further control for the latent vulnerability of users to hate speech. We find that, all else equal, users with fewer followers, fewer friends, fewer posts, and older accounts share more hate speech. This shows that users with little social influence tend to share more hate speech. Further, we find substantial heterogeneity across different types of hate speech. For example, racist and misogynistic hate is spread mostly by users with little social influence. In contrast, political anti-Trump and anti-right-wing hate is reshared by users with larger social influence. Overall, understanding the factors that drive users to share hate speech is crucial for detecting individuals at risk of engaging in harmful behavior and for designing effective mitigation strategies.         ",
    "url": "https://arxiv.org/abs/2310.15772",
    "authors": [
      "Dominique Geissler",
      "Abdurahman Maarouf",
      "Stefan Feuerriegel"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2310.17180",
    "title": "A Forward Reachability Perspective on Robust Control Invariance and Discount Factors in Reachability Analysis",
    "abstract": "           Control invariant sets are crucial for various methods that aim to design safe control policies for systems whose state constraints must be satisfied over an indefinite time horizon. In this article, we explore the connections among reachability, control invariance, and Control Barrier Functions (CBFs). Unlike prior formulations based on backward reachability concepts, by examining a forward reachability problem, we are able to establish a strong link between these three concepts. First, our findings show that the inevitable Forward Reachable Tube (FRT), which is the set of states such that every trajectory reaching the FRT must have passed through a given initial set of states, is precisely this initial set of states itself if it is a robust control invariant set with a differentiable boundary. We highlight that this statement may not hold if the boundary is not differentiable. Next, we formulate a differential game between the control and disturbance, where the inevitable FRT is characterized by the zero-superlevel set of the value function. By incorporating a discount factor in the cost function of the game, the barrier constraint of the CBF naturally arises as the constraint that is imposed on the optimal control policy. Combining these results, the value function of our FRT formulation serves as a CBF-like function, and conversely, any valid CBF is also a forward reachability value function inside the control invariant set, thereby revealing the inverse optimality of the CBF. This strong link we establish between the reachability problem and the barrier constraint, while guaranteeing the continuity of the value function, is not achievable by previous backward reachability-based formulations. As such, our work fills a crucial gap in the existing literature that is vital for constructing valid CBFs to ensure safety.         ",
    "url": "https://arxiv.org/abs/2310.17180",
    "authors": [
      "Jason J. Choi",
      "Donggun Lee",
      "Boyang Li",
      "Jonathan P. How",
      "Koushil Sreenath",
      "Sylvia L. Herbert",
      "Claire J. Tomlin"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2311.03583",
    "title": "Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu Search",
    "abstract": "           This work studies a central extremal graph theory problem inspired by a 1975 conjecture of Erd\u0151s, which aims to find graphs with a given size (number of nodes) that maximize the number of edges without having 3- or 4-cycles. We formulate this problem as a sequential decision-making problem and compare AlphaZero, a neural network-guided tree search, with tabu search, a heuristic local search method. Using either method, by introducing a curriculum -- jump-starting the search for larger graphs using good graphs found at smaller sizes -- we improve the state-of-the-art lower bounds for several sizes. We also propose a flexible graph-generation environment and a permutation-invariant network architecture for learning to search in the space of graphs.         ",
    "url": "https://arxiv.org/abs/2311.03583",
    "authors": [
      "Abbas Mehrabian",
      "Ankit Anand",
      "Hyunjik Kim",
      "Nicolas Sonnerat",
      "Matej Balog",
      "Gheorghe Comanici",
      "Tudor Berariu",
      "Andrew Lee",
      "Anian Ruoss",
      "Anna Bulanova",
      "Daniel Toyama",
      "Sam Blackwell",
      "Bernardino Romera Paredes",
      "Petar Veli\u010dkovi\u0107",
      "Laurent Orseau",
      "Joonkyung Lee",
      "Anurag Murty Naredla",
      "Doina Precup",
      "Adam Zsolt Wagner"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.06648",
    "title": "Design of Reconfigurable Intelligent Surfaces by Using S-Parameter Multiport Network Theory -- Optimization and Full-Wave Validation",
    "abstract": "           Multiport network theory has been proved to be a suitable abstraction model for analyzing and optimizing reconfigurable intelligent surfaces (RISs) in an electromagnetically consistent manner, especially for studying the impact of the electromagnetic mutual coupling among radiating elements that are spaced less than half of the wavelength apart and for considering the interrelation between the amplitude and phase of the reflection coefficients. Both representations in terms of Z-parameter (impedance) and S-parameter (scattering) matrices are widely utilized. In this paper, we embrace multiport network theory for analyzing and optimizing the reradiation properties of RIS-aided channels, and provide four new contributions. (i) We offer a thorough comparison between the Z-parameter and S-parameter representations. This comparison allows us to unveil that typical scattering models utilized for RIS-aided channels ignore the structural scattering from an RIS, which is well documented in antenna theory. We show that the structural scattering results in an unwanted specular reflection. (ii) We develop an iterative algorithm for optimizing, in the presence of electromagnetic mutual coupling, the tunable loads of an RIS based on the S-parameters representation. We prove that small perturbations of the step size of the algorithm result in larger variations of the S-parameter matrix compared with the Z-parameter matrix, resulting in a faster convergence rate. (iii) We generalize the proposed algorithm to suppress the specular reflection due to the structural scattering, while maximizing the received power towards the direction of interest, and analyze the effectiveness and tradeoffs of the proposed approach. (iv) We validate the theoretical findings and algorithms with numerical simulations and a commercial full-wave electromagnetic simulator based on the method of moments.         ",
    "url": "https://arxiv.org/abs/2311.06648",
    "authors": [
      "Andrea Abrardo",
      "Alberto Toccafondi",
      "Marco Di Renzo"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2311.14120",
    "title": "Weight fluctuations in (deep) linear neural networks and a derivation of the inverse-variance flatness relation",
    "abstract": "           We investigate the stationary (late-time) training regime of single- and two-layer underparameterized linear neural networks within the continuum limit of stochastic gradient descent (SGD) for synthetic Gaussian data. In the case of a single-layer network in the weakly underparameterized regime, the spectrum of the noise covariance matrix deviates notably from the Hessian, which can be attributed to the broken detailed balance of SGD dynamics. The weight fluctuations are in this case generally anisotropic, but effectively experience an isotropic loss. For an underparameterized two-layer network, we describe the stochastic dynamics of the weights in each layer and analyze the associated stationary covariances. We identify the inter-layer coupling as a distinct source of anisotropy for the weight fluctuations. In contrast to the single-layer case, the weight fluctuations are effectively subject to an anisotropic loss, the flatness of which is inversely related to the fluctuation variance. We thereby provide an analytical derivation of the recently observed inverse variance-flatness relation in a model of a deep linear neural network.         ",
    "url": "https://arxiv.org/abs/2311.14120",
    "authors": [
      "Markus Gross",
      "Arne P. Raulf",
      "Christoph R\u00e4th"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)"
    ]
  },
  {
    "id": "arXiv:2312.14658",
    "title": "Room Acoustic Rendering Networks with Control of Scattering and Early Reflections",
    "abstract": "           Room acoustic synthesis can be used in Virtual Reality (VR), Augmented Reality (AR) and gaming applications to enhance listeners' sense of immersion, realism and externalisation. A common approach is to use Geometrical Acoustics (GA) models to compute impulse responses at interactive speed, and fast convolution methods to apply said responses in real time. Alternatively, delay-network-based models are capable of modeling certain aspects of room acoustics, but with a significantly lower computational cost. In order to bridge the gap between these classes of models, recent work introduced delay network designs that approximate Acoustic Radiance Transfer (ART), a GA model that simulates the transfer of acoustic energy between discrete surface patches in an environment. This paper presents two key extensions of such designs. The first extension involves a new physically-based and stability-preserving design of the feedback matrices, enabling more accurate control of scattering and, more in general, of late reverberation properties. The second extension allows an arbitrary number of early reflections to be modeled with high accuracy, meaning the network can be scaled at will between computational cost and early reverb precision. The proposed extensions are compared to the baseline ART-approximating delay network as well as two reference GA models. The evaluation is based on objective measures of perceptually-relevant features, including frequency-dependent reverberation times, echo density build-up, and early decay time. Results show how the proposed extensions result in a significant improvement over the baseline model, especially for the case of non-convex geometries or the case of unevenly distributed wall absorption, both scenarios of broad practical interest.         ",
    "url": "https://arxiv.org/abs/2312.14658",
    "authors": [
      "Matteo Scerbo",
      "Lauri Savioja",
      "Enzo De Sena"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2401.01192",
    "title": "Deep-ELA: Deep Exploratory Landscape Analysis with Self-Supervised Pretrained Transformers for Single- and Multi-Objective Continuous Optimization Problems",
    "abstract": "           In many recent works, the potential of Exploratory Landscape Analysis (ELA) features to numerically characterize, in particular, single-objective continuous optimization problems has been demonstrated. These numerical features provide the input for all kinds of machine learning tasks on continuous optimization problems, ranging, i.a., from High-level Property Prediction to Automated Algorithm Selection and Automated Algorithm Configuration. Without ELA features, analyzing and understanding the characteristics of single-objective continuous optimization problems is -- to the best of our knowledge -- very limited. Yet, despite their usefulness, as demonstrated in several past works, ELA features suffer from several drawbacks. These include, in particular, (1.) a strong correlation between multiple features, as well as (2.) its very limited applicability to multi-objective continuous optimization problems. As a remedy, recent works proposed deep learning-based approaches as alternatives to ELA. In these works, e.g., point-cloud transformers were used to characterize an optimization problem's fitness landscape. However, these approaches require a large amount of labeled training data. Within this work, we propose a hybrid approach, Deep-ELA, which combines (the benefits of) deep learning and ELA features. Specifically, we pre-trained four transformers on millions of randomly generated optimization problems to learn deep representations of the landscapes of continuous single- and multi-objective optimization problems. Our proposed framework can either be used out-of-the-box for analyzing single- and multi-objective continuous optimization problems, or subsequently fine-tuned to various tasks focussing on algorithm behavior and problem understanding.         ",
    "url": "https://arxiv.org/abs/2401.01192",
    "authors": [
      "Moritz Vinzent Seiler",
      "Pascal Kerschke",
      "Heike Trautmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2401.05373",
    "title": "Dynamic Spiking Graph Neural Networks",
    "abstract": "           The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \\underline{Dy}namic \\underline{S}p\\underline{i}king \\underline{G}raph \\underline{N}eural Networks (\\method{}). To mitigate the information loss problem, \\method{} propagates early-layer information directly to the last layer for information compensation. To accommodate the memory requirements, we apply the implicit differentiation on the equilibrium state, which does not rely on the exact reverse of the forward computation. While traditional implicit differentiation methods are usually used for static situations, \\method{} extends it to the dynamic graph setting. Extensive experiments on three large-scale real-world dynamic graph datasets validate the effectiveness of \\method{} on dynamic node classification tasks with lower computational costs.         ",
    "url": "https://arxiv.org/abs/2401.05373",
    "authors": [
      "Nan Yin",
      "Mengzhu Wang",
      "Zhenghan Chen",
      "Giulia De Masi",
      "Bin Gu",
      "Huan Xiong"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2401.05925",
    "title": "Learning Segmented 3D Gaussians via Efficient Feature Unprojection for Zero-shot Neural Scene Segmentation",
    "abstract": "           Zero-shot neural scene segmentation, which reconstructs 3D neural segmentation field without manual annotations, serves as an effective way for scene understanding. However, existing models, especially the efficient 3D Gaussian-based methods, struggle to produce compact segmentation results. This issue stems primarily from their redundant learnable attributes assigned on individual Gaussians, leading to a lack of robustness against the 3D-inconsistencies in zero-shot generated raw labels. To address this problem, our work, named Compact Segmented 3D Gaussians (CoSegGaussians), proposes the Feature Unprojection and Fusion module as the segmentation field, which utilizes a shallow decoder generalizable for all Gaussians based on high-level features. Specifically, leveraging the learned Gaussian geometric parameters, semantic-aware image-based features are introduced into the scene via our unprojection technique. The lifted features, together with spatial information, are fed into the multi-scale aggregation decoder to generate segmentation identities for all Gaussians. Furthermore, we design CoSeg Loss to boost model robustness against 3D-inconsistent noises. Experimental results show that our model surpasses baselines on zero-shot semantic segmentation task, improving by ~10% mIoU over the best baseline. Code and more results will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2401.05925",
    "authors": [
      "Bin Dou",
      "Tianyu Zhang",
      "Zhaohui Wang",
      "Yongjia Ma",
      "Zejian Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.09786",
    "title": "Adaptive Self-training Framework for Fine-grained Scene Graph Generation",
    "abstract": "           Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is beneficial when adopting our proposed self-training framework to the state-of-the-art message-passing neural network (MPNN)-based SGG models. Our extensive experiments verify the effectiveness of ST-SGG on various SGG models, particularly in enhancing the performance on fine-grained predicate classes.         ",
    "url": "https://arxiv.org/abs/2401.09786",
    "authors": [
      "Kibum Kim",
      "Kanghoon Yoon",
      "Yeonjun In",
      "Jinyoung Moon",
      "Donghyun Kim",
      "Chanyoung Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.17010",
    "title": "Finetuning Large Language Models for Vulnerability Detection",
    "abstract": "           This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, handling class imbalance, and improving performance on difficult vulnerability detection datasets. This demonstrates the potential for transfer learning by finetuning large pretrained language models for specialized source code analysis tasks.         ",
    "url": "https://arxiv.org/abs/2401.17010",
    "authors": [
      "Alexey Shestov",
      "Rodion Levichev",
      "Ravil Mussabayev",
      "Evgeny Maslov",
      "Anton Cheshkov",
      "Pavel Zadorozhny"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.01163",
    "title": "Enhanced Urban Region Profiling with Adversarial Contrastive Learning",
    "abstract": "           Urban region profiling is influential for smart cities and sustainable development. However, extracting fine-grained semantics and generating robust urban region embeddings from noisy and incomplete urban data is challenging. In response, we present EUPAC (Enhanced Urban Region Profiling with Adversarial Contrastive Learning), a novel framework that enhances the robustness of urban region embeddings through joint optimization of attentive supervised and adversarial contrastive modules. Specifically, region heterogeneous graphs containing human mobility data, point of interest information, and geographic neighborhood details for each region are fed into our model, which generates region embeddings that preserve intra-region and inter-region dependencies through graph convolutional networks and multi-head attention. Meanwhile, we introduce spatially learnable augmentation to generate positive samples that are semantically similar and spatially close to the anchor, preparing for subsequent contrastive learning. Furthermore, we propose an adversarial training method to construct an effective pretext task by generating strong positive pairs and mining hard negative pairs for the region embeddings. Finally, we jointly optimize attentive supervised and adversarial contrastive learning to encourage the model to capture the high-level semantics of region embeddings while ignoring the noisy and irrelevant details. Extensive experiments on real-world datasets demonstrate the superiority of our model over state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2402.01163",
    "authors": [
      "Weiliang Chen",
      "Qianqian Ren",
      "Lin Pan",
      "Shengxi Fu",
      "Jinbao Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.06500",
    "title": "On the Fly Detection of Root Causes from Observed Data with Application to IT Systems",
    "abstract": "           This paper introduces a new structural causal model tailored for representing threshold-based IT systems and presents a new algorithm designed to rapidly detect root causes of anomalies in such systems. When root causes are not causally related, the method is proven to be correct; while an extension is proposed based on the intervention of an agent to relax this assumption. Our algorithm and its agent-based extension leverage causal discovery from offline data and engage in subgraph traversal when encountering new anomalies in online data. Our extensive experiments demonstrate the superior performance of our methods, even when applied to data generated from alternative structural causal models or real IT monitoring data.         ",
    "url": "https://arxiv.org/abs/2402.06500",
    "authors": [
      "Lei Zan",
      "Charles K. Assaad",
      "Emilie Devijver",
      "Eric Gaussier",
      "Ali A\u00eft-Bachir"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.13700",
    "title": "On the Conflict of Robustness and Learning in Collaborative Machine Learning",
    "abstract": "           Collaborative Machine Learning (CML) allows participants to jointly train a machine learning model while keeping their training data private. In many scenarios where CML is seen as the solution to privacy issues, such as health-related applications, safety is also a primary concern. To ensure that CML processes produce models that output correct and reliable decisions \\emph{even in the presence of potentially untrusted participants}, researchers propose to use \\textit{robust aggregators} to filter out malicious contributions that negatively influence the training process. In this work, we formalize the two prevalent forms of robust aggregators in the literature. We then show that neither can provide the intended protection: either they use distance-based metrics that cannot reliably identify malicious inputs to training; or use metrics based on the behavior of the loss function which create a conflict with the ability of CML participants to learn, i.e., they cannot eliminate the risk of compromise without preventing learning.         ",
    "url": "https://arxiv.org/abs/2402.13700",
    "authors": [
      "Mathilde Raynal",
      "Carmela Troncoso"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.14309",
    "title": "YOLO-TLA: An Efficient and Lightweight Small Object Detection Model based on YOLOv5",
    "abstract": "           Object detection, a crucial aspect of computer vision, has seen significant advancements in accuracy and robustness. Despite these advancements, practical applications still face notable challenges, primarily the inaccurate detection or missed detection of small objects. In this paper, we propose YOLO-TLA, an advanced object detection model building on YOLOv5. We first introduce an additional detection layer for small objects in the neck network pyramid architecture, thereby producing a feature map of a larger scale to discern finer features of small objects. Further, we integrate the C3CrossCovn module into the backbone network. This module uses sliding window feature extraction, which effectively minimizes both computational demand and the number of parameters, rendering the model more compact. Additionally, we have incorporated a global attention mechanism into the backbone network. This mechanism combines the channel information with global information to create a weighted feature map. This feature map is tailored to highlight the attributes of the object of interest, while effectively ignoring irrelevant details. In comparison to the baseline YOLOv5s model, our newly developed YOLO-TLA model has shown considerable improvements on the MS COCO validation dataset, with increases of 4.6% in mAP@0.5 and 4% in mAP@0.5:0.95, all while keeping the model size compact at 9.49M parameters. Further extending these improvements to the YOLOv5m model, the enhanced version exhibited a 1.7% and 1.9% increase in mAP@0.5 and mAP@0.5:0.95, respectively, with a total of 27.53M parameters. These results validate the YOLO-TLA model's efficient and effective performance in small object detection, achieving high accuracy with fewer parameters and computational demands.         ",
    "url": "https://arxiv.org/abs/2402.14309",
    "authors": [
      "Chun-Lin Ji",
      "Tao Yu",
      "Peng Gao",
      "Fei Wang",
      "Ru-Yue Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.15988",
    "title": "Towards Fair Graph Anomaly Detection: Problem, Benchmark Datasets, and Evaluation",
    "abstract": "           The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while avoiding biased predictions against individuals from sensitive subgroups. However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes. To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel datasets constructed from the social media platforms Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 nodes, respectively, and leverage political leanings as sensitive attributes and misinformation spreaders as anomaly labels. We demonstrate that our FairGAD datasets significantly differ from the synthetic datasets used by the research community. Using our datasets, we investigate the performance-fairness trade-off in nine existing GAD and non-graph AD methods on five state-of-the-art fairness methods. Our code and datasets are available at this https URL ",
    "url": "https://arxiv.org/abs/2402.15988",
    "authors": [
      "Neng Kai Nigel Neo",
      "Yeon-Chang Lee",
      "Yiqiao Jin",
      "Sang-Wook Kim",
      "Srijan Kumar"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.16828",
    "title": "Training Neural Networks from Scratch with Parallel Low-Rank Adapters",
    "abstract": "           The scalability of deep learning models is fundamentally limited by computing resources, memory, and communication. Although methods like low-rank adaptation (LoRA) have reduced the cost of model finetuning, its application in model pre-training remains largely unexplored. This paper explores extending LoRA to model pre-training, identifying the inherent constraints and limitations of standard LoRA in this context. We introduce LoRA-the-Explorer (LTE), a novel bi-level optimization algorithm designed to enable parallel training of multiple low-rank heads across computing nodes, thereby reducing the need for frequent synchronization. Our approach includes extensive experimentation on vision transformers using various vision datasets, demonstrating that LTE is competitive with standard pre-training.         ",
    "url": "https://arxiv.org/abs/2402.16828",
    "authors": [
      "Minyoung Huh",
      "Brian Cheung",
      "Jeremy Bernstein",
      "Phillip Isola",
      "Pulkit Agrawal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.00815",
    "title": "RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records",
    "abstract": "           We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2403.00815",
    "authors": [
      "Ran Xu",
      "Wenqi Shi",
      "Yue Yu",
      "Yuchen Zhuang",
      "Bowen Jin",
      "May D. Wang",
      "Joyce C. Ho",
      "Carl Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Other Quantitative Biology (q-bio.OT)"
    ]
  },
  {
    "id": "arXiv:2403.10997",
    "title": "N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields",
    "abstract": "           Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision. To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities. Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments. Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to distill the CLIP embeddings using deferred volumetric rendering at varying physical scales, creating a coarse-to-fine representation. Extensive experiments show that our approach outperforms the state-of-the-art feature field distillation methods on tasks such as open-vocabulary 3D segmentation and localization, demonstrating the effectiveness of the learned nested feature field.         ",
    "url": "https://arxiv.org/abs/2403.10997",
    "authors": [
      "Yash Bhalgat",
      "Iro Laina",
      "Jo\u00e3o F. Henriques",
      "Andrew Zisserman",
      "Andrea Vedaldi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.14027",
    "title": "EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship Detection through Edge-Cloud Collaboration",
    "abstract": "           Detecting marine objects inshore presents challenges owing to algorithmic intricacies and complexities in system deployment. We propose a difficulty-aware edge-cloud collaborative sensing system that splits the task into object localization and fine-grained classification. Objects are classified either at the edge or within the cloud, based on their estimated difficulty. The framework comprises a low-power device-tailored front-end model for object localization, classification, and difficulty estimation, along with a transformer-graph convolutional network-based back-end model for fine-grained classification. Our system demonstrates superior performance (mAP@0.5 +4.3%}) on widely used marine object detection datasets, significantly reducing both data transmission volume (by 95.43%) and energy consumption (by 72.7%}) at the system level. We validate the proposed system across various embedded system platforms and in real-world scenarios involving drone deployment.         ",
    "url": "https://arxiv.org/abs/2403.14027",
    "authors": [
      "Wenjun Huang",
      "Hanning Chen",
      "Yang Ni",
      "Arghavan Rezvani",
      "Sanggeon Yun",
      "Sungheon Jeon",
      "Eric Pedley",
      "Mohsen Imani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.15454",
    "title": "Emotion Detection with Transformers: A Comparative Study",
    "abstract": "           In this study, we explore the application of transformer-based models for emotion classification on text data. We train and evaluate several pre-trained transformer models, on the Emotion dataset using different variants of transformers. The paper also analyzes some factors that in-fluence the performance of the model, such as the fine-tuning of the transformer layer, the trainability of the layer, and the preprocessing of the text data. Our analysis reveals that commonly applied techniques like removing punctuation and stop words can hinder model performance. This might be because transformers strength lies in understanding contextual relationships within text. Elements like punctuation and stop words can still convey sentiment or emphasis and removing them might disrupt this context.         ",
    "url": "https://arxiv.org/abs/2403.15454",
    "authors": [
      "Mahdi Rezapour"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2404.00586",
    "title": "RLGNet: Repeating-Local-Global History Network for Temporal Knowledge Graph Reasoning",
    "abstract": "           Temporal Knowledge Graph (TKG) reasoning involves predicting future events based on historical information. However, due to the unpredictability of future events, this task is highly challenging. To address this issue, we propose a multi-scale hybrid architecture model based on ensemble learning, called RLGNet (Repeating-Local-Global History Network). Inspired by the application of multi-scale information in other fields, we introduce the concept of multi-scale information into TKG reasoning. Specifically, RLGNet captures and integrates different levels of historical information by combining modules that process information at various scales. The model comprises three modules: the Repeating History Module focuses on identifying repetitive patterns and trends in historical data, the Local History Module captures short-term changes and details, and the Global History Module provides a macro perspective on long-term changes. Additionally, to address the limitations of previous single-architecture models in generalizing across single-step and multi-step reasoning tasks, we adopted architectures based on Recurrent Neural Networks (RNN) and Multi-Layer Perceptrons (MLP) for the Local and Global History Modules, respectively. This hybrid architecture design enables the model to complement both multi-step and single-step reasoning capabilities. Finally, to address the issue of noise in TKGs, we adopt an ensemble learning strategy, combining the predictions of the three modules to reduce the impact of noise on the final prediction results. In the evaluation on six benchmark datasets, our approach generally outperforms existing TKG reasoning models in multi-step and single-step reasoning tasks.         ",
    "url": "https://arxiv.org/abs/2404.00586",
    "authors": [
      "Ao Lv",
      "Guige Ouyang",
      "Yongzhong Huang",
      "Yue Chen",
      "Haoran Xie"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.01582",
    "title": "BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System",
    "abstract": "           Text plagiarism detection task is a common natural language processing task that aims to detect whether a given text contains plagiarism or copying from other texts. In existing research, detection of high level plagiarism is still a challenge due to the lack of high quality datasets. In this paper, we propose a plagiarized text data generation method based on GPT-3.5, which produces 32,927 pairs of text plagiarism detection datasets covering a wide range of plagiarism methods, bridging the gap in this part of research. Meanwhile, we propose a plagiarism identification method based on Faiss with BERT with high efficiency and high accuracy. Our experiments show that the performance of this model outperforms other models in several metrics, including 98.86\\%, 98.90%, 98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively. At the end, we also provide a user-friendly demo platform that allows users to upload a text library and intuitively participate in the plagiarism analysis.         ",
    "url": "https://arxiv.org/abs/2404.01582",
    "authors": [
      "Jiarong Xian",
      "Jibao Yuan",
      "Peiwei Zheng",
      "Dexian Chen",
      "Nie yuntao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2404.02877",
    "title": "FlightScope: A Deep Comprehensive Review of Aircraft Detection Algorithms in Satellite Imagery",
    "abstract": "           Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research highlight the nuanced performance landscapes of these algorithms, with YOLOv5 emerging as a robust solution for aerial object detection, underlining its importance through superior mean average precision, Recall, and Intersection over Union scores. The findings described here underscore the fundamental role of algorithm selection aligned with the specific demands of satellite imagery analysis and extend a comprehensive framework to evaluate model efficacy. The benchmark toolkit and codes, available via this https URL, aims to further exploration and innovation in the realm of remote sensing object detection, paving the way for improved analytical methodologies in satellite imagery applications.         ",
    "url": "https://arxiv.org/abs/2404.02877",
    "authors": [
      "Safouane El Ghazouali",
      "Arnaud Gucciardi",
      "Francesca Venturini",
      "Nicola Venturi",
      "Michael Rueegsegger",
      "Umberto Michelucci"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.06313",
    "title": "On adversarial training and the 1 Nearest Neighbor classifier",
    "abstract": "           The ability to fool deep learning classifiers with tiny perturbations of the input has lead to the development of adversarial training in which the loss with respect to adversarial examples is minimized in addition to the training examples. While adversarial training improves the robustness of the learned classifiers, the procedure is computationally expensive, sensitive to hyperparameters and may still leave the classifier vulnerable to other types of small perturbations. In this paper we compare the performance of adversarial training to that of the simple 1 Nearest Neighbor (1NN) classifier. We prove that under reasonable assumptions, the 1NN classifier will be robust to {\\em any} small image perturbation of the training images. In experiments with 135 different binary image classification problems taken from CIFAR10, MNIST and Fashion-MNIST we find that 1NN outperforms TRADES (a powerful adversarial training algorithm) in terms of average adversarial accuracy. In additional experiments with 69 robust models taken from the current adversarial robustness leaderboard, we find that 1NN outperforms almost all of them in terms of robustness to perturbations that are only slightly different from those used during training. Taken together, our results suggest that modern adversarial training methods still fall short of the robustness of the simple 1NN classifier. our code can be found at \\url{this https URL} \\keywords{Adversarial training}         ",
    "url": "https://arxiv.org/abs/2404.06313",
    "authors": [
      "Amir Hagai",
      "Yair Weiss"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.09871",
    "title": "Explainable Online Unsupervised Anomaly Detection for Cyber-Physical Systems via Causal Discovery from Time Series",
    "abstract": "           Online unsupervised detection of anomalies is crucial to guarantee the correct operation of cyber-physical systems and the safety of humans interacting with them. State-of-the-art approaches based on deep learning via neural networks achieve outstanding performance at anomaly recognition, evaluating the discrepancy between a normal model of the system (with no anomalies) and the real-time stream of sensor time series. However, large training data and time are typically required, and explainability is still a challenge to identify the root of the anomaly and implement predictive maintainance. In this paper, we use causal discovery to learn a normal causal graph of the system, and we evaluate the persistency of causal links during real-time acquisition of sensor data to promptly detect anomalies. On two benchmark anomaly detection datasets, we show that our method has higher training efficiency, outperforms the accuracy of state-of-the-art neural architectures and correctly identifies the sources of >10 different anomalies. The code is at this https URL.         ",
    "url": "https://arxiv.org/abs/2404.09871",
    "authors": [
      "Daniele Meli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2404.13426",
    "title": "Data Privacy Vocabulary (DPV) -- Version 2",
    "abstract": "           The Data Privacy Vocabulary (DPV), developed by the W3C Data Privacy Vocabularies and Controls Community Group (DPVCG), enables the creation of machine-readable, interoperable, and standards-based representations for describing the processing of personal data. The group has also published extensions to the DPV to describe specific applications to support legislative requirements such as the EU's GDPR. The DPV fills a crucial niche in the state of the art by providing a vocabulary that can be embedded and used alongside other existing standards such as W3C ODRL, and which can be customised and extended for adapting to specifics of use-cases or domains. This article describes the version 2 iteration of the DPV in terms of its contents, methodology, current adoptions and uses, and future potential. It also describes the relevance and role of DPV in acting as a common vocabulary to support various regulatory (e.g. EU's DGA and AI Act) and community initiatives (e.g. Solid) emerging across the globe.         ",
    "url": "https://arxiv.org/abs/2404.13426",
    "authors": [
      "Harshvardhan J. Pandit",
      "Beatriz Esteves",
      "Georg P. Krog",
      "Paul Ryan",
      "Delaram Golpayegani",
      "Julian Flake"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2404.14631",
    "title": "Learning Word Embedding with Better Distance Weighting and Window Size Scheduling",
    "abstract": "           Distributed word representation (a.k.a. word embedding) is a key focus in natural language processing (NLP). As a highly successful word embedding model, Word2Vec offers an efficient method for learning distributed word representations on large datasets. However, Word2Vec lacks consideration for distances between center and context words. We propose two novel methods, Learnable Formulated Weights (LFW) and Epoch-based Dynamic Window Size (EDWS), to incorporate distance information into two variants of Word2Vec, the Continuous Bag-of-Words (CBOW) model and the Continuous Skip-gram (Skip-gram) model. For CBOW, LFW uses a formula with learnable parameters that best reflects the relationship of influence and distance between words to calculate distance-related weights for average pooling, providing insights for future NLP text modeling research. For Skip-gram, we improve its dynamic window size strategy to introduce distance information in a more balanced way. Experiments prove the effectiveness of LFW and EDWS in enhancing Word2Vec's performance, surpassing previous state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2404.14631",
    "authors": [
      "Chaohao Yang",
      "Chris Ding"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.14701",
    "title": "Deep neural networks for choice analysis: Enhancing behavioral regularity with gradient regularization",
    "abstract": "           Deep neural networks (DNNs) frequently present behaviorally irregular patterns, significantly limiting their practical potentials and theoretical validity in travel behavior modeling. This study proposes strong and weak behavioral regularities as novel metrics to evaluate the monotonicity of individual demand functions (known as the \"law of demand\"), and further designs a constrained optimization framework with six gradient regularizers to enhance DNNs' behavioral regularity. The proposed framework is applied to travel survey data from Chicago and London to examine the trade-off between predictive power and behavioral regularity for large vs. small sample scenarios and in-domain vs. out-of-domain generalizations. The results demonstrate that, unlike models with strong behavioral foundations such as the multinomial logit, the benchmark DNNs cannot guarantee behavioral regularity. However, gradient regularization (GR) increases DNNs' behavioral regularity by around 6 percentage points (pp) while retaining their relatively high predictive power. In the small sample scenario, GR is more effective than in the large sample scenario, simultaneously improving behavioral regularity by about 20 pp and log-likelihood by around 1.7%. Comparing with the in-domain generalization of DNNs, GR works more effectively in out-of-domain generalization: it drastically improves the behavioral regularity of poorly performing benchmark DNNs by around 65 pp, indicating the criticality of behavioral regularization for enhancing model transferability and application in forecasting. Moreover, the proposed framework is applicable to other NN-based choice models such as TasteNets. Future studies could use behavioral regularity as a metric along with log-likelihood in evaluating travel demand models, and investigate other methods to further enhance behavioral regularity when adopting complex machine learning models.         ",
    "url": "https://arxiv.org/abs/2404.14701",
    "authors": [
      "Siqi Feng",
      "Rui Yao",
      "Stephane Hess",
      "Ricardo A. Daziano",
      "Timothy Brathwaite",
      "Joan Walker",
      "Shenhao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.16251",
    "title": "Prompt Leakage effect and defense strategies for multi-turn LLM interactions",
    "abstract": "           Prompt leakage poses a compelling security and privacy threat in LLM applications. Leakage of system prompts may compromise intellectual property, and act as adversarial reconnaissance for an attacker. A systematic evaluation of prompt leakage threats and mitigation strategies is lacking, especially for multi-turn LLM interactions. In this paper, we systematically investigate LLM vulnerabilities against prompt leakage for 10 closed- and open-source LLMs, across four domains. We design a unique threat model which leverages the LLM sycophancy effect and elevates the average attack success rate (ASR) from 17.7% to 86.2% in a multi-turn setting. Our standardized setup further allows dissecting leakage of specific prompt contents such as task instructions and knowledge documents. We measure the mitigation effect of 7 black-box defense strategies, along with finetuning an open-source model to defend against leakage attempts. We present different combination of defenses against our threat model, including a cost analysis. Our study highlights key takeaways for building secure LLM applications and provides directions for research in multi-turn LLM interactions         ",
    "url": "https://arxiv.org/abs/2404.16251",
    "authors": [
      "Divyansh Agarwal",
      "Alexander R. Fabbri",
      "Ben Risher",
      "Philippe Laban",
      "Shafiq Joty",
      "Chien-Sheng Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2404.16500",
    "title": "Conformal Prediction of Motion Control Performance for an Automated Vehicle in Presence of Actuator Degradations and Failures",
    "abstract": "           Automated driving systems require monitoring mechanisms to ensure safe operation, especially if system components degrade or fail. Their runtime self-representation plays a key role as it provides a-priori knowledge about the system's capabilities and limitations. In this paper, we propose a data-driven approach for deriving such a self-representation model for the motion controller of an automated vehicle. A conformalized prediction model is learned and allows estimating how operational conditions as well as potential degradations and failures of the vehicle's actuators impact motion control performance. During runtime behavior generation, our predictor can provide a heuristic for determining the admissible action space.         ",
    "url": "https://arxiv.org/abs/2404.16500",
    "authors": [
      "Richard Schubert",
      "Marvin Loba",
      "Jasper S\u00fcnnemann",
      "Torben Stolte",
      "Markus Maurer"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2404.19109",
    "title": "The Shape of Money Laundering: Subgraph Representation Learning on the Blockchain with the Elliptic2 Dataset",
    "abstract": "           Subgraph representation learning is a technique for analyzing local structures (or shapes) within complex networks. Enabled by recent developments in scalable Graph Neural Networks (GNNs), this approach encodes relational information at a subgroup level (multiple connected nodes) rather than at a node level of abstraction. We posit that certain domain applications, such as anti-money laundering (AML), are inherently subgraph problems and mainstream graph techniques have been operating at a suboptimal level of abstraction. This is due in part to the scarcity of annotated datasets of real-world size and complexity, as well as the lack of software tools for managing subgraph GNN workflows at scale. To enable work in fundamental algorithms as well as domain applications in AML and beyond, we introduce Elliptic2, a large graph dataset containing 122K labeled subgraphs of Bitcoin clusters within a background graph consisting of 49M node clusters and 196M edge transactions. The dataset provides subgraphs known to be linked to illicit activity for learning the set of \"shapes\" that money laundering exhibits in cryptocurrency and accurately classifying new criminal activity. Along with the dataset we share our graph techniques, software tooling, promising early experimental results, and new domain insights already gleaned from this approach. Taken together, we find immediate practical value in this approach and the potential for a new standard in anti-money laundering and forensic analytics in cryptocurrencies and other financial networks.         ",
    "url": "https://arxiv.org/abs/2404.19109",
    "authors": [
      "Claudio Bellei",
      "Muhua Xu",
      "Ross Phillips",
      "Tom Robinson",
      "Mark Weber",
      "Tim Kaler",
      "Charles E. Leiserson",
      "Arvind",
      "Jie Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "General Finance (q-fin.GN)"
    ]
  },
  {
    "id": "arXiv:2405.02771",
    "title": "MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation Learning",
    "abstract": "           The volume of unlabelled Earth observation (EO) data is huge, but many important applications lack labelled training data. However, EO data offers the unique opportunity to pair data from different modalities and sensors automatically based on geographic location and time, at virtually no human labor cost. We seize this opportunity to create MMEarth, a diverse multi-modal pretraining dataset at global scale. Using this new corpus of 1.2 million locations, we propose a Multi-Pretext Masked Autoencoder (MP-MAE) approach to learn general-purpose representations for optical satellite images. Our approach builds on the ConvNeXt V2 architecture, a fully convolutional masked autoencoder (MAE). Drawing upon a suite of multi-modal pretext tasks, we demonstrate that our MP-MAE approach outperforms both MAEs pretrained on ImageNet and MAEs pretrained on domain-specific satellite images. This is shown on several downstream tasks including image classification and semantic segmentation. We find that pretraining with multi-modal pretext tasks notably improves the linear probing performance compared to pretraining on optical satellite images only. This also leads to better label efficiency and parameter efficiency which are crucial aspects in global scale applications.         ",
    "url": "https://arxiv.org/abs/2405.02771",
    "authors": [
      "Vishal Nedungadi",
      "Ankit Kariryaa",
      "Stefan Oehmcke",
      "Serge Belongie",
      "Christian Igel",
      "Nico Lang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.03724",
    "title": "GraphSL: An Open-Source Library for Graph Source Localization Approaches and Benchmark Datasets",
    "abstract": "           We introduce GraphSL, a new library for studying the graph source localization problem. graph diffusion and graph source localization are inverse problems in nature: graph diffusion predicts information diffusions from information sources, while graph source localization predicts information sources from information diffusions. GraphSL facilitates the exploration of various graph diffusion models for simulating information diffusions and enables the evaluation of cutting-edge source localization approaches on established benchmark datasets. The source code of GraphSL is made available at Github Repository (this https URL). Bug reports and feedback can be directed to the Github issues page (this https URL).         ",
    "url": "https://arxiv.org/abs/2405.03724",
    "authors": [
      "Junxiang Wang",
      "Liang Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2405.04941",
    "title": "Imprecise Probabilities Meet Partial Observability: Game Semantics for Robust POMDPs",
    "abstract": "           Partially observable Markov decision processes (POMDPs) rely on the key assumption that probability distributions are precisely known. Robust POMDPs (RPOMDPs) alleviate this concern by defining imprecise probabilities, referred to as uncertainty sets. While robust MDPs have been studied extensively, work on RPOMDPs is limited and primarily focuses on algorithmic solution methods. We expand the theoretical understanding of RPOMDPs by showing that 1) different assumptions on the uncertainty sets affect optimal policies and values; 2) RPOMDPs have a partially observable stochastic game (POSG) semantic; and 3) the same RPOMDP with different assumptions leads to semantically different POSGs and, thus, different policies and values. These novel semantics for RPOMDPs give access to results for POSGs, studied in game theory; concretely, we show the existence of a Nash equilibrium. Finally, we classify the existing RPOMDP literature using our semantics, clarifying under which uncertainty assumptions these existing works operate.         ",
    "url": "https://arxiv.org/abs/2405.04941",
    "authors": [
      "Eline M. Bovy",
      "Marnix Suilen",
      "Sebastian Junges",
      "Nils Jansen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2405.06884",
    "title": "Efficient PAC Learnability of Dynamical Systems Over Multilayer Networks",
    "abstract": "           Networked dynamical systems are widely used as formal models of real-world cascading phenomena, such as the spread of diseases and information. Prior research has addressed the problem of learning the behavior of an unknown dynamical system when the underlying network has a single layer. In this work, we study the learnability of dynamical systems over multilayer networks, which are more realistic and challenging. First, we present an efficient PAC learning algorithm with provable guarantees to show that the learner only requires a small number of training examples to infer an unknown system. We further provide a tight analysis of the Natarajan dimension which measures the model complexity. Asymptotically, our bound on the Nararajan dimension is tight for almost all multilayer graphs. The techniques and insights from our work provide the theoretical foundations for future investigations of learning problems for multilayer dynamical systems.         ",
    "url": "https://arxiv.org/abs/2405.06884",
    "authors": [
      "Zirou Qiu",
      "Abhijin Adiga",
      "Madhav V. Marathe",
      "S. S. Ravi",
      "Daniel J. Rosenkrantz",
      "Richard E. Stearns",
      "Anil Vullikanti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.07291",
    "title": "Robust Beamforming with Gradient-based Liquid Neural Network",
    "abstract": "           Millimeter-wave (mmWave) multiple-input multiple-output (MIMO) communication with the advanced beamforming technologies is a key enabler to meet the growing demands of future mobile communication. However, the dynamic nature of cellular channels in large-scale urban mmWave MIMO communication scenarios brings substantial challenges, particularly in terms of complexity and robustness. To address these issues, we propose a robust gradient-based liquid neural network (GLNN) framework that utilizes ordinary differential equation-based liquid neurons to solve the beamforming problem. Specifically, our proposed GLNN framework takes gradients of the optimization objective function as inputs to extract the high-order channel feature information, and then introduces a residual connection to mitigate the training burden. Furthermore, we use the manifold learning technique to compress the search space of the beamforming problem. These designs enable the GLNN to effectively maintain low complexity while ensuring strong robustness to noisy and highly dynamic channels. Extensive simulation results demonstrate that the GLNN can achieve 4.15% higher spectral efficiency than that of typical iterative algorithms, and reduce the time consumption to only 1.61% that of conventional methods.         ",
    "url": "https://arxiv.org/abs/2405.07291",
    "authors": [
      "Xinquan Wang",
      "Fenghao Zhu",
      "Chongwen Huang",
      "Ahmed Alhammadi",
      "Faouzi Bader",
      "Zhaoyang Zhang",
      "Chau Yuen",
      "Merouane Debbah"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2405.11408",
    "title": "Workload Prediction in P4 Programmable Switches",
    "abstract": "           The rapid expansion of cloud services and their unpredictable workload demands present significant challenges in resource management. Traditional resource management approaches, primarily based on static rules and thresholds, often fail to ensure cost-effectiveness and optimal resource utilization. This research introduces a predictive model designed to forecast traffic demand, aiming to shift from a reactive to a proactive resource management approach. By integrating advanced predictive analytics with the capabilities of P4 programmable switches, this study seeks to enhance the efficiency of resource utilization and improve system robustness. The goal is to equip organizations with the agility and economic efficiency required to navigate the complexities of dynamic cloud environments effectively. This approach not only promises to refine microservice resource allocation but also supports the broader objective of fostering more resilient and efficient cloud infrastructures.         ",
    "url": "https://arxiv.org/abs/2405.11408",
    "authors": [
      "Boyang Yan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2405.18507",
    "title": "Injecting Hierarchical Biological Priors into Graph Neural Networks for Flow Cytometry Prediction",
    "abstract": "           In the complex landscape of hematologic samples such as peripheral blood or bone marrow derived from flow cytometry (FC) data, cell-level prediction presents profound challenges. This work explores injecting hierarchical prior knowledge into graph neural networks (GNNs) for single-cell multi-class classification of tabular cellular data. By representing the data as graphs and encoding hierarchical relationships between classes, we propose our hierarchical plug-in method to be applied to several GNN models, namely, FCHC-GNN, and effectively designed to capture neighborhood information crucial for single-cell FC domain. Extensive experiments on our cohort of 19 distinct patients, demonstrate that incorporating hierarchical biological constraints boosts performance significantly across multiple metrics compared to baseline GNNs without such priors. The proposed approach highlights the importance of structured inductive biases for gaining improved generalization in complex biological prediction tasks.         ",
    "url": "https://arxiv.org/abs/2405.18507",
    "authors": [
      "Fatemeh Nassajian Mojarrad",
      "Lorenzo Bini",
      "Thomas Matthes",
      "St\u00e9phane Marchand-Maillet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2406.01316",
    "title": "Enhancing Inertial Hand based HAR through Joint Representation of Language, Pose and Synthetic IMUs",
    "abstract": "           Due to the scarcity of labeled sensor data in HAR, prior research has turned to video data to synthesize Inertial Measurement Units (IMU) data, capitalizing on its rich activity annotations. However, generating IMU data from videos presents challenges for HAR in real-world settings, attributed to the poor quality of synthetic IMU data and its limited efficacy in subtle, fine-grained motions. In this paper, we propose Multi$^3$Net, our novel multi-modal, multitask, and contrastive-based framework approach to address the issue of limited data. Our pretraining procedure uses videos from online repositories, aiming to learn joint representations of text, pose, and IMU simultaneously. By employing video data and contrastive learning, our method seeks to enhance wearable HAR performance, especially in recognizing subtle activities.Our experimental findings validate the effectiveness of our approach in improving HAR performance with IMU data. We demonstrate that models trained with synthetic IMU data generated from videos using our method surpass existing approaches in recognizing fine-grained activities.         ",
    "url": "https://arxiv.org/abs/2406.01316",
    "authors": [
      "Vitor Fortes Rey",
      "Lala Shakti Swarup Ray",
      "Xia Qingxin",
      "Kaishun Wu",
      "Paul Lukowicz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.01805",
    "title": "TabMDA: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting",
    "abstract": "           Tabular data is prevalent in many critical domains, yet it is often challenging to acquire in large quantities. This scarcity usually results in poor performance of machine learning models on such data. Data augmentation, a common strategy for performance improvement in vision and language tasks, typically underperforms for tabular data due to the lack of explicit symmetries in the input space. To overcome this challenge, we introduce TabMDA, a novel method for manifold data augmentation on tabular data. This method utilises a pre-trained in-context model, such as TabPFN, to map the data into an embedding space. TabMDA performs label-invariant transformations by encoding the data multiple times with varied contexts. This process explores the learned embedding space of the underlying in-context models, thereby enlarging the training dataset. TabMDA is a training-free method, making it applicable to any classifier. We evaluate TabMDA on five standard classifiers and observe significant performance improvements across various tabular datasets. Our results demonstrate that TabMDA provides an effective way to leverage information from pre-trained in-context models to enhance the performance of downstream classifiers. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.01805",
    "authors": [
      "Andrei Margeloiu",
      "Adri\u00e1n Bazaga",
      "Nikola Simidjievski",
      "Pietro Li\u00f2",
      "Mateja Jamnik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.02370",
    "title": "Query-based Semantic Gaussian Field for Scene Representation in Reinforcement Learning",
    "abstract": "           Latent scene representation plays a significant role in training reinforcement learning (RL) agents. To obtain good latent vectors describing the scenes, recent works incorporate the 3D-aware latent-conditioned NeRF pipeline into scene representation learning. However, these NeRF-related methods struggle to perceive 3D structural information due to the inefficient dense sampling in volumetric rendering. Moreover, they lack fine-grained semantic information included in their scene representation vectors because they evenly consider free and occupied spaces. Both of them can destroy the performance of downstream RL tasks. To address the above challenges, we propose a novel framework that adopts the efficient 3D Gaussian Splatting (3DGS) to learn 3D scene representation for the first time. In brief, we present the Query-based Generalizable 3DGS to bridge the 3DGS technique and scene representations with more geometrical awareness than those in NeRFs. Moreover, we present the Hierarchical Semantics Encoding to ground the fine-grained semantic features to 3D Gaussians and further distilled to the scene representation vectors. We conduct extensive experiments on two RL platforms including Maniskill2 and Robomimic across 10 different tasks. The results show that our method outperforms the other 5 baselines by a large margin. We achieve the best success rates on 8 tasks and the second-best on the other two tasks.         ",
    "url": "https://arxiv.org/abs/2406.02370",
    "authors": [
      "Jiaxu Wang",
      "Ziyi Zhang",
      "Qiang Zhang",
      "Jia Li",
      "Jingkai Sun",
      "Mingyuan Sun",
      "Junhao He",
      "Renjing Xu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2406.03519",
    "title": "Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning",
    "abstract": "           High utility and rigorous data privacy are of the main goals of a federated learning (FL) system, which learns a model from the data distributed among some clients. The latter has been tried to achieve by using differential privacy in FL (DPFL). There is often heterogeneity in clients privacy requirements, and existing DPFL works either assume uniform privacy requirements for clients or are not applicable when server is not fully trusted (our setting). Furthermore, there is often heterogeneity in batch and/or dataset size of clients, which as shown, results in extra variation in the DP noise level across clients model updates. With these sources of heterogeneity, straightforward aggregation strategies, e.g., assigning clients aggregation weights proportional to their privacy parameters will lead to lower utility. We propose Robust-HDP, which efficiently estimates the true noise level in clients model updates and reduces the noise-level in the aggregated model updates considerably. Robust-HDP improves utility and convergence speed, while being safe to the clients that may maliciously send falsified privacy parameter to server. Extensive experimental results on multiple datasets and our theoretical analysis confirm the effectiveness of Robust-HDP. Our code can be found here.         ",
    "url": "https://arxiv.org/abs/2406.03519",
    "authors": [
      "Saber Malekmohammadi",
      "Yaoliang Yu",
      "Yang Cao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2406.08749",
    "title": "Mathematical models for off-ball scoring prediction in basketball",
    "abstract": "           In professional basketball, the accurate prediction of scoring opportunities based on strategic decision-making is crucial for spatial and player evaluations. However, traditional models often face challenges in accounting for the complexities of off-ball movements, which are essential for comprehensive performance evaluations. In this study, we propose two mathematical models to predict off-ball scoring opportunities in basketball, considering pass-to-score and dribble-to-score sequences: the Ball Movement for Off-ball Scoring (BMOS) and the Ball Intercept and Movement for Off-ball Scoring (BIMOS) models. The BMOS model adapts principles from the Off-Ball Scoring Opportunities (OBSO) model, originally designed for soccer, to basketball, whereas the BIMOS model also incorporates the likelihood of interception during ball movements. We evaluated these models using player tracking data from 630 NBA games in the 2015-2016 regular season, demonstrating that the BIMOS model outperforms the BMOS model in terms of team scoring prediction accuracy, while also highlighting its potential for further development. Overall, the BIMOS model provides valuable insights for tactical analysis and player evaluation in basketball.         ",
    "url": "https://arxiv.org/abs/2406.08749",
    "authors": [
      "Rikako Kono",
      "Keisuke Fujii"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.10369",
    "title": "On the Preservation of Input/Output Directed Graph Informativeness under Crossover",
    "abstract": "           There is a broad class of networks which connect inputs to outputs. We provide a strong theoretical foundation for crossover across this class and connect it to informativeness, a measure of the connectedness of inputs to outputs. We define Input/Output Directed Graphs (or IOD Graphs) as graphs with nodes $N$ and directed edges $E$, where $N$ contains (a) a set of \"input nodes\" $I \\subset N$, where each $i \\in I$ has no incoming edges and any number of outgoing edges, and (b) a set of \"output nodes\" $O \\subset N$, where each $o \\in O$ has no outgoing edges and any number of incoming edges, and $I\\cap O = \\emptyset$. We define informativeness, which involves the connections via directed paths from the input nodes to the output nodes: A partially informative IOD Graph has at least one path from an input to an output, a very informative IOD Graph has a path from every input to some output, and a fully informative IOD Graph has a path from every input to every output. A perceptron is an example of an IOD Graph. If it has non-zero weights and any number of layers, it is fully informative. As links are removed (assigned zero weight), the perceptron might become very, partially, or not informative. We define a crossover operation on IOD Graphs in which we find subgraphs with matching sets of forward and backward directed links to \"swap.\" With this operation, IOD Graphs can be subject to evolutionary computation methods. We show that fully informative parents may yield a non-informative child. We also show that under conditions of contiguousness and the no dangling nodes condition, crossover compatible, partially informative parents yield partially informative children, and very informative input parents with partially informative output parents yield very informative children. However, even under these conditions, full informativeness may not be retained.         ",
    "url": "https://arxiv.org/abs/2406.10369",
    "authors": [
      "Andreas Duus Pape",
      "J. David Schaffer",
      "Hiroki Sayama",
      "Christopher Zosh"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2406.13891",
    "title": "DPO: Dual-Perturbation Optimization for Test-time Adaptation in 3D Object Detection",
    "abstract": "           LiDAR-based 3D object detection has seen impressive advances in recent times. However, deploying trained 3D detectors in the real world often yields unsatisfactory performance when the distribution of the test data significantly deviates from the training data due to different weather conditions, object sizes, \\textit{etc}. A key factor in this performance degradation is the diminished generalizability of pre-trained models, which creates a sharp loss landscape during training. Such sharpness, when encountered during testing, can precipitate significant performance declines, even with minor data variations. To address the aforementioned challenges, we propose \\textbf{dual-perturbation optimization (DPO)} for \\textbf{\\underline{T}est-\\underline{t}ime \\underline{A}daptation in \\underline{3}D \\underline{O}bject \\underline{D}etection (TTA-3OD)}. We minimize the sharpness to cultivate a flat loss landscape to ensure model resiliency to minor data variations, thereby enhancing the generalization of the adaptation process. To fully capture the inherent variability of the test point clouds, we further introduce adversarial perturbation to the input BEV features to better simulate the noisy test environment. As the dual perturbation strategy relies on trustworthy supervision signals, we utilize a reliable Hungarian matcher to filter out pseudo-labels sensitive to perturbations. Additionally, we introduce early Hungarian cutoff to avoid error accumulation from incorrect pseudo-labels by halting the adaptation process. Extensive experiments across three types of transfer tasks demonstrate that the proposed DPO significantly surpasses previous state-of-the-art approaches, specifically on Waymo $\\rightarrow$ KITTI, outperforming the most competitive baseline by 57.72\\% in $\\text{AP}_\\text{3D}$ and reaching 91\\% of the fully supervised upper bound.         ",
    "url": "https://arxiv.org/abs/2406.13891",
    "authors": [
      "Zhuoxiao Chen",
      "Zixin Wang",
      "Yadan Luo",
      "Sen Wang",
      "Zi Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.17538",
    "title": "Three-Stream Temporal-Shift Attention Network Based on Self-Knowledge Distillation for Micro-Expression Recognition",
    "abstract": "           Micro-expressions are subtle facial movements that occur spontaneously when people try to conceal real emotions. Micro-expression recognition is crucial in many fields, including criminal analysis and psychotherapy. However, micro-expression recognition is challenging since micro-expressions have low intensity and public datasets are small in size. To this end, a three-stream temporal-shift attention network based on self-knowledge distillation called SKD-TSTSAN is proposed in this paper. Firstly, to address the low intensity of muscle movements, we utilize learning-based motion magnification modules to enhance the intensity of muscle movements. Secondly, we employ efficient channel attention modules in the local-spatial stream to make the network focus on facial regions that are highly relevant to micro-expressions. In addition, temporal shift modules are used in the dynamic-temporal stream, which enables temporal modeling with no additional parameters by mixing motion information from two different temporal domains. Furthermore, we introduce self-knowledge distillation into the micro-expression recognition task by introducing auxiliary classifiers and using the deepest section of the network for supervision, encouraging all blocks to fully explore the features of the training set. Finally, extensive experiments are conducted on four public datasets: CASME II, SAMM, MMEW, and CAS(ME)3. The experimental results demonstrate that our SKD-TSTSAN outperforms other existing methods and achieves new state-of-the-art performance. Our code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.17538",
    "authors": [
      "Guanghao Zhu",
      "Lin Liu",
      "Yuhao Hu",
      "Haixin Sun",
      "Fang Liu",
      "Xiaohui Du",
      "Ruqian Hao",
      "Juanxiu Liu",
      "Yong Liu",
      "Hao Deng",
      "Jing Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.03190",
    "title": "Cutting through the noise to motivate people: A comprehensive analysis of COVID-19 social media posts de/motivating vaccination",
    "abstract": "           The COVID-19 pandemic exposed significant weaknesses in the healthcare information system. The overwhelming volume of misinformation on social media and other socioeconomic factors created extraordinary challenges to motivate people to take proper precautions and get vaccinated. In this context, our work explored a novel direction by analyzing an extensive dataset collected over two years, identifying the topics de/motivating the public about COVID-19 vaccination. We analyzed these topics based on time, geographic location, and political orientation. We noticed that while the motivating topics remain the same over time and geographic location, the demotivating topics change rapidly. We also identified that intrinsic motivation, rather than external mandate, is more advantageous to inspire the public. This study addresses scientific communication and public motivation in social media. It can help public health officials, policymakers, and social media platforms develop more effective messaging strategies to cut through the noise of misinformation and educate the public about scientific findings.         ",
    "url": "https://arxiv.org/abs/2407.03190",
    "authors": [
      "Ashiqur Rahman",
      "Ehsan Mohammadi",
      "Hamed Alhoori"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2407.05781",
    "title": "Regret Analysis of Multi-task Representation Learning for Linear-Quadratic Adaptive Control",
    "abstract": "           Representation learning is a powerful tool that enables learning over large multitudes of agents or domains by enforcing that all agents operate on a shared set of learned features. However, many robotics or controls applications that would benefit from collaboration operate in settings with changing environments and goals, whereas most guarantees for representation learning are stated for static settings. Toward rigorously establishing the benefit of representation learning in dynamic settings, we analyze the regret of multi-task representation learning for linear-quadratic control. This setting introduces unique challenges. Firstly, we must account for and balance the $\\textit{misspecification}$ introduced by an approximate representation. Secondly, we cannot rely on the parameter update schemes of single-task online LQR, for which least-squares often suffices, and must devise a novel scheme to ensure sufficient improvement. We demonstrate that for settings where exploration is \"benign\", the regret of any agent after $T$ timesteps scales as $\\tilde O(\\sqrt{T/H})$, where $H$ is the number of agents. In settings with \"difficult\" exploration, the regret scales as $\\tilde O(\\sqrt{d_u d_\\theta} \\sqrt{T} + T^{3/4}/H^{1/5})$, where $d_x$ is the state-space dimension, $d_u$ is the input dimension, and $d_\\theta$ is the task-specific parameter count. In both cases, by comparing to the minimax single-task regret $O(\\sqrt{d_x d_u^2}\\sqrt{T})$, we see a benefit of a large number of agents. Notably, in the difficult exploration case, by sharing a representation across tasks, the effective task-specific parameter count can often be small $d_\\theta < d_x d_u$. Lastly, we provide numerical validation of the trends we predict.         ",
    "url": "https://arxiv.org/abs/2407.05781",
    "authors": [
      "Bruce D. Lee",
      "Leonardo F. Toso",
      "Thomas T. Zhang",
      "James Anderson",
      "Nikolai Matni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2407.07058",
    "title": "An efficient implementation for solving the all pairs minimax path problem in an undirected dense graph",
    "abstract": "           We provide an efficient $ O(n^2) $ implementation for solving the all pairs minimax path problem or widest path problem in an undirected dense graph. It is a code implementation of the Algorithm 4 (MMJ distance by Calculation and Copy) in a previous paper. The distance matrix is also called the all points path distance (APPD). We conducted experiments to test the implementation and algorithm, compared it with several other algorithms for solving the APPD matrix. Result shows Algorithm 4 works good for solving the widest path or minimax path APPD matrix. It can drastically improve the efficiency for computing the APPD matrix. There are several theoretical outcomes which claim the APPD matrix can be solved accurately in $ O(n^2) $ . However, they are impractical because there is no code implementation of these algorithms. It seems Algorithm 4 is the first algorithm that has an actual code implementation for solving the APPD matrix of minimax path or widest path problem in $ O(n^2) $, in an undirected dense graph.         ",
    "url": "https://arxiv.org/abs/2407.07058",
    "authors": [
      "Gangli Liu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2407.08730",
    "title": "Evaluating Deep Neural Networks in Deployment (A Comparative and Replicability Study)",
    "abstract": "           As deep neural networks (DNNs) are increasingly used in safety-critical applications, there is a growing concern for their reliability. Even highly trained, high-performant networks are not 100% accurate. However, it is very difficult to predict their behavior during deployment without ground truth. In this paper, we provide a comparative and replicability study on recent approaches that have been proposed to evaluate the reliability of DNNs in deployment. We find that it is hard to run and reproduce the results for these approaches on their replication packages and even more difficult to run them on artifacts other than their own. Further, it is difficult to compare the effectiveness of the approaches, due to the lack of clearly defined evaluation metrics. Our results indicate that more effort is needed in our research community to obtain sound techniques for evaluating the reliability of neural networks in safety-critical domains. To this end, we contribute an evaluation framework that incorporates the considered approaches and enables evaluation on common benchmarks, using common metrics.         ",
    "url": "https://arxiv.org/abs/2407.08730",
    "authors": [
      "Eduard Pinconschi",
      "Divya Gopinath",
      "Rui Abreu",
      "Corina S. Pasareanu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2407.09174",
    "title": "DART: An Automated End-to-End Object Detection Pipeline with Data Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label Review, and Model Training",
    "abstract": "           Accurate real-time object detection is vital across numerous industrial applications, from safety monitoring to quality control. Traditional approaches, however, are hindered by arduous manual annotation and data collection, struggling to adapt to ever-changing environments and novel target objects. To address these limitations, this paper presents DART, an innovative automated end-to-end pipeline that revolutionizes object detection workflows from data collection to model evaluation. It eliminates the need for laborious human labeling and extensive data collection while achieving outstanding accuracy across diverse scenarios. DART encompasses four key stages: (1) Data Diversification using subject-driven image generation (DreamBooth with SDXL), (2) Annotation via open-vocabulary object detection (Grounding DINO) to generate bounding box and class labels, (3) Review of generated images and pseudo-labels by large multimodal models (InternVL-1.5 and GPT-4o) to guarantee credibility, and (4) Training of real-time object detectors (YOLOv8 and YOLOv10) using the verified data. We apply DART to a self-collected dataset of construction machines named Liebherr Product, which contains over 15K high-quality images across 23 categories. The current instantiation of DART significantly increases average precision (AP) from 0.064 to 0.832. Its modular design ensures easy exchangeability and extensibility, allowing for future algorithm upgrades, seamless integration of new object categories, and adaptability to customized environments without manual labeling and additional data collection. The code and dataset are released at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.09174",
    "authors": [
      "Chen Xin",
      "Andreas Hartel",
      "Enkelejda Kasneci"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.11026",
    "title": "Precise and Efficient Orbit Prediction in LEO with Machine Learning using Exogenous Variables",
    "abstract": "           The increasing volume of space objects in Earth's orbit presents a significant challenge for Space Situational Awareness (SSA). And in particular, accurate orbit prediction is crucial to anticipate the position and velocity of space objects, for collision avoidance and space debris mitigation. When performing Orbit Prediction (OP), it is necessary to consider the impact of non-conservative forces, such as atmospheric drag and gravitational perturbations, that contribute to uncertainty around the future position of spacecraft and space debris alike. Conventional propagator methods like the SGP4 inadequately account for these forces, while numerical propagators are able to model the forces at a high computational cost. To address these limitations, we propose an orbit prediction algorithm utilizing machine learning. This algorithm forecasts state vectors on a spacecraft using past positions and environmental variables like atmospheric density from external sources. The orbital data used in the paper is gathered from precision ephemeris data from the International Laser Ranging Service (ILRS), for the period of almost a year. We show how the use of machine learning and time-series techniques can produce low positioning errors at a very low computational cost, thus significantly improving SSA capabilities by providing faster and reliable orbit determination for an ever increasing number of space objects.         ",
    "url": "https://arxiv.org/abs/2407.11026",
    "authors": [
      "Francisco Caldas",
      "Cl\u00e1udia Soares"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.12068",
    "title": "Learning on Graphs with Large Language Models(LLMs): A Deep Dive into Model Robustness",
    "abstract": "           Large Language Models (LLMs) have demonstrated remarkable performance across various natural language processing tasks. Recently, several LLMs-based pipelines have been developed to enhance learning on graphs with text attributes, showcasing promising performance. However, graphs are well-known to be susceptible to adversarial attacks and it remains unclear whether LLMs exhibit robustness in learning on graphs. To address this gap, our work aims to explore the potential of LLMs in the context of adversarial attacks on graphs. Specifically, we investigate the robustness against graph structural and textual perturbations in terms of two dimensions: LLMs-as-Enhancers and LLMs-as-Predictors. Through extensive experiments, we find that, compared to shallow models, both LLMs-as-Enhancers and LLMs-as-Predictors offer superior robustness against structural and textual attacks.Based on these findings, we carried out additional analyses to investigate the underlying causes. Furthermore, we have made our benchmark library openly available to facilitate quick and fair evaluations, and to encourage ongoing innovative research in this field.         ",
    "url": "https://arxiv.org/abs/2407.12068",
    "authors": [
      "Kai Guo",
      "Zewen Liu",
      "Zhikai Chen",
      "Hongzhi Wen",
      "Wei Jin",
      "Jiliang Tang",
      "Yi Chang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.13349",
    "title": "DCNv3: Towards Next Generation Deep Cross Network for CTR Prediction",
    "abstract": "           Deep & Cross Network and its derivative models have become an important paradigm in click-through rate (CTR) prediction due to their effective balance between computational cost and performance. However, these models face four major limitations: (1) while most models claim to capture high-order feature interactions, they often do so implicitly and non-interpretably through deep neural networks (DNN), which limits the trustworthiness of the model's predictions; (2) the performance of existing explicit feature interaction methods is often weaker than that of implicit DNN, undermining their necessity; (3) many models fail to adaptively filter noise while enhancing the order of feature interactions; (4) the fusion methods of most models cannot provide suitable supervision signals for their different interaction methods. To address the identified limitations, this paper proposes the next generation Deep Cross Network (DCNv3) and Shallow & Deep Cross Network (SDCNv3). These models ensure interpretability in feature interaction modeling while exponentially increasing the order of feature interactions to achieve genuine Deep Crossing rather than just Deep & Cross. Additionally, we employ a Self-Mask operation to filter noise and reduce the number of parameters in the cross network by half. In the fusion layer, we use a simple yet effective loss weight calculation method called Tri-BCE to provide appropriate supervision signals. Comprehensive experiments on six datasets demonstrate the effectiveness, efficiency, and interpretability of DCNv3 and SDCNv3. The code, running logs, and detailed hyperparameter configurations are available at: https://anonymous.4open.science/r/DCNv3-E352.         ",
    "url": "https://arxiv.org/abs/2407.13349",
    "authors": [
      "Honghao Li",
      "Yiwen Zhang",
      "Yi Zhang",
      "Hanwei Li",
      "Lei Sang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.13863",
    "title": "A Closer Look at GAN Priors: Exploiting Intermediate Features for Enhanced Model Inversion Attacks",
    "abstract": "           Model Inversion (MI) attacks aim to reconstruct privacy-sensitive training data from released models by utilizing output information, raising extensive concerns about the security of Deep Neural Networks (DNNs). Recent advances in generative adversarial networks (GANs) have contributed significantly to the improved performance of MI attacks due to their powerful ability to generate realistic images with high fidelity and appropriate semantics. However, previous MI attacks have solely disclosed private information in the latent space of GAN priors, limiting their semantic extraction and transferability across multiple target models and datasets. To address this challenge, we propose a novel method, Intermediate Features enhanced Generative Model Inversion (IF-GMI), which disassembles the GAN structure and exploits features between intermediate blocks. This allows us to extend the optimization space from latent code to intermediate features with enhanced expressive capabilities. To prevent GAN priors from generating unrealistic images, we apply a L1 ball constraint to the optimization process. Experiments on multiple benchmarks demonstrate that our method significantly outperforms previous approaches and achieves state-of-the-art results under various settings, especially in the out-of-distribution (OOD) scenario. Our code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2407.13863",
    "authors": [
      "Yixiang Qiu",
      "Hao Fang",
      "Hongyao Yu",
      "Bin Chen",
      "MeiKang Qiu",
      "Shu-Tao Xia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.14726",
    "title": "MetaAug: Meta-Data Augmentation for Post-Training Quantization",
    "abstract": "           Post-Training Quantization (PTQ) has received significant attention because it requires only a small set of calibration data to quantize a full-precision model, which is more practical in real-world applications in which full access to a large training set is not available. However, it often leads to overfitting on the small calibration dataset. Several methods have been proposed to address this issue, yet they still rely on only the calibration set for the quantization and they do not validate the quantized model due to the lack of a validation set. In this work, we propose a novel meta-learning based approach to enhance the performance of post-training quantization. Specifically, to mitigate the overfitting problem, instead of only training the quantized model using the original calibration set without any validation during the learning process as in previous PTQ works, in our approach, we both train and validate the quantized model using two different sets of images. In particular, we propose a meta-learning based approach to jointly optimize a transformation network and a quantized model through bi-level optimization. The transformation network modifies the original calibration data and the modified data will be used as the training set to learn the quantized model with the objective that the quantized model achieves a good performance on the original calibration data. Extensive experiments on the widely used ImageNet dataset with different neural network architectures demonstrate that our approach outperforms the state-of-the-art PTQ methods.         ",
    "url": "https://arxiv.org/abs/2407.14726",
    "authors": [
      "Cuong Pham",
      "Hoang Anh Dung",
      "Cuong C. Nguyen",
      "Trung Le",
      "Dinh Phung",
      "Gustavo Carneiro",
      "Thanh-Toan Do"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.14759",
    "title": "Autonomous Nonlinear Passive Transmit-Receive Switch for Compact IoT Devices: A Three-Port Agile Network",
    "abstract": "           Recent advancements in RF technologies, especially Internet of Things (IoT) devices, require compact and integrated RF circulators or transmit-receive (TR) switches for efficient resource use. Although conventional techniques are crucial in managing signal flow to prevent signal interference to sensitive receiver components, they have some drawbacks, such as limited isolation, low switching speed, complex circuitry, bulkiness, and high cost. This work presents a smart, miniaturized, nonlinear TR switch capable of operating over a wide frequency range (0.8 - 1.3 GHz), making it suitable for IoT frequency bands. The switch achieves high isolation, low insertion loss, and intelligent transitions between transmitter and receiver without requiring external control or bias pins.         ",
    "url": "https://arxiv.org/abs/2407.14759",
    "authors": [
      "Rasool Keshavarz",
      "Amber Abdullah",
      "Negin Shariati"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2407.14923",
    "title": "RayFormer: Improving Query-Based Multi-Camera 3D Object Detection via Ray-Centric Strategies",
    "abstract": "           The recent advances in query-based multi-camera 3D object detection are featured by initializing object queries in the 3D space, and then sampling features from perspective-view images to perform multi-round query refinement. In such a framework, query points near the same camera ray are likely to sample similar features from very close pixels, resulting in ambiguous query features and degraded detection accuracy. To this end, we introduce RayFormer, a camera-ray-inspired query-based 3D object detector that aligns the initialization and feature extraction of object queries with the optical characteristics of cameras. Specifically, RayFormer transforms perspective-view image features into bird's eye view (BEV) via the lift-splat-shoot method and segments the BEV map to sectors based on the camera rays. Object queries are uniformly and sparsely initialized along each camera ray, facilitating the projection of different queries onto different areas in the image to extract distinct features. Besides, we leverage the instance information of images to supplement the uniformly initialized object queries by further involving additional queries along the ray from 2D object detection boxes. To extract unique object-level features that cater to distinct queries, we design a ray sampling method that suitably organizes the distribution of feature sampling points on both images and bird's eye view. Extensive experiments are conducted on the nuScenes dataset to validate our proposed ray-inspired model design. The proposed RayFormer achieves 55.5% mAP and 63.3% NDS, respectively. Our codes will be made available.         ",
    "url": "https://arxiv.org/abs/2407.14923",
    "authors": [
      "Xiaomeng Chu",
      "Jiajun Deng",
      "Guoliang You",
      "Yifan Duan",
      "Yao Li",
      "Yanyong Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.16716",
    "title": "Exploring The Neural Burden In Pruned Models: An Insight Inspired By Neuroscience",
    "abstract": "           Vision Transformer and its variants have been adopted in many visual tasks due to their powerful capabilities, which also bring significant challenges in computation and storage. Consequently, researchers have introduced various compression methods in recent years, among which the pruning techniques are widely used to remove a significant fraction of the network. Therefore, these methods can reduce significant percent of the FLOPs, but often lead to a decrease in model performance. To investigate the underlying causes, we focus on the pruning methods specifically belonging to the pruning-during-training category, then drew inspiration from neuroscience and propose a new concept for artificial neural network models named Neural Burden. We investigate its impact in the model pruning process, and subsequently explore a simple yet effective approach to mitigate the decline in model performance, which can be applied to any pruning-during-training technique. Extensive experiments indicate that the neural burden phenomenon indeed exists, and show the potential of our method. We hope that our findings can provide valuable insights for future research. Code will be made publicly available after this paper is published.         ",
    "url": "https://arxiv.org/abs/2407.16716",
    "authors": [
      "Zeyu Wang",
      "Weichen Dai",
      "Xiangyu Zhou",
      "Ji Qi",
      "Yi Zhou"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.17053",
    "title": "Automated Code-centric Software Vulnerability Assessment: How Far Are We? An Empirical Study in C/C++",
    "abstract": "           Background: The C and C++ languages hold significant importance in Software Engineering research because of their widespread use in practice. Numerous studies have utilized Machine Learning (ML) and Deep Learning (DL) techniques to detect software vulnerabilities (SVs) in the source code written in these languages. However, the application of these techniques in function-level SV assessment has been largely unexplored. SV assessment is increasingly crucial as it provides detailed information on the exploitability, impacts, and severity of security defects, thereby aiding in their prioritization and remediation. Aims: We conduct the first empirical study to investigate and compare the performance of ML and DL models, many of which have been used for SV detection, for function-level SV assessment in C/C++. Method: Using 9,993 vulnerable C/C++ functions, we evaluated the performance of six multi-class ML models and five multi-class DL models for the SV assessment at the function level based on the Common Vulnerability Scoring System (CVSS). We further explore multi-task learning, which can leverage common vulnerable code to predict all SV assessment outputs simultaneously in a single model, and compare the effectiveness and efficiency of this model type with those of the original multi-class models. Results: We show that ML has matching or even better performance compared to the multi-class DL models for function-level SV assessment with significantly less training time. Employing multi-task learning allows the DL models to perform significantly better, with an average of 8-22% increase in Matthews Correlation Coefficient (MCC). Conclusions: We distill the practices of using data-driven techniques for function-level SV assessment in C/C++, including the use of multi-task DL to balance efficiency and effectiveness. This can establish a strong foundation for future work in this area.         ",
    "url": "https://arxiv.org/abs/2407.17053",
    "authors": [
      "Anh The Nguyen",
      "Triet Huynh Minh Le",
      "M. Ali Babar"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2407.17164",
    "title": "Robust Deep Hawkes Process under Label Noise of Both Event and Occurrence",
    "abstract": "           Integrating deep neural networks with the Hawkes process has significantly improved predictive capabilities in finance, health informatics, and information technology. Nevertheless, these models often face challenges in real-world settings, particularly due to substantial label noise. This issue is of significant concern in the medical field, where label noise can arise from delayed updates in electronic medical records or misdiagnoses, leading to increased prediction risks. Our research indicates that deep Hawkes process models exhibit reduced robustness when dealing with label noise, particularly when it affects both event types and timing. To address these challenges, we first investigate the influence of label noise in approximated intensity functions and present a novel framework, the Robust Deep Hawkes Process (RDHP), to overcome the impact of label noise on the intensity function of Hawkes models, considering both the events and their occurrences. We tested RDHP using multiple open-source benchmarks with synthetic noise and conducted a case study on obstructive sleep apnea-hypopnea syndrome (OSAHS) in a real-world setting with inherent label noise. The results demonstrate that RDHP can effectively perform classification and regression tasks, even in the presence of noise related to events and their timing. To the best of our knowledge, this is the first study to successfully address both event and time label noise in deep Hawkes process models, offering a promising solution for medical applications, specifically in diagnosing OSAHS.         ",
    "url": "https://arxiv.org/abs/2407.17164",
    "authors": [
      "Xiaoyu Tan",
      "Bin Li",
      "Xihe Qiu",
      "Jingjing Huang",
      "Yinghui Xu",
      "Wei Chu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.17234",
    "title": "Intent-guided Heterogeneous Graph Contrastive Learning for Recommendation",
    "abstract": "           Contrastive Learning (CL)-based recommender systems have gained prominence in the context of Heterogeneous Graph (HG) due to their capacity to enhance the consistency of representations across different views. However, existing frameworks often neglect the fact that user-item interactions within HG are governed by diverse latent intents (e.g., brand preferences or demographic characteristics of item audiences), which are pivotal in capturing fine-grained relations. The exploration of these underlying intents, particularly through the lens of meta-paths in HGs, presents us with two principal challenges: i) How to integrate CL with intents; ii) How to mitigate noise from meta-path-driven intents. To address these challenges, we propose an innovative framework termed Intent-guided Heterogeneous Graph Contrastive Learning (IHGCL), which designed to enhance CL-based recommendation by capturing the intents contained within meta-paths. Specifically, the IHGCL framework includes: i) a meta-path-based Dual Contrastive Learning (DCL) approach to effectively integrate intents into the recommendation, constructing intent-intent contrast and intent-interaction contrast; ii) a Bottlenecked AutoEncoder (BAE) that combines mask propagation with the information bottleneck principle to significantly reduce noise perturbations introduced by meta-paths. Empirical evaluations conducted across six distinct datasets demonstrate the superior performance of our IHGCL framework relative to conventional baseline methods. Our model implementation is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.17234",
    "authors": [
      "Lei Sang",
      "Yu Wang",
      "Yi Zhang",
      "Yiwen Zhang",
      "Xindong Wu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.18373",
    "title": "Physics Informed Kolmogorov-Arnold Neural Networks for Dynamical Analysis via Efficent-KAN and WAV-KAN",
    "abstract": "           Physics-informed neural networks have proven to be a powerful tool for solving differential equations, leveraging the principles of physics to inform the learning process. However, traditional deep neural networks often face challenges in achieving high accuracy without incurring significant computational costs. In this work, we implement the Physics-Informed Kolmogorov-Arnold Neural Networks (PIKAN) through efficient-KAN and WAV-KAN, which utilize the Kolmogorov-Arnold representation theorem. PIKAN demonstrates superior performance compared to conventional deep neural networks, achieving the same level of accuracy with fewer layers and reduced computational overhead. We explore both B-spline and wavelet-based implementations of PIKAN and benchmark their performance across various ordinary and partial differential equations using unsupervised (data-free) and supervised (data-driven) techniques. For certain differential equations, the data-free approach suffices to find accurate solutions, while in more complex scenarios, the data-driven method enhances the PIKAN's ability to converge to the correct solution. We validate our results against numerical solutions and achieve $99 \\%$ accuracy in most scenarios.         ",
    "url": "https://arxiv.org/abs/2407.18373",
    "authors": [
      "Subhajit Patra",
      "Sonali Panda",
      "Bikram Keshari Parida",
      "Mahima Arya",
      "Kurt Jacobs",
      "Denys I. Bondar",
      "Abhijit Sen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18551",
    "title": "Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network",
    "abstract": "           Trajectory prediction is crucial for autonomous driving as it aims to forecast the future movements of traffic participants. Traditional methods usually perform holistic inference on the trajectories of agents, neglecting the differences in prediction difficulty among agents. This paper proposes a novel Difficulty-Guided Feature Enhancement Network (DGFNet), which leverages the prediction difficulty differences among agents for multi-agent trajectory prediction. Firstly, we employ spatio-temporal feature encoding and interaction to capture rich spatio-temporal features. Secondly, a difficulty-guided decoder is used to control the flow of future trajectories into subsequent modules, obtaining reliable future trajectories. Then, feature interaction and fusion are performed through the future feature interaction module. Finally, the fused agent features are fed into the final predictor to generate the predicted trajectory distributions for multiple participants. Experimental results demonstrate that our DGFNet achieves state-of-the-art performance on the Argoverse 1\\&2 motion forecasting benchmarks. Ablation studies further validate the effectiveness of each module. Moreover, compared with SOTA methods, our method balances trajectory prediction accuracy and real-time inference speed.         ",
    "url": "https://arxiv.org/abs/2407.18551",
    "authors": [
      "Guipeng Xin",
      "Duanfeng Chu",
      "Liping Lu",
      "Zejian Deng",
      "Yuang Lu",
      "Xigang Wu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18571",
    "title": "Speech Bandwidth Expansion Via High Fidelity Generative Adversarial Networks",
    "abstract": "           Speech bandwidth expansion is crucial for expanding the frequency range of low-bandwidth speech signals, thereby improving audio quality, clarity and perceptibility in digital applications. Its applications span telephony, compression, text-to-speech synthesis, and speech recognition. This paper presents a novel approach using a high-fidelity generative adversarial network, unlike cascaded systems, our system is trained end-to-end on paired narrowband and wideband speech signals. Our method integrates various bandwidth upsampling ratios into a single unified model specifically designed for speech bandwidth expansion applications. Our approach exhibits robust performance across various bandwidth expansion factors, including those not encountered during training, demonstrating zero-shot capability. To the best of our knowledge, this is the first work to showcase this capability. The experimental results demonstrate that our method outperforms previous end-to-end approaches, as well as interpolation and traditional techniques, showcasing its effectiveness in practical speech enhancement applications.         ",
    "url": "https://arxiv.org/abs/2407.18571",
    "authors": [
      "Mahmoud Salhab",
      "Haidar Harmanani"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2407.18606",
    "title": "A data balancing approach towards design of an expert system for Heart Disease Prediction",
    "abstract": "           Heart disease is a serious global health issue that claims millions of lives every year. Early detection and precise prediction are critical to the prevention and successful treatment of heart related issues. A lot of research utilizes machine learning (ML) models to forecast cardiac disease and obtain early detection. In order to do predictive analysis on \"Heart disease health indicators \" dataset. We employed five machine learning methods in this paper: Decision Tree (DT), Random Forest (RF), Linear Discriminant Analysis, Extra Tree Classifier, and AdaBoost. The model is further examined using various feature selection (FS) techniques. To enhance the baseline model, we have separately applied four FS techniques: Sequential Forward FS, Sequential Backward FS, Correlation Matrix, and Chi2. Lastly, K means SMOTE oversampling is applied to the models to enable additional analysis. The findings show that when it came to predicting heart disease, ensemble approaches in particular, random forests performed better than individual classifiers. The presence of smoking, blood pressure, cholesterol, and physical inactivity were among the major predictors that were found. The accuracy of the Random Forest and Decision Tree model was 99.83%. This paper demonstrates how machine learning models can improve the accuracy of heart disease prediction, especially when using ensemble methodologies. The models provide a more accurate risk assessment than traditional methods since they incorporate a large number of factors and complex algorithms.         ",
    "url": "https://arxiv.org/abs/2407.18606",
    "authors": [
      "Rahul Karmakar",
      "Udita Ghosh",
      "Arpita Pal",
      "Sattwiki Dey",
      "Debraj Malik",
      "Priyabrata Sain"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18752",
    "title": "Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery",
    "abstract": "           Causal discovery aims to estimate causal structures among variables based on observational data. Large Language Models (LLMs) offer a fresh perspective to tackle the causal discovery problem by reasoning on the metadata associated with variables rather than their actual data values, an approach referred to as knowledge-based causal discovery. In this paper, we investigate the capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1 billion parameters) with prompt-based learning for knowledge-based causal discovery. Specifically, we present KG Structure as Prompt, a novel approach for integrating structural information from a knowledge graph, such as common neighbor nodes and metapaths, into prompt-based learning to enhance the capabilities of SLMs. Experimental results on three types of biomedical and open-domain datasets under few-shot settings demonstrate the effectiveness of our approach, surpassing most baselines and even conventional fine-tuning approaches trained on full datasets. Our findings further highlight the strong capabilities of SLMs: in combination with knowledge graphs and prompt-based learning, SLMs demonstrate the potential to surpass LLMs with larger number of parameters. Our code and datasets are available on GitHub.         ",
    "url": "https://arxiv.org/abs/2407.18752",
    "authors": [
      "Yuni Susanti",
      "Michael F\u00e4rber"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18910",
    "title": "Do We Really Need Graph Convolution During Training? Light Post-Training Graph-ODE for Efficient Recommendation",
    "abstract": "           The efficiency and scalability of graph convolution networks (GCNs) in training recommender systems (RecSys) have been persistent concerns, hindering their deployment in real-world applications. This paper presents a critical examination of the necessity of graph convolutions during the training phase and introduces an innovative alternative: the Light Post-Training Graph Ordinary-Differential-Equation (LightGODE). Our investigation reveals that the benefits of GCNs are more pronounced during testing rather than training. Motivated by this, LightGODE utilizes a novel post-training graph convolution method that bypasses the computation-intensive message passing of GCNs and employs a non-parametric continuous graph ordinary-differential-equation (ODE) to dynamically model node representations. This approach drastically reduces training time while achieving fine-grained post-training graph convolution to avoid the distortion of the original training embedding space, termed the embedding discrepancy issue. We validate our model across several real-world datasets of different scales, demonstrating that LightGODE not only outperforms GCN-based models in terms of efficiency and effectiveness but also significantly mitigates the embedding discrepancy commonly associated with deeper graph convolution layers. Our LightGODE challenges the prevailing paradigms in RecSys training and suggests re-evaluating the role of graph convolutions, potentially guiding future developments of efficient large-scale graph-based RecSys.         ",
    "url": "https://arxiv.org/abs/2407.18910",
    "authors": [
      "Weizhi Zhang",
      "Liangwei Yang",
      "Zihe Song",
      "Henry Peng Zou",
      "Ke Xu",
      "Liancheng Fang",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2112.14233",
    "title": "Multitask Learning and Bandits via Robust Statistics",
    "abstract": "           Decision-makers often simultaneously face many related but heterogeneous learning problems. For instance, a large retailer may wish to learn product demand at different stores to solve pricing or inventory problems, making it desirable to learn jointly for stores serving similar customers; alternatively, a hospital network may wish to learn patient risk at different providers to allocate personalized interventions, making it desirable to learn jointly for hospitals serving similar patient populations. Motivated by real datasets, we study a natural setting where the unknown parameter in each learning instance can be decomposed into a shared global parameter plus a sparse instance-specific term. We propose a novel two-stage multitask learning estimator that exploits this structure in a sample-efficient way, using a unique combination of robust statistics (to learn across similar instances) and LASSO regression (to debias the results). Our estimator yields improved sample complexity bounds in the feature dimension $d$ relative to commonly-employed estimators; this improvement is exponential for \"data-poor\" instances, which benefit the most from multitask learning. We illustrate the utility of these results for online learning by embedding our multitask estimator within simultaneous contextual bandit algorithms. We specify a dynamic calibration of our estimator to appropriately balance the bias-variance tradeoff over time, improving the resulting regret bounds in the context dimension $d$. Finally, we illustrate the value of our approach on synthetic and real datasets.         ",
    "url": "https://arxiv.org/abs/2112.14233",
    "authors": [
      "Kan Xu",
      "Hamsa Bastani"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2308.04428",
    "title": "Sample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data",
    "abstract": "           A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators $M$ from noisy vector measurements $y = Mx + w$, where the covariates $x$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic representation learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, $\\texttt{De-bias & Feature-Whiten}$ ($\\texttt{DFW}$), of the popular alternating minimization-descent scheme proposed independently in Collins et al., (2021) and Nayer and Vaswani (2022), and establish linear convergence to the optimal representation with noise level scaling down with the $\\textit{total}$ source data size. This leads to generalization bounds on the same order as an oracle empirical risk minimizer. We verify the vital importance of $\\texttt{DFW}$ on various numerical simulations. In particular, we show that vanilla alternating-minimization descent fails catastrophically even for iid, but mildly non-isotropic data. Our analysis unifies and generalizes prior work, and provides a flexible framework for a wider range of applications, such as in controls and dynamical systems.         ",
    "url": "https://arxiv.org/abs/2308.04428",
    "authors": [
      "Thomas T.C.K. Zhang",
      "Leonardo F. Toso",
      "James Anderson",
      "Nikolai Matni"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2310.09597",
    "title": "Adaptive maximization of social welfare",
    "abstract": "           We consider the problem of repeatedly choosing policies to maximize social welfare. Welfare is a weighted sum of private utility and public revenue. Earlier outcomes inform later policies. Utility is not observed, but indirectly inferred. Response functions are learned through experimentation. We derive a lower bound on regret, and a matching adversarial upper bound for a variant of the Exp3 algorithm. Cumulative regret grows at a rate of $T^{2/3}$. This implies that (i) welfare maximization is harder than the multi-armed bandit problem (with a rate of $T^{1/2}$ for finite policy sets), and (ii) our algorithm achieves the optimal rate. For the stochastic setting, if social welfare is concave, we can achieve a rate of $T^{1/2}$ (for continuous policy sets), using a dyadic search algorithm. We analyze an extension to nonlinear income taxation, and sketch an extension to commodity taxation. We compare our setting to monopoly pricing (which is easier), and price setting for bilateral trade (which is harder).         ",
    "url": "https://arxiv.org/abs/2310.09597",
    "authors": [
      "Nicolo Cesa-Bianchi",
      "Roberto Colomboni",
      "Maximilian Kasy"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.18203",
    "title": "Exploring the space of graphs with fixed discrete curvatures",
    "abstract": "           Discrete curvatures are quantities associated to the nodes and edges of a graph that reflect the local geometry around them. These curvatures have a rich mathematical theory and they have recently found success as a tool to analyze networks across a wide range of domains. In this work, we consider the problem of constructing graphs with a prescribed set of discrete edge curvatures, and explore the space of such graphs. We address this problem in two ways: first, we develop an evolutionary algorithm to sample graphs with discrete curvatures close to a given set. We use this algorithm to explore how other network statistics vary when constrained by the discrete curvatures in the network. Second, we solve the exact reconstruction problem for the specific case of Forman-Ricci curvature. By leveraging the theory of Markov bases, we obtain a finite set of rewiring moves that connects the space of all graphs with a fixed discrete curvature.         ",
    "url": "https://arxiv.org/abs/2402.18203",
    "authors": [
      "Michelle Roost",
      "Karel Devriendt",
      "Giulio Zucal",
      "J\u00fcrgen Jost"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2403.16397",
    "title": "RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band Radiomap Reconstruction via Graph Attention Networks",
    "abstract": "           Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless communications for tasks such as spectrum management and network planning. However, traditional machine-learning-based MB-RMR methods, which rely heavily on simulated data or complete structured ground truth, face significant deployment challenges. These challenges stem from the differences between simulated and actual data, as well as the scarcity of real-world measurements. To address these challenges, our study presents RadioGAT, a novel framework based on Graph Attention Network (GAT) tailored for MB-RMR within a single area, eliminating the need for multi-region datasets. RadioGAT innovatively merges model-based spatial-spectral correlation encoding with data-driven radiomap generalization, thus minimizing the reliance on extensive data sources. The framework begins by transforming sparse multi-band data into a graph structure through an innovative encoding strategy that leverages radio propagation models to capture the spatial-spectral correlation inherent in the data. This graph-based representation not only simplifies data handling but also enables tailored label sampling during training, significantly enhancing the framework's adaptability for deployment. Subsequently, The GAT is employed to generalize the radiomap information across various frequency bands. Extensive experiments using raytracing datasets based on real-world environments have demonstrated RadioGAT's enhanced accuracy in supervised learning settings and its robustness in semi-supervised scenarios. These results underscore RadioGAT's effectiveness and practicality for MB-RMR in environments with limited data availability.         ",
    "url": "https://arxiv.org/abs/2403.16397",
    "authors": [
      "Xiaojie Li",
      "Songyang Zhang",
      "Hang Li",
      "Xiaoyang Li",
      "Lexi Xu",
      "Haigao Xu",
      "Hui Mei",
      "Guangxu Zhu",
      "Nan Qi",
      "Ming Xiao"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.07934",
    "title": "Identifying macro conditional independencies and macro total effects in summary causal graphs with latent confounding",
    "abstract": "           Understanding causal relations in dynamic systems is essential in epidemiology. While causal inference methods have been extensively studied, they often rely on fully specified causal graphs, which may not always be available in complex dynamic systems. Partially specified causal graphs, such as summary causal graphs (SCGs), provide a simplified representation of causal relations, omitting temporal information and focusing on high-level causal structures. This simplification introduces new challenges concerning the types of queries of interest: macro queries, which involve relationships between clusters represented as vertices in the graph, and micro queries, which pertain to relationships between variables that are not directly visible through the vertices of the graph. In this paper, we first clearly distinguish between macro conditional independencies and micro conditional independencies and between macro total effects and micro total effects. Then, we demonstrate the soundness and completeness of the d-separation to identify macro conditional independencies in SCGs. Furthermore, we establish that the do-calculus is sound and complete for identifying macro total effects in SCGs. Finally, we give a graphical characterization for the non-identifiability of macro total effects in SCGs.         ",
    "url": "https://arxiv.org/abs/2407.07934",
    "authors": [
      "Simon Ferreira",
      "Charles K. Assaad"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.16715",
    "title": "Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning",
    "abstract": "           In clinical treatment, identifying potential adverse reactions of drugs can help assist doctors in making medication decisions. In response to the problems in previous studies that features are high-dimensional and sparse, independent prediction models need to be constructed for each adverse reaction of drugs, and the prediction accuracy is low, this paper develops an adverse drug reaction prediction model based on knowledge graph embedding and deep learning, which can predict experimental results. Unified prediction of adverse drug reactions covered. Knowledge graph embedding technology can fuse the associated information between drugs and alleviate the shortcomings of high-dimensional sparsity in feature matrices, and the efficient training capabilities of deep learning can improve the prediction accuracy of the model. This article builds an adverse drug reaction knowledge graph based on drug feature data; by analyzing the embedding effect of the knowledge graph under different embedding strategies, the best embedding strategy is selected to obtain sample vectors; and then a convolutional neural network model is constructed to predict adverse reactions. The results show that under the DistMult embedding model and 400-dimensional embedding strategy, the convolutional neural network model has the best prediction effect; the average accuracy, F_1 score, recall rate and area under the curve of repeated experiments are better than the methods reported in the literature. The obtained prediction model has good prediction accuracy and stability, and can provide an effective reference for later safe medication guidance.         ",
    "url": "https://arxiv.org/abs/2407.16715",
    "authors": [
      "Yufeng Li",
      "Wenchao Zhao",
      "Bo Dang",
      "Xu Yan",
      "Weimin Wang",
      "Min Gao",
      "Mingxuan Xiao"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.16739",
    "title": "Forecasting Automotive Supply Chain Shortfalls with Heterogeneous Time Series",
    "abstract": "           Operational disruptions can significantly impact companies performance. Ford, with its 37 plants globally, uses 17 billion parts annually to manufacture six million cars and trucks. With up to ten tiers of suppliers between the company and raw materials, any extended disruption in this supply chain can cause substantial financial losses. Therefore, the ability to forecast and identify such disruptions early is crucial for maintaining seamless operations. In this study, we demonstrate how we construct a dataset consisting of many multivariate time series to forecast first-tier supply chain disruptions, utilizing features related to capacity, inventory, utilization, and processing, as outlined in the classical Factory Physics framework. This dataset is technically challenging due to its vast scale of over five hundred thousand time series. Furthermore, these time series, while exhibiting certain similarities, also display heterogeneity within specific subgroups. To address these challenges, we propose a novel methodology that integrates an enhanced Attention Sequence to Sequence Deep Learning architecture, using Neural Network Embeddings to model group effects, with a Survival Analysis model. This model is designed to learn intricate heterogeneous data patterns related to operational disruptions. Our model has demonstrated a strong performance, achieving 0.85 precision and 0.8 recall during the Quality Assurance (QA) phase across Ford's five North American plants. Additionally, to address the common criticism of Machine Learning models as black boxes, we show how the SHAP framework can be used to generate feature importance from the model predictions. It offers valuable insights that can lead to actionable strategies and highlights the potential of advanced machine learning for managing and mitigating supply chain risks in the automotive industry.         ",
    "url": "https://arxiv.org/abs/2407.16739",
    "authors": [
      "Bach Viet Do",
      "Xingyu Li",
      "Chaoye Pan",
      "Oleg Gusikhin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  }
]