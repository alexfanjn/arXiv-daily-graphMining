[
  {
    "id": "arXiv:2407.18264",
    "title": "Latency optimized Deep Neural Networks (DNNs): An Artificial Intelligence approach at the Edge using Multiprocessor System on Chip (MPSoC)",
    "abstract": "           Almost in every heavily computation-dependent application, from 6G communication systems to autonomous driving platforms, a large portion of computing should be near to the client side. Edge computing (AI at Edge) in mobile devices is one of the optimized approaches for addressing this requirement. Therefore, in this work, the possibilities and challenges of implementing a low-latency and power-optimized smart mobile system are examined. Utilizing Field Programmable Gate Array (FPGA) based solutions at the edge will lead to bandwidth-optimized designs and as a consequence can boost the computational effectiveness at a system-level deadline. Moreover, various performance aspects and implementation feasibilities of Neural Networks (NNs) on both embedded FPGA edge devices (using Xilinx Multiprocessor System on Chip (MPSoC)) and Cloud are discussed throughout this research. The main goal of this work is to demonstrate a hybrid system that uses the deep learning programmable engine developed by Xilinx Inc. as the main component of the hardware accelerator. Then based on this design, an efficient system for mobile edge computing is represented by utilizing an embedded solution.         ",
    "url": "https://arxiv.org/abs/2407.18264",
    "authors": [
      "Seyed Nima Omidsajedi",
      "Rekha Reddy",
      "Jianming Yi",
      "Jan Herbst",
      "Christoph Lipps",
      "Hans Dieter Schotten"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18267",
    "title": "MCU-MixQ: A HW/SW Co-optimized Mixed-precision Neural Network Design Framework for MCUs",
    "abstract": "           Mixed-precision neural network (MPNN) that utilizes just enough data width for the neural network processing is an effective approach to meet the stringent resources constraints including memory and computing of MCUs. Nevertheless, there is still a lack of sub-byte and mixed-precision SIMD operations in MCU-class ISA and the limited computing capability of MCUs remains underutilized, which further aggravates the computing bound encountered in neural network processing. As a result, the benefits of MPNNs cannot be fully unleashed. In this work, we propose to pack multiple low-bitwidth arithmetic operations within a single instruction multiple data (SIMD) instructions in typical MCUs, and then develop an efficient convolution operator by exploring both the data parallelism and computing parallelism in convolution along with the proposed SIMD packing. Finally, we further leverage Neural Architecture Search (NAS) to build a HW/SW co-designed MPNN design framework, namely MCU-MixQ. This framework can optimize both the MPNN quantization and MPNN implementation efficiency, striking an optimized balance between neural network performance and accuracy. According to our experiment results, MCU-MixQ achieves 2.1$\\times$ and 1.4$\\times$ speedup over CMix-NN and MCUNet respectively under the same resource constraints.         ",
    "url": "https://arxiv.org/abs/2407.18267",
    "authors": [
      "Junfeng Gong",
      "Cheng Liu",
      "Long Cheng",
      "Huawei Li",
      "Xiaowei Li"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18271",
    "title": "Large Language Model for Verilog Generation with Golden Code Feedback",
    "abstract": "           Recent advancements in large language models (LLMs) have catalyzed significant interest in the automatic generation of Register-Transfer Level (RTL) code, particularly Verilog, from natural language instructions. While commercial LLMs like ChatGPT have dominated this domain, open-source alternatives have lagged considerably in performance, limiting the flexibility and data privacy of this emerging technology. This study introduces a novel approach utilizing reinforcement learning with golden code feedback to enhance the performance of pre-trained models. Leveraging open-source data and base models, we have achieved state-of-the-art (SOTA) results with a substantial margin. Notably, our 6.7B parameter model \\ours{} demonstrates superior performance compared to current best-in-class 13B and 16B models. Furthermore, through a comprehensive analysis of the limitations in direct fine-tuning and the training dynamics of reinforcement learning, we posit that the development of comprehensive supervisory signals, which are align with the inherent parallel semantics of Verilog code, is critical to effective generation. The code and data associated with this research are publicly available at \\url{this https URL}. The model weights can be accessed at \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2407.18271",
    "authors": [
      "Ning Wang",
      "Bingkun Yao",
      "Jie Zhou",
      "Xi Wang",
      "Zhe Jiang",
      "Nan Guan"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18274",
    "title": "Adaptive Differentially Private Structural Entropy Minimization for Unsupervised Social Event Detection",
    "abstract": "           Social event detection refers to extracting relevant message clusters from social media data streams to represent specific events in the real world. Social event detection is important in numerous areas, such as opinion analysis, social safety, and decision-making. Most current methods are supervised and require access to large amounts of data. These methods need prior knowledge of the events and carry a high risk of leaking sensitive information in the messages, making them less applicable in open-world settings. Therefore, conducting unsupervised detection while fully utilizing the rich information in the messages and protecting data privacy remains a significant challenge. To this end, we propose a novel social event detection framework, ADP-SEMEvent, an unsupervised social event detection method that prioritizes privacy. Specifically, ADP-SEMEvent is divided into two stages, i.e., the construction stage of the private message graph and the clustering stage of the private message graph. In the first stage, an adaptive differential privacy approach is used to construct a private message graph. In this process, our method can adaptively apply differential privacy based on the events occurring each day in an open environment to maximize the use of the privacy budget. In the second stage, to address the reduction in data utility caused by noise, a novel 2-dimensional structural entropy minimization algorithm based on optimal subgraphs is used to detect events in the message graph. The highlight of this process is unsupervised and does not compromise differential privacy. Extensive experiments on two public datasets demonstrate that ADP-SEMEvent can achieve detection performance comparable to state-of-the-art methods while maintaining reasonable privacy budget parameters.         ",
    "url": "https://arxiv.org/abs/2407.18274",
    "authors": [
      "Zhiwei Yang",
      "Yuecen Wei",
      "Haoran Li",
      "Qian Li",
      "Lei Jiang",
      "Li Sun",
      "Xiaoyan Yu",
      "Chunming Hu",
      "Hao Peng"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18275",
    "title": "Relations between average clustering coefficient and another centralities in graphs",
    "abstract": "           Relations between average clustering coefficient and global clustering coefficient, local efficiency, radiality, closeness, betweenness and stress centralities were obtained for simple graphs.         ",
    "url": "https://arxiv.org/abs/2407.18275",
    "authors": [
      "Mikhail Tuzhilin"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2407.18277",
    "title": "Online Social Network Data-Driven Early Detection on Short-Form Video Addiction",
    "abstract": "           Short-form video (SFV) has become a globally popular form of entertainment in recent years, appearing on major social media platforms. However, current research indicate that short video addiction can lead to numerous negative effects on both physical and psychological health, such as decreased attention span and reduced motivation to learn. Additionally, Short-form Video Addiction (SFVA) has been linked to other issues such as a lack of psychological support in real life, family or academic pressure, and social anxiety. Currently, the detection of SFVA typically occurs only after users experience negative consequences. Therefore, we aim to construct a short video addiction dataset based on social network behavior and design an early detection framework for SFVA. Previous mental health detection research on online social media has mostly focused on detecting depression and suicidal tendency. In this study, we propose the first early detection framework for SFVA EarlySD. We first introduce large language models (LLMs) to address the common issues of sparsity and missing data in graph datasets. Meanwhile, we categorize social network behavior data into different modalities and design a heterogeneous social network structure as the primary basis for detecting SFVA. We conduct a series of quantitative analysis on short video addicts using our self-constructed dataset, and perform extensive experiments to validate the effectiveness of our method EarlySD, using social data and heterogeneous social graphs in the detection of short video addiction.         ",
    "url": "https://arxiv.org/abs/2407.18277",
    "authors": [
      "Fang-Yu Kuo"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18278",
    "title": "Talking Wikidata: Communication patterns and their impact on community engagement in collaborative knowledge graphs",
    "abstract": "           We study collaboration patterns of Wikidata, one of the world's largest collaborative knowledge graph communities. Wikidata lacks long-term engagement with a small group of priceless members, 0.8%, to be responsible for 80% of contributions. Therefore, it is essential to investigate their behavioural patterns and find ways to enhance their contributions and participation. Previous studies have highlighted the importance of discussions among contributors in understanding these patterns. To investigate this, we analyzed all the discussions on Wikidata and used a mixed methods approach, including statistical tests, network analysis, and text and graph embedding representations. Our research showed that the interactions between Wikidata editors form a small world network where the content of a post influences the continuity of conversations. We also found that the account age of Wikidata members and their conversations are significant factors in their long-term engagement with the project. Our findings can benefit the Wikidata community by helping them improve their practices to increase contributions and enhance long-term participation.         ",
    "url": "https://arxiv.org/abs/2407.18278",
    "authors": [
      "Elisavet Koutsiana",
      "Ioannis Reklos",
      "Kholoud Saad Alghamdi",
      "Nitisha Jain",
      "Albert Mero\u00f1o-Pe\u00f1uela",
      "Elena Simperl"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2407.18317",
    "title": "CavDetect: A DBSCAN Algorithm based Novel Cavity Detection Model on Protein Structure",
    "abstract": "           Cavities on the structures of proteins are formed due to interaction between proteins and some small molecules, known as ligands. These are basically the locations where ligands bind with proteins. Actual detection of such locations is all-important to succeed in the entire drug design process. This study proposes a Voronoi Tessellation based novel cavity detection model that is used to detect cavities on the structure of proteins. As the atom space of protein structure is dense and of large volumes and the DBSCAN (Density Based Spatial Clustering of Applications with Noise) algorithm can handle such type of data very well as well as it is not mandatory to have knowledge about the numbers of clusters (cavities) in data as priori in this algorithm, this study proposes to implement the proposed algorithm with the DBSCAN algorithm.         ",
    "url": "https://arxiv.org/abs/2407.18317",
    "authors": [
      "Swati Adhikari",
      "Parthajit Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2407.18324",
    "title": "AMA-LSTM: Pioneering Robust and Fair Financial Audio Analysis for Stock Volatility Prediction",
    "abstract": "           Stock volatility prediction is an important task in the financial industry. Recent advancements in multimodal methodologies, which integrate both textual and auditory data, have demonstrated significant improvements in this domain, such as earnings calls (Earnings calls are public available and often involve the management team of a public company and interested parties to discuss the company's earnings). However, these multimodal methods have faced two drawbacks. First, they often fail to yield reliable models and overfit the data due to their absorption of stochastic information from the stock market. Moreover, using multimodal models to predict stock volatility suffers from gender bias and lacks an efficient way to eliminate such bias. To address these aforementioned problems, we use adversarial training to generate perturbations that simulate the inherent stochasticity and bias, by creating areas resistant to random information around the input space to improve model robustness and fairness. Our comprehensive experiments on two real-world financial audio datasets reveal that this method exceeds the performance of current state-of-the-art solution. This confirms the value of adversarial training in reducing stochasticity and bias for stock volatility prediction tasks.         ",
    "url": "https://arxiv.org/abs/2407.18324",
    "authors": [
      "Shengkun Wang",
      "Taoran Ji",
      "Jianfeng He",
      "Mariam Almutairi",
      "Dan Wang",
      "Linhan Wang",
      "Min Zhang",
      "Chang-Tien Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)",
      "Computational Finance (q-fin.CP)",
      "Statistical Finance (q-fin.ST)"
    ]
  },
  {
    "id": "arXiv:2407.18326",
    "title": "Classification-Based Automatic HDL Code Generation Using LLMs",
    "abstract": "           While large language models (LLMs) have demonstrated the ability to generate hardware description language (HDL) code for digital circuits, they still suffer from the hallucination problem, which leads to the generation of incorrect HDL code or misunderstanding of specifications. In this work, we introduce a human-expert-inspired method to mitigate the hallucination of LLMs and improve the performance in HDL code generation. We first let LLMs classify the type of the circuit based on the specifications. Then, according to the type of the circuit, we split the tasks into several sub-procedures, including information extraction and human-like design flow using Electronic Design Automation (EDA) tools. Besides, we also use a search method to mitigate the variation in code generation. Experimental results show that our method can significantly improve the functional correctness of the generated Verilog and reduce the hallucination of LLMs.         ",
    "url": "https://arxiv.org/abs/2407.18326",
    "authors": [
      "Wenhao Sun",
      "Bing Li",
      "Grace Li Zhang",
      "Xunzhao Yin",
      "Cheng Zhuo",
      "Ulf Schlichtmann"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18333",
    "title": "AutoVCoder: A Systematic Framework for Automated Verilog Code Generation using LLMs",
    "abstract": "           Recently, the use of large language models (LLMs) for software code generation, e.g., C/C++ and Python, has proven a great success. However, LLMs still suffer from low syntactic and functional correctness when it comes to the generation of register-transfer level (RTL) code, such as Verilog. To address this issue, in this paper, we develop AutoVCoder, a systematic open-source framework that significantly improves the LLMs' correctness of generating Verilog code and enhances the quality of its output at the same time. Our framework integrates three novel techniques, including a high-quality hardware dataset generation approach, a two-round LLM fine-tuning method and a domain-specific retrieval-augmented generation (RAG) mechanism. Experimental results demonstrate that AutoVCoder outperforms both industrial and academic LLMs in Verilog code generation. Specifically, AutoVCoder shows a 0.5% and 2.2% improvement in functional correctness on the EvalMachine and EvalHuman benchmarks compared with BetterV, and also achieves a 3.4% increase in syntax correctness and a 3.4% increase in functional correctness on the RTLLM benchmark compared with RTLCoder.         ",
    "url": "https://arxiv.org/abs/2407.18333",
    "authors": [
      "Mingzhe Gao",
      "Jieru Zhao",
      "Zhe Lin",
      "Wenchao Ding",
      "Xiaofeng Hou",
      "Yu Feng",
      "Chao Li",
      "Minyi Guo"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18342",
    "title": "MicroOpt: Model-driven Slice Resource Optimization in 5G and Beyond Networks",
    "abstract": "           A pivotal attribute of 5G networks is their capability to cater to diverse application requirements. This is achieved by creating logically isolated virtual networks, or slices, with distinct service level agreements (SLAs) tailored to specific use cases. However, efficiently allocating resources to maintain slice SLA is challenging due to varying traffic and QoS requirements. Traditional peak traffic-based resource allocation leads to over-provisioning, as actual traffic rarely peaks. Additionally, the complex relationship between resource allocation and QoS in end-to-end slices spanning different network segments makes conventional optimization techniques impractical. Existing approaches in this domain use network models or simulations and various optimization methods but struggle with optimality, tractability, and generalizability across different slice types. In this paper, we propose MicroOpt, a novel framework that leverages a differentiable neural network-based slice model with gradient descent for resource optimization and Lagrangian decomposition for QoS constraint satisfaction. We evaluate MicroOpt against two state-of-the-art approaches using an open-source 5G testbed with real-world traffic traces. Our results demonstrate up to 21.9% improvement in resource allocation compared to these approaches across various scenarios, including different QoS thresholds and dynamic slice traffic.         ",
    "url": "https://arxiv.org/abs/2407.18342",
    "authors": [
      "Muhammad Sulaiman",
      "Mahdieh Ahmadi",
      "Bo Sun",
      "Niloy Saha",
      "Mohammad A. Salahuddin",
      "Raouf Boutaba",
      "Aladdin Saleh"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2407.18352",
    "title": "HPAC-ML: A Programming Model for Embedding ML Surrogates in Scientific Applications",
    "abstract": "           The end of Dennard scaling and the slowdown of Moore's Law led to heterogeneous architectures benefiting machine learning (ML) algorithms. These hardware advancements and the development of intuitive domain-specific languages have made ML more accessible, democratizing innovation. ML models surpass traditional approximation limits, broadening opportunities and evolving from statistical to complex function modeling. Consequently, scientific applications leverage ML models for enhanced execution speeds. However, integrating ML models remains manual and complex, slowing the adoption of ML as an approximation technique in modern applications. We propose an easy-to-use directive-based programming model that enables developers to describe the use of ML models in scientific applications. The runtime support, as instructed by the programming model, performs data assimilation using the original algorithm and can replace the algorithm with model inference. Our evaluation across five benchmarks, testing over 5000 ML models, shows up to 83.6x speed improvements with minimal accuracy loss (as low as 0.01 RMSE).         ",
    "url": "https://arxiv.org/abs/2407.18352",
    "authors": [
      "Zane Fink",
      "Konstantinos Parasyris",
      "Praneet Rathi",
      "Giorgis Georgakoudis",
      "Harshitha Menon",
      "Peer-Timo Bremer"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2407.18367",
    "title": "Robust Claim Verification Through Fact Detection",
    "abstract": "           Claim verification can be a challenging task. In this paper, we present a method to enhance the robustness and reasoning capabilities of automated claim verification through the extraction of short facts from evidence. Our novel approach, FactDetect, leverages Large Language Models (LLMs) to generate concise factual statements from evidence and label these facts based on their semantic relevance to the claim and evidence. The generated facts are then combined with the claim and evidence. To train a lightweight supervised model, we incorporate a fact-detection task into the claim verification process as a multitasking approach to improve both performance and explainability. We also show that augmenting FactDetect in the claim verification prompt enhances performance in zero-shot claim verification using LLMs. Our method demonstrates competitive results in the supervised claim verification model by 15% on the F1 score when evaluated for challenging scientific claim verification datasets. We also demonstrate that FactDetect can be augmented with claim and evidence for zero-shot prompting (AugFactDetect) in LLMs for verdict prediction. We show that AugFactDetect outperforms the baseline with statistical significance on three challenging scientific claim verification datasets with an average of 17.3% performance gain compared to the best performing baselines.         ",
    "url": "https://arxiv.org/abs/2407.18367",
    "authors": [
      "Nazanin Jafari",
      "James Allan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18373",
    "title": "Physics Informed Kolmogorov-Arnold Neural Networks for Dynamical Analysis via Efficent-KAN and WAV-KAN",
    "abstract": "           Physics-informed neural networks have proven to be a powerful tool for solving differential equations, leveraging the principles of physics to inform the learning process. However, traditional deep neural networks often face challenges in achieving high accuracy without incurring significant computational costs. In this work, we implement the Physics-Informed Kolmogorov-Arnold Neural Networks (PIKAN) through efficient-KAN and WAV-KAN, which utilize the Kolmogorov-Arnold representation theorem. PIKAN demonstrates superior performance compared to conventional deep neural networks, achieving the same level of accuracy with fewer layers and reduced computational overhead. We explore both B-spline and wavelet-based implementations of PIKAN and benchmark their performance across various ordinary and partial differential equations using unsupervised (data-free) and supervised (data-driven) techniques. For certain differential equations, the data-free approach suffices to find accurate solutions, while in more complex scenarios, the data-driven method enhances the PIKAN's ability to converge to the correct solution. We validate our results against numerical solutions and achieve $99 \\%$ accuracy in most scenarios.         ",
    "url": "https://arxiv.org/abs/2407.18373",
    "authors": [
      "Subhajit Patra",
      "Sonali Panda",
      "Bikram Keshari Parida",
      "Mahima Arya",
      "Kurt Jacobs",
      "Denys I. Bondar",
      "Abhijit Sen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18381",
    "title": "Neural Surface Detection for Unsigned Distance Fields",
    "abstract": "           Extracting surfaces from Signed Distance Fields (SDFs) can be accomplished using traditional algorithms, such as Marching Cubes. However, since they rely on sign flips across the surface, these algorithms cannot be used directly on Unsigned Distance Fields (UDFs). In this work, we introduce a deep-learning approach to taking a UDF and turning it locally into an SDF, so that it can be effectively triangulated using existing algorithms. We show that it achieves better accuracy in surface detection than existing methods. Furthermore it generalizes well to unseen shapes and datasets, while being parallelizable. We also demonstrate the flexibily of the method by using it in conjunction with DualMeshUDF, a state of the art dual meshing method that can operate on UDFs, improving its results and removing the need to tune its parameters.         ",
    "url": "https://arxiv.org/abs/2407.18381",
    "authors": [
      "Federico Stella",
      "Nicolas Talabot",
      "Hieu Le",
      "Pascal Fua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.18397",
    "title": "Gaussian Process Kolmogorov-Arnold Networks",
    "abstract": "           In this paper, we introduce a probabilistic extension to Kolmogorov Arnold Networks (KANs) by incorporating Gaussian Process (GP) as non-linear neurons, which we refer to as GP-KAN. A fully analytical approach to handling the output distribution of one GP as an input to another GP is achieved by considering the function inner product of a GP function sample with the input distribution. These GP neurons exhibit robust non-linear modelling capabilities while using few parameters and can be easily and fully integrated in a feed-forward network structure. They provide inherent uncertainty estimates to the model prediction and can be trained directly on the log-likelihood objective function, without needing variational lower bounds or approximations. In the context of MNIST classification, a model based on GP-KAN of 80 thousand parameters achieved 98.5% prediction accuracy, compared to current state-of-the-art models with 1.5 million parameters.         ",
    "url": "https://arxiv.org/abs/2407.18397",
    "authors": [
      "Andrew Siyuan Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.18402",
    "title": "The seismic purifier: An unsupervised approach to seismic signal detection via representation learning",
    "abstract": "           In this paper, we develop an unsupervised learning approach to earthquake detection. We train a specific class of deep auto-encoders that learn to reproduce the input waveforms after a data-compressive bottleneck, and then use a simple triggering algorithm at the bottleneck to label waveforms as noise or signal. Our approach is motivated by the intuition that efficient compression of data should represent signals differently from noise, and is facilitated by a time-axis-preserving approach to auto-encoding and intuitively-motivated choices on the architecture and triggering. We demonstrate that the detection performance of the unsupervised approach is comparable to, and in some cases better than, some of the state-of-the-art supervised methods. Moreover, it has strong \\emph{cross-dataset generalization}. By experimenting with various modifications, we demonstrate that the detection performance is insensitive to various technical choices made in the algorithm. Our approach has the potential to be useful for other signal detection problems with time series data.         ",
    "url": "https://arxiv.org/abs/2407.18402",
    "authors": [
      "Onur Efe",
      "Arkadas Ozakin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Geophysics (physics.geo-ph)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.18413",
    "title": "Simulation of Neural Responses to Classical Music Using Organoid Intelligence Methods",
    "abstract": "           Music is a complex auditory stimulus capable of eliciting significant changes in brain activity, influencing cognitive processes such as memory, attention, and emotional regulation. However, the underlying mechanisms of music-induced cognitive processes remain largely unknown. Organoid intelligence and deep learning models show promise for simulating and analyzing these neural responses to classical music, an area significantly unexplored in computational neuroscience. Hence, we present the PyOrganoid library, an innovative tool that facilitates the simulation of organoid learning models, integrating sophisticated machine learning techniques with biologically inspired organoid simulations. Our study features the development of the Pianoid model, a \"deep organoid learning\" model that utilizes a Bidirectional LSTM network to predict EEG responses based on audio features from classical music recordings. This model demonstrates the feasibility of using computational methods to replicate complex neural processes, providing valuable insights into music perception and cognition. Likewise, our findings emphasize the utility of synthetic models in neuroscience research and highlight the PyOrganoid library's potential as a versatile tool for advancing studies in neuroscience and artificial intelligence.         ",
    "url": "https://arxiv.org/abs/2407.18413",
    "authors": [
      "Daniel Szelogowski"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2407.18414",
    "title": "Adversarial Robust Decision Transformer: Enhancing Robustness of RvS via Minimax Returns-to-go",
    "abstract": "           Decision Transformer (DT), as one of the representative Reinforcement Learning via Supervised Learning (RvS) methods, has achieved strong performance in offline learning tasks by leveraging the powerful Transformer architecture for sequential decision-making. However, in adversarial environments, these methods can be non-robust, since the return is dependent on the strategies of both the decision-maker and adversary. Training a probabilistic model conditioned on observed return to predict action can fail to generalize, as the trajectories that achieve a return in the dataset might have done so due to a weak and suboptimal behavior adversary. To address this, we propose a worst-case-aware RvS algorithm, the Adversarial Robust Decision Transformer (ARDT), which learns and conditions the policy on in-sample minimax returns-to-go. ARDT aligns the target return with the worst-case return learned through minimax expectile regression, thereby enhancing robustness against powerful test-time adversaries. In experiments conducted on sequential games with full data coverage, ARDT can generate a maximin (Nash Equilibrium) strategy, the solution with the largest adversarial robustness. In large-scale sequential games and continuous adversarial RL environments with partial data coverage, ARDT demonstrates significantly superior robustness to powerful test-time adversaries and attains higher worst-case returns compared to contemporary DT methods.         ",
    "url": "https://arxiv.org/abs/2407.18414",
    "authors": [
      "Xiaohang Tang",
      "Afonso Marques",
      "Parameswaran Kamalaruban",
      "Ilija Bogunovic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18419",
    "title": "Non-intrusive model reduction of advection-dominated hyperbolic problems using neural network shift augmented manifold transformations",
    "abstract": "           Advection-dominated problems are commonly noticed in nature, engineering systems, and a wide range of industrial processes. For these problems, linear approximation methods (proper orthogonal decomposition and reduced basis method) are not suitable, as the Kolmogorov $n$-width decay is slow, leading to inefficient and inaccurate reduced order models. There are few non-linear approaches to accelerate the Kolmogorov $n$-width decay. In this work, we use a neural-network shift augmented transformation technique, that employs automatic-shit detection and detects the optimal non-linear transformation of the full-order model solution manifold $\\mathcal{M}$. We exploit a deep-learning framework to derive parameter-dependent bijective mapping between the manifold $\\mathcal{M}$ and the transformed manifold $\\tilde{\\mathcal{M}}$. It consists of two neural networks, 1) ShiftNet, to employ automatic-shift detection by learning the shift-operator, which finds the optimal shifts for numerous snapshots of the full-order solution manifold, to accelerate the Kolmogorov $n$-width decay, and 2) InterpNet, which learns the reference configuration and can reconstruct the field values of the same, for each shifted grid distribution. We construct non-intrusive reduced order models on the resulting transformed linear subspaces and employ automatic-shift detection for predictions. We test our methodology on advection-dominated problems, such as 1D travelling waves, 2D isentropic convective vortex and 2D two-phase flow test cases. This work leads to the development of the complete NNsPOD-ROM algorithm for model reduction of advection-dominated problems, comprising both offline-online stages.         ",
    "url": "https://arxiv.org/abs/2407.18419",
    "authors": [
      "Harshith Gowrachari",
      "Nicola Demo",
      "Giovanni Stabile",
      "Gianluigi Rozza"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2407.18424",
    "title": "Model-driven Heart Rate Estimation and Heart Murmur Detection based on Phonocardiogram",
    "abstract": "           Acoustic signals are crucial for health monitoring, particularly heart sounds which provide essential data like heart rate and detect cardiac anomalies such as murmurs. This study utilizes a publicly available phonocardiogram (PCG) dataset to estimate heart rate using model-driven methods and extends the best-performing model to a multi-task learning (MTL) framework for simultaneous heart rate estimation and murmur detection. Heart rate estimates are derived using a sliding window technique on heart sound snippets, analyzed with a combination of acoustic features (Mel spectrogram, cepstral coefficients, power spectral density, root mean square energy). Our findings indicate that a 2D convolutional neural network (\\textbf{\\texttt{2dCNN}}) is most effective for heart rate estimation, achieving a mean absolute error (MAE) of 1.312 bpm. We systematically investigate the impact of different feature combinations and find that utilizing all four features yields the best results. The MTL model (\\textbf{\\texttt{2dCNN-MTL}}) achieves accuracy over 95% in murmur detection, surpassing existing models, while maintaining an MAE of 1.636 bpm in heart rate estimation, satisfying the requirements stated by Association for the Advancement of Medical Instrumentation (AAMI).         ",
    "url": "https://arxiv.org/abs/2407.18424",
    "authors": [
      "Jingping Nie",
      "Ran Liu",
      "Behrooz Mahasseni",
      "Erdrin Azemi",
      "Vikramjit Mitra"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2407.18433",
    "title": "Investigating the Privacy Risk of Using Robot Vacuum Cleaners in Smart Environments",
    "abstract": "           Robot vacuum cleaners have become increasingly popular and are widely used in various smart environments. To improve user convenience, manufacturers also introduced smartphone applications that enable users to customize cleaning settings or access information about their robot vacuum cleaners. While this integration enhances the interaction between users and their robot vacuum cleaners, it results in potential privacy concerns because users' personal information may be exposed. To address these concerns, end-to-end encryption is implemented between the application, cloud service, and robot vacuum cleaners to secure the exchanged information. Nevertheless, network header metadata remains unencrypted and it is still vulnerable to network eavesdropping. In this paper, we investigate the potential risk of private information exposure through such metadata. A popular robot vacuum cleaner was deployed in a real smart environment where passive network eavesdropping was conducted during several selected cleaning events. Our extensive analysis, based on Association Rule Learning, demonstrates that it is feasible to identify certain events using only the captured Internet traffic metadata, thereby potentially exposing private user information and raising privacy concerns.         ",
    "url": "https://arxiv.org/abs/2407.18433",
    "authors": [
      "Benjamin Ulsmaag",
      "Jia-Chun Lin",
      "Ming-Chang Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18434",
    "title": "A stabilized three fields formulation for Discrete Fracture Networks",
    "abstract": "           We propose a hybridized domain decomposition formulation of the discrete fracture network model, allowing for independent discretization of the individual fractures. A natural norm stabilization, obtained by penalizing the residual measured in the norm for the space where it naturally lives, is added to the local problem in the individual fracture so that no compatibility condition of inf-sup type is required between the Lagrange multiplier and the primal unknown, which can then be discretized independently of each other. Optimal stability and error estimates are proven, which are confirmed by numerical tests.         ",
    "url": "https://arxiv.org/abs/2407.18434",
    "authors": [
      "Stefano Berrone",
      "Silvia Bertoluzza",
      "Stefano Scial\u00f2"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2407.18439",
    "title": "Impact of Recurrent Neural Networks and Deep Learning Frameworks on Real-time Lightweight Time Series Anomaly Detection",
    "abstract": "           Real-time lightweight time series anomaly detection has become increasingly crucial in cybersecurity and many other domains. Its ability to adapt to unforeseen pattern changes and swiftly identify anomalies enables prompt responses and critical decision-making. While several such anomaly detection approaches have been introduced in recent years, they primarily utilize a single type of recurrent neural networks (RNNs) and have been implemented in only one deep learning framework. It is unclear how the use of different types of RNNs available in various deep learning frameworks affects the performance of these anomaly detection approaches due to the absence of comprehensive evaluations. Arbitrarily choosing a RNN variant and a deep learning framework to implement an anomaly detection approach may not reflect its true performance and could potentially mislead users into favoring one approach over another. In this paper, we aim to study the influence of various types of RNNs available in popular deep learning frameworks on real-time lightweight time series anomaly detection. We reviewed several state-of-the-art approaches and implemented a representative anomaly detection approach using well-known RNN variants supported by three widely recognized deep learning frameworks. A comprehensive evaluation is then conducted to analyze the performance of each implementation across real-world, open-source time series datasets. The evaluation results provide valuable guidance for selecting the appropriate RNN variant and deep learning framework for real-time, lightweight time series anomaly detection.         ",
    "url": "https://arxiv.org/abs/2407.18439",
    "authors": [
      "Ming-Chang Lee",
      "Jia-Chun Lin",
      "Sokratis Katsikas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18440",
    "title": "Robust Optimal Network Topology Switching for Zero Dynamics Attacks",
    "abstract": "           The intrinsic, sampling, and enforced zero dynamics attacks (ZDAs) are among the most detrimental but common stealthy attacks in robotics, aerospace, and cyber-physical systems. They exploit internal dynamics, discretization, redundancy/asynchronous actuation and sensing, to construct disruptive attacks that are completely stealthy in the measurement. Surprisingly, they work even when the systems are both controllable and observable. This paper presents a novel framework to robustly and optimally detect and mitigate ZDAs for networked linear control systems. We define controllability, observability, robustness, and sensitivity metrics, written explicitly in terms of the system topology, thereby proposing a robust and optimal switching topology formulation for resilient ZDA detection and mitigation. We equivalently reformulate this problem as a rank-constrained optimization problem (i.e., optimization with a convex objective function subject to convex constraints and rank constraints) and solve it using a convex rank minimization approach. The effectiveness of our method is demonstrated using networked dynamics subject to ZDAs         ",
    "url": "https://arxiv.org/abs/2407.18440",
    "authors": [
      "Hiroyasu Tsukamoto",
      "Joshua D. Ibrahim",
      "Joudi Hajar",
      "James Ragan",
      "Soon-Jo Chung",
      "Fred Y. Hadaegh"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2407.18442",
    "title": "Guidance-Based Prompt Data Augmentation in Specialized Domains for Named Entity Recognition",
    "abstract": "           While the abundance of rich and vast datasets across numerous fields has facilitated the advancement of natural language processing, sectors in need of specialized data types continue to struggle with the challenge of finding quality data. Our study introduces a novel guidance data augmentation technique utilizing abstracted context and sentence structures to produce varied sentences while maintaining context-entity relationships, addressing data scarcity challenges. By fostering a closer relationship between context, sentence structure, and role of entities, our method enhances data augmentation's effectiveness. Consequently, by showcasing diversification in both entity-related vocabulary and overall sentence structure, and simultaneously improving the training performance of named entity recognition task.         ",
    "url": "https://arxiv.org/abs/2407.18442",
    "authors": [
      "Hyeonseok Kang",
      "Hyein Seo",
      "Jeesu Jung",
      "Sangkeun Jung",
      "Du-Seong Chang",
      "Riwoo Chung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.18443",
    "title": "HybridDepth: Robust Depth Fusion for Mobile AR by Leveraging Depth from Focus and Single-Image Priors",
    "abstract": "           We propose HYBRIDDEPTH, a robust depth estimation pipeline that addresses the unique challenges of depth estimation for mobile AR, such as scale ambiguity, hardware heterogeneity, and generalizability. HYBRIDDEPTH leverages the camera features available on mobile devices. It effectively combines the scale accuracy inherent in Depth from Focus (DFF) methods with the generalization capabilities enabled by strong single-image depth priors. By utilizing the focal planes of a mobile camera, our approach accurately captures depth values from focused pixels and applies these values to compute scale and shift parameters for transforming relative depths into metric depths. We test our pipeline as an end-to-end system, with a newly developed mobile client to capture focal stacks, which are then sent to a GPU-powered server for depth estimation. Through comprehensive quantitative and qualitative analyses, we demonstrate that HYBRIDDEPTH not only outperforms state-of-the-art (SOTA) models in common datasets (DDFF12, NYU Depth v2) and a real-world AR dataset ARKitScenes but also demonstrates strong zero-shot generalization. For example, HYBRIDDEPTH trained on NYU Depth v2 achieves comparable performance on the DDFF12 to existing models trained on DDFF12. it also outperforms all the SOTA models in zero-shot performance on the ARKitScenes dataset. Additionally, we conduct a qualitative comparison between our model and the ARCore framework, demonstrating that our models output depth maps are significantly more accurate in terms of structural details and metric accuracy. The source code of this project is available at github.         ",
    "url": "https://arxiv.org/abs/2407.18443",
    "authors": [
      "Ashkan Ganj",
      "Hang Su",
      "Tian Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.18445",
    "title": "Capturing the security expert knowledge in feature selection for web application attack detection",
    "abstract": "           This article puts forward the use of mutual information values to replicate the expertise of security professionals in selecting features for detecting web attacks. The goal is to enhance the effectiveness of web application firewalls (WAFs). Web applications are frequently vulnerable to various security threats, making WAFs essential for their protection. WAFs analyze HTTP traffic using rule-based approaches to identify known attack patterns and to detect and block potential malicious requests. However, a major challenge is the occurrence of false positives, which can lead to blocking legitimate traffic and impact the normal functioning of the application. The problem is addressed as an approach that combines supervised learning for feature selection with a semi-supervised learning scenario for training a One-Class SVM model. The experimental findings show that the model trained with features selected by the proposed algorithm outperformed the expert-based selection approach in terms of performance. Additionally, the results obtained by the traditional rule-based WAF ModSecurity, configured with a vanilla set of OWASP CRS rules, were also improved.         ",
    "url": "https://arxiv.org/abs/2407.18445",
    "authors": [
      "Amanda Riverol",
      "Gustavo Betarte",
      "Rodrigo Mart\u00ednez",
      "\u00c1lvaro Pardo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18448",
    "title": "Regret-Optimal Defense Against Stealthy Adversaries: A System Level Approach",
    "abstract": "           Modern control designs in robotics, aerospace, and cyber-physical systems heavily depend on real-world data obtained through the system outputs. In the face of system faults and malicious attacks, however, these outputs can be compromised to misrepresent some portion of the system information that critically affects their secure and trustworthy operation. In this paper, we introduce a novel regret-optimal control framework for designing controllers that render a linear system robust against stealthy attacks, including sensor and actuator attacks, as well as external disturbances. In particular, we establish (a) a convex optimization-based system metric to quantify the regret with the worst-case stealthy attack (the true performance minus the optimal performance in hindsight with the knowledge of the stealthy attack), which improves and adaptively interpolates $\\mathcal{H}_2$ and $\\mathcal{H}_{\\infty}$ norms in the presence of stealthy adversaries, (b) an optimization problem for minimizing the regret of 1 expressed in the system level parameterization, which is useful for its localized and distributed implementation in large-scale systems, and (c) a rank-constrained optimization problem (i.e., optimization with a convex objective subject to convex constraints and rank constraints) equivalent to the optimization problem of (b). Finally, we conduct a numerical simulation which showcases the effectiveness of our approach.         ",
    "url": "https://arxiv.org/abs/2407.18448",
    "authors": [
      "Hiroyasu Tsukamoto",
      "Joudi Hajar",
      "Soon-Jo Chung",
      "Fred Y. Hadaegh"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2407.18451",
    "title": "Gaussian Lane Keeping: A Robust Prediction Baseline",
    "abstract": "           Predicting agents' behavior for vehicles and pedestrians is challenging due to a myriad of factors including the uncertainty attached to different intentions, inter-agent interactions, traffic (environment) rules, individual inclinations, and agent dynamics. Consequently, a plethora of neural network-driven prediction models have been introduced in the literature to encompass these intricacies to accurately predict the agent behavior. Nevertheless, many of these approaches falter when confronted with scenarios beyond their training datasets, and lack interpretability, raising concerns about their suitability for real-world applications such as autonomous driving. Moreover, these models frequently demand additional training, substantial computational resources, or specific input features necessitating extensive implementation endeavors. In response, we propose Gaussian Lane Keeping (GLK), a robust prediction method for autonomous vehicles that can provide a solid baseline for comparison when developing new algorithms and a sanity check for real-world deployment. We provide several extensions to the GLK model, evaluate it on the CitySim dataset, and show that it outperforms the neural-network based predictions.         ",
    "url": "https://arxiv.org/abs/2407.18451",
    "authors": [
      "David Isele",
      "Piyush Gupta",
      "Xinyi Liu",
      "Sangjae Bae"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2407.18457",
    "title": "Improved randomized neural network methods with boundary processing for solving elliptic equations",
    "abstract": "           We present two improved randomized neural network methods, namely RNN-Scaling and RNN-Boundary-Processing (RNN-BP) methods, for solving elliptic equations such as the Poisson equation and the biharmonic equation. The RNN-Scaling method modifies the optimization objective by increasing the weight of boundary equations, resulting in a more accurate approximation. We propose the boundary processing techniques on the rectangular domain that enforce the RNN method to satisfy the non-homogeneous Dirichlet and clamped boundary conditions exactly. We further prove that the RNN-BP method is exact for some solutions with specific forms and validate it numerically. Numerical experiments demonstrate that the RNN-BP method is the most accurate among the three methods, the error is reduced by 6 orders of magnitude for some tests.         ",
    "url": "https://arxiv.org/abs/2407.18457",
    "authors": [
      "Huifang Zhou",
      "Zhiqiang Sheng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2407.18462",
    "title": "MistralBSM: Leveraging Mistral-7B for Vehicular Networks Misbehavior Detection",
    "abstract": "           Vehicular networks are exposed to various threats resulting from malicious attacks. These threats compromise the security and reliability of communications among road users, thereby jeopardizing road and traffic safety. One of the main vectors of these attacks within vehicular networks is misbehaving vehicles. To address this challenge, we propose deploying a pretrained Large Language Model (LLM)-empowered Misbehavior Detection System (MDS) within an edge-cloud detection framework. Specifically, we fine-tune Mistral-7B, a state-of-the-art LLM, as the edge component to enable real-time detection, whereas a larger LLM deployed in the cloud can conduct a more comprehensive analysis. Our experiments conducted on the extended VeReMi dataset demonstrate Mistral-7B's superior performance, achieving 98\\% accuracy compared to other LLMs such as LLAMA2-7B and RoBERTa. Additionally, we investigate the impact of window size on computational costs to optimize deployment efficiency. Leveraging LLMs in MDS shows interesting results in improving the detection of vehicle misbehavior, consequently strengthening vehicular network security to ensure the safety of road users.         ",
    "url": "https://arxiv.org/abs/2407.18462",
    "authors": [
      "Wissal Hamhoum",
      "Soumaya Cherkaoui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.18470",
    "title": "Synergizing Knowledge Graphs with Large Language Models: A Comprehensive Review and Future Prospects",
    "abstract": "           Recent advancements have witnessed the ascension of Large Language Models (LLMs), endowed with prodigious linguistic capabilities, albeit marred by shortcomings including factual inconsistencies and opacity. Conversely, Knowledge Graphs (KGs) harbor verifiable knowledge and symbolic reasoning prowess, thereby complementing LLMs' deficiencies. Against this backdrop, the synergy between KGs and LLMs emerges as a pivotal research direction. Our contribution in this paper is a comprehensive dissection of the latest developments in integrating KGs with LLMs. Through meticulous analysis of their confluence points and methodologies, we introduce a unifying framework designed to elucidate and stimulate further exploration among scholars engaged in cognate disciplines. This framework serves a dual purpose: it consolidates extant knowledge while simultaneously delineating novel avenues for real-world deployment, thereby amplifying the translational impact of academic research.         ",
    "url": "https://arxiv.org/abs/2407.18470",
    "authors": [
      "DaiFeng Li",
      "Fan Xu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.18472",
    "title": "FedUD: Exploiting Unaligned Data for Cross-Platform Federated Click-Through Rate Prediction",
    "abstract": "           Click-through rate (CTR) prediction plays an important role in online advertising platforms. Most existing methods use data from the advertising platform itself for CTR prediction. As user behaviors also exist on many other platforms, e.g., media platforms, it is beneficial to further exploit such complementary information for better modeling user interest and for improving CTR prediction performance. However, due to privacy concerns, data from different platforms cannot be uploaded to a server for centralized model training. Vertical federated learning (VFL) provides a possible solution which is able to keep the raw data on respective participating parties and learn a collaborative model in a privacy-preserving way. However, traditional VFL methods only utilize aligned data with common keys across parties, which strongly restricts their application scope. In this paper, we propose FedUD, which is able to exploit unaligned data, in addition to aligned data, for more accurate federated CTR prediction. FedUD contains two steps. In the first step, FedUD utilizes aligned data across parties like traditional VFL, but it additionally includes a knowledge distillation module. This module distills useful knowledge from the guest party's high-level representations and guides the learning of a representation transfer network. In the second step, FedUD applies the learned knowledge to enrich the representations of the host party's unaligned data such that both aligned and unaligned data can contribute to federated model training. Experiments on two real-world datasets demonstrate the superior performance of FedUD for federated CTR prediction.         ",
    "url": "https://arxiv.org/abs/2407.18472",
    "authors": [
      "Wentao Ouyang",
      "Rui Dong",
      "Ri Tao",
      "Xiangzheng Liu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18480",
    "title": "Scalable Graph Compressed Convolutions",
    "abstract": "           Designing effective graph neural networks (GNNs) with message passing has two fundamental challenges, i.e., determining optimal message-passing pathways and designing local aggregators. Previous methods of designing optimal pathways are limited with information loss on the input features. On the other hand, existing local aggregators generally fail to extract multi-scale features and approximate diverse operators under limited parameter scales. In contrast to these methods, Euclidean convolution has been proven as an expressive aggregator, making it a perfect candidate for GNN construction. However, the challenges of generalizing Euclidean convolution to graphs arise from the irregular structure of graphs. To bridge the gap between Euclidean space and graph topology, we propose a differentiable method that applies permutations to calibrate input graphs for Euclidean convolution. The permutations constrain all nodes in a row regardless of their input order and therefore enable the flexible generalization of Euclidean convolution to graphs. Based on the graph calibration, we propose the Compressed Convolution Network (CoCN) for hierarchical graph representation learning. CoCN follows local feature-learning and global parameter-sharing mechanisms of convolution neural networks. The whole model can be trained end-to-end, with compressed convolution applied to learn individual node features and their corresponding structure features. CoCN can further borrow successful practices from Euclidean convolution, including residual connection and inception mechanism. We validate CoCN on both node-level and graph-level benchmarks. CoCN achieves superior performance over competitive GNN baselines. Codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.18480",
    "authors": [
      "Junshu Sun",
      "Chenxue Yang",
      "Shuhui Wang",
      "Qingming Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18487",
    "title": "SMPISD-MTPNet: Scene Semantic Prior-Assisted Infrared Ship Detection Using Multi-Task Perception Networks",
    "abstract": "           Infrared ship detection (IRSD) has received increasing attention in recent years due to the robustness of infrared images to adverse weather. However, a large number of false alarms may occur in complex scenes. To address these challenges, we propose the Scene Semantic Prior-Assisted Multi-Task Perception Network (SMPISD-MTPNet), which includes three stages: scene semantic extraction, deep feature extraction, and prediction. In the scene semantic extraction stage, we employ a Scene Semantic Extractor (SSE) to guide the network by the features extracted based on expert knowledge. In the deep feature extraction stage, a backbone network is employed to extract deep features. These features are subsequently integrated by a fusion network, enhancing the detection capabilities across targets of varying sizes. In the prediction stage, we utilize the Multi-Task Perception Module, which includes the Gradient-based Module and the Scene Segmentation Module, enabling precise detection of small and dim targets within complex scenes. For the training process, we introduce the Soft Fine-tuning training strategy to suppress the distortion caused by data augmentation. Besides, due to the lack of a publicly available dataset labelled for scenes, we introduce the Infrared Ship Dataset with Scene Segmentation (IRSDSS). Finally, we evaluate the network and compare it with state-of-the-art (SOTA) methods, indicating that SMPISD-MTPNet outperforms existing approaches. The source code and dataset for this research can be accessed at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.18487",
    "authors": [
      "Chen Hu",
      "Xiaogang Dong",
      "Yian Huang Lele Wang",
      "Liang Xu",
      "Tian Pu",
      "Zhenming Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.18489",
    "title": "Mini-Batch Gradient-Based MCMC for Decentralized Massive MIMO Detection",
    "abstract": "           Massive multiple-input multiple-output (MIMO) technology has significantly enhanced spectral and power efficiency in cellular communications and is expected to further evolve towards extra-large-scale MIMO. However, centralized processing for massive MIMO faces practical obstacles, including excessive computational complexity and a substantial volume of baseband data to be exchanged. To address these challenges, decentralized baseband processing has emerged as a promising solution. This approach involves partitioning the antenna array into clusters with dedicated computing hardware for parallel processing. In this paper, we investigate the gradient-based Markov chain Monte Carlo (MCMC) method -- an advanced MIMO detection technique known for its near-optimal performance in centralized implementation -- within the context of a decentralized baseband processing architecture. This decentralized design mitigates the computation burden at a single processing unit by utilizing computational resources in a distributed and parallel manner. Additionally, we integrate the mini-batch stochastic gradient descent method into the proposed decentralized detector, achieving remarkable performance with high efficiency. Simulation results demonstrate substantial performance gains of the proposed method over existing decentralized detectors across various scenarios. Moreover, complexity analysis reveals the advantages of the proposed decentralized strategy in terms of computation delay and interconnection bandwidth when compared to conventional centralized detectors.         ",
    "url": "https://arxiv.org/abs/2407.18489",
    "authors": [
      "Xingyu Zhou",
      "Le Liang",
      "Jing Zhang",
      "Chao-Kai Wen",
      "Shi Jin"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.18492",
    "title": "Neural Modulation Alteration to Positive and Negative Emotions in Depressed Patients: Insights from fMRI Using Positive/Negative Emotion Atlas",
    "abstract": "           Background: Although it has been noticed that depressed patients show differences in processing emotions, the precise neural modulation mechanisms of positive and negative emotions remain elusive. FMRI is a cutting-edge medical imaging technology renowned for its high spatial resolution and dynamic temporal information, making it particularly suitable for the neural dynamics of depression research. Methods: To address this gap, our study firstly leveraged fMRI to delineate activated regions associated with positive and negative emotions in healthy individuals, resulting in the creation of positive emotion atlas (PEA) and negative emotion atlas (NEA). Subsequently, we examined neuroimaging changes in depression patients using these atlases and evaluated their diagnostic performance based on machine learning. Results: Our findings demonstrate that the classification accuracy of depressed patients based on PEA and NEA exceeded 0.70, a notable improvement compared to the whole-brain atlases. Furthermore, ALFF analysis unveiled significant differences between depressed patients and healthy controls in eight functional clusters during the NEA, focusing on the left cuneus, cingulate gyrus, and superior parietal lobule. In contrast, the PEA revealed more pronounced differences across fifteen clusters, involving the right fusiform gyrus, parahippocampal gyrus, and inferior parietal lobule. Limitations: Due to the limited sample size and subtypes of depressed patients, the efficacy may need further validation in future. Conclusions: These findings emphasize the complex interplay between emotion modulation and depression, showcasing significant alterations in both PEA and NEA among depression patients. This research enhances our understanding of emotion modulation in depression, with implications for diagnosis and treatment evaluation.         ",
    "url": "https://arxiv.org/abs/2407.18492",
    "authors": [
      "Yu Feng",
      "Weiming Zeng",
      "Yifan Xie",
      "Hongyu Chen",
      "Lei Wang",
      "Yingying Wang",
      "Hongjie Yan",
      "Kaile Zhang",
      "Ran Tao",
      "Wai Ting Siok",
      "Nizhuan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.18496",
    "title": "Towards More Accurate Prediction of Human Empathy and Emotion in Text and Multi-turn Conversations by Combining Advanced NLP, Transformers-based Networks, and Linguistic Methodologies",
    "abstract": "           Based on the WASSA 2022 Shared Task on Empathy Detection and Emotion Classification, we predict the level of empathic concern and personal distress displayed in essays. For the first stage of this project we implemented a Feed-Forward Neural Network using sentence-level embeddings as features. We experimented with four different embedding models for generating the inputs to the neural network. The subsequent stage builds upon the previous work and we have implemented three types of revisions. The first revision focuses on the enhancements to the model architecture and the training approach. The second revision focuses on handling class imbalance using stratified data sampling. The third revision focuses on leveraging lexical resources, where we apply four different resources to enrich the features associated with the dataset. During the final stage of this project, we have created the final end-to-end system for the primary task using an ensemble of models to revise primary task performance. Additionally, as part of the final stage, these approaches have been adapted to the WASSA 2023 Shared Task on Empathy Emotion and Personality Detection in Interactions, in which the empathic concern, emotion polarity, and emotion intensity in dyadic text conversations are predicted.         ",
    "url": "https://arxiv.org/abs/2407.18496",
    "authors": [
      "Manisha Singh",
      "Divy Sharma",
      "Alonso Ma",
      "Nora Goldfine"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18500",
    "title": "Revisit Event Generation Model: Self-Supervised Learning of Event-to-Video Reconstruction with Implicit Neural Representations",
    "abstract": "           Reconstructing intensity frames from event data while maintaining high temporal resolution and dynamic range is crucial for bridging the gap between event-based and frame-based computer vision. Previous approaches have depended on supervised learning on synthetic data, which lacks interpretability and risk over-fitting to the setting of the event simulator. Recently, self-supervised learning (SSL) based methods, which primarily utilize per-frame optical flow to estimate intensity via photometric constancy, has been actively investigated. However, they are vulnerable to errors in the case of inaccurate optical flow. This paper proposes a novel SSL event-to-video reconstruction approach, dubbed EvINR, which eliminates the need for labeled data or optical flow estimation. Our core idea is to reconstruct intensity frames by directly addressing the event generation model, essentially a partial differential equation (PDE) that describes how events are generated based on the time-varying brightness signals. Specifically, we utilize an implicit neural representation (INR), which takes in spatiotemporal coordinate $(x, y, t)$ and predicts intensity values, to represent the solution of the event generation equation. The INR, parameterized as a fully-connected Multi-layer Perceptron (MLP), can be optimized with its temporal derivatives supervised by events. To make EvINR feasible for online requisites, we propose several acceleration techniques that substantially expedite the training process. Comprehensive experiments demonstrate that our EvINR surpasses previous SSL methods by 38% w.r.t. Mean Squared Error (MSE) and is comparable or superior to SoTA supervised methods. Project page: this https URL.         ",
    "url": "https://arxiv.org/abs/2407.18500",
    "authors": [
      "Zipeng Wang",
      "Yunfan Lu",
      "Lin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.18503",
    "title": "Homomorphic Encryption-Enabled Federated Learning for Privacy-Preserving Intrusion Detection in Resource-Constrained IoV Networks",
    "abstract": "           This paper aims to propose a novel framework to address the data privacy issue for Federated Learning (FL)-based Intrusion Detection Systems (IDSs) in Internet-of-Vehicles(IoVs) with limited computational resources. In particular, in conventional FL systems, it is usually assumed that the computing nodes have sufficient computational resources to process the training tasks. However, in practical IoV systems, vehicles usually have limited computational resources to process intensive training tasks, compromising the effectiveness of deploying FL in IDSs. While offloading data from vehicles to the cloud can mitigate this issue, it introduces significant privacy concerns for vehicle users (VUs). To resolve this issue, we first propose a highly-effective framework using homomorphic encryption to secure data that requires offloading to a centralized server for processing. Furthermore, we develop an effective training algorithm tailored to handle the challenges of FL-based systems with encrypted data. This algorithm allows the centralized server to directly compute on quantum-secure encrypted ciphertexts without needing decryption. This approach not only safeguards data privacy during the offloading process from VUs to the centralized server but also enhances the efficiency of utilizing FL for IDSs in IoV systems. Our simulation results show that our proposed approach can achieve a performance that is as close to that of the solution without encryption, with a gap of less than 0.8%.         ",
    "url": "https://arxiv.org/abs/2407.18503",
    "authors": [
      "Bui Duc Manh",
      "Chi-Hieu Nguyen",
      "Dinh Thai Hoang",
      "Diep N. Nguyen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.18517",
    "title": "SLIM: Style-Linguistics Mismatch Model for Generalized Audio Deepfake Detection",
    "abstract": "           Audio deepfake detection (ADD) is crucial to combat the misuse of speech synthesized from generative AI models. Existing ADD models suffer from generalization issues, with a large performance discrepancy between in-domain and out-of-domain data. Moreover, the black-box nature of existing models limits their use in real-world scenarios, where explanations are required for model decisions. To alleviate these issues, we introduce a new ADD model that explicitly uses the StyleLInguistics Mismatch (SLIM) in fake speech to separate them from real speech. SLIM first employs self-supervised pretraining on only real samples to learn the style-linguistics dependency in the real class. The learned features are then used in complement with standard pretrained acoustic features (e.g., Wav2vec) to learn a classifier on the real and fake classes. When the feature encoders are frozen, SLIM outperforms benchmark methods on out-of-domain datasets while achieving competitive results on in-domain data. The features learned by SLIM allow us to quantify the (mis)match between style and linguistic content in a sample, hence facilitating an explanation of the model decision.         ",
    "url": "https://arxiv.org/abs/2407.18517",
    "authors": [
      "Yi Zhu",
      "Surya Koppisetti",
      "Trang Tran",
      "Gaurav Bharaj"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2407.18519",
    "title": "TCGPN: Temporal-Correlation Graph Pre-trained Network for Stock Forecasting",
    "abstract": "           Recently, the incorporation of both temporal features and the correlation across time series has become an effective approach in time series prediction. Spatio-Temporal Graph Neural Networks (STGNNs) demonstrate good performance on many Temporal-correlation Forecasting Problem. However, when applied to tasks lacking periodicity, such as stock data prediction, the effectiveness and robustness of STGNNs are found to be unsatisfactory. And STGNNs are limited by memory savings so that cannot handle problems with a large number of nodes. In this paper, we propose a novel approach called the Temporal-Correlation Graph Pre-trained Network (TCGPN) to address these limitations. TCGPN utilize Temporal-correlation fusion encoder to get a mixed representation and pre-training method with carefully designed temporal and correlation pre-training tasks. Entire structure is independent of the number and order of nodes, so better results can be obtained through various data enhancements. And memory consumption during training can be significantly reduced through multiple sampling. Experiments are conducted on real stock market data sets CSI300 and CSI500 that exhibit minimal periodicity. We fine-tune a simple MLP in downstream tasks and achieve state-of-the-art results, validating the capability to capture more robust temporal correlation patterns.         ",
    "url": "https://arxiv.org/abs/2407.18519",
    "authors": [
      "Wenbo Yan",
      "Ying Tan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Statistical Finance (q-fin.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.18523",
    "title": "DTFormer: A Transformer-Based Method for Discrete-Time Dynamic Graph Representation Learning",
    "abstract": "           Discrete-Time Dynamic Graphs (DTDGs), which are prevalent in real-world implementations and notable for their ease of data acquisition, have garnered considerable attention from both academic researchers and industry practitioners. The representation learning of DTDGs has been extensively applied to model the dynamics of temporally changing entities and their evolving connections. Currently, DTDG representation learning predominantly relies on GNN+RNN architectures, which manifest the inherent limitations of both Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs). GNNs suffer from the over-smoothing issue as the models architecture goes deeper, while RNNs struggle to capture long-term dependencies effectively. GNN+RNN architectures also grapple with scaling to large graph sizes and long sequences. Additionally, these methods often compute node representations separately and focus solely on individual node characteristics, thereby overlooking the behavior intersections between the two nodes whose link is being predicted, such as instances where the two nodes appear together in the same context or share common neighbors. This paper introduces a novel representation learning method DTFormer for DTDGs, pivoting from the traditional GNN+RNN framework to a Transformer-based architecture. Our approach exploits the attention mechanism to concurrently process topological information within the graph at each timestamp and temporal dynamics of graphs along the timestamps, circumventing the aforementioned fundamental weakness of both GNNs and RNNs. Moreover, we enhance the model's expressive capability by incorporating the intersection relationships among nodes and integrating a multi-patching module. Extensive experiments conducted on six public dynamic graph benchmark datasets confirm our model's efficacy, achieving the SOTA performance.         ",
    "url": "https://arxiv.org/abs/2407.18523",
    "authors": [
      "Xi Chen",
      "Yun Xiong",
      "Siwei Zhang",
      "Jiawei Zhang",
      "Yao Zhang",
      "Shiyang Zhou",
      "Xixi Wu",
      "Mingyang Zhang",
      "Tengfei Liu",
      "Weiqiang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18541",
    "title": "Towards Improving NAM-to-Speech Synthesis Intelligibility using Self-Supervised Speech Models",
    "abstract": "           We propose a novel approach to significantly improve the intelligibility in the Non-Audible Murmur (NAM)-to-speech conversion task, leveraging self-supervision and sequence-to-sequence (Seq2Seq) learning techniques. Unlike conventional methods that explicitly record ground-truth speech, our methodology relies on self-supervision and speech-to-speech synthesis to simulate ground-truth speech. Despite utilizing simulated speech, our method surpasses the current state-of-the-art (SOTA) by 29.08% improvement in the Mel-Cepstral Distortion (MCD) metric. Additionally, we present error rates and demonstrate our model's proficiency to synthesize speech in novel voices of interest. Moreover, we present a methodology for augmenting the existing CSTR NAM TIMIT Plus corpus, setting a benchmark with a Word Error Rate (WER) of 42.57% to gauge the intelligibility of the synthesized speech. Speech samples can be found at this https URL ",
    "url": "https://arxiv.org/abs/2407.18541",
    "authors": [
      "Neil Shah",
      "Shirish Karande",
      "Vineet Gandhi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2407.18544",
    "title": "Utilising Explainable Techniques for Quality Prediction in a Complex Textiles Manufacturing Use Case",
    "abstract": "           This paper develops an approach to classify instances of product failure in a complex textiles manufacturing dataset using explainable techniques. The dataset used in this study was obtained from a New Zealand manufacturer of woollen carpets and rugs. In investigating the trade-off between accuracy and explainability, three different tree-based classification algorithms were evaluated: a Decision Tree and two ensemble methods, Random Forest and XGBoost. Additionally, three feature selection methods were also evaluated: the SelectKBest method, using chi-squared as the scoring function, the Pearson Correlation Coefficient, and the Boruta algorithm. Not surprisingly, the ensemble methods typically produced better results than the Decision Tree model. The Random Forest model yielded the best results overall when combined with the Boruta feature selection technique. Finally, a tree ensemble explaining technique was used to extract rule lists to capture necessary and sufficient conditions for classification by a trained model that could be easily interpreted by a human. Notably, several features that were in the extracted rule lists were statistical features and calculated features that were added to the original dataset. This demonstrates the influence that bringing in additional information during the data preprocessing stages can have on the ultimate model performance.         ",
    "url": "https://arxiv.org/abs/2407.18544",
    "authors": [
      "Briony Forsberg",
      "Dr Henry Williams",
      "Prof Bruce MacDonald",
      "Tracy Chen",
      "Dr Reza Hamzeh",
      "Dr Kirstine Hulse"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18546",
    "title": "Statistical Analysis of the Properties of Geometric Network with Node Mobility",
    "abstract": "           The movement changes the underlying spatial representation of the participated mobile objects or nodes. In real world scenario, such mobile nodes can be part of any biological network, transportation network, social network, human interaction, etc. The change in the geometry leads to the change in various desirable properties of real-world networks especially in human interaction networks. In real life, human movement is concerned for better lifestyle where they form their new connections due to the geographical changes. Therefore, in this paper, we design a model for geometric networks with mobile nodes (GNMN) and conduct a comprehensive statistical analysis of their properties. We analyze the effect of node mobility by evaluating key network metrics such as connectivity, node degree distribution, second hop neighbors, and centrality measures. Through extensive simulations, we observe significant variations in the behavior of geometric networks with mobile nodes.         ",
    "url": "https://arxiv.org/abs/2407.18546",
    "authors": [
      "Md. Arquam",
      "Utkarsh Tiwari",
      "Suchi Kumari"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2407.18551",
    "title": "Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network",
    "abstract": "           Trajectory prediction is crucial for autonomous driving as it aims to forecast the future movements of traffic participants. Traditional methods usually perform holistic inference on the trajectories of agents, neglecting the differences in prediction difficulty among agents. This paper proposes a novel Difficulty-Guided Feature Enhancement Network (DGFNet), which leverages the prediction difficulty differences among agents for multi-agent trajectory prediction. Firstly, we employ spatio-temporal feature encoding and interaction to capture rich spatio-temporal features. Secondly, a difficulty-guided decoder is used to control the flow of future trajectories into subsequent modules, obtaining reliable future trajectories. Then, feature interaction and fusion are performed through the future feature interaction module. Finally, the fused agent features are fed into the final predictor to generate the predicted trajectory distributions for multiple participants. Experimental results demonstrate that our DGFNet achieves state-of-the-art performance on the Argoverse 1\\&2 motion forecasting benchmarks. Ablation studies further validate the effectiveness of each module. Moreover, compared with SOTA methods, our method balances trajectory prediction accuracy and real-time inference speed.         ",
    "url": "https://arxiv.org/abs/2407.18551",
    "authors": [
      "Guipeng Xin",
      "Duanfeng Chu",
      "Liping Lu",
      "Zejian Deng",
      "Yuang Lu",
      "Xigang Wu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18553",
    "title": "REAPER: Reasoning based Retrieval Planning for Complex RAG Systems",
    "abstract": "           Complex dialog systems often use retrieved evidence to facilitate factual responses. Such RAG (Retrieval Augmented Generation) systems retrieve from massive heterogeneous data stores that are usually architected as multiple indexes or APIs instead of a single monolithic source. For a given query, relevant evidence needs to be retrieved from one or a small subset of possible retrieval sources. Complex queries can even require multi-step retrieval. For example, a conversational agent on a retail site answering customer questions about past orders will need to retrieve the appropriate customer order first and then the evidence relevant to the customer's question in the context of the ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by interleaving reasoning and retrieval steps. However, each reasoning step directly adds to the latency of the system. For large models (>100B parameters) this latency cost is significant -- in the order of multiple seconds. Multi-agent systems may classify the query to a single Agent associated with a retrieval source, though this means that a (small) classification model dictates the performance of a large language model. In this work we present REAPER (REAsoning-based PlannER) - an LLM based planner to generate retrieval plans in conversational systems. We show significant gains in latency over Agent-based systems and are able to scale easily to new and unseen use cases as compared to classification-based planning. Though our method can be applied to any RAG system, we show our results in the context of Rufus -- Amazon's conversational shopping assistant.         ",
    "url": "https://arxiv.org/abs/2407.18553",
    "authors": [
      "Ashutosh Joshi",
      "Sheikh Muhammad Sarwar",
      "Samarth Varshney",
      "Sreyashi Nag",
      "Shrivats Agrawal",
      "Juhi Naik"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.18554",
    "title": "Skin Cancer Detection utilizing Deep Learning: Classification of Skin Lesion Images using a Vision Transformer",
    "abstract": "           Skin cancer detection still represents a major challenge in healthcare. Common detection methods can be lengthy and require human assistance which falls short in many countries. Previous research demonstrates how convolutional neural networks (CNNs) can help effectively through both automation and an accuracy that is comparable to the human level. However, despite the progress in previous decades, the precision is still limited, leading to substantial misclassifications that have a serious impact on people's health. Hence, we employ a Vision Transformer (ViT) that has been developed in recent years based on the idea of a self-attention mechanism, specifically two configurations of a pre-trained ViT. We generally find superior metrics for classifying skin lesions after comparing them to base models such as decision tree classifier and k-nearest neighbor (KNN) classifier, as well as to CNNs and less complex ViTs. In particular, we attach greater importance to the performance of melanoma, which is the most lethal type of skin cancer. The ViT-L32 model achieves an accuracy of 91.57% and a melanoma recall of 58.54%, while ViT-L16 achieves an accuracy of 92.79% and a melanoma recall of 56.10%. This offers a potential tool for faster and more accurate diagnoses and an overall improvement for the healthcare sector.         ",
    "url": "https://arxiv.org/abs/2407.18554",
    "authors": [
      "Carolin Flosdorf",
      "Justin Engelker",
      "Igor Keller",
      "Nicolas Mohr"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.18556",
    "title": "Look Globally and Reason: Two-stage Path Reasoning over Sparse Knowledge Graphs",
    "abstract": "           Sparse Knowledge Graphs (KGs), frequently encountered in real-world applications, contain fewer facts in the form of (head entity, relation, tail entity) compared to more populated KGs. The sparse KG completion task, which reasons answers for given queries in the form of (head entity, relation, ?) for sparse KGs, is particularly challenging due to the necessity of reasoning missing facts based on limited facts. Path-based models, known for excellent explainability, are often employed for this task. However, existing path-based models typically rely on external models to fill in missing facts and subsequently perform path reasoning. This approach introduces unexplainable factors or necessitates meticulous rule design. In light of this, this paper proposes an alternative approach by looking inward instead of seeking external assistance. We introduce a two-stage path reasoning model called LoGRe (Look Globally and Reason) over sparse KGs. LoGRe constructs a relation-path reasoning schema by globally analyzing the training data to alleviate the sparseness problem. Based on this schema, LoGRe then aggregates paths to reason out answers. Experimental results on five benchmark sparse KG datasets demonstrate the effectiveness of the proposed LoGRe model.         ",
    "url": "https://arxiv.org/abs/2407.18556",
    "authors": [
      "Saiping Guan",
      "Jiyao Wei",
      "Xiaolong Jin",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18560",
    "title": "On the Number of Observation Nodes in Boolean Networks",
    "abstract": "           A Boolean network (BN) is called observable if any initial state can be uniquely determined from the output sequence. In the existing literature on observability of BNs, there is almost no research on the relationship between the number of observation nodes and the observability of BNs, which is an important and practical issue. In this paper, we mainly focus on three types of BNs with $n$ nodes (i.e., $K$-AND-OR-BNs, $K$-XOR-BNs, and $K$-NC-BNs, where $K$ is the number of input nodes for each node and NC means nested canalyzing) and study the upper and lower bounds of the number of observation nodes for these BNs. First, we develop a novel technique using information entropy to derive a general lower bound of the number of observation nodes, and conclude that the number of observation nodes cannot be smaller than $\\left[(1-K)+\\frac{2^{K}-1}{2^{K}}\\log_{2}(2^{K}-1)\\right]n$ to ensure that any $K$-AND-OR-BN is observable, and similarly, some lower bound is also obtained for $K$-NC-BNs. Then for any type of BN, we also develop two new techniques to infer the general lower bounds, using counting identical states at time 1 and counting the number of fixed points, respectively. On the other hand, we derive nontrivial upper bounds of the number of observation nodes by combinatorial analysis of several types of BNs. Specifically, we indicate that $\\left(\\frac{2^{K}-K-1}{2^{K}-1}\\right)n,~1$, and $\\lceil \\frac{n}{K}\\rceil$ are the best case upper bounds for $K$-AND-OR-BNs, $K$-XOR-BNs, and $K$-NC-BN, respectively.         ",
    "url": "https://arxiv.org/abs/2407.18560",
    "authors": [
      "Liangjie Sun",
      "Wai-Ki Ching",
      "Tatsuya Akutsu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2407.18562",
    "title": "Learning Robust Named Entity Recognizers From Noisy Data With Retrieval Augmentation",
    "abstract": "           Named entity recognition (NER) models often struggle with noisy inputs, such as those with spelling mistakes or errors generated by Optical Character Recognition processes, and learning a robust NER model is challenging. Existing robust NER models utilize both noisy text and its corresponding gold text for training, which is infeasible in many real-world applications in which gold text is not available. In this paper, we consider a more realistic setting in which only noisy text and its NER labels are available. We propose to retrieve relevant text of the noisy text from a knowledge corpus and use it to enhance the representation of the original noisy input. We design three retrieval methods: sparse retrieval based on lexicon similarity, dense retrieval based on semantic similarity, and self-retrieval based on task-specific text. After retrieving relevant text, we concatenate the retrieved text with the original noisy text and encode them with a transformer network, utilizing self-attention to enhance the contextual token representations of the noisy text using the retrieved text. We further employ a multi-view training framework that improves robust NER without retrieving text during inference. Experiments show that our retrieval-augmented model achieves significant improvements in various noisy NER settings.         ",
    "url": "https://arxiv.org/abs/2407.18562",
    "authors": [
      "Chaoyi Ai",
      "Yong Jiang",
      "Shen Huang",
      "Pengjun Xie",
      "Kewei Tu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18564",
    "title": "Unveiling Privacy Vulnerabilities: Investigating the Role of Structure in Graph Data",
    "abstract": "           The public sharing of user information opens the door for adversaries to infer private data, leading to privacy breaches and facilitating malicious activities. While numerous studies have concentrated on privacy leakage via public user attributes, the threats associated with the exposure of user relationships, particularly through network structure, are often neglected. This study aims to fill this critical gap by advancing the understanding and protection against privacy risks emanating from network structure, moving beyond direct connections with neighbors to include the broader implications of indirect network structural patterns. To achieve this, we first investigate the problem of Graph Privacy Leakage via Structure (GPS), and introduce a novel measure, the Generalized Homophily Ratio, to quantify the various mechanisms contributing to privacy breach risks in GPS. Based on this insight, we develop a novel graph private attribute inference attack, which acts as a pivotal tool for evaluating the potential for privacy leakage through network structures under worst-case scenarios. To protect users' private data from such vulnerabilities, we propose a graph data publishing method incorporating a learnable graph sampling technique, effectively transforming the original graph into a privacy-preserving version. Extensive experiments demonstrate that our attack model poses a significant threat to user privacy, and our graph data publishing method successfully achieves the optimal privacy-utility trade-off compared to baselines.         ",
    "url": "https://arxiv.org/abs/2407.18564",
    "authors": [
      "Hanyang Yuan",
      "Jiarong Xu",
      "Cong Wang",
      "Ziqi Yang",
      "Chunping Wang",
      "Keting Yin",
      "Yang Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2407.18571",
    "title": "Speech Bandwidth Expansion Via High Fidelity Generative Adversarial Networks",
    "abstract": "           Speech bandwidth expansion is crucial for expanding the frequency range of low-bandwidth speech signals, thereby improving audio quality, clarity and perceptibility in digital applications. Its applications span telephony, compression, text-to-speech synthesis, and speech recognition. This paper presents a novel approach using a high-fidelity generative adversarial network, unlike cascaded systems, our system is trained end-to-end on paired narrowband and wideband speech signals. Our method integrates various bandwidth upsampling ratios into a single unified model specifically designed for speech bandwidth expansion applications. Our approach exhibits robust performance across various bandwidth expansion factors, including those not encountered during training, demonstrating zero-shot capability. To the best of our knowledge, this is the first work to showcase this capability. The experimental results demonstrate that our method outperforms previous end-to-end approaches, as well as interpolation and traditional techniques, showcasing its effectiveness in practical speech enhancement applications.         ",
    "url": "https://arxiv.org/abs/2407.18571",
    "authors": [
      "Mahmoud Salhab",
      "Haidar Harmanani"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2407.18591",
    "title": "Reconstruction of geometric random graphs with the Simple algorithm",
    "abstract": "           Graph reconstruction can efficiently detect the underlying topology of massive networks such as the Internet. Given a query oracle and a set of nodes, the goal is to obtain the edge set by performing as few queries as possible. An algorithm for graph reconstruction is the Simple algorithm (Mathieu & Zhou, 2023), which reconstructs bounded-degree graphs in $\\tilde{O}(n^{3/2})$ queries. We extend the use of this algorithm to the class of geometric random graphs with connection radius $r \\sim n^k$, with diverging average degree. We show that for this class of graphs, the query complexity is $\\tilde{O}(n^{2k+1})$ when k > 3/20. This query complexity is up to a polylog(n) term equal to the number of edges in the graph, which means that the reconstruction algorithm is almost edge-optimal. We also show that with only $n^{1+o(1)}$ queries it is already possible to reconstruct at least 75% of the non-edges of a geometric random graph, in both the sparse and dense setting. Finally, we show that the number of queries is indeed of the same order as the number of edges on the basis of simulations.         ",
    "url": "https://arxiv.org/abs/2407.18591",
    "authors": [
      "Clara Stegehuis",
      "Lotte Weedage"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2407.18606",
    "title": "A data balancing approach designing of an expert system for Heart Disease Prediction",
    "abstract": "           Heart disease is a major global health concern that results in millions of deaths annually. Prevention and effective treatment of heart-related problems depend heavily on early detection and accurate prediction. It was previously predicted accurately with machine learning methods. This innovative development in healthcare has the power to transform preventative care and save a great deal of lives. The study starts with a thorough assessment of the literature that covers a wide range of topics, including pre-processing techniques, performance evaluation measures, datasets used in heart disease research, predictive modeling strategies, diagnostic methodologies, and current issues in the field. Building on these fundamental understandings, the background section describes the particular actions conducted in this investigation, such as the description of the dataset, data pre-treatment techniques, label encoding, feature selection methodology, algorithm selection tactics, and stringent performance evaluation techniques.The results indicate that ensemble methods, particularly random forests, outperformed individual classifiers in predicting heart disease. Key predictors identified included hypertension, cholesterol levels, smoking status, and physical inactivity. The Decision Tree and Random Forest model achieved an accuracy of 99.83%. This work demonstrates how machine learning models, particularly ensemble approaches, can increase the precision of heart disease prediction. In comparison to conventional techniques, the models offer a more reliable risk assessment since they integrate a wide range of variables and sophisticated algorithms. The results open the door to tailored healthcare treatments that facilitate early identification and treatment of cardiac disease.         ",
    "url": "https://arxiv.org/abs/2407.18606",
    "authors": [
      "Rahul Karmakar",
      "Udita Ghosh",
      "Arpita Pal",
      "Sattwiki Dey",
      "Debraj Malik",
      "Priyabrata Sain"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18607",
    "title": "Using GPT-4 to guide causal machine learning",
    "abstract": "           Since its introduction to the public, ChatGPT has had an unprecedented impact. While some experts praised AI advancements and highlighted their potential risks, others have been critical about the accuracy and usefulness of Large Language Models (LLMs). In this paper, we are interested in the ability of LLMs to identify causal relationships. We focus on the well-established GPT-4 (Turbo) and evaluate its performance under the most restrictive conditions, by isolating its ability to infer causal relationships based solely on the variable labels without being given any context, demonstrating the minimum level of effectiveness one can expect when it is provided with label-only information. We show that questionnaire participants judge the GPT-4 graphs as the most accurate in the evaluated categories, closely followed by knowledge graphs constructed by domain experts, with causal Machine Learning (ML) far behind. We use these results to highlight the important limitation of causal ML, which often produces causal graphs that violate common sense, affecting trust in them. However, we show that pairing GPT-4 with causal ML overcomes this limitation, resulting in graphical structures learnt from real data that align more closely with those identified by domain experts, compared to structures learnt by causal ML alone. Overall, our findings suggest that despite GPT-4 not being explicitly designed to reason causally, it can still be a valuable tool for causal representation, as it improves the causal discovery process of causal ML algorithms that are designed to do just that.         ",
    "url": "https://arxiv.org/abs/2407.18607",
    "authors": [
      "Anthony C. Constantinou",
      "Neville K. Kitson",
      "Alessio Zanga"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18613",
    "title": "Dilated Strip Attention Network for Image Restoration",
    "abstract": "           Image restoration is a long-standing task that seeks to recover the latent sharp image from its deteriorated counterpart. Due to the robust capacity of self-attention to capture long-range dependencies, transformer-based methods or some attention-based convolutional neural networks have demonstrated promising results on many image restoration tasks in recent years. However, existing attention modules encounters limited receptive fields or abundant parameters. In order to integrate contextual information more effectively and efficiently, in this paper, we propose a dilated strip attention network (DSAN) for image restoration. Specifically, to gather more contextual information for each pixel from its neighboring pixels in the same row or column, a dilated strip attention (DSA) mechanism is elaborately proposed. By employing the DSA operation horizontally and vertically, each location can harvest the contextual information from a much wider region. In addition, we utilize multi-scale receptive fields across different feature groups in DSA to improve representation learning. Extensive experiments show that our DSAN outperforms state-of-the-art algorithms on several image restoration tasks.         ",
    "url": "https://arxiv.org/abs/2407.18613",
    "authors": [
      "Fangwei Hao",
      "Jiesheng Wu",
      "Ji Du",
      "Yinjie Wang",
      "Jing Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2407.18632",
    "title": "Robust VAEs via Generating Process of Noise Augmented Data",
    "abstract": "           Advancing defensive mechanisms against adversarial attacks in generative models is a critical research topic in machine learning. Our study focuses on a specific type of generative models - Variational Auto-Encoders (VAEs). Contrary to common beliefs and existing literature which suggest that noise injection towards training data can make models more robust, our preliminary experiments revealed that naive usage of noise augmentation technique did not substantially improve VAE robustness. In fact, it even degraded the quality of learned representations, making VAEs more susceptible to adversarial perturbations. This paper introduces a novel framework that enhances robustness by regularizing the latent space divergence between original and noise-augmented data. Through incorporating a paired probabilistic prior into the standard variational lower bound, our method significantly boosts defense against adversarial attacks. Our empirical evaluations demonstrate that this approach, termed Robust Augmented Variational Auto-ENcoder (RAVEN), yields superior performance in resisting adversarial inputs on widely-recognized benchmark datasets.         ",
    "url": "https://arxiv.org/abs/2407.18632",
    "authors": [
      "Hiroo Irobe",
      "Wataru Aoki",
      "Kimihiro Yamazaki",
      "Yuhui Zhang",
      "Takumi Nakagawa",
      "Hiroki Waida",
      "Yuichiro Wada",
      "Takafumi Kanamori"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18639",
    "title": "Vulnerability Detection in Ethereum Smart Contracts via Machine Learning: A Qualitative Analysis",
    "abstract": "           Smart contracts are central to a myriad of critical blockchain applications, from financial transactions to supply chain management. However, their adoption is hindered by security vulnerabilities that can result in significant financial losses. Most vulnerability detection tools and methods available nowadays leverage either static analysis methods or machine learning. Unfortunately, as valuable as they are, both approaches suffer from limitations that make them only partially effective. In this survey, we analyze the state of the art in machine-learning vulnerability detection for Ethereum smart contracts, by categorizing existing tools and methodologies, evaluating them, and highlighting their limitations. Our critical assessment unveils issues such as restricted vulnerability coverage and dataset construction flaws, providing us with new metrics to overcome the difficulties that restrain a sound comparison of existing solutions. Driven by our findings, we discuss best practices to enhance the accuracy, scope, and efficiency of vulnerability detection in smart contracts. Our guidelines address the known flaws while at the same time opening new avenues for research and development. By shedding light on current challenges and offering novel directions for improvement, we contribute to the advancement of secure smart contract development and blockchain technology as a whole.         ",
    "url": "https://arxiv.org/abs/2407.18639",
    "authors": [
      "Dalila Ressi",
      "Alvise Span\u00f2",
      "Lorenzo Benetollo",
      "Carla Piazza",
      "Michele Bugliesi",
      "Sabina Rossi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18655",
    "title": "Aspects of importance sampling in parameter selection for neural networks using ridgelet transform",
    "abstract": "           The choice of parameters in neural networks is crucial in the performance, and an oracle distribution derived from the ridgelet transform enables us to obtain suitable initial parameters. In other words, the distribution of parameters is connected to the integral representation of target functions. The oracle distribution allows us to avoid the conventional backpropagation learning process; only a linear regression is enough to construct the neural network in simple cases. This study provides a new look at the oracle distributions and ridgelet transforms, i.e., an aspect of importance sampling. In addition, we propose extensions of the parameter sampling methods. We demonstrate the aspect of importance sampling and the proposed sampling algorithms via one-dimensional and high-dimensional examples; the results imply that the magnitude of weight parameters could be more crucial than the intercept parameters.         ",
    "url": "https://arxiv.org/abs/2407.18655",
    "authors": [
      "Hikaru Homma",
      "Jun Ohkubo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18658",
    "title": "Adversarial Robustification via Text-to-Image Diffusion Models",
    "abstract": "           Adversarial robustness has been conventionally believed as a challenging property to encode for neural networks, requiring plenty of training data. In the recent paradigm of adopting off-the-shelf models, however, access to their training data is often infeasible or not practical, while most of such models are not originally trained concerning adversarial robustness. In this paper, we develop a scalable and model-agnostic solution to achieve adversarial robustness without using any data. Our intuition is to view recent text-to-image diffusion models as \"adaptable\" denoisers that can be optimized to specify target tasks. Based on this, we propose: (a) to initiate a denoise-and-classify pipeline that offers provable guarantees against adversarial attacks, and (b) to leverage a few synthetic reference images generated from the text-to-image model that enables novel adaptation schemes. Our experiments show that our data-free scheme applied to the pre-trained CLIP could improve the (provable) adversarial robustness of its diverse zero-shot classification derivatives (while maintaining their accuracy), significantly surpassing prior approaches that utilize the full training data. Not only for CLIP, we also demonstrate that our framework is easily applicable for robustifying other visual classifiers efficiently.         ",
    "url": "https://arxiv.org/abs/2407.18658",
    "authors": [
      "Daewon Choi",
      "Jongheon Jeong",
      "Huiwon Jang",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18689",
    "title": "The BIAS Detection Framework: Bias Detection in Word Embeddings and Language Models for European Languages",
    "abstract": "           The project BIAS: Mitigating Diversity Biases of AI in the Labor Market is a four-year project funded by the European commission and supported by the Swiss State Secretariat for Education, Research and Innovation (SERI). As part of the project, novel bias detection methods to identify societal bias in language models and word embeddings in European languages are developed, with particular attention to linguistic and geographic particularities. This technical report describes the overall architecture and components of the BIAS Detection Framework. The code described in this technical report is available and will be updated and expanded continuously with upcoming results from the BIAS project. The details about the datasets for the different languages are described in corresponding papers at scientific venues.         ",
    "url": "https://arxiv.org/abs/2407.18689",
    "authors": [
      "Alexandre Puttick",
      "Leander Rankwiler",
      "Catherine Ikae",
      "Mascha Kurpicz-Briki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.18691",
    "title": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing Heterogeneous Temporal Dynamics",
    "abstract": "           Real-time condition monitoring is crucial for the reliable and efficient operation of complex systems. However, relying solely on physical sensors can be limited due to their cost, placement constraints, or inability to directly measure certain critical parameters. Virtual sensing addresses these limitations by leveraging readily available sensor data and system knowledge to estimate inaccessible parameters or infer system states. The increasing complexity of industrial systems necessitates deployments of sensors with diverse modalities to provide a comprehensive understanding of system states. These sensors capture data at varying frequencies to monitor both rapid and slowly varying system dynamics, as well as local and global state evolutions of the systems. This leads to heterogeneous temporal dynamics, which, particularly under varying operational end environmental conditions, pose a significant challenge for accurate virtual sensing. To address this, we propose a Heterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly models signals from diverse sensors and integrates operating conditions into the model architecture. We evaluate HTGNN using two newly released datasets: a bearing dataset with diverse load conditions for bearing load prediction and a year-long simulated dataset for predicting bridge live loads. Our results demonstrate that HTGNN significantly outperforms established baseline methods in both tasks, particularly under highly varying operating conditions. These results highlight HTGNN's potential as a robust and accurate virtual sensing approach for complex systems, paving the way for improved monitoring, predictive maintenance, and enhanced system performance.         ",
    "url": "https://arxiv.org/abs/2407.18691",
    "authors": [
      "Mengjie Zhao",
      "Cees Taal",
      "Stephan Baggerohr",
      "Olga Fink"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2407.18707",
    "title": "Finite Neural Networks as Mixtures of Gaussian Processes: From Provable Error Bounds to Prior Selection",
    "abstract": "           Infinitely wide or deep neural networks (NNs) with independent and identically distributed (i.i.d.) parameters have been shown to be equivalent to Gaussian processes. Because of the favorable properties of Gaussian processes, this equivalence is commonly employed to analyze neural networks and has led to various breakthroughs over the years. However, neural networks and Gaussian processes are equivalent only in the limit; in the finite case there are currently no methods available to approximate a trained neural network with a Gaussian model with bounds on the approximation error. In this work, we present an algorithmic framework to approximate a neural network of finite width and depth, and with not necessarily i.i.d. parameters, with a mixture of Gaussian processes with error bounds on the approximation error. In particular, we consider the Wasserstein distance to quantify the closeness between probabilistic models and, by relying on tools from optimal transport and Gaussian processes, we iteratively approximate the output distribution of each layer of the neural network as a mixture of Gaussian processes. Crucially, for any NN and $\\epsilon >0$ our approach is able to return a mixture of Gaussian processes that is $\\epsilon$-close to the NN at a finite set of input points. Furthermore, we rely on the differentiability of the resulting error bound to show how our approach can be employed to tune the parameters of a NN to mimic the functional behavior of a given Gaussian process, e.g., for prior selection in the context of Bayesian inference. We empirically investigate the effectiveness of our results on both regression and classification problems with various neural network architectures. Our experiments highlight how our results can represent an important step towards understanding neural network predictions and formally quantifying their uncertainty.         ",
    "url": "https://arxiv.org/abs/2407.18707",
    "authors": [
      "Steven Adams",
      "Patan\u00e8",
      "Morteza Lahijanian",
      "Luca Laurenti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.18715",
    "title": "BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation",
    "abstract": "           Scene Graph Generation (SGG) remains a challenging task due to its compositional property. Previous approaches improve prediction efficiency by learning in an end-to-end manner. However, these methods exhibit limited performance as they assume unidirectional conditioning between entities and predicates, leading to insufficient information interaction. To address this limitation, we propose a novel bidirectional conditioning factorization for SGG, introducing efficient interaction between entities and predicates. Specifically, we develop an end-to-end scene graph generation model, Bidirectional Conditioning Transformer (BCTR), to implement our factorization. BCTR consists of two key modules. First, the Bidirectional Conditioning Generator (BCG) facilitates multi-stage interactive feature augmentation between entities and predicates, enabling mutual benefits between the two predictions. Second, Random Feature Alignment (RFA) regularizes the feature space by distilling multi-modal knowledge from pre-trained models, enhancing BCTR's ability on tailed categories without relying on statistical priors. We conduct a series of experiments on Visual Genome and Open Image V6, demonstrating that BCTR achieves state-of-the-art performance on both benchmarks. The code will be available upon acceptance of the paper.         ",
    "url": "https://arxiv.org/abs/2407.18715",
    "authors": [
      "Peng Hao",
      "Xiaobing Wang",
      "Yingying Jiang",
      "Hanchao Jia",
      "Xiaoshuai Hao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.18735",
    "title": "AutoRDF2GML: Facilitating RDF Integration in Graph Machine Learning",
    "abstract": "           In this paper, we introduce AutoRDF2GML, a framework designed to convert RDF data into data representations tailored for graph machine learning tasks. AutoRDF2GML enables, for the first time, the creation of both content-based features -- i.e., features based on RDF datatype properties -- and topology-based features -- i.e., features based on RDF object properties. Characterized by automated feature extraction, AutoRDF2GML makes it possible even for users less familiar with RDF and SPARQL to generate data representations ready for graph machine learning tasks, such as link prediction, node classification, and graph classification. Furthermore, we present four new benchmark datasets for graph machine learning, created from large RDF knowledge graphs using our framework. These datasets serve as valuable resources for evaluating graph machine learning approaches, such as graph neural networks. Overall, our framework effectively bridges the gap between the Graph Machine Learning and Semantic Web communities, paving the way for RDF-based machine learning applications.         ",
    "url": "https://arxiv.org/abs/2407.18735",
    "authors": [
      "Michael F\u00e4rber",
      "David Lamprecht",
      "Yuni Susanti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.18752",
    "title": "Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery",
    "abstract": "           Causal discovery aims to estimate causal structures among variables based on observational data. Large Language Models (LLMs) offer a fresh perspective to tackle the causal discovery problem by reasoning on the metadata associated with variables rather than their actual data values, an approach referred to as knowledge-based causal discovery. In this paper, we investigate the capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1 billion parameters) with prompt-based learning for knowledge-based causal discovery. Specifically, we present KG Structure as Prompt, a novel approach for integrating structural information from a knowledge graph, such as common neighbor nodes and metapaths, into prompt-based learning to enhance the capabilities of SLMs. Experimental results on three types of biomedical and open-domain datasets under few-shot settings demonstrate the effectiveness of our approach, surpassing most baselines and even conventional fine-tuning approaches trained on full datasets. Our findings further highlight the strong capabilities of SLMs: in combination with knowledge graphs and prompt-based learning, SLMs demonstrate the potential to surpass LLMs with larger number of parameters. Our code and datasets are available on GitHub.         ",
    "url": "https://arxiv.org/abs/2407.18752",
    "authors": [
      "Yuni Susanti",
      "Michael F\u00e4rber"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18756",
    "title": "Evaluating Human Trajectory Prediction with Metamorphic Testing",
    "abstract": "           The prediction of human trajectories is important for planning in autonomous systems that act in the real world, e.g. automated driving or mobile robots. Human trajectory prediction is a noisy process, and no prediction does precisely match any future trajectory. It is therefore approached as a stochastic problem, where the goal is to minimise the error between the true and the predicted trajectory. In this work, we explore the application of metamorphic testing for human trajectory prediction. Metamorphic testing is designed to handle unclear or missing test oracles. It is well-designed for human trajectory prediction, where there is no clear criterion of correct or incorrect human behaviour. Metamorphic relations rely on transformations over source test cases and exploit invariants. A setting well-designed for human trajectory prediction where there are many symmetries of expected human behaviour under variations of the input, e.g. mirroring and rescaling of the input data. We discuss how metamorphic testing can be applied to stochastic human trajectory prediction and introduce the Wasserstein Violation Criterion to statistically assess whether a follow-up test case violates a label-preserving metamorphic relation.         ",
    "url": "https://arxiv.org/abs/2407.18756",
    "authors": [
      "Helge Spieker",
      "Nassim Belmecheri",
      "Arnaud Gotlieb",
      "Nadjib Lazaar"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18760",
    "title": "Java-Class-Hijack: Software Supply Chain Attack for Java based on Maven Dependency Resolution and Java Classloading",
    "abstract": "           We introduce Java-Class-Hijack, a novel software supply chain attack that enables an attacker to inject malicious code by crafting a class that shadows a legitimate class that is in the dependency tree. We describe the attack, provide a proof-of-concept demonstrating its feasibility, and replicate it in the German Corona-Warn-App server application. The proof-of-concept illustrates how a transitive dependency deep within the dependency tree can hijack a class from a direct dependency and entirely alter its behavior, posing a significant security risk to Java applications. The replication on the Corona-Warn-App demonstrates how compromising a small JSON validation library could result in a complete database takeover.         ",
    "url": "https://arxiv.org/abs/2407.18760",
    "authors": [
      "Federico Bono",
      "Frank Reyes",
      "Aman Sharma",
      "Benoit Baudry",
      "Martin Monperrus"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2407.18766",
    "title": "Secrecy Performance Analysis of Integrated RF-UWOC IoT Networks Enabled by UAV and Underwater-RIS",
    "abstract": "           In the sixth-generation (6G) Internet of Things (IoT) networks, the use of UAV-mounted base stations and reconfigurable intelligent surfaces (RIS) has been considered to enhance coverage, flexibility, and security in non-terrestrial networks (NTNs). In addition to aerial networks enabled by NTN technologies, the integration of underwater networks with 6G IoT can be considered one of the most innovative challenges in future IoT. Along with such trends in IoT, this study investigates the secrecy performance of IoT networks that integrate radio frequency (RF) UAV-based NTNs and underwater optical wireless communication (UOWC) links with an RIS. Considering three potential eavesdropping scenarios (RF signal, UOWC signal, and both), we derive closed-form expressions for secrecy performance metrics, including average secrecy capacity, secrecy outage probability, probability of strictly positive secrecy capacity, and effective secrecy throughput. Extensive numerical analyses and Monte Carlo simulations elucidate the impact of system parameters such as fading severity, the number of RIS reflecting elements, underwater turbulence, pointing errors, and detection techniques on system security. The findings offer comprehensive design guidelines for developing such a network aiming to enhance secrecy performance and ensure secure communication in diverse and challenging environments.         ",
    "url": "https://arxiv.org/abs/2407.18766",
    "authors": [
      "Abrar Bin Sarawar",
      "A. S. M. Badrudduza",
      "Md. Ibrahim",
      "Imran Shafique Ansari",
      "Heejung Yu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.18772",
    "title": "Learning production functions for supply chains with graph neural networks",
    "abstract": "           The global economy relies on the flow of goods over supply chain networks, with nodes as firms and edges as transactions between firms. While we may observe these external transactions, they are governed by unseen production functions, which determine how firms internally transform the input products they receive into output products that they sell. In this setting, it can be extremely valuable to infer these production functions, to better understand and improve supply chains, and to forecast future transactions more accurately. However, existing graph neural networks (GNNs) cannot capture these hidden relationships between nodes' inputs and outputs. Here, we introduce a new class of models for this setting, by combining temporal GNNs with a novel inventory module, which learns production functions via attention weights and a special loss function. We evaluate our models extensively on real supply chains data, along with data generated from our new open-source simulator, SupplySim. Our models successfully infer production functions, with a 6-50% improvement over baselines, and forecast future transactions on real and synthetic data, outperforming baselines by 11-62%.         ",
    "url": "https://arxiv.org/abs/2407.18772",
    "authors": [
      "Serina Chang",
      "Zhiyin Lin",
      "Benjamin Yan",
      "Swapnil Bembde",
      "Qi Xiu",
      "Chi Heem Wong",
      "Yu Qin",
      "Frank Kloster",
      "Alex Luo",
      "Raj Palleti",
      "Jure Leskovec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2407.18787",
    "title": "Automatic Detection of Moral Values in Music Lyrics",
    "abstract": "           Moral values play a fundamental role in how we evaluate information, make decisions, and form judgements around important social issues. The possibility to extract morality rapidly from lyrics enables a deeper understanding of our music-listening behaviours. Building on the Moral Foundations Theory (MFT), we tasked a set of transformer-based language models (BERT) fine-tuned on 2,721 synthetic lyrics generated by a large language model (GPT-4) to detect moral values in 200 real music lyrics annotated by two experts.We evaluate their predictive capabilities against a series of baselines including out-of-domain (BERT fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4) classification. The proposed models yielded the best accuracy across experiments, with an average F1 weighted score of 0.8. This performance is, on average, 5% higher than out-of-domain and zero-shot models. When examining precision in binary classification, the proposed models perform on average 12% higher than the baselines.Our approach contributes to annotation-free and effective lyrics morality learning, and provides useful insights into the knowledge distillation of LLMs regarding moral expression in music, and the potential impact of these technologies on the creative industries and musical culture.         ",
    "url": "https://arxiv.org/abs/2407.18787",
    "authors": [
      "Vjosa Preniqi",
      "Iacopo Ghinassi",
      "Julia Ive",
      "Kyriaki Kalimeri",
      "Charalampos Saitis"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2407.18789",
    "title": "Granularity is crucial when applying differential privacy to text: An investigation for neural machine translation",
    "abstract": "           Applying differential privacy (DP) by means of the DP-SGD algorithm to protect individual data points during training is becoming increasingly popular in NLP. However, the choice of granularity at which DP is applied is often neglected. For example, neural machine translation (NMT) typically operates on the sentence-level granularity. From the perspective of DP, this setup assumes that each sentence belongs to a single person and any two sentences in the training dataset are independent. This assumption is however violated in many real-world NMT datasets, e.g. those including dialogues. For proper application of DP we thus must shift from sentences to entire documents. In this paper, we investigate NMT at both the sentence and document levels, analyzing the privacy/utility trade-off for both scenarios, and evaluating the risks of not using the appropriate privacy granularity in terms of leaking personally identifiable information (PII). Our findings indicate that the document-level NMT system is more resistant to membership inference attacks, emphasizing the significance of using the appropriate granularity when working with DP.         ",
    "url": "https://arxiv.org/abs/2407.18789",
    "authors": [
      "Doan Nam Long Vu",
      "Timour Igamberdiev",
      "Ivan Habernal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.18798",
    "title": "Predicting 3D Rigid Body Dynamics with Deep Residual Network",
    "abstract": "           This study investigates the application of deep residual networks for predicting the dynamics of interacting three-dimensional rigid bodies. We present a framework combining a 3D physics simulator implemented in C++ with a deep learning model constructed using PyTorch. The simulator generates training data encompassing linear and angular motion, elastic collisions, fluid friction, gravitational effects, and damping. Our deep residual network, consisting of an input layer, multiple residual blocks, and an output layer, is designed to handle the complexities of 3D dynamics. We evaluate the network's performance using a datasetof 10,000 simulated scenarios, each involving 3-5 interacting rigid bodies. The model achieves a mean squared error of 0.015 for position predictions and 0.022 for orientation predictions, representing a 25% improvement over baseline methods. Our results demonstrate the network's ability to capture intricate physical interactions, with particular success in predicting elastic collisions and rotational dynamics. This work significantly contributes to physics-informed machine learning by showcasing the immense potential of deep residual networks in modeling complex 3D physical systems. We discuss our approach's limitations and propose future directions for improving generalization to more diverse object shapes and materials.         ",
    "url": "https://arxiv.org/abs/2407.18798",
    "authors": [
      "Abiodun Finbarrs Oketunji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18803",
    "title": "Design Frictions on Social Media: Balancing Reduced Mindless Scrolling and User Satisfaction",
    "abstract": "           Design features of social media platforms, such as infinite scroll, increase users' likelihood of experiencing normative dissociation -- a mental state of absorption that diminishes self-awareness and disrupts memory. This paper investigates how adding design frictions into the interface of a social media platform reduce mindless scrolling and user satisfaction. We conducted a study with 30 participants and compared their memory recognition of posts in two scenarios: one where participants had to react to each post to access further content and another using an infinite scroll design. Participants who used the design frictions interface exhibited significantly better content recall, although a majority of participants found the interface frustrating. We discuss design recommendations and scenarios where adding design frictions to social media platforms can be beneficial.         ",
    "url": "https://arxiv.org/abs/2407.18803",
    "authors": [
      "Nicolas Ruiz",
      "Grabriela Molina Le\u00f3n",
      "Hendrik Heuer"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2407.18807",
    "title": "Robust Learning in Bayesian Parallel Branching Graph Neural Networks: The Narrow Width Limit",
    "abstract": "           The infinite width limit of random neural networks is known to result in Neural Networks as Gaussian Process (NNGP) (Lee et al. [2018]), characterized by task-independent kernels. It is widely accepted that larger network widths contribute to improved generalization (Park et al. [2019]). However, this work challenges this notion by investigating the narrow width limit of the Bayesian Parallel Branching Graph Neural Network (BPB-GNN), an architecture that resembles residual networks. We demonstrate that when the width of a BPB-GNN is significantly smaller compared to the number of training examples, each branch exhibits more robust learning due to a symmetry breaking of branches in kernel renormalization. Surprisingly, the performance of a BPB-GNN in the narrow width limit is generally superior or comparable to that achieved in the wide width limit in bias-limited scenarios. Furthermore, the readout norms of each branch in the narrow width limit are mostly independent of the architectural hyperparameters but generally reflective of the nature of the data. Our results characterize a newly defined narrow-width regime for parallel branching networks in general.         ",
    "url": "https://arxiv.org/abs/2407.18807",
    "authors": [
      "Zechen Zhang",
      "Haim Sompolinsky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18809",
    "title": "Joint Slot and Power Optimization for Grant Free Random Access with Unknown and Heterogeneous Device Activity",
    "abstract": "           Grant Free Random Access (GFRA) is a popular protocol in the Internet of Things (IoT) to reduce the control signaling. GFRA is a framed protocol where each frame is split into two parts: device identification; and data transmission part which can be viewed as a form of Frame Slotted ALOHA (FSA). A common assumption in FSA is device homogeneity; that is the probability that a device seeks to transmit data in a particular frame is common for all devices and independent of the other devices. Recent work has investigated the possibility of tuning the FSA protocol to the statistics of the network by changing the probability for a particular device to access a particular slot. However, power control with a successive interference cancellation (SIC) receiver has not yet been considered to further increase the performance of the tuned FSA protocols. In this paper, we propose algorithms to jointly optimize both the slot selection and the transmit power of the devices to minimize the outage of the devices in the network. We show via a simulation study that our algorithms can outperform baselines (including slotted ALOHA) in terms of expected number of devices transmitting without outage and in term of transmit power.         ",
    "url": "https://arxiv.org/abs/2407.18809",
    "authors": [
      "Alix Jeannerot",
      "Malcolm Egan",
      "Jean-Marie Gorce"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.18813",
    "title": "HERO-SLAM: Hybrid Enhanced Robust Optimization of Neural SLAM",
    "abstract": "           Simultaneous Localization and Mapping (SLAM) is a fundamental task in robotics, driving numerous applications such as autonomous driving and virtual reality. Recent progress on neural implicit SLAM has shown encouraging and impressive results. However, the robustness of neural SLAM, particularly in challenging or data-limited situations, remains an unresolved issue. This paper presents HERO-SLAM, a Hybrid Enhanced Robust Optimization method for neural SLAM, which combines the benefits of neural implicit field and feature-metric optimization. This hybrid method optimizes a multi-resolution implicit field and enhances robustness in challenging environments with sudden viewpoint changes or sparse data collection. Our comprehensive experimental results on benchmarking datasets validate the effectiveness of our hybrid approach, demonstrating its superior performance over existing implicit field-based methods in challenging scenarios. HERO-SLAM provides a new pathway to enhance the stability, performance, and applicability of neural SLAM in real-world scenarios. Code is available on the project page: this https URL.         ",
    "url": "https://arxiv.org/abs/2407.18813",
    "authors": [
      "Zhe Xin",
      "Yufeng Yue",
      "Liangjun Zhang",
      "Chenming Wu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2407.18832",
    "title": "Accurate and Scalable Detection and Investigation of Cyber Persistence Threats",
    "abstract": "           In Advanced Persistent Threat (APT) attacks, achieving stealthy persistence within target systems is often crucial for an attacker's success. This persistence allows adversaries to maintain prolonged access, often evading detection mechanisms. Recognizing its pivotal role in the APT lifecycle, this paper introduces Cyber Persistence Detector (CPD), a novel system dedicated to detecting cyber persistence through provenance analytics. CPD is founded on the insight that persistent operations typically manifest in two phases: the \"persistence setup\" and the subsequent \"persistence execution\". By causally relating these phases, we enhance our ability to detect persistent threats. First, CPD discerns setups signaling an impending persistent threat and then traces processes linked to remote connections to identify persistence execution activities. A key feature of our system is the introduction of pseudo-dependency edges (pseudo-edges), which effectively connect these disjoint phases using data provenance analysis, and expert-guided edges, which enable faster tracing and reduced log size. These edges empower us to detect persistence threats accurately and efficiently. Moreover, we propose a novel alert triage algorithm that further reduces false positives associated with persistence threats. Evaluations conducted on well-known datasets demonstrate that our system reduces the average false positive rate by 93% compared to state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2407.18832",
    "authors": [
      "Qi Liu",
      "Muhammad Shoaib",
      "Mati Ur Rehman",
      "Kaibin Bao",
      "Veit Hagenmeyer",
      "Wajih Ul Hassan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.18838",
    "title": "The Role of Temporal Hierarchy in Spiking Neural Networks",
    "abstract": "           Spiking Neural Networks (SNNs) have the potential for rich spatio-temporal signal processing thanks to exploiting both spatial and temporal parameters. The temporal dynamics such as time constants of the synapses and neurons and delays have been recently shown to have computational benefits that help reduce the overall number of parameters required in the network and increase the accuracy of the SNNs in solving temporal tasks. Optimizing such temporal parameters, for example, through gradient descent, gives rise to a temporal architecture for different problems. As has been shown in machine learning, to reduce the cost of optimization, architectural biases can be applied, in this case in the temporal domain. Such inductive biases in temporal parameters have been found in neuroscience studies, highlighting a hierarchy of temporal structure and input representation in different layers of the cortex. Motivated by this, we propose to impose a hierarchy of temporal representation in the hidden layers of SNNs, highlighting that such an inductive bias improves their performance. We demonstrate the positive effects of temporal hierarchy in the time constants of feed-forward SNNs applied to temporal tasks (Multi-Time-Scale XOR and Keyword Spotting, with a benefit of up to 4.1% in classification accuracy). Moreover, we show that such architectural biases, i.e. hierarchy of time constants, naturally emerge when optimizing the time constants through gradient descent, initialized as homogeneous values. We further pursue this proposal in temporal convolutional SNNs, by introducing the hierarchical bias in the size and dilation of temporal kernels, giving rise to competitive results in popular temporal spike-based datasets.         ",
    "url": "https://arxiv.org/abs/2407.18838",
    "authors": [
      "Filippo Moro",
      "Pau Vilimelis Aceituno",
      "Laura Kriener",
      "Melika Payvand"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18847",
    "title": "Enhancing material property prediction with ensemble deep graph convolutional networks",
    "abstract": "           Machine learning (ML) models have emerged as powerful tools for accelerating materials discovery and design by enabling accurate predictions of properties from compositional and structural data. These capabilities are vital for developing advanced technologies across fields such as energy, electronics, and biomedicine, potentially reducing the time and resources needed for new material exploration and promoting rapid innovation cycles. Recent efforts have focused on employing advanced ML algorithms, including deep learning - based graph neural network, for property prediction. Additionally, ensemble models have proven to enhance the generalizability and robustness of ML and DL. However, the use of such ensemble strategies in deep graph networks for material property prediction remains underexplored. Our research provides an in-depth evaluation of ensemble strategies in deep learning - based graph neural network, specifically targeting material property prediction tasks. By testing the Crystal Graph Convolutional Neural Network (CGCNN) and its multitask version, MT-CGCNN, we demonstrated that ensemble techniques, especially prediction averaging, substantially improve precision beyond traditional metrics for key properties like formation energy per atom ($\\Delta E^{f}$), band gap ($E_{g}$) and density ($\\rho$) in 33,990 stable inorganic materials. These findings support the broader application of ensemble methods to enhance predictive accuracy in the field.         ",
    "url": "https://arxiv.org/abs/2407.18847",
    "authors": [
      "Chowdhury Mohammad Abid Rahman",
      "Ghadendra Bhandari",
      "Nasser M Nasrabadi",
      "Aldo H. Romero",
      "Prashnna K. Gyawali"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18848",
    "title": "Repairing Networks of $\\mathcal{EL_\\perp}$ Ontologies using Weakening and Completing -- Extended version",
    "abstract": "           The quality of ontologies and their alignments is crucial for developing high-quality semantics-based applications. Traditional debugging techniques repair ontology networks by removing unwanted axioms and mappings, but may thereby remove consequences that are correct in the domain of the ontology network. In this paper we propose a framework for repairing ontology networks that deals with this issue. It defines basic operations such as debugging, weakening and completing. Further, it defines combination operators that reflect choices in how and when to use the basic operators, as well as choices regarding the autonomy level of the ontologies and alignments in the ontology network. We show the influence of the combination operators on the quality of the repaired network and present an implemented tool. By using our framework together with existing algorithms for debugging, weakening and completing, we essentially provide a blueprint for extending previous work and systems.         ",
    "url": "https://arxiv.org/abs/2407.18848",
    "authors": [
      "Ying Li",
      "Patrick Lambrix"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2407.18849",
    "title": "MNTD: An Efficient Dynamic Community Detector Based on Nonnegative Tensor Decomposition",
    "abstract": "           Dynamic community detection is crucial for elucidating the temporal evolution of social structures, information dissemination, and interactive behaviors within complex networks. Nonnegative matrix factorization provides an efficient framework for identifying communities in static networks but fall short in depicting temporal variations in community affiliations. To solve this problem, this paper proposes a Modularity maximization-incorporated Nonnegative Tensor RESCAL Decomposition (MNTD) model for dynamic community detection. This method serves two primary functions: a) Nonnegative tensor RESCAL decomposition extracts latent community structures in different time slots, highlighting the persistence and transformation of communities; and b) Incorporating an initial community structure into the modularity maximization algorithm, facilitating more precise community segmentations. Comparative analysis of real-world datasets shows that the MNTD is superior to state-of-the-art dynamic community detection methods in the accuracy of community detection.         ",
    "url": "https://arxiv.org/abs/2407.18849",
    "authors": [
      "Hao Fang",
      "Qu Wang",
      "Qicong Hu",
      "Hao Wu"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2407.18858",
    "title": "HADES: Detecting Active Directory Attacks via Whole Network Provenance Analytics",
    "abstract": "           Due to its crucial role in identity and access management in modern enterprise networks, Active Directory (AD) is a top target of Advanced Persistence Threat (APT) actors. Conventional intrusion detection systems (IDS) excel at identifying malicious behaviors caused by malware, but often fail to detect stealthy attacks launched by APT actors. Recent advance in provenance-based IDS (PIDS) shows promises by exposing malicious system activities in causal attack graphs. However, existing approaches are restricted to intra-machine tracing, and unable to reveal the scope of attackers' traversal inside a network. We propose HADES, the first PIDS capable of performing accurate causality-based cross-machine tracing by leveraging a novel concept called logon session based execution partitioning to overcome several challenges in cross-machine tracing. We design HADES as an efficient on-demand tracing system, which performs whole-network tracing only when it first identifies an authentication anomaly signifying an ongoing AD attack, for which we introduce a novel lightweight authentication anomaly detection model rooted in our extensive analysis of AD attacks. To triage attack alerts, we present a new algorithm integrating two key insights we identified in AD attacks. Our evaluations show that HADES outperforms both popular open source detection systems and a prominent commercial AD attack detector.         ",
    "url": "https://arxiv.org/abs/2407.18858",
    "authors": [
      "Qi Liu",
      "Kaibin Bao",
      "Wajih Ul Hassan",
      "Veit Hagenmeyer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.18865",
    "title": "Downlink CCM Estimation via Representation Learning with Graph Regularization",
    "abstract": "           In this paper, we propose an algorithm for downlink (DL) channel covariance matrix (CCM) estimation for frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) communication systems with base station (BS) possessing a uniform linear array (ULA) antenna structure. We make use of the inherent similarity between the uplink (UL) CCM and the DL CCM due to angular reciprocity. We consider a setting where the UL CCM is mapped to DL CCM by a mapping function. We first present a theoretical error analysis of learning a nonlinear embedding by constructing a mapping function, which points to the importance of the Lipschitz regularity of the mapping function for achieving high estimation performance. Then, based on the theoretical ground, we propose a representation learning algorithm as a solution for the estimation problem, where Gaussian RBF kernel interpolators are chosen to map UL CCMs to their DL counterparts. The proposed algorithm is based on the optimization of an objective function that fits a regression model between the DL CCM and UL CCM samples in the training dataset and preserves the local geometric structure of the data in the UL CCM space, while explicitly regulating the Lipschitz continuity of the mapping function in light of our theoretical findings. The proposed algorithm surpasses benchmark methods in terms of three error metrics as shown by simulations.         ",
    "url": "https://arxiv.org/abs/2407.18865",
    "authors": [
      "Melih Can Zerin",
      "Elif Vural",
      "Ali \u00d6zg\u00fcr Y\u0131lmaz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.18875",
    "title": "Generative Adversarial Networks for Imputing Sparse Learning Performance",
    "abstract": "           Learning performance data, such as correct or incorrect responses to questions in Intelligent Tutoring Systems (ITSs) is crucial for tracking and assessing the learners' progress and mastery of knowledge. However, the issue of data sparsity, characterized by unexplored questions and missing attempts, hampers accurate assessment and the provision of tailored, personalized instruction within ITSs. This paper proposes using the Generative Adversarial Imputation Networks (GAIN) framework to impute sparse learning performance data, reconstructed into a three-dimensional (3D) tensor representation across the dimensions of learners, questions and attempts. Our customized GAIN-based method computational process imputes sparse data in a 3D tensor space, significantly enhanced by convolutional neural networks for its input and output layers. This adaptation also includes the use of a least squares loss function for optimization and aligns the shapes of the input and output with the dimensions of the questions-attempts matrices along the learners' dimension. Through extensive experiments on six datasets from various ITSs, including AutoTutor, ASSISTments and MATHia, we demonstrate that the GAIN approach generally outperforms existing methods such as tensor factorization and other generative adversarial network (GAN) based approaches in terms of imputation accuracy. This finding enhances comprehensive learning data modeling and analytics in AI-based education.         ",
    "url": "https://arxiv.org/abs/2407.18875",
    "authors": [
      "Liang Zhang",
      "Mohammed Yeasin",
      "Jionghao Lin",
      "Felix Havugimana",
      "Xiangen Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18877",
    "title": "Code Structure-Aware through Line-level Semantic Learning for Code Vulnerability Detection",
    "abstract": "           Different from the flow semantics of natural languages, programming languages are inherently rigid in structure and grammar. Existing fine-tuning methodologies for code vulnerability detection generally treat code as long text sequences, stripping away structural elements such as newlines ('/n') and whitespace. However, this approach inadvertently results in the loss of crucial structural information, diminishing the distinct characteristics of code and impairing the accuracy of vulnerability detection. To address these challenges, we propose a novel network architecture method based on pre-trained code models, which incorporates structural information awareness. We propose an enhanced code text processing workflow that retains structural elements prior to modeling. This refinement allows the model to retain and exploit line-level structural information and semantic information during the modeling process. Furthermore, we introduce a new network architecture, the Code Structure-Aware Network through Line-level Semantic Learning (CSLS), which integrates three key components: global vulnerability awareness, line-structural awareness, and sensitive-line awareness. We have conducted comprehensive experiments using vulnerability detection datasets from real-world projects. Extensive experiments were conducted on vulnerability detection datasets derived from real-world projects. The results demonstrate that our new code pre-processing flow significantly improves existing baselines (e.g., a 3\\% accuracy improvement on the Devign dataset when applied to popular models such as CoderBert and UniXcoder). The proposed network architecture also demonstrates superior accuracy in detecting vulnerabilities, surpassing newly established benchmarks. These findings underscore the importance of structural information in enhancing the efficacy of code vulnerability detection models.         ",
    "url": "https://arxiv.org/abs/2407.18877",
    "authors": [
      "Ziliang Wang",
      "Ge Li",
      "Jia Li",
      "Yihong Dong",
      "Yingfei Xiong",
      "Zhi Jin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2407.18887",
    "title": "Embedding And Clustering Your Data Can Improve Contrastive Pretraining",
    "abstract": "           Recent studies of large-scale contrastive pretraining in the text embedding domain show that using single-source minibatches, rather than mixed-source minibatches, can substantially improve overall model accuracy. In this work, we explore extending training data stratification beyond source granularity by leveraging a pretrained text embedding model and the classic k-means clustering algorithm to further split training data apart by the semantic clusters within each source. Experimentally, we observe a notable increase in NDCG@10 when pretraining a BERT-based text embedding model on query-passage pairs from the MSMARCO passage retrieval dataset. Additionally, we conceptually connect our clustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B methodology and the nearest-neighbor-based hard-negative mining aspect of the ANCE methodology and discuss how this unified view motivates future lines of research on the organization of contrastive pretraining data.         ",
    "url": "https://arxiv.org/abs/2407.18887",
    "authors": [
      "Luke Merrick"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.18906",
    "title": "A Scalable Quantum Non-local Neural Network for Image Classification",
    "abstract": "           Non-local operations play a crucial role in computer vision enabling the capture of long-range dependencies through weighted sums of features across the input, surpassing the constraints of traditional convolution operations that focus solely on local neighborhoods. Non-local operations typically require computing pairwise relationships between all elements in a set, leading to quadratic complexity in terms of time and memory. Due to the high computational and memory demands, scaling non-local neural networks to large-scale problems can be challenging. This article introduces a hybrid quantum-classical scalable non-local neural network, referred to as Quantum Non-Local Neural Network (QNL-Net), to enhance pattern recognition. The proposed QNL-Net relies on inherent quantum parallelism to allow the simultaneous processing of a large number of input features enabling more efficient computations in quantum-enhanced feature space and involving pairwise relationships through quantum entanglement. We benchmark our proposed QNL-Net with other quantum counterparts to binary classification with datasets MNIST and CIFAR-10. The simulation findings showcase our QNL-Net achieves cutting-edge accuracy levels in binary image classification among quantum classifiers while utilizing fewer qubits.         ",
    "url": "https://arxiv.org/abs/2407.18906",
    "authors": [
      "Sparsh Gupta",
      "Debanjan Konar",
      "Vaneet Aggarwal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2407.18910",
    "title": "Do We Really Need Graph Convolution During Training? Light Post-Training Graph-ODE for Efficient Recommendation",
    "abstract": "           The efficiency and scalability of graph convolution networks (GCNs) in training recommender systems (RecSys) have been persistent concerns, hindering their deployment in real-world applications. This paper presents a critical examination of the necessity of graph convolutions during the training phase and introduces an innovative alternative: the Light Post-Training Graph Ordinary-Differential-Equation (LightGODE). Our investigation reveals that the benefits of GCNs are more pronounced during testing rather than training. Motivated by this, LightGODE utilizes a novel post-training graph convolution method that bypasses the computation-intensive message passing of GCNs and employs a non-parametric continuous graph ordinary-differential-equation (ODE) to dynamically model node representations. This approach drastically reduces training time while achieving fine-grained post-training graph convolution to avoid the distortion of the original training embedding space, termed the embedding discrepancy issue. We validate our model across several real-world datasets of different scales, demonstrating that LightGODE not only outperforms GCN-based models in terms of efficiency and effectiveness but also significantly mitigates the embedding discrepancy commonly associated with deeper graph convolution layers. Our LightGODE challenges the prevailing paradigms in RecSys training and suggests re-evaluating the role of graph convolutions, potentially guiding future developments of efficient large-scale graph-based RecSys.         ",
    "url": "https://arxiv.org/abs/2407.18910",
    "authors": [
      "Weizhi Zhang",
      "Liangwei Yang",
      "Zihe Song",
      "Henry Peng Zou",
      "Ke Xu",
      "Henry Peng Zou",
      "Liancheng Fang",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.18306",
    "title": "Design and demonstration of an operating system for executing applications on quantum network nodes",
    "abstract": "           The goal of future quantum networks is to enable new internet applications that are impossible to achieve using solely classical communication. Up to now, demonstrations of quantum network applications and functionalities on quantum processors have been performed in ad-hoc software that was specific to the experimental setup, programmed to perform one single task (the application experiment) directly into low-level control devices using expertise in experimental physics. Here, we report on the design and implementation of the first architecture capable of executing quantum network applications on quantum processors in platform-independent high-level software. We demonstrate the architecture's capability to execute applications in high-level software, by implementing it as a quantum network operating system -- QNodeOS -- and executing test programs including a delegated computation from a client to a server on two quantum network nodes based on nitrogen-vacancy (NV) centers in diamond. We show how our architecture allows us to maximize the use of quantum network hardware, by multitasking different applications on a quantum network for the first time. Our architecture can be used to execute programs on any quantum processor platform corresponding to our system model, which we illustrate by demonstrating an additional driver for QNodeOS for a trapped-ion quantum network node based on a single $^{40}\\text{Ca}^+$ atom. Our architecture lays the groundwork for computer science research in the domain of quantum network programming, and paves the way for the development of software that can bring quantum network technology to society.         ",
    "url": "https://arxiv.org/abs/2407.18306",
    "authors": [
      "Carlo Delle Donne",
      "Mariagrazia Iuliano",
      "Bart van der Vecht",
      "Guilherme Maciel Ferreira",
      "Hana Jirovsk\u00e1",
      "Thom van der Steenhoven",
      "Axel Dahlberg",
      "Matt Skrzypczyk",
      "Dario Fioretto",
      "Markus Teller",
      "Pavel Filippov",
      "Alejandro Rodr\u00edguez-Pardo Montblanch",
      "Julius Fischer",
      "Benjamin van Ommen",
      "Nicolas Demetriou",
      "Dominik Leichtle",
      "Luka Music",
      "Harold Ollivier",
      "Ingmar te Raa",
      "Wojciech Kozlowski",
      "Tim Taminiau",
      "Przemys\u0142aw Pawe\u0142czak",
      "Tracy Northup",
      "Ronald Hanson",
      "Stephanie Wehner"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Networking and Internet Architecture (cs.NI)",
      "Operating Systems (cs.OS)"
    ]
  },
  {
    "id": "arXiv:2407.18372",
    "title": "Thermodynamics-Consistent Graph Neural Networks",
    "abstract": "           We propose excess Gibbs free energy graph neural networks (GE-GNNs) for predicting composition-dependent activity coefficients of binary mixtures. The GE-GNN architecture ensures thermodynamic consistency by predicting the molar excess Gibbs free energy and using thermodynamic relations to obtain activity coefficients. As these are differential, automatic differentiation is applied to learn the activity coefficients in an end-to-end manner. Since the architecture is based on fundamental thermodynamics, we do not require additional loss terms to learn thermodynamic consistency. As the output is a fundamental property, we neither impose thermodynamic modeling limitations and assumptions. We demonstrate high accuracy and thermodynamic consistency of the activity coefficient predictions.         ",
    "url": "https://arxiv.org/abs/2407.18372",
    "authors": [
      "Jan G. Rittig",
      "Alexander Mitsos"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ]
  },
  {
    "id": "arXiv:2407.18648",
    "title": "Fast and Reliable Probabilistic Reflectometry Inversion with Prior-Amortized Neural Posterior Estimation",
    "abstract": "           Reconstructing the structure of thin films and multilayers from measurements of scattered X-rays or neutrons is key to progress in physics, chemistry, and biology. However, finding all structures compatible with reflectometry data is computationally prohibitive for standard algorithms, which typically results in unreliable analysis with only a single potential solution identified. We address this lack of reliability with a probabilistic deep learning method that identifies all realistic structures in seconds, setting new standards in reflectometry. Our method, Prior-Amortized Neural Posterior Estimation (PANPE), combines simulation-based inference with novel adaptive priors that inform the inference network about known structural properties and controllable experimental conditions. PANPE networks support key scenarios such as high-throughput sample characterization, real-time monitoring of evolving structures, or the co-refinement of several experimental data sets, and can be adapted to provide fast, reliable, and flexible inference across many other inverse problems.         ",
    "url": "https://arxiv.org/abs/2407.18648",
    "authors": [
      "Vladimir Starostin",
      "Maximilian Dax",
      "Alexander Gerlach",
      "Alexander Hinderhofer",
      "\u00c1lvaro Tejero-Cantero",
      "Frank Schreiber"
    ],
    "subjectives": [
      "Applied Physics (physics.app-ph)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.18732",
    "title": "A Physics-Informed Neural Network-Based Approach for the Spatial Upsampling of Spherical Microphone Arrays",
    "abstract": "           Spherical microphone arrays are convenient tools for capturing the spatial characteristics of a sound field. However, achieving superior spatial resolution requires arrays with numerous capsules, consequently leading to expensive devices. To address this issue, we present a method for spatially upsampling spherical microphone arrays with a limited number of capsules. Our approach exploits a physics-informed neural network with Rowdy activation functions, leveraging physical constraints to provide high-order microphone array signals, starting from low-order devices. Results show that, within its domain of application, our approach outperforms a state of the art method based on signal processing for spherical microphone arrays upsampling.         ",
    "url": "https://arxiv.org/abs/2407.18732",
    "authors": [
      "Federico Miotello",
      "Ferdinando Terminiello",
      "Mirco Pezzoli",
      "Alberto Bernardini",
      "Fabio Antonacci",
      "Augusto Sarti"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.18755",
    "title": "Score matching through the roof: linear, nonlinear, and latent variables causal discovery",
    "abstract": "           Causal discovery from observational data holds great promise, but existing methods rely on strong assumptions about the underlying causal structure, often requiring full observability of all relevant variables. We tackle these challenges by leveraging the score function $\\nabla \\log p(X)$ of observed variables for causal discovery and propose the following contributions. First, we generalize the existing results of identifiability with the score to additive noise models with minimal requirements on the causal mechanisms. Second, we establish conditions for inferring causal relations from the score even in the presence of hidden variables; this result is two-faced: we demonstrate the score's potential as an alternative to conditional independence tests to infer the equivalence class of causal graphs with hidden variables, and we provide the necessary conditions for identifying direct causes in latent variable models. Building on these insights, we propose a flexible algorithm for causal discovery across linear, nonlinear, and latent variable models, which we empirically validate.         ",
    "url": "https://arxiv.org/abs/2407.18755",
    "authors": [
      "Francesco Montagna",
      "Philipp M. Faller",
      "Patrick Bloebaum",
      "Elke Kirschbaum",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2407.18800",
    "title": "Towards Characterization of 5-List-Colorability of Toroidal Graphs",
    "abstract": "           Through computer-assisted enumeration, we list minimal obstructions for 5-choosability of graphs on the torus with the following additional property: There exists a cyclic system of non-contractible triangles around the torus where the consecutive triangles are at distance at most four. This condition is satisfied by all previously known obstructions, and we verify that there are no additional obstructions with this property. This supports the conjecture that a toroidal graph is 5-choosable if and only if it is 5-colorable.         ",
    "url": "https://arxiv.org/abs/2407.18800",
    "authors": [
      "Zden\u011bk Dvo\u0159\u00e1k",
      "F\u00e9lix Moreno Pe\u00f1arrubia"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2407.18802",
    "title": "Log-Concave Coupling for Sampling Neural Net Posteriors",
    "abstract": "           In this work, we present a sampling algorithm for single hidden layer neural networks. This algorithm is built upon a recursive series of Bayesian posteriors using a method we call Greedy Bayes. Sampling of the Bayesian posterior for neuron weight vectors $w$ of dimension $d$ is challenging because of its multimodality. Our algorithm to tackle this problem is based on a coupling of the posterior density for $w$ with an auxiliary random variable $\\xi$. The resulting reverse conditional $w|\\xi$ of neuron weights given auxiliary random variable is shown to be log concave. In the construction of the posterior distributions we provide some freedom in the choice of the prior. In particular, for Gaussian priors on $w$ with suitably small variance, the resulting marginal density of the auxiliary variable $\\xi$ is proven to be strictly log concave for all dimensions $d$. For a uniform prior on the unit $\\ell_1$ ball, evidence is given that the density of $\\xi$ is again strictly log concave for sufficiently large $d$. The score of the marginal density of the auxiliary random variable $\\xi$ is determined by an expectation over $w|\\xi$ and thus can be computed by various rapidly mixing Markov Chain Monte Carlo methods. Moreover, the computation of the score of $\\xi$ permits methods of sampling $\\xi$ by a stochastic diffusion (Langevin dynamics) with drift function built from this score. With such dynamics, information-theoretic methods pioneered by Bakry and Emery show that accurate sampling of $\\xi$ is obtained rapidly when its density is indeed strictly log-concave. After which, one more draw from $w|\\xi$, produces neuron weights $w$ whose marginal distribution is from the desired posterior.         ",
    "url": "https://arxiv.org/abs/2407.18802",
    "authors": [
      "Curtis McDonald",
      "Andrew R Barron"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18808",
    "title": "Learning Chaotic Systems and Long-Term Predictions with Neural Jump ODEs",
    "abstract": "           The Path-dependent Neural Jump ODE (PD-NJ-ODE) is a model for online prediction of generic (possibly non-Markovian) stochastic processes with irregular (in time) and potentially incomplete (with respect to coordinates) observations. It is a model for which convergence to the $L^2$-optimal predictor, which is given by the conditional expectation, is established theoretically. Thereby, the training of the model is solely based on a dataset of realizations of the underlying stochastic process, without the need of knowledge of the law of the process. In the case where the underlying process is deterministic, the conditional expectation coincides with the process itself. Therefore, this framework can equivalently be used to learn the dynamics of ODE or PDE systems solely from realizations of the dynamical system with different initial conditions. We showcase the potential of our method by applying it to the chaotic system of a double pendulum. When training the standard PD-NJ-ODE method, we see that the prediction starts to diverge from the true path after about half of the evaluation time. In this work we enhance the model with two novel ideas, which independently of each other improve the performance of our modelling setup. The resulting dynamics match the true dynamics of the chaotic system very closely. The same enhancements can be used to provably enable the PD-NJ-ODE to learn long-term predictions for general stochastic datasets, where the standard model fails. This is verified in several experiments.         ",
    "url": "https://arxiv.org/abs/2407.18808",
    "authors": [
      "Florian Krach",
      "Josef Teichmann"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2407.18811",
    "title": "Interpreting artificial neural networks to detect genome-wide association signals for complex traits",
    "abstract": "           Investigating the genetic architecture of complex diseases is challenging due to the highly polygenic and interactive landscape of genetic and environmental factors. Although genome-wide association studies (GWAS) have identified thousands of variants for multiple complex phenotypes, conventional statistical approaches can be limited by simplified assumptions such as linearity and lack of epistasis models. In this work, we trained artificial neural networks for predicting complex traits using both simulated and real genotype/phenotype datasets. We extracted feature importance scores via different post hoc interpretability methods to identify potentially associated loci (PAL) for the target phenotype. Simulations we performed with various parameters demonstrated that associated loci can be detected with good precision using strict selection criteria, but downstream analyses are required for fine-mapping the exact variants due to linkage disequilibrium, similarly to conventional GWAS. By applying our approach to the schizophrenia cohort in the Estonian Biobank, we were able to detect multiple PAL related to this highly polygenic and heritable disorder. We also performed enrichment analyses with PAL in genic regions, which predominantly identified terms associated with brain morphology. With further improvements in model optimization and confidence measures, artificial neural networks can enhance the identification of genomic loci associated with complex diseases, providing a more comprehensive approach for GWAS and serving as initial screening tools for subsequent functional studies. Keywords: Deep learning, interpretability, genome-wide association studies, complex diseases         ",
    "url": "https://arxiv.org/abs/2407.18811",
    "authors": [
      "Burak Yelmen",
      "Maris Alver",
      "Estonian Biobank Research Team",
      "Flora Jay",
      "Lili Milani"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2407.18837",
    "title": "Distributionally Robust Kalman Filtering over Finite and Infinite Horizon",
    "abstract": "           This paper investigates the distributionally robust filtering of signals generated by state-space models driven by exogenous disturbances with noisy observations in finite and infinite horizon scenarios. The exact joint probability distribution of the disturbances and noise is unknown but assumed to reside within a Wasserstein-2 ambiguity ball centered around a given nominal distribution. We aim to derive a causal estimator that minimizes the worst-case mean squared estimation error among all possible distributions within this ambiguity set. We remove the iid restriction in prior works by permitting arbitrarily time-correlated disturbances and noises. In the finite horizon setting, we reduce this problem to a semi-definite program (SDP), with computational complexity scaling with the time horizon. For infinite horizon settings, we characterize the optimal estimator using Karush-Kuhn-Tucker (KKT) conditions. Although the optimal estimator lacks a rational form, i.e., a finite-dimensional state-space realization, it can be fully described by a finite-dimensional parameter. {Leveraging this parametrization, we propose efficient algorithms that compute the optimal estimator with arbitrary fidelity in the frequency domain.} Moreover, given any finite degree, we provide an efficient convex optimization algorithm that finds the finite-dimensional state-space estimator that best approximates the optimal non-rational filter in ${\\cal H}_\\infty$ norm. This facilitates the practical implementation of the infinite horizon filter without having to grapple with the ill-scaled SDP from finite time. Finally, numerical simulations demonstrate the effectiveness of our approach in practical scenarios.         ",
    "url": "https://arxiv.org/abs/2407.18837",
    "authors": [
      "Taylan Kargin",
      "Joudi Hajar",
      "Vikrant Malik",
      "Babak Hassibi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2407.18909",
    "title": "Hybrid summary statistics: neural weak lensing inference beyond the power spectrum",
    "abstract": "           In inference problems, we often have domain knowledge which allows us to define summary statistics that capture most of the information content in a dataset. In this paper, we present a hybrid approach, where such physics-based summaries are augmented by a set of compressed neural summary statistics that are optimised to extract the extra information that is not captured by the predefined summaries. The resulting statistics are very powerful inputs to simulation-based or implicit inference of model parameters. We apply this generalisation of Information Maximising Neural Networks (IMNNs) to parameter constraints from tomographic weak gravitational lensing convergence maps to find summary statistics that are explicitly optimised to complement angular power spectrum estimates. We study several dark matter simulation resolutions in low- and high-noise regimes. We show that i) the information-update formalism extracts at least $3\\times$ and up to $8\\times$ as much information as the angular power spectrum in all noise regimes, ii) the network summaries are highly complementary to existing 2-point summaries, and iii) our formalism allows for networks with smaller, physically-informed architectures to match much larger regression networks with far fewer simulations needed to obtain asymptotically optimal inference.         ",
    "url": "https://arxiv.org/abs/2407.18909",
    "authors": [
      "T. Lucas Makinen",
      "Alan Heavens",
      "Natalia Porqueres",
      "Tom Charnock",
      "Axel Lapel",
      "Benjamin D. Wandelt"
    ],
    "subjectives": [
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)",
      "Machine Learning (stat.ML)",
      "Other Statistics (stat.OT)"
    ]
  },
  {
    "id": "arXiv:2208.05870",
    "title": "Constrained and unconstrained stable discrete minimizations for p-robust local reconstructions in vertex patches in the de Rham complex",
    "abstract": "           We analyze constrained and unconstrained minimization problems on patches of tetrahedra sharing a common vertex with discontinuous piecewise polynomial data of degree p. We show that the discrete minimizers in the spaces of piecewise polynomials of degree p conforming in the H1, H(curl), or H(div) spaces are as good as the minimizers in these entire (infinite-dimensional) Sobolev spaces, up to a constant that is independent of p. These results are useful in the analysis and design of finite element methods, namely for devising stable local commuting projectors and establishing local-best/global-best equivalences in a priori analysis and in the context of a posteriori error estimation. Unconstrained minimization in H1 and constrained minimization in H(div) have been previously treated in the literature. Along with improvement of the results in the H1 and H(div) cases, our key contribution is the treatment of the H(curl) framework. This enables us to cover the whole De Rham diagram in three space dimensions in a single setting.         ",
    "url": "https://arxiv.org/abs/2208.05870",
    "authors": [
      "T. Chaumont-Frelet",
      "M. Vohralik"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ]
  },
  {
    "id": "arXiv:2211.06841",
    "title": "Point-DAE: Denoising Autoencoders for Self-supervised Point Cloud Learning",
    "abstract": "           Masked autoencoder has demonstrated its effectiveness in self-supervised point cloud learning. Considering that masking is a kind of corruption, in this work we explore a more general denoising autoencoder for point cloud learning (Point-DAE) by investigating more types of corruptions beyond masking. Specifically, we degrade the point cloud with certain corruptions as input, and learn an encoder-decoder model to reconstruct the original point cloud from its corrupted version. Three corruption families (\\ie, density/masking, noise, and affine transformation) and a total of fourteen corruption types are investigated with traditional non-Transformer encoders. Besides the popular masking corruption, we identify another effective corruption family, \\ie, affine transformation. The affine transformation disturbs all points globally, which is complementary to the masking corruption where some local regions are dropped. We also validate the effectiveness of affine transformation corruption with the Transformer backbones, where we decompose the reconstruction of the complete point cloud into the reconstructions of detailed local patches and rough global shape, alleviating the position leakage problem in the reconstruction. Extensive experiments on tasks of object classification, few-shot learning, robustness testing, part segmentation, and 3D object detection validate the effectiveness of the proposed method. The codes are available at \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2211.06841",
    "authors": [
      "Yabin Zhang",
      "Jiehong Lin",
      "Ruihuang Li",
      "Kui Jia",
      "Lei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2301.12783",
    "title": "The Leafed Induced Subtree in chordal and bounded treewidth graphs",
    "abstract": "           In the Fully Leafed Induced Subtrees, one is given a graph $G$ and two integers $a$ and $b$ and the question is to find an induced subtree of $G$ with $a$ vertices and at least $b$ leaves. This problem is known to be NP-complete even when the input graph is $4$-regular. Polynomial algorithms are known when the input graph is restricted to be a tree or series-parallel. In this paper we generalize these results by providing an FPT algorithm parameterized by treewidth. We also provide a polynomial algorithm when the input graph is restricted to be a chordal graph.         ",
    "url": "https://arxiv.org/abs/2301.12783",
    "authors": [
      "Julien Baste"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2309.03731",
    "title": "Using representation balancing to learn conditional-average dose responses from clustered data",
    "abstract": "           Estimating a unit's responses to interventions with an associated dose, the \"conditional average dose response\" (CADR), is relevant in a variety of domains, from healthcare to business, economics, and beyond. Such a response typically needs to be estimated from observational data, which introduces several challenges. That is why the machine learning (ML) community has proposed several tailored CADR estimators. Yet, the proposal of most of these methods requires strong assumptions on the distribution of data and the assignment of interventions, which go beyond the standard assumptions in causal inference. Whereas previous works have so far focused on smooth shifts in covariate distributions across doses, in this work, we will study estimating CADR from clustered data and where different doses are assigned to different segments of a population. On a novel benchmarking dataset, we show the impacts of clustered data on model performance and propose an estimator, CBRNet, that learns cluster-agnostic and hence dose-agnostic covariate representations through representation balancing for unbiased CADR inference. We run extensive experiments to illustrate the workings of our method and compare it with the state of the art in ML for CADR estimation.         ",
    "url": "https://arxiv.org/abs/2309.03731",
    "authors": [
      "Christopher Bockel-Rickermann",
      "Toon Vanderschueren",
      "Jeroen Berrevoets",
      "Tim Verdonck",
      "Wouter Verbeke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2310.15469",
    "title": "The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks",
    "abstract": "           The rapid advancements of large language models (LLMs) have raised public concerns about the privacy leakage of personally identifiable information (PII) within their extensive training datasets. Recent studies have demonstrated that an adversary could extract highly sensitive privacy data from the training data of LLMs with carefully designed prompts. However, these attacks suffer from the model's tendency to hallucinate and catastrophic forgetting (CF) in the pre-training stage, rendering the veracity of divulged PIIs negligible. In our research, we propose a novel attack, Janus, which exploits the fine-tuning interface to recover forgotten PIIs from the pre-training data in LLMs. We formalize the privacy leakage problem in LLMs and explain why forgotten PIIs can be recovered through empirical analysis on open-source language models. Based upon these insights, we evaluate the performance of Janus on both open-source language models and two latest LLMs, i.e., GPT-3.5-Turbo and LLaMA-2-7b. Our experiment results show that Janus amplifies the privacy risks by over 10 times in comparison with the baseline and significantly outperforms the state-of-the-art privacy extraction attacks including prefix attacks and in-context learning (ICL). Furthermore, our analysis validates that existing fine-tuning APIs provided by OpenAI and Azure AI Studio are susceptible to our Janus attack, allowing an adversary to conduct such an attack at a low cost.         ",
    "url": "https://arxiv.org/abs/2310.15469",
    "authors": [
      "Xiaoyi Chen",
      "Siyuan Tang",
      "Rui Zhu",
      "Shijun Yan",
      "Lei Jin",
      "Zihao Wang",
      "Liya Su",
      "Zhikun Zhang",
      "XiaoFeng Wang",
      "Haixu Tang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2311.12715",
    "title": "Attacks on fairness in Federated Learning",
    "abstract": "           Federated Learning is an important emerging distributed training paradigm that keeps data private on clients. It is now well understood that by controlling only a small subset of FL clients, it is possible to introduce a backdoor to a federated learning model, in the presence of certain attributes. In this paper, we present a new type of attack that compromises the fairness of the trained model. Fairness is understood to be the attribute-level performance distribution of a trained model. It is particularly salient in domains where, for example, skewed accuracy discrimination between subpopulations could have disastrous consequences. We find that by employing a threat model similar to that of a backdoor attack, an attacker is able to influence the aggregated model to have an unfair performance distribution between any given set of attributes. Furthermore, we find that this attack is possible by controlling only a single client. While combating naturally induced unfairness in FL has previously been discussed in depth, its artificially induced kind has been neglected. We show that defending against attacks on fairness should be a critical consideration in any situation where unfairness in a trained model could benefit a user who participated in its training.         ",
    "url": "https://arxiv.org/abs/2311.12715",
    "authors": [
      "Joseph Rance",
      "Filip Svoboda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2311.12902",
    "title": "Enhancing Solutions for Complex PDEs: Introducing Complementary Convolution and Equivariant Attention in Fourier Neural Operators",
    "abstract": "           Neural operators improve conventional neural networks by expanding their capabilities of functional mappings between different function spaces to solve partial differential equations (PDEs). One of the most notable methods is the Fourier Neural Operator (FNO), which draws inspiration from Green's function method and directly approximates operator kernels in the frequency domain. However, after empirical observation followed by theoretical validation, we demonstrate that the FNO approximates kernels primarily in a relatively low-frequency domain. This suggests a limited capability in solving complex PDEs, particularly those characterized by rapid coefficient changes and oscillations in the solution space. Such cases are crucial in specific scenarios, like atmospheric convection and ocean circulation. To address this challenge, inspired by the translation equivariant of the convolution kernel, we propose a novel hierarchical Fourier neural operator along with convolution-residual layers and attention mechanisms to make them complementary in the frequency domain to solve complex PDEs. We perform experiments on forward and reverse problems of multiscale elliptic equations, Navier-Stokes equations, and other physical scenarios, and find that the proposed method achieves superior performance in these PDE benchmarks, especially for equations characterized by rapid coefficient variations.         ",
    "url": "https://arxiv.org/abs/2311.12902",
    "authors": [
      "Xuanle Zhao",
      "Yue Sun",
      "Tielin Zhang",
      "Bo Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2312.09554",
    "title": "Embodied Laser Attack:Leveraging Scene Priors to Achieve Agent-based Robust Non-contact Attacks",
    "abstract": "           As physical adversarial attacks become extensively applied in unearthing the potential risk of security-critical scenarios, especially in dynamic scenarios, their vulnerability to environmental variations has also been brought to light. The non-robust nature of physical adversarial attack methods brings less-than-stable performance consequently. Although methods such as EOT have enhanced the robustness of traditional contact attacks like adversarial patches, they fall short in practicality and concealment within dynamic environments such as traffic scenarios. Meanwhile, non-contact laser attacks, while offering enhanced adaptability, face constraints due to a limited optimization space for their attributes, rendering EOT less effective. This limitation underscores the necessity for developing a new strategy to augment the robustness of such practices. To address these issues, this paper introduces the Embodied Laser Attack (ELA), a novel framework that leverages the embodied intelligence paradigm of Perception-Decision-Control to dynamically tailor non-contact laser attacks. For the perception module, given the challenge of simulating the victim's view by full-image transformation, ELA has innovatively developed a local perspective transformation network, based on the intrinsic prior knowledge of traffic scenes and enables effective and efficient estimation. For the decision and control module, ELA trains an attack agent with data-driven reinforcement learning instead of adopting time-consuming heuristic algorithms, making it capable of instantaneously determining a valid attack strategy with the perceived information by well-designed rewards, which is then conducted by a controllable laser emitter. Experimentally, we apply our framework to diverse traffic scenarios both in the digital and physical world, verifying the effectiveness of our method under dynamic successive scenes.         ",
    "url": "https://arxiv.org/abs/2312.09554",
    "authors": [
      "Yitong Sun",
      "Yao Huang",
      "Xingxing Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.11587",
    "title": "Relightable Neural Actor with Intrinsic Decomposition and Pose Control",
    "abstract": "           Creating a controllable and relightable digital avatar from multi-view video with fixed illumination is a very challenging problem since humans are highly articulated, creating pose-dependent appearance effects, and skin as well as clothing require space-varying BRDF modeling. Existing works on creating animatible avatars either do not focus on relighting at all, require controlled illumination setups, or try to recover a relightable avatar from very low cost setups, i.e. a single RGB video, at the cost of severely limited result quality, e.g. shadows not even being modeled. To address this, we propose Relightable Neural Actor, a new video-based method for learning a pose-driven neural human model that can be relighted, allows appearance editing, and models pose-dependent effects such as wrinkles and self-shadows. Importantly, for training, our method solely requires a multi-view recording of the human under a known, but static lighting condition. To tackle this challenging problem, we leverage an implicit geometry representation of the actor with a drivable density field that models pose-dependent deformations and derive a dynamic mapping between 3D and UV spaces, where normal, visibility, and materials are effectively encoded. To evaluate our approach in real-world scenarios, we collect a new dataset with four identities recorded under different light conditions, indoors and outdoors, providing the first benchmark of its kind for human relighting, and demonstrating state-of-the-art relighting results for novel human poses.         ",
    "url": "https://arxiv.org/abs/2312.11587",
    "authors": [
      "Diogo Luvizon",
      "Vladislav Golyanik",
      "Adam Kortylewski",
      "Marc Habermann",
      "Christian Theobalt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2401.09786",
    "title": "Adaptive Self-training Framework for Fine-grained Scene Graph Generation",
    "abstract": "           Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is beneficial when adopting our proposed self-training framework to the state-of-the-art message-passing neural network (MPNN)-based SGG models. Our extensive experiments verify the effectiveness of ST-SGG on various SGG models, particularly in enhancing the performance on fine-grained predicate classes.         ",
    "url": "https://arxiv.org/abs/2401.09786",
    "authors": [
      "Kibum Kim",
      "Kanghoon Yoon",
      "Yeonjun In",
      "Jinyoung Moon",
      "Donghyun Kim",
      "Chanyoung Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.13882",
    "title": "Robust Transmission Design for RIS-Assisted Integrated Sensing and Communication Systems",
    "abstract": "           As a critical technology for next-generation communication networks, integrated sensing and communication (ISAC) aims to achieve the harmonious coexistence of communication and sensing. The degrees-of-freedom (DoF) of ISAC is limited due to multiple performance metrics used for communication and sensing. Reconfigurable Intelligent Surfaces (RIS) composed of metamaterials can enhance the DoF in the spatial domain of ISAC systems. However, the availability of perfect Channel State Information (CSI) is a prerequisite for the gain brought by RIS, which is not realistic in practical environments. Therefore, under the imperfect CSI condition, we propose a decomposition-based large deviation inequality approach to eliminate the impact of CSI error on communication rate and sensing Cram\u00e9r-Rao bound (CRB). Then, an alternating optimization (AO) algorithm based on semi-definite relaxation (SDR) and gradient extrapolated majorization-maximization (GEMM) is proposed to solve the transmit beamforming and discrete RIS beamforming problems. We also analyze the complexity and convergence of the proposed algorithm. Simulation results show that the proposed algorithms can effectively eliminate the influence of CSI error and have good convergence performance. Notably, when CSI error exists, the gain brought by RIS will decrease with the increase of the number of RIS elements. Finally, we summarize and outline future research directions.         ",
    "url": "https://arxiv.org/abs/2401.13882",
    "authors": [
      "Yongqing Xu",
      "Yong Li",
      "Tony Q. S. Quek"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2402.04417",
    "title": "Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit",
    "abstract": "           We study a robust, i.e. in presence of malicious participants, multi-agent multi-armed bandit problem where multiple participants are distributed on a fully decentralized blockchain, with the possibility of some being malicious. The rewards of arms are homogeneous among the honest participants, following time-invariant stochastic distributions, which are revealed to the participants only when certain conditions are met to ensure that the coordination mechanism is secure enough. The coordination mechanism's objective is to efficiently ensure the cumulative rewards gained by the honest participants are maximized. To this end, we are the first to incorporate advanced techniques from blockchains, as well as novel mechanisms, into such a cooperative decision making framework to design optimal strategies for honest participants. This framework allows various malicious behaviors and the maintenance of security and participant privacy. More specifically, we select a pool of validators who communicate to all participants, design a new consensus mechanism based on digital signatures for these validators, invent a UCB-based strategy that requires less information from participants through secure multi-party computation, and design the chain-participant interaction and an incentive mechanism to encourage participants' participation. Notably, we are the first to prove the theoretical regret of the proposed algorithm and claim its optimality. Unlike existing work that integrates blockchains with learning problems such as federated learning which mainly focuses on optimality via computational experiments, we demonstrate that the regret of honest participants is upper bounded by $\\log{T}$ under certain assumptions. The regret bound is consistent with the multi-agent multi-armed bandit problem, both without malicious participants and with purely Byzantine attacks which do not affect the entire system.         ",
    "url": "https://arxiv.org/abs/2402.04417",
    "authors": [
      "Mengfan Xu",
      "Diego Klabjan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2402.06357",
    "title": "The SkipSponge Attack: Sponge Weight Poisoning of Deep Neural Networks",
    "abstract": "           Sponge attacks aim to increase the energy consumption and computation time of neural networks deployed on hardware accelerators. Existing sponge attacks can be performed during inference via sponge examples or during training via Sponge Poisoning. Sponge examples leverage perturbations added to the model's input to increase energy and latency, while Sponge Poisoning alters the objective function of a model to induce inference-time energy effects. In this work, we propose a novel sponge attack called SkipSponge. SkipSponge is the first sponge attack that is performed directly on the parameters of a pre-trained model using only a few data samples. Our experiments show that SkipSponge can successfully increase the energy consumption of image classification models, GANs, and autoencoders with fewer samples required than Sponge Poisoning. We show that poisoning defenses are ineffective if not adjusted specifically for the defense against SkipSponge (i.e., they decrease target layer bias values). Our work shows that SkipSponge is more effective on the GANs and the autoencoders than the state-of-the-art. Additionally, SkipSponge is stealthier than the previous Sponge Poisoning attack as it does not require significant changes in the victim model's weights. Our experiments indicate that the SkipSponge attack can be performed even when an attacker has access to only 1% of the entire dataset and reaches up to 13% energy increase.         ",
    "url": "https://arxiv.org/abs/2402.06357",
    "authors": [
      "Jona te Lintelo",
      "Stefanos Koffas",
      "Stjepan Picek"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.08983",
    "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
    "abstract": "           As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their token probabilities, while simultaneously attenuating the probabilities of token sequences that are aligned with the objectives of jailbreak attacks. We perform extensive experiments on five LLMs using six state-of-the-art jailbreak attacks and four benchmark datasets. Our results show that SafeDecoding significantly reduces the attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries. SafeDecoding outperforms six defense methods.         ",
    "url": "https://arxiv.org/abs/2402.08983",
    "authors": [
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Luyao Niu",
      "Jinyuan Jia",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.15374",
    "title": "Outlier detection by ensembling uncertainty with negative objectness",
    "abstract": "           Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn negative objectness at pasted negative instances. Our models outperform the current state-of-the art on standard benchmarks for image-wide and pixel-level outlier detection with and without training on real negative data.         ",
    "url": "https://arxiv.org/abs/2402.15374",
    "authors": [
      "Anja Deli\u0107",
      "Matej Grci\u0107",
      "Sini\u0161a \u0160egvi\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.17363",
    "title": "CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks",
    "abstract": "           Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT). Generative models are often used to address the issue of imbalanced node categories in dynamic graphs. Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes. This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information. Labels are transformed into embedding vectors, serving as conditional constraints to control the generation of synthetic data across multiple categories. Using a multi-stage loss, the distribution of synthetic data is adjusted to closely resemble that of real data. In extensive experiments, we show that CGGM's synthetic data outperforms state-of-the-art methods across various metrics. Our results demonstrate efficient generation of diverse data categories, robustly enhancing multi-category classification model performance.         ",
    "url": "https://arxiv.org/abs/2402.17363",
    "authors": [
      "Xianshi Su",
      "Munan Li",
      "Tongbang Jiang",
      "Hao Long"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.00783",
    "title": "On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs",
    "abstract": "           Plan synthesis aims to generate a course of actions or policies to transit given initial states to goal states, provided domain models that could be designed by experts or learnt from training data or interactions with the world. Intrigued by the claims of emergent planning capabilities in large language models (LLMs), works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs. In this paper, we aim to further study the insight of the planning capability of LLMs by investigating the roles of LLMs in off-the-shelf planning frameworks. To do this, we investigate the effectiveness of embedding LLMs into one of the well-known planning frameworks, graph-based planning, proposing a novel LLMs-based planning framework with LLMs embedded in two levels of planning graphs, i.e., mutual constraints generation level and constraints solving level. We empirically exhibit the effectiveness of our proposed framework in various planning domains.         ",
    "url": "https://arxiv.org/abs/2403.00783",
    "authors": [
      "Hankz Hankui Zhuo",
      "Xin Chen",
      "Rong Pan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2403.04202",
    "title": "Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents",
    "abstract": "           Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents: a promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents; however, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., focused on maximizing outcomes over time), norm-based (i.e., conforming to specific norms), or virtue-based (i.e., considering a combination of different virtues). The extent to which agents' co-development may be impacted by such moral heterogeneity in populations is not well understood. In this paper, we present a study of the learning dynamics of morally heterogeneous populations interacting in a social dilemma setting. Using an Iterated Prisoner's Dilemma environment with a partner selection mechanism, we investigate the extent to which the prevalence of diverse moral agents in populations affects individual agents' learning behaviors and emergent population-level outcomes. We observe several types of non-trivial interactions between pro-social and anti-social agents, and find that certain types of moral agents are able to steer selfish agents towards more cooperative behavior.         ",
    "url": "https://arxiv.org/abs/2403.04202",
    "authors": [
      "Elizaveta Tennant",
      "Stephen Hailes",
      "Mirco Musolesi"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.09793",
    "title": "Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning",
    "abstract": "           Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot's navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot's exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot's social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, which states that social acting is oriented toward the acting of others. The DRL policy is trained in an environment where other agents interact socially integrated and reward the robot's behavior individually. The simulation results indicate that the proposed socially integrated navigation approach outperforms a socially aware approach in terms of ego navigation performance while significantly reducing the negative impact on all agents within the environment.         ",
    "url": "https://arxiv.org/abs/2403.09793",
    "authors": [
      "Daniel Fl\u00f6gel",
      "Lars Fischer",
      "Thomas Rudolf",
      "Tobias Sch\u00fcrmann",
      "S\u00f6ren Hohmann"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2403.10519",
    "title": "Frozen Feature Augmentation for Few-Shot Image Classification",
    "abstract": "           Training a linear classifier or lightweight model on top of pretrained vision model outputs, so-called 'frozen features', leads to impressive performance on a number of downstream few-shot tasks. Currently, frozen features are not modified during training. On the other hand, when networks are trained directly on images, data augmentation is a standard recipe that improves performance with no substantial overhead. In this paper, we conduct an extensive pilot study on few-shot image classification that explores applying data augmentations in the frozen feature space, dubbed 'frozen feature augmentation (FroFA)', covering twenty augmentations in total. Our study demonstrates that adopting a deceptively simple pointwise FroFA, such as brightness, can improve few-shot performance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets.         ",
    "url": "https://arxiv.org/abs/2403.10519",
    "authors": [
      "Andreas B\u00e4r",
      "Neil Houlsby",
      "Mostafa Dehghani",
      "Manoj Kumar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2404.06939",
    "title": "Fast System Technology Co-Optimization Framework for Emerging Technology Based on Graph Neural Networks",
    "abstract": "           This paper proposes a fast system technology co-optimization (STCO) framework that optimizes power, performance, and area (PPA) for next-generation IC design, addressing the challenges and opportunities presented by novel materials and device architectures. We focus on accelerating the technology level of STCO using AI techniques, by employing graph neural network (GNN)-based approaches for both TCAD simulation and cell library characterization, which are interconnected through a unified compact model, collectively achieving over a 100X speedup over traditional methods. These advancements enable comprehensive STCO iterations with runtime speedups ranging from 1.9X to 14.1X and supports both emerging and traditional technologies.         ",
    "url": "https://arxiv.org/abs/2404.06939",
    "authors": [
      "Tianliang Ma",
      "Guangxi Fan",
      "Xuguang Sun",
      "Zhihui Deng",
      "Kainlu Low",
      "Leilai Shao"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.13671",
    "title": "FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and High-Quality Localization",
    "abstract": "           Zero-shot anomaly detection (ZSAD) methods entail detecting anomalies directly without access to any known normal or abnormal samples within the target item categories. Existing approaches typically rely on the robust generalization capabilities of multimodal pretrained models, computing similarities between manually crafted textual features representing \"normal\" or \"abnormal\" semantics and image features to detect anomalies and localize anomalous patches. However, the generic descriptions of \"abnormal\" often fail to precisely match diverse types of anomalies across different object categories. Additionally, computing feature similarities for single patches struggles to pinpoint specific locations of anomalies with various sizes and scales. To address these issues, we propose a novel ZSAD method called FiLo, comprising two components: adaptively learned Fine-Grained Description (FG-Des) and position-enhanced High-Quality Localization (HQ-Loc). FG-Des introduces fine-grained anomaly descriptions for each category using Large Language Models (LLMs) and employs adaptively learned textual templates to enhance the accuracy and interpretability of anomaly detection. HQ-Loc, utilizing Grounding DINO for preliminary localization, position-enhanced text prompts, and Multi-scale Multi-shape Cross-modal Interaction (MMCI) module, facilitates more accurate localization of anomalies of different sizes and shapes. Experimental results on datasets like MVTec and VisA demonstrate that FiLo significantly improves the performance of ZSAD in both detection and localization, achieving state-of-the-art performance with an image-level AUC of 83.9% and a pixel-level AUC of 95.9% on the VisA dataset. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2404.13671",
    "authors": [
      "Zhaopeng Gu",
      "Bingke Zhu",
      "Guibo Zhu",
      "Yingying Chen",
      "Hao Li",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.05526",
    "title": "Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview",
    "abstract": "           Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D scene representation, offering high-fidelity renderings and reconstructions from a set of sparse and unstructured sensor data. In the context of autonomous robotics, where perception and understanding of the environment are pivotal, NeRF holds immense promise for improving performance. In this paper, we present a comprehensive survey and analysis of the state-of-the-art techniques for utilizing NeRF to enhance the capabilities of autonomous robots. We especially focus on the perception, localization and navigation, and decision-making modules of autonomous robots and delve into tasks crucial for autonomous operation, including 3D reconstruction, segmentation, pose estimation, simultaneous localization and mapping (SLAM), navigation and planning, and interaction. Our survey meticulously benchmarks existing NeRF-based methods, providing insights into their strengths and limitations. Moreover, we explore promising avenues for future research and development in this domain. Notably, we discuss the integration of advanced techniques such as 3D Gaussian splatting (3DGS), large language models (LLM), and generative AIs, envisioning enhanced reconstruction efficiency, scene understanding, decision-making capabilities. This survey serves as a roadmap for researchers seeking to leverage NeRFs to empower autonomous robots, paving the way for innovative solutions that can navigate and interact seamlessly in complex environments.         ",
    "url": "https://arxiv.org/abs/2405.05526",
    "authors": [
      "Yuhang Ming",
      "Xingrui Yang",
      "Weihan Wang",
      "Zheng Chen",
      "Jinglun Feng",
      "Yifan Xing",
      "Guofeng Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2405.20584",
    "title": "Disrupting Diffusion: Token-Level Attention Erasure Attack against Diffusion-based Customization",
    "abstract": "           With the development of diffusion-based customization methods like DreamBooth, individuals now have access to train the models that can generate their personalized images. Despite the convenience, malicious users have misused these techniques to create fake images, thereby triggering a privacy security crisis. In light of this, proactive adversarial attacks are proposed to protect users against customization. The adversarial examples are trained to distort the customization model's outputs and thus block the misuse. In this paper, we propose DisDiff (Disrupting Diffusion), a novel adversarial attack method to disrupt the diffusion model outputs. We first delve into the intrinsic image-text relationships, well-known as cross-attention, and empirically find that the subject-identifier token plays an important role in guiding image generation. Thus, we propose the Cross-Attention Erasure module to explicitly \"erase\" the indicated attention maps and disrupt the text guidance. Besides,we analyze the influence of the sampling process of the diffusion model on Projected Gradient Descent (PGD) attack and introduce a novel Merit Sampling Scheduler to adaptively modulate the perturbation updating amplitude in a step-aware manner. Our DisDiff outperforms the state-of-the-art methods by 12.75% of FDFR scores and 7.25% of ISM scores across two facial benchmarks and two commonly used prompts on average.         ",
    "url": "https://arxiv.org/abs/2405.20584",
    "authors": [
      "Yisu Liu",
      "Jinyang An",
      "Wanqian Zhang",
      "Dayan Wu",
      "Jingzi Gu",
      "Zheng Lin",
      "Weiping Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.01253",
    "title": "animal2vec and MeerKAT: A self-supervised transformer for rare-event raw audio input and a large-scale reference dataset for bioacoustics",
    "abstract": "           Bioacoustic research, vital for understanding animal behavior, conservation, and ecology, faces a monumental challenge: analyzing vast datasets where animal vocalizations are rare. While deep learning techniques are becoming standard, adapting them to bioacoustics remains difficult. We address this with animal2vec, an interpretable large transformer model, and a self-supervised training scheme tailored for sparse and unbalanced bioacoustic data. It learns from unlabeled audio and then refines its understanding with labeled data. Furthermore, we introduce and publicly release MeerKAT: Meerkat Kalahari Audio Transcripts, a dataset of meerkat (Suricata suricatta) vocalizations with millisecond-resolution annotations, the largest labeled dataset on non-human terrestrial mammals currently available. Our model outperforms existing methods on MeerKAT and the publicly available NIPS4Bplus birdsong dataset. Moreover, animal2vec performs well even with limited labeled data (few-shot learning). animal2vec and MeerKAT provide a new reference point for bioacoustic research, enabling scientists to analyze large amounts of data even with scarce ground truth information.         ",
    "url": "https://arxiv.org/abs/2406.01253",
    "authors": [
      "Julian C. Sch\u00e4fer-Zimmermann",
      "Vlad Demartsev",
      "Baptiste Averly",
      "Kiran Dhanjal-Adams",
      "Mathieu Duteil",
      "Gabriella Gall",
      "Marius Fai\u00df",
      "Lily Johnson-Ulrich",
      "Dan Stowell",
      "Marta B. Manser",
      "Marie A. Roch",
      "Ariana Strandburg-Peshkin"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Quantitative Methods (q-bio.QM)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2406.03733",
    "title": "Credit Card Fraud Detection Using Advanced Transformer Model",
    "abstract": "           With the proliferation of various online and mobile payment systems, credit card fraud has emerged as a significant threat to financial security. This study focuses on innovative applications of the latest Transformer models for more robust and precise fraud detection. To ensure the reliability of the data, we meticulously processed the data sources, balancing the dataset to address the issue of data sparsity significantly. We also selected highly correlated vectors to strengthen the training this http URL guarantee the reliability and practicality of the new Transformer model, we conducted performance comparisons with several widely adopted models, including Support Vector Machine (SVM), Random Forest, Neural Network, and Logistic Regression. We rigorously compared these models using metrics such as Precision, Recall, and F1 Score. Through these detailed analyses and comparisons, we present to the readers a highly efficient and powerful anti-fraud mechanism with promising prospects. The results demonstrate that the Transformer model not only excels in traditional applications but also shows great potential in niche areas like fraud detection, offering a substantial advancement in the field.         ",
    "url": "https://arxiv.org/abs/2406.03733",
    "authors": [
      "Chang Yu",
      "Yongshun Xu",
      "Jin Cao",
      "Ye Zhang",
      "Yinxin Jin",
      "Mengran Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.04727",
    "title": "MMPolymer: A Multimodal Multitask Pretraining Framework for Polymer Property Prediction",
    "abstract": "           Polymers are high-molecular-weight compounds constructed by the covalent bonding of numerous identical or similar monomers so that their 3D structures are complex yet exhibit unignorable regularity. Typically, the properties of a polymer, such as plasticity, conductivity, bio-compatibility, and so on, are highly correlated with its 3D structure. However, existing polymer property prediction methods heavily rely on the information learned from polymer SMILES sequences (P-SMILES strings) while ignoring crucial 3D structural information, resulting in sub-optimal performance. In this work, we propose MMPolymer, a novel multimodal multitask pretraining framework incorporating polymer 1D sequential and 3D structural information to encourage downstream polymer property prediction tasks. Besides, considering the scarcity of polymer 3D data, we further introduce the \"Star Substitution\" strategy to extract 3D structural information effectively. During pretraining, in addition to predicting masked tokens and recovering clear 3D coordinates, MMPolymer achieves the cross-modal alignment of latent representations. Then we further fine-tune the pretrained MMPolymer for downstream polymer property prediction tasks in the supervised learning paradigm. Experiments show that MMPolymer achieves state-of-the-art performance in downstream property prediction tasks. Moreover, given the pretrained MMPolymer, utilizing merely a single modality in the fine-tuning phase can also outperform existing methods, showcasing the exceptional capability of MMPolymer in polymer feature extraction and utilization.         ",
    "url": "https://arxiv.org/abs/2406.04727",
    "authors": [
      "Fanmeng Wang",
      "Wentao Guo",
      "Minjie Cheng",
      "Shen Yuan",
      "Hongteng Xu",
      "Zhifeng Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.19136",
    "title": "YZS-model: A Predictive Model for Organic Drug Solubility Based on Graph Convolutional Networks and Transformer-Attention",
    "abstract": "           Accurate prediction of drug molecule solubility is crucial for therapeutic effectiveness and safety. Traditional methods often miss complex molecular structures, leading to inaccuracies. We introduce the YZS-Model, a deep learning framework integrating Graph Convolutional Networks (GCN), Transformer architectures, and Long Short-Term Memory (LSTM) networks to enhance prediction precision. GCNs excel at capturing intricate molecular topologies by modeling the relationships between atoms and bonds. Transformers, with their self-attention mechanisms, effectively identify long-range dependencies within molecules, capturing global interactions. LSTMs process sequential data, preserving long-term dependencies and integrating temporal information within molecular sequences. This multifaceted approach leverages the strengths of each component, resulting in a model that comprehensively understands and predicts molecular properties. Trained on 9,943 compounds and tested on an anticancer dataset, the YZS-Model achieved an $R^2$ of 0.59 and an RMSE of 0.57, outperforming benchmark models ($R^2$ of 0.52 and RMSE of 0.61). In an independent test, it demonstrated an RMSE of 1.05, improving accuracy by 45.9%. The integration of these deep learning techniques allows the YZS-Model to learn valuable features from complex data without predefined parameters, handle large datasets efficiently, and adapt to various molecular types. This comprehensive capability significantly improves predictive accuracy and model generalizability. Its precision in solubility predictions can expedite drug development by optimizing candidate selection, reducing costs, and enhancing efficiency. Our research underscores deep learning's transformative potential in pharmaceutical science, particularly for solubility prediction and drug design.         ",
    "url": "https://arxiv.org/abs/2406.19136",
    "authors": [
      "Chenxu Wang",
      "Haowei Ming",
      "Jian He",
      "Yao Lu",
      "Junhong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.02394",
    "title": "Similarity Distance-Based Label Assignment for Tiny Object Detection",
    "abstract": "           Tiny object detection is becoming one of the most challenging tasks in computer vision because of the limited object size and lack of information. The label assignment strategy is a key factor affecting the accuracy of object detection. Although there are some effective label assignment strategies for tiny objects, most of them focus on reducing the sensitivity to the bounding boxes to increase the number of positive samples and have some fixed hyperparameters need to set. However, more positive samples may not necessarily lead to better detection results, in fact, excessive positive samples may lead to more false positives. In this paper, we introduce a simple but effective strategy named the Similarity Distance (SimD) to evaluate the similarity between bounding boxes. This proposed strategy not only considers both location and shape similarity but also learns hyperparameters adaptively, ensuring that it can adapt to different datasets and various object sizes in a dataset. Our approach can be simply applied in common anchor-based detectors in place of the IoU for label assignment and Non Maximum Suppression (NMS). Extensive experiments on four mainstream tiny object detection datasets demonstrate superior performance of our method, especially, 1.8 AP points and 4.1 AP points of very tiny higher than the state-of-the-art competitors on AI-TOD. Code is available at: \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2407.02394",
    "authors": [
      "Shuohao Shi",
      "Qiang Fang",
      "Tong Zhao",
      "Xin Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.06174",
    "title": "The Tug-of-War Between Deepfake Generation and Detection",
    "abstract": "           Multimodal generative models are rapidly evolving, leading to a surge in the generation of realistic video and audio that offers exciting possibilities but also serious risks. Deepfake videos, which can convincingly impersonate individuals, have particularly garnered attention due to their potential misuse in spreading misinformation and creating fraudulent content. This survey paper examines the dual landscape of deepfake video generation and detection, emphasizing the need for effective countermeasures against potential abuses. We provide a comprehensive overview of current deepfake generation techniques, including face swapping, reenactment, and audio-driven animation, which leverage cutting-edge technologies like GANs and diffusion models to produce highly realistic fake videos. Additionally, we analyze various detection approaches designed to differentiate authentic from altered videos, from detecting visual artifacts to deploying advanced algorithms that pinpoint inconsistencies across video and audio signals. The effectiveness of these detection methods heavily relies on the diversity and quality of datasets used for training and evaluation. We discuss the evolution of deepfake datasets, highlighting the importance of robust, diverse, and frequently updated collections to enhance the detection accuracy and generalizability. As deepfakes become increasingly indistinguishable from authentic content, developing advanced detection techniques that can keep pace with generation technologies is crucial. We advocate for a proactive approach in the \"tug-of-war\" between deepfake creators and detectors, emphasizing the need for continuous research collaboration, standardization of evaluation metrics, and the creation of comprehensive benchmarks.         ",
    "url": "https://arxiv.org/abs/2407.06174",
    "authors": [
      "Hannah Lee",
      "Changyeon Lee",
      "Kevin Farhat",
      "Lin Qiu",
      "Steve Geluso",
      "Aerin Kim",
      "Oren Etzioni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.09174",
    "title": "DART: An Automated End-to-End Object Detection Pipeline with Data Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label Review, and Model Training",
    "abstract": "           Accurate real-time object detection is vital across numerous industrial applications, from safety monitoring to quality control. Traditional approaches, however, are hindered by arduous manual annotation and data collection, struggling to adapt to ever-changing environments and novel target objects. To address these limitations, this paper presents DART, an innovative automated end-to-end pipeline that revolutionizes object detection workflows from data collection to model evaluation. It eliminates the need for laborious human labeling and extensive data collection while achieving outstanding accuracy across diverse scenarios. DART encompasses four key stages: (1) Data Diversification using subject-driven image generation (DreamBooth with SDXL), (2) Annotation via open-vocabulary object detection (Grounding DINO) to generate bounding box and class labels (3) Review of generated images and pseudo-labels by large multimodal models (InternVL-1.5 and GPT-4o) to guarantee credibility, (4) Training of real-time object detectors (YOLOv8 and YOLOv10) using the verified data as ground truth. We apply DART to a self-collected dataset of construction machines named Liebherr Product, which contains over 15K high-quality images across 23 categories. The current instantiation of DART significantly increases average precision (AP) from 0.064 to 0.832. Its modular design ensures easy exchangeability and extensibility, allowing for future algorithm upgrades, seamless integration of new object categories, and adaptability to customized environments without manual labeling and additional data collection. The code and dataset are released at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.09174",
    "authors": [
      "Chen Xin",
      "Andreas Hartel",
      "Enkelejda Kasneci"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.14575",
    "title": "Regression prediction algorithm for energy consumption regression in cloud computing based on horned lizard algorithm optimised convolutional neural network-bidirectional gated recurrent unit",
    "abstract": "           For this paper, a prediction study of cloud computing energy consumption was conducted by optimising the data regression algorithm based on the horned lizard optimisation algorithm for Convolutional Neural Networks-Bi-Directional Gated Recurrent Units. Firstly, through Spearman correlation analysis of CPU, usage, memory usage, network traffic, power consumption, number of instructions executed, execution time and energy efficiency, we found that power consumption has the highest degree of positive correlation with energy efficiency, while CPU usage has the highest degree of negative correlation with energy efficiency. In our experiments, we introduced a random forest model and an optimisation model based on the horned lizard optimisation algorithm for testing, and the results show that the optimisation algorithm has better prediction results compared to the random forest model. Specifically, the mean square error (MSE) of the optimisation algorithm is 0.01 smaller than that of the random forest model, and the mean absolute error (MAE) is 0.01 smaller than that of the random forest.3 The results of the combined metrics show that the optimisation algorithm performs more accurately and reliably in predicting energy efficiency. This research result provides new ideas and methods to improve the energy efficiency of cloud computing systems. This research not only expands the scope of application in the field of cloud computing, but also provides a strong support for improving the energy use efficiency of the system.         ",
    "url": "https://arxiv.org/abs/2407.14575",
    "authors": [
      "Feiyang Li",
      "Zinan Cao",
      "Qixuan Yu",
      "Xirui Tang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.14644",
    "title": "Human-Interpretable Adversarial Prompt Attack on Large Language Models with Situational Context",
    "abstract": "           Previous research on testing the vulnerabilities in Large Language Models (LLMs) using adversarial attacks has primarily focused on nonsensical prompt injections, which are easily detected upon manual or automated review (e.g., via byte entropy). However, the exploration of innocuous human-understandable malicious prompts augmented with adversarial injections remains limited. In this research, we explore converting a nonsensical suffix attack into a sensible prompt via a situation-driven contextual re-writing. This allows us to show suffix conversion without any gradients, using only LLMs to perform the attacks, and thus better understand the scope of possible risks. We combine an independent, meaningful adversarial insertion and situations derived from movies to check if this can trick an LLM. The situations are extracted from the IMDB dataset, and prompts are defined following a few-shot chain-of-thought prompting. Our approach demonstrates that a successful situation-driven attack can be executed on both open-source and proprietary LLMs. We find that across many LLMs, as few as 1 attempt produces an attack and that these attacks transfer between LLMs.         ",
    "url": "https://arxiv.org/abs/2407.14644",
    "authors": [
      "Nilanjana Das",
      "Edward Raff",
      "Manas Gaur"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.16326",
    "title": "On The Expressive Power of Knowledge Graph Embedding Methods",
    "abstract": "           Knowledge Graph Embedding (KGE) is a popular approach, which aims to represent entities and relations of a knowledge graph in latent spaces. Their representations are known as embeddings. To measure the plausibility of triplets, score functions are defined over embedding spaces. Despite wide dissemination of KGE in various tasks, KGE methods have limitations in reasoning abilities. In this paper we propose a mathematical framework to compare reasoning abilities of KGE methods. We show that STransE has a higher capability than TransComplEx, and then present new STransCoRe method, which improves the STransE by combining it with the TransCoRe insights, which can reduce the STransE space complexity.         ",
    "url": "https://arxiv.org/abs/2407.16326",
    "authors": [
      "Jiexing Gao",
      "Dmitry Rodin",
      "Vasily Motolygin",
      "Denis Zaytsev"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.16789",
    "title": "What Matters in Range View 3D Object Detection",
    "abstract": "           Lidar-based perception pipelines rely on 3D object detection models to interpret complex scenes. While multiple representations for lidar exist, the range-view is enticing since it losslessly encodes the entire lidar sensor output. In this work, we achieve state-of-the-art amongst range-view 3D object detection models without using multiple techniques proposed in past range-view literature. We explore range-view 3D object detection across two modern datasets with substantially different properties: Argoverse 2 and Waymo Open. Our investigation reveals key insights: (1) input feature dimensionality significantly influences the overall performance, (2) surprisingly, employing a classification loss grounded in 3D spatial proximity works as well or better compared to more elaborate IoU-based losses, and (3) addressing non-uniform lidar density via a straightforward range subsampling technique outperforms existing multi-resolution, range-conditioned networks. Our experiments reveal that techniques proposed in recent range-view literature are not needed to achieve state-of-the-art performance. Combining the above findings, we establish a new state-of-the-art model for range-view 3D object detection -- improving AP by 2.2% on the Waymo Open dataset while maintaining a runtime of 10 Hz. We establish the first range-view model on the Argoverse 2 dataset and outperform strong voxel-based baselines. All models are multi-class and open-source. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.16789",
    "authors": [
      "Benjamin Wilson",
      "Nicholas Autio Mitchell",
      "Jhony Kaesemodel Pontes",
      "James Hays"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.16929",
    "title": "Synthetic Data, Similarity-based Privacy Metrics, and Regulatory (Non-)Compliance",
    "abstract": "           In this paper, we argue that similarity-based privacy metrics cannot ensure regulatory compliance of synthetic data. Our analysis and counter-examples show that they do not protect against singling out and linkability and, among other fundamental issues, completely ignore the motivated intruder test.         ",
    "url": "https://arxiv.org/abs/2407.16929",
    "authors": [
      "Georgi Ganev"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2407.17790",
    "title": "Exploring the Limitations of Kolmogorov-Arnold Networks in Classification: Insights to Software Training and Hardware Implementation",
    "abstract": "           Kolmogorov-Arnold Networks (KANs), a novel type of neural network, have recently gained popularity and attention due to the ability to substitute multi-layer perceptions (MLPs) in artificial intelligence (AI) with higher accuracy and interoperability. However, KAN assessment is still limited and cannot provide an in-depth analysis of a specific domain. Furthermore, no study has been conducted on the implementation of KANs in hardware design, which would directly demonstrate whether KANs are truly superior to MLPs in practical applications. As a result, in this paper, we focus on verifying KANs for classification issues, which are a common but significant topic in AI using four different types of datasets. Furthermore, the corresponding hardware implementation is considered using the Vitis high-level synthesis (HLS) tool. To the best of our knowledge, this is the first article to implement hardware for KAN. The results indicate that KANs cannot achieve more accuracy than MLPs in high complex datasets while utilizing substantially higher hardware resources. Therefore, MLP remains an effective approach for achieving accuracy and efficiency in software and hardware implementation.         ",
    "url": "https://arxiv.org/abs/2407.17790",
    "authors": [
      "Van Duy Tran",
      "Tran Xuan Hieu Le",
      "Thi Diem Tran",
      "Hoai Luan Pham",
      "Vu Trung Duong Le",
      "Tuan Hai Vu",
      "Van Tinh Nguyen",
      "Yasuhiko Nakashima"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2407.17792",
    "title": "Harnessing Temporal Causality for Advanced Temporal Action Detection",
    "abstract": "           As a fundamental task in long-form video understanding, temporal action detection (TAD) aims to capture inherent temporal relations in untrimmed videos and identify candidate actions with precise boundaries. Over the years, various networks, including convolutions, graphs, and transformers, have been explored for effective temporal modeling for TAD. However, these modules typically treat past and future information equally, overlooking the crucial fact that changes in action boundaries are essentially causal events. Inspired by this insight, we propose leveraging the temporal causality of actions to enhance TAD representation by restricting the model's access to only past or future context. We introduce CausalTAD, which combines causal attention and causal Mamba to achieve state-of-the-art performance on multiple benchmarks. Notably, with CausalTAD, we ranked 1st in the Action Recognition, Action Detection, and Audio-Based Interaction Detection tracks at the EPIC-Kitchens Challenge 2024, as well as 1st in the Moment Queries track at the Ego4D Challenge 2024. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.17792",
    "authors": [
      "Shuming Liu",
      "Lin Sui",
      "Chen-Lin Zhang",
      "Fangzhou Mu",
      "Chen Zhao",
      "Bernard Ghanem"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.17856",
    "title": "MDS-ED: Multimodal Decision Support in the Emergency Department -- a Benchmark Dataset for Diagnoses and Deterioration Prediction in Emergency Medicine",
    "abstract": "           Background: Benchmarking medical decision support algorithms often struggles due to limited access to datasets, narrow prediction tasks, and restricted input modalities. These limitations affect their clinical relevance and performance in high-stakes areas like emergency care, complicating replication, validation, and improvement of benchmarks. Methods: We introduce a dataset based on MIMIC-IV, benchmarking protocol, and initial results for evaluating multimodal decision support in the emergency department (ED). We use diverse data modalities from the first 1.5 hours of patient arrival, including demographics, biometrics, vital signs, lab values, and electrocardiogram waveforms. We analyze 1443 clinical labels across two contexts: predicting diagnoses with ICD-10 codes and forecasting patient deterioration. Results: Our multimodal diagnostic model achieves an AUROC score over 0.8 in a statistically significant manner for 357 out of 1428 conditions, including cardiac issues like myocardial infarction and non-cardiac conditions such as renal disease and diabetes. The deterioration model scores above 0.8 in a statistically significant manner for 13 out of 15 targets, including critical events like cardiac arrest and mechanical ventilation, ICU admission as well as short- and long-term mortality. Incorporating raw waveform data significantly improves model performance, which represents one of the first robust demonstrations of this effect. Conclusions: This study highlights the uniqueness of our dataset, which encompasses a wide range of clinical tasks and utilizes a comprehensive set of features collected early during the emergency after arriving at the ED. The strong performance, as evidenced by high AUROC scores across diagnostic and deterioration targets, underscores the potential of our approach to revolutionize decision-making in acute and emergency medicine.         ",
    "url": "https://arxiv.org/abs/2407.17856",
    "authors": [
      "Juan Miguel Lopez Alcaraz",
      "Hjalmar Bouma",
      "Nils Strodthoff"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.17904",
    "title": "Exploring the Effect of Dataset Diversity in Self-Supervised Learning for Surgical Computer Vision",
    "abstract": "           Over the past decade, computer vision applications in minimally invasive surgery have rapidly increased. Despite this growth, the impact of surgical computer vision remains limited compared to other medical fields like pathology and radiology, primarily due to the scarcity of representative annotated data. Whereas transfer learning from large annotated datasets such as ImageNet has been conventionally the norm to achieve high-performing models, recent advancements in self-supervised learning (SSL) have demonstrated superior performance. In medical image analysis, in-domain SSL pretraining has already been shown to outperform ImageNet-based initialization. Although unlabeled data in the field of surgical computer vision is abundant, the diversity within this data is limited. This study investigates the role of dataset diversity in SSL for surgical computer vision, comparing procedure-specific datasets against a more heterogeneous general surgical dataset across three different downstream surgical applications. The obtained results show that using solely procedure-specific data can lead to substantial improvements of 13.8%, 9.5%, and 36.8% compared to ImageNet pretraining. However, extending this data with more heterogeneous surgical data further increases performance by an additional 5.0%, 5.2%, and 2.5%, suggesting that increasing diversity within SSL data is beneficial for model performance. The code and pretrained model weights are made publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.17904",
    "authors": [
      "Tim J.M. Jaspers",
      "Ronald L.P.D. de Jong",
      "Yasmina Al Khalil",
      "Tijn Zeelenberg",
      "Carolus H.J. Kusters",
      "Yiping Li",
      "Romy C. van Jaarsveld",
      "Franciscus H.A. Bakker",
      "Jelle P. Ruurda",
      "Willem M. Brinkman",
      "Peter H.N. De With",
      "Fons van der Sommen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.18213",
    "title": "Exploring Scaling Trends in LLM Robustness",
    "abstract": "           Language model capabilities predictably improve from scaling a model's size and training data. Motivated by this, increasingly large language models have been trained, yielding an array of impressive capabilities. Yet these models are vulnerable to adversarial prompts, such as \"jailbreaks\" that hijack models to perform undesired behaviors, posing a significant risk of misuse. Prior work indicates that computer vision models become more robust with model and data scaling, raising the question: does language model robustness also improve with scale? We study this question empirically, finding that larger models respond substantially better to adversarial training, but there is little to no benefit from model scale in the absence of explicit defenses.         ",
    "url": "https://arxiv.org/abs/2407.18213",
    "authors": [
      "Nikolaus Howe",
      "Micha\u0142 Zajac",
      "Ian McKenzie",
      "Oskar Hollinsworth",
      "Tom Tseng",
      "Pierre-Luc Bacon",
      "Adam Gleave"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2306.12288",
    "title": "R\\'enyi--Sobolev Inequalities and Connections to Spectral Graph Theory",
    "abstract": "           In this paper, we generalize the log-Sobolev inequalities to R\u00e9nyi--Sobolev inequalities by replacing the entropy with the two-parameter entropy, which is a generalized version of entropy and closely related to R\u00e9nyi divergences. We derive the sharp nonlinear dimension-free version of this kind of inequalities. Interestingly, the resultant inequalities show a transition phenomenon depending on the parameters. We then connect R\u00e9nyi--Sobolev inequalities to contractive and data-processing inequalities, concentration inequalities, and spectral graph theory. Our proofs in this paper are based on the information-theoretic characterization of the R\u00e9nyi--Sobolev inequalities, as well as the method of types.         ",
    "url": "https://arxiv.org/abs/2306.12288",
    "authors": [
      "Lei Yu",
      "Hao Wu"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)",
      "Functional Analysis (math.FA)"
    ]
  },
  {
    "id": "arXiv:2308.08426",
    "title": "Differentiable Robust Model Predictive Control",
    "abstract": "           Deterministic model predictive control (MPC), while powerful, is often insufficient for effectively controlling autonomous systems in the real-world. Factors such as environmental noise and model error can cause deviations from the expected nominal performance. Robust MPC algorithms aim to bridge this gap between deterministic and uncertain control. However, these methods are often excessively difficult to tune for robustness due to the nonlinear and non-intuitive effects that controller parameters have on performance. To address this challenge, we first present a unifying perspective on differentiable optimization for control using the implicit function theorem (IFT), from which existing state-of-the art methods can be derived. Drawing parallels with differential dynamic programming, the IFT enables the derivation of an efficient differentiable optimal control framework. The derived scheme is subsequently paired with a tube-based MPC architecture to facilitate the automatic and real-time tuning of robust controllers in the presence of large uncertainties and disturbances. The proposed algorithm is benchmarked on multiple nonlinear robotic systems, including two systems in the MuJoCo simulator environment and one hardware experiment on the Robotarium testbed, to demonstrate its efficacy.         ",
    "url": "https://arxiv.org/abs/2308.08426",
    "authors": [
      "Alex Oshin",
      "Hassan Almubarak",
      "Evangelos A. Theodorou"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2405.19995",
    "title": "Symmetries in Overparametrized Neural Networks: A Mean-Field View",
    "abstract": "           We develop a Mean-Field (MF) view of the learning dynamics of overparametrized Artificial Neural Networks (NN) under data symmetric in law wrt the action of a general compact group $G$. We consider for this a class of generalized shallow NNs given by an ensemble of $N$ multi-layer units, jointly trained using stochastic gradient descent (SGD) and possibly symmetry-leveraging (SL) techniques, such as Data Augmentation (DA), Feature Averaging (FA) or Equivariant Architectures (EA). We introduce the notions of weakly and strongly invariant laws (WI and SI) on the parameter space of each single unit, corresponding, respectively, to $G$-invariant distributions, and to distributions supported on parameters fixed by the group action (which encode EA). This allows us to define symmetric models compatible with taking $N\\to\\infty$ and give an interpretation of the asymptotic dynamics of DA, FA and EA in terms of Wasserstein Gradient Flows describing their MF limits. When activations respect the group action, we show that, for symmetric data, DA, FA and freely-trained models obey the exact same MF dynamic, which stays in the space of WI laws and minimizes therein the population risk. We also give a counterexample to the general attainability of an optimum over SI laws. Despite this, quite remarkably, we show that the set of SI laws is also preserved by the MF dynamics even when freely trained. This sharply contrasts the finite-$N$ setting, in which EAs are generally not preserved by unconstrained SGD. We illustrate the validity of our findings as $N$ gets larger in a teacher-student experimental setting, training a student NN to learn from a WI, SI or arbitrary teacher model through various SL schemes. We last deduce a data-driven heuristic to discover the largest subspace of parameters supporting SI distributions for a problem, that could be used for designing EA with minimal generalization error.         ",
    "url": "https://arxiv.org/abs/2405.19995",
    "authors": [
      "Javier Maass",
      "Joaquin Fontbona"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2407.14651",
    "title": "Improving Representation of High-frequency Components for Medical Foundation Models",
    "abstract": "           Foundation models have recently attracted significant attention for their impressive generalizability across diverse downstream tasks. However, these models are demonstrated to exhibit great limitations in representing high-frequency components and fine-grained details. In many medical imaging tasks, the precise representation of such information is crucial due to the inherently intricate anatomical structures, sub-visual features, and complex boundaries involved. Consequently, the limited representation of prevalent foundation models can result in significant performance degradation or even failure in these tasks. To address these challenges, we propose a novel pretraining strategy, named Frequency-advanced Representation Autoencoder (Frepa). Through high-frequency masking and low-frequency perturbation combined with adversarial learning, Frepa encourages the encoder to effectively represent and preserve high-frequency components in the image embeddings. Additionally, we introduce an innovative histogram-equalized image masking strategy, extending the Masked Autoencoder approach beyond ViT to other architectures such as Swin Transformer and convolutional networks. We develop Frepa across nine medical modalities and validate it on 32 downstream tasks for both 2D images and 3D volume data. Without fine-tuning, Frepa can outperform other self-supervised pretraining methods and, in some cases, even surpasses task-specific trained models. This improvement is particularly significant for tasks involving fine-grained details, such as achieving up to a +15% increase in DSC for retina vessel segmentation and a +7% increase in IoU for lung nodule detection. Further experiments quantitatively reveal that Frepa enables superior high-frequency representations and preservation in the embeddings, underscoring its potential for developing more generalized and universal medical image foundation models.         ",
    "url": "https://arxiv.org/abs/2407.14651",
    "authors": [
      "Yuetan Chu",
      "Yilan Zhang",
      "Zhongyi Han",
      "Changchun Yang",
      "Longxi Zhou",
      "Gongning Luo",
      "Xin Gao"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  }
]