[
  {
    "id": "arXiv:2409.02938",
    "title": "CortexCompile: Harnessing Cortical-Inspired Architectures for Enhanced Multi-Agent NLP Code Synthesis",
    "abstract": "           Current approaches to automated code generation often rely on monolithic models that lack real-time adaptability and scalability. This limitation is particularly evident in complex programming tasks that require dynamic adjustment and efficiency. The integration of neuroscience principles into Natural Language Processing (NLP) has the potential to revolutionize automated code generation. This paper presents CortexCompile, a novel modular system inspired by the specialized functions of the human brain's cortical regions. By emulating the distinct roles of the Prefrontal Cortex, Parietal Cortex, Temporal Lobe, and Motor Cortex, CortexCompile achieves significant advancements in scalability, efficiency, and adaptability compared to traditional monolithic models like GPT-4o. The system's architecture features a Task Orchestration Agent that manages dynamic task delegation and parallel processing, facilitating the generation of highly accurate and optimized code across increasingly complex programming tasks. Experimental evaluations demonstrate that CortexCompile consistently outperforms GPT-4o in development time, accuracy, and user satisfaction, particularly in tasks involving real-time strategy games and first-person shooters. These findings underscore the viability of neuroscience-inspired architectures in addressing the limitations of current NLP models, paving the way for more efficient and human-like AI systems.         ",
    "url": "https://arxiv.org/abs/2409.02938",
    "authors": [
      "Gautham Ramachandran",
      "Rick Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.02965",
    "title": "Do We Trust What They Say or What They Do? A Multimodal User Embedding Provides Personalized Explanations",
    "abstract": "           With the rapid development of social media, the importance of analyzing social network user data has also been put on the agenda. User representation learning in social media is a critical area of research, based on which we can conduct personalized content delivery, or detect malicious actors. Being more complicated than many other types of data, social network user data has inherent multimodal nature. Various multimodal approaches have been proposed to harness both text (i.e. post content) and relation (i.e. inter-user interaction) information to learn user embeddings of higher quality. The advent of Graph Neural Network models enables more end-to-end integration of user text embeddings and user interaction graphs in social networks. However, most of those approaches do not adequately elucidate which aspects of the data - text or graph structure information - are more helpful for predicting each specific user under a particular task, putting some burden on personalized downstream analysis and untrustworthy information filtering. We propose a simple yet effective framework called Contribution-Aware Multimodal User Embedding (CAMUE) for social networks. We have demonstrated with empirical evidence, that our approach can provide personalized explainable predictions, automatically mitigating the impact of unreliable information. We also conducted case studies to show how reasonable our results are. We observe that for most users, graph structure information is more trustworthy than text information, but there are some reasonable cases where text helps more. Our work paves the way for more explainable, reliable, and effective social media user embedding which allows for better personalized content delivery.         ",
    "url": "https://arxiv.org/abs/2409.02965",
    "authors": [
      "Zhicheng Ren",
      "Zhiping Xiao",
      "Yizhou Sun"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.02976",
    "title": "Hallucination Detection in LLMs: Fast and Memory-Efficient Finetuned Models",
    "abstract": "           Uncertainty estimation is a necessary component when implementing AI in high-risk settings, such as autonomous cars, medicine, or insurances. Large Language Models (LLMs) have seen a surge in popularity in recent years, but they are subject to hallucinations, which may cause serious harm in high-risk settings. Despite their success, LLMs are expensive to train and run: they need a large amount of computations and memory, preventing the use of ensembling methods in practice. In this work, we present a novel method that allows for fast and memory-friendly training of LLM ensembles. We show that the resulting ensembles can detect hallucinations and are a viable approach in practice as only one GPU is needed for training and inference.         ",
    "url": "https://arxiv.org/abs/2409.02976",
    "authors": [
      "Gabriel Y. Arteaga",
      "Thomas B. Sch\u00f6n",
      "Nicolas Pielawski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.03022",
    "title": "Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes",
    "abstract": "           We introduce Boundless, a photo-realistic synthetic data generation system for enabling highly accurate object detection in dense urban streetscapes. Boundless can replace massive real-world data collection and manual ground-truth object annotation (labeling) with an automated and configurable process. Boundless is based on the Unreal Engine 5 (UE5) City Sample project with improvements enabling accurate collection of 3D bounding boxes across different lighting and scene variability conditions. We evaluate the performance of object detection models trained on the dataset generated by Boundless when used for inference on a real-world dataset acquired from medium-altitude cameras. We compare the performance of the Boundless-trained model against the CARLA-trained model and observe an improvement of 7.8 mAP. The results we achieved support the premise that synthetic data generation is a credible methodology for training/fine-tuning scalable object detection models for urban scenes.         ",
    "url": "https://arxiv.org/abs/2409.03022",
    "authors": [
      "Mehmet Kerem Turkcan",
      "Ian Li",
      "Chengbo Zang",
      "Javad Ghaderi",
      "Gil Zussman",
      "Zoran Kostic"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03024",
    "title": "NUMOSIM: A Synthetic Mobility Dataset with Anomaly Detection Benchmarks",
    "abstract": "           Collecting real-world mobility data is challenging. It is often fraught with privacy concerns, logistical difficulties, and inherent biases. Moreover, accurately annotating anomalies in large-scale data is nearly impossible, as it demands meticulous effort to distinguish subtle and complex patterns. These challenges significantly impede progress in geospatial anomaly detection research by restricting access to reliable data and complicating the rigorous evaluation, comparison, and benchmarking of methodologies. To address these limitations, we introduce a synthetic mobility dataset, NUMOSIM, that provides a controlled, ethical, and diverse environment for benchmarking anomaly detection techniques. NUMOSIM simulates a wide array of realistic mobility scenarios, encompassing both typical and anomalous behaviours, generated through advanced deep learning models trained on real mobility data. This approach allows NUMOSIM to accurately replicate the complexities of real-world movement patterns while strategically injecting anomalies to challenge and evaluate detection algorithms based on how effectively they capture the interplay between demographic, geospatial, and temporal factors. Our goal is to advance geospatial mobility analysis by offering a realistic benchmark for improving anomaly detection and mobility modeling techniques. To support this, we provide open access to the NUMOSIM dataset, along with comprehensive documentation, evaluation metrics, and benchmark results.         ",
    "url": "https://arxiv.org/abs/2409.03024",
    "authors": [
      "Chris Stanford",
      "Suman Adari",
      "Xishun Liao",
      "Yueshuai He",
      "Qinhua Jiang",
      "Chenchen Kuai",
      "Jiaqi Ma",
      "Emmanuel Tung",
      "Yinlong Qian",
      "Lingyi Zhao",
      "Zihao Zhou",
      "Zeeshan Rasheed",
      "Khurram Shafique"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03034",
    "title": "MDNF: Multi-Diffusion-Nets for Neural Fields on Meshes",
    "abstract": "           We propose a novel framework for representing neural fields on triangle meshes that is multi-resolution across both spatial and frequency domains. Inspired by the Neural Fourier Filter Bank (NFFB), our architecture decomposes the spatial and frequency domains by associating finer spatial resolution levels with higher frequency bands, while coarser resolutions are mapped to lower frequencies. To achieve geometry-aware spatial decomposition we leverage multiple DiffusionNet components, each associated with a different spatial resolution level. Subsequently, we apply a Fourier feature mapping to encourage finer resolution levels to be associated with higher frequencies. The final signal is composed in a wavelet-inspired manner using a sine-activated MLP, aggregating higher-frequency signals on top of lower-frequency ones. Our architecture attains high accuracy in learning complex neural fields and is robust to discontinuities, exponential scale variations of the target field, and mesh modification. We demonstrate the effectiveness of our approach through its application to diverse neural fields, such as synthetic RGB functions, UV texture coordinates, and vertex normals, illustrating different challenges. To validate our method, we compare its performance against two alternatives, showcasing the advantages of our multi-resolution architecture.         ",
    "url": "https://arxiv.org/abs/2409.03034",
    "authors": [
      "Avigail Cohen Rimon",
      "Tal Shnitzer",
      "Mirela Ben Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03046",
    "title": "Oddballness: universal anomaly detection with language models",
    "abstract": "           We present a new method to detect anomalies in texts (in general: in sequences of any data), using language models, in a totally unsupervised manner. The method considers probabilities (likelihoods) generated by a language model, but instead of focusing on low-likelihood tokens, it considers a new metric introduced in this paper: oddballness. Oddballness measures how ``strange'' a given token is according to the language model. We demonstrate in grammatical error detection tasks (a specific case of text anomaly detection) that oddballness is better than just considering low-likelihood events, if a totally unsupervised setup is assumed.         ",
    "url": "https://arxiv.org/abs/2409.03046",
    "authors": [
      "Filip Grali\u0144ski",
      "Ryszard Staruch",
      "Krzysztof Jurkiewicz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.03060",
    "title": "Better Verified Explanations with Applications to Incorrectness and Out-of-Distribution Detection",
    "abstract": "           Building on VeriX (Verified eXplainability, arXiv:2212.01051), a system for producing optimal verified explanations for machine learning model outputs, we present VeriX+, which significantly improves both the size and the generation time of verified explanations. We introduce a bound propagation-based sensitivity technique to improve the size, and a binary search-based traversal with confidence ranking for improving time -- the two techniques are orthogonal and can be used independently or together. We also show how to adapt the QuickXplain (Junker 2004) algorithm to our setting to provide a trade-off between size and time. Experimental evaluations on standard benchmarks demonstrate significant improvements on both metrics, e.g., a size reduction of 38% on the GTSRB dataset and a time reduction of 90% on MNIST. We also explore applications of our verified explanations and show that explanation size is a useful proxy for both incorrectness detection and out-of-distribution detection.         ",
    "url": "https://arxiv.org/abs/2409.03060",
    "authors": [
      "Min Wu",
      "Xiaofu Li",
      "Haoze Wu",
      "Clark Barrett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.03061",
    "title": "Incorporating dense metric depth into neural 3D representations for view synthesis and relighting",
    "abstract": "           Synthesizing accurate geometry and photo-realistic appearance of small scenes is an active area of research with compelling use cases in gaming, virtual reality, robotic-manipulation, autonomous driving, convenient product capture, and consumer-level photography. When applying scene geometry and appearance estimation techniques to robotics, we found that the narrow cone of possible viewpoints due to the limited range of robot motion and scene clutter caused current estimation techniques to produce poor quality estimates or even fail. On the other hand, in robotic applications, dense metric depth can often be measured directly using stereo and illumination can be controlled. Depth can provide a good initial estimate of the object geometry to improve reconstruction, while multi-illumination images can facilitate relighting. In this work we demonstrate a method to incorporate dense metric depth into the training of neural 3D representations and address an artifact observed while jointly refining geometry and appearance by disambiguating between texture and geometry edges. We also discuss a multi-flash stereo camera system developed to capture the necessary data for our pipeline and show results on relighting and view synthesis with a few training views.         ",
    "url": "https://arxiv.org/abs/2409.03061",
    "authors": [
      "Arkadeep Narayan Chaudhury",
      "Igor Vasiljevic",
      "Sergey Zakharov",
      "Vitor Guizilini",
      "Rares Ambrus",
      "Srinivasa Narasimhan",
      "Christopher G. Atkeson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.03067",
    "title": "A Comparative Study of Offline Models and Online LLMs in Fake News Detection",
    "abstract": "           Fake news detection remains a critical challenge in today's rapidly evolving digital landscape, where misinformation can spread faster than ever before. Traditional fake news detection models often rely on static datasets and auxiliary information, such as metadata or social media interactions, which limits their adaptability to real-time scenarios. Recent advancements in Large Language Models (LLMs) have demonstrated significant potential in addressing these challenges due to their extensive pre-trained knowledge and ability to analyze textual content without relying on auxiliary data. However, many of these LLM-based approaches are still rooted in static datasets, with limited exploration into their real-time processing capabilities. This paper presents a systematic evaluation of both traditional offline models and state-of-the-art LLMs for real-time fake news detection. We demonstrate the limitations of existing offline models, including their inability to adapt to dynamic misinformation patterns. Furthermore, we show that newer LLM models with online capabilities, such as GPT-4, Claude, and Gemini, are better suited for detecting emerging fake news in real-time contexts. Our findings emphasize the importance of transitioning from offline to online LLM models for real-time fake news detection. Additionally, the public accessibility of LLMs enhances their scalability and democratizes the tools needed to combat misinformation. By leveraging real-time data, our work marks a significant step toward more adaptive, effective, and scalable fake news detection systems.         ",
    "url": "https://arxiv.org/abs/2409.03067",
    "authors": [
      "Ruoyu Xu",
      "Gaoxiang Li"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2409.03077",
    "title": "Backdoor defense, learnability and obfuscation",
    "abstract": "           We introduce a formal notion of defendability against backdoors using a game between an attacker and a defender. In this game, the attacker modifies a function to behave differently on a particular input known as the \"trigger\", while behaving the same almost everywhere else. The defender then attempts to detect the trigger at evaluation time. If the defender succeeds with high enough probability, then the function class is said to be defendable. The key constraint on the attacker that makes defense possible is that the attacker's strategy must work for a randomly-chosen trigger. Our definition is simple and does not explicitly mention learning, yet we demonstrate that it is closely connected to learnability. In the computationally unbounded setting, we use a voting algorithm of Hanneke et al. (2022) to show that defendability is essentially determined by the VC dimension of the function class, in much the same way as PAC learnability. In the computationally bounded setting, we use a similar argument to show that efficient PAC learnability implies efficient defendability, but not conversely. On the other hand, we use indistinguishability obfuscation to show that the class of polynomial size circuits is not efficiently defendable. Finally, we present polynomial size decision trees as a natural example for which defense is strictly easier than learning. Thus, we identify efficient defendability as a notable intermediate concept in between efficient learnability and obfuscation.         ",
    "url": "https://arxiv.org/abs/2409.03077",
    "authors": [
      "Paul Christiano",
      "Jacob Hilton",
      "Victor Lecomte",
      "Mark Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.03109",
    "title": "FIDAVL: Fake Image Detection and Attribution using Vision-Language Model",
    "abstract": "           We introduce FIDAVL: Fake Image Detection and Attribution using a Vision-Language Model. FIDAVL is a novel and efficient mul-titask approach inspired by the synergies between vision and language processing. Leveraging the benefits of zero-shot learning, FIDAVL exploits the complementarity between vision and language along with soft prompt-tuning strategy to detect fake images and accurately attribute them to their originating source models. We conducted extensive experiments on a comprehensive dataset comprising synthetic images generated by various state-of-the-art models. Our results demonstrate that FIDAVL achieves an encouraging average detection accuracy of 95.42% and F1-score of 95.47% while also obtaining noteworthy performance metrics, with an average F1-score of 92.64% and ROUGE-L score of 96.50% for attributing synthetic images to their respective source generation models. The source code of this work will be publicly released at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.03109",
    "authors": [
      "Mamadou Keita",
      "Wassim Hamidouche",
      "Hessen Bougueffa Eutamene",
      "Abdelmalik Taleb-Ahmed",
      "Abdenour Hadid"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.03115",
    "title": "Probing self-attention in self-supervised speech models for cross-linguistic differences",
    "abstract": "           Speech models have gained traction thanks to increase in accuracy from novel transformer architectures. While this impressive increase in performance across automatic speech recognition (ASR) benchmarks is noteworthy, there is still much that is unknown about the use of attention mechanisms for speech-related tasks. For example, while it is assumed that these models are learning language-independent (i.e., universal) speech representations, there has not yet been an in-depth exploration of what it would mean for the models to be language-independent. In the current paper, we explore this question within the realm of self-attention mechanisms of one small self-supervised speech transformer model (TERA). We find that even with a small model, the attention heads learned are diverse ranging from almost entirely diagonal to almost entirely global regardless of the training language. We highlight some notable differences in attention patterns between Turkish and English and demonstrate that the models do learn important phonological information during pretraining. We also present a head ablation study which shows that models across languages primarily rely on diagonal heads to classify phonemes.         ",
    "url": "https://arxiv.org/abs/2409.03115",
    "authors": [
      "Sai Gopinath",
      "Joselyn Rodriguez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03127",
    "title": "Fast algorithms to improve fair information access in networks",
    "abstract": "           When information spreads across a network via pairwise sharing, large disparities in information access can arise from the network's structural heterogeneity. Algorithms to improve the fairness of information access seek to maximize the minimum access of a node to information by sequentially selecting new nodes to seed with the spreading information. However, existing algorithms are computationally expensive. Here, we develop and evaluate a set of 10 new scalable algorithms to improve information access in social networks; in order to compare them to the existing state-of-the-art, we introduce both a new performance metric and a new benchmark corpus of networks. Additionally, we investigate the degree to which algorithm performance on minimizing information access gaps can be predicted ahead of time from features of a network's structure. We find that while no algorithm is strictly superior to all others across networks, our new scalable algorithms are competitive with the state-of-the-art and orders of magnitude faster. We introduce a meta-learner approach that learns which of the fast algorithms is best for a specific network and is on average only 20% less effective than the state-of-the-art performance on held-out data, while about 75-130 times faster. Furthermore, on about 20% of networks the meta-learner's performance exceeds the state-of-the-art.         ",
    "url": "https://arxiv.org/abs/2409.03127",
    "authors": [
      "Dennis Robert Windham",
      "Caroline J. Wendt",
      "Alex Crane",
      "Sorelle A. Friedler",
      "Blair D. Sullivan",
      "Aaron Clauset"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2409.03129",
    "title": "Subsidy design for better social outcomes",
    "abstract": "           Overcoming the impact of selfish behavior of rational players in multiagent systems is a fundamental problem in game theory. Without any intervention from a central agent, strategic users take actions in order to maximize their personal utility, which can lead to extremely inefficient overall system performance, often indicated by a high Price of Anarchy. Recent work (Lin et al. 2021) investigated and formalized yet another undesirable behavior of rational agents, that of avoiding freely available information about the game for selfish reasons, leading to worse social outcomes. A central planner can significantly mitigate these issues by injecting a subsidy to reduce certain costs associated with the system and obtain net gains in the system performance. Crucially, the planner needs to determine how to allocate this subsidy effectively. We formally show that designing subsidies that perfectly optimize the social good, in terms of minimizing the Price of Anarchy or preventing the information avoidance behavior, is computationally hard under standard complexity theoretic assumptions. On the positive side, we show that we can learn provably good values of subsidy in repeated games coming from the same domain. This data-driven subsidy design approach avoids solving computationally hard problems for unseen games by learning over polynomially many games. We also show that optimal subsidy can be learned with no-regret given an online sequence of games, under mild assumptions on the cost matrix. Our study focuses on two distinct games: a Bayesian extension of the well-studied fair cost-sharing game, and a component maintenance game with engineering applications.         ",
    "url": "https://arxiv.org/abs/2409.03129",
    "authors": [
      "Maria-Florina Balcan",
      "Matteo Pozzi",
      "Dravyansh Sharma"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03131",
    "title": "Well, that escalated quickly: The Single-Turn Crescendo Attack (STCA)",
    "abstract": "           This paper explores a novel approach to adversarial attacks on large language models (LLM): the Single-Turn Crescendo Attack (STCA). The STCA builds upon the multi-turn crescendo attack established by Mark Russinovich, Ahmed Salem, Ronen Eldan. Traditional multi-turn adversarial strategies gradually escalate the context to elicit harmful or controversial responses from LLMs. However, this paper introduces a more efficient method where the escalation is condensed into a single interaction. By carefully crafting the prompt to simulate an extended dialogue, the attack bypasses typical content moderation systems, leading to the generation of responses that would normally be filtered out. I demonstrate this technique through a few case studies. The results highlight vulnerabilities in current LLMs and underscore the need for more robust safeguards. This work contributes to the broader discourse on responsible AI (RAI) safety and adversarial testing, providing insights and practical examples for researchers and developers. This method is unexplored in the literature, making it a novel contribution to the field.         ",
    "url": "https://arxiv.org/abs/2409.03131",
    "authors": [
      "Alan Aqrawi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.03141",
    "title": "Towards Autonomous Cybersecurity: An Intelligent AutoML Framework for Autonomous Intrusion Detection",
    "abstract": "           The rapid evolution of mobile networks from 5G to 6G has necessitated the development of autonomous network management systems, such as Zero-Touch Networks (ZTNs). However, the increased complexity and automation of these networks have also escalated cybersecurity risks. Existing Intrusion Detection Systems (IDSs) leveraging traditional Machine Learning (ML) techniques have shown effectiveness in mitigating these risks, but they often require extensive manual effort and expert knowledge. To address these challenges, this paper proposes an Automated Machine Learning (AutoML)-based autonomous IDS framework towards achieving autonomous cybersecurity for next-generation networks. To achieve autonomous intrusion detection, the proposed AutoML framework automates all critical procedures of the data analytics pipeline, including data pre-processing, feature engineering, model selection, hyperparameter tuning, and model ensemble. Specifically, it utilizes a Tabular Variational Auto-Encoder (TVAE) method for automated data balancing, tree-based ML models for automated feature selection and base model learning, Bayesian Optimization (BO) for hyperparameter optimization, and a novel Optimized Confidence-based Stacking Ensemble (OCSE) method for automated model ensemble. The proposed AutoML-based IDS was evaluated on two public benchmark network security datasets, CICIDS2017 and 5G-NIDD, and demonstrated improved performance compared to state-of-the-art cybersecurity methods. This research marks a significant step towards fully autonomous cybersecurity in next-generation networks, potentially revolutionizing network security applications.         ",
    "url": "https://arxiv.org/abs/2409.03141",
    "authors": [
      "Li Yang",
      "Abdallah Shami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2409.03142",
    "title": "Causal Temporal Representation Learning with Nonstationary Sparse Transition",
    "abstract": "           Causal Temporal Representation Learning (Ctrl) methods aim to identify the temporal causal dynamics of complex nonstationary temporal sequences. Despite the success of existing Ctrl methods, they require either directly observing the domain variables or assuming a Markov prior on them. Such requirements limit the application of these methods in real-world scenarios when we do not have such prior knowledge of the domain variables. To address this problem, this work adopts a sparse transition assumption, aligned with intuitive human understanding, and presents identifiability results from a theoretical perspective. In particular, we explore under what conditions on the significance of the variability of the transitions we can build a model to identify the distribution shifts. Based on the theoretical result, we introduce a novel framework, Causal Temporal Representation Learning with Nonstationary Sparse Transition (CtrlNS), designed to leverage the constraints on transition sparsity and conditional independence to reliably identify both distribution shifts and latent factors. Our experimental evaluations on synthetic and real-world datasets demonstrate significant improvements over existing baselines, highlighting the effectiveness of our approach.         ",
    "url": "https://arxiv.org/abs/2409.03142",
    "authors": [
      "Xiangchen Song",
      "Zijian Li",
      "Guangyi Chen",
      "Yujia Zheng",
      "Yewen Fan",
      "Xinshuai Dong",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2409.03163",
    "title": "CyberDep: Towards the Analysis of Cyber-Physical Power System Interdependencies Using Bayesian Networks and Temporal Data",
    "abstract": "           Modern-day power systems have become increasingly cyber-physical due to the ongoing developments to the grid that include the rise of distributed energy generation and the increase of the deployment of many cyber devices for monitoring and control, such as the Supervisory Control and Data Acquisition (SCADA) system. Such capabilities have made the power system more vulnerable to cyber-attacks that can harm the physical components of the system. As such, it is of utmost importance to study both the physical and cyber components together, focusing on characterizing and quantifying the interdependency between these components. This paper focuses on developing an algorithm, named CyberDep, for Bayesian network generation through conditional probability calculations of cyber traffic flows between system nodes. Additionally, CyberDep is implemented on the temporal data of the cyber-physical emulation of the WSCC 9-bus power system. The results of this work provide a visual representation of the probabilistic relationships within the cyber and physical components of the system, aiding in cyber-physical interdependency quantification.         ",
    "url": "https://arxiv.org/abs/2409.03163",
    "authors": [
      "Leen Al Homoud",
      "Katherine Davis",
      "Shamina Hossain-McKenzie",
      "Nicholas Jacobs"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.03175",
    "title": "Data-based approaches to learning and control by similarity between heterogeneous systems",
    "abstract": "           This paper proposes basic definitions of similarity and similarity indexes between admissible behaviors of heterogeneous host and guest systems and further presents a similarity-based learning control framework by exploiting offline sampled data. By exploring helpful geometric properties of the admissible behavior and decomposing it into the subspace and offset components, the similarity indexes between two admissible behaviors are defined as the principal angles between their corresponding subspace components. By reconstructing the admissible behaviors leveraging sampled data, an efficient strategy for calculating the similarity indexes is developed, based on which a similarity-based learning control framework is proposed. It is shown that the host system can directly accomplish the same control tasks by utilizing the successful experience from the guest system, without having to undergo the trial-and-error process.         ",
    "url": "https://arxiv.org/abs/2409.03175",
    "authors": [
      "Chenchao Wang",
      "Deyuan Meng"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.03183",
    "title": "Bypassing DARCY Defense: Indistinguishable Universal Adversarial Triggers",
    "abstract": "           Neural networks (NN) classification models for Natural Language Processing (NLP) are vulnerable to the Universal Adversarial Triggers (UAT) attack that triggers a model to produce a specific prediction for any input. DARCY borrows the \"honeypot\" concept to bait multiple trapdoors, effectively detecting the adversarial examples generated by UAT. Unfortunately, we find a new UAT generation method, called IndisUAT, which produces triggers (i.e., tokens) and uses them to craft adversarial examples whose feature distribution is indistinguishable from that of the benign examples in a randomly-chosen category at the detection layer of DARCY. The produced adversarial examples incur the maximal loss of predicting results in the DARCY-protected models. Meanwhile, the produced triggers are effective in black-box models for text generation, text inference, and reading comprehension. Finally, the evaluation results under NN models for NLP tasks indicate that the IndisUAT method can effectively circumvent DARCY and penetrate other defenses. For example, IndisUAT can reduce the true positive rate of DARCY's detection by at least 40.8% and 90.6%, and drop the accuracy by at least 33.3% and 51.6% in the RNN and CNN models, respectively. IndisUAT reduces the accuracy of the BERT's adversarial defense model by at least 34.0%, and makes the GPT-2 language model spew racist outputs even when conditioned on non-racial context.         ",
    "url": "https://arxiv.org/abs/2409.03183",
    "authors": [
      "Zuquan Peng",
      "Yuanyuan He",
      "Jianbing Ni",
      "Ben Niu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.03187",
    "title": "How noise affects memory in linear recurrent networks",
    "abstract": "           The effects of noise on memory in a linear recurrent network are theoretically investigated. Memory is characterized by its ability to store previous inputs in its instantaneous state of network, which receives a correlated or uncorrelated noise. Two major properties are revealed: First, the memory reduced by noise is uniquely determined by the noise's power spectral density (PSD). Second, the memory will not decrease regardless of noise intensity if the PSD is in a certain class of distribution (including power law). The results are verified using the human brain signals, showing good agreement.         ",
    "url": "https://arxiv.org/abs/2409.03187",
    "authors": [
      "JingChuan Guan",
      "Tomoyuki Kubota",
      "Yasuo Kuniyoshi",
      "Kohei Nakajima"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2409.03203",
    "title": "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification",
    "abstract": "           Sentiment classification (SC) often suffers from low-resource challenges such as domain-specific contexts, imbalanced label distributions, and few-shot scenarios. The potential of the diffusion language model (LM) for textual data augmentation (DA) remains unexplored, moreover, textual DA methods struggle to balance the diversity and consistency of new samples. Most DA methods either perform logical modifications or rephrase less important tokens in the original sequence with the language model. In the context of SC, strong emotional tokens could act critically on the sentiment of the whole sequence. Therefore, contrary to rephrasing less important context, we propose DiffusionCLS to leverage a diffusion LM to capture in-domain knowledge and generate pseudo samples by reconstructing strong label-related tokens. This approach ensures a balance between consistency and diversity, avoiding the introduction of noise and augmenting crucial features of datasets. DiffusionCLS also comprises a Noise-Resistant Training objective to help the model generalize. Experiments demonstrate the effectiveness of our method in various low-resource scenarios including domain-specific and domain-general problems. Ablation studies confirm the effectiveness of our framework's modules, and visualization studies highlight optimal deployment conditions, reinforcing our conclusions.         ",
    "url": "https://arxiv.org/abs/2409.03203",
    "authors": [
      "Zhuowei Chen",
      "Lianxi Wang",
      "Yuben Wu",
      "Xinfeng Liao",
      "Yujia Tian",
      "Junyang Zhong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.03220",
    "title": "FairQuant: Certifying and Quantifying Fairness of Deep Neural Networks",
    "abstract": "           We propose a method for formally certifying and quantifying individual fairness of deep neural networks (DNN). Individual fairness guarantees that any two individuals who are identical except for a legally protected attribute (e.g., gender or race) receive the same treatment. While there are existing techniques that provide such a guarantee, they tend to suffer from lack of scalability or accuracy as the size and input dimension of the DNN increase. Our method overcomes this limitation by applying abstraction to a symbolic interval based analysis of the DNN followed by iterative refinement guided by the fairness property. Furthermore, our method lifts the symbolic interval based analysis from conventional qualitative certification to quantitative certification, by computing the percentage of individuals whose classification outputs are provably fair, instead of merely deciding if the DNN is fair. We have implemented our method and evaluated it on deep neural networks trained on four popular fairness research datasets. The experimental results show that our method is not only more accurate than state-of-the-art techniques but also several orders-of-magnitude faster.         ",
    "url": "https://arxiv.org/abs/2409.03220",
    "authors": [
      "Brian Hyeongseok Kim",
      "Jingbo Wang",
      "Chao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2409.03223",
    "title": "Why mamba is effective? Exploit Linear Transformer-Mamba Network for Multi-Modality Image Fusion",
    "abstract": "           Multi-modality image fusion aims to integrate the merits of images from different sources and render high-quality fusion images. However, existing feature extraction and fusion methods are either constrained by inherent local reduction bias and static parameters during inference (CNN) or limited by quadratic computational complexity (Transformers), and cannot effectively extract and fuse features. To solve this problem, we propose a dual-branch image fusion network called Tmamba. It consists of linear Transformer and Mamba, which has global modeling capabilities while maintaining linear complexity. Due to the difference between the Transformer and Mamba structures, the features extracted by the two branches carry channel and position information respectively. T-M interaction structure is designed between the two branches, using global learnable parameters and convolutional layers to transfer position and channel information respectively. We further propose cross-modal interaction at the attention level to obtain cross-modal attention. Experiments show that our Tmamba achieves promising results in multiple fusion tasks, including infrared-visible image fusion and medical image fusion. Code with checkpoints will be available after the peer-review process.         ",
    "url": "https://arxiv.org/abs/2409.03223",
    "authors": [
      "Chenguang Zhu",
      "Shan Gao",
      "Huafeng Chen",
      "Guangqian Guo",
      "Chaowei Wang",
      "Yaoxing Wang",
      "Chen Shu Lei",
      "Quanjiang Fan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03231",
    "title": "State-space models are accurate and efficient neural operators for dynamical systems",
    "abstract": "           Physics-informed machine learning (PIML) has emerged as a promising alternative to classical methods for predicting dynamical systems, offering faster and more generalizable solutions. However, existing models, including recurrent neural networks (RNNs), transformers, and neural operators, face challenges such as long-time integration, long-range dependencies, chaotic dynamics, and extrapolation, to name a few. To this end, this paper introduces state-space models implemented in Mamba for accurate and efficient dynamical system operator learning. Mamba addresses the limitations of existing architectures by dynamically capturing long-range dependencies and enhancing computational efficiency through reparameterization techniques. To extensively test Mamba and compare against another 11 baselines, we introduce several strict extrapolation testbeds that go beyond the standard interpolation benchmarks. We demonstrate Mamba's superior performance in both interpolation and challenging extrapolation tasks. Mamba consistently ranks among the top models while maintaining the lowest computational cost and exceptional extrapolation capabilities. Moreover, we demonstrate the good performance of Mamba for a real-world application in quantitative systems pharmacology for assessing the efficacy of drugs in tumor growth under limited data scenarios. Taken together, our findings highlight Mamba's potential as a powerful tool for advancing scientific machine learning in dynamical systems modeling. (The code will be available at this https URL upon acceptance.)         ",
    "url": "https://arxiv.org/abs/2409.03231",
    "authors": [
      "Zheyuan Hu",
      "Nazanin Ahmadi Daryakenari",
      "Qianli Shen",
      "Kenji Kawaguchi",
      "George Em Karniadakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2409.03236",
    "title": "Unveiling Context-Related Anomalies: Knowledge Graph Empowered Decoupling of Scene and Action for Human-Related Video Anomaly Detection",
    "abstract": "           Detecting anomalies in human-related videos is crucial for surveillance applications. Current methods primarily include appearance-based and action-based techniques. Appearance-based methods rely on low-level visual features such as color, texture, and shape. They learn a large number of pixel patterns and features related to known scenes during training, making them effective in detecting anomalies within these familiar contexts. However, when encountering new or significantly changed scenes, i.e., unknown scenes, they often fail because existing SOTA methods do not effectively capture the relationship between actions and their surrounding scenes, resulting in low generalization. In contrast, action-based methods focus on detecting anomalies in human actions but are usually less informative because they tend to overlook the relationship between actions and their scenes, leading to incorrect detection. For instance, the normal event of running on the beach and the abnormal event of running on the street might both be considered normal due to the lack of scene information. In short, current methods struggle to integrate low-level visual and high-level action features, leading to poor anomaly detection in varied and complex scenes. To address this challenge, we propose a novel decoupling-based architecture for human-related video anomaly detection (DecoAD). DecoAD significantly improves the integration of visual and action features through the decoupling and interweaving of scenes and actions, thereby enabling a more intuitive and accurate understanding of complex behaviors and scenes. DecoAD supports fully supervised, weakly supervised, and unsupervised settings.         ",
    "url": "https://arxiv.org/abs/2409.03236",
    "authors": [
      "Chenglizhao Chen",
      "Xinyu Liu",
      "Mengke Song",
      "Luming Li",
      "Xu Yu",
      "Shanchen Pang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03237",
    "title": "Robust Q-Learning under Corrupted Rewards",
    "abstract": "           Recently, there has been a surge of interest in analyzing the non-asymptotic behavior of model-free reinforcement learning algorithms. However, the performance of such algorithms in non-ideal environments, such as in the presence of corrupted rewards, is poorly understood. Motivated by this gap, we investigate the robustness of the celebrated Q-learning algorithm to a strong-contamination attack model, where an adversary can arbitrarily perturb a small fraction of the observed rewards. We start by proving that such an attack can cause the vanilla Q-learning algorithm to incur arbitrarily large errors. We then develop a novel robust synchronous Q-learning algorithm that uses historical reward data to construct robust empirical Bellman operators at each time step. Finally, we prove a finite-time convergence rate for our algorithm that matches known state-of-the-art bounds (in the absence of attacks) up to a small inevitable $O(\\varepsilon)$ error term that scales with the adversarial corruption fraction $\\varepsilon$. Notably, our results continue to hold even when the true reward distributions have infinite support, provided they admit bounded second moments.         ",
    "url": "https://arxiv.org/abs/2409.03237",
    "authors": [
      "Sreejeet Maity",
      "Aritra Mitra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2409.03239",
    "title": "DiffGrad for Physics-Informed Neural Networks",
    "abstract": "           Physics-Informed Neural Networks (PINNs) are regarded as state-of-the-art tools for addressing highly nonlinear problems based on partial differential equations. Despite their broad range of applications, PINNs encounter several performance challenges, including issues related to efficiency, minimization of computational cost, and enhancement of accuracy. Burgers' equation, a fundamental equation in fluid dynamics that is extensively used in PINNs, provides flexible results with the Adam optimizer that does not account for past gradients. This paper introduces a novel strategy for solving Burgers' equation by incorporating DiffGrad with PINNs, a method that leverages the difference between current and immediately preceding gradients to enhance performance. A comprehensive computational analysis is conducted using optimizers such as Adam, Adamax, RMSprop, and DiffGrad to evaluate and compare their effectiveness. Our approach includes visualizing the solutions over space at various time intervals to demonstrate the accuracy of the network. The results show that DiffGrad not only improves the accuracy of the solution but also reduces training time compared to the other optimizers.         ",
    "url": "https://arxiv.org/abs/2409.03239",
    "authors": [
      "Jamshaid Ul Rahman",
      "Nimra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2409.03243",
    "title": "Robust End-to-End Image Transmission with Residual Learning",
    "abstract": "           Recently, deep learning (DL) based image transmission at the physical layer (PL) has become a rising trend due to its ability to significantly outperform conventional separation-based digital transmissions. However, implementing solutions at the PL requires a major shift in established standards, such as those in cellular communications. Application layer (AL) solutions present a more feasible and standards-compliant alternative. In this work, we propose a layered image transmission scheme at the AL that is robust to end-to-end (E2E) channel errors. The base layer transmits a coarse image, while the enhancement layer transmits the residual between the original and coarse images. By mapping the residual image into a latent representation that aligns with the structure of the E2E channel, our proposed solution demonstrates high robustness to E2E channel errors.         ",
    "url": "https://arxiv.org/abs/2409.03243",
    "authors": [
      "Cenk M. Yetis"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2409.03252",
    "title": "Gr-IoU: Ground-Intersection over Union for Robust Multi-Object Tracking with 3D Geometric Constraints",
    "abstract": "           We propose a Ground IoU (Gr-IoU) to address the data association problem in multi-object tracking. When tracking objects detected by a camera, it often occurs that the same object is assigned different IDs in consecutive frames, especially when objects are close to each other or overlapping. To address this issue, we introduce Gr-IoU, which takes into account the 3D structure of the scene. Gr-IoU transforms traditional bounding boxes from the image space to the ground plane using the vanishing point geometry. The IoU calculated with these transformed bounding boxes is more sensitive to the front-to-back relationships of objects, thereby improving data association accuracy and reducing ID switches. We evaluated our Gr-IoU method on the MOT17 and MOT20 datasets, which contain diverse tracking scenarios including crowded scenes and sequences with frequent occlusions. Experimental results demonstrated that Gr-IoU outperforms conventional real-time methods without appearance features.         ",
    "url": "https://arxiv.org/abs/2409.03252",
    "authors": [
      "Keisuke Toida",
      "Naoki Kato",
      "Osamu Segawa",
      "Takeshi Nakamura",
      "Kazuhiro Hotta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03254",
    "title": "Granular-ball Representation Learning for Deep CNN on Learning with Label Noise",
    "abstract": "           In actual scenarios, whether manually or automatically annotated, label noise is inevitably generated in the training data, which can affect the effectiveness of deep CNN models. The popular solutions require data cleaning or designing additional optimizations to punish the data with mislabeled data, thereby enhancing the robustness of models. However, these methods come at the cost of weakening or even losing some data during the training process. As we know, content is the inherent attribute of an image that does not change with changes in annotations. In this study, we propose a general granular-ball computing (GBC) module that can be embedded into a CNN model, where the classifier finally predicts the label of granular-ball ($gb$) samples instead of each individual samples. Specifically, considering the classification task: (1) in forward process, we split the input samples as $gb$ samples at feature-level, each of which can correspond to multiple samples with varying numbers and share one single label; (2) during the backpropagation process, we modify the gradient allocation strategy of the GBC module to enable it to propagate normally; and (3) we develop an experience replay policy to ensure the stability of the training process. Experiments demonstrate that the proposed method can improve the robustness of CNN models with no additional data or optimization.         ",
    "url": "https://arxiv.org/abs/2409.03254",
    "authors": [
      "Dawei Dai",
      "Hao Zhu",
      "Shuyin Xia",
      "Guoyin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.03258",
    "title": "GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding",
    "abstract": "           Although Large Language Models (LLMs) have demonstrated potential in processing graphs, they struggle with comprehending graphical structure information through prompts of graph description sequences, especially as the graph size increases. We attribute this challenge to the uneven memory performance of LLMs across different positions in graph description sequences, known as ''positional biases''. To address this, we propose GraphInsight, a novel framework aimed at improving LLMs' comprehension of both macro- and micro-level graphical information. GraphInsight is grounded in two key strategies: 1) placing critical graphical information in positions where LLMs exhibit stronger memory performance, and 2) investigating a lightweight external knowledge base for regions with weaker memory performance, inspired by retrieval-augmented generation (RAG). Moreover, GraphInsight explores integrating these two strategies into LLM agent processes for composite graph tasks that require multi-step reasoning. Extensive empirical studies on benchmarks with a wide range of evaluation tasks show that GraphInsight significantly outperforms all other graph description methods (e.g., prompting techniques and reordering strategies) in understanding graph structures of varying sizes.         ",
    "url": "https://arxiv.org/abs/2409.03258",
    "authors": [
      "Yukun Cao",
      "Shuo Han",
      "Zengyi Gao",
      "Zezhong Ding",
      "Xike Xie",
      "S. Kevin Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.03267",
    "title": "No Man is an Island: Towards Fully Automatic Programming by Code Search, Code Generation and Program Repair",
    "abstract": "           Automatic programming attempts to minimize human intervention in the generation of executable code, and has been a long-standing challenge in the software engineering community. To advance automatic programming, researchers are focusing on three primary directions: (1) code search that reuses existing code snippets from external databases; (2) code generation that produces new code snippets from natural language; and (3) program repair that refines existing code snippets by fixing detected bugs. Despite significant advancements, the effectiveness of state-of-the-art techniques is still limited, such as the usability of searched code and the correctness of generated code. Motivated by the real-world programming process, where developers usually use various external tools to aid their coding processes, such as code search engines and code testing tools, in this work, we propose \\toolname{}, an automatic programming framework that leverages recent large language models (LLMs) to integrate the three research areas to address their inherent limitations. In particular, our framework first leverages different code search strategies to retrieve similar code snippets, which are then used to further guide the code generation process of LLMs. Our framework further validates the quality of generated code by compilers and test cases, and constructs repair prompts to query LLMs for generating correct patches. We conduct preliminary experiments to demonstrate the potential of our framework, \\eg helping CodeLlama solve 267 programming problems with an improvement of 62.53\\%. As a generic framework, \\toolname{} can integrate various code search, generation, and repair tools, combining these three research areas together for the first time. More importantly, it demonstrates the potential of using traditional SE tools to enhance the usability of LLMs in automatic programming.         ",
    "url": "https://arxiv.org/abs/2409.03267",
    "authors": [
      "Quanjun Zhang",
      "Chunrong Fang",
      "Ye Shang",
      "Tongke Zhang",
      "Shengcheng Yu",
      "Zhenyu Chen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2409.03273",
    "title": "Robust synchronization and policy adaptation for networked heterogeneous agents",
    "abstract": "           We propose a robust adaptive online synchronization method for leader-follower networks of nonlinear heterogeneous agents with system uncertainties and input magnitude saturation. Synchronization is achieved using a Distributed input Magnitude Saturation Adaptive Control with Reinforcement Learning (DMSAC-RL), which improves the empirical performance of policies trained on off-the-shelf models using Reinforcement Learning (RL) strategies. The leader observes the performance of a reference model, and followers observe the states and actions of the agents they are connected to, but not the reference model. The leader and followers may differ from the reference model in which the RL control policy was trained. DMSAC-RL uses an internal loop that adjusts the learned policy for the agents in the form of augmented input to solve the distributed control problem, including input-matched uncertainty parameters. We show that the synchronization error of the heterogeneous network is Uniformly Ultimately Bounded (UUB). Numerical analysis of a network of Multiple Input Multiple Output (MIMO) systems supports our theoretical findings.         ",
    "url": "https://arxiv.org/abs/2409.03273",
    "authors": [
      "Miguel F. Arevalo-Castiblanco",
      "Eduardo Mojica-Nava and",
      "C\u00e9sar A. Uribe"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.03274",
    "title": "Recent Advances in Attack and Defense Approaches of Large Language Models",
    "abstract": "           Large Language Models (LLMs) have revolutionized artificial intelligence and machine learning through their advanced text processing and generating capabilities. However, their widespread deployment has raised significant safety and reliability concerns. Established vulnerabilities in deep neural networks, coupled with emerging threat models, may compromise security evaluations and create a false sense of security. Given the extensive research in the field of LLM security, we believe that summarizing the current state of affairs will help the research community better understand the present landscape and inform future developments. This paper reviews current research on LLM vulnerabilities and threats, and evaluates the effectiveness of contemporary defense mechanisms. We analyze recent studies on attack vectors and model weaknesses, providing insights into attack mechanisms and the evolving threat landscape. We also examine current defense strategies, highlighting their strengths and limitations. By contrasting advancements in attack and defense methodologies, we identify research gaps and propose future directions to enhance LLM security. Our goal is to advance the understanding of LLM safety challenges and guide the development of more robust security measures.         ",
    "url": "https://arxiv.org/abs/2409.03274",
    "authors": [
      "Jing Cui",
      "Yishi Xu",
      "Zhewei Huang",
      "Shuchang Zhou",
      "Jianbin Jiao",
      "Junge Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.03276",
    "title": "Tensor network square root Kalman filter for online Gaussian process regression",
    "abstract": "           The state-of-the-art tensor network Kalman filter lifts the curse of dimensionality for high-dimensional recursive estimation problems. However, the required rounding operation can cause filter divergence due to the loss of positive definiteness of covariance matrices. We solve this issue by developing, for the first time, a tensor network square root Kalman filter, and apply it to high-dimensional online Gaussian process regression. In our experiments, we demonstrate that our method is equivalent to the conventional Kalman filter when choosing a full-rank tensor network. Furthermore, we apply our method to a real-life system identification problem where we estimate $4^{14}$ parameters on a standard laptop. The estimated model outperforms the state-of-the-art tensor network Kalman filter in terms of prediction accuracy and uncertainty quantification.         ",
    "url": "https://arxiv.org/abs/2409.03276",
    "authors": [
      "Clara Menzen",
      "Manon Kok",
      "Kim Batselier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03282",
    "title": "Interpretable mixture of experts for time series prediction under recurrent and non-recurrent conditions",
    "abstract": "           Non-recurrent conditions caused by incidents are different from recurrent conditions that follow periodic patterns. Existing traffic speed prediction studies are incident-agnostic and use one single model to learn all possible patterns from these drastically diverse conditions. This study proposes a novel Mixture of Experts (MoE) model to improve traffic speed prediction under two separate conditions, recurrent and non-recurrent (i.e., with and without incidents). The MoE leverages separate recurrent and non-recurrent expert models (Temporal Fusion Transformers) to capture the distinct patterns of each traffic condition. Additionally, we propose a training pipeline for non-recurrent models to remedy the limited data issues. To train our model, multi-source datasets, including traffic speed, incident reports, and weather data, are integrated and processed to be informative features. Evaluations on a real road network demonstrate that the MoE achieves lower errors compared to other benchmark algorithms. The model predictions are interpreted in terms of temporal dependencies and variable importance in each condition separately to shed light on the differences between recurrent and non-recurrent conditions.         ",
    "url": "https://arxiv.org/abs/2409.03282",
    "authors": [
      "Zemian Ke",
      "Haocheng Duan",
      "Sean Qian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2409.03284",
    "title": "iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models",
    "abstract": "           Most available data is unstructured, making it challenging to access valuable information. Automatically building Knowledge Graphs (KGs) is crucial for structuring data and making it accessible, allowing users to search for information effectively. KGs also facilitate insights, inference, and reasoning. Traditional NLP methods, such as named entity recognition and relation extraction, are key in information retrieval but face limitations, including the use of predefined entity types and the need for supervised learning. Current research leverages large language models' capabilities, such as zero- or few-shot learning. However, unresolved and semantically duplicated entities and relations still pose challenges, leading to inconsistent graphs and requiring extensive post-processing. Additionally, most approaches are topic-dependent. In this paper, we propose iText2KG, a method for incremental, topic-independent KG construction without post-processing. This plug-and-play, zero-shot method is applicable across a wide range of KG construction scenarios and comprises four modules: Document Distiller, Incremental Entity Extractor, Incremental Relation Extractor, and Graph Integrator and Visualization. Our method demonstrates superior performance compared to baseline methods across three scenarios: converting scientific papers to graphs, websites to graphs, and CVs to graphs.         ",
    "url": "https://arxiv.org/abs/2409.03284",
    "authors": [
      "Yassir Lairgi",
      "Ludovic Moncla",
      "R\u00e9my Cazabet",
      "Khalid Benabdeslem",
      "Pierre Cl\u00e9au"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2409.03295",
    "title": "N-gram Prediction and Word Difference Representations for Language Modeling",
    "abstract": "           Causal language modeling (CLM) serves as the foundational framework underpinning remarkable successes of recent large language models (LLMs). Despite its success, the training approach for next word prediction poses a potential risk of causing the model to overly focus on local dependencies within a sentence. While prior studies have been introduced to predict future N words simultaneously, they were primarily applied to tasks such as masked language modeling (MLM) and neural machine translation (NMT). In this study, we introduce a simple N-gram prediction framework for the CLM task. Moreover, we introduce word difference representation (WDR) as a surrogate and contextualized target representation during model training on the basis of N-gram prediction framework. To further enhance the quality of next word prediction, we propose an ensemble method that incorporates the future N words' prediction results. Empirical evaluations across multiple benchmark datasets encompassing CLM and NMT tasks demonstrate the significant advantages of our proposed methods over the conventional CLM.         ",
    "url": "https://arxiv.org/abs/2409.03295",
    "authors": [
      "DongNyeong Heo",
      "Daniela Noemi Rim",
      "Heeyoul Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.03303",
    "title": "Improving Robustness to Multiple Spurious Correlations by Multi-Objective Optimization",
    "abstract": "           We study the problem of training an unbiased and accurate model given a dataset with multiple biases. This problem is challenging since the multiple biases cause multiple undesirable shortcuts during training, and even worse, mitigating one may exacerbate the other. We propose a novel training method to tackle this challenge. Our method first groups training data so that different groups induce different shortcuts, and then optimizes a linear combination of group-wise losses while adjusting their weights dynamically to alleviate conflicts between the groups in performance; this approach, rooted in the multi-objective optimization theory, encourages to achieve the minimax Pareto solution. We also present a new benchmark with multiple biases, dubbed MultiCelebA, for evaluating debiased training methods under realistic and challenging scenarios. Our method achieved the best on three datasets with multiple biases, and also showed superior performance on conventional single-bias datasets.         ",
    "url": "https://arxiv.org/abs/2409.03303",
    "authors": [
      "Nayeong Kim",
      "Juwon Kang",
      "Sungsoo Ahn",
      "Jungseul Ok",
      "Suha Kwak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03320",
    "title": "YOLO-PPA based Efficient Traffic Sign Detection for Cruise Control in Autonomous Driving",
    "abstract": "           It is very important to detect traffic signs efficiently and accurately in autonomous driving systems. However, the farther the distance, the smaller the traffic signs. Existing object detection algorithms can hardly detect these small scaled this http URL addition, the performance of embedded devices on vehicles limits the scale of detection this http URL address these challenges, a YOLO PPA based traffic sign detection algorithm is proposed in this paper.The experimental results on the GTSDB dataset show that compared to the original YOLO, the proposed method improves inference efficiency by 11.2%. The mAP 50 is also improved by 93.2%, which demonstrates the effectiveness of the proposed YOLO PPA.         ",
    "url": "https://arxiv.org/abs/2409.03320",
    "authors": [
      "Jingyu Zhang",
      "Wenqing Zhang",
      "Chaoyi Tan",
      "Xiangtian Li",
      "Qianyi Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.03326",
    "title": "Enhancing User-Centric Privacy Protection: An Interactive Framework through Diffusion Models and Machine Unlearning",
    "abstract": "           In the realm of multimedia data analysis, the extensive use of image datasets has escalated concerns over privacy protection within such data. Current research predominantly focuses on privacy protection either in data sharing or upon the release of trained machine learning models. Our study pioneers a comprehensive privacy protection framework that safeguards image data privacy concurrently during data sharing and model publication. We propose an interactive image privacy protection framework that utilizes generative machine learning models to modify image information at the attribute level and employs machine unlearning algorithms for the privacy preservation of model parameters. This user-interactive framework allows for adjustments in privacy protection intensity based on user feedback on generated images, striking a balance between maximal privacy safeguarding and maintaining model performance. Within this framework, we instantiate two modules: a differential privacy diffusion model for protecting attribute information in images and a feature unlearning algorithm for efficient updates of the trained model on the revised image dataset. Our approach demonstrated superiority over existing methods on facial datasets across various attribute classifications.         ",
    "url": "https://arxiv.org/abs/2409.03326",
    "authors": [
      "Huaxi Huang",
      "Xin Yuan",
      "Qiyu Liao",
      "Dadong Wang",
      "Tongliang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03328",
    "title": "Pareto Set Prediction Assisted Bilevel Multi-objective Optimization",
    "abstract": "           Bilevel optimization problems comprise an upper level optimization task that contains a lower level optimization task as a constraint. While there is a significant and growing literature devoted to solving bilevel problems with single objective at both levels using evolutionary computation, there is relatively scarce work done to address problems with multiple objectives (BLMOP) at both levels. For black-box BLMOPs, the existing evolutionary techniques typically utilize nested search, which in its native form consumes large number of function evaluations. In this work, we propose to reduce this expense by predicting the lower level Pareto set for a candidate upper level solution directly, instead of conducting an optimization from scratch. Such a prediction is significantly challenging for BLMOPs as it involves one-to-many mapping scenario. We resolve this bottleneck by supplementing the dataset using a helper variable and construct a neural network, which can then be trained to map the variables in a meaningful manner. Then, we embed this initialization within a bilevel optimization framework, termed Pareto set prediction assisted evolutionary bilevel multi-objective optimization (PSP-BLEMO). Systematic experiments with existing state-of-the-art methods are presented to demonstrate its benefit. The experiments show that the proposed approach is competitive across a range of problems, including both deceptive and non-deceptive problems         ",
    "url": "https://arxiv.org/abs/2409.03328",
    "authors": [
      "Bing Wang",
      "Hemant K. Singh",
      "Tapabrata Ray"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2409.03403",
    "title": "RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning",
    "abstract": "           Scaling up robot learning requires large and diverse datasets, and how to efficiently reuse collected data and transfer policies to new embodiments remains an open question. Emerging research such as the Open-X Embodiment (OXE) project has shown promise in leveraging skills by combining datasets including different robots. However, imbalances in the distribution of robot types and camera angles in many datasets make policies prone to overfit. To mitigate this issue, we propose RoVi-Aug, which leverages state-of-the-art image-to-image generative models to augment robot data by synthesizing demonstrations with different robots and camera views. Through extensive physical experiments, we show that, by training on robot- and viewpoint-augmented data, RoVi-Aug can zero-shot deploy on an unseen robot with significantly different camera angles. Compared to test-time adaptation algorithms such as Mirage, RoVi-Aug requires no extra processing at test time, does not assume known camera angles, and allows policy fine-tuning. Moreover, by co-training on both the original and augmented robot datasets, RoVi-Aug can learn multi-robot and multi-task policies, enabling more efficient transfer between robots and skills and improving success rates by up to 30%.         ",
    "url": "https://arxiv.org/abs/2409.03403",
    "authors": [
      "Lawrence Yunliang Chen",
      "Chenfeng Xu",
      "Karthik Dharmarajan",
      "Zubair Irshad",
      "Richard Cheng",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Quan Vuong",
      "Ken Goldberg"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.03424",
    "title": "Weight Conditioning for Smooth Optimization of Neural Networks",
    "abstract": "           In this article, we introduce a novel normalization technique for neural network weight matrices, which we term weight conditioning. This approach aims to narrow the gap between the smallest and largest singular values of the weight matrices, resulting in better-conditioned matrices. The inspiration for this technique partially derives from numerical linear algebra, where well-conditioned matrices are known to facilitate stronger convergence results for iterative solvers. We provide a theoretical foundation demonstrating that our normalization technique smoothens the loss landscape, thereby enhancing convergence of stochastic gradient descent algorithms. Empirically, we validate our normalization across various neural network architectures, including Convolutional Neural Networks (CNNs), Vision Transformers (ViT), Neural Radiance Fields (NeRF), and 3D shape modeling. Our findings indicate that our normalization method is not only competitive but also outperforms existing weight normalization techniques from the literature.         ",
    "url": "https://arxiv.org/abs/2409.03424",
    "authors": [
      "Hemanth Saratchandran",
      "Thomas X. Wang",
      "Simon Lucey"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03433",
    "title": "An innovation-based cycle-slip, multipath estimation, detection and mitigation method for tightly coupled GNSS/INS/Vision navigation in urban areas",
    "abstract": "           Precise, consistent, and reliable positioning is crucial for a multitude of uses. In order to achieve high precision global positioning services, multi-sensor fusion techniques, such as the Global Navigation Satellite System (GNSS)/Inertial Navigation System (INS)/Vision integration system, combine the strengths of various sensors. This technique is essential for localization in complex environments and has been widely used in the mass market. However, frequent signal deterioration and blocking in urban environments exacerbates the degradation of GNSS positioning and negatively impacts the performance of the multi-sensor integration system. For GNSS pseudorange and carrier phase observation data in the urban environment, we offer an innovation-based cycle slip/multipath estimation, detection, and mitigation (I-EDM) method to reduce the influence of multipath effects and cycle slips on location induced by obstruction in urban settings. The method obtains the innovations of GNSS observations with the cluster analysis method. Then the innovations are used to detect the cycle slips and multipath. Compared with the residual-based method, the innovation-based method avoids the residual overfitting caused by the least square method, resulting in better detection of outliers within the GNSS observations. The vehicle tests carried out in urban settings verify the proposed approach. Experimental results indicate that the accuracy of 0.23m, 0.11m, and 0.31m in the east, north and up components can be achieved by the GNSS/INS/Vision tightly coupled system with the I-EDM method, which has a maximum of 21.6% improvement when compared with the residual-based EDM (R-EDM) method.         ",
    "url": "https://arxiv.org/abs/2409.03433",
    "authors": [
      "Bo Xu",
      "Shoujian Zhang",
      "Jingrong Wang",
      "Jiancheng Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.03445",
    "title": "Neural HD Map Generation from Multiple Vectorized Tiles Locally Produced by Autonomous Vehicles",
    "abstract": "           High-definition (HD) map is a fundamental component of autonomous driving systems, as it can provide precise environmental information about driving scenes. Recent work on vectorized map generation could produce merely 65% local map elements around the ego-vehicle at runtime by one tour with onboard sensors, leaving a puzzle of how to construct a global HD map projected in the world coordinate system under high-quality standards. To address the issue, we present GNMap as an end-to-end generative neural network to automatically construct HD maps with multiple vectorized tiles which are locally produced by autonomous vehicles through several tours. It leverages a multi-layer and attention-based autoencoder as the shared network, of which parameters are learned from two different tasks (i.e., pretraining and finetuning, respectively) to ensure both the completeness of generated maps and the correctness of element categories. Abundant qualitative evaluations are conducted on a real-world dataset and experimental results show that GNMap can surpass the SOTA method by more than 5% F1 score, reaching the level of industrial usage with a small amount of manual modification. We have already deployed it at Navinfo Co., Ltd., serving as an indispensable software to automatically build HD maps for autonomous driving systems.         ",
    "url": "https://arxiv.org/abs/2409.03445",
    "authors": [
      "Miao Fan",
      "Yi Yao",
      "Jianping Zhang",
      "Xiangbo Song",
      "Daihui Wu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.03458",
    "title": "Non-Uniform Illumination Attack for Fooling Convolutional Neural Networks",
    "abstract": "           Convolutional Neural Networks (CNNs) have made remarkable strides; however, they remain susceptible to vulnerabilities, particularly in the face of minor image perturbations that humans can easily recognize. This weakness, often termed as 'attacks', underscores the limited robustness of CNNs and the need for research into fortifying their resistance against such manipulations. This study introduces a novel Non-Uniform Illumination (NUI) attack technique, where images are subtly altered using varying NUI masks. Extensive experiments are conducted on widely-accepted datasets including CIFAR10, TinyImageNet, and CalTech256, focusing on image classification with 12 different NUI attack models. The resilience of VGG, ResNet, MobilenetV3-small and InceptionV3 models against NUI attacks are evaluated. Our results show a substantial decline in the CNN models' classification accuracy when subjected to NUI attacks, indicating their vulnerability under non-uniform illumination. To mitigate this, a defense strategy is proposed, including NUI-attacked images, generated through the new NUI transformation, into the training set. The results demonstrate a significant enhancement in CNN model performance when confronted with perturbed images affected by NUI attacks. This strategy seeks to bolster CNN models' resilience against NUI attacks.         ",
    "url": "https://arxiv.org/abs/2409.03458",
    "authors": [
      "Akshay Jain",
      "Shiv Ram Dubey",
      "Satish Kumar Singh",
      "KC Santosh",
      "Bidyut Baran Chaudhuri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03460",
    "title": "LowFormer: Hardware Efficient Design for Convolutional Transformer Backbones",
    "abstract": "           Research in efficient vision backbones is evolving into models that are a mixture of convolutions and transformer blocks. A smart combination of both, architecture-wise and component-wise is mandatory to excel in the speedaccuracy trade-off. Most publications focus on maximizing accuracy and utilize MACs (multiply accumulate operations) as an efficiency metric. The latter however often do not measure accurately how fast a model actually is due to factors like memory access cost and degree of parallelism. We analyzed common modules and architectural design choices for backbones not in terms of MACs, but rather in actual throughput and latency, as the combination of the latter two is a better representation of the efficiency of models in real applications. We applied the conclusions taken from that analysis to create a recipe for increasing hardware-efficiency in macro design. Additionally we introduce a simple slimmed-down version of MultiHead Self-Attention, that aligns with our analysis. We combine both macro and micro design to create a new family of hardware-efficient backbone networks called LowFormer. LowFormer achieves a remarkable speedup in terms of throughput and latency, while achieving similar or better accuracy than current state-of-the-art efficient backbones. In order to prove the generalizability of our hardware-efficient design, we evaluate our method on GPU, mobile GPU and ARM CPU. We further show that the downstream tasks object detection and semantic segmentation profit from our hardware-efficient architecture. Code and models are available at this https URL altair199797/LowFormer.         ",
    "url": "https://arxiv.org/abs/2409.03460",
    "authors": [
      "Moritz Nottebaum",
      "Matteo Dunnhofer",
      "Christian Micheloni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03463",
    "title": "Characterizing Massive Activations of Attention Mechanism in Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNNs) have become increasingly popular for effectively modeling data with graph structures. Recently, attention mechanisms have been integrated into GNNs to improve their ability to capture complex patterns. This paper presents the first comprehensive study revealing a critical, unexplored consequence of this integration: the emergence of Massive Activations (MAs) within attention layers. We introduce a novel method for detecting and analyzing MAs, focusing on edge features in different graph transformer architectures. Our study assesses various GNN models using benchmark datasets, including ZINC, TOX21, and PROTEINS. Key contributions include (1) establishing the direct link between attention mechanisms and MAs generation in GNNs, (2) developing a robust definition and detection method for MAs based on activation ratio distributions, (3) introducing the Explicit Bias Term (EBT) as a potential countermeasure and exploring it as an adversarial framework to assess models robustness based on the presence or absence of MAs. Our findings highlight the prevalence and impact of attention-induced MAs across different architectures, such as GraphTransformer, GraphiT, and SAN. The study reveals the complex interplay between attention mechanisms, model architecture, dataset characteristics, and MAs emergence, providing crucial insights for developing more robust and reliable graph models.         ",
    "url": "https://arxiv.org/abs/2409.03463",
    "authors": [
      "Lorenzo Bini",
      "Marco Sorbi",
      "Stephane Marchand-Maillet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.03504",
    "title": "HGAMN: Heterogeneous Graph Attention Matching Network for Multilingual POI Retrieval at Baidu Maps",
    "abstract": "           The increasing interest in international travel has raised the demand of retrieving point of interests in multiple languages. This is even superior to find local venues such as restaurants and scenic spots in unfamiliar languages when traveling abroad. Multilingual POI retrieval, enabling users to find desired POIs in a demanded language using queries in numerous languages, has become an indispensable feature of today's global map applications such as Baidu Maps. This task is non-trivial because of two key challenges: (1) visiting sparsity and (2) multilingual query-POI matching. To this end, we propose a Heterogeneous Graph Attention Matching Network (HGAMN) to concurrently address both challenges. Specifically, we construct a heterogeneous graph that contains two types of nodes: POI node and query node using the search logs of Baidu Maps. To alleviate challenge \\#1, we construct edges between different POI nodes to link the low-frequency POIs with the high-frequency ones, which enables the transfer of knowledge from the latter to the former. To mitigate challenge \\#2, we construct edges between POI and query nodes based on the co-occurrences between queries and POIs, where queries in different languages and formulations can be aggregated for individual POIs. Moreover, we develop an attention-based network to jointly learn node representations of the heterogeneous graph and further design a cross-attention module to fuse the representations of both types of nodes for query-POI relevance scoring. Extensive experiments conducted on large-scale real-world datasets from Baidu Maps demonstrate the superiority and effectiveness of HGAMN. In addition, HGAMN has already been deployed in production at Baidu Maps, and it successfully keeps serving hundreds of millions of requests every day.         ",
    "url": "https://arxiv.org/abs/2409.03504",
    "authors": [
      "Jizhou Huang",
      "Haifeng Wang",
      "Yibo Sun",
      "Miao Fan",
      "Zhengjie Huang",
      "Chunyuan Yuan",
      "Yawen Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2409.03543",
    "title": "Prediction Accuracy & Reliability: Classification and Object Localization under Distribution Shift",
    "abstract": "           Natural distribution shift causes a deterioration in the perception performance of convolutional neural networks (CNNs). This comprehensive analysis for real-world traffic data addresses: 1) investigating the effect of natural distribution shift and weather augmentations on both detection quality and confidence estimation, 2) evaluating model performance for both classification and object localization, and 3) benchmarking two common uncertainty quantification methods - Ensembles and different variants of Monte-Carlo (MC) Dropout - under natural and close-to-natural distribution shift. For this purpose, a novel dataset has been curated from publicly available autonomous driving datasets. The in-distribution (ID) data is based on cutouts of a single object, for which both class and bounding box annotations are available. The six distribution-shift datasets cover adverse weather scenarios, simulated rain and fog, corner cases, and out-of-distribution data. A granular analysis of CNNs under distribution shift allows to quantize the impact of different types of shifts on both, task performance and confidence estimation: ConvNeXt-Tiny is more robust than EfficientNet-B0; heavy rain degrades classification stronger than localization, contrary to heavy fog; integrating MC-Dropout into selected layers only has the potential to enhance task performance and confidence estimation, whereby the identification of these layers depends on the type of distribution shift and the considered task.         ",
    "url": "https://arxiv.org/abs/2409.03543",
    "authors": [
      "Fabian Diet",
      "Moussa Kassem Sbeyti",
      "Michelle Karg"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03544",
    "title": "CTMBIDS: Convolutional Tsetlin Machine Based Intrusion Detection System for DDoS attacks in an SDN environment",
    "abstract": "           Software Defined Networks (SDN) face many security challenges today. A great deal of research has been done within the field of Intrusion Detection Systems (IDS) in these networks. Yet, numerous approaches still rely on deep learning algorithms. These algorithms suffer from complexity in implementation, high processing power and high memory consumption. In addition to security issues, firstly, the number of datasets that are based on SDN protocols are very small. Secondly, the ones that are available encompass numerous attacks in the network and do not focus on a single attack. For this reason, to introduce an SDN-based IDS with a focus on Distributed Denial of Service (DDoS) attacks, it is necessary to generate a DDoS-oriented dataset whose features can train a high-quality IDS. In this work, in order to address two important challenges in SDNs, initially, we generate three DDoS attack datasets based on three common and different network topologies. In the second step, using the Convolutional Tsetlin Machine (CTM), we introduce a lightweight IDS for DDoS attack dubbed CTMBIDS. The lightweight nature of the CTMBIDS stems from its low memory consumption and also its interpretability compared to the existing complex deep learning models. The low usage of system resources for the CTMBIDS makes it an ideal choice for an optimal software that consumes the SDN controllers least amount of memory. Also, in order to ascertain the quality of the generated datasets, we compare the CTMBIDS empirical results with the DDoS attacks of the KDDCup99 benchmark dataset as well. Since the main focus of this work is on a lightweight IDS, the results show the CTMBIDS performs much more efficiently than deep learning based approaches. Furthermore, the results also show in most datasets, the proposed method has relatively equal or better accuracy and also consumes much less memory than the existing methods.         ",
    "url": "https://arxiv.org/abs/2409.03544",
    "authors": [
      "Rasoul Jafari Gohari",
      "Laya Aliahmadipour",
      "Marjan Kuchaki Rafsanjani"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.03547",
    "title": "A Silicon Photonic Neural Network for Chromatic Dispersion Compensation in 20 Gbps PAM4 Signal at 125 km and Its Scalability up to 100 Gbps",
    "abstract": "           A feed-forward photonic neural network (PNN) is tested for chromatic dispersion compensation in Intensity Modulation/Direct Detection optical links. The PNN is based on a sequence of linear and nonlinear transformations. The linear stage is constituted by an 8-tap time-delayed complex perceptron implemented on a Silicon-On-insulator platform and acting as a tunable optical filter. The nonlinear stage is provided by the square modulus of the electrical field applied at the end-of-line photodetector. The training maximizes the separation between the optical levels (i.e. the eye diagram aperture), with consequent reduction of the Bit Error Rate. Effective equalization is experimentally demonstrated for 20 Gbps 4-level Pulse Amplitude Modulated signal up to 125 km. An evolutionary algorithm and a gradient-based approach are tested for the training and then compared in terms of repeatability and convergence time. The optimal weights resulting from the training are interpreted in light of the theoretical transfer function of the optical fiber. Finally, a simulative study proves the scalability of the layout to larger bandwidths, up to 100 Gbps.         ",
    "url": "https://arxiv.org/abs/2409.03547",
    "authors": [
      "Emiliano Staffoli",
      "Gianpietro Maddinelli",
      "Lorenzo Pavesi"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Applied Physics (physics.app-ph)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2409.03553",
    "title": "Organized Grouped Discrete Representation for Object-Centric Learning",
    "abstract": "           Object-Centric Learning (OCL) represents dense image or video pixels as sparse object features. Representative methods utilize discrete representation composed of Variational Autoencoder (VAE) template features to suppress pixel-level information redundancy and guide object-level feature aggregation. The most recent advancement, Grouped Discrete Representation (GDR), further decomposes these template features into attributes. However, its naive channel grouping as decomposition may erroneously group channels belonging to different attributes together and discretize them as sub-optimal template attributes, which losses information and harms expressivity. We propose Organized GDR (OGDR) to organize channels belonging to the same attributes together for correct decomposition from features into attributes. In unsupervised segmentation experiments, OGDR is fully superior to GDR in augmentating classical transformer-based OCL methods; it even improves state-of-the-art diffusion-based ones. Codebook PCA and representation similarity analyses show that compared with GDR, our OGDR eliminates redundancy and preserves information better for guiding object representation learning. The source code is available in the supplementary material.         ",
    "url": "https://arxiv.org/abs/2409.03553",
    "authors": [
      "Rongzhen Zhao",
      "Vivienne Wang",
      "Juho Kannala",
      "Joni Pajarinen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03555",
    "title": "Unified Framework for Neural Network Compression via Decomposition and Optimal Rank Selection",
    "abstract": "           Despite their high accuracy, complex neural networks demand significant computational resources, posing challenges for deployment on resource-constrained devices such as mobile phones and embedded systems. Compression algorithms have been developed to address these challenges by reducing model size and computational demands while maintaining accuracy. Among these approaches, factorization methods based on tensor decomposition are theoretically sound and effective. However, they face difficulties in selecting the appropriate rank for decomposition. This paper tackles this issue by presenting a unified framework that simultaneously applies decomposition and optimal rank selection, employing a composite compression loss within defined rank constraints. Our approach includes an automatic rank search in a continuous space, efficiently identifying optimal rank configurations without the use of training data, making it computationally efficient. Combined with a subsequent fine-tuning step, our approach maintains the performance of highly compressed models on par with their original counterparts. Using various benchmark datasets, we demonstrate the efficacy of our method through a comprehensive analysis.         ",
    "url": "https://arxiv.org/abs/2409.03555",
    "authors": [
      "Ali Aghababaei-Harandi",
      "Massih-Reza Amini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03579",
    "title": "Disjoint Compatibility via Graph Classes",
    "abstract": "           Two plane drawings of graphs on the same set of points are called disjoint compatible if their union is plane and they do not have an edge in common. Let $S$ be a convex point set of $2n \\geq 10$ points and let $\\mathcal{H}$ be a family of plane drawings on $S$. Two plane perfect matchings $M_1$ and $M_2$ on $S$ (which do not need to be disjoint nor compatible) are \\emph{disjoint $\\mathcal{H}$-compatible} if there exists a drawing in $\\mathcal{H}$ which is disjoint compatible to both $M_1$ and $M_2$ In this work, we consider the graph which has all plane perfect matchings as vertices and where two vertices are connected by an edge if the matchings are disjoint $\\mathcal{H}$-compatible. We study the diameter of this graph when $\\mathcal{H}$ is the family of all plane spanning trees, caterpillars or paths. We show that in the first two cases the graph is connected with constant and linear diameter, respectively, while in the third case it is disconnected.         ",
    "url": "https://arxiv.org/abs/2409.03579",
    "authors": [
      "Oswin Aichholzer",
      "Julia Obmann",
      "Pavel Pat\u00e1k",
      "Daniel Perz",
      "Josef Tkadlec",
      "Birgit Vogtenhuber"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2409.03594",
    "title": "A Complete Landscape of EFX Allocations of Mixed Manna on Graphs",
    "abstract": "           We study envy-free up to any item (EFX) allocations on graphs where vertices and edges represent agents and items respectively. An agent is only interested in items that are incident to her and all other items have zero marginal values to her. Christodoulou et al. [EC, 2023] first proposed this setting and studied the case of goods. We extend this setting to the case of mixed manna where an item may be liked or disliked by its endpoint agents. In our problem, an agent has an arbitrary valuation over her incident items such that the items she likes have non-negative marginal values to her and those she dislikes have non-positive marginal values. We provide a complete study of the four notions of EFX for mixed manna in the literature, which differ by whether the removed item can have zero marginal value. We prove that an allocation that satisfies the notion of EFX where the virtually-removed item could always have zero marginal value may not exist and determining its existence is NP-complete, while one that satisfies any of the other three notions always exists and can be computed in polynomial time. We also prove that an orientation (i.e., a special allocation where each edge must be allocated to one of its endpoint agents) that satisfies any of the four notions may not exist, and determining its existence is NP-complete.         ",
    "url": "https://arxiv.org/abs/2409.03594",
    "authors": [
      "Yu Zhou",
      "Tianze Wei",
      "Minming Li",
      "Bo Li"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2409.03598",
    "title": "A practical approach to evaluating the adversarial distance for machine learning classifiers",
    "abstract": "           Robustness is critical for machine learning (ML) classifiers to ensure consistent performance in real-world applications where models may encounter corrupted or adversarial inputs. In particular, assessing the robustness of classifiers to adversarial inputs is essential to protect systems from vulnerabilities and thus ensure safety in use. However, methods to accurately compute adversarial robustness have been challenging for complex ML models and high-dimensional data. Furthermore, evaluations typically measure adversarial accuracy on specific attack budgets, limiting the informative value of the resulting metrics. This paper investigates the estimation of the more informative adversarial distance using iterative adversarial attacks and a certification approach. Combined, the methods provide a comprehensive evaluation of adversarial robustness by computing estimates for the upper and lower bounds of the adversarial distance. We present visualisations and ablation studies that provide insights into how this evaluation method should be applied and parameterised. We find that our adversarial attack approach is effective compared to related implementations, while the certification method falls short of expectations. The approach in this paper should encourage a more informative way of evaluating the adversarial robustness of ML classifiers.         ",
    "url": "https://arxiv.org/abs/2409.03598",
    "authors": [
      "Georg Siedel",
      "Ekagra Gupta",
      "Andrey Morozov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03612",
    "title": "VFLGAN-TS: Vertical Federated Learning-based Generative Adversarial Networks for Publication of Vertically Partitioned Time-Series Data",
    "abstract": "           In the current artificial intelligence (AI) era, the scale and quality of the dataset play a crucial role in training a high-quality AI model. However, often original data cannot be shared due to privacy concerns and regulations. A potential solution is to release a synthetic dataset with a similar distribution to the private dataset. Nevertheless, in some scenarios, the attributes required to train an AI model are distributed among different parties, and the parties cannot share the local data for synthetic data construction due to privacy regulations. In PETS 2024, we recently introduced the first Vertical Federated Learning-based Generative Adversarial Network (VFLGAN) for publishing vertically partitioned static data. However, VFLGAN cannot effectively handle time-series data, presenting both temporal and attribute dimensions. In this article, we proposed VFLGAN-TS, which combines the ideas of attribute discriminator and vertical federated learning to generate synthetic time-series data in the vertically partitioned scenario. The performance of VFLGAN-TS is close to that of its counterpart, which is trained in a centralized manner and represents the upper limit for VFLGAN-TS. To further protect privacy, we apply a Gaussian mechanism to make VFLGAN-TS satisfy an $(\\epsilon,\\delta)$-differential privacy. Besides, we develop an enhanced privacy auditing scheme to evaluate the potential privacy breach through the framework of VFLGAN-TS and synthetic datasets.         ",
    "url": "https://arxiv.org/abs/2409.03612",
    "authors": [
      "Xun Yuan",
      "Zilong Zhao",
      "Prosanta Gope",
      "Biplab Sikdar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.03630",
    "title": "Generalizing Linear Graphs and Bond Graph Models with Hetero-functional Graphs for System-of-Systems Engineering Applications",
    "abstract": "           In the 20th century, individual technology products like the generator, telephone, and automobile were connected to form many of the large-scale, complex, infrastructure networks we know today: the power grid, the communication infrastructure, and the transportation system. Progressively, these networked systems began interacting, forming what is now known as systems-of-systems. Because the component systems in the system-of-systems differ, modeling and analysis techniques with primitives applicable across multiple domains or disciplines are needed. For example, linear graphs and bond graphs have been used extensively in the electrical engineering, mechanical engineering, and mechatronic fields to design and analyze a wide variety of engineering systems. In contrast, hetero-functional graph theory (HFGT) has emerged to study many complex engineering systems and systems-of-systems (e.g. electric power, potable water, wastewater, natural gas, oil, coal, multi-modal transportation, mass-customized production, and personalized healthcare delivery systems). This paper seeks to relate hetero-functional graphs to linear graphs and bond graphs and demonstrate that the former is a generalization of the latter two. The contribution is relayed in three stages. First, the three modeling techniques are compared conceptually. Next, these techniques are contrasted on six example systems: (a) an electrical system, (b) a translational mechanical system, (c) a rotational mechanical system, (d) a fluidic system, (e) a thermal system, and (f) a multi-energy (electro-mechanical) system. Finally, this paper proves mathematically that hetero-functional graphs are a formal generalization of both linear graphs and bond graphs.         ",
    "url": "https://arxiv.org/abs/2409.03630",
    "authors": [
      "Ehsanoddin Ghorbanichemazkati",
      "Amro M. Farid"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.03634",
    "title": "Surface-Centric Modeling for High-Fidelity Generalizable Neural Surface Reconstruction",
    "abstract": "           Reconstructing the high-fidelity surface from multi-view images, especially sparse images, is a critical and practical task that has attracted widespread attention in recent years. However, existing methods are impeded by the memory constraint or the requirement of ground-truth depths and cannot recover satisfactory geometric details. To this end, we propose SuRF, a new Surface-centric framework that incorporates a new Region sparsification based on a matching Field, achieving good trade-offs between performance, efficiency and scalability. To our knowledge, this is the first unsupervised method achieving end-to-end sparsification powered by the introduced matching field, which leverages the weight distribution to efficiently locate the boundary regions containing surface. Instead of predicting an SDF value for each voxel, we present a new region sparsification approach to sparse the volume by judging whether the voxel is inside the surface region. In this way, our model can exploit higher frequency features around the surface with less memory and computational consumption. Extensive experiments on multiple benchmarks containing complex large-scale scenes show that our reconstructions exhibit high-quality details and achieve new state-of-the-art performance, i.e., 46% improvements with 80% less memory consumption. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.03634",
    "authors": [
      "Rui Peng",
      "Shihe Shen",
      "Kaiqiang Xiong",
      "Huachen Gao",
      "Jianbo Jiao",
      "Xiaodong Gu",
      "Ronggang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03646",
    "title": "Limited but consistent gains in adversarial robustness by co-training object recognition models with human EEG",
    "abstract": "           In contrast to human vision, artificial neural networks (ANNs) remain relatively susceptible to adversarial attacks. To address this vulnerability, efforts have been made to transfer inductive bias from human brains to ANNs, often by training the ANN representations to match their biological counterparts. Previous works relied on brain data acquired in rodents or primates using invasive techniques, from specific regions of the brain, under non-natural conditions (anesthetized animals), and with stimulus datasets lacking diversity and naturalness. In this work, we explored whether aligning model representations to human EEG responses to a rich set of real-world images increases robustness to ANNs. Specifically, we trained ResNet50-backbone models on a dual task of classification and EEG prediction; and evaluated their EEG prediction accuracy and robustness to adversarial attacks. We observed significant correlation between the networks' EEG prediction accuracy, often highest around 100 ms post stimulus onset, and their gains in adversarial robustness. Although effect size was limited, effects were consistent across different random initializations and robust for architectural variants. We further teased apart the data from individual EEG channels and observed strongest contribution from electrodes in the parieto-occipital regions. The demonstrated utility of human EEG for such tasks opens up avenues for future efforts that scale to larger datasets under diverse stimuli conditions with the promise of stronger effects.         ",
    "url": "https://arxiv.org/abs/2409.03646",
    "authors": [
      "Manshan Guo",
      "Bhavin Choksi",
      "Sari Sadiya",
      "Alessandro T. Gifford",
      "Martina G. Vilas",
      "Radoslaw M. Cichy",
      "Gemma Roig"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2409.03657",
    "title": "Unsupervised Anomaly Detection and Localization with Generative Adversarial Networks",
    "abstract": "           We propose a novel unsupervised anomaly detection approach using generative adversarial networks and SOP-derived spectrograms. Demonstrating remarkable efficacy, our method achieves over 97% accuracy on SOP datasets from both submarine and terrestrial fiber links, all achieved without the need for labelled data.         ",
    "url": "https://arxiv.org/abs/2409.03657",
    "authors": [
      "Khouloud Abdelli",
      "Matteo Lonardi",
      "Jurgen Gripp",
      "Samuel Olsson",
      "Fabien Boitier",
      "Patricia Layec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2409.03662",
    "title": "The representation landscape of few-shot learning and fine-tuning in large language models",
    "abstract": "           In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. However, little is known about whether they induce similar representations inside LLMs. We approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, we compare how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while the landscape of ICL representations is characterized by less defined peaks. Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models.         ",
    "url": "https://arxiv.org/abs/2409.03662",
    "authors": [
      "Diego Doimo",
      "Alessandro Serra",
      "Alessio Ansuini",
      "Alberto Cazzaniga"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03663",
    "title": "Weather-Adaptive Multi-Step Forecasting of State of Polarization Changes in Aerial Fibers Using Wavelet Neural Networks",
    "abstract": "           We introduce a novel weather-adaptive approach for multi-step forecasting of multi-scale SOP changes in aerial fiber links. By harnessing the discrete wavelet transform and incorporating weather data, our approach improves forecasting accuracy by over 65% in RMSE and 63% in MAPE compared to baselines.         ",
    "url": "https://arxiv.org/abs/2409.03663",
    "authors": [
      "Khouloud Abdelli",
      "Matteo Lonardi",
      "Jurgen Gripp",
      "Samuel Olsson Fabien Boitier",
      "Patricia Layec"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03667",
    "title": "Threat Classification on Deployed Optical Networks Using MIMO Digital Fiber Sensing, Wavelets, and Machine Learning",
    "abstract": "           We demonstrate mechanical threats classification including jackhammers and excavators, leveraging wavelet transform of MIMO-DFS output data across a 57-km operational network link. Our machine learning framework incorporates transfer learning and shows 93% classification accuracy from field data, with benefits for optical network supervision.         ",
    "url": "https://arxiv.org/abs/2409.03667",
    "authors": [
      "Khouloud Abdelli",
      "Henrique Pavani",
      "Christian Dorize",
      "Sterenn Guerrier",
      "Haik Mardoyan",
      "Patricia Layec",
      "Jeremie Renaudier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2409.03690",
    "title": "Gathering Information about a Graph by Counting Walks from a Single Vertex",
    "abstract": "           We say that a vertex $v$ in a connected graph $G$ is decisive if the numbers of walks from $v$ of each length determine the graph $G$ rooted at $v$ up to isomorphism among all connected rooted graphs with the same number of vertices. On the other hand, $v$ is called ambivalent if it has the same walk counts as a vertex in a non-isomorphic connected graph with the same number of vertices as $G$. Using the classical constructions of cospectral trees, we first observe that ambivalent vertices exist in almost all trees. If a graph $G$ is determined by spectrum and its characteristic polynomial is irreducible, then we prove that all vertices of $G$ are decisive. Note that both assumptions are conjectured to be true for almost all graphs. Without using any assumption, we are able to prove that the vertices of a random graph are with high probability distinguishable from each other by the numbers of closed walks of length at most 4. As a consequence, the closed walk counts for lengths 2, 3, and 4 provide a canonical labeling of a random graph. Answering a question posed in chemical graph theory, we finally show that all walk counts for a vertex in an $n$-vertex graph are determined by the counts for the $2n$ shortest lengths, and the bound $2n$ is here asymptotically tight.         ",
    "url": "https://arxiv.org/abs/2409.03690",
    "authors": [
      "Frank Fuhlbr\u00fcck",
      "Johannes K\u00f6bler",
      "Oleg Verbitsky",
      "Maksim Zhukovskii"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2409.03697",
    "title": "Classification and Prediction of Heart Diseases using Machine Learning Algorithms",
    "abstract": "           Heart disease is a serious worldwide health issue because it claims the lives of many people who might have been treated if the disease had been identified earlier. The leading cause of death in the world is cardiovascular disease, usually referred to as heart disease. Creating reliable, effective, and precise predictions for these diseases is one of the biggest issues facing the medical world today. Although there are tools for predicting heart diseases, they are either expensive or challenging to apply for determining a patient's risk. The best classifier for foretelling and spotting heart disease was the aim of this research. This experiment examined a range of machine learning approaches, including Logistic Regression, K-Nearest Neighbor, Support Vector Machine, and Artificial Neural Networks, to determine which machine learning algorithm was most effective at predicting heart diseases. One of the most often utilized data sets for this purpose, the UCI heart disease repository provided the data set for this study. The K-Nearest Neighbor technique was shown to be the most effective machine learning algorithm for determining whether a patient has heart disease. It will be beneficial to conduct further studies on the application of additional machine learning algorithms for heart disease prediction.         ",
    "url": "https://arxiv.org/abs/2409.03697",
    "authors": [
      "Akua Sekyiwaa Osei-Nkwantabisa",
      "Redeemer Ntumy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03707",
    "title": "A Different Level Text Protection Mechanism With Differential Privacy",
    "abstract": "           The article introduces a method for extracting words of different degrees of importance based on the BERT pre-training model and proves the effectiveness of this method. The article also discusses the impact of maintaining the same perturbation results for words of different importance on the overall text utility. This method can be applied to long text protection.         ",
    "url": "https://arxiv.org/abs/2409.03707",
    "authors": [
      "Qingwen Fu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.03708",
    "title": "RAG based Question-Answering for Contextual Response Prediction System",
    "abstract": "           Large Language Models (LLMs) have shown versatility in various Natural Language Processing (NLP) tasks, including their potential as effective question-answering systems. However, to provide precise and relevant information in response to specific customer queries in industry settings, LLMs require access to a comprehensive knowledge base to avoid hallucinations. Retrieval Augmented Generation (RAG) emerges as a promising technique to address this challenge. Yet, developing an accurate question-answering framework for real-world applications using RAG entails several challenges: 1) data availability issues, 2) evaluating the quality of generated content, and 3) the costly nature of human evaluation. In this paper, we introduce an end-to-end framework that employs LLMs with RAG capabilities for industry use cases. Given a customer query, the proposed system retrieves relevant knowledge documents and leverages them, along with previous chat history, to generate response suggestions for customer service agents in the contact centers of a major retail company. Through comprehensive automated and human evaluations, we show that this solution outperforms the current BERT-based algorithms in accuracy and relevance. Our findings suggest that RAG-based LLMs can be an excellent support to human customer service representatives by lightening their workload.         ",
    "url": "https://arxiv.org/abs/2409.03708",
    "authors": [
      "Sriram Veturi",
      "Saurabh Vaichal",
      "Nafis Irtiza Tripto",
      "Reshma Lal Jagadheesh",
      "Nian Yan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2409.03710",
    "title": "Inverse decision-making using neural amortized Bayesian actors",
    "abstract": "           Bayesian observer and actor models have provided normative explanations for many behavioral phenomena in perception, sensorimotor control, and other areas of cognitive science and neuroscience. They attribute behavioral variability and biases to different interpretable entities such as perceptual and motor uncertainty, prior beliefs, and behavioral costs. However, when extending these models to more complex tasks with continuous actions, solving the Bayesian decision-making problem is often analytically intractable. Moreover, inverting such models to perform inference over their parameters given behavioral data is computationally even more difficult. Therefore, researchers typically constrain their models to easily tractable components, such as Gaussian distributions or quadratic cost functions, or resort to numerical methods. To overcome these limitations, we amortize the Bayesian actor using a neural network trained on a wide range of different parameter settings in an unsupervised fashion. Using the pre-trained neural network enables performing gradient-based Bayesian inference of the Bayesian actor model's parameters. We show on synthetic data that the inferred posterior distributions are in close alignment with those obtained using analytical solutions where they exist. Where no analytical solution is available, we recover posterior distributions close to the ground truth. We then show that identifiability problems between priors and costs can arise in more complex cost functions. Finally, we apply our method to empirical data and show that it explains systematic individual differences of behavioral patterns.         ",
    "url": "https://arxiv.org/abs/2409.03710",
    "authors": [
      "Dominik Straub",
      "Tobias F. Niehues",
      "Jan Peters",
      "Constantin A. Rothkopf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2409.03718",
    "title": "Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with Image-Based Surface Representation",
    "abstract": "           Generating high-quality 3D objects from textual descriptions remains a challenging problem due to computational cost, the scarcity of 3D data, and complex 3D representations. We introduce Geometry Image Diffusion (GIMDiffusion), a novel Text-to-3D model that utilizes geometry images to efficiently represent 3D shapes using 2D images, thereby avoiding the need for complex 3D-aware architectures. By integrating a Collaborative Control mechanism, we exploit the rich 2D priors of existing Text-to-Image models such as Stable Diffusion. This enables strong generalization even with limited 3D training data (allowing us to use only high-quality training data) as well as retaining compatibility with guidance techniques such as IPAdapter. In short, GIMDiffusion enables the generation of 3D assets at speeds comparable to current Text-to-Image models. The generated objects consist of semantically meaningful, separate parts and include internal structures, enhancing both usability and versatility.         ",
    "url": "https://arxiv.org/abs/2409.03718",
    "authors": [
      "Slava Elizarov",
      "Ciara Rowles",
      "Simon Donn\u00e9"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2409.03731",
    "title": "A Deep Generative Learning Approach for Two-stage Adaptive Robust Optimization",
    "abstract": "           Two-stage adaptive robust optimization is a powerful approach for planning under uncertainty that aims to balance costs of \"here-and-now\" first-stage decisions with those of \"wait-and-see\" recourse decisions made after uncertainty is realized. To embed robustness against uncertainty, modelers typically assume a simple polyhedral or ellipsoidal set over which contingencies may be realized. However, these simple uncertainty sets tend to yield highly conservative decision-making when uncertainties are high-dimensional. In this work, we introduce AGRO, a column-and-constraint generation algorithm that performs adversarial generation for two-stage adaptive robust optimization using a variational autoencoder. AGRO identifies realistic and cost-maximizing contingencies by optimizing over spherical uncertainty sets in a latent space using a projected gradient ascent approach that differentiates the optimal recourse cost with respect to the latent variable. To demonstrate the cost- and time-efficiency of our approach experimentally, we apply AGRO to an adaptive robust capacity expansion problem for a regional power system and show that AGRO is able to reduce costs by up to 7.8% and runtimes by up to 77% in comparison to the conventional column-and-constraint generation algorithm.         ",
    "url": "https://arxiv.org/abs/2409.03731",
    "authors": [
      "Aron Brenner",
      "Rahman Khorramfar",
      "Jennifer Sun",
      "Saurabh Amin"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03733",
    "title": "Planning In Natural Language Improves LLM Search For Code Generation",
    "abstract": "           While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute has not yet yielded analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language. Based on this insight, we propose PLANSEARCH, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding). PLANSEARCH generates a diverse set of observations about the problem and then uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PLANSEARCH explores a significantly more diverse range of potential solutions compared to baseline search methods. Using PLANSEARCH on top of Claude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on LiveCodeBench, outperforming both the best score achieved without search (pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%). Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains due to search as a direct function of the diversity over generated ideas.         ",
    "url": "https://arxiv.org/abs/2409.03733",
    "authors": [
      "Evan Wang",
      "Federico Cassano",
      "Catherine Wu",
      "Yunfeng Bai",
      "Will Song",
      "Vaskar Nath",
      "Ziwen Han",
      "Sean Hendryx",
      "Summer Yue",
      "Hugh Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.03740",
    "title": "Differentiable Discrete Event Simulation for Queuing Network Control",
    "abstract": "           Queuing network control is essential for managing congestion in job-processing systems such as service systems, communication networks, and manufacturing processes. Despite growing interest in applying reinforcement learning (RL) techniques, queueing network control poses distinct challenges, including high stochasticity, large state and action spaces, and lack of stability. To tackle these challenges, we propose a scalable framework for policy optimization based on differentiable discrete event simulation. Our main insight is that by implementing a well-designed smoothing technique for discrete event dynamics, we can compute pathwise policy gradients for large-scale queueing networks using auto-differentiation software (e.g., Tensorflow, PyTorch) and GPU parallelization. Through extensive empirical experiments, we observe that our policy gradient estimators are several orders of magnitude more accurate than typical REINFORCE-based estimators. In addition, We propose a new policy architecture, which drastically improves stability while maintaining the flexibility of neural-network policies. In a wide variety of scheduling and admission control tasks, we demonstrate that training control policies with pathwise gradients leads to a 50-1000x improvement in sample efficiency over state-of-the-art RL methods. Unlike prior tailored approaches to queueing, our methods can flexibly handle realistic scenarios, including systems operating in non-stationary environments and those with non-exponential interarrival/service times.         ",
    "url": "https://arxiv.org/abs/2409.03740",
    "authors": [
      "Ethan Che",
      "Jing Dong",
      "Hongseok Namkoong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2409.03757",
    "title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding",
    "abstract": "           Complex 3D scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success. However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. To address this issue, we present a comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios. Our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3D foundation models. We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks. These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks.         ",
    "url": "https://arxiv.org/abs/2409.03757",
    "authors": [
      "Yunze Man",
      "Shuhong Zheng",
      "Zhipeng Bao",
      "Martial Hebert",
      "Liang-Yan Gui",
      "Yu-Xiong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.02963",
    "title": "Fair Minimum Representation Clustering via Integer Programming",
    "abstract": "           Clustering is an unsupervised learning task that aims to partition data into a set of clusters. In many applications, these clusters correspond to real-world constructs (e.g., electoral districts, playlists, TV channels) whose benefit can only be attained by groups when they reach a minimum level of representation (e.g., 50\\% to elect their desired candidate). In this paper, we study the k-means and k-medians clustering problems with the additional constraint that each group (e.g., demographic group) must have a minimum level of representation in at least a given number of clusters. We formulate the problem through a mixed-integer optimization framework and present an alternating minimization algorithm, called MiniReL, that directly incorporates the fairness constraints. While incorporating the fairness criteria leads to an NP-Hard assignment problem within the algorithm, we provide computational approaches that make the algorithm practical even for large datasets. Numerical results show that the approach is able to create fairer clusters with practically no increase in the clustering cost across standard benchmark datasets.         ",
    "url": "https://arxiv.org/abs/2409.02963",
    "authors": [
      "Connor Lawless",
      "Oktay Gunluk"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03146",
    "title": "Envisioning an Optimal Network of Space-Based Lasers for Orbital Debris Remediation",
    "abstract": "           The rapid increase in resident space objects, including satellites and orbital debris, threatens the safety and sustainability of space missions. This paper explores orbital debris remediation using laser ablation with a network of collaborative space-based lasers. A novel delta-v vector analysis framework quantifies the effects of multiple simultaneous laser-to-debris (L2D) engagements by leveraging a vector composition of imparted delta-v vectors. The paper introduces the Concurrent Location-Scheduling Problem (CLSP), which optimizes the placement of laser platforms and schedules L2D engagements to maximize debris remediation capacity. Due to the computational complexity of CLSP, it is decomposed into two sequential subproblems: (1) optimal laser platform locations are determined using the Maximal Covering Location Problem, and (2) a novel integer linear programming-based approach schedules L2D engagements within the network configuration to maximize remediation capacity. Computational experiments are conducted to evaluate the proposed framework's effectiveness under various mission scenarios, demonstrating key network functions such as collaborative nudging, deorbiting, and just-in-time collision avoidance. A cost-benefit analysis further explores how varying the number and distribution of laser platforms affects debris remediation capacity, providing insights into optimizing the performance of space-based laser networks.         ",
    "url": "https://arxiv.org/abs/2409.03146",
    "authors": [
      "David O. Williams Rogers",
      "Matthew C. Fox",
      "Paul R. Stysley",
      "Hang Woon Lee"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.03253",
    "title": "SpinMultiNet: Neural Network Potential Incorporating Spin Degrees of Freedom with Multi-Task Learning",
    "abstract": "           Neural Network Potentials (NNPs) have attracted significant attention as a method for accelerating density functional theory (DFT) calculations. However, conventional NNP models typically do not incorporate spin degrees of freedom, limiting their applicability to systems where spin states critically influence material properties, such as transition metal oxides. This study introduces SpinMultiNet, a novel NNP model that integrates spin degrees of freedom through multi-task learning. SpinMultiNet achieves accurate predictions without relying on correct spin values obtained from DFT calculations. Instead, it utilizes initial spin estimates as input and leverages multi-task learning to optimize the spin latent representation while maintaining both $E(3)$ and time-reversal equivariance. Validation on a dataset of transition metal oxides demonstrates the high predictive accuracy of SpinMultiNet. The model successfully reproduces the energy ordering of stable spin configurations originating from superexchange interactions and accurately captures the rhombohedral distortion of the rocksalt structure. These results pave the way for new possibilities in materials simulations that consider spin degrees of freedom, promising future applications in large-scale simulations of various material systems, including magnetic materials.         ",
    "url": "https://arxiv.org/abs/2409.03253",
    "authors": [
      "Koki Ueno",
      "Satoru Ohuchi",
      "Kazuhide Ichikawa",
      "Kei Amii",
      "Kensuke Wakasugi"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03302",
    "title": "Fourier Neural Operators for Learning Dynamics in Quantum Spin Systems",
    "abstract": "           Fourier Neural Operators (FNOs) excel on tasks using functional data, such as those originating from partial differential equations. Such characteristics render them an effective approach for simulating the time evolution of quantum wavefunctions, which is a computationally challenging, yet coveted task for understanding quantum systems. In this manuscript, we use FNOs to model the evolution of random quantum spin systems, so chosen due to their representative quantum dynamics and minimal symmetry. We explore two distinct FNO architectures and examine their performance for learning and predicting time evolution using both random and low-energy input states. Additionally, we apply FNOs to a compact set of Hamiltonian observables ($\\sim\\text{poly}(n)$) instead of the entire $2^n$ quantum wavefunction, which greatly reduces the size of our inputs and outputs and, consequently, the requisite dimensions of the resulting FNOs. Moreover, this Hamiltonian observable-based method demonstrates that FNOs can effectively distill information from high-dimensional spaces into lower-dimensional spaces. The extrapolation of Hamiltonian observables to times later than those used in training is of particular interest, as this stands to fundamentally increase the simulatability of quantum systems past both the coherence times of contemporary quantum architectures and the circuit-depths of tractable tensor networks.         ",
    "url": "https://arxiv.org/abs/2409.03302",
    "authors": [
      "Freya Shah",
      "Taylor L. Patti",
      "Julius Berner",
      "Bahareh Tolooshams",
      "Jean Kossaifi",
      "Anima Anandkumar"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03367",
    "title": "TBConvL-Net: A Hybrid Deep Learning Architecture for Robust Medical Image Segmentation",
    "abstract": "           Deep learning has shown great potential for automated medical image segmentation to improve the precision and speed of disease diagnostics. However, the task presents significant difficulties due to variations in the scale, shape, texture, and contrast of the pathologies. Traditional convolutional neural network (CNN) models have certain limitations when it comes to effectively modelling multiscale context information and facilitating information interaction between skip connections across levels. To overcome these limitations, a novel deep learning architecture is introduced for medical image segmentation, taking advantage of CNNs and vision transformers. Our proposed model, named TBConvL-Net, involves a hybrid network that combines the local features of a CNN encoder-decoder architecture with long-range and temporal dependencies using biconvolutional long-short-term memory (LSTM) networks and vision transformers (ViT). This enables the model to capture contextual channel relationships in the data and account for the uncertainty of segmentation over time. Additionally, we introduce a novel composite loss function that considers both the segmentation robustness and the boundary agreement of the predicted output with the gold standard. Our proposed model shows consistent improvement over the state of the art on ten publicly available datasets of seven different medical imaging modalities.         ",
    "url": "https://arxiv.org/abs/2409.03367",
    "authors": [
      "Shahzaib Iqbal",
      "Tariq M. Khan",
      "Syed S. Naqvi",
      "Asim Naveed",
      "Erik Meijering"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.03394",
    "title": "Partitioning 2-edge-coloured bipartite graphs into monochromatic cycles",
    "abstract": "           Given an $r$-colouring of the edges of a graph $G$, we say that it can be partitioned into $p$ monochromatic cycles when there exists a set of $p$ vertex-disjoint monochromatic cycles covering all the vertices of $G$. In the literature of this problem, an edge and a single vertex both count as a cycle. We show that for every $2$-colouring of the edges of a complete balanced bipartite graph, $K_{n,n}$, it can be partitioned into at most 4 monochromatic cycles. This type of question was first studied in 1970 for complete graphs and in 1983, by Gy\u00e1rf\u00e1s and Lehel, for $K_{n,n}$. In 2014, Pokrovskiy, has showed that any $2$-colouring of the edges of $K_{n,n}$ can be partitioned into at most $3$ paths. It turns out that finding monochromatic cycles instead of paths is a natural question that has also being asked for other graphs. In 2015, Schaudt and Stein have showed that at most 14 cycles are necessary.         ",
    "url": "https://arxiv.org/abs/2409.03394",
    "authors": [
      "Fabr\u00edcio Siqueira Benevides",
      "Arthur Lima Quintino",
      "Alexandre Talon"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2409.03492",
    "title": "Distributionally Robust Optimisation with Bayesian Ambiguity Sets",
    "abstract": "           Decision making under uncertainty is challenging since the data-generating process (DGP) is often unknown. Bayesian inference proceeds by estimating the DGP through posterior beliefs about the model's parameters. However, minimising the expected risk under these posterior beliefs can lead to sub-optimal decisions due to model uncertainty or limited, noisy observations. To address this, we introduce Distributionally Robust Optimisation with Bayesian Ambiguity Sets (DRO-BAS) which hedges against uncertainty in the model by optimising the worst-case risk over a posterior-informed ambiguity set. We show that our method admits a closed-form dual representation for many exponential family members and showcase its improved out-of-sample robustness against existing Bayesian DRO methodology in the Newsvendor problem.         ",
    "url": "https://arxiv.org/abs/2409.03492",
    "authors": [
      "Charita Dellaporta",
      "Patrick O'Hara",
      "Theodoros Damoulas"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03559",
    "title": "Nonlinear identifiability of directed acyclic graphs with partial excitation and measurement",
    "abstract": "           We analyze the identifiability of directed acyclic graphs in the case of partial excitation and measurement. We consider an additive model where the nonlinear functions located in the edges depend only on a past input, and we analyze the identifiability problem in the class of pure nonlinear functions satisfying $f(0)=0$. We show that any identification pattern (set of measured nodes and set of excited nodes) requires the excitation of sources, measurement of sinks and the excitation or measurement of the other nodes. Then, we show that a directed acyclic graph (DAG) is identifiable with a given identification pattern if and only if it is identifiable with the measurement of all the nodes. Next, we analyze the case of trees where we prove that any identification pattern guarantees the identifiability of the network. Finally, by introducing the notion of a generic nonlinear network matrix, we provide sufficient conditions for the identifiability of DAGs based on the notion of vertex-disjoint paths.         ",
    "url": "https://arxiv.org/abs/2409.03559",
    "authors": [
      "Renato Vizuete",
      "Julien M. Hendrickx"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.03618",
    "title": "DART2: a robust multiple testing method to smartly leverage helpful or misleading ancillary information",
    "abstract": "           In many applications of multiple testing, ancillary information is available, reflecting the hypothesis null or alternative status. Several methods have been developed to leverage this ancillary information to enhance testing power, typically requiring the ancillary information is helpful enough to ensure favorable performance. In this paper, we develop a robust and effective distance-assisted multiple testing procedure named DART2, designed to be powerful and robust regardless of the quality of ancillary information. When the ancillary information is helpful, DART2 can asymptotically control FDR while improving power; otherwise, DART2 can still control FDR and maintain power at least as high as ignoring the ancillary information. We demonstrated DART2's superior performance compared to existing methods through numerical studies under various settings. In addition, DART2 has been applied to a gene association study where we have shown its superior accuracy and robustness under two different types of ancillary information.         ",
    "url": "https://arxiv.org/abs/2409.03618",
    "authors": [
      "Xuechan Li",
      "Jichun Xie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03655",
    "title": "Privacy versus Emotion Preservation Trade-offs in Emotion-Preserving Speaker Anonymization",
    "abstract": "           Advances in speech technology now allow unprecedented access to personally identifiable information through speech. To protect such information, the differential privacy field has explored ways to anonymize speech while preserving its utility, including linguistic and paralinguistic aspects. However, anonymizing speech while maintaining emotional state remains challenging. We explore this problem in the context of the VoicePrivacy 2024 challenge. Specifically, we developed various speaker anonymization pipelines and find that approaches either excel at anonymization or preserving emotion state, but not both simultaneously. Achieving both would require an in-domain emotion recognizer. Additionally, we found that it is feasible to train a semi-effective speaker verification system using only emotion representations, demonstrating the challenge of separating these two modalities.         ",
    "url": "https://arxiv.org/abs/2409.03655",
    "authors": [
      "Zexin Cai",
      "Henry Li Xinyuan",
      "Ashi Garg",
      "Leibny Paola Garc\u00eda-Perera",
      "Kevin Duh",
      "Sanjeev Khudanpur",
      "Nicholas Andrews",
      "Matthew Wiesner"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03669",
    "title": "A method to benchmark high-dimensional process drift detection",
    "abstract": "           Process curves are multi-variate finite time series data coming from manufacturing processes. This paper studies machine learning methods for drifts of process curves. A theoretic framework to synthetically generate process curves in a controlled way is introduced in order to benchmark machine learning algorithms for process drift detection. A evaluation score, called the temporal area under the curve, is introduced, which allows to quantify how well machine learning models unveil curves belonging to drift segments. Finally, a benchmark study comparing popular machine learning approaches on synthetic data generated with the introduced framework shown.         ",
    "url": "https://arxiv.org/abs/2409.03669",
    "authors": [
      "Edgar Wolf",
      "Tobias Windisch"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:1904.00008",
    "title": "Quadrotor Manipulation System: Development of a Robust Contact Force Estimation and Impedance Control Scheme Based on DOb and FTRLS",
    "abstract": "           The research on aerial manipulation systems has been increased rapidly in recent years. These systems are very attractive for a wide range of applications due to their unique features. However, dynamics, control and manipulation tasks of such systems are quite challenging because they are naturally unstable, have very fast dynamics, have strong nonlinearities, are very susceptible to parameters variations due to carrying a payload besides the external disturbances, and have complex inverse kinematics. In addition, the manipulation tasks require estimating (applying) a certain force of (at) the end-effector as well as the accurate positioning of it. Thus, in this article, a robust force estimation and impedance control scheme is proposed to address these issues. The robustness is achieved based on the Disturbance Observer (DOb) technique. Then, a tracking and performance low computational linear controller is used. For teleoperation purpose, the contact force needs to be identified. However, the current developed techniques for force estimation have limitations because they are based on ignoring some dynamics and/or requiring of an indicator of the environment contact. Unlike these techniques, we propose a technique based on linearization capabilities of DOb and a Fast Tracking Recursive Least Squares (FTRLS) algorithm. The complex inverse kinematics problem of such a system is solved by a Jacobin based algorithm. The stability analysis of the proposed scheme is presented. The algorithm is tested to achieve tracking of task space reference trajectories besides the impedance control. The efficiency of the proposed technique is enlightened via numerical simulation.         ",
    "url": "https://arxiv.org/abs/1904.00008",
    "authors": [
      "Ahmed Khalifa",
      "Mohamed Fanni",
      "Alaa Khalifa"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2006.15136",
    "title": "Homotopy Theoretic and Categorical Models of Neural Information Networks",
    "abstract": "           In this paper we develop a novel mathematical formalism for the modeling of neural information networks endowed with additional structure in the form of assignments of resources, either computational or metabolic or informational. The starting point for this construction is the notion of summing functors and of Segal's Gamma-spaces in homotopy theory. The main results in this paper include functorial assignments of concurrent/distributed computing architectures and associated binary codes to networks and their subsystems, a categorical form of the Hopfield network dynamics, which recovers the usual Hopfield equations when applied to a suitable category of weighted codes, a functorial assignment to networks of corresponding information structures and information cohomology, and a cohomological version of integrated information.         ",
    "url": "https://arxiv.org/abs/2006.15136",
    "authors": [
      "Yuri Manin",
      "Matilde Marcolli"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2104.09008",
    "title": "Kernel Adversarial Learning for Real-world Image Super-resolution",
    "abstract": "           Current deep image super-resolution (SR) approaches aim to restore high-resolution images from down-sampled images or by assuming degradation from simple Gaussian kernels and additive noises. However, these techniques only assume crude approximations of the real-world image degradation process, which should involve complex kernels and noise patterns that are difficult to model using simple assumptions. In this paper, we propose a more realistic process to synthesise low-resolution images for real-world image SR by introducing a new Kernel Adversarial Learning Super-resolution (KASR) framework. In the proposed framework, degradation kernels and noises are adaptively modelled rather than explicitly specified. Moreover, we also propose a high-frequency selective objective and an iterative supervision process to further boost the model SR reconstruction accuracy. Extensive experiments validate the effectiveness of the proposed framework on real-world datasets.         ",
    "url": "https://arxiv.org/abs/2104.09008",
    "authors": [
      "Hu Wang",
      "Congbo Ma",
      "Jianpeng Zhang",
      "Wei Emma Zhang",
      "Gustavo Carneiro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2105.10535",
    "title": "Ising Machines' Dynamics and Regularization for Near-Optimal Large and Massive MIMO Detection",
    "abstract": "           Optimal MIMO detection has been one of the most challenging and computationally inefficient tasks in wireless systems. We show that the new analog computing techniques like Coherent Ising Machines (CIM) are promising candidates for performing near-optimal MIMO detection. We propose a novel regularized Ising formulation for MIMO detection that mitigates a common error floor problem and further evolves it into an algorithm that achieves near-optimal MIMO detection. Massive MIMO systems, that have a large number of antennas at the Access point (AP), allow linear detectors to be near-optimal. However, the simplified detection in these systems comes at the cost of overall throughput, which could be improved by supporting more users. By means of numerical simulations, we show that in principle a MIMO detector based on a hybrid use of a CIM would allow us to add more transmitter antennas/users and increase the overall throughput of the cell by a significant factor. This would open up the opportunity to operate using more aggressive modulation and coding schemes and hence achieve high throughput: for a $16\\times16$ large MIMO system, we estimate around 2.5$\\times$ more throughput in mid-SNR regime ($\\approx 12 dB$) and 2$\\times$ more throughput in high-SNR regime( $>$ 20dB) than the industry standard, Minimum-Mean Square Error decoding (MMSE).         ",
    "url": "https://arxiv.org/abs/2105.10535",
    "authors": [
      "Abhishek Kumar Singh",
      "Kyle Jamieson",
      "Davide Venturelli",
      "Peter McMahon"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2209.06388",
    "title": "TSFool: Crafting Highly-Imperceptible Adversarial Time Series through Multi-Objective Attack",
    "abstract": "           Recent years have witnessed the success of recurrent neural network (RNN) models in time series classification (TSC). However, neural networks (NNs) are vulnerable to adversarial samples, which cause real-life adversarial attacks that undermine the robustness of AI models. To date, most existing attacks target at feed-forward NNs and image recognition tasks, but they cannot perform well on RNN-based TSC. This is due to the cyclical computation of RNN, which prevents direct model differentiation. In addition, the high visual sensitivity of time series to perturbations also poses challenges to local objective optimization of adversarial samples. In this paper, we propose an efficient method called TSFool to craft highly-imperceptible adversarial time series for RNN-based TSC. The core idea is a new global optimization objective known as \"Camouflage Coefficient\" that captures the imperceptibility of adversarial samples from the class distribution. Based on this, we reduce the adversarial attack problem to a multi-objective optimization problem that enhances the perturbation quality. Furthermore, to speed up the optimization process, we propose to use a representation model for RNN to capture deeply embedded vulnerable samples whose features deviate from the latent manifold. Experiments on 11 UCR and UEA datasets showcase that TSFool significantly outperforms six white-box and three black-box benchmark attacks in terms of effectiveness, efficiency and imperceptibility from various perspectives including standard measure, human study and real-world defense.         ",
    "url": "https://arxiv.org/abs/2209.06388",
    "authors": [
      "Yanyun Wang",
      "Dehui Du",
      "Haibo Hu",
      "Zi Liang",
      "Yuanhao Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2211.13670",
    "title": "SmartIntentNN: Towards Smart Contract Intent Detection",
    "abstract": "           Smart contracts on the blockchain offer decentralized financial services but often lack robust security measures, resulting in significant economic losses. Although substantial research has focused on identifying vulnerabilities, a notable gap remains in evaluating the malicious intent behind their development. To address this, we introduce \\textsc{SmartIntentNN} (Smart Contract Intent Neural Network), a deep learning-based tool designed to automate the detection of developers' intent in smart contracts. Our approach integrates a Universal Sentence Encoder for contextual representation of smart contract code, employs a K-means clustering algorithm to highlight intent-related code features, and utilizes a bidirectional LSTM-based multi-label classification network to predict ten distinct types of high-risk intent. Evaluations on a dataset of 10,000 smart contracts demonstrate that \\textsc{SmartIntentNN} surpasses all baselines, achieving an F1-score of up to 0.8633. A demo video is available at \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2211.13670",
    "authors": [
      "Youwei Huang",
      "Sen Fang",
      "Jianwen Li",
      "Bin Hu",
      "Tao Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2301.09043",
    "title": "CodeScore: Evaluating Code Generation by Learning Code Execution",
    "abstract": "           A proper code evaluation metric (CEM) profoundly impacts the evolution of code generation, which is an important research field in NLP and software engineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU) suffer from two significant drawbacks. 1. They primarily measure the surface differences between codes without considering their functional equivalence. However, functional equivalence is pivotal in evaluating the effectiveness of code generation, as different codes can perform identical operations. 2. They are predominantly designed for the Ref-only input format. However, code evaluation necessitates versatility in input formats. Aside from Ref-only, there are NL-only and Ref\\&NL formats, which existing match-based CEMs cannot effectively accommodate. In this paper, we propose CodeScore, a large language model (LLM)-based CEM, which estimates the functional correctness of generated code on three input types. To acquire CodeScore, we present UniCE, a unified code generation learning framework, for LLMs to learn code execution (i.e., learning PassRatio and Executability of generated code) with unified input. Extensive experimental results on multiple code evaluation datasets demonstrate that CodeScore absolutely improves up to 58.87% correlation with functional correctness compared to other CEMs, achieves state-of-the-art performance, and effectively handles three input formats.         ",
    "url": "https://arxiv.org/abs/2301.09043",
    "authors": [
      "Yihong Dong",
      "Jiazheng Ding",
      "Xue Jiang",
      "Ge Li",
      "Zhuo Li",
      "Zhi Jin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2302.04823",
    "title": "Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments",
    "abstract": "           Deriving robust control policies for realistic urban navigation scenarios is not a trivial task. In an end-to-end approach, these policies must map high-dimensional images from the vehicle's cameras to low-level actions such as steering and throttle. While pure Reinforcement Learning (RL) approaches are based exclusively on engineered rewards, Generative Adversarial Imitation Learning (GAIL) agents learn from expert demonstrations while interacting with the environment, which favors GAIL on tasks for which a reward signal is difficult to derive, such as autonomous driving. However, training deep networks directly from raw images on RL tasks is known to be unstable and troublesome. To deal with that, this work proposes a hierarchical GAIL-based architecture (hGAIL) which decouples representation learning from the driving task to solve the autonomous navigation of a vehicle. The proposed architecture consists of two modules: a GAN (Generative Adversarial Net) which generates an abstract mid-level input representation, which is the Bird's-Eye View (BEV) from the surroundings of the vehicle; and the GAIL which learns to control the vehicle based on the BEV predictions from the GAN as input. hGAIL is able to learn both the policy and the mid-level representation simultaneously as the agent interacts with the environment. Our experiments made in the CARLA simulation environment have shown that GAIL exclusively from cameras (without BEV) fails to even learn the task, while hGAIL, after training exclusively on one city, was able to autonomously navigate successfully in 98% of the intersections of a new city not used in training phase. Videos and code available at: this https URL ",
    "url": "https://arxiv.org/abs/2302.04823",
    "authors": [
      "Gustavo Claudio Karl Couto",
      "Eric Aislan Antonelo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2304.12830",
    "title": "Uplink MIMO Detection using Ising Machines: A Multi-Stage Ising Approach",
    "abstract": "           Multiple-Input-Multiple-Output~(MIMO) signal detection is central to every state-of-the-art communication system, and enhancements in error performance and computation complexity of MIMO detection would significantly enhance data rate and latency experienced by the users. Theoretically, the optimal MIMO detector is the maximum-likelihood (ML) MIMO detector; however, due to its extremely high complexity, it is not feasible for large real-world communication systems. Over the past few years, algorithms based on physics-inspired Ising solvers, like Coherent Ising machines and Quantum Annealers, have shown significant performance improvements for the MIMO detection problem. However, the current state-of-the-art is limited to low-order modulations or systems with few users. In this paper, we propose an adaptive multi-stage Ising machine-based MIMO detector that extends the performance gains of physics-inspired computation to Large and Massive MIMO systems with a large number of users and very high modulation schemes~(up to 256-QAM). We enhance our previously proposed delta Ising formulation and develop a heuristic that adaptively optimizes the performance and complexity of our proposed method. We perform extensive micro-benchmarking to optimize several free parameters of the system and evaluate our methods' BER and spectral efficiency for Large and Massive MIMO systems (up to 32 users and 256 QAM modulation).         ",
    "url": "https://arxiv.org/abs/2304.12830",
    "authors": [
      "Abhishek Kumar Singh",
      "Ari Kapelyan",
      "Davide Venturelli",
      "Kyle Jamieson"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2306.01095",
    "title": "Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization",
    "abstract": "           Bayesian optimization (BO) provides a powerful framework for optimizing black-box, expensive-to-evaluate functions. It is therefore an attractive tool for engineering design problems, typically involving multiple objectives. Thanks to the rapid advances in fabrication and measurement methods as well as parallel computing infrastructure, querying many design problems can be heavily parallelized. This class of problems challenges BO with an unprecedented setup where it has to deal with very large batches, shifting its focus from sample efficiency to iteration efficiency. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our key contribution is a highly scalable, sample-based acquisition function that performs a non-dominated sorting of not only the objectives but also their associated uncertainty. We show that our acquisition function in combination with different Bayesian neural network surrogates is effective in data-intensive environments with a minimal number of iterations. We demonstrate the superiority of our method by comparing it with state-of-the-art multi-objective optimizations. We perform our evaluation on two real-world problems -- airfoil design and 3D printing -- showcasing the applicability and efficiency of our approach. Our code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2306.01095",
    "authors": [
      "Navid Ansari",
      "Alireza Javanmardi",
      "Eyke H\u00fcllermeier",
      "Hans-Peter Seidel",
      "Vahid Babaei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2306.16674",
    "title": "Online learning for robust voltage control under uncertain grid topology",
    "abstract": "           Voltage control generally requires accurate information about the grid's topology in order to guarantee network stability. However, accurate topology identification is challenging for existing methods, especially as the grid is subject to increasingly frequent reconfiguration due to the adoption of renewable energy. In this work, we combine a nested convex body chasing algorithm with a robust predictive controller to achieve provably finite-time convergence to safe voltage limits in the online setting where there is uncertainty in both the network topology as well as load and generation variations. In an online fashion, our algorithm narrows down the set of possible grid models that are consistent with observations and adjusts reactive power generation accordingly to keep voltages within desired safety limits. Our approach can also incorporate existing partial knowledge of the network to improve voltage control performance. We demonstrate the effectiveness of our approach in a case study on a Southern California Edison 56-bus distribution system. Our experiments show that in practical settings, the controller is indeed able to narrow the set of consistent topologies quickly enough to make control decisions that ensure stability in both linearized and realistic non-linear models of the distribution grid.         ",
    "url": "https://arxiv.org/abs/2306.16674",
    "authors": [
      "Christopher Yeh",
      "Jing Yu",
      "Yuanyuan Shi",
      "Adam Wierman"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2307.07136",
    "title": "STCSNN: High energy efficiency spike-train level spiking neural networks with spatio-temporal conversion",
    "abstract": "           Brain-inspired spiking neuron networks (SNNs) have attracted widespread research interest due to their low power features, high biological plausibility, and strong spatiotemporal information processing capability. Although adopting a surrogate gradient (SG) makes the non-differentiability SNN trainable, achieving comparable accuracy for ANNs and keeping low-power features simultaneously is still tricky. In this paper, we proposed an energy-efficient spike-train level spiking neural network with spatio-temporal conversion, which has low computational cost and high accuracy. In the STCSNN, spatio-temporal conversion blocks (STCBs) are proposed to keep the low power features of SNNs and improve accuracy. However, STCSNN cannot adopt backpropagation algorithms directly due to the non-differentiability nature of spike trains. We proposed a suitable learning rule for STCSNNs by deducing the equivalent gradient of STCB. We evaluate the proposed STCSNN on static and neuromorphic datasets, including Fashion-Mnist, Cifar10, Cifar100, TinyImageNet, and DVS-Cifar10. The experiment results show that our proposed STCSNN outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time steps and being highly energy-efficient.         ",
    "url": "https://arxiv.org/abs/2307.07136",
    "authors": [
      "Changqing Xu",
      "Yi Liu",
      "Yintang Yang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2309.02384",
    "title": "Parameter Dependent Robust Control Invariant Sets for LPV Systems with Bounded Parameter Variation Rate",
    "abstract": "           Real-time measurements of the scheduling parameter of linear parameter-varying (LPV) systems enables the synthesis of robust control invariant (RCI) sets and parameter dependent controllers inducing invariance. We present a method to synthesize parameter-dependent robust control invariant (PD-RCI) sets for LPV systems with bounded parameter variation, in which invariance is induced using PD-vertex control laws. The PD-RCI sets are parameterized as configuration-constrained polytopes that admit a joint parameterization of their facets and vertices. The proposed sets and associated control laws are computed by solving a single semidefinite programing (SDP) problem. Through numerical examples, we demonstrate that the proposed method outperforms state-of-the-art methods for synthesizing PD-RCI sets, both with respect to conservativeness and computational load.         ",
    "url": "https://arxiv.org/abs/2309.02384",
    "authors": [
      "Sampath Kumar Mulagaleti",
      "Manas Mejari",
      "Alberto Bemporad"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2309.04644",
    "title": "Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay",
    "abstract": "           Neural Collapse (NC) is a geometric structure recently observed at the terminal phase of training deep neural networks, which states that last-layer feature vectors for the same class would \"collapse\" to a single point, while features of different classes become equally separated. We demonstrate that batch normalization (BN) and weight decay (WD) critically influence the emergence of NC. In the near-optimal loss regime, we establish an asymptotic lower bound on the emergence of NC that depends only on the WD value, training loss, and the presence of last-layer BN. Our experiments substantiate theoretical insights by showing that models demonstrate a stronger presence of NC with BN, appropriate WD values, lower loss, and lower last-layer feature norm. Our findings offer a novel perspective in studying the role of BN and WD in shaping neural network features.         ",
    "url": "https://arxiv.org/abs/2309.04644",
    "authors": [
      "Leyan Pan",
      "Xinyuan Cao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.00813",
    "title": "OceanNet: A principled neural operator-based digital twin for regional oceans",
    "abstract": "           While data-driven approaches demonstrate great potential in atmospheric modeling and weather forecasting, ocean modeling poses distinct challenges due to complex bathymetry, land, vertical structure, and flow non-linearity. This study introduces OceanNet, a principled neural operator-based digital twin for ocean circulation. OceanNet uses a Fourier neural operator and predictor-evaluate-corrector integration scheme to mitigate autoregressive error growth and enhance stability over extended time scales. A spectral regularizer counteracts spectral bias at smaller scales. OceanNet is applied to the northwest Atlantic Ocean western boundary current (the Gulf Stream), focusing on the task of seasonal prediction for Loop Current eddies and the Gulf Stream meander. Trained using historical sea surface height (SSH) data, OceanNet demonstrates competitive forecast skill by outperforming SSH predictions by an uncoupled, state-of-the-art dynamical ocean model forecast, reducing computation by 500,000 times. These accomplishments demonstrate the potential of physics-inspired deep neural operators as cost-effective alternatives to high-resolution numerical ocean models.         ",
    "url": "https://arxiv.org/abs/2310.00813",
    "authors": [
      "Ashesh Chattopadhyay",
      "Michael Gray",
      "Tianning Wu",
      "Anna B. Lowe",
      "Ruoying He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Chaotic Dynamics (nlin.CD)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2310.13845",
    "title": "Spectral-Aware Augmentation for Enhanced Graph Representation Learning",
    "abstract": "           Graph Contrastive Learning (GCL) has demonstrated remarkable effectiveness in learning representations on graphs in recent years. To generate ideal augmentation views, the augmentation generation methods should preserve essential information while discarding less relevant details for downstream tasks. However, current augmentation methods usually involve random topology corruption in the spatial domain, which fails to adequately address information spread across different frequencies in the spectral domain. Our preliminary study highlights this issue, demonstrating that spatial random perturbations impact all frequency bands almost uniformly. Given that task-relevant information typically resides in specific spectral regions that vary across graphs, this one-size-fits-all approach can pose challenges. We argue that indiscriminate spatial random perturbation might unintentionally weaken task-relevant information, reducing its effectiveness. To tackle this challenge, we propose applying perturbations selectively, focusing on information specific to different frequencies across diverse graphs. In this paper, we present GASSER, a model that applies tailored perturbations to specific frequencies of graph structures in the spectral domain, guided by spectral hints. Through extensive experimentation and theoretical analysis, we demonstrate that the augmentation views generated by GASSER are adaptive, controllable, and intuitively aligned with the homophily ratios and spectrum of graph structures.         ",
    "url": "https://arxiv.org/abs/2310.13845",
    "authors": [
      "Kaiqi Yang",
      "Haoyu Han",
      "Wei Jin",
      "Hui Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2311.00889",
    "title": "SALLM: Security Assessment of Generated Code",
    "abstract": "           With the growing popularity of Large Language Models (LLMs) in software engineers' daily practices, it is important to ensure that the code generated by these tools is not only functionally correct but also free of vulnerabilities. Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code. There are two contributing factors to the insecure code generation. First, existing datasets used to evaluate LLMs do not adequately represent genuine software engineering tasks sensitive to security. Instead, they are often based on competitive programming challenges or classroom-type coding tasks. In real-world applications, the code produced is integrated into larger codebases, introducing potential security risks. Second, existing evaluation metrics primarily focus on the functional correctness of the generated code while ignoring security considerations. Therefore, in this paper, we described SALLM, a framework to benchmark LLMs' abilities to generate secure code systematically. This framework has three major components: a novel dataset of security-centric Python prompts, configurable assessment techniques to evaluate the generated code, and novel metrics to evaluate the models' performance from the perspective of secure code generation.         ",
    "url": "https://arxiv.org/abs/2311.00889",
    "authors": [
      "Mohammed Latif Siddiq",
      "Joanna C. S. Santos",
      "Sajith Devareddy",
      "Anna Muller"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2311.06551",
    "title": "FDNet: Feature Decoupled Segmentation Network for Tooth CBCT Image",
    "abstract": "           Precise Tooth Cone Beam Computed Tomography (CBCT) image segmentation is crucial for orthodontic treatment planning. In this paper, we propose FDNet, a Feature Decoupled Segmentation Network, to excel in the face of the variable dental conditions encountered in CBCT scans, such as complex artifacts and indistinct tooth boundaries. The Low-Frequency Wavelet Transform (LF-Wavelet) is employed to enrich the semantic content by emphasizing the global structural integrity of the teeth, while the SAM encoder is leveraged to refine the boundary delineation, thus improving the contrast between adjacent dental structures. By integrating these dual aspects, FDNet adeptly addresses the semantic gap, providing a detailed and accurate segmentation. The framework's effectiveness is validated through rigorous benchmarks, achieving the top Dice and IoU scores of 85.28% and 75.23%, respectively. This innovative decoupling of semantic and boundary features capitalizes on the unique strengths of each element to elevate the quality of segmentation performance.         ",
    "url": "https://arxiv.org/abs/2311.06551",
    "authors": [
      "Xiang Feng",
      "Chengkai Wang",
      "Chengyu Wu",
      "Yunxiang Li",
      "Yongbo He",
      "Shuai Wang",
      "Yaiqi Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2311.15327",
    "title": "FRAC-Q-Learning: A Reinforcement Learning with Boredom Avoidance Processes for Social Robots",
    "abstract": "           The reinforcement learning algorithms have often been applied to social robots. However, most reinforcement learning algorithms were not optimized for the use of social robots, and consequently they may bore users. We proposed a new reinforcement learning method specialized for the social robot, the FRAC-Q-learning, that can avoid user boredom. The proposed algorithm consists of a forgetting process in addition to randomizing and categorizing processes. This study evaluated interest and boredom hardness scores of the FRAC-Q-learning by a comparison with the traditional Q-learning. The FRAC-Q-learning showed significantly higher trend of interest score, and indicated significantly harder to bore users compared to the traditional Q-learning. Therefore, the FRAC-Q-learning can contribute to develop a social robot that will not bore users. The proposed algorithm has a potential to apply for Web-based communication and educational systems. This paper presents the entire process, detailed implementation and a detailed evaluation method of the of the FRAC-Q-learning for the first time.         ",
    "url": "https://arxiv.org/abs/2311.15327",
    "authors": [
      "Akinari Onishi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2312.04183",
    "title": "Enhanced Uplink Data Detection for Massive MIMO with 1-Bit ADCs: Analysis and Joint Detection",
    "abstract": "           We present a new analytical framework on the uplink data detection for massive multiple-input multiple-output systems with 1-bit analog-to-digital converters (ADCs). We first characterize the expected values of the soft-estimated symbols (after the linear receiver and prior to the data detection), which are affected by the 1-bit quantization during both the channel estimation and the uplink data transmission. In our analysis, we consider conventional receivers such as maximum ratio combining (MRC), zero forcing, and minimum mean squared error (MMSE), with multiple user equipments (UEs) and correlated Rayleigh fading. Additionally, we design a linear minimum mean dispersion (LMMD) receiver tailored for the data detection with 1-bit ADCs, which exploits the expected values of the soft-estimated symbols previously derived. Then, we propose a joint data detection (JD) strategy that exploits the interdependence among the soft-estimated symbols of the interfering UEs, along with its low-complexity variant. These strategies are compared with the robust maximum likelihood data detection with 1-bit ADCs. Numerical results examining the symbol error rate show that MMSE exhibits a considerable performance gain over MRC, whereas the proposed LMMD receiver significantly outperforms all the conventional receivers. Lastly, the proposed JD and its low-complexity variant provide a significant boost in comparison with the single-UE data detection.         ",
    "url": "https://arxiv.org/abs/2312.04183",
    "authors": [
      "Amin Radbord",
      "Italo Atzeni",
      "Antti Tolli"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2312.05490",
    "title": "Shapley Values-enabled Progressive Pseudo Bag Augmentation for Whole Slide Image Classification",
    "abstract": "           In computational pathology, whole-slide image (WSI) classification presents a formidable challenge due to its gigapixel resolution and limited fine-grained annotations. Multiple-instance learning (MIL) offers a weakly supervised solution, yet refining instance-level information from bag-level labels remains challenging. While most of the conventional MIL methods use attention scores to estimate instance importance scores (IIS) which contribute to the prediction of the slide labels, these often lead to skewed attention distributions and inaccuracies in identifying crucial instances. To address these issues, we propose a new approach inspired by cooperative game theory: employing Shapley values to assess each instance's contribution, thereby improving IIS estimation. The computation of the Shapley value is then accelerated using attention, meanwhile retaining the enhanced instance identification and prioritization. We further introduce a framework for the progressive assignment of pseudo bags based on estimated IIS, encouraging more balanced attention distributions in MIL models. Our extensive experiments on CAMELYON-16, BRACS, TCGA-LUNG, and TCGA-BRCA datasets show our method's superiority over existing state-of-the-art approaches, offering enhanced interpretability and class-wise insights. Our source code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2312.05490",
    "authors": [
      "Renao Yan",
      "Qiehe Sun",
      "Cheng Jin",
      "Yiqing Liu",
      "Yonghong He",
      "Tian Guan",
      "Hao Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.08207",
    "title": "Black-box Membership Inference Attacks against Fine-tuned Diffusion Models",
    "abstract": "           With the rapid advancement of diffusion-based image-generative models, the quality of generated images has become increasingly photorealistic. Moreover, with the release of high-quality pre-trained image-generative models, a growing number of users are downloading these pre-trained models to fine-tune them with downstream datasets for various image-generation tasks. However, employing such powerful pre-trained models in downstream tasks presents significant privacy leakage risks. In this paper, we propose the first reconstruction-based membership inference attack framework, tailored for recent diffusion models, and in the more stringent black-box access setting. Considering four distinct attack scenarios and three types of attacks, this framework is capable of targeting any popular conditional generator model, achieving high precision, evidenced by an impressive AUC of $0.95$.         ",
    "url": "https://arxiv.org/abs/2312.08207",
    "authors": [
      "Yan Pang",
      "Tianhao Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2401.16185",
    "title": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning",
    "abstract": "           Large language models (LLMs) have demonstrated significant potential in various tasks, including vulnerability detection. However, current efforts in this area are preliminary, lacking clarity on whether LLMs' vulnerability reasoning capabilities stem from the models themselves or external aids such as knowledge retrieval and tooling support. This paper aims to isolate LLMs' vulnerability reasoning from other capabilities, such as vulnerability knowledge adoption, context information retrieval, and structured output generation. We introduce LLM4Vuln, a unified evaluation framework that separates and assesses LLMs' vulnerability reasoning capabilities and examines improvements when combined with other enhancements. We conducted controlled experiments with 97 ground-truth vulnerabilities and 97 non-vulnerable cases in Solidity and Java, testing them in a total of 9,312 scenarios across four LLMs (GPT-4, GPT-3.5, Mixtral, and Llama 3). Our findings reveal the varying impacts of knowledge enhancement, context supplementation, prompt schemes, and models. Additionally, we identified 14 zero-day vulnerabilities in four pilot bug bounty programs, resulting in \\$3,576 in bounties.         ",
    "url": "https://arxiv.org/abs/2401.16185",
    "authors": [
      "Yuqiang Sun",
      "Daoyuan Wu",
      "Yue Xue",
      "Han Liu",
      "Wei Ma",
      "Lyuye Zhang",
      "Yang Liu",
      "Yingjiu Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2402.11940",
    "title": "AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization",
    "abstract": "           Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. This paper presents a novel adversarial attack strategy, AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by a customised differential evolution method to optimise the perturbations of pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchmark datasets against multiple victim models. The experimental results demonstrate that our method outperforms current leading-edge techniques by achieving consistently higher attack success rates.         ",
    "url": "https://arxiv.org/abs/2402.11940",
    "authors": [
      "Jiyao Li",
      "Mingze Ni",
      "Yifei Dong",
      "Tianqing Zhu",
      "Wei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.16567",
    "title": "Aligning Large Language Models to a Domain-specific Graph Database for NL2GQL",
    "abstract": "           Graph Databases (Graph DB) find extensive application across diverse domains such as finance, social networks, and medicine. Yet, the translation of Natural Language (NL) into the Graph Query Language (GQL), referred to as NL2GQL, poses significant challenges owing to its intricate and specialized nature. Some approaches have sought to utilize Large Language Models (LLMs) to address analogous tasks like text2SQL. Nonetheless, in the realm of NL2GQL tasks tailored to a particular domain, the absence of domain-specific NL-GQL data pairs adds complexity to aligning LLMs with the graph DB. To tackle this challenge, we present a well-defined pipeline. Initially, we utilize ChatGPT to generate NL-GQL data pairs, leveraging the provided graph DB with self-instruction. Subsequently, we employ the generated data to fine-tune LLMs, ensuring alignment between LLMs and the graph DB. Moreover, we find the importance of relevant schema in efficiently generating accurate GQLs. Thus, we introduce a method to extract relevant schema as the input context. We evaluate our method using two carefully constructed datasets derived from graph DBs in the finance and medicine domains, named FinGQL and MediGQL. Experimental results reveal that our approach significantly outperforms a set of baseline methods, with improvements of 5.90 and 6.36 absolute points on EM, and 6.00 and 7.09 absolute points on EX for FinGQL and MediGQL, respectively.         ",
    "url": "https://arxiv.org/abs/2402.16567",
    "authors": [
      "Yuanyuan Liang",
      "Keren Tan",
      "Tingyu Xie",
      "Wenbiao Tao",
      "Siyuan Wang",
      "Yunshi Lan",
      "Weining Qian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2402.17988",
    "title": "Constrained Decoding for Fill-in-the-Middle Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars",
    "abstract": "           Large Language Models are powerful tools for program synthesis and advanced auto-completion, but come with no guarantee that their output code is syntactically correct. This paper contributes an incremental parser that allows early rejection of syntactically incorrect code, as well as efficient detection of complete programs for fill-in-the-middle (FIM) tasks. We extend the Earley parsing algorithm to allow for left and right quotients of context-free grammars, and develop methods to handle quotienting of several context-sensitive features present in the grammars of many common programming languages. The result of these contributions is an efficient, general, and well-grounded method for left and right quotient parsing. To validate our theoretical contributions -- and the effectiveness of certain design decisions -- we evaluate our method on the particularly difficult case of FIM completion for Python 3, with syntax-correctness constraints. Our results demonstrate that constrained generation can significantly reduce the incidence of syntax errors in recommended code.         ",
    "url": "https://arxiv.org/abs/2402.17988",
    "authors": [
      "Daniel Melcer",
      "Nathan Fulton",
      "Sanjay Krishna Gouda",
      "Haifeng Qian"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2403.08109",
    "title": "VANP: Learning Where to See for Navigation with Self-Supervised Vision-Action Pre-Training",
    "abstract": "           Humans excel at efficiently navigating through crowds without collision by focusing on specific visual regions relevant to navigation. However, most robotic visual navigation methods rely on deep learning models pre-trained on vision tasks, which prioritize salient objects -- not necessarily relevant to navigation and potentially misleading. Alternative approaches train specialized navigation models from scratch, requiring significant computation. On the other hand, self-supervised learning has revolutionized computer vision and natural language processing, but its application to robotic navigation remains underexplored due to the difficulty of defining effective self-supervision signals. Motivated by these observations, in this work, we propose a Self-Supervised Vision-Action Model for Visual Navigation Pre-Training (VANP). Instead of detecting salient objects that are beneficial for tasks such as classification or detection, VANP learns to focus only on specific visual regions that are relevant to the navigation task. To achieve this, VANP uses a history of visual observations, future actions, and a goal image for self-supervision, and embeds them using two small Transformer Encoders. Then, VANP maximizes the information between the embeddings by using a mutual information maximization objective function. We demonstrate that most VANP-extracted features match with human navigation intuition. VANP achieves comparable performance as models learned end-to-end with half the training time and models trained on a large-scale, fully supervised dataset, i.e., ImageNet, with only 0.08% data.         ",
    "url": "https://arxiv.org/abs/2403.08109",
    "authors": [
      "Mohammad Nazeri",
      "Junzhe Wang",
      "Amirreza Payandeh",
      "Xuesu Xiao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.13965",
    "title": "ConGeo: Robust Cross-view Geo-localization across Ground View Variations",
    "abstract": "           Cross-view geo-localization aims at localizing a ground-level query image by matching it to its corresponding geo-referenced aerial view. In real-world scenarios, the task requires accommodating diverse ground images captured by users with varying orientations and reduced field of views (FoVs). However, existing learning pipelines are orientation-specific or FoV-specific, demanding separate model training for different ground view variations. Such models heavily depend on the North-aligned spatial correspondence and predefined FoVs in the training data, compromising their robustness across different settings. To tackle this challenge, we propose ConGeo, a single- and cross-view Contrastive method for Geo-localization: it enhances robustness and consistency in feature representations to improve a model's invariance to orientation and its resilience to FoV variations, by enforcing proximity between ground view variations of the same location. As a generic learning objective for cross-view geo-localization, when integrated into state-of-the-art pipelines, ConGeo significantly boosts the performance of three base models on four geo-localization benchmarks for diverse ground view variations and outperforms competing methods that train separate models for each ground view variation.         ",
    "url": "https://arxiv.org/abs/2403.13965",
    "authors": [
      "Li Mi",
      "Chang Xu",
      "Javiera Castillo-Navarro",
      "Syrielle Montariol",
      "Wen Yang",
      "Antoine Bosselut",
      "Devis Tuia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.19319",
    "title": "Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation",
    "abstract": "           We present Mesh2NeRF, an approach to derive ground-truth radiance fields from textured meshes for 3D generation tasks. Many 3D generative approaches represent 3D scenes as radiance fields for training. Their ground-truth radiance fields are usually fitted from multi-view renderings from a large-scale synthetic 3D dataset, which often results in artifacts due to occlusions or under-fitting issues. In Mesh2NeRF, we propose an analytic solution to directly obtain ground-truth radiance fields from 3D meshes, characterizing the density field with an occupancy function featuring a defined surface thickness, and determining view-dependent color through a reflection function considering both the mesh and environment lighting. Mesh2NeRF extracts accurate radiance fields which provides direct supervision for training generative NeRFs and single scene representation. We validate the effectiveness of Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in PSNR for view synthesis in single scene representation on the ABO dataset, a 0.69 PSNR enhancement in the single-view conditional generation of ShapeNet Cars, and notably improved mesh extraction from NeRF in the unconditional generation of Objaverse Mugs.         ",
    "url": "https://arxiv.org/abs/2403.19319",
    "authors": [
      "Yujin Chen",
      "Yinyu Nie",
      "Benjamin Ummenhofer",
      "Reiner Birkl",
      "Michael Paulitsch",
      "Matthias M\u00fcller",
      "Matthias Nie\u00dfner"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2404.13993",
    "title": "Zero-Shot Character Identification and Speaker Prediction in Comics via Iterative Multimodal Fusion",
    "abstract": "           Recognizing characters and predicting speakers of dialogue are critical for comic processing tasks, such as voice generation or translation. However, because characters vary by comic title, supervised learning approaches like training character classifiers which require specific annotations for each comic title are infeasible. This motivates us to propose a novel zero-shot approach, allowing machines to identify characters and predict speaker names based solely on unannotated comic images. In spite of their importance in real-world applications, these task have largely remained unexplored due to challenges in story comprehension and multimodal integration. Recent large language models (LLMs) have shown great capability for text understanding and reasoning, while their application to multimodal content analysis is still an open problem. To address this problem, we propose an iterative multimodal framework, the first to employ multimodal information for both character identification and speaker prediction tasks. Our experiments demonstrate the effectiveness of the proposed framework, establishing a robust baseline for these tasks. Furthermore, since our method requires no training data or annotations, it can be used as-is on any comic series.         ",
    "url": "https://arxiv.org/abs/2404.13993",
    "authors": [
      "Yingxuan Li",
      "Ryota Hinami",
      "Kiyoharu Aizawa",
      "Yusuke Matsui"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2404.14886",
    "title": "GCEPNet: Graph Convolution-Enhanced Expectation Propagation for Massive MIMO Detection",
    "abstract": "           Massive MIMO (multiple-input multiple-output) detection is an important topic in wireless communication and various machine learning based methods have been developed recently for this task. Expectation Propagation (EP) and its variants are widely used for MIMO detection and have achieved the best performance. However, EP-based solvers fail to capture the correlation between unknown variables, leading to a loss of information, and in addition, they are computationally expensive. In this paper, we show that the real-valued system can be modeled as spectral signal convolution on graph, through which the correlation between unknown variables can be captured. Based on such analysis, we propose graph convolution-enhanced expectation propagation (GCEPNet). GCEPNet incorporates data-dependent attention scores into Chebyshev polynomial for powerful graph convolution with better generalization capacity. It enables a better estimation of the cavity distribution for EP and empirically achieves the state-of-the-art (SOTA) MIMO detection performance with much faster inference speed. To our knowledge, we are the first to shed light on the connection between the system model and graph convolution, and the first to design the data-dependent coefficients for graph convolution.         ",
    "url": "https://arxiv.org/abs/2404.14886",
    "authors": [
      "Qincheng Lu",
      "Sitao Luan",
      "Xiao-Wen Chang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.15254",
    "title": "UniMERNet: A Universal Network for Real-World Mathematical Expression Recognition",
    "abstract": "           The paper introduces the UniMER dataset, marking the first study on Mathematical Expression Recognition (MER) targeting complex real-world scenarios. The UniMER dataset includes a large-scale training set, UniMER-1M, which offers unprecedented scale and diversity with one million training instances to train high-quality, robust models. Additionally, UniMER features a meticulously designed, diverse test set, UniMER-Test, which covers a variety of formula distributions found in real-world scenarios, providing a more comprehensive and fair evaluation. To better utilize the UniMER dataset, the paper proposes a Universal Mathematical Expression Recognition Network (UniMERNet), tailored to the characteristics of formula recognition. UniMERNet consists of a carefully designed encoder that incorporates detail-aware and local context features, and an optimized decoder for accelerated performance. Extensive experiments conducted using the UniMER-1M dataset and UniMERNet demonstrate that training on the large-scale UniMER-1M dataset can produce a more generalizable formula recognition model, significantly outperforming all previous datasets. Furthermore, the introduction of UniMERNet enhances the model's performance in formula recognition, achieving higher accuracy and speeds. All data, models, and code are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2404.15254",
    "authors": [
      "Bin Wang",
      "Zhuangcheng Gu",
      "Guang Liang",
      "Chao Xu",
      "Bo Zhang",
      "Botian Shi",
      "Conghui He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2404.17399",
    "title": "Evaluations of Machine Learning Privacy Defenses are Misleading",
    "abstract": "           Empirical defenses for machine learning privacy forgo the provable guarantees of differential privacy in the hope of achieving higher utility while resisting realistic adversaries. We identify severe pitfalls in existing empirical privacy evaluations (based on membership inference attacks) that result in misleading conclusions. In particular, we show that prior evaluations fail to characterize the privacy leakage of the most vulnerable samples, use weak attacks, and avoid comparisons with practical differential privacy baselines. In 5 case studies of empirical privacy defenses, we find that prior evaluations underestimate privacy leakage by an order of magnitude. Under our stronger evaluation, none of the empirical defenses we study are competitive with a properly tuned, high-utility DP-SGD baseline (with vacuous provable guarantees).         ",
    "url": "https://arxiv.org/abs/2404.17399",
    "authors": [
      "Michael Aerni",
      "Jie Zhang",
      "Florian Tram\u00e8r"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.18519",
    "title": "On the Impact of Data Heterogeneity in Federated Learning Environments with Application to Healthcare Networks",
    "abstract": "           Federated Learning (FL) allows multiple privacy-sensitive applications to leverage their dataset for a global model construction without any disclosure of the information. One of those domains is healthcare, where groups of silos collaborate in order to generate a global predictor with improved accuracy and generalization. However, the inherent challenge lies in the high heterogeneity of medical data, necessitating sophisticated techniques for assessment and compensation. This paper presents a comprehensive exploration of the mathematical formalization and taxonomy of heterogeneity within FL environments, focusing on the intricacies of medical data. In particular, we address the evaluation and comparison of the most popular FL algorithms with respect to their ability to cope with quantity-based, feature and label distribution-based heterogeneity. The goal is to provide a quantitative evaluation of the impact of data heterogeneity in FL systems for healthcare networks as well as a guideline on FL algorithm selection. Our research extends beyond existing studies by benchmarking seven of the most common FL algorithms against the unique challenges posed by medical data use cases. The paper targets the prediction of the risk of stroke recurrence through a set of tabular clinical reports collected by different federated hospital silos: data heterogeneity frequently encountered in this scenario and its impact on FL performance are discussed.         ",
    "url": "https://arxiv.org/abs/2404.18519",
    "authors": [
      "Usevalad Milasheuski",
      "Luca Barbieri",
      "Bernardo Camajori Tedeschini",
      "Monica Nicoli",
      "Stefano Savazzi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2405.01787",
    "title": "Towards Neural Synthesis for SMT-Assisted Proof-Oriented Programming",
    "abstract": "           Proof-oriented programs mix computational content with proofs of program correctness. However, the human effort involved in programming and proving is still substantial, despite the use of Satisfiability Modulo Theories (SMT) solvers to automate proofs in languages such as F*. Seeking to spur research on using AI to automate the construction of proof-oriented programs, we curate a dataset of 600K lines of open-source F* programs and proofs, including software used in production systems ranging from Windows and Linux to Python and Firefox. Our dataset includes around 32K top-level F* definitions, each representing a type-directed program and proof synthesis problem producing a definition given a formal specification expressed as an F* type. We provide a program fragment checker that queries F* to check the correctness of candidate solutions. We also report on an extended version of our dataset containing a total of 940K lines of programs and proofs, with a total of 54k top-level F* definitions. We believe this is the largest corpus of SMT-assisted program proofs coupled with a reproducible program-fragment checker. Grounded in this dataset, we investigate the use of AI to synthesize programs and their proofs in F*, with promising results. Our main finding in that the performance of fine-tuned smaller language models (such as Phi-2 or StarCoder) compare favorably with large language models (such as GPT-4), at a much lower computational cost. We also identify various type-based retrieval augmentation techniques and find that they boost performance significantly. With detailed error analysis and case studies, we identify potential strengths and weaknesses of models and techniques and suggest directions for future improvements.         ",
    "url": "https://arxiv.org/abs/2405.01787",
    "authors": [
      "Saikat Chakraborty",
      "Gabriel Ebner",
      "Siddharth Bhat",
      "Sarah Fakhoury",
      "Sakina Fatima",
      "Shuvendu Lahiri",
      "Nikhil Swamy"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2405.04370",
    "title": "Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos",
    "abstract": "           Understanding how humans would behave during hand-object interaction is vital for applications in service robot manipulation and extended reality. To achieve this, some recent works have been proposed to simultaneously forecast hand trajectories and object affordances on human egocentric videos. The joint prediction serves as a comprehensive representation of future hand-object interactions in 2D space, indicating potential human motion and motivation. However, the existing approaches mostly adopt the autoregressive paradigm for unidirectional prediction, which lacks mutual constraints within the holistic future sequence, and accumulates errors along the time axis. Meanwhile, these works basically overlook the effect of camera egomotion on first-person view predictions. To address these limitations, we propose a novel diffusion-based interaction prediction method, namely Diff-IP2D, to forecast future hand trajectories and object affordances concurrently in an iterative non-autoregressive manner. We transform the sequential 2D images into latent feature space and design a denoising diffusion model to predict future latent interaction features conditioned on past ones. Motion features are further integrated into the conditional denoising process to enable Diff-IP2D aware of the camera wearer's dynamics for more accurate interaction prediction. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art baselines on both the off-the-shelf metrics and our newly proposed evaluation protocol. This highlights the efficacy of leveraging a generative paradigm for 2D hand-object interaction prediction. The code of Diff-IP2D will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2405.04370",
    "authors": [
      "Junyi Ma",
      "Jingyi Xu",
      "Xieyuanli Chen",
      "Hesheng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2405.19921",
    "title": "MCDS-VSS: Moving Camera Dynamic Scene Video Semantic Segmentation by Filtering with Self-Supervised Geometry and Motion",
    "abstract": "           Autonomous systems, such as self-driving cars, rely on reliable semantic environment perception for decision making. Despite great advances in video semantic segmentation, existing approaches ignore important inductive biases and lack structured and interpretable internal representations. In this work, we propose MCDS-VSS, a structured filter model that learns in a self-supervised manner to estimate scene geometry and ego-motion of the camera, while also estimating the motion of external objects. Our model leverages these representations to improve the temporal consistency of semantic segmentation without sacrificing segmentation accuracy. MCDS-VSS follows a prediction-fusion approach in which scene geometry and camera motion are first used to compensate for ego-motion, then residual flow is used to compensate motion of dynamic objects, and finally the predicted scene features are fused with the current features to obtain a temporally consistent scene segmentation. Our model parses automotive scenes into multiple decoupled interpretable representations such as scene geometry, ego-motion, and object motion. Quantitative evaluation shows that MCDS-VSS achieves superior temporal consistency on video sequences while retaining competitive segmentation performance.         ",
    "url": "https://arxiv.org/abs/2405.19921",
    "authors": [
      "Angel Villar-Corrales",
      "Moritz Austermann",
      "Sven Behnke"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2406.06949",
    "title": "Triple-domain Feature Learning with Frequency-aware Memory Enhancement for Moving Infrared Small Target Detection",
    "abstract": "           As a sub-field of object detection, moving infrared small target detection presents significant challenges due to tiny target sizes and low contrast against backgrounds. Currently-existing methods primarily rely on the features extracted only from spatio-temporal domain. Frequency domain has hardly been concerned yet, although it has been widely applied in image processing. To extend feature source domains and enhance feature representation, we propose a new Triple-domain Strategy (Tridos) with the frequency-aware memory enhancement on spatio-temporal domain for infrared small target detection. In this scheme, it effectively detaches and enhances frequency features by a local-global frequency-aware module with Fourier transform. Inspired by human visual system, our memory enhancement is designed to capture the spatial relations of infrared targets among video frames. Furthermore, it encodes temporal dynamics motion features via differential learning and residual enhancing. Additionally, we further design a residual compensation to reconcile possible cross-domain feature mismatches. To our best knowledge, proposed Tridos is the first work to explore infrared target feature learning comprehensively in spatio-temporal-frequency domains. The extensive experiments on three datasets (i.e., DAUB, ITSDT-15K and IRDST) validate that our triple-domain infrared feature learning scheme could often be obviously superior to state-of-the-art ones. Source codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.06949",
    "authors": [
      "Weiwei Duan",
      "Luping Ji",
      "Shengjia Chen",
      "Sicheng Zhu",
      "Mao Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.08150",
    "title": "Hypergraph Multi-modal Large Language Model: Exploiting EEG and Eye-tracking Modalities to Evaluate Heterogeneous Responses for Video Understanding",
    "abstract": "           Understanding of video creativity and content often varies among individuals, with differences in focal points and cognitive levels across different ages, experiences, and genders. There is currently a lack of research in this area, and most existing benchmarks suffer from several drawbacks: 1) a limited number of modalities and answers with restrictive length; 2) the content and scenarios within the videos are excessively monotonous, transmitting allegories and emotions that are overly simplistic. To bridge the gap to real-world applications, we introduce a large-scale Subjective Response Indicators for Advertisement Videos dataset, namely SRI-ADV. Specifically, we collected real changes in Electroencephalographic (EEG) and eye-tracking regions from different demographics while they viewed identical video content. Utilizing this multi-modal dataset, we developed tasks and protocols to analyze and evaluate the extent of cognitive understanding of video content among different users. Along with the dataset, we designed a Hypergraph Multi-modal Large Language Model (HMLLM) to explore the associations among different demographics, video elements, EEG, and eye-tracking indicators. HMLLM could bridge semantic gaps across rich modalities and integrate information beyond different modalities to perform logical reasoning. Extensive experimental evaluations on SRI-ADV and other additional video-based generative performance benchmarks demonstrate the effectiveness of our method. The codes and dataset will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.08150",
    "authors": [
      "Minghui Wu",
      "Chenxu Zhao",
      "Anyang Su",
      "Donglin Di",
      "Tianyu Fu",
      "Da An",
      "Min He",
      "Ya Gao",
      "Meng Ma",
      "Kun Yan",
      "Ping Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.08572",
    "title": "Boosting Adversarial Transferability for Skeleton-based Action Recognition via Exploring the Model Posterior Space",
    "abstract": "           Skeletal motion plays a pivotal role in human activity recognition (HAR). Recently, attack methods have been proposed to identify the universal vulnerability of skeleton-based HAR(S-HAR). However, the research of adversarial transferability on S-HAR is largely missing. More importantly, existing attacks all struggle in transfer across unknown S-HAR models. We observed that the key reason is that the loss landscape of the action recognizers is rugged and sharp. Given the established correlation in prior studies~\\cite{qin2022boosting,wu2020towards} between loss landscape and adversarial transferability, we assume and empirically validate that smoothing the loss landscape could potentially improve adversarial transferability on S-HAR. This is achieved by proposing a new post-train Dual Bayesian strategy, which can effectively explore the model posterior space for a collection of surrogates without the need for re-training. Furthermore, to craft adversarial examples along the motion manifold, we incorporate the attack gradient with information of the motion dynamics in a Bayesian manner. Evaluated on benchmark datasets, e.g. HDM05 and NTU 60, the average transfer success rate can reach as high as 35.9\\% and 45.5\\% respectively. In comparison, current state-of-the-art skeletal attacks achieve only 3.6\\% and 9.8\\%. The high adversarial transferability remains consistent across various surrogate, victim, and even defense models. Through a comprehensive analysis of the results, we provide insights on what surrogates are more likely to exhibit transferability, to shed light on future research.         ",
    "url": "https://arxiv.org/abs/2407.08572",
    "authors": [
      "Yunfeng Diao",
      "Baiqi Wu",
      "Ruixuan Zhang",
      "Xun Yang",
      "Meng Wang",
      "He Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.08762",
    "title": "Commute-Time-Optimised Graphs for GNNs",
    "abstract": "           We explore graph rewiring methods that optimise commute time. Recent graph rewiring approaches facilitate long-range interactions in sparse graphs, making such rewirings commute-time-optimal on average. However, when an expert prior exists on which node pairs should or should not interact, a superior rewiring would favour short commute times between these privileged node pairs. We construct two synthetic datasets with known priors reflecting realistic settings, and use these to motivate two bespoke rewiring methods that incorporate the known prior. We investigate the regimes where our rewiring improves test performance on the synthetic datasets. Finally, we perform a case study on a real-world citation graph to investigate the practical implications of our work.         ",
    "url": "https://arxiv.org/abs/2407.08762",
    "authors": [
      "Igor Sterner",
      "Shiye Su",
      "Petar Veli\u010dkovi\u0107"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.12113",
    "title": "A Graph-based Adversarial Imitation Learning Framework for Reliable & Realtime Fleet Scheduling in Urban Air Mobility",
    "abstract": "           The advent of Urban Air Mobility (UAM) presents the scope for a transformative shift in the domain of urban transportation. However, its widespread adoption and economic viability depends in part on the ability to optimally schedule the fleet of aircraft across vertiports in a UAM network, under uncertainties attributed to airspace congestion, changing weather conditions, and varying demands. This paper presents a comprehensive optimization formulation of the fleet scheduling problem, while also identifying the need for alternate solution approaches, since directly solving the resulting integer nonlinear programming problem is computationally prohibitive for daily fleet scheduling. Previous work has shown the effectiveness of using (graph) reinforcement learning (RL) approaches to train real-time executable policy models for fleet scheduling. However, such policies can often be brittle on out-of-distribution scenarios or edge cases. Moreover, training performance also deteriorates as the complexity (e.g., number of constraints) of the problem increases. To address these issues, this paper presents an imitation learning approach where the RL-based policy exploits expert demonstrations yielded by solving the exact optimization using a Genetic Algorithm. The policy model comprises Graph Neural Network (GNN) based encoders that embed the space of vertiports and aircraft, Transformer networks to encode demand, passenger fare, and transport cost profiles, and a Multi-head attention (MHA) based decoder. Expert demonstrations are used through the Generative Adversarial Imitation Learning (GAIL) algorithm. Interfaced with a UAM simulation environment involving 8 vertiports and 40 aircrafts, in terms of the daily profits earned reward, the new imitative approach achieves better mean performance and remarkable improvement in the case of unseen worst-case scenarios, compared to pure RL results.         ",
    "url": "https://arxiv.org/abs/2407.12113",
    "authors": [
      "Prithvi Poddar",
      "Steve Paul",
      "Souma Chowdhury"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2407.14051",
    "title": "Robust error estimates of PINN in one-dimensional boundary value problems for linear elliptic equations",
    "abstract": "           In this paper, we study physics-informed neural networks (PINN) to approximate solutions to one-dimensional boundary value problems for linear elliptic equations and establish robust error estimates of PINN regardless of the quantities of the coefficients. In particular, we rigorously demonstrate the existence and uniqueness of solutions using the Sobolev space theory based on a variational approach. Deriving $L^2$-contraction estimates, we show that the error, defined as the mean square of the differences between the true solution and our trial function at the sample points, is dominated by the training loss. Furthermore, we show that as the quantities of the coefficients for the differential equation increase, the error-to-loss ratio rapidly decreases. Our theoretical and experimental results confirm the robustness of the error regardless of the quantities of the coefficients.         ",
    "url": "https://arxiv.org/abs/2407.14051",
    "authors": [
      "Jihahm Yoo",
      "Haesung Lee"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ]
  },
  {
    "id": "arXiv:2407.17622",
    "title": "Towards Neural Network based Cognitive Models of Dynamic Decision-Making by Humans",
    "abstract": "           Modeling human cognitive processes in dynamic decision-making tasks has been an endeavor in AI for a long time because such models can help make AI systems more intuitive, personalized, mitigate any human biases, and enhance training in simulation. Some initial work has attempted to utilize neural networks (and large language models) but often assumes one common model for all humans and aims to emulate human behavior in aggregate. However, the behavior of each human is distinct, heterogeneous, and relies on specific past experiences in certain tasks. For instance, consider two individuals responding to a phishing email: one who has previously encountered and identified similar threats may recognize it quickly, while another without such experience might fall for the scam. In this work, we build on Instance Based Learning (IBL) that posits that human decisions are based on similar situations encountered in the past. However, IBL relies on simple fixed form functions to capture the mapping from past situations to current decisions. To that end, we propose two new attention-based neural network models to have open form non-linear functions to model distinct and heterogeneous human decision-making in dynamic settings. We experiment with two distinct datasets gathered from human subject experiment data, one focusing on detection of phishing email by humans and another where humans act as attackers in a cybersecurity setting and decide on an attack option. We conducted extensive experiments with our two neural network models, IBL, and GPT3.5, and demonstrate that the neural network models outperform IBL significantly in representing human decision-making, while providing similar interpretability of human decisions as IBL. Overall, our work yields promising results for further use of neural networks in cognitive modeling of human decision making.         ",
    "url": "https://arxiv.org/abs/2407.17622",
    "authors": [
      "Changyu Chen",
      "Shashank Reddy Chirra",
      "Maria Jos\u00e9 Ferreira",
      "Cleotilde Gonzalez",
      "Arunesh Sinha",
      "Pradeep Varakantham"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2408.05074",
    "title": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large Language Model Structuring of Large-Scale Unstructured Electronic Health Records",
    "abstract": "           Accurate patient selection is critical in radiotherapy (RT) to prevent ineffective treatments. Traditional survival prediction models, relying on structured data, often lack precision. This study explores the potential of large language models (LLMs) to structure unstructured electronic health record (EHR) data, thereby improving survival prediction accuracy through comprehensive clinical information integration. Data from 34,276 patients treated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed, encompassing both structured and unstructured data. An open-source LLM was used to structure the unstructured EHR data via single-shot learning, with its performance compared against a domain-specific medical LLM and a smaller variant. Survival prediction models were developed using statistical, machine learning, and deep learning approaches, incorporating both structured and LLM-structured data. Clinical experts evaluated the accuracy of the LLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring unstructured EHR data without additional training, significantly outperforming the domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs were more effective, particularly in extracting clinically relevant features like general condition and disease extent, which closely correlated with patient survival. Incorporating LLM-structured clinical features into survival prediction models significantly improved accuracy, with the C-index of deep learning models increasing from 0.737 to 0.820. These models also became more interpretable by emphasizing clinically significant factors. This study shows that general-domain LLMs, even without specific medical training, can effectively structure large-scale unstructured EHR data, substantially enhancing the accuracy and interpretability of clinical predictive models.         ",
    "url": "https://arxiv.org/abs/2408.05074",
    "authors": [
      "Sangjoon Park",
      "Chan Woo Wee",
      "Seo Hee Choi",
      "Kyung Hwan Kim",
      "Jee Suk Chang",
      "Hong In Yoon",
      "Ik Jae Lee",
      "Yong Bae Kim",
      "Jaeho Cho",
      "Ki Chang Keum",
      "Chang Geol Lee",
      "Hwa Kyung Byun",
      "Woong Sub Koom"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.10468",
    "title": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted Influence Functions",
    "abstract": "           The responses generated by Large Language Models (LLMs) can include sensitive information from individuals and organizations, leading to potential privacy leakage. This work implements Influence Functions (IFs) to trace privacy leakage back to the training data, thereby mitigating privacy concerns of Language Models (LMs). However, we notice that current IFs struggle to accurately estimate the influence of tokens with large gradient norms, potentially overestimating their influence. When tracing the most influential samples, this leads to frequently tracing back to samples with large gradient norm tokens, overshadowing the actual most influential samples even if their influences are well estimated. To address this issue, we propose Heuristically Adjusted IF (HAIF), which reduces the weight of tokens with large gradient norms, thereby significantly improving the accuracy of tracing the most influential samples. To establish easily obtained groundtruth for tracing privacy leakage, we construct two datasets, PII-E and PII-CR, representing two distinct scenarios: one with identical text in the model outputs and pre-training data, and the other where models leverage their reasoning abilities to generate text divergent from pre-training data. HAIF significantly improves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E dataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA IFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs on real-world pretraining data CLUECorpus2020, demonstrating strong robustness regardless prompt and response lengths.         ",
    "url": "https://arxiv.org/abs/2408.10468",
    "authors": [
      "Jinxin Liu",
      "Zao Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2408.13106",
    "title": "NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks",
    "abstract": "           Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition/translation, speaker verification and diarization, etc. However, most of current approaches are computationally expensive due to lacking in sub-sampling or using clustering based speech quantization. In this paper, we propose a simplified and more efficient self-supervised learning framework termed as NeMo Encoder for Speech Tasks (NEST). Specifically, we adopt the FastConformer architecture, which has an 8x sub-sampling rate and is faster than Transformer or Conformer architectures. Instead of clustering-based token generation, we resort to fixed random projection for its simplicity and effectiveness. We also propose a generalized noisy speech augmentation that teaches the model to disentangle the main speaker from noise or other speakers. Experiments show that NEST improves over existing self-supervised models on a variety of speech processing tasks.         ",
    "url": "https://arxiv.org/abs/2408.13106",
    "authors": [
      "He Huang",
      "Taejin Park",
      "Kunal Dhawan",
      "Ivan Medennikov",
      "Krishna C. Puvvada",
      "Nithin Rao Koluguri",
      "Weiqing Wang",
      "Jagadeesh Balam",
      "Boris Ginsburg"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2408.13958",
    "title": "Prediction of COPD Using Machine Learning, Clinical Summary Notes, and Vital Signs",
    "abstract": "           Chronic obstructive pulmonary disease (COPD) is a chronic inflammatory lung disease that causes obstructed airflow from the lungs. In the United States, more than 15.7 million Americans have been diagnosed with COPD, with 96% of individuals living with at least one other chronic health condition. It is the 4th leading cause of death in the country. Over 2.2 million patients are admitted to hospitals annually due to COPD exacerbations. Monitoring and predicting patient exacerbations on-time could save their life. This paper presents two different predictive models to predict COPD exacerbation using AI and natural language processing (NLP) approaches. These models use respiration summary notes, symptoms, and vital signs. To train and test these models, data records containing physiologic signals and vital signs time series were used. These records were captured from patient monitors and comprehensive clinical data obtained from hospital medical information systems for tens of thousands of Intensive Care Unit (ICU) patients. We achieved an area under the Receiver operating characteristic (ROC) curve of 0.82 in detection and prediction of COPD exacerbation.         ",
    "url": "https://arxiv.org/abs/2408.13958",
    "authors": [
      "Negar Orangi-Fard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.14762",
    "title": "Explainable Hierarchical Urban Representation Learning for Commuting Flow Prediction",
    "abstract": "           Commuting flow prediction is an essential task for municipal operations in the real world. Previous studies have revealed that it is feasible to estimate the commuting origin-destination (OD) demand within a city using multiple auxiliary data. However, most existing methods are not suitable to deal with a similar task at a large scale, namely within a prefecture or the whole nation, owing to the increased number of geographical units that need to be maintained. In addition, region representation learning is a universal approach for gaining urban knowledge for diverse metropolitan downstream tasks. Although many researchers have developed comprehensive frameworks to describe urban units from multi-source data, they have not clarified the relationship between the selected geographical elements. Furthermore, metropolitan areas naturally preserve ranked structures, like cities and their inclusive districts, which makes elucidating relations between cross-level urban units necessary. Therefore, we develop a heterogeneous graph-based model to generate meaningful region embeddings at multiple spatial resolutions for predicting different types of inter-level OD flows. To demonstrate the effectiveness of the proposed method, extensive experiments were conducted using real-world aggregated mobile phone datasets collected from Shizuoka Prefecture, Japan. The results indicate that our proposed model outperforms existing models in terms of a uniform urban structure. We extend the understanding of predicted results using reasonable explanations to enhance the credibility of the model.         ",
    "url": "https://arxiv.org/abs/2408.14762",
    "authors": [
      "Mingfei Cai",
      "Yanbo Pang",
      "Yoshihide Sekimoto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2408.16686",
    "title": "CW-CNN & CW-AN: Convolutional Networks and Attention Networks for CW-Complexes",
    "abstract": "           We present a novel framework for learning on CW-complex structured data points. Recent advances have discussed CW-complexes as ideal learning representations for problems in cheminformatics. However, there is a lack of available machine learning methods suitable for learning on CW-complexes. In this paper we develop notions of convolution and attention that are well defined for CW-complexes. These notions enable us to create the first Hodge informed neural network that can receive a CW-complex as input. We illustrate and interpret this framework in the context of supervised prediction.         ",
    "url": "https://arxiv.org/abs/2408.16686",
    "authors": [
      "Rahul Khorana"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.17235",
    "title": "AI-Driven Intrusion Detection Systems (IDS) on the ROAD Dataset: A Comparative Analysis for Automotive Controller Area Network (CAN)",
    "abstract": "           The integration of digital devices in modern vehicles has revolutionized automotive technology, enhancing safety and the overall driving experience. The Controller Area Network (CAN) bus is a central system for managing in-vehicle communication between the electronic control units (ECUs). However, the CAN protocol poses security challenges due to inherent vulnerabilities, lacking encryption and authentication, which, combined with an expanding attack surface, necessitates robust security measures. In response to this challenge, numerous Intrusion Detection Systems (IDS) have been developed and deployed. Nonetheless, an open, comprehensive, and realistic dataset to test the effectiveness of such IDSs remains absent in the existing literature. This paper addresses this gap by considering the latest ROAD dataset, containing stealthy and sophisticated injections. The methodology involves dataset labelling and the implementation of both state-of-the-art deep learning models and traditional machine learning models to show the discrepancy in performance between the datasets most commonly used in the literature and the ROAD dataset, a more realistic alternative.         ",
    "url": "https://arxiv.org/abs/2408.17235",
    "authors": [
      "Lorenzo Guerra",
      "Linhan Xu",
      "Paolo Bellavista",
      "Thomas Chapuis",
      "Guillaume Duc",
      "Pavlo Mozharovskyi",
      "Van-Tam Nguyen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.01722",
    "title": "ACCESS-FL: Agile Communication and Computation for Efficient Secure Aggregation in Stable Federated Learning Networks",
    "abstract": "           Federated Learning (FL) is a promising distributed learning framework designed for privacy-aware applications. FL trains models on client devices without sharing the client's data and generates a global model on a server by aggregating model updates. Traditional FL approaches risk exposing sensitive client data when plain model updates are transmitted to the server, making them vulnerable to security threats such as model inversion attacks where the server can infer the client's original training data from monitoring the changes of the trained model in different rounds. Google's Secure Aggregation (SecAgg) protocol addresses this threat by employing a double-masking technique, secret sharing, and cryptography computations in honest-but-curious and adversarial scenarios with client dropouts. However, in scenarios without the presence of an active adversary, the computational and communication cost of SecAgg significantly increases by growing the number of clients. To address this issue, in this paper, we propose ACCESS-FL, a communication-and-computation-efficient secure aggregation method designed for honest-but-curious scenarios in stable FL networks with a limited rate of client dropout. ACCESS-FL reduces the computation/communication cost to a constant level (independent of the network size) by generating shared secrets between only two clients and eliminating the need for double masking, secret sharing, and cryptography computations. To evaluate the performance of ACCESS-FL, we conduct experiments using the MNIST, FMNIST, and CIFAR datasets to verify the performance of our proposed method. The evaluation results demonstrate that our proposed method significantly reduces computation and communication overhead compared to state-of-the-art methods, SecAgg and SecAgg+.         ",
    "url": "https://arxiv.org/abs/2409.01722",
    "authors": [
      "Niousha Nazemi",
      "Omid Tavallaie",
      "Shuaijun Chen",
      "Anna Maria Mandalari",
      "Kanchana Thilakarathna",
      "Ralph Holz",
      "Hamed Haddadi",
      "Albert Y. Zomaya"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2409.01731",
    "title": "Stacked ensemble\\-based mutagenicity prediction model using multiple modalities with graph attention network",
    "abstract": "           Mutagenicity is a concern due to its association with genetic mutations which can result in a variety of negative consequences, including the development of cancer. Earlier identification of mutagenic compounds in the drug development process is therefore crucial for preventing the progression of unsafe candidates and reducing development costs. While computational techniques, especially machine learning models have become increasingly prevalent for this endpoint, they rely on a single modality. In this work, we introduce a novel stacked ensemble based mutagenicity prediction model which incorporate multiple modalities such as simplified molecular input line entry system (SMILES) and molecular graph. These modalities capture diverse information about molecules such as substructural, physicochemical, geometrical and topological. To derive substructural, geometrical and physicochemical information, we use SMILES, while topological information is extracted through a graph attention network (GAT) via molecular graph. Our model uses a stacked ensemble of machine learning classifiers to make predictions using these multiple features. We employ the explainable artificial intelligence (XAI) technique SHAP (Shapley Additive Explanations) to determine the significance of each classifier and the most relevant features in the prediction. We demonstrate that our method surpasses SOTA methods on two standard datasets across various metrics. Notably, we achieve an area under the curve of 95.21\\% on the Hansen benchmark dataset, affirming the efficacy of our method in predicting mutagenicity. We believe that this research will captivate the interest of both clinicians and computational biologists engaged in translational research.         ",
    "url": "https://arxiv.org/abs/2409.01731",
    "authors": [
      "Tanya Liyaqat",
      "Tanvir Ahmad",
      "Mohammad Kashif",
      "Chandni Saxena"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.02066",
    "title": "Robust Clustering on High-Dimensional Data with Stochastic Quantization",
    "abstract": "           This paper addresses the limitations of traditional vector quantization (clustering) algorithms, particularly K-Means and its variant K-Means++, and explores the Stochastic Quantization (SQ) algorithm as a scalable alternative for high-dimensional unsupervised and semi-supervised learning problems. Some traditional clustering algorithms suffer from inefficient memory utilization during computation, necessitating the loading of all data samples into memory, which becomes impractical for large-scale datasets. While variants such as Mini-Batch K-Means partially mitigate this issue by reducing memory usage, they lack robust theoretical convergence guarantees due to the non-convex nature of clustering problems. In contrast, the Stochastic Quantization algorithm provides strong theoretical convergence guarantees, making it a robust alternative for clustering tasks. We demonstrate the computational efficiency and rapid convergence of the algorithm on an image classification problem with partially labeled data, comparing model accuracy across various ratios of labeled to unlabeled data. To address the challenge of high dimensionality, we trained Triplet Network to encode images into low-dimensional representations in a latent space, which serve as a basis for comparing the efficiency of both the Stochastic Quantization algorithm and traditional quantization algorithms. Furthermore, we enhance the algorithm's convergence speed by introducing modifications with an adaptive learning rate.         ",
    "url": "https://arxiv.org/abs/2409.02066",
    "authors": [
      "Anton Kozyriev",
      "Vladimir Norkin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2409.02115",
    "title": "Deep Neural Implicit Representation of Accessibility for Multi-Axis Manufacturing",
    "abstract": "           One of the main concerns in design and process planning for multi-axis additive and subtractive manufacturing is collision avoidance between moving objects (e.g., tool assemblies) and stationary objects (e.g., a part unified with fixtures). The collision measure for various pairs of relative rigid translations and rotations between the two pointsets can be conceptualized by a compactly supported scalar field over the 6D non-Euclidean configuration space. Explicit representation and computation of this field is costly in both time and space. If we fix $O(m)$ sparsely sampled rotations (e.g., tool orientations), computation of the collision measure field as a convolution of indicator functions of the 3D pointsets over a uniform grid (i.e., voxelized geometry) of resolution $O(n^3)$ via fast Fourier transforms (FFTs) scales as in $O(mn^3 \\log n)$ in time and $O(mn^3)$ in space. In this paper, we develop an implicit representation of the collision measure field via deep neural networks (DNNs). We show that our approach is able to accurately interpolate the collision measure from a sparse sampling of rotations, and can represent the collision measure field with a small memory footprint. Moreover, we show that this representation can be efficiently updated through fine-tuning to more efficiently train the network on multi-resolution data, as well as accommodate incremental changes to the geometry (such as might occur in iterative processes such as topology optimization of the part subject to CNC tool accessibility constraints).         ",
    "url": "https://arxiv.org/abs/2409.02115",
    "authors": [
      "George P. Harabin",
      "Amir Mirzendehdel",
      "Morad Behandish"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.02649",
    "title": "OpenFact at CheckThat! 2024: Combining Multiple Attack Methods for Effective Adversarial Text Generation",
    "abstract": "           This paper presents the experiments and results for the CheckThat! Lab at CLEF 2024 Task 6: Robustness of Credibility Assessment with Adversarial Examples (InCrediblAE). The primary objective of this task was to generate adversarial examples in five problem domains in order to evaluate the robustness of widely used text classification methods (fine-tuned BERT, BiLSTM, and RoBERTa) when applied to credibility assessment issues. This study explores the application of ensemble learning to enhance adversarial attacks on natural language processing (NLP) models. We systematically tested and refined several adversarial attack methods, including BERT-Attack, Genetic algorithms, TextFooler, and CLARE, on five datasets across various misinformation tasks. By developing modified versions of BERT-Attack and hybrid methods, we achieved significant improvements in attack effectiveness. Our results demonstrate the potential of modification and combining multiple methods to create more sophisticated and effective adversarial attack strategies, contributing to the development of more robust and secure systems.         ",
    "url": "https://arxiv.org/abs/2409.02649",
    "authors": [
      "W\u0142odzimierz Lewoniewski",
      "Piotr Stolarski",
      "Milena Str\u00f3\u017cyna",
      "Elzbieta Lewa\u0144ska",
      "Aleksandra Wojewoda",
      "Ewelina Ksi\u0119\u017cniak",
      "Marcin Sawi\u0144ski"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.02727",
    "title": "Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?",
    "abstract": "           The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.         ",
    "url": "https://arxiv.org/abs/2409.02727",
    "authors": [
      "Yixuan Tang",
      "Yi Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2312.14847",
    "title": "Large Scale Training of Graph Neural Networks for Optimal Markov-Chain Partitioning Using the Kemeny Constant",
    "abstract": "           Traditional clustering algorithms often struggle to capture the complex relationships within graphs and generalise to arbitrary clustering criteria. The emergence of graph neural networks (GNNs) as a powerful framework for learning representations of graph data provides new approaches to solving the problem. Previous work has shown GNNs to be capable of proposing partitionings using a variety of criteria, however, these approaches have not yet been extended to work on Markov chains or kinetic networks. These arise frequently in the study of molecular systems and are of particular interest to the biochemical modelling community. In this work, we propose several GNN-based architectures to tackle the graph partitioning problem for Markov Chains described as kinetic networks. This approach aims to minimize how much a proposed partitioning changes the Kemeny constant. We propose using an encoder-decoder architecture and show how simple GraphSAGE-based GNNs with linear layers can outperform much larger and more expressive attention-based models in this context. As a proof of concept, we first demonstrate the method's ability to cluster randomly connected graphs. We also use a linear chain architecture corresponding to a 1D free energy profile as our kinetic network. Subsequently, we demonstrate the effectiveness of our method through experiments on a data set derived from molecular dynamics. We compare the performance of our method to other partitioning techniques such as PCCA+. We explore the importance of feature and hyperparameter selection and propose a general strategy for large-scale parallel training of GNNs for discovering optimal graph partitionings.         ",
    "url": "https://arxiv.org/abs/2312.14847",
    "authors": [
      "Sam Alexander Martino",
      "Jo\u00e3o Morado",
      "Chenghao Li",
      "Zhenghao Lu",
      "Edina Rosta"
    ],
    "subjectives": [
      "Biological Physics (physics.bio-ph)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2404.12415",
    "title": "Prediction of soil fertility parameters using USB-microscope imagery and portable X-ray fluorescence spectrometry",
    "abstract": "           This study investigated the use of portable X-ray fluorescence (PXRF) spectrometry and soil image analysis for rapid soil fertility assessment, with a focus on key indicators such as available boron (B), organic carbon (OC), available manganese (Mn), available sulfur (S), and the sulfur availability index (SAI). A total of 1,133 soil samples from diverse agro-climatic zones in Eastern India were analyzed. The research integrated color and texture features from microscopic soil images, PXRF data, and auxiliary soil variables (AVs) using a Random Forest model. Results showed that combining image features (IFs) with AVs significantly improved prediction accuracy for available B (R2 = 0.80) and OC (R2 = 0.88). A data fusion approach, incorporating IFs, AVs, and PXRF data, further enhanced predictions for available Mn and SAI, with R2 values of 0.72 and 0.70, respectively. The study highlights the potential of integrating these technologies to offer rapid, cost-effective soil testing methods, paving the way for more advanced predictive models and a deeper understanding of soil fertility. Future work should explore the application of deep learning models on a larger dataset, incorporating soils from a wider range of agro-climatic zones under field conditions.         ",
    "url": "https://arxiv.org/abs/2404.12415",
    "authors": [
      "Shubhadip Dasgupta",
      "Satwik Pate",
      "Divya Rathore",
      "L.G. Divyanth",
      "Ayan Das",
      "Anshuman Nayak",
      "Subhadip Dey",
      "Asim Biswas",
      "David C. Weindorf",
      "Bin Li",
      "Sergio Henrique Godinho Silva",
      "Bruno Teixeira Ribeiro",
      "Sanjay Srivastava",
      "Somsubhra Chakraborty"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.09676",
    "title": "Optimizing Byte-level Representation for End-to-end ASR",
    "abstract": "           We propose a novel approach to optimizing a byte-level representation for end-to-end automatic speech recognition (ASR). Byte-level representation is often used by large scale multilingual ASR systems when the character set of the supported languages is large. The compactness and universality of byte-level representation allow the ASR models to use smaller output vocabularies and therefore, provide more flexibility. UTF-8 is a commonly used byte-level representation for multilingual ASR, but it is not designed to optimize machine learning tasks directly. By using auto-encoder and vector quantization, we show that we can optimize a byte-level representation for ASR and achieve better accuracy. Our proposed framework can incorporate information from different modalities, and provides an error correction mechanism. In an English/Mandarin dictation task, we show that a bilingual ASR model built with this approach can outperform UTF-8 representation by 5% relative in error rate.         ",
    "url": "https://arxiv.org/abs/2406.09676",
    "authors": [
      "Roger Hsiao",
      "Liuhui Deng",
      "Erik McDermott",
      "Ruchir Travadi",
      "Xiaodan Zhuang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.12951",
    "title": "On z-coloring and ${\\rm b}^{\\ast}$-coloring of graphs as improved variants of the b-coloring",
    "abstract": "           Let $G$ be a simple graph and $c$ a proper vertex coloring of $G$. A vertex $u$ is called b-vertex in $(G,c)$ if all colors except $c(u)$ appear in the neighborhood of $u$. By a ${\\rm b}^{\\ast}$-coloring of $G$ using colors $\\{1, \\ldots, k\\}$ we define a proper vertex coloring $c$ such that there is a b-vertex $u$ (called nice vertex) such that for each $j\\in \\{1, \\ldots, k\\}$ with $j\\not=c(u)$, $u$ is adjacent to a b-vertex of color $j$. The ${\\rm b}^{\\ast}$-chromatic number of $G$ (denoted by ${\\rm b}^{\\ast}(G)$) is the largest integer $k$ such that $G$ has a ${\\rm b}^{\\ast}$-coloring using $k$ colors. Every graph $G$ admits a ${\\rm b}^{\\ast}$-coloring which is an improvement over the famous b-coloring. A z-coloring of $G$ is a coloring $c$ using colors $\\{1, 2, \\ldots, k\\}$ containing a nice vertex of color $k$ such that for each two colors $i<j$, each vertex of color $j$ has a neighbor of color $i$ in the graph (i.e. $c$ is obtained from a greedy coloring of $G$). We prove that ${\\rm b}^{\\ast}(G)$ cannot be approximated within any constant factor unless $P=NP$. We obtain results for ${\\rm b}^{\\ast}$-coloring and z-coloring of block graphs, cacti, $P_4$-sparse graphs and graphs with girth greater than $4$. We prove that z-coloring and ${\\rm b}^{\\ast}$-coloring have a locality property. A linear 0-1 programming model is also presented for z-coloring of graphs. The positive results suggest that researches can be focused on ${\\rm b}^{\\ast}$-coloring (or z-coloring) instead of b-coloring of graphs.         ",
    "url": "https://arxiv.org/abs/2408.12951",
    "authors": [
      "Manouchehr Zaker"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2408.15391",
    "title": "Examining the Interplay Between Privacy and Fairness for Speech Processing: A Review and Perspective",
    "abstract": "           Speech technology has been increasingly deployed in various areas of daily life including sensitive domains such as healthcare and law enforcement. For these technologies to be effective, they must work reliably for all users while preserving individual privacy. Although tradeoffs between privacy and utility, as well as fairness and utility, have been extensively researched, the specific interplay between privacy and fairness in speech processing remains underexplored. This review and position paper offers an overview of emerging privacy-fairness tradeoffs throughout the entire machine learning lifecycle for speech processing. By drawing on well-established frameworks on fairness and privacy, we examine existing biases and sources of privacy harm that coexist during the development of speech processing models. We then highlight how corresponding privacy-enhancing technologies have the potential to inadvertently increase these biases and how bias mitigation strategies may conversely reduce privacy. By raising open questions, we advocate for a comprehensive evaluation of privacy-fairness tradeoffs for speech technology and the development of privacy-enhancing and fairness-aware algorithms in this domain.         ",
    "url": "https://arxiv.org/abs/2408.15391",
    "authors": [
      "Anna Leschanowsky",
      "Sneha Das"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2409.01444",
    "title": "A causal viewpoint on prediction model performance under changes in case-mix: discrimination and calibration respond differently for prognosis and diagnosis predictions",
    "abstract": "           Prediction models inform important clinical decisions, aiding in diagnosis, prognosis, and treatment planning. The predictive performance of these models is typically assessed through discrimination and calibration. However, changes in the distribution of the data impact model performance. In health-care, a typical change is a shift in case-mix: for example, for cardiovascular risk management, a general practitioner sees a different mix of patients than a specialist in a tertiary hospital. This work introduces a novel framework that differentiates the effects of case-mix shifts on discrimination and calibration based on the causal direction of the prediction task. When prediction is in the causal direction (often the case for prognosis predictions), calibration remains stable under case-mix shifts, while discrimination does not. Conversely, when predicting in the anti-causal direction (often with diagnosis predictions), discrimination remains stable, but calibration does not. A simulation study and empirical validation using cardiovascular disease prediction models demonstrate the implications of this framework. This framework provides critical insights for evaluating and deploying prediction models across different clinical settings, emphasizing the importance of understanding the causal structure of the prediction task.         ",
    "url": "https://arxiv.org/abs/2409.01444",
    "authors": [
      "Wouter A.C. van Amsterdam"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.01733",
    "title": "Improving the Crossing Lemma by Characterizing Dense 2-Planar and 3-Planar Graphs",
    "abstract": "           The classical Crossing Lemma by Ajtai et al.~and Leighton from 1982 gave an important lower bound of $c \\frac{m^3}{n^2}$ for the number of crossings in any drawing of a given graph of $n$ vertices and $m$ edges. The original value was $c= 1/100$, which then has gradually been improved. Here, the bounds for the density of $k$-planar graphs played a central role. Our new insight is that for $k=2,3$ the $k$-planar graphs have substantially fewer edges if specific local configurations that occur in drawings of $k$-planar graphs of maximum density are forbidden. Therefore, we are able to derive better bounds for the crossing number $\\text{cr}(G)$ of a given graph $G$. In particular, we achieve a bound of $\\text{cr}(G) \\ge \\frac{37}{9}m-\\frac{155}{9}(n-2)$ for the range of $5n < m \\le 6n$, while our second bound $\\text{cr}(G) \\ge 5m - \\frac{203}{9}(n-2)$ is even stronger for larger $m>6n$. For $m > 6.77n$, we finally apply the standard probabilistic proof from the BOOK and obtain an improved constant of $c>1/27.48$ in the Crossing Lemma. Note that the previous constant was $1/29$. Although this improvement is not too impressive, we consider our technique as an important new tool, which might be helpful in various other applications.         ",
    "url": "https://arxiv.org/abs/2409.01733",
    "authors": [
      "Aaron B\u00fcngener",
      "Michael Kaufmann"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2409.02430",
    "title": "Transfer-based Adversarial Poisoning Attacks for Online (MIMO-)Deep Receviers",
    "abstract": "           Recently, the design of wireless receivers using deep neural networks (DNNs), known as deep receivers, has attracted extensive attention for ensuring reliable communication in complex channel environments. To adapt quickly to dynamic channels, online learning has been adopted to update the weights of deep receivers with over-the-air data (e.g., pilots). However, the fragility of neural models and the openness of wireless channels expose these systems to malicious attacks. To this end, understanding these attack methods is essential for robust receiver design. In this paper, we propose a transfer-based adversarial poisoning attack method for online receivers.Without knowledge of the attack target, adversarial perturbations are injected to the pilots, poisoning the online deep receiver and impairing its ability to adapt to dynamic channels and nonlinear effects. In particular, our attack method targets Deep Soft Interference Cancellation (DeepSIC)[1] using online meta-learning. As a classical model-driven deep receiver, DeepSIC incorporates wireless domain knowledge into its architecture. This integration allows it to adapt efficiently to time-varying channels with only a small number of pilots, achieving optimal performance in a multi-input and multi-output (MIMO) scenario.The deep receiver in this scenario has a number of applications in the field of wireless communication, which motivates our study of the attack methods targeting it.Specifically, we demonstrate the effectiveness of our attack in simulations on synthetic linear, synthetic nonlinear, static, and COST 2100 channels. Simulation results indicate that the proposed poisoning attack significantly reduces the performance of online receivers in rapidly changing scenarios.         ",
    "url": "https://arxiv.org/abs/2409.02430",
    "authors": [
      "Kunze Wu",
      "Weiheng Jiang",
      "Dusit Niyato",
      "Yinghuan Li",
      "Chuang Luo"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.02457",
    "title": "On Oriented Diameter of Power Graphs",
    "abstract": "           In this paper, we study the oriented diameter of power graphs of groups. We show that a $2$-edge connected power graph of a finite group has oriented diameter at most $4$. We prove that the power graph of a cyclic group of order $n$ has oriented diameter $2$ for all $n\\neq 2,4,6$. Until our work, to the best of our knowledge, no infinite family of graphs with oriented diameter 2 had been identified except for subclasses of complete graphs. Finally, we give a complete characterization of the oriented diameter of the power graphs of nilpotent groups. This, in turn, gives an algorithm for computing the oriented diameter of the power graph of a given nilpotent group that runs in time polynomial in the size of the group.         ",
    "url": "https://arxiv.org/abs/2409.02457",
    "authors": [
      "Deepu Benson",
      "Bireswar Das",
      "Dipan Dey",
      "Jinia Ghosh"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  }
]