[
  {
    "id": "arXiv:2409.07466",
    "title": "An Artificial Neural Network for Image Classification Inspired by Aversive Olfactory Learning Circuits in Caenorhabditis Elegans",
    "abstract": "           This study introduces an artificial neural network (ANN) for image classification task, inspired by the aversive olfactory learning circuits of the nematode Caenorhabditis elegans (C. elegans). Despite the remarkable performance of ANNs in a variety of tasks, they face challenges such as excessive parameterization, high training costs and limited generalization capabilities. C. elegans, with its simple nervous system comprising only 302 neurons, serves as a paradigm in neurobiological research and is capable of complex behaviors including learning. This research identifies key neural circuits associated with aversive olfactory learning in C. elegans through behavioral experiments and high-throughput gene sequencing, translating them into an image classification ANN architecture. Additionally, two other image classification ANNs with distinct architectures were constructed for comparative performance analysis to highlight the advantages of bio-inspired design. The results indicate that the ANN inspired by the aversive olfactory learning circuits of C. elegans achieves higher accuracy, better consistency and faster convergence rates in image classification task, especially when tackling more complex classification challenges. This study not only showcases the potential of bio-inspired design in enhancing ANN capabilities but also provides a novel perspective and methodology for future ANN design.         ",
    "url": "https://arxiv.org/abs/2409.07466",
    "authors": [
      "Xuebin Wang",
      "Chunxiuzi Liu",
      "Meng Zhao",
      "Ke Zhang",
      "Zengru Di",
      "He Liu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2409.07469",
    "title": "Small Object Detection for Indoor Assistance to the Blind using YOLO NAS Small and Super Gradients",
    "abstract": "           Advancements in object detection algorithms have opened new avenues for assistive technologies that cater to the needs of visually impaired individuals. This paper presents a novel approach for indoor assistance to the blind by addressing the challenge of small object detection. We propose a technique YOLO NAS Small architecture, a lightweight and efficient object detection model, optimized using the Super Gradients training framework. This combination enables real-time detection of small objects crucial for assisting the blind in navigating indoor environments, such as furniture, appliances, and household items. Proposed method emphasizes low latency and high accuracy, enabling timely and informative voice-based guidance to enhance the user's spatial awareness and interaction with their surroundings. The paper details the implementation, experimental results, and discusses the system's effectiveness in providing a practical solution for indoor assistance to the visually impaired.         ",
    "url": "https://arxiv.org/abs/2409.07469",
    "authors": [
      "Rashmi BN",
      "R. Guru",
      "Anusuya M A"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.07475",
    "title": "Cross-Cultural Communication in the Digital Age: An Analysis of Cultural Representation and Inclusivity in Emojis",
    "abstract": "           Emojis have become a universal language in the digital world, enabling users to express emotions, ideas, and identities across diverse cultural contexts. As emojis incorporate more cultural symbols and diverse representations, they play a crucial role in cross-cultural communication. This research project aims to analyze the representation of different cultures in emojis, investigate how emojis facilitate cross-cultural communication and promote inclusivity, and explore the impact of emojis on understanding and interpretation in different cultural contexts.         ",
    "url": "https://arxiv.org/abs/2409.07475",
    "authors": [
      "Lingfeng Li",
      "Xiangwen Zheng"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2409.07494",
    "title": "Ethereum Fraud Detection via Joint Transaction Language Model and Graph Representation Learning",
    "abstract": "           Ethereum faces growing fraud threats. Current fraud detection methods, whether employing graph neural networks or sequence models, fail to consider the semantic information and similarity patterns within transactions. Moreover, these approaches do not leverage the potential synergistic benefits of combining both types of models. To address these challenges, we propose TLMG4Eth that combines a transaction language model with graph-based methods to capture semantic, similarity, and structural features of transaction data in Ethereum. We first propose a transaction language model that converts numerical transaction data into meaningful transaction sentences, enabling the model to learn explicit transaction semantics. Then, we propose a transaction attribute similarity graph to learn transaction similarity information, enabling us to capture intuitive insights into transaction anomalies. Additionally, we construct an account interaction graph to capture the structural information of the account transaction network. We employ a deep multi-head attention network to fuse transaction semantic and similarity embeddings, and ultimately propose a joint training approach for the multi-head attention network and the account interaction graph to obtain the synergistic benefits of both.         ",
    "url": "https://arxiv.org/abs/2409.07494",
    "authors": [
      "Yifan Jia",
      "Yanbin Wang",
      "Jianguo Sun",
      "Yiwei Liu",
      "Zhang Sheng",
      "Ye Tian"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07500",
    "title": "DV-FSR: A Dual-View Target Attack Framework for Federated Sequential Recommendation",
    "abstract": "           Federated recommendation (FedRec) preserves user privacy by enabling decentralized training of personalized models, but this architecture is inherently vulnerable to adversarial attacks. Significant research has been conducted on targeted attacks in FedRec systems, motivated by commercial and social influence considerations. However, much of this work has largely overlooked the differential robustness of recommendation models. Moreover, our empirical findings indicate that existing targeted attack methods achieve only limited effectiveness in Federated Sequential Recommendation (FSR) tasks. Driven by these observations, we focus on investigating targeted attacks in FSR and propose a novel dualview attack framework, named DV-FSR. This attack method uniquely combines a sampling-based explicit strategy with a contrastive learning-based implicit gradient strategy to orchestrate a coordinated attack. Additionally, we introduce a specific defense mechanism tailored for targeted attacks in FSR, aiming to evaluate the mitigation effects of the attack method we proposed. Extensive experiments validate the effectiveness of our proposed approach on representative sequential models.         ",
    "url": "https://arxiv.org/abs/2409.07500",
    "authors": [
      "Qitao Qin",
      "Yucong Luo",
      "Mingyue Cheng",
      "Qingyang Mao",
      "Chenyi Lei"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2409.07503",
    "title": "AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs",
    "abstract": "           Jailbreak vulnerabilities in Large Language Models (LLMs) refer to methods that extract malicious content from the model by carefully crafting prompts or suffixes, which has garnered significant attention from the research community. However, traditional attack methods, which primarily focus on the semantic level, are easily detected by the model. These methods overlook the difference in the model's alignment protection capabilities at different output stages. To address this issue, we propose an adaptive position pre-fill jailbreak attack approach for executing jailbreak attacks on LLMs. Our method leverages the model's instruction-following capabilities to first output pre-filled safe content, then exploits its narrative-shifting abilities to generate harmful content. Extensive black-box experiments demonstrate our method can improve the attack success rate by 47% on the widely recognized secure model (Llama2) compared to existing approaches. Our code can be found at: this https URL.         ",
    "url": "https://arxiv.org/abs/2409.07503",
    "authors": [
      "Lijia Lv",
      "Weigang Zhang",
      "Xuehai Tang",
      "Jie Wen",
      "Feng Liu",
      "Jizhong Han",
      "Songlin Hu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.07505",
    "title": "A Survey of Anomaly Detection in In-Vehicle Networks",
    "abstract": "           Modern vehicles are equipped with Electronic Control Units (ECU) that are used for controlling important vehicle functions including safety-critical operations. ECUs exchange information via in-vehicle communication buses, of which the Controller Area Network (CAN bus) is by far the most widespread representative. Problems that may occur in the vehicle's physical parts or malicious attacks may cause anomalies in the CAN traffic, impairing the correct vehicle operation. Therefore, the detection of such anomalies is vital for vehicle safety. This paper reviews the research on anomaly detection for in-vehicle networks, more specifically for the CAN bus. Our main focus is the evaluation of methods used for CAN bus anomaly detection together with the datasets used in such analysis. To provide the reader with a more comprehensive understanding of the subject, we first give a brief review of related studies on time series-based anomaly detection. Then, we conduct an extensive survey of recent deep learning-based techniques as well as conventional techniques for CAN bus anomaly detection. Our comprehensive analysis delves into anomaly detection algorithms employed in in-vehicle networks, specifically focusing on their learning paradigms, inherent strengths, and weaknesses, as well as their efficacy when applied to CAN bus datasets. Lastly, we highlight challenges and open research problems in CAN bus anomaly detection.         ",
    "url": "https://arxiv.org/abs/2409.07505",
    "authors": [
      "\u00d6vg\u00fc \u00d6zdemir",
      "M. Tu\u011fberk \u0130\u015fyapar",
      "P\u0131nar Karag\u00f6z",
      "Klaus Werner Schmidt",
      "Demet Demir",
      "N. Alpay Karag\u00f6z"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2409.07507",
    "title": "Traceable LLM-based validation of statements in knowledge graphs",
    "abstract": "           This article presents a method for verifying RDF triples using LLMs, with an emphasis on providing traceable arguments. Because the LLMs cannot currently reliably identify the origin of the information used to construct the response to the user query, our approach is to avoid using internal LLM factual knowledge altogether. Instead, verified RDF statements are compared to chunks of external documents retrieved through a web search or Wikipedia. To assess the possible application of this workflow on biosciences content, we evaluated 1,719 positive statements from the BioRED dataset and the same number of newly generated negative statements. The resulting precision is 88%, and recall is 44%. This indicates that the method requires human oversight. We demonstrate the method on Wikidata, where a SPARQL query is used to automatically retrieve statements needing verification. Overall, the results suggest that LLMs could be used for large-scale verification of statements in KGs, a task previously unfeasible due to human annotation costs.         ",
    "url": "https://arxiv.org/abs/2409.07507",
    "authors": [
      "Daniel Adam",
      "Tom\u00e1\u0161 Kliegr"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07541",
    "title": "ENACT: Entropy-based Clustering of Attention Input for Improving the Computational Performance of Object Detection Transformers",
    "abstract": "           Transformers demonstrate competitive performance in terms of precision on the problem of vision-based object detection. However, they require considerable computational resources due to the quadratic size of the attention weights. In this work, we propose to cluster the transformer input on the basis of its entropy. The reason for this is that the self-information of each pixel (whose sum is the entropy), is likely to be similar among pixels corresponding to the same objects. Clustering reduces the size of data given as input to the transformer and therefore reduces training time and GPU memory usage, while at the same time preserves meaningful information to be passed through the remaining parts of the network. The proposed process is organized in a module called ENACT, that can be plugged-in any transformer architecture that consists of a multi-head self-attention computation in its encoder. We ran extensive experiments using the COCO object detection dataset, and three detection transformers. The obtained results demonstrate that in all tested cases, there is consistent reduction in the required computational resources, while the precision of the detection task is only slightly reduced. The code of the ENACT module will become available at this https URL ",
    "url": "https://arxiv.org/abs/2409.07541",
    "authors": [
      "Giorgos Savathrakis",
      "Antonis Argyros"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.07551",
    "title": "Machine Learning Approaches for Defect Detection in a Microwell-based Medical Device",
    "abstract": "           Microfluidic devices offer numerous advantages in medical applications, including the capture of single cells in microwell-based platforms for genomic analysis. As the cost of sequencing decreases, the demand for high-throughput single-cell analysis devices increases, leading to more microwells in a single device. However, their small size and large quantity increase the quality control (QC) effort. Currently, QC steps are still performed manually in some devices, requiring intensive training and time and causing inconsistency between different operators. A way to overcome this issue is to through automated defect detection. Computer vision can quickly analyze a large number of images in a short time and can be applied in defect detection. Automated defect detection can replace manual inspection, potentially decreasing variations in QC results. We report a machine learning (ML) algorithm that applies a convolution neural network (CNN) model with 9 layers and 64 units, incorporating dropouts and regularizations. This algorithm can analyze a large number of microwells produced by injection molding, significantly increasing the number of images analyzed compared to manual operator, improving QC, and ensuring the delivery of high-quality products to customers.         ",
    "url": "https://arxiv.org/abs/2409.07551",
    "authors": [
      "Xueying Zhao",
      "Yan Chen",
      "Yuefu Jiang",
      "Amie Radenbaugh",
      "Jamie Moskwa",
      "Devon Jensen"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2409.07577",
    "title": "Self-Masking Networks for Unsupervised Adaptation",
    "abstract": "           With the advent of billion-parameter foundation models, efficient fine-tuning has become increasingly important for the adaptation of models to downstream tasks. However, especially in computer vision, it can be hard to achieve good performance when access to quality labeled data is lacking. In this work, we propose a method adapting pretrained generalist models in a self-supervised manner by learning binary masks. These self-supervised masking networks (SMNs) are up to 79x more efficient to store and significantly improve performance on label-efficient downstream tasks. We validate the usefulness of learning binary masks as a fine-tuning method on 8 datasets and 3 model architectures, and we demonstrate the effectiveness of SMNs in 3 label-efficient settings.         ",
    "url": "https://arxiv.org/abs/2409.07577",
    "authors": [
      "Alfonso Taboada Warmerdam",
      "Mathilde Caron",
      "Yuki M. Asano"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07581",
    "title": "Violence detection in videos using deep recurrent and convolutional neural networks",
    "abstract": "           Violence and abnormal behavior detection research have known an increase of interest in recent years, due mainly to a rise in crimes in large cities worldwide. In this work, we propose a deep learning architecture for violence detection which combines both recurrent neural networks (RNNs) and 2-dimensional convolutional neural networks (2D CNN). In addition to video frames, we use optical flow computed using the captured sequences. CNN extracts spatial characteristics in each frame, while RNN extracts temporal characteristics. The use of optical flow allows to encode the movements in the scenes. The proposed approaches reach the same level as the state-of-the-art techniques and sometime surpass them. It was validated on 3 databases achieving good results.         ",
    "url": "https://arxiv.org/abs/2409.07581",
    "authors": [
      "Abdarahmane Traor\u00e9",
      "Moulay A. Akhloufi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.07582",
    "title": "Minimizing Embedding Distortion for Robust Out-of-Distribution Performance",
    "abstract": "           Foundational models, trained on vast and diverse datasets, have demonstrated remarkable capabilities in generalizing across different domains and distributions for various zero-shot tasks. Our work addresses the challenge of retaining these powerful generalization capabilities when adapting foundational models to specific downstream tasks through fine-tuning. To this end, we introduce a novel approach we call \"similarity loss\", which can be incorporated into the fine-tuning process of any task. By minimizing the distortion of fine-tuned embeddings from the pre-trained embeddings, our method strikes a balance between task-specific adaptation and preserving broad generalization abilities. We evaluate our approach on two diverse tasks: image classification on satellite imagery and face recognition, focusing on open-class and domain shift scenarios to assess out-of-distribution (OOD) performance. We demonstrate that this approach significantly improves OOD performance while maintaining strong in-distribution (ID) performance.         ",
    "url": "https://arxiv.org/abs/2409.07582",
    "authors": [
      "Tom Shaked",
      "Yuval Goldman",
      "Oran Shayer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.07585",
    "title": "Efficient Localized Adaptation of Neural Weather Forecasting: A Case Study in the MENA Region",
    "abstract": "           Accurate weather and climate modeling is critical for both scientific advancement and safeguarding communities against environmental risks. Traditional approaches rely heavily on Numerical Weather Prediction (NWP) models, which simulate energy and matter flow across Earth's systems. However, heavy computational requirements and low efficiency restrict the suitability of NWP, leading to a pressing need for enhanced modeling techniques. Neural network-based models have emerged as promising alternatives, leveraging data-driven approaches to forecast atmospheric variables. In this work, we focus on limited-area modeling and train our model specifically for localized region-level downstream tasks. As a case study, we consider the MENA region due to its unique climatic challenges, where accurate localized weather forecasting is crucial for managing water resources, agriculture and mitigating the impacts of extreme weather events. This targeted approach allows us to tailor the model's capabilities to the unique conditions of the region of interest. Our study aims to validate the effectiveness of integrating parameter-efficient fine-tuning (PEFT) methodologies, specifically Low-Rank Adaptation (LoRA) and its variants, to enhance forecast accuracy, as well as training speed, computational resource utilization, and memory efficiency in weather and climate modeling for specific regions.         ",
    "url": "https://arxiv.org/abs/2409.07585",
    "authors": [
      "Muhammad Akhtar Munir",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2409.07586",
    "title": "Analyzing the Impact of Copying-and-Pasting Vulnerable Solidity Code Snippets from Question-and-Answer Websites",
    "abstract": "           Ethereum smart contracts are executable programs deployed on a blockchain. Once deployed, they cannot be updated due to their inherent immutability. Moreover, they often manage valuable assets that are worth millions of dollars, making them attractive targets for attackers. The introduction of vulnerabilities in programs due to the reuse of vulnerable code posted on Q&A websites such as Stack Overflow is not a new issue. However, little effort has been made to analyze the extent of this issue on deployed smart contracts. In this paper, we conduct a study on the impact of vulnerable code reuse from Q&A websites during the development of smart contracts and provide tools uniquely fit to detect vulnerable code patterns in complete and incomplete Smart Contract code. This paper proposes a pattern-based vulnerability detection tool that is able to analyze code snippets (i.e., incomplete code) as well as full smart contracts based on the concept of code property graphs. We also propose a methodology that leverages fuzzy hashing to quickly detect code clones of vulnerable snippets among deployed smart contracts. Our results show that our vulnerability search, as well as our code clone detection, are comparable to state-of-the-art while being applicable to code snippets. Our large-scale study on 18,660 code snippets reveals that 4,596 of them are vulnerable, out of which 616 can be found in 7,852 deployed smart contracts. These results highlight that the reuse of vulnerable code snippets is indeed an issue in currently deployed smart contracts.         ",
    "url": "https://arxiv.org/abs/2409.07586",
    "authors": [
      "Konrad Weiss",
      "Christof Ferreira Torres",
      "Florian Wendland"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.07588",
    "title": "2D bidirectional gated recurrent unit convolutional Neural networks for end-to-end violence detection In videos",
    "abstract": "           Abnormal behavior detection, action recognition, fight and violence detection in videos is an area that has attracted a lot of interest in recent years. In this work, we propose an architecture that combines a Bidirectional Gated Recurrent Unit (BiGRU) and a 2D Convolutional Neural Network (CNN) to detect violence in video sequences. A CNN is used to extract spatial characteristics from each frame, while the BiGRU extracts temporal and local motion characteristics using CNN extracted features from multiple frames. The proposed end-to-end deep learning network is tested in three public datasets with varying scene complexities. The proposed network achieves accuracies up to 98%. The obtained results are promising and show the performance of the proposed end-to-end approach.         ",
    "url": "https://arxiv.org/abs/2409.07588",
    "authors": [
      "Abdarahmane Traor\u00e9",
      "Moulay A. Akhloufi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.07589",
    "title": "Multi-scale spatiotemporal representation learning for EEG-based emotion recognition",
    "abstract": "           EEG-based emotion recognition holds significant potential in the field of brain-computer interfaces. A key challenge lies in extracting discriminative spatiotemporal features from electroencephalogram (EEG) signals. Existing studies often rely on domain-specific time-frequency features and analyze temporal dependencies and spatial characteristics separately, neglecting the interaction between local-global relationships and spatiotemporal dynamics. To address this, we propose a novel network called Multi-Scale Inverted Mamba (MS-iMamba), which consists of Multi-Scale Temporal Blocks (MSTB) and Temporal-Spatial Fusion Blocks (TSFB). Specifically, MSTBs are designed to capture both local details and global temporal dependencies across different scale subsequences. The TSFBs, implemented with an inverted Mamba structure, focus on the interaction between dynamic temporal dependencies and spatial characteristics. The primary advantage of MS-iMamba lies in its ability to leverage reconstructed multi-scale EEG sequences, exploiting the interaction between temporal and spatial features without the need for domain-specific time-frequency feature extraction. Experimental results on the DEAP, DREAMER, and SEED datasets demonstrate that MS-iMamba achieves classification accuracies of 94.86%, 94.94%, and 91.36%, respectively, using only four-channel EEG signals, outperforming state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2409.07589",
    "authors": [
      "Xin Zhou",
      "Xiaojing Peng"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2409.07609",
    "title": "A Cost-Aware Approach to Adversarial Robustness in Neural Networks",
    "abstract": "           Considering the growing prominence of production-level AI and the threat of adversarial attacks that can evade a model at run-time, evaluating the robustness of models to these evasion attacks is of critical importance. Additionally, testing model changes likely means deploying the models to (e.g. a car or a medical imaging device), or a drone to see how it affects performance, making un-tested changes a public problem that reduces development speed, increases cost of development, and makes it difficult (if not impossible) to parse cause from effect. In this work, we used survival analysis as a cloud-native, time-efficient and precise method for predicting model performance in the presence of adversarial noise. For neural networks in particular, the relationships between the learning rate, batch size, training time, convergence time, and deployment cost are highly complex, so researchers generally rely on benchmark datasets to assess the ability of a model to generalize beyond the training data. To address this, we propose using accelerated failure time models to measure the effect of hardware choice, batch size, number of epochs, and test-set accuracy by using adversarial attacks to induce failures on a reference model architecture before deploying the model to the real world. We evaluate several GPU types and use the Tree Parzen Estimator to maximize model robustness and minimize model run-time simultaneously. This provides a way to evaluate the model and optimise it in a single step, while simultaneously allowing us to model the effect of model parameters on training time, prediction time, and accuracy. Using this technique, we demonstrate that newer, more-powerful hardware does decrease the training time, but with a monetary and power cost that far outpaces the marginal gains in accuracy.         ",
    "url": "https://arxiv.org/abs/2409.07609",
    "authors": [
      "Charles Meyers",
      "Mohammad Reza Saleh Sedghpour",
      "Tommy L\u00f6fstedt",
      "Erik Elmroth"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2409.07611",
    "title": "Detection and Classification of Twitter Users' Opinions on Drought Crises in Iran Using Machine Learning Techniques",
    "abstract": "           The main objective of this research is to identify and classify the opinions of Persian-speaking Twitter users related to drought crises in Iran and subsequently develop a model for detecting these opinions on the platform. To achieve this, a model has been developed using machine learning and text mining methods to detect the opinions of Persian-speaking Twitter users regarding the drought issues in Iran. The statistical population for the research included 42,028 drought-related tweets posted over a one-year period. These tweets were extracted from Twitter using keywords related to the drought crises in Iran. Subsequently, a sample of 2,300 tweets was qualitatively analyzed, labeled, categorized, and examined. Next, a four-category classification of users` opinions regarding drought crises and Iranians' resilience to these crises was identified. Based on these four categories, a machine learning model based on logistic regression was trained to predict and detect various opinions in Twitter posts. The developed model exhibits an accuracy of 66.09% and an F-score of 60%, indicating that this model has good performance for detecting Iranian Twitter users' opinions regarding drought crises. The ability to detect opinions regarding drought crises on platforms like Twitter using machine learning methods can intelligently represent the resilience level of the Iranian society in the face of these crises, and inform policymakers in this area about changes in public opinion.         ",
    "url": "https://arxiv.org/abs/2409.07611",
    "authors": [
      "Somayeh Labafi",
      "Leila Rabiei",
      "Zeinab Rajabi"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2409.07615",
    "title": "Zero-Shot Machine-Generated Text Detection Using Mixture of Large Language Models",
    "abstract": "           The dissemination of Large Language Models (LLMs), trained at scale, and endowed with powerful text-generating abilities has vastly increased the threats posed by generative AI technologies by reducing the cost of producing harmful, toxic, faked or forged content. In response, various proposals have been made to automatically discriminate artificially generated from human-written texts, typically framing the problem as a classification problem. Most approaches evaluate an input document by a well-chosen detector LLM, assuming that low-perplexity scores reliably signal machine-made content. As using one single detector can induce brittleness of performance, we instead consider several and derive a new, theoretically grounded approach to combine their respective strengths. Our experiments, using a variety of generator LLMs, suggest that our method effectively increases the robustness of detection.         ",
    "url": "https://arxiv.org/abs/2409.07615",
    "authors": [
      "Matthieu Dubois",
      "Fran\u00e7ois Yvon",
      "Pablo Piantanida"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.07616",
    "title": "Public-key encryption from a trapdoor one-way embedding of $SL_2(\\mathbb{N}$)",
    "abstract": "           We obfuscate words of a given length in a free monoid on two generators with a simple factorization algorithm (namely $SL_2(\\mathbb{N})$) to create a public-key encryption scheme. We provide a reference implementation in Python and suggested parameters. The security analysis is between weak and non-existent, left to future work.         ",
    "url": "https://arxiv.org/abs/2409.07616",
    "authors": [
      "Robert Hines"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Number Theory (math.NT)"
    ]
  },
  {
    "id": "arXiv:2409.07669",
    "title": "A Mixed-Methods Study of Open-Source Software Maintainers On Vulnerability Management and Platform Security Features",
    "abstract": "           In open-source software (OSS), software vulnerabilities have significantly increased. Although researchers have investigated the perspectives of vulnerability reporters and OSS contributor security practices, understanding the perspectives of OSS maintainers on vulnerability management and platform security features is currently understudied. In this paper, we investigate the perspectives of OSS maintainers who maintain projects listed in the GitHub Advisory Database. We explore this area by conducting two studies: identifying aspects through a listing survey ($n_1=80$) and gathering insights from semi-structured interviews ($n_2=22$). Of the 37 identified aspects, we find that supply chain mistrust and lack of automation for vulnerability management are the most challenging, and barriers to adopting platform security features include a lack of awareness and the perception that they are not necessary. Surprisingly, we find that despite being previously vulnerable, some maintainers still allow public vulnerability reporting, or ignore reports altogether. Based on our findings, we discuss implications for OSS platforms and how the research community can better support OSS vulnerability management efforts.         ",
    "url": "https://arxiv.org/abs/2409.07669",
    "authors": [
      "Jessy Ayala",
      "Yu-Jye Tung",
      "Joshua Garcia"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.07671",
    "title": "Transformed Physics-Informed Neural Networks for The Convection-Diffusion Equation",
    "abstract": "           Singularly perturbed problems are known to have solutions with steep boundary layers that are hard to resolve numerically. Traditional numerical methods, such as Finite Difference Methods (FDMs), require a refined mesh to obtain stable and accurate solutions. As Physics-Informed Neural Networks (PINNs) have been shown to successfully approximate solutions to differential equations from various fields, it is natural to examine their performance on singularly perturbed problems. The convection-diffusion equation is a representative example of such a class of problems, and we consider the use of PINNs to produce numerical solutions of this equation. We study two ways to use PINNS: as a method for correcting oscillatory discrete solutions obtained using FDMs, and as a method for modifying reduced solutions of unperturbed problems. For both methods, we also examine the use of input transformation to enhance accuracy, and we explain the behavior of input transformations analytically, with the help of neural tangent kernels.         ",
    "url": "https://arxiv.org/abs/2409.07671",
    "authors": [
      "Jiajing Guan",
      "Howard Elman"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07684",
    "title": "Modeling Information Narrative Detection and Evolution on Telegram during the Russia-Ukraine War",
    "abstract": "           Following the Russian Federation's full-scale invasion of Ukraine in February 2022, a multitude of information narratives emerged within both pro-Russian and pro-Ukrainian communities online. As the conflict progresses, so too do the information narratives, constantly adapting and influencing local and global community perceptions and attitudes. This dynamic nature of the evolving information environment (IE) underscores a critical need to fully discern how narratives evolve and affect online communities. Existing research, however, often fails to capture information narrative evolution, overlooking both the fluid nature of narratives and the internal mechanisms that drive their evolution. Recognizing this, we introduce a novel approach designed to both model narrative evolution and uncover the underlying mechanisms driving them. In this work we perform a comparative discourse analysis across communities on Telegram covering the initial three months following the invasion. First, we uncover substantial disparities in narratives and perceptions between pro-Russian and pro-Ukrainian communities. Then, we probe deeper into prevalent narratives of each group, identifying key themes and examining the underlying mechanisms fueling their evolution. Finally, we explore influences and factors that may shape the development and spread of narratives.         ",
    "url": "https://arxiv.org/abs/2409.07684",
    "authors": [
      "Patrick Gerard",
      "Svitlana Volkova",
      "Louis Penafiel",
      "Kristina Lerman",
      "Tim Weninger"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.07701",
    "title": "TMFNet: Two-Stream Multi-Channels Fusion Networks for Color Image Operation Chain Detection",
    "abstract": "           Image operation chain detection techniques have gained increasing attention recently in the field of multimedia forensics. However, existing detection methods suffer from the generalization problem. Moreover, the channel correlation of color images that provides additional forensic evidence is often ignored. To solve these issues, in this article, we propose a novel two-stream multi-channels fusion networks for color image operation chain detection in which the spatial artifact stream and the noise residual stream are explored in a complementary manner. Specifically, we first propose a novel deep residual architecture without pooling in the spatial artifact stream for learning the global features representation of multi-channel correlation. Then, a set of filters is designed to aggregate the correlation information of multi-channels while capturing the low-level features in the noise residual stream. Subsequently, the high-level features are extracted by the deep residual model. Finally, features from the two streams are fed into a fusion module, to effectively learn richer discriminative representations of the operation chain. Extensive experiments show that the proposed method achieves state-of-the-art generalization ability while maintaining robustness to JPEG compression. The source code used in these experiments will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.07701",
    "authors": [
      "Yakun Niu",
      "Lei Tan",
      "Lei Zhang",
      "Xianyu Zuo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2409.07706",
    "title": "Attack End-to-End Autonomous Driving through Module-Wise Noise",
    "abstract": "           With recent breakthroughs in deep neural networks, numerous tasks within autonomous driving have exhibited remarkable performance. However, deep learning models are susceptible to adversarial attacks, presenting significant security risks to autonomous driving systems. Presently, end-to-end architectures have emerged as the predominant solution for autonomous driving, owing to their collaborative nature across different tasks. Yet, the implications of adversarial attacks on such models remain relatively unexplored. In this paper, we conduct comprehensive adversarial security research on the modular end-to-end autonomous driving model for the first time. We thoroughly consider the potential vulnerabilities in the model inference process and design a universal attack scheme through module-wise noise injection. We conduct large-scale experiments on the full-stack autonomous driving model and demonstrate that our attack method outperforms previous attack methods. We trust that our research will offer fresh insights into ensuring the safety and reliability of autonomous driving systems.         ",
    "url": "https://arxiv.org/abs/2409.07706",
    "authors": [
      "Lu Wang",
      "Tianyuan Zhang",
      "Yikai Han",
      "Muyang Fang",
      "Ting Jin",
      "Jiaqi Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.07709",
    "title": "Harnessing TI Feeds for Exploitation Detection",
    "abstract": "           Many organizations rely on Threat Intelligence (TI) feeds to assess the risk associated with security threats. Due to the volume and heterogeneity of data, it is prohibitive to manually analyze the threat information available in different loosely structured TI feeds. Thus, there is a need to develop automated methods to vet and extract actionable information from TI feeds. To this end, we present a machine learning pipeline to automatically detect vulnerability exploitation from TI feeds. We first model threat vocabulary in loosely structured TI feeds using state-of-the-art embedding techniques (Doc2Vec and BERT) and then use it to train a supervised machine learning classifier to detect exploitation of security vulnerabilities. We use our approach to identify exploitation events in 191 different TI feeds. Our longitudinal evaluation shows that it is able to accurately identify exploitation events from TI feeds only using past data for training and even on TI feeds withheld from training. Our proposed approach is useful for a variety of downstream tasks such as data-driven vulnerability risk assessment.         ",
    "url": "https://arxiv.org/abs/2409.07709",
    "authors": [
      "Kajal Patel",
      "Zubair Shafiq",
      "Mateus Nogueira",
      "Daniel Sadoc Menasch\u00e9",
      "Enrico Lovat",
      "Taimur Kashif",
      "Ashton Woiwood",
      "Matheus Martins"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2409.07712",
    "title": "Virtual Node Generation for Node Classification in Sparsely-Labeled Graphs",
    "abstract": "           In the broader machine learning literature, data-generation methods demonstrate promising results by generating additional informative training examples via augmenting sparse labels. Such methods are less studied in graphs due to the intricate dependencies among nodes in complex topology structures. This paper presents a novel node generation method that infuses a small set of high-quality synthesized nodes into the graph as additional labeled nodes to optimally expand the propagation of labeled information. By simply infusing additional nodes, the framework is orthogonal to the graph learning and downstream classification techniques, and thus is compatible with most popular graph pre-training (self-supervised learning), semi-supervised learning, and meta-learning methods. The contribution lies in designing the generated node set by solving a novel optimization problem. The optimization places the generated nodes in a manner that: (1) minimizes the classification loss to guarantee training accuracy and (2) maximizes label propagation to low-confidence nodes in the downstream task to ensure high-quality propagation. Theoretically, we show that the above dual optimization maximizes the global confidence of node classification. Our Experiments demonstrate statistically significant performance improvements over 14 baselines on 10 publicly available datasets.         ",
    "url": "https://arxiv.org/abs/2409.07712",
    "authors": [
      "Hang Cui",
      "Tarek Abdelzaher"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07716",
    "title": "Polarization Detection on Social Networks: dual contrastive objectives for Self-supervision",
    "abstract": "           Echo chambers and online discourses have become prevalent social phenomena where communities engage in dramatic intra-group confirmations and inter-group hostility. Polarization detection is a rising research topic for detecting and identifying such polarized groups. Previous works on polarization detection primarily focus on hand-crafted features derived from dataset-specific characteristics and prior knowledge, which fail to generalize to other datasets. This paper proposes a unified self-supervised polarization detection framework, outperforming previous methods in unsupervised and semi-supervised polarization detection tasks on various publicly available datasets. Our framework utilizes a dual contrastive objective (DocTra): (1) interaction-level: to contrast between node interactions to extract critical features on interaction patterns, and (2) feature-level: to contrast extracted polarized and invariant features to encourage feature decoupling. Our experiments extensively evaluate our methods again 7 baselines on 7 public datasets, demonstrating significant performance improvements.         ",
    "url": "https://arxiv.org/abs/2409.07716",
    "authors": [
      "Hang Cui",
      "Tarek Abdelzaher"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2409.07717",
    "title": "Eyes on the Phish(er): Towards Understanding Users' Email Processing Pattern and Mental Models in Phishing Detection",
    "abstract": "           Phishing emails typically masquerade themselves as reputable identities to trick people into providing sensitive information and credentials. Despite advancements in cybersecurity, attackers continuously adapt, posing ongoing threats to individuals and organisations. While email users are the last line of defence, they are not always well-prepared to detect phishing emails. This study examines how workload affects susceptibility to phishing, using eye-tracking technology to observe participants' reading patterns and interactions with tailored phishing emails. Incorporating both quantitative and qualitative analysis, we investigate users' attention to two phishing indicators, email sender and hyperlink URLs, and their reasons for assessing the trustworthiness of emails and falling for phishing emails. Our results provide concrete evidence that attention to the email sender can reduce phishing susceptibility. While we found no evidence that attention to the actual URL in the browser influences phishing detection, attention to the text masking links can increase phishing susceptibility. We also highlight how email relevance, familiarity, and visual presentation impact first impressions of email trustworthiness and phishing susceptibility.         ",
    "url": "https://arxiv.org/abs/2409.07717",
    "authors": [
      "Sijie Zhuo",
      "Robert Biddle",
      "Jared Daniel Recomendable",
      "Giovanni Russello",
      "Danielle Lottridge"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.07720",
    "title": "Keeping it Authentic: The Social Footprint of the Trolls Network",
    "abstract": "           In 2016, a network of social media accounts animated by Russian operatives attempted to divert political discourse within the American public around the presidential elections. This was a coordinated effort, part of a Russian-led complex information operation. Utilizing the anonymity and outreach of social media platforms Russian operatives created an online astroturf that is in direct contact with regular Americans, promoting Russian agenda and goals. The elusiveness of this type of adversarial approach rendered security agencies helpless, stressing the unique challenges this type of intervention presents. Building on existing scholarship on the functions within influence networks on social media, we suggest a new approach to map those types of operations. We argue that pretending to be legitimate social actors obliges the network to adhere to social expectations, leaving a social footprint. To test the robustness of this social footprint we train artificial intelligence to identify it and create a predictive model. We use Twitter data identified as part of the Russian influence network for training the artificial intelligence and to test the prediction. Our model attains 88% prediction accuracy for the test set. Testing our prediction on two additional models results in 90.7% and 90.5% accuracy, validating our model. The predictive and validation results suggest that building a machine learning model around social functions within the Russian influence network can be used to map its actors and functions.         ",
    "url": "https://arxiv.org/abs/2409.07720",
    "authors": [
      "Ori Swed",
      "Sachith Dassanayaka",
      "Dimitri Volchenkov"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2409.07725",
    "title": "GRE^2-MDCL: Graph Representation Embedding Enhanced via Multidimensional Contrastive Learning",
    "abstract": "           Graph representation learning has emerged as a powerful tool for preserving graph topology when mapping nodes to vector representations, enabling various downstream tasks such as node classification and community detection. However, most current graph neural network models face the challenge of requiring extensive labeled data, which limits their practical applicability in real-world scenarios where labeled data is scarce. To address this challenge, researchers have explored Graph Contrastive Learning (GCL), which leverages enhanced graph data and contrastive learning techniques. While promising, existing GCL methods often struggle with effectively capturing both local and global graph structures, and balancing the trade-off between nodelevel and graph-level representations. In this work, we propose Graph Representation Embedding Enhanced via Multidimensional Contrastive Learning (GRE2-MDCL). Our model introduces a novel triple network architecture with a multi-head attention GNN as the core. GRE2-MDCL first globally and locally augments the input graph using SVD and LAGNN techniques. It then constructs a multidimensional contrastive loss, incorporating cross-network, cross-view, and neighbor contrast, to optimize the model. Extensive experiments on benchmark datasets Cora, Citeseer, and PubMed demonstrate that GRE2-MDCL achieves state-of-the-art performance, with average accuracies of 82.5%, 72.5%, and 81.6% respectively. Visualizations further show tighter intra-cluster aggregation and clearer inter-cluster boundaries, highlighting the effectiveness of our framework in improving upon baseline GCL models.         ",
    "url": "https://arxiv.org/abs/2409.07725",
    "authors": [
      "Kaizhe Fan",
      "Quanjun Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.07734",
    "title": "DFDG: Data-Free Dual-Generator Adversarial Distillation for One-Shot Federated Learning",
    "abstract": "           Federated Learning (FL) is a distributed machine learning scheme in which clients jointly participate in the collaborative training of a global model by sharing model information rather than their private datasets. In light of concerns associated with communication and privacy, one-shot FL with a single communication round has emerged as a de facto promising solution. However, existing one-shot FL methods either require public datasets, focus on model homogeneous settings, or distill limited knowledge from local models, making it difficult or even impractical to train a robust global model. To address these limitations, we propose a new data-free dual-generator adversarial distillation method (namely DFDG) for one-shot FL, which can explore a broader local models' training space via training dual generators. DFDG is executed in an adversarial manner and comprises two parts: dual-generator training and dual-model distillation. In dual-generator training, we delve into each generator concerning fidelity, transferability and diversity to ensure its utility, and additionally tailor the cross-divergence loss to lessen the overlap of dual generators' output spaces. In dual-model distillation, the trained dual generators work together to provide the training data for updates of the global model. At last, our extensive experiments on various image classification tasks show that DFDG achieves significant performance gains in accuracy compared to SOTA baselines.         ",
    "url": "https://arxiv.org/abs/2409.07734",
    "authors": [
      "Kangyang Luo",
      "Shuai Wang",
      "Yexuan Fu",
      "Renrong Shao",
      "Xiang Li",
      "Yunshi Lan",
      "Ming Gao",
      "Jinlong Shu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07746",
    "title": "Learning Brain Tumor Representation in 3D High-Resolution MR Images via Interpretable State Space Models",
    "abstract": "           Learning meaningful and interpretable representations from high-dimensional volumetric magnetic resonance (MR) images is essential for advancing personalized medicine. While Vision Transformers (ViTs) have shown promise in handling image data, their application to 3D multi-contrast MR images faces challenges due to computational complexity and interpretability. To address this, we propose a novel state-space-model (SSM)-based masked autoencoder which scales ViT-like models to handle high-resolution data effectively while also enhancing the interpretability of learned representations. We propose a latent-to-spatial mapping technique that enables direct visualization of how latent features correspond to specific regions in the input volumes in the context of SSM. We validate our method on two key neuro-oncology tasks: identification of isocitrate dehydrogenase mutation status and 1p/19q co-deletion classification, achieving state-of-the-art accuracy. Our results highlight the potential of SSM-based self-supervised learning to transform radiomics analysis by combining efficiency and interpretability.         ",
    "url": "https://arxiv.org/abs/2409.07746",
    "authors": [
      "Qingqiao Hu",
      "Daoan Zhang",
      "Jiebo Luo",
      "Zhenyu Gong",
      "Benedikt Wiestler",
      "Jianguo Zhang",
      "Hongwei Bran Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.07747",
    "title": "Multi-object event graph representation learning for Video Question Answering",
    "abstract": "           Video question answering (VideoQA) is a task to predict the correct answer to questions posed about a given video. The system must comprehend spatial and temporal relationships among objects extracted from videos to perform causal and temporal reasoning. While prior works have focused on modeling individual object movements using transformer-based methods, they falter when capturing complex scenarios involving multiple objects (e.g., \"a boy is throwing a ball in a hoop\"). We propose a contrastive language event graph representation learning method called CLanG to address this limitation. Aiming to capture event representations associated with multiple objects, our method employs a multi-layer GNN-cluster module for adversarial graph representation learning, enabling contrastive learning between the question text and its relevant multi-object event graph. Our method outperforms a strong baseline, achieving up to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and TGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal and temporal questions, highlighting its strength in reasoning multiple object-based events.         ",
    "url": "https://arxiv.org/abs/2409.07747",
    "authors": [
      "Yanan Wang",
      "Shuichiro Haruta",
      "Donghuo Zeng",
      "Julio Vizcarra",
      "Mori Kurokawa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.07748",
    "title": "Top-down Activity Representation Learning for Video Question Answering",
    "abstract": "           Capturing complex hierarchical human activities, from atomic actions (e.g., picking up one present, moving to the sofa, unwrapping the present) to contextual events (e.g., celebrating Christmas) is crucial for achieving high-performance video question answering (VideoQA). Recent works have expanded multimodal models (e.g., CLIP, LLaVA) to process continuous video sequences, enhancing the model's temporal reasoning capabilities. However, these approaches often fail to capture contextual events that can be decomposed into multiple atomic actions non-continuously distributed over relatively long-term sequences. In this paper, to leverage the spatial visual context representation capability of the CLIP model for obtaining non-continuous visual representations in terms of contextual events in videos, we convert long-term video sequences into a spatial image domain and finetune the multimodal model LLaVA for the VideoQA task. Our approach achieves competitive performance on the STAR task, in particular, with a 78.4% accuracy score, exceeding the current state-of-the-art score by 2.8 points on the NExTQA task.         ",
    "url": "https://arxiv.org/abs/2409.07748",
    "authors": [
      "Yanan Wang",
      "Shuichiro Haruta",
      "Donghuo Zeng",
      "Julio Vizcarra",
      "Mori Kurokawa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.07762",
    "title": "Exploring Kolmogorov-Arnold networks for realistic image sharpness assessment",
    "abstract": "           Score prediction is crucial in realistic image sharpness assessment after informative features are collected. Recently, Kolmogorov-Arnold networks (KANs) have been developed and witnessed remarkable success in data fitting. This study presents Taylor series based KAN (TaylorKAN). Then, different KANs are explored on four realistic image databases (BID2011, CID2013, CLIVE, and KonIQ-10k) for score prediction by using 15 mid-level features and 2048 high-level features. When setting support vector regression as the baseline, experimental results indicate KANs are generally better or competitive, TaylorKAN is the best on three databases using mid-level feature input, while KANs are inferior on CLIVE when high-level features are used. This is the first study that explores KANs for image quality assessment. It sheds lights on how to select and improve KANs on related tasks.         ",
    "url": "https://arxiv.org/abs/2409.07762",
    "authors": [
      "Shaode Yu",
      "Ze Chen",
      "Zhimu Yang",
      "Jiacheng Gu",
      "Bizu Feng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07763",
    "title": "Reimagining Linear Probing: Kolmogorov-Arnold Networks in Transfer Learning",
    "abstract": "           This paper introduces Kolmogorov-Arnold Networks (KAN) as an enhancement to the traditional linear probing method in transfer learning. Linear probing, often applied to the final layer of pre-trained models, is limited by its inability to model complex relationships in data. To address this, we propose substituting the linear probing layer with KAN, which leverages spline-based representations to approximate intricate functions. In this study, we integrate KAN with a ResNet-50 model pre-trained on ImageNet and evaluate its performance on the CIFAR-10 dataset. We perform a systematic hyperparameter search, focusing on grid size and spline degree (k), to optimize KAN's flexibility and accuracy. Our results demonstrate that KAN consistently outperforms traditional linear probing, achieving significant improvements in accuracy and generalization across a range of configurations. These findings indicate that KAN offers a more powerful and adaptable alternative to conventional linear probing techniques in transfer learning.         ",
    "url": "https://arxiv.org/abs/2409.07763",
    "authors": [
      "Sheng Shen",
      "Rabih Younes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.07766",
    "title": "Resilient Learning-Based Control Under Denial-of-Service Attacks",
    "abstract": "           In this paper, we have proposed a resilient reinforcement learning method for discrete-time linear systems with unknown parameters, under denial-of-service (DoS) attacks. The proposed method is based on policy iteration that learns the optimal controller from input-state data amidst DoS attacks. We achieve an upper bound for the DoS duration to ensure closed-loop stability. The resilience of the closed-loop system, when subjected to DoS attacks with the learned controller and an internal model, has been thoroughly examined. The effectiveness of the proposed methodology is demonstrated on an inverted pendulum on a cart.         ",
    "url": "https://arxiv.org/abs/2409.07766",
    "authors": [
      "Sayan Chakraborty",
      "Weinan Gao",
      "Kyriakos G. Vamvoudakis",
      "Zhong-Ping Jiang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.07775",
    "title": "A Spatiotemporal Stealthy Backdoor Attack against Cooperative Multi-Agent Deep Reinforcement Learning",
    "abstract": "           Recent studies have shown that cooperative multi-agent deep reinforcement learning (c-MADRL) is under the threat of backdoor attacks. Once a backdoor trigger is observed, it will perform abnormal actions leading to failures or malicious goals. However, existing proposed backdoors suffer from several issues, e.g., fixed visual trigger patterns lack stealthiness, the backdoor is trained or activated by an additional network, or all agents are backdoored. To this end, in this paper, we propose a novel backdoor attack against c-MADRL, which attacks the entire multi-agent team by embedding the backdoor only in a single agent. Firstly, we introduce adversary spatiotemporal behavior patterns as the backdoor trigger rather than manual-injected fixed visual patterns or instant status and control the attack duration. This method can guarantee the stealthiness and practicality of injected backdoors. Secondly, we hack the original reward function of the backdoored agent via reward reverse and unilateral guidance during training to ensure its adverse influence on the entire team. We evaluate our backdoor attacks on two classic c-MADRL algorithms VDN and QMIX, in a popular c-MADRL environment SMAC. The experimental results demonstrate that our backdoor attacks are able to reach a high attack success rate (91.6\\%) while maintaining a low clean performance variance rate (3.7\\%).         ",
    "url": "https://arxiv.org/abs/2409.07775",
    "authors": [
      "Yinbo Yu",
      "Saihao Yan",
      "Jiajia Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.07776",
    "title": "Training Spiking Neural Networks via Augmented Direct Feedback Alignment",
    "abstract": "           Spiking neural networks (SNNs), the models inspired by the mechanisms of real neurons in the brain, transmit and represent information by employing discrete action potentials or spikes. The sparse, asynchronous properties of information processing make SNNs highly energy efficient, leading to SNNs being promising solutions for implementing neural networks in neuromorphic devices. However, the nondifferentiable nature of SNN neurons makes it a challenge to train them. The current training methods of SNNs that are based on error backpropagation (BP) and precisely designing surrogate gradient are difficult to implement and biologically implausible, hindering the implementation of SNNs on neuromorphic devices. Thus, it is important to train SNNs with a method that is both physically implementatable and biologically plausible. In this paper, we propose using augmented direct feedback alignment (aDFA), a gradient-free approach based on random projection, to train SNNs. This method requires only partial information of the forward process during training, so it is easy to implement and biologically plausible. We systematically demonstrate the feasibility of the proposed aDFA-SNNs scheme, propose its effective working range, and analyze its well-performing settings by employing genetic algorithm. We also analyze the impact of crucial features of SNNs on the scheme, thus demonstrating its superiority and stability over BP and conventional direct feedback alignment. Our scheme can achieve competitive performance without accurate prior knowledge about the utilized system, thus providing a valuable reference for physically training SNNs.         ",
    "url": "https://arxiv.org/abs/2409.07776",
    "authors": [
      "Yongbo Zhang",
      "Katsuma Inoue",
      "Mitsumasa Nakajima",
      "Toshikazu Hashimoto",
      "Yasuo Kuniyoshi",
      "Kohei Nakajima"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07779",
    "title": "ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation",
    "abstract": "           Medical image segmentation, a crucial task in computer vision, facilitates the automated delineation of anatomical structures and pathologies, supporting clinicians in diagnosis, treatment planning, and disease monitoring. Notably, transformers employing shifted window-based self-attention have demonstrated exceptional performance. However, their reliance on local window attention limits the fusion of local and global contextual information, crucial for segmenting microtumors and miniature organs. To address this limitation, we propose the Adaptive Semantic Segmentation Network (ASSNet), a transformer architecture that effectively integrates local and global features for precise medical image segmentation. ASSNet comprises a transformer-based U-shaped encoder-decoder network. The encoder utilizes shifted window self-attention across five resolutions to extract multi-scale features, which are then propagated to the decoder through skip connections. We introduce an augmented multi-layer perceptron within the encoder to explicitly model long-range dependencies during feature extraction. Recognizing the constraints of conventional symmetrical encoder-decoder designs, we propose an Adaptive Feature Fusion (AFF) decoder to complement our encoder. This decoder incorporates three key components: the Long Range Dependencies (LRD) block, the Multi-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC) block. These components synergistically facilitate the effective fusion of multi-scale features extracted by the decoder while capturing long-range dependencies and refining object boundaries. Comprehensive experiments on diverse medical image segmentation tasks, including multi-organ, liver tumor, and bladder tumor segmentation, demonstrate that ASSNet achieves state-of-the-art results. Code and models are available at: \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2409.07779",
    "authors": [
      "Fuchen Zheng",
      "Xinyi Chen",
      "Xuhang Chen",
      "Haolun Li",
      "Xiaojiao Guo",
      "Guoheng Huang",
      "Chi-Man Pun",
      "Shoujun Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.07785",
    "title": "Critical link identification of power system vulnerability based on modified graph attention network",
    "abstract": "           With the expansion of the power grid and the increase of the proportion of new energy sources, the uncertainty and random factors of the power grid increase, endangering the safe operation of the system. It is particularly important to find out the critical links of vulnerability in the power grid to ensure the reliability of the power grid operation. Aiming at the problem that the identification speed of the traditional critical link of vulnerability identification methods is slow and difficult to meet the actual operation requirements of the power grid, the improved graph attention network (IGAT) based identification method of the critical link is proposed. First, the evaluation index set is established by combining the complex network theory and the actual operation data of power grid. Secondly, IGAT is used to dig out the mapping relationship between various indicators and critical links of vulnerability during the operation of the power grid, establish the identification model of critical links of vulnerability, and optimize the original graph attention network considering the training accuracy and efficiency. Thirdly, the original data set is obtained through simulation, and the identification model is trained, verified and tested. Finally, the model is applied to the improved IEEE 30-node system and the actual power grid, and the results show that the proposed method is feasible, and the accuracy and speed are better than that of traditional methods. It has certain engineering utilization value.         ",
    "url": "https://arxiv.org/abs/2409.07785",
    "authors": [
      "Changgang Wang",
      "Xianwei Wang",
      "Yu Cao",
      "Yang Li",
      "Qi Lv",
      "Yaoxin Zhang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.07787",
    "title": "Stable Language Model Pre-training by Reducing Embedding Variability",
    "abstract": "           Stable pre-training is essential for achieving better-performing language models. However, tracking pre-training stability by calculating gradient variance at every step is impractical due to the significant computational costs. We explore Token Embedding Variability (TEV) as a simple and efficient proxy for assessing pre-training stability in language models with pre-layer normalization, given that shallower layers are more prone to gradient explosion (section 2.2). Moreover, we propose Multi-head Low-Rank Attention (MLRA) as an architecture to alleviate such instability by limiting the exponential growth of output embedding variance, thereby preventing the gradient explosion (section 3.2). Empirical results on GPT-2 with MLRA demonstrate increased stability and lower perplexity, particularly in deeper models.         ",
    "url": "https://arxiv.org/abs/2409.07787",
    "authors": [
      "Woojin Chung",
      "Jiwoo Hong",
      "Na Min An",
      "James Thorne",
      "Se-Young Yun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.07794",
    "title": "Efficient Learning of Balanced Signed Graphs via Iterative Linear Programming",
    "abstract": "           Signed graphs are equipped with both positive and negative edge weights, encoding pairwise correlations as well as anti-correlations in data. A balanced signed graph has no cycles of odd number of negative edges. Laplacian of a balanced signed graph has eigenvectors that map simply to ones in a similarity-transformed positive graph Laplacian, thus enabling reuse of well-studied spectral filters designed for positive graphs. We propose a fast method to learn a balanced signed graph Laplacian directly from data. Specifically, for each node $i$, to determine its polarity $\\beta_i \\in \\{-1,1\\}$ and edge weights $\\{w_{i,j}\\}_{j=1}^N$, we extend a sparse inverse covariance formulation based on linear programming (LP) called CLIME, by adding linear constraints to enforce ``consistent\" signs of edge weights $\\{w_{i,j}\\}_{j=1}^N$ with the polarities of connected nodes -- i.e., positive/negative edges connect nodes of same/opposing polarities. For each LP, we adapt projections on convex set (POCS) to determine a suitable CLIME parameter $\\rho > 0$ that guarantees LP feasibility. We solve the resulting LP via an off-the-shelf LP solver in $\\mathcal{O}(N^{2.055})$. Experiments on synthetic and real-world datasets show that our balanced graph learning method outperforms competing methods and enables the use of spectral filters and graph convolutional networks (GCNs) designed for positive graphs on signed graphs.         ",
    "url": "https://arxiv.org/abs/2409.07794",
    "authors": [
      "Haruki Yokota",
      "Hiroshi Higashi",
      "Yuichi Tanaka",
      "Gene Cheung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2409.07809",
    "title": "Controllable Synthetic Clinical Note Generation with Privacy Guarantees",
    "abstract": "           In the field of machine learning, domain-specific annotated data is an invaluable resource for training effective models. However, in the medical domain, this data often includes Personal Health Information (PHI), raising significant privacy concerns. The stringent regulations surrounding PHI limit the availability and sharing of medical datasets, which poses a substantial challenge for researchers and practitioners aiming to develop advanced machine learning models. In this paper, we introduce a novel method to \"clone\" datasets containing PHI. Our approach ensures that the cloned datasets retain the essential characteristics and utility of the original data without compromising patient privacy. By leveraging differential-privacy techniques and a novel fine-tuning task, our method produces datasets that are free from identifiable information while preserving the statistical properties necessary for model training. We conduct utility testing to evaluate the performance of machine learning models trained on the cloned datasets. The results demonstrate that our cloned datasets not only uphold privacy standards but also enhance model performance compared to those trained on traditional anonymized datasets. This work offers a viable solution for the ethical and effective utilization of sensitive medical data in machine learning, facilitating progress in medical research and the development of robust predictive models.         ",
    "url": "https://arxiv.org/abs/2409.07809",
    "authors": [
      "Tal Baumel",
      "Andre Manoel",
      "Daniel Jones",
      "Shize Su",
      "Huseyin Inan",
      "Aaron",
      "Bornstein",
      "Robert Sim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07823",
    "title": "Online vs Offline: A Comparative Study of First-Party and Third-Party Evaluations of Social Chatbots",
    "abstract": "           This paper explores the efficacy of online versus offline evaluation methods in assessing conversational chatbots, specifically comparing first-party direct interactions with third-party observational assessments. By extending a benchmarking dataset of user dialogs with empathetic chatbots with offline third-party evaluations, we present a systematic comparison between the feedback from online interactions and the more detached offline third-party evaluations. Our results reveal that offline human evaluations fail to capture the subtleties of human-chatbot interactions as effectively as online assessments. In comparison, automated third-party evaluations using a GPT-4 model offer a better approximation of first-party human judgments given detailed instructions. This study highlights the limitations of third-party evaluations in grasping the complexities of user experiences and advocates for the integration of direct interaction feedback in conversational AI evaluation to enhance system development and user satisfaction.         ",
    "url": "https://arxiv.org/abs/2409.07823",
    "authors": [
      "Ekaterina Svikhnushina",
      "Pearl Pu"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.07833",
    "title": "Classifying Images with CoLaNET Spiking Neural Network -- the MNIST Example",
    "abstract": "           In the present paper, it is shown how the columnar/layered CoLaNET spiking neural network (SNN) architecture can be used in supervised learning image classification tasks. Image pixel brightness is coded by the spike count during image presentation period. Image class label is indicated by activity of special SNN input nodes (one node per class). The CoLaNET classification accuracy is evaluated on the MNIST benchmark. It is demonstrated that CoLaNET is almost as accurate as the most advanced machine learning algorithms (not using convolutional approach).         ",
    "url": "https://arxiv.org/abs/2409.07833",
    "authors": [
      "Mikhail Kiselev"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2409.07839",
    "title": "FPMT: Enhanced Semi-Supervised Model for Traffic Incident Detection",
    "abstract": "           For traffic incident detection, the acquisition of data and labels is notably resource-intensive, rendering semi-supervised traffic incident detection both a formidable and consequential challenge. Thus, this paper focuses on traffic incident detection with a semi-supervised learning way. It proposes a semi-supervised learning model named FPMT within the framework of MixText. The data augmentation module introduces Generative Adversarial Networks to balance and expand the dataset. During the mix-up process in the hidden space, it employs a probabilistic pseudo-mixing mechanism to enhance regularization and elevate model precision. In terms of training strategy, it initiates with unsupervised training on all data, followed by supervised fine-tuning on a subset of labeled data, and ultimately completing the goal of semi-supervised training. Through empirical validation on four authentic datasets, our FPMT model exhibits outstanding performance across various metrics. Particularly noteworthy is its robust performance even in scenarios with low label rates.         ",
    "url": "https://arxiv.org/abs/2409.07839",
    "authors": [
      "Xinying Lu",
      "Jianli Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.07850",
    "title": "Enhancing Cross-Market Recommendation System with Graph Isomorphism Networks: A Novel Approach to Personalized User Experience",
    "abstract": "           In today's world of globalized commerce, cross-market recommendation systems (CMRs) are crucial for providing personalized user experiences across diverse market segments. However, traditional recommendation algorithms have difficulties dealing with market specificity and data sparsity, especially in new or emerging markets. In this paper, we propose the CrossGR model, which utilizes Graph Isomorphism Networks (GINs) to improve CMR systems. It outperforms existing benchmarks in NDCG@10 and HR@10 metrics, demonstrating its adaptability and accuracy in handling diverse market segments. The CrossGR model is adaptable and accurate, making it well-suited for handling the complexities of cross-market recommendation tasks. Its robustness is demonstrated by consistent performance across different evaluation timeframes, indicating its potential to cater to evolving market trends and user preferences. Our findings suggest that GINs represent a promising direction for CMRs, paving the way for more sophisticated, personalized, and context-aware recommendation systems in the dynamic landscape of global e-commerce.         ",
    "url": "https://arxiv.org/abs/2409.07850",
    "authors": [
      "S\u00fcmeyye \u00d6zt\u00fcrk",
      "Ahmed Burak Ercan",
      "Resul Tugay",
      "\u015eule G\u00fcnd\u00fcz \u00d6\u011f\u00fcd\u00fcc\u00fc"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07855",
    "title": "MSMF: Multi-Scale Multi-Modal Fusion for Enhanced Stock Market Prediction",
    "abstract": "           This paper presents MSMF (Multi-Scale Multi-Modal Fusion), a novel approach for enhanced stock market prediction. MSMF addresses key challenges in multi-modal stock analysis by integrating a modality completion encoder, multi-scale feature extraction, and an innovative fusion mechanism. Our model leverages blank learning and progressive fusion to balance complementarity and redundancy across modalities, while multi-scale alignment facilitates direct correlations between heterogeneous data types. We introduce Multi-Granularity Gates and a specialized architecture to optimize the integration of local and global information for different tasks. Additionally, a Task-targeted Prediction layer is employed to preserve both coarse and fine-grained features during fusion. Experimental results demonstrate that MSMF outperforms existing methods, achieving significant improvements in accuracy and reducing prediction errors across various stock market forecasting tasks. This research contributes valuable insights to the field of multi-modal financial analysis and offers a robust framework for enhanced market prediction.         ",
    "url": "https://arxiv.org/abs/2409.07855",
    "authors": [
      "Jiahao Qin"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2409.07884",
    "title": "Graph Neural Networks for Parkinsons Disease Detection",
    "abstract": "           Despite the promising performance of state of the art approaches for Parkinsons Disease (PD) detection, these approaches often analyze individual speech segments in isolation, which can lead to suboptimal results. Dysarthric cues that characterize speech impairments from PD patients are expected to be related across segments from different speakers. Isolated segment analysis fails to exploit these inter segment relationships. Additionally, not all speech segments from PD patients exhibit clear dysarthric symptoms, introducing label noise that can negatively affect the performance and generalizability of current approaches. To address these challenges, we propose a novel PD detection framework utilizing Graph Convolutional Networks (GCNs). By representing speech segments as nodes and capturing the similarity between segments through edges, our GCN model facilitates the aggregation of dysarthric cues across the graph, effectively exploiting segment relationships and mitigating the impact of label noise. Experimental results demonstrate theadvantages of the proposed GCN model for PD detection and provide insights into its underlying mechanisms         ",
    "url": "https://arxiv.org/abs/2409.07884",
    "authors": [
      "Shakeel A. Sheikh",
      "Yacouba Kaloga",
      "Ina Kodrasi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2409.07889",
    "title": "BLens: Contrastive Captioning of Binary Functions using Ensemble Embedding",
    "abstract": "           Function names can greatly aid human reverse engineers, which has spurred development of machine learning-based approaches to predicting function names in stripped binaries. Much current work in this area now uses transformers, applying a metaphor of machine translation from code to function names. Still, function naming models face challenges in generalizing to projects completely unrelated to the training set. In this paper, we take a completely new approach by transferring advances in automated image captioning to the domain of binary reverse engineering, such that different parts of a binary function can be associated with parts of its name. We propose BLens, which combines multiple binary function embeddings into a new ensemble representation, aligns it with the name representation latent space via a contrastive learning approach, and generates function names with a transformer architecture tailored for function names. In our experiments, we demonstrate that BLens significantly outperforms the state of the art. In the usual setting of splitting per binary, we achieve an $F_1$ score of 0.77 compared to 0.67. Moreover, in the cross-project setting, which emphasizes generalizability, we achieve an $F_1$ score of 0.46 compared to 0.29.         ",
    "url": "https://arxiv.org/abs/2409.07889",
    "authors": [
      "Tristan Benoit",
      "Yunru Wang",
      "Moritz Dannehl",
      "Johannes Kinder"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.07901",
    "title": "Bridging Discrete and Continuous: A Multimodal Strategy for Complex Emotion Detection",
    "abstract": "           In the domain of human-computer interaction, accurately recognizing and interpreting human emotions is crucial yet challenging due to the complexity and subtlety of emotional expressions. This study explores the potential for detecting a rich and flexible range of emotions through a multimodal approach which integrates facial expressions, voice tones, and transcript from video clips. We propose a novel framework that maps variety of emotions in a three-dimensional Valence-Arousal-Dominance (VAD) space, which could reflect the fluctuations and positivity/negativity of emotions to enable a more variety and comprehensive representation of emotional states. We employed K-means clustering to transit emotions from traditional discrete categorization to a continuous labeling system and built a classifier for emotion recognition upon this system. The effectiveness of the proposed model is evaluated using the MER2024 dataset, which contains culturally consistent video clips from Chinese movies and TV series, annotated with both discrete and open-vocabulary emotion labels. Our experiment successfully achieved the transformation between discrete and continuous models, and the proposed model generated a more diverse and comprehensive set of emotion vocabulary while maintaining strong accuracy.         ",
    "url": "https://arxiv.org/abs/2409.07901",
    "authors": [
      "Jiehui Jia",
      "Huan Zhang",
      "Jinhua Liang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2409.07911",
    "title": "Tera-SpaceCom: GNN-based Deep Reinforcement Learning for Joint Resource Allocation and Task Offloading in TeraHertz Band Space Networks",
    "abstract": "           Terahertz (THz) space communications (Tera-SpaceCom) is envisioned as a promising technology to enable various space science and communication applications. Mainly, the realm of Tera-SpaceCom consists of THz sensing for space exploration, data centers in space providing cloud services for space exploration tasks, and a low earth orbit (LEO) mega-constellation relaying these tasks to ground stations (GSs) or data centers via THz links. Moreover, to reduce the computational burden on data centers as well as resource consumption and latency in the relaying process, the LEO mega-constellation provides satellite edge computing (SEC) services to directly compute space exploration tasks without relaying these tasks to data centers. The LEO satellites that receive space exploration tasks offload (i.e., distribute) partial tasks to their neighboring LEO satellites, to further reduce their computational burden. However, efficient joint communication resource allocation and computing task offloading for the Tera-SpaceCom SEC network is an NP-hard mixed-integer nonlinear programming problem (MINLP), due to the discrete nature of space exploration tasks and sub-arrays as well as the continuous nature of transmit power. To tackle this challenge, a graph neural network (GNN)-deep reinforcement learning (DRL)-based joint resource allocation and task offloading (GRANT) algorithm is proposed with the target of long-term resource efficiency (RE). Particularly, GNNs learn relationships among different satellites from their connectivity information. Furthermore, multi-agent and multi-task mechanisms cooperatively train task offloading and resource allocation. Compared with benchmark solutions, GRANT not only achieves the highest RE with relatively low latency, but realizes the fewest trainable parameters and the shortest running time.         ",
    "url": "https://arxiv.org/abs/2409.07911",
    "authors": [
      "Zhifeng Hu",
      "Chong Han",
      "Wolfgang Gerstacker",
      "Ian F. Akyildiz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07912",
    "title": "Multi-granularity Score-based Generative Framework Enables Efficient Inverse Design of Complex Organics",
    "abstract": "           Efficiently retrieving an enormous chemical library to design targeted molecules is crucial for accelerating drug discovery, organic chemistry, and optoelectronic materials. Despite the emergence of generative models to produce novel drug-like molecules, in a more realistic scenario, the complexity of functional groups (e.g., pyrene, acenaphthylene, and bridged-ring systems) and extensive molecular scaffolds remain challenging obstacles for the generation of complex organics. Traditionally, the former demands an extra learning process, e.g., molecular pre-training, and the latter requires expensive computational resources. To address these challenges, we propose OrgMol-Design, a multi-granularity framework for efficiently designing complex organics. Our OrgMol-Design is composed of a score-based generative model via fragment prior for diverse coarse-grained scaffold generation and a chemical-rule-aware scoring model for fine-grained molecular structure design, circumventing the difficulty of intricate substructure learning without losing connection details among fragments. Our approach achieves state-of-the-art performance in four real-world and more challenging benchmarks covering broader scientific domains, outperforming advanced molecule generative models. Additionally, it delivers a substantial speedup and graphics memory reduction compared to diffusion-based graph models. Our results also demonstrate the importance of leveraging fragment prior for a generalized molecule inverse design model.         ",
    "url": "https://arxiv.org/abs/2409.07912",
    "authors": [
      "Zijun Chen",
      "Yu Wang",
      "Liuzhenghao Lv",
      "Hao Li",
      "Zongying Lin",
      "Li Yuan",
      "Yonghong Tian"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2409.07918",
    "title": "Tidal MerzA: Combining affective modelling and autonomous code generation through Reinforcement Learning",
    "abstract": "           This paper presents Tidal-MerzA, a novel system designed for collaborative performances between humans and a machine agent in the context of live coding, specifically focusing on the generation of musical patterns. Tidal-MerzA fuses two foundational models: ALCAA (Affective Live Coding Autonomous Agent) and Tidal Fuzz, a computational framework. By integrating affective modelling with computational generation, this system leverages reinforcement learning techniques to dynamically adapt music composition parameters within the TidalCycles framework, ensuring both affective qualities to the patterns and syntactical correctness. The development of Tidal-MerzA introduces two distinct agents: one focusing on the generation of mini-notation strings for musical expression, and another on the alignment of music with targeted affective states through reinforcement learning. This approach enhances the adaptability and creative potential of live coding practices and allows exploration of human-machine creative interactions. Tidal-MerzA advances the field of computational music generation, presenting a novel methodology for incorporating artificial intelligence into artistic practices.         ",
    "url": "https://arxiv.org/abs/2409.07918",
    "authors": [
      "Elizabeth Wilson",
      "Gy\u00f6rgy Fazekas",
      "Geraint Wiggins"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2409.07925",
    "title": "A framework for measuring the training efficiency of a neural architecture",
    "abstract": "           Measuring Efficiency in neural network system development is an open research problem. This paper presents an experimental framework to measure the training efficiency of a neural architecture. To demonstrate our approach, we analyze the training efficiency of Convolutional Neural Networks and Bayesian equivalents on the MNIST and CIFAR-10 tasks. Our results show that training efficiency decays as training progresses and varies across different stopping criteria for a given neural model and learning task. We also find a non-linear relationship between training stopping criteria, training Efficiency, model size, and training Efficiency. Furthermore, we illustrate the potential confounding effects of overtraining on measuring the training efficiency of a neural architecture. Regarding relative training efficiency across different architectures, our results indicate that CNNs are more efficient than BCNNs on both datasets. More generally, as a learning task becomes more complex, the relative difference in training efficiency between different architectures becomes more pronounced.         ",
    "url": "https://arxiv.org/abs/2409.07925",
    "authors": [
      "Eduardo Cueto-Mendoza",
      "John D. Kelleher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.07931",
    "title": "Task-Augmented Cross-View Imputation Network for Partial Multi-View Incomplete Multi-Label Classification",
    "abstract": "           In real-world scenarios, multi-view multi-label learning often encounters the challenge of incomplete training data due to limitations in data collection and unreliable annotation processes. The absence of multi-view features impairs the comprehensive understanding of samples, omitting crucial details essential for classification. To address this issue, we present a task-augmented cross-view imputation network (TACVI-Net) for the purpose of handling partial multi-view incomplete multi-label classification. Specifically, we employ a two-stage network to derive highly task-relevant features to recover the missing views. In the first stage, we leverage the information bottleneck theory to obtain a discriminative representation of each view by extracting task-relevant information through a view-specific encoder-classifier architecture. In the second stage, an autoencoder based multi-view reconstruction network is utilized to extract high-level semantic representation of the augmented features and recover the missing data, thereby aiding the final classification task. Extensive experiments on five datasets demonstrate that our TACVI-Net outperforms other state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2409.07931",
    "authors": [
      "Xiaohuan Lu",
      "Lian Zhao",
      "Wai Keung Wong",
      "Jie Wen",
      "Jiang Long",
      "Wulin Xie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.07932",
    "title": "Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies",
    "abstract": "           Graph path search is a classic computer science problem that has been recently approached with Reinforcement Learning (RL) due to its potential to outperform prior methods. Existing RL techniques typically assume a global view of the network, which is not suitable for large-scale, dynamic, and privacy-sensitive settings. An area of particular interest is search in social networks due to its numerous applications. Inspired by seminal work in experimental sociology, which showed that decentralized yet efficient search is possible in social networks, we frame the problem as a collaborative task between multiple agents equipped with a limited local view of the network. We propose a multi-agent approach for graph path search that successfully leverages both homophily and structural heterogeneity. Our experiments, carried out over synthetic and real-world social networks, demonstrate that our model significantly outperforms learned and heuristic baselines. Furthermore, our results show that meaningful embeddings for graph navigation can be constructed using reward-driven learning.         ",
    "url": "https://arxiv.org/abs/2409.07932",
    "authors": [
      "Alexei Pisacane",
      "Victor-Alexandru Darvariu",
      "Mirco Musolesi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2409.07946",
    "title": "Collaborative Automatic Modulation Classification via Deep Edge Inference for Hierarchical Cognitive Radio Networks",
    "abstract": "           In hierarchical cognitive radio networks, edge or cloud servers utilize the data collected by edge devices for modulation classification, which, however, is faced with problems of the transmission overhead, data privacy, and computation load. In this article, an edge learning (EL) based framework jointly mobilizing the edge device and the edge server for intelligent co-inference is proposed to realize the collaborative automatic modulation classification (C-AMC) between them. A spectrum semantic compression neural network (SSCNet) with the lightweight structure is designed for the edge device to compress the collected raw data into a compact semantic message that is then sent to the edge server via the wireless channel. On the edge server side, a modulation classification neural network (MCNet) combining bidirectional long short-term memory (Bi?LSTM) and multi-head attention layers is elaborated to deter?mine the modulation type from the noisy semantic message. By leveraging the computation resources of both the edge device and the edge server, high transmission overhead and risks of data privacy leakage are avoided. The simulation results verify the effectiveness of the proposed C-AMC framework, significantly reducing the model size and computational complexity.         ",
    "url": "https://arxiv.org/abs/2409.07946",
    "authors": [
      "Chaowei He",
      "Peihao Dong",
      "Fuhui Zhou",
      "Qihui Wu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2409.07958",
    "title": "Enhanced Online Grooming Detection Employing Context Determination and Message-Level Analysis",
    "abstract": "           Online Grooming (OG) is a prevalent threat facing predominately children online, with groomers using deceptive methods to prey on the vulnerability of children on social media/messaging platforms. These attacks can have severe psychological and physical impacts, including a tendency towards revictimization. Current technical measures are inadequate, especially with the advent of end-to-end encryption which hampers message monitoring. Existing solutions focus on the signature analysis of child abuse media, which does not effectively address real-time OG detection. This paper proposes that OG attacks are complex, requiring the identification of specific communication patterns between adults and children. It introduces a novel approach leveraging advanced models such as BERT and RoBERTa for Message-Level Analysis and a Context Determination approach for classifying actor interactions, including the introduction of Actor Significance Thresholds and Message Significance Thresholds. The proposed method aims to enhance accuracy and robustness in detecting OG by considering the dynamic and multi-faceted nature of these attacks. Cross-dataset experiments evaluate the robustness and versatility of our approach. This paper's contributions include improved detection methodologies and the potential for application in various scenarios, addressing gaps in current literature and practices.         ",
    "url": "https://arxiv.org/abs/2409.07958",
    "authors": [
      "Jake Street",
      "Isibor Ihianle",
      "Funminiyi Olajide",
      "Ahmad Lotfi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.07964",
    "title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks",
    "abstract": "           Wireless networks are increasingly facing challenges due to their expanding scale and complexity. These challenges underscore the need for advanced AI-driven strategies, particularly in the upcoming 6G networks. In this article, we introduce WirelessAgent, a novel approach leveraging large language models (LLMs) to develop AI agents capable of managing complex tasks in wireless networks. It can effectively improve network performance through advanced reasoning, multimodal data processing, and autonomous decision making. Thereafter, we demonstrate the practical applicability and benefits of WirelessAgent for network slicing management. The experimental results show that WirelessAgent is capable of accurately understanding user intent, effectively allocating slice resources, and consistently maintaining optimal performance.         ",
    "url": "https://arxiv.org/abs/2409.07964",
    "authors": [
      "Jingwen Tong",
      "Jiawei Shao",
      "Qiong Wu",
      "Wei Guo",
      "Zijian Li",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07972",
    "title": "Deep Height Decoupling for Precise Vision-based 3D Occupancy Prediction",
    "abstract": "           The task of vision-based 3D occupancy prediction aims to reconstruct 3D geometry and estimate its semantic classes from 2D color images, where the 2D-to-3D view transformation is an indispensable step. Most previous methods conduct forward projection, such as BEVPooling and VoxelPooling, both of which map the 2D image features into 3D grids. However, the current grid representing features within a certain height range usually introduces many confusing features that belong to other height ranges. To address this challenge, we present Deep Height Decoupling (DHD), a novel framework that incorporates explicit height prior to filter out the confusing features. Specifically, DHD first predicts height maps via explicit supervision. Based on the height distribution statistics, DHD designs Mask Guided Height Sampling (MGHS) to adaptively decoupled the height map into multiple binary masks. MGHS projects the 2D image features into multiple subspaces, where each grid contains features within reasonable height ranges. Finally, a Synergistic Feature Aggregation (SFA) module is deployed to enhance the feature representation through channel and spatial affinities, enabling further occupancy refinement. On the popular Occ3D-nuScenes benchmark, our method achieves state-of-the-art performance even with minimal input frames. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.07972",
    "authors": [
      "Yuan Wu",
      "Zhiqiang Yan",
      "Zhengxue Wang",
      "Xiang Li",
      "Le Hui",
      "Jian Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.07973",
    "title": "Sparse R-CNN OBB: Ship Target Detection in SAR Images Based on Oriented Sparse Proposals",
    "abstract": "           We present Sparse R-CNN OBB, a novel framework for the detection of oriented objects in SAR images leveraging sparse learnable proposals. The Sparse R-CNN OBB has streamlined architecture and ease of training as it utilizes a sparse set of 300 proposals instead of training a proposals generator on hundreds of thousands of anchors. To the best of our knowledge, Sparse R-CNN OBB is the first to adopt the concept of sparse learnable proposals for the detection of oriented objects, as well as for the detection of ships in Synthetic Aperture Radar (SAR) images. The detection head of the baseline model, Sparse R-CNN, is re-designed to enable the model to capture object orientation. We also fine-tune the model on RSDD-SAR dataset and provide a performance comparison to state-of-the-art models. Experimental results shows that Sparse R-CNN OBB achieves outstanding performance, surpassing other models on both inshore and offshore scenarios. The code is available at: this http URL.         ",
    "url": "https://arxiv.org/abs/2409.07973",
    "authors": [
      "Kamirul Kamirul",
      "Odysseas Pappas",
      "Alin Achim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.07984",
    "title": "SPARK: Self-supervised Personalized Real-time Monocular Face Capture",
    "abstract": "           Feedforward monocular face capture methods seek to reconstruct posed faces from a single image of a person. Current state of the art approaches have the ability to regress parametric 3D face models in real-time across a wide range of identities, lighting conditions and poses by leveraging large image datasets of human faces. These methods however suffer from clear limitations in that the underlying parametric face model only provides a coarse estimation of the face shape, thereby limiting their practical applicability in tasks that require precise 3D reconstruction (aging, face swapping, digital make-up, ...). In this paper, we propose a method for high-precision 3D face capture taking advantage of a collection of unconstrained videos of a subject as prior information. Our proposal builds on a two stage approach. We start with the reconstruction of a detailed 3D face avatar of the person, capturing both precise geometry and appearance from a collection of videos. We then use the encoder from a pre-trained monocular face reconstruction method, substituting its decoder with our personalized model, and proceed with transfer learning on the video collection. Using our pre-estimated image formation model, we obtain a more precise self-supervision objective, enabling improved expression and pose alignment. This results in a trained encoder capable of efficiently regressing pose and expression parameters in real-time from previously unseen images, which combined with our personalized geometry model yields more accurate and high fidelity mesh inference. Through extensive qualitative and quantitative evaluation, we showcase the superiority of our final model as compared to state-of-the-art baselines, and demonstrate its generalization ability to unseen pose, expression and lighting.         ",
    "url": "https://arxiv.org/abs/2409.07984",
    "authors": [
      "Kelian Baert",
      "Shrisha Bharadwaj",
      "Fabien Castan",
      "Benoit Maujean",
      "Marc Christie",
      "Victoria Abrevaya",
      "Adnane Boukhayma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07989",
    "title": "Enhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms",
    "abstract": "           In the context of few-shot classification, the goal is to train a classifier using a limited number of samples while maintaining satisfactory performance. However, traditional metric-based methods exhibit certain limitations in achieving this objective. These methods typically rely on a single distance value between the query feature and support feature, thereby overlooking the contribution of shallow features. To overcome this challenge, we propose a novel approach in this paper. Our approach involves utilizing multi-output embedding network that maps samples into distinct feature spaces. The proposed method extract feature vectors at different stages, enabling the model to capture both global and abstract features. By utilizing these diverse feature spaces, our model enhances its performance. Moreover, employing a self-attention mechanism improves the refinement of features at each stage, leading to even more robust representations and improved overall performance. Furthermore, assigning learnable weights to each stage significantly improved performance and results. We conducted comprehensive evaluations on the MiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way 5-shot scenarios. Additionally, we performed a cross-domain task from MiniImageNet to the CUB dataset, achieving high accuracy in the testing domain. These evaluations demonstrate the efficacy of our proposed method in comparison to state-of-the-art approaches. this https URL ",
    "url": "https://arxiv.org/abs/2409.07989",
    "authors": [
      "Fatemeh Askari",
      "Amirreza Fateh",
      "Mohammad Reza Mohammadi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.07997",
    "title": "Privacy-preserving federated prediction of pain intensity change based on multi-center survey data",
    "abstract": "           Background: Patient-reported survey data are used to train prognostic models aimed at improving healthcare. However, such data are typically available multi-centric and, for privacy reasons, cannot easily be centralized in one data repository. Models trained locally are less accurate, robust, and generalizable. We present and apply privacy-preserving federated machine learning techniques for prognostic model building, where local survey data never leaves the legally safe harbors of the medical centers. Methods: We used centralized, local, and federated learning techniques on two healthcare datasets (GLA:D data from the five health regions of Denmark and international SHARE data of 27 countries) to predict two different health outcomes. We compared linear regression, random forest regression, and random forest classification models trained on local data with those trained on the entire data in a centralized and in a federated fashion. Results: In GLA:D data, federated linear regression (R2 0.34, RMSE 18.2) and federated random forest regression (R2 0.34, RMSE 18.3) models outperform their local counterparts (i.e., R2 0.32, RMSE 18.6, R2 0.30, RMSE 18.8) with statistical significance. We also found that centralized models (R2 0.34, RMSE 18.2, R2 0.32, RMSE 18.5, respectively) did not perform significantly better than the federated models. In SHARE, the federated model (AC 0.78, AUROC: 0.71) and centralized model (AC 0.84, AUROC: 0.66) perform significantly better than the local models (AC: 0.74, AUROC: 0.69). Conclusion: Federated learning enables the training of prognostic models from multi-center surveys without compromising privacy and with only minimal or no compromise regarding model performance.         ",
    "url": "https://arxiv.org/abs/2409.07997",
    "authors": [
      "Supratim Das",
      "Mahdie Rafie",
      "Paula Kammer",
      "S\u00f8ren T. Skou",
      "Dorte T. Gr\u00f8nne",
      "Ewa M. Roos",
      "Andr\u00e9 Hajek",
      "Hans-Helmut K\u00f6nig",
      "Md Shihab Ullaha",
      "Niklas Probul",
      "Jan Baumbacha",
      "Linda Baumbach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.08010",
    "title": "Multiplex Graph Contrastive Learning with Soft Negatives",
    "abstract": "           Graph Contrastive Learning (GCL) seeks to learn nodal or graph representations that contain maximal consistent information from graph-structured data. While node-level contrasting modes are dominating, some efforts commence to explore consistency across different scales. Yet, they tend to lose consistent information and be contaminated by disturbing features. Here, we introduce MUX-GCL, a novel cross-scale contrastive learning paradigm that utilizes multiplex representations as effective patches. While this learning mode minimizes contaminating noises, a commensurate contrasting strategy using positional affinities further avoids information loss by correcting false negative pairs across scales. Extensive downstream experiments demonstrate that MUX-GCL yields multiple state-of-the-art results on public datasets. Our theoretical analysis further guarantees the new objective function as a stricter lower bound of mutual information of raw input features and output embeddings, which rationalizes this paradigm. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.08010",
    "authors": [
      "Zhenhao Zhao",
      "Minhong Zhu",
      "Chen Wang",
      "Sijia Wang",
      "Jiqiang Zhang",
      "Li Chen",
      "Weiran Cai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.08020",
    "title": "Network Anomaly Traffic Detection via Multi-view Feature Fusion",
    "abstract": "           Traditional anomalous traffic detection methods are based on single-view analysis, which has obvious limitations in dealing with complex attacks and encrypted communications. In this regard, we propose a Multi-view Feature Fusion (MuFF) method for network anomaly traffic detection. MuFF models the temporal and interactive relationships of packets in network traffic based on the temporal and interactive viewpoints respectively. It learns temporal and interactive features. These features are then fused from different perspectives for anomaly traffic detection. Extensive experiments on six real traffic datasets show that MuFF has excellent performance in network anomalous traffic detection, which makes up for the shortcomings of detection under a single perspective.         ",
    "url": "https://arxiv.org/abs/2409.08020",
    "authors": [
      "Song Hao",
      "Wentao Fu",
      "Xuanze Chen",
      "Chengxiang Jin",
      "Jiajun Zhou",
      "Shanqing Yu",
      "Qi Xuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.08023",
    "title": "Edge-Wise Graph-Instructed Neural Networks",
    "abstract": "           The problem of multi-task regression over graph nodes has been recently approached through Graph-Instructed Neural Network (GINN), which is a promising architecture belonging to the subset of message-passing graph neural networks. In this work, we discuss the limitations of the Graph-Instructed (GI) layer, and we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages of the EWGI layer and we provide numerical evidence that EWGINNs perform better than GINNs over graph-structured input data with chaotic connectivity, like the ones inferred from the Erdos-R\u00e9nyi graph.         ",
    "url": "https://arxiv.org/abs/2409.08023",
    "authors": [
      "Francesco Della Santa",
      "Antonio Mastropietro",
      "Sandra Pieraccini",
      "Francesco Vaccarino"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2409.08036",
    "title": "Heterogeneous Sheaf Neural Networks",
    "abstract": "           Heterogeneous graphs, with nodes and edges of different types, are commonly used to model relational structures in many real-world applications. Standard Graph Neural Networks (GNNs) struggle to process heterogeneous data due to oversmoothing. Instead, current approaches have focused on accounting for the heterogeneity in the model architecture, leading to increasingly complex models. Inspired by recent work, we propose using cellular sheaves to model the heterogeneity in the graph's underlying topology. Instead of modelling the data as a graph, we represent it as cellular sheaves, which allows us to encode the different data types directly in the data structure, eliminating the need to inject them into the architecture. We introduce HetSheaf, a general framework for heterogeneous sheaf neural networks, and a series of heterogeneous sheaf predictors to better encode the data's heterogeneity into the sheaf structure. Finally, we empirically evaluate HetSheaf on several standard heterogeneous graph benchmarks, achieving competitive results whilst being more parameter-efficient.         ",
    "url": "https://arxiv.org/abs/2409.08036",
    "authors": [
      "Luke Braithwaite",
      "Iulia Duta",
      "Pietro Li\u00f2"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.08037",
    "title": "Fine-Grained Complexity of Multiple Domination and Dominating Patterns in Sparse Graphs",
    "abstract": "           The study of domination in graphs has led to a variety of domination problems studied in the literature. Most of these follow the following general framework: Given a graph $G$ and an integer $k$, decide if there is a set $S$ of $k$ vertices such that (1) some inner property $\\phi(S)$ (e.g., connectedness) is satisfied, and (2) each vertex $v$ satisfies some domination property $\\rho(S, v)$ (e.g., there is an $s\\in S$ that is adjacent to $v$). Since many real-world graphs are sparse, we seek to determine the optimal running time of such problems in both the number $n$ of vertices and the number $m$ of edges in $G$. While the classic dominating set problem admits a rather limited improvement in sparse graphs (Fischer, K\u00fcnnemann, Redzic SODA'24), we show that natural variants studied in the literature admit much larger speed-ups, with a diverse set of possible running times. Specifically, we obtain conditionally optimal algorithms for: 1) $r$-Multiple $k$-Dominating Set (each vertex must be adjacent to at least $r$ vertices in $S$): If $r\\le k-2$, we obtain a running time of $(m/n)^{r} n^{k-r+o(1)}$ that is conditionally optimal assuming the 3-uniform hyperclique hypothesis. In sparse graphs, this fully interpolates between $n^{k-1\\pm o(1)}$ and $n^{2\\pm o(1)}$, depending on $r$. Curiously, when $r=k-1$, we obtain a randomized algorithm beating $(m/n)^{k-1} n^{1+o(1)}$ and we show that this algorithm is close to optimal under the $k$-clique hypothesis. 2) $H$-Dominating Set ($S$ must induce a pattern $H$). We conditionally settle the complexity of three such problems: (a) Dominating Clique ($H$ is a $k$-clique), (b) Maximal Independent Set of size $k$ ($H$ is an independent set on $k$ vertices), (c) Dominating Induced Matching ($H$ is a perfect matching on $k$ vertices).         ",
    "url": "https://arxiv.org/abs/2409.08037",
    "authors": [
      "Marvin K\u00fcnnemann",
      "Mirza Redzic"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2409.08045",
    "title": "Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking",
    "abstract": "           In this paper, we show that with the ability to jailbreak a GenAI model, attackers can escalate the outcome of attacks against RAG-based GenAI-powered applications in severity and scale. In the first part of the paper, we show that attackers can escalate RAG membership inference attacks and RAG entity extraction attacks to RAG documents extraction attacks, forcing a more severe outcome compared to existing attacks. We evaluate the results obtained from three extraction methods, the influence of the type and the size of five embeddings algorithms employed, the size of the provided context, and the GenAI engine. We show that attackers can extract 80%-99.8% of the data stored in the database used by the RAG of a Q&A chatbot. In the second part of the paper, we show that attackers can escalate the scale of RAG data poisoning attacks from compromising a single GenAI-powered application to compromising the entire GenAI ecosystem, forcing a greater scale of damage. This is done by crafting an adversarial self-replicating prompt that triggers a chain reaction of a computer worm within the ecosystem and forces each affected application to perform a malicious activity and compromise the RAG of additional applications. We evaluate the performance of the worm in creating a chain of confidential data extraction about users within a GenAI ecosystem of GenAI-powered email assistants and analyze how the performance of the worm is affected by the size of the context, the adversarial self-replicating prompt used, the type and size of the embeddings algorithm employed, and the number of hops in the propagation. Finally, we review and analyze guardrails to protect RAG-based inference and discuss the tradeoffs.         ",
    "url": "https://arxiv.org/abs/2409.08045",
    "authors": [
      "Stav Cohen",
      "Ron Bitton",
      "Ben Nassi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.08056",
    "title": "Expansive Supervision for Neural Radiance Field",
    "abstract": "           Neural Radiance Fields have achieved success in creating powerful 3D media representations with their exceptional reconstruction capabilities. However, the computational demands of volume rendering pose significant challenges during model training. Existing acceleration techniques often involve redesigning the model architecture, leading to limitations in compatibility across different frameworks. Furthermore, these methods tend to overlook the substantial memory costs incurred. In response to these challenges, we introduce an expansive supervision mechanism that efficiently balances computational load, rendering quality and flexibility for neural radiance field training. This mechanism operates by selectively rendering a small but crucial subset of pixels and expanding their values to estimate the error across the entire area for each iteration. Compare to conventional supervision, our method effectively bypasses redundant rendering processes, resulting in notable reductions in both time and memory consumption. Experimental results demonstrate that integrating expansive supervision within existing state-of-the-art acceleration frameworks can achieve 69% memory savings and 42% time savings, with negligible compromise in visual quality.         ",
    "url": "https://arxiv.org/abs/2409.08056",
    "authors": [
      "Weixiang Zhang",
      "Shuzhao Xie",
      "Shijia Ge",
      "Wei Yao",
      "Chen Tang",
      "Zhi Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.08066",
    "title": "Self-Supervised Learning of Iterative Solvers for Constrained Optimization",
    "abstract": "           Obtaining the solution of constrained optimization problems as a function of parameters is very important in a multitude of applications, such as control and planning. Solving such parametric optimization problems in real time can present significant challenges, particularly when it is necessary to obtain highly accurate solutions or batches of solutions. To solve these challenges, we propose a learning-based iterative solver for constrained optimization which can obtain very fast and accurate solutions by customizing the solver to a specific parametric optimization problem. For a given set of parameters of the constrained optimization problem, we propose a first step with a neural network predictor that outputs primal-dual solutions of a reasonable degree of accuracy. This primal-dual solution is then improved to a very high degree of accuracy in a second step by a learned iterative solver in the form of a neural network. A novel loss function based on the Karush-Kuhn-Tucker conditions of optimality is introduced, enabling fully self-supervised training of both neural networks without the necessity of prior sampling of optimizer solutions. The evaluation of a variety of quadratic and nonlinear parametric test problems demonstrates that the predictor alone is already competitive with recent self-supervised schemes for approximating optimal solutions. The second step of our proposed learning-based iterative constrained optimizer achieves solutions with orders of magnitude better accuracy than other learning-based approaches, while being faster to evaluate than state-of-the-art solvers and natively allowing for GPU parallelization.         ",
    "url": "https://arxiv.org/abs/2409.08066",
    "authors": [
      "Lukas L\u00fcken",
      "Sergio Lucia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2409.08075",
    "title": "Computational Algorithms for the Product Form Solution of Closed Queuing Networks with Finite Buffers and Skip-Over Policy",
    "abstract": "           Closed queuing networks with finite capacity buffers and skip-over policies are fundamental models in the performance evaluation of computer and communication systems. This technical report presents the details of computational algorithms to derive the key performance metrics for such networks. The primary focus is on the efficient computation of the normalization constant, which is critical for determining the steady-state probabilities of the network states under investigation. A convolution algorithm is proposed, which paves the way for the computation of key performance indices, such as queue length distribution and throughput, accommodating the intricacies introduced by finite capacity constraints and skip-over mechanisms. Finally, an extension of the traditional Mean Value Analysis algorithm addressing numerical stability is provided. The approaches discussed here allow make the investigation of large-scale networks feasible and enable the development of robust implementations of these techniques for practical use.         ",
    "url": "https://arxiv.org/abs/2409.08075",
    "authors": [
      "Gianfranco Balbo",
      "Andrea Marin",
      "Diletta Olliaro",
      "Matteo Sereno"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2409.08087",
    "title": "Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks",
    "abstract": "           Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.         ",
    "url": "https://arxiv.org/abs/2409.08087",
    "authors": [
      "Benji Peng",
      "Keyu Chen",
      "Ming Li",
      "Pohsun Feng",
      "Ziqian Bi",
      "Junyu Liu",
      "Qian Niu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.08098",
    "title": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal",
    "abstract": "           This paper explores the intersection of technological innovation and access to justice by developing a benchmark for predicting case outcomes in the UK Employment Tribunal (UKET). To address the challenge of extensive manual annotation, the study employs a large language model (LLM) for automatic annotation, resulting in the creation of the CLC-UKET dataset. The dataset consists of approximately 19,000 UKET cases and their metadata. Comprehensive legal annotations cover facts, claims, precedent references, statutory references, case outcomes, reasons and jurisdiction codes. Facilitated by the CLC-UKET data, we examine a multi-class case outcome prediction task in the UKET. Human predictions are collected to establish a performance reference for model comparison. Empirical results from baseline models indicate that finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET prediction task. The performance of zero-shot LLMs can be enhanced by integrating task-related information into few-shot examples. We hope that the CLC-UKET dataset, along with human annotations and empirical findings, can serve as a valuable benchmark for employment-related dispute resolution.         ",
    "url": "https://arxiv.org/abs/2409.08098",
    "authors": [
      "Huiyuan Xie",
      "Felix Steffek",
      "Joana Ribeiro de Faria",
      "Christine Carter",
      "Jonathan Rutherford"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.08106",
    "title": "Hypergraph Change Point Detection using Adapted Cardinality-Based Gadgets: Applications in Dynamic Legal Structures",
    "abstract": "           Hypergraphs provide a robust framework for modeling complex systems with higher-order interactions. However, analyzing them in dynamic settings presents significant computational challenges. To address this, we introduce a novel method that adapts the cardinality-based gadget to convert hypergraphs into strongly connected weighted directed graphs, complemented by a symmetrized combinatorial Laplacian. We demonstrate that the harmonic mean of the conductance and edge expansion of the original hypergraph can be upper-bounded by the conductance of the transformed directed graph, effectively preserving crucial cut information. Additionally, we analyze how the resulting Laplacian relates to that derived from the star expansion. Our approach was validated through change point detection experiments on both synthetic and real datasets, showing superior performance over clique and star expansions in maintaining spectral information in dynamic settings. Finally, we applied our method to analyze a dynamic legal hypergraph constructed from extensive United States court opinion data.         ",
    "url": "https://arxiv.org/abs/2409.08106",
    "authors": [
      "Hiroki Matsumoto",
      "Takahiro Yoshida",
      "Ryoma Kondo",
      "Ryohei Hisano"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2409.08111",
    "title": "Towards a graph-based foundation model for network traffic analysis",
    "abstract": "           Foundation models have shown great promise in various fields of study. A potential application of such models is in computer network traffic analysis, where these models can grasp the complexities of network traffic dynamics and adapt to any specific task or network environment with minimal fine-tuning. Previous approaches have used tokenized hex-level packet data and the model architecture of large language transformer models. We propose a new, efficient graph-based alternative at the flow-level. Our approach represents network traffic as a dynamic spatio-temporal graph, employing a self-supervised link prediction pretraining task to capture the spatial and temporal dynamics in this network graph framework. To evaluate the effectiveness of our approach, we conduct a few-shot learning experiment for three distinct downstream network tasks: intrusion detection, traffic classification, and botnet classification. Models finetuned from our pretrained base achieve an average performance increase of 6.87\\% over training from scratch, demonstrating their ability to effectively learn general network traffic dynamics during pretraining. This success suggests the potential for a large-scale version to serve as an operational foundational model.         ",
    "url": "https://arxiv.org/abs/2409.08111",
    "authors": [
      "Louis Van Langendonck",
      "Ismael Castell-Uroz",
      "Pere Barlet-Ros"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2409.08115",
    "title": "Anonymized Network Sensing Graph Challenge",
    "abstract": "           The MIT/IEEE/Amazon GraphChallenge encourages community approaches to developing new solutions for analyzing graphs and sparse data derived from social media, sensor feeds, and scientific data to discover relationships between events as they unfold in the field. The anonymized network sensing Graph Challenge seeks to enable large, open, community-based approaches to protecting networks. Many large-scale networking problems can only be solved with community access to very broad data sets with the highest regard for privacy and strong community buy-in. Such approaches often require community-based data sharing. In the broader networking community (commercial, federal, and academia) anonymized source-to-destination traffic matrices with standard data sharing agreements have emerged as a data product that can meet many of these requirements. This challenge provides an opportunity to highlight novel approaches for optimizing the construction and analysis of anonymized traffic matrices using over 100 billion network packets derived from the largest Internet telescope in the world (CAIDA). This challenge specifies the anonymization, construction, and analysis of these traffic matrices. A GraphBLAS reference implementation is provided, but the use of GraphBLAS is not required in this Graph Challenge. As with prior Graph Challenges the goal is to provide a well-defined context for demonstrating innovation. Graph Challenge participants are free to select (with accompanying explanation) the Graph Challenge elements that are appropriate for highlighting their innovations.         ",
    "url": "https://arxiv.org/abs/2409.08115",
    "authors": [
      "Hayden Jananthan",
      "Michael Jones",
      "William Arcand",
      "David Bestor",
      "William Bergeron",
      "Daniel Burrill",
      "Aydin Buluc",
      "Chansup Byun",
      "Timothy Davis",
      "Vijay Gadepally",
      "Daniel Grant",
      "Michael Houle",
      "Matthew Hubbell",
      "Piotr Luszczek",
      "Peter Michaleas",
      "Lauren Milechin",
      "Chasen Milner",
      "Guillermo Morales",
      "Andrew Morris",
      "Julie Mullen",
      "Ritesh Patel",
      "Alex Pentland",
      "Sandeep Pisharody",
      "Andrew Prout",
      "Albert Reuther",
      "Antonio Rosa",
      "Gabriel Wachman",
      "Charles Yee",
      "Jeremy Kepner"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Discrete Mathematics (cs.DM)",
      "Performance (cs.PF)",
      "Software Engineering (cs.SE)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2409.08122",
    "title": "GAZEploit: Remote Keystroke Inference Attack by Gaze Estimation from Avatar Views in VR/MR Devices",
    "abstract": "           The advent and growing popularity of Virtual Reality (VR) and Mixed Reality (MR) solutions have revolutionized the way we interact with digital platforms. The cutting-edge gaze-controlled typing methods, now prevalent in high-end models of these devices, e.g., Apple Vision Pro, have not only improved user experience but also mitigated traditional keystroke inference attacks that relied on hand gestures, head movements and acoustic side-channels. However, this advancement has paradoxically given birth to a new, potentially more insidious cyber threat, GAZEploit. In this paper, we unveil GAZEploit, a novel eye-tracking based attack specifically designed to exploit these eye-tracking information by leveraging the common use of virtual appearances in VR applications. This widespread usage significantly enhances the practicality and feasibility of our attack compared to existing methods. GAZEploit takes advantage of this vulnerability to remotely extract gaze estimations and steal sensitive keystroke information across various typing scenarios-including messages, passwords, URLs, emails, and passcodes. Our research, involving 30 participants, achieved over 80% accuracy in keystroke inference. Alarmingly, our study also identified over 15 top-rated apps in the Apple Store as vulnerable to the GAZEploit attack, emphasizing the urgent need for bolstered security measures for this state-of-the-art VR/MR text entry method.         ",
    "url": "https://arxiv.org/abs/2409.08122",
    "authors": [
      "Hanqiu Wang",
      "Zihao Zhan",
      "Haoqi Shan",
      "Siqi Dai",
      "Max Panoff",
      "Shuo Wang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.08160",
    "title": "On the Role of Context in Reading Time Prediction",
    "abstract": "           We present a new perspective on how readers integrate context during real-time language comprehension. Our proposals build on surprisal theory, which posits that the processing effort of a linguistic unit (e.g., a word) is an affine function of its in-context information content. We first observe that surprisal is only one out of many potential ways that a contextual predictor can be derived from a language model. Another one is the pointwise mutual information (PMI) between a unit and its context, which turns out to yield the same predictive power as surprisal when controlling for unigram frequency. Moreover, both PMI and surprisal are correlated with frequency. This means that neither PMI nor surprisal contains information about context alone. In response to this, we propose a technique where we project surprisal onto the orthogonal complement of frequency, yielding a new contextual predictor that is uncorrelated with frequency. Our experiments show that the proportion of variance in reading times explained by context is a lot smaller when context is represented by the orthogonalized predictor. From an interpretability standpoint, this indicates that previous studies may have overstated the role that context has in predicting reading times.         ",
    "url": "https://arxiv.org/abs/2409.08160",
    "authors": [
      "Andreas Opedal",
      "Eleanor Chodroff",
      "Ryan Cotterell",
      "Ethan Gotlieb Wilcox"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.08167",
    "title": "High-Frequency Anti-DreamBooth: Robust Defense Against Image Synthesis",
    "abstract": "           Recently, text-to-image generative models have been misused to create unauthorized malicious images of individuals, posing a growing social problem. Previous solutions, such as Anti-DreamBooth, add adversarial noise to images to protect them from being used as training data for malicious generation. However, we found that the adversarial noise can be removed by adversarial purification methods such as DiffPure. Therefore, we propose a new adversarial attack method that adds strong perturbation on the high-frequency areas of images to make it more robust to adversarial purification. Our experiment showed that the adversarial images retained noise even after adversarial purification, hindering malicious image generation.         ",
    "url": "https://arxiv.org/abs/2409.08167",
    "authors": [
      "Takuto Onikubo",
      "Yusuke Matsui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.08211",
    "title": "Graph Laplacian-based Bayesian Multi-fidelity Modeling",
    "abstract": "           We present a novel probabilistic approach for generating multi-fidelity data while accounting for errors inherent in both low- and high-fidelity data. In this approach a graph Laplacian constructed from the low-fidelity data is used to define a multivariate Gaussian prior density for the coordinates of the true data points. In addition, few high-fidelity data points are used to construct a conjugate likelihood term. Thereafter, Bayes rule is applied to derive an explicit expression for the posterior density which is also multivariate Gaussian. The maximum \\textit{a posteriori} (MAP) estimate of this density is selected to be the optimal multi-fidelity estimate. It is shown that the MAP estimate and the covariance of the posterior density can be determined through the solution of linear systems of equations. Thereafter, two methods, one based on spectral truncation and another based on a low-rank approximation, are developed to solve these equations efficiently. The multi-fidelity approach is tested on a variety of problems in solid and fluid mechanics with data that represents vectors of quantities of interest and discretized spatial fields in one and two dimensions. The results demonstrate that by utilizing a small fraction of high-fidelity data, the multi-fidelity approach can significantly improve the accuracy of a large collection of low-fidelity data points.         ",
    "url": "https://arxiv.org/abs/2409.08211",
    "authors": [
      "Orazio Pinti",
      "Jeremy M. Budd",
      "Franca Hoffmann",
      "Assad A. Oberai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2409.08217",
    "title": "CliquePH: Higher-Order Information for Graph Neural Networks through Persistent Homology on Clique Graphs",
    "abstract": "           Graph neural networks have become the default choice by practitioners for graph learning tasks such as graph classification and node classification. Nevertheless, popular graph neural network models still struggle to capture higher-order information, i.e., information that goes \\emph{beyond} pairwise interactions. Recent work has shown that persistent homology, a tool from topological data analysis, can enrich graph neural networks with topological information that they otherwise could not capture. Calculating such features is efficient for dimension 0 (connected components) and dimension 1 (cycles). However, when it comes to higher-order structures, it does not scale well, with a complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the order of the structures. In this work, we introduce a novel method that extracts information about higher-order structures in the graph while still using the efficient low-dimensional persistent homology algorithm. On standard benchmark datasets, we show that our method can lead to up to $31\\%$ improvements in test accuracy.         ",
    "url": "https://arxiv.org/abs/2409.08217",
    "authors": [
      "Davide Buffelli",
      "Farzin Soleymani",
      "Bastian Rieck"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.08219",
    "title": "Graph Inspection for Robotic Motion Planning: Do Arithmetic Circuits Help?",
    "abstract": "           We investigate whether algorithms based on arithmetic circuits are a viable alternative to existing solvers for Graph Inspection, a problem with direct application in robotic motion planning. Specifically, we seek to address the high memory usage of existing solvers. Aided by novel theoretical results enabling fast solution recovery, we implement a circuit-based solver for Graph Inspection which uses only polynomial space and test it on several realistic robotic motion planning datasets. In particular, we provide a comprehensive experimental evaluation of a suite of engineered algorithms for three key subroutines. While this evaluation demonstrates that circuit-based methods are not yet practically competitive for our robotics application, it also provides insights which may guide future efforts to bring circuit-based algorithms from theory to practice.         ",
    "url": "https://arxiv.org/abs/2409.08219",
    "authors": [
      "Matthias Bentert",
      "Daniel Coimbra Salomao",
      "Alex Crane",
      "Yosuke Mizutani",
      "Felix Reidl",
      "Blair D. Sullivan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2409.08221",
    "title": "Tweezers: A Framework for Security Event Detection via Event Attribution-centric Tweet Embedding",
    "abstract": "           Twitter is recognized as a crucial platform for the dissemination and gathering of Cyber Threat Intelligence (CTI). Its capability to provide real-time, actionable intelligence makes it an indispensable tool for detecting security events, helping security professionals cope with ever-growing threats. However, the large volume of tweets and inherent noises of human-crafted tweets pose significant challenges in accurately identifying security events. While many studies tried to filter out event-related tweets based on keywords, they are not effective due to their limitation in understanding the semantics of tweets. Another challenge in security event detection from Twitter is the comprehensive coverage of security events. Previous studies emphasized the importance of early detection of security events, but they overlooked the importance of event coverage. To cope with these challenges, in our study, we introduce a novel event attribution-centric tweet embedding method to enable the high precision and coverage of events. Our experiment result shows that the proposed method outperforms existing text and graph-based tweet embedding methods in identifying security events. Leveraging this novel embedding approach, we have developed and implemented a framework, Tweezers, that is applicable to security event detection from Twitter for CTI gathering. This framework has demonstrated its effectiveness, detecting twice as many events compared to established baselines. Additionally, we have showcased two applications, built on Tweezers for the integration and inspection of security events, i.e., security event trend analysis and informative security user identification.         ",
    "url": "https://arxiv.org/abs/2409.08221",
    "authors": [
      "Jian Cui",
      "Hanna Kim",
      "Eugene Jang",
      "Dayeon Yim",
      "Kicheol Kim",
      "Yongjae Lee",
      "Jin-Woo Chung",
      "Seungwon Shin",
      "Xiaojing Liao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.08222",
    "title": "Multi-Robot Coordination Induced in Hazardous Environments through an Adversarial Graph-Traversal Game",
    "abstract": "           This paper presents a game theoretic formulation of a graph traversal problem, with applications to robots moving through hazardous environments in the presence of an adversary, as in military and security applications. The blue team of robots moves in an environment modeled by a time-varying graph, attempting to reach some goal with minimum cost, while the red team controls how the graph changes to maximize the cost. The problem is formulated as a stochastic game, so that Nash equilibrium strategies can be computed numerically. Bounds are provided for the game value, with a guarantee that it solves the original problem. Numerical simulations demonstrate the results and the effectiveness of this method, particularly showing the benefit of mixing actions for both players, as well as beneficial coordinated behavior, where blue robots split up and/or synchronize to traverse risky edges.         ",
    "url": "https://arxiv.org/abs/2409.08222",
    "authors": [
      "James Berneburg",
      "Xuan Wang",
      "Xuesu Xiao",
      "Daigo Shishika"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.08228",
    "title": "Improving Initial Transients of Online Learning Echo State Network Control System via Feedback Adjustment",
    "abstract": "           Echo state networks (ESNs) have gained popularity in online learning control systems due to their easy training. However, online learning ESN controllers often undergo slow convergence and produce unexpected outputs during the initial transient phase. Existing solutions, such as prior training or control mode switching, can be complex and have drawbacks. This work offers a simple yet effective method to address these initial transients by integrating a feedback proportional-differential (P-D) controller. Simulation results show that the proposed control system exhibits fast convergence and strong robustness against plant dynamics and hyperparameter changes. This work is expected to offer practical benefits for engineers seeking to implement online learning ESN control systems.         ",
    "url": "https://arxiv.org/abs/2409.08228",
    "authors": [
      "Junyi Shen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.08237",
    "title": "Multi-Model based Federated Learning Against Model Poisoning Attack: A Deep Learning Based Model Selection for MEC Systems",
    "abstract": "           Federated Learning (FL) enables training of a global model from distributed data, while preserving data privacy. However, the singular-model based operation of FL is open with uploading poisoned models compatible with the global model structure and can be exploited as a vulnerability to conduct model poisoning attacks. This paper proposes a multi-model based FL as a proactive mechanism to enhance the opportunity of model poisoning attack mitigation. A master model is trained by a set of slave models. To enhance the opportunity of attack mitigation, the structure of client models dynamically change within learning epochs, and the supporter FL protocol is provided. For a MEC system, the model selection problem is modeled as an optimization to minimize loss and recognition time, while meeting a robustness confidence. In adaption with dynamic network condition, a deep reinforcement learning based model selection is proposed. For a DDoS attack detection scenario, results illustrate a competitive accuracy gain under poisoning attack with the scenario that the system is without attack, and also a potential of recognition time improvement.         ",
    "url": "https://arxiv.org/abs/2409.08237",
    "authors": [
      "Somayeh Kianpisheh",
      "Chafika Benzaid",
      "Tarik Taleb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2409.08255",
    "title": "LoRID: Low-Rank Iterative Diffusion for Adversarial Purification",
    "abstract": "           This work presents an information-theoretic examination of diffusion-based purification methods, the state-of-the-art adversarial defenses that utilize diffusion models to remove malicious perturbations in adversarial examples. By theoretically characterizing the inherent purification errors associated with the Markov-based diffusion purifications, we introduce LoRID, a novel Low-Rank Iterative Diffusion purification method designed to remove adversarial perturbation with low intrinsic purification errors. LoRID centers around a multi-stage purification process that leverages multiple rounds of diffusion-denoising loops at the early time-steps of the diffusion models, and the integration of Tucker decomposition, an extension of matrix factorization, to remove adversarial noise at high-noise regimes. Consequently, LoRID increases the effective diffusion time-steps and overcomes strong adversarial attacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ, and ImageNet datasets under both white-box and black-box settings.         ",
    "url": "https://arxiv.org/abs/2409.08255",
    "authors": [
      "Geigh Zollicoffer",
      "Minh Vu",
      "Ben Nebgen",
      "Juan Castorena",
      "Boian Alexandrov",
      "Manish Bhattarai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.07457",
    "title": "LSST: Learned Single-Shot Trajectory and Reconstruction Network for MR Imaging",
    "abstract": "           Single-shot magnetic resonance (MR) imaging acquires the entire k-space data in a single shot and it has various applications in whole-body imaging. However, the long acquisition time for the entire k-space in single-shot fast spin echo (SSFSE) MR imaging poses a challenge, as it introduces T2-blur in the acquired images. This study aims to enhance the reconstruction quality of SSFSE MR images by (a) optimizing the trajectory for measuring the k-space, (b) acquiring fewer samples to speed up the acquisition process, and (c) reducing the impact of T2-blur. The proposed method adheres to physics constraints due to maximum gradient strength and slew-rate available while optimizing the trajectory within an end-to-end learning framework. Experiments were conducted on publicly available fastMRI multichannel dataset with 8-fold and 16-fold acceleration factors. An experienced radiologist's evaluation on a five-point Likert scale indicates improvements in the reconstruction quality as the ACL fibers are sharper than comparative methods.         ",
    "url": "https://arxiv.org/abs/2409.07457",
    "authors": [
      "Hemant Kumar Aggarwal",
      "Sudhanya Chatterjee",
      "Dattesh Shanbhag",
      "Uday Patil",
      "K.V.S. Hari"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.07468",
    "title": "Machine Learning of Nonlinear Dynamical Systems with Control Parameters Using Feedforward Neural Networks",
    "abstract": "           Several authors have reported that the echo state network reproduces bifurcation diagrams of some nonlinear differential equations using the data for a few control parameters. We demonstrate that a simpler feedforward neural network can also reproduce the bifurcation diagram of the logistics map and synchronization transition in globally coupled Stuart-Landau equations.         ",
    "url": "https://arxiv.org/abs/2409.07468",
    "authors": [
      "Hidetsugu Sakaguchi"
    ],
    "subjectives": [
      "Chaotic Dynamics (nlin.CD)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ]
  },
  {
    "id": "arXiv:2409.07480",
    "title": "EEG-Language Modeling for Pathology Detection",
    "abstract": "           Multimodal language modeling constitutes a recent breakthrough which leverages advances in large language models to pretrain capable multimodal models. The integration of natural language during pretraining has been shown to significantly improve learned representations, particularly in computer vision. However, the efficacy of multimodal language modeling in the realm of functional brain data, specifically for advancing pathology detection, remains unexplored. This study pioneers EEG-language models trained on clinical reports and 15000 EEGs. We extend methods for multimodal alignment to this novel domain and investigate which textual information in reports is useful for training EEG-language models. Our results indicate that models learn richer representations from being exposed to a variety of report segments, including the patient's clinical history, description of the EEG, and the physician's interpretation. Compared to models exposed to narrower clinical text information, we find such models to retrieve EEGs based on clinical reports (and vice versa) with substantially higher accuracy. Yet, this is only observed when using a contrastive learning approach. Particularly in regimes with few annotations, we observe that representations of EEG-language models can significantly improve pathology detection compared to those of EEG-only models, as demonstrated by both zero-shot classification and linear probes. In sum, these results highlight the potential of integrating brain activity data with clinical text, suggesting that EEG-language models represent significant progress for clinical applications.         ",
    "url": "https://arxiv.org/abs/2409.07480",
    "authors": [
      "Sam Gijsen",
      "Kerstin Ritter"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07485",
    "title": "Optimization and Deployment of Deep Neural Networks for PPG-based Blood Pressure Estimation Targeting Low-power Wearables",
    "abstract": "           PPG-based Blood Pressure (BP) estimation is a challenging biosignal processing task for low-power devices such as wearables. State-of-the-art Deep Neural Networks (DNNs) trained for this task implement either a PPG-to-BP signal-to-signal reconstruction or a scalar BP value regression and have been shown to outperform classic methods on the largest and most complex public datasets. However, these models often require excessive parameter storage or computational effort for wearable deployment, exceeding the available memory or incurring too high latency and energy consumption. In this work, we describe a fully-automated DNN design pipeline, encompassing HW-aware Neural Architecture Search (NAS) and Quantization, thanks to which we derive accurate yet lightweight models, that can be deployed on an ultra-low-power multicore System-on-Chip (SoC), GAP8. Starting from both regression and signal-to-signal state-of-the-art models on four public datasets, we obtain optimized versions that achieve up to 4.99% lower error or 73.36% lower size at iso-error. Noteworthy, while the most accurate SoA network on the largest dataset can not fit the GAP8 memory, all our optimized models can; our most accurate DNN consumes as little as 0.37 mJ while reaching the lowest MAE of 8.08 on Diastolic BP estimation.         ",
    "url": "https://arxiv.org/abs/2409.07485",
    "authors": [
      "Alessio Burrello",
      "Francesco Carlucci",
      "Giovanni Pollo",
      "Xiaying Wang",
      "Massimo Poncino",
      "Enrico Macii",
      "Luca Benini",
      "Daniele Jahier Pagliari"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07493",
    "title": "Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review",
    "abstract": "           The Complex Emotion Recognition System (CERS) deciphers complex emotional states by examining combinations of basic emotions expressed, their interconnections, and the dynamic variations. Through the utilization of advanced algorithms, CERS provides profound insights into emotional dynamics, facilitating a nuanced understanding and customized responses. The attainment of such a level of emotional recognition in machines necessitates the knowledge distillation and the comprehension of novel concepts akin to human cognition. The development of AI systems for discerning complex emotions poses a substantial challenge with significant implications for affective computing. Furthermore, obtaining a sizable dataset for CERS proves to be a daunting task due to the intricacies involved in capturing subtle emotions, necessitating specialized methods for data collection and processing. Incorporating physiological signals such as Electrocardiogram (ECG) and Electroencephalogram (EEG) can notably enhance CERS by furnishing valuable insights into the user's emotional state, enhancing the quality of datasets, and fortifying system dependability. A comprehensive literature review was conducted in this study to assess the efficacy of machine learning, deep learning, and meta-learning approaches in both basic and complex emotion recognition utilizing EEG, ECG signals, and facial expression datasets. The chosen research papers offer perspectives on potential applications, clinical implications, and results of CERSs, with the objective of promoting their acceptance and integration into clinical decision-making processes. This study highlights research gaps and challenges in understanding CERSs, encouraging further investigation by relevant studies and organizations. Lastly, the significance of meta-learning approaches in improving CERS performance and guiding future research endeavors is underscored.         ",
    "url": "https://arxiv.org/abs/2409.07493",
    "authors": [
      "Javad Hassannataj Joloudari",
      "Mohammad Maftoun",
      "Bahareh Nakisa",
      "Roohallah Alizadehsani",
      "Meisam Yadollahzadeh-Tabari"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07498",
    "title": "Structural Robustness and Vulnerability of Networks",
    "abstract": "           Networks are useful descriptions of the structure of many complex systems. Unsurprisingly, it is thus important to analyze the robustness of networks in many scientific disciplines. In applications in communication, logistics, finance, ecology, biomedicine, and many other fields, researchers have studied the robustness of networks to the removal of nodes, edges, or other subnetworks to identify and characterize robust network structures. A major challenge in the study of network robustness is that researchers have reported that different and seemingly contradictory network properties are correlated with a network's robustness. Using a framework by Alderson and Doyle~\\cite{Alderson2010}, we categorize several notions of network robustness and we examine these ostensible contradictions. We survey studies of network robustness with a focus on (1)~identifying robustness specifications in common use, (2)~understanding when these specifications are appropriate, and (3)~understanding the conditions under which one can expect different notions of robustness to yield similar results. With this review, we aim to give researchers an overview of the large, interdisciplinary body of work on network robustness and develop practical guidance for the design of computational experiments to study a network's robustness.         ",
    "url": "https://arxiv.org/abs/2409.07498",
    "authors": [
      "Alice C. Schwarze",
      "Jessica Jiang",
      "Jonny Wray",
      "Mason A. Porter"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2409.07523",
    "title": "Using Neural Network Models to Estimate Stellar Ages from Lithium Equivalent Widths: An EAGLES Expansion",
    "abstract": "           We present an Artificial Neural Network (ANN) model of photospheric lithium depletion in cool stars (3000 < Teff / K < 6500), producing estimates and probability distributions of age from Li I 6708A equivalent width (LiEW) and effective temperature data inputs. The model is trained on the same sample of 6200 stars from 52 open clusters, observed in the Gaia-ESO spectroscopic survey, and used to calibrate the previously published analytical EAGLES model, with ages 2 - 6000 Myr and -0.3 < [Fe/H] < 0.2. The additional flexibility of the ANN provides some improvements, including better modelling of the \"lithium dip\" at ages < 50 Myr and Teff ~ 3500K, and of the intrinsic dispersion in LiEW at all ages. Poor age discrimination is still an issue at ages > 1 Gyr, confirming that additional modelling flexibility is not sufficient to fully represent the LiEW - age - Teff relationship, and suggesting the involvement of further astrophysical parameters. Expansion to include such parameters - rotation, accretion, and surface gravity - is discussed, and the use of an ANN means these can be more easily included in future iterations, alongside more flexible functional forms for the LiEW dispersion. Our methods and ANN model are provided in an updated version 2.0 of the EAGLES software.         ",
    "url": "https://arxiv.org/abs/2409.07523",
    "authors": [
      "George Weaver",
      "Robin D. Jeffries",
      "Richard J. Jackson"
    ],
    "subjectives": [
      "Solar and Stellar Astrophysics (astro-ph.SR)",
      "Astrophysics of Galaxies (astro-ph.GA)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07556",
    "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
    "abstract": "           In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot text-based speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves the state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. Source code and demos are released.         ",
    "url": "https://arxiv.org/abs/2409.07556",
    "authors": [
      "Helin Wang",
      "Meng Yu",
      "Jiarui Hai",
      "Chen Chen",
      "Yuchen Hu",
      "Rilin Chen",
      "Najim Dehak",
      "Dong Yu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2409.07632",
    "title": "Learning Robust Observable to Address Noise in Quantum Machine Learning",
    "abstract": "           Quantum Machine Learning (QML) has emerged as a promising field that combines the power of quantum computing with the principles of machine learning. One of the significant challenges in QML is dealing with noise in quantum systems, especially in the Noisy Intermediate-Scale Quantum (NISQ) era. Noise in quantum systems can introduce errors in quantum computations and degrade the performance of quantum algorithms. In this paper, we propose a framework for learning observables that are robust against noisy channels in quantum systems. We demonstrate that it is possible to learn observables that remain invariant under the effects of noise and show that this can be achieved through a machine-learning approach. We present a toy example using a Bell state under a depolarization channel to illustrate the concept of robust observables. We then describe a machine-learning framework for learning such observables across six two-qubit quantum circuits and five noisy channels. Our results show that it is possible to learn observables that are more robust to noise than conventional observables. We discuss the implications of this finding for quantum machine learning, including potential applications in enhancing the stability of QML models in noisy environments. By developing techniques for learning robust observables, we can improve the performance and reliability of quantum machine learning models in the presence of noise, contributing to the advancement of practical QML applications in the NISQ era.         ",
    "url": "https://arxiv.org/abs/2409.07632",
    "authors": [
      "Bikram Khanal",
      "Pablo Rivas"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07652",
    "title": "Gaussian Process Upper Confidence Bounds in Distributed Point Target Tracking over Wireless Sensor Networks",
    "abstract": "           Uncertainty quantification plays a key role in the development of autonomous systems, decision-making, and tracking over wireless sensor networks (WSNs). However, there is a need of providing uncertainty confidence bounds, especially for distributed machine learning-based tracking, dealing with different volumes of data collected by sensors. This paper aims to fill in this gap and proposes a distributed Gaussian process (DGP) approach for point target tracking and derives upper confidence bounds (UCBs) of the state estimates. A unique contribution of this paper includes the derived theoretical guarantees on the proposed approach and its maximum accuracy for tracking with and without clutter measurements. Particularly, the developed approaches with uncertainty bounds are generic and can provide trustworthy solutions with an increased level of reliability. A novel hybrid Bayesian filtering method is proposed to enhance the DGP approach by adopting a Poisson measurement likelihood model. The proposed approaches are validated over a WSN case study, where sensors have limited sensing ranges. Numerical results demonstrate the tracking accuracy and robustness of the proposed approaches. The derived UCBs constitute a tool for trustworthiness evaluation of DGP approaches. The simulation results reveal that the proposed UCBs successfully encompass the true target states with 88% and 42% higher probability in X and Y coordinates, respectively, when compared to the confidence interval-based method.         ",
    "url": "https://arxiv.org/abs/2409.07652",
    "authors": [
      "Xingchi Liu",
      "Lyudmila Mihaylova",
      "Jemin George",
      "Tien Pham"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2409.07733",
    "title": "Self-similarity of temporal interaction networks arises from hyperbolic geometry with time-varying curvature",
    "abstract": "           The self-similarity of complex systems has been studied intensely across different domains due to its potential applications in system modeling, complexity analysis, etc., as well as for deep theoretical interest. Existing studies rely on scale transformations conceptualized over either a definite geometric structure of the system (very often realized as length-scale transformations) or purely temporal scale transformations. However, many physical and social systems are observed as temporal interactions among agents without any definitive geometry. Yet, one can imagine the existence of an underlying notion of distance as the interactions are mostly localized. Analysing only the time-scale transformations over such systems would uncover only a limited aspect of the complexity. In this work, we propose a novel technique of scale transformation that dissects temporal interaction networks under spatio-temporal scales, namely, flow scales. Upon experimenting with multiple social and biological interaction networks, we find that many of them possess a finite fractal dimension under flow-scale transformation. Finally, we relate the emergence of flow-scale self-similarity to the latent geometry of such networks. We observe strong evidence that justifies the assumption of an underlying, variable-curvature hyperbolic geometry that induces self-similarity of temporal interaction networks. Our work bears implications for modeling temporal interaction networks at different scales and uncovering their latent geometric structures.         ",
    "url": "https://arxiv.org/abs/2409.07733",
    "authors": [
      "Subhabrata Dutta",
      "Dipankar Das",
      "Tanmoy Chakraborty"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2409.07769",
    "title": "Mesh-based Super-Resolution of Fluid Flows with Multiscale Graph Neural Networks",
    "abstract": "           A graph neural network (GNN) approach is introduced in this work which enables mesh-based three-dimensional super-resolution of fluid flows. In this framework, the GNN is designed to operate not on the full mesh-based field at once, but on localized meshes of elements (or cells) directly. To facilitate mesh-based GNN representations in a manner similar to spectral (or finite) element discretizations, a baseline GNN layer (termed a message passing layer, which updates local node properties) is modified to account for synchronization of coincident graph nodes, rendering compatibility with commonly used element-based mesh connectivities. The architecture is multiscale in nature, and is comprised of a combination of coarse-scale and fine-scale message passing layer sequences (termed processors) separated by a graph unpooling layer. The coarse-scale processor embeds a query element (alongside a set number of neighboring coarse elements) into a single latent graph representation using coarse-scale synchronized message passing over the element neighborhood, and the fine-scale processor leverages additional message passing operations on this latent graph to correct for interpolation errors. Demonstration studies are performed using hexahedral mesh-based data from Taylor-Green Vortex flow simulations at Reynolds numbers of 1600 and 3200. Through analysis of both global and local errors, the results ultimately show how the GNN is able to produce accurate super-resolved fields compared to targets in both coarse-scale and multiscale model configurations. Reconstruction errors for fixed architectures were found to increase in proportion to the Reynolds number, while the inclusion of surrounding coarse element neighbors was found to improve predictions at Re=1600, but not at Re=3200.         ",
    "url": "https://arxiv.org/abs/2409.07769",
    "authors": [
      "Shivam Barwey",
      "Pinaki Pal",
      "Saumil Patel",
      "Riccardo Balin",
      "Bethany Lusch",
      "Venkatram Vishwanath",
      "Romit Maulik",
      "Ramesh Balakrishnan"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2409.07902",
    "title": "Conformal Distributed Remote Inference in Sensor Networks Under Reliability and Communication Constraints",
    "abstract": "           This paper presents communication-constrained distributed conformal risk control (CD-CRC) framework, a novel decision-making framework for sensor networks under communication constraints. Targeting multi-label classification problems, such as segmentation, CD-CRC dynamically adjusts local and global thresholds used to identify significant labels with the goal of ensuring a target false negative rate (FNR), while adhering to communication capacity limits. CD-CRC builds on online exponentiated gradient descent to estimate the relative quality of the observations of different sensors, and on online conformal risk control (CRC) as a mechanism to control local and global thresholds. CD-CRC is proved to offer deterministic worst-case performance guarantees in terms of FNR and communication overhead, while the regret performance in terms of false positive rate (FPR) is characterized as a function of the key hyperparameters. Simulation results highlight the effectiveness of CD-CRC, particularly in communication resource-constrained environments, making it a valuable tool for enhancing the performance and reliability of distributed sensor networks.         ",
    "url": "https://arxiv.org/abs/2409.07902",
    "authors": [
      "Meiyi Zhu",
      "Matteo Zecchin",
      "Sangwoo Park",
      "Caili Guo",
      "Chunyan Feng",
      "Petar Popovski",
      "Osvaldo Simeone"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.07930",
    "title": "A convolutional neural network approach to deblending seismic data",
    "abstract": "           For economic and efficiency reasons, blended acquisition of seismic data is becoming more and more commonplace. Seismic deblending methods are always computationally demanding and normally consist of multiple processing steps. Besides, the parameter setting is not always trivial. Machine learning-based processing has the potential to significantly reduce processing time and to change the way seismic deblending is carried out. We present a data-driven deep learning-based method for fast and efficient seismic deblending. The blended data are sorted from the common source to the common channel domain to transform the character of the blending noise from coherent events to incoherent distributions. A convolutional neural network (CNN) is designed according to the special character of seismic data, and performs deblending with comparable results to those obtained with conventional industry deblending algorithms. To ensure authenticity, the blending was done numerically and only field seismic data were employed, including more than 20000 training examples. After training and validation of the network, seismic deblending can be performed in near real time. Experiments also show that the initial signal to noise ratio (SNR) is the major factor controlling the quality of the final deblended result. The network is also demonstrated to be robust and adaptive by using the trained model to firstly deblend a new data set from a different geological area with a slightly different delay time setting, and secondly deblend shots with blending noise in the top part of the data.         ",
    "url": "https://arxiv.org/abs/2409.07930",
    "authors": [
      "Jing Sun",
      "Sigmund Slang",
      "Thomas Elboth",
      "Thomas Larsen Greiner",
      "Steven McDonald",
      "Leiv-J Gelius"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.07948",
    "title": "Quickest Change Detection Using Mismatched CUSUM",
    "abstract": "           The field of quickest change detection (QCD) concerns design and analysis of algorithms to estimate in real time the time at which an important event takes place and identify properties of the post-change behavior. The goal is to devise a stopping time adapted to the observations that minimizes an $L_1$ loss. Approximately optimal solutions are well known under a variety of assumptions. In the work surveyed here we consider the CUSUM statistic, which is defined as a one-dimensional reflected random walk driven by a functional of the observations. It is known that the optimal functional is a log likelihood ratio subject to special statical assumptions. The paper concerns model free approaches to detection design, considering the following questions: 1. What is the performance for a given functional of the observations? 2. How do the conclusions change when there is dependency between pre- and post-change behavior? 3. How can techniques from statistics and machine learning be adapted to approximate the best functional in a given class?         ",
    "url": "https://arxiv.org/abs/2409.07948",
    "authors": [
      "Austin Cooper",
      "Sean Meyn"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2409.07956",
    "title": "Community detection in multi-layer networks by regularized debiased spectral clustering",
    "abstract": "           Community detection is a crucial problem in the analysis of multi-layer networks. In this work, we introduce a new method, called regularized debiased sum of squared adjacency matrices (RDSoS), to detect latent communities in multi-layer networks. RDSoS is developed based on a novel regularized Laplacian matrix that regularizes the debiased sum of squared adjacency matrices. In contrast, the classical regularized Laplacian matrix typically regularizes the adjacency matrix of a single-layer network. Therefore, at a high level, our regularized Laplacian matrix extends the classical regularized Laplacian matrix to multi-layer networks. We establish the consistency property of RDSoS under the multi-layer stochastic block model (MLSBM) and further extend RDSoS and its theoretical results to the degree-corrected version of the MLSBM model. The effectiveness of the proposed methods is evaluated and demonstrated through synthetic and real datasets.         ",
    "url": "https://arxiv.org/abs/2409.07956",
    "authors": [
      "Huan Qing"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2409.08041",
    "title": "Interaction graphs of isomorphic automata networks II: universal dynamics",
    "abstract": "           An automata network with $n$ components over a finite alphabet $Q$ of size $q$ is a discrete dynamical system described by the successive iterations of a function $f:Q^n\\to Q^n$. In most applications, the main parameter is the interaction graph of $f$: the digraph with vertex set $[n]$ that contains an arc from $j$ to $i$ if $f_i$ depends on input $j$. What can be said on the set $\\mathbb{G}(f)$ of the interaction graphs of the automata networks isomorphic to $f$? It seems that this simple question has never been studied. In a previous paper, we prove that the complete digraph $K_n$, with $n^2$ arcs, is universal in that $K_n\\in \\mathbb{G}(f)$ whenever $f$ is not constant nor the identity (and $n\\geq 5$). In this paper, taking the opposite direction, we prove that there exists universal automata networks $f$, in that $\\mathbb{G}(f)$ contains all the digraphs on $[n]$, excepted the empty one. Actually, we prove that the presence of only three specific digraphs in $\\mathbb{G}(f)$ implies the universality of $f$, and we prove that this forces the alphabet size $q$ to have at least $n$ prime factors (with multiplicity). However, we prove that for any fixed $q\\geq 3$, there exists almost universal functions, that is, functions $f:Q^n\\to Q^n$ such that the probability that a random digraph belongs to $\\mathbb{G}(f)$ tends to $1$ as $n\\to\\infty$. We do not know if this holds in the binary case $q=2$, providing only partial results.         ",
    "url": "https://arxiv.org/abs/2409.08041",
    "authors": [
      "Florian Bridoux",
      "Aymeric Picard Marchetto",
      "Adrien Richard"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2409.08068",
    "title": "AutoPET Challenge: Tumour Synthesis for Data Augmentation",
    "abstract": "           Accurate lesion segmentation in whole-body PET/CT scans is crucial for cancer diagnosis and treatment planning, but limited datasets often hinder the performance of automated segmentation models. In this paper, we explore the potential of leveraging the deep prior from a generative model to serve as a data augmenter for automated lesion segmentation in PET/CT scans. We adapt the DiffTumor method, originally designed for CT images, to generate synthetic PET-CT images with lesions. Our approach trains the generative model on the AutoPET dataset and uses it to expand the training data. We then compare the performance of segmentation models trained on the original and augmented datasets. Our findings show that the model trained on the augmented dataset achieves a higher Dice score, demonstrating the potential of our data augmentation approach. In a nutshell, this work presents a promising direction for improving lesion segmentation in whole-body PET/CT scans with limited datasets, potentially enhancing the accuracy and reliability of cancer diagnostics.         ",
    "url": "https://arxiv.org/abs/2409.08068",
    "authors": [
      "Lap Yan Lennon Chan",
      "Chenxin Li",
      "Yixuan Yuan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2409.08148",
    "title": "Faster Speech-LLaMA Inference with Multi-token Prediction",
    "abstract": "           Large language models (LLMs) have become proficient at solving a wide variety of tasks, including those involving multi-modal inputs. In particular, instantiating an LLM (such as LLaMA) with a speech encoder and training it on paired data imparts speech recognition (ASR) abilities to the decoder-only model, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of auto-regressive inference and the relatively large decoder, Speech-LLaMA models require relatively high inference time. In this work, we propose to speed up Speech-LLaMA inference by predicting multiple tokens in the same decoding step. We explore several model architectures that enable this, and investigate their performance using threshold-based and verification-based inference strategies. We also propose a prefix-based beam search decoding method that allows efficient minimum word error rate (MWER) training for such models. We evaluate our models on a variety of public benchmarks, where they reduce the number of decoder calls by ~3.2x while maintaining or improving WER performance.         ",
    "url": "https://arxiv.org/abs/2409.08148",
    "authors": [
      "Desh Raj",
      "Gil Keren",
      "Junteng Jia",
      "Jay Mahadeokar",
      "Ozlem Kalinli"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2409.08170",
    "title": "AD-Lite Net: A Lightweight and Concatenated CNN Model for Alzheimer's Detection from MRI Images",
    "abstract": "           Alzheimer's Disease (AD) is a non-curable progressive neurodegenerative disorder that affects the human brain, leading to a decline in memory, cognitive abilities, and eventually, the ability to carry out daily tasks. Manual diagnosis of Alzheimer's disease from MRI images is fraught with less sensitivity and it is a very tedious process for neurologists. Therefore, there is a need for an automatic Computer Assisted Diagnosis (CAD) system, which can detect AD at early stages with higher accuracy. In this research, we have proposed a novel AD-Lite Net model (trained from scratch), that could alleviate the aforementioned problem. The novelties we bring here in this research are, (I) We have proposed a very lightweight CNN model by incorporating Depth Wise Separable Convolutional (DWSC) layers and Global Average Pooling (GAP) layers. (II) We have leveraged a ``parallel concatenation block'' (pcb), in the proposed AD-Lite Net model. This pcb consists of a Transformation layer (Tx-layer), followed by two convolutional layers, which are thereby concatenated with the original base model. This Tx-layer converts the features into very distinct kind of features, which are imperative for the Alzheimer's disease. As a consequence, the proposed AD-Lite Net model with ``parallel concatenation'' converges faster and automatically mitigates the class imbalance problem from the MRI datasets in a very generalized way. For the validity of our proposed model, we have implemented it on three different MRI datasets. Furthermore, we have combined the ADNI and AD datasets and subsequently performed a 10-fold cross-validation experiment to verify the model's generalization ability. Extensive experimental results showed that our proposed model has outperformed all the existing CNN models, and one recent trend Vision Transformer (ViT) model by a significant margin.         ",
    "url": "https://arxiv.org/abs/2409.08170",
    "authors": [
      "Santanu Roy",
      "Archit Gupta",
      "Shubhi Tiwari",
      "Palak Sahu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2110.15600",
    "title": "Data Driven based Dynamic Correction Prediction Model for NOx Emission of Coal Fired Boiler",
    "abstract": "           The real-time prediction of NOx emissions is of great significance for pollutant emission control and unit operation of coal-fired power plants. Aiming at dealing with the large time delay and strong nonlinear characteristics of the combustion process, a dynamic correction prediction model considering the time delay is proposed. First, the maximum information coefficient (MIC) is used to calculate the delay time between related parameters and NOx emissions, and the modeling data set is reconstructed; then, an adaptive feature selection algorithm based on Lasso and ReliefF is constructed to filter out the high correlation with NOx emissions. Parameters; Finally, an extreme learning machine (ELM) model combined with error correction was established to achieve the purpose of dynamically predicting the concentration of nitrogen oxides. Experimental results based on actual data show that the same variable has different delay times under load conditions such as rising, falling, and steady; and there are differences in model characteristic variables under different load conditions; dynamic error correction strategies effectively improve modeling accuracy; proposed The prediction error of the algorithm under different working conditions is less than 2%, which can accurately predict the NOx concentration at the combustion outlet, and provide guidance for NOx emission monitoring and combustion process optimization.         ",
    "url": "https://arxiv.org/abs/2110.15600",
    "authors": [
      "Zhenhao Tang",
      "Deyu Zhu",
      "Yang Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2201.12091",
    "title": "Linear Adversarial Concept Erasure",
    "abstract": "           Modern neural models trained on textual data rely on pre-trained representations that emerge without direct supervision. As these representations are increasingly being used in real-world applications, the inability to \\emph{control} their content becomes an increasingly important problem. We formulate the problem of identifying and erasing a linear subspace that corresponds to a given concept, in order to prevent linear predictors from recovering the concept. We model this problem as a constrained, linear maximin game, and show that existing solutions are generally not optimal for this task. We derive a closed-form solution for certain objectives, and propose a convex relaxation, \\method, that works well for others. When evaluated in the context of binary gender removal, the method recovers a low-dimensional subspace whose removal mitigates bias by intrinsic and extrinsic evaluation. We show that the method is highly expressive, effectively mitigating bias in deep nonlinear classifiers while maintaining tractability and interpretability.         ",
    "url": "https://arxiv.org/abs/2201.12091",
    "authors": [
      "Shauli Ravfogel",
      "Michael Twiton",
      "Yoav Goldberg",
      "Ryan Cotterell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2306.10614",
    "title": "Identifiable causal inference with noisy treatment and no side information",
    "abstract": "           In some causal inference scenarios, the treatment variable is measured inaccurately, for instance in epidemiology or econometrics. Failure to correct for the effect of this measurement error can lead to biased causal effect estimates. Previous research has not studied methods that address this issue from a causal viewpoint while allowing for complex nonlinear dependencies and without assuming access to side information. For such a scenario, this study proposes a model that assumes a continuous treatment variable that is inaccurately measured. Building on existing results for measurement error models, we prove that our model's causal effect estimates are identifiable, even without side information and knowledge of the measurement error variance. Our method relies on a deep latent variable model in which Gaussian conditionals are parameterized by neural networks, and we develop an amortized importance-weighted variational objective for training the model. Empirical results demonstrate the method's good performance with unknown measurement error. More broadly, our work extends the range of applications in which reliable causal inference can be conducted.         ",
    "url": "https://arxiv.org/abs/2306.10614",
    "authors": [
      "Antti P\u00f6ll\u00e4nen",
      "Pekka Marttinen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2308.05842",
    "title": "Performance Analysis for Hybrid Sub-6GHz-mmWave-THz Networks with Downlink and Uplink Decoupled Cell Association",
    "abstract": "           It is expected that 5G/6G networks will exploit sub-6 GHz, millimetre wave (mmWave) and terahertz (THz) frequency bands simultaneously and will increase flexibility in user equipment (UE)-cell association. In this paper, we introduce a novel stochastic geometry-based framework for the analysis of the signal-to-interference-plus-noise-ratio (SINR) and rate coverage in a multi-tier hybrid sub-6GHz-mmWave-THz network, where each tier has a particular base station (BS) density, transmit power, bandwidth, number of BS antennas, and cell-association bias factor. The proposed framework incorporates the effects of sub-6 GHz, mmWave and THz channel characteristics, BS beamforming gain, and blockages. We investigate the downlink (DL) and uplink (UL) decoupled cell-association strategy and characterise the per-tier cell-association probability. Based on that, we analytically derive the SINR and rate coverage probabilities for both DL and UL transmissions. The analytical results are validated via extensive Monte Carlo simulations. Numerical results demonstrate the superiority of the DL and UL decoupled cell-association strategy in terms of SINR and rate coverage over its coupled counterpart.         ",
    "url": "https://arxiv.org/abs/2308.05842",
    "authors": [
      "Yunbai Wang",
      "Chen Chen",
      "Xiaoli Chu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2309.06688",
    "title": "Cleaning Up the Streets: Understanding Motivations, Mental Models, and Concerns of Users Flagging Social Media Posts",
    "abstract": "           Social media platforms offer flagging, a technical feature that empowers users to report inappropriate posts or bad actors to reduce online harm. The deceptively simple flagging interfaces on nearly all major social media platforms disguise complex underlying interactions among users, algorithms, and moderators. Through semi-structured interviews with 25 social media users experienced in flagging inappropriate content, we examine end-users' understanding of flagging procedures, explore the factors that motivate them to flag, and surface their cognitive and privacy concerns. Our findings uncover a lack of procedural transparency in flagging mechanisms that create gaps in users' mental models. Regardless, users strongly believe that it is crucial for platforms to provide flagging options. We highlight how flags raise questions about distributing labor and responsibility between platforms and users for addressing online harm. We recommend innovations in the flagging design space that assist user comprehension, ensure privacy, and reduce cognitive burdens.         ",
    "url": "https://arxiv.org/abs/2309.06688",
    "authors": [
      "Alice Qian Zhang",
      "Kaitlin Montague",
      "Shagun Jhaver"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2310.00183",
    "title": "On the Equivalence of Graph Convolution and Mixup",
    "abstract": "           This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \\textit{Homophily Relabel} - assigning the target node's label to all its neighbors, and 2) \\textit{Test-Time Mixup} - Mixup the feature during the test time. We establish this equivalence mathematically by demonstrating that graph convolution networks (GCN) and simplified graph convolution (SGC) can be expressed as a form of Mixup. We also empirically verify the equivalence by training an MLP using the two conditions to achieve comparable performance.         ",
    "url": "https://arxiv.org/abs/2310.00183",
    "authors": [
      "Xiaotian Han",
      "Hanqing Zeng",
      "Yu Chen",
      "Shaoliang Nie",
      "Jingzhou Liu",
      "Kanika Narang",
      "Zahra Shakeri",
      "Karthik Abinav Sankararaman",
      "Song Jiang",
      "Madian Khabsa",
      "Qifan Wang",
      "Xia Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.07109",
    "title": "SparseCoder: Advancing Source Code Analysis with Sparse Attention and Learned Token Pruning",
    "abstract": "           As software projects rapidly evolve, software artifacts become more complex and defects behind get harder to identify. The emerging Transformer-based approaches, though achieving remarkable performance, struggle with long code sequences due to their self-attention mechanism, which scales quadratically with the sequence length. This paper introduces SparseCoder, an innovative approach incorporating sparse attention and learned token pruning (LTP) method (adapted from natural language processing) to address this limitation. Compared to previous state-of-the-art models CodeBERT, RoBERTa, and CodeT5, our experiments demonstrate that SparseCoder can handle significantly longer input sequences--at least twice as long, within the limits of our hardware resources and data statistics. Additionally, SparseCoder is four times faster than other methods measured in runtime, achieving a 50% reduction in floating point operations per second (FLOPs) with a negligible performance drop of less than 1% compared to Transformers using sparse attention (Sparse Atten). Plotting FLOPs of model inference against token lengths reveals that SparseCoder scales linearly, whereas other methods, including the current state-of-the-art model CodeT5, scale quadratically. Moreover, SparseCoder enhances interpretability by visualizing non-trivial tokens layer-wise.         ",
    "url": "https://arxiv.org/abs/2310.07109",
    "authors": [
      "Xueqi Yang",
      "Mariusz Jakubowski",
      "Li Kang",
      "Haojie Yu",
      "Tim Menzies"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2311.06942",
    "title": "Resilient Graph Neural Networks: A Coupled Dynamical Systems Approach",
    "abstract": "           Graph Neural Networks (GNNs) have established themselves as a key component in addressing diverse graph-based tasks. Despite their notable successes, GNNs remain susceptible to input perturbations in the form of adversarial attacks. This paper introduces an innovative approach to fortify GNNs against adversarial perturbations through the lens of coupled dynamical systems. Our method introduces graph neural layers based on differential equations with contractive properties, which, as we show, improve the robustness of GNNs. A distinctive feature of the proposed approach is the simultaneous learned evolution of both the node features and the adjacency matrix, yielding an intrinsic enhancement of model robustness to perturbations in the input features and the connectivity of the graph. We mathematically derive the underpinnings of our novel architecture and provide theoretical insights to reason about its expected behavior. We demonstrate the efficacy of our method through numerous real-world benchmarks, reading on par or improved performance compared to existing methods.         ",
    "url": "https://arxiv.org/abs/2311.06942",
    "authors": [
      "Moshe Eliasof",
      "Davide Murari",
      "Ferdia Sherry",
      "Carola-Bibiane Sch\u00f6nlieb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2312.08134",
    "title": "MToP: A MATLAB Optimization Platform for Evolutionary Multitasking",
    "abstract": "           Evolutionary multitasking (EMT) has emerged as a popular topic of evolutionary computation over the past years. It aims to concurrently address multiple optimization tasks within limited computing resources, leveraging inter-task knowledge transfer techniques. Despite the abundance of multitask evolutionary algorithms (MTEAs) proposed for multitask optimization (MTO), there remains a comprehensive software platform to help researchers evaluate MTEA performance on benchmark MTO problems as well as explore real-world applications. To bridge this gap, we introduce the first open-source optimization platform, named MTO-Platform (MToP), for EMT. MToP incorporates over 40 MTEAs, more than 150 MTO problem cases with real-world applications, and over 20 performance metrics. Moreover, to facilitate comparative analyses between MTEAs and traditional evolutionary algorithms, we adapted over 40 popular single-task evolutionary algorithms to address MTO problems. MToP boasts a user-friendly graphical interface, facilitating results analysis, data export, and schematics visualization. More importantly, MToP is designed with extensibility in mind, allowing users to develop new algorithms and tackle emerging problem domains. The source code of MToP is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2312.08134",
    "authors": [
      "Yanchi Li",
      "Wenyin Gong",
      "Fei Ming",
      "Tingyu Zhang",
      "Shuijia Li",
      "Qiong Gu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2312.17679",
    "title": "Data Augmentation for Supervised Graph Outlier Detection with Latent Diffusion Models",
    "abstract": "           Graph outlier detection is a prominent task of research and application in the realm of graph neural networks. It identifies the outlier nodes that exhibit deviation from the majority in the graph. One of the fundamental challenges confronting supervised graph outlier detection algorithms is the prevalent issue of class imbalance, where the scarcity of outlier instances compared to normal instances often results in suboptimal performance. Conventional methods mitigate the imbalance by reweighting instances in the estimation of the loss function, assigning higher weights to outliers and lower weights to inliers. Nonetheless, these strategies are prone to overfitting and underfitting, respectively. Recently, generative models, especially diffusion models, have demonstrated their efficacy in synthesizing high-fidelity images. Despite their extraordinary generation quality, their potential in data augmentation for supervised graph outlier detection remains largely underexplored. To bridge this gap, we introduce GODM, a novel data augmentation for mitigating class imbalance in supervised Graph Outlier detection with latent Diffusion Models. Specifically, our proposed method consists of three key components: (1) Variantioanl Encoder maps the heterogeneous information inherent within the graph data into a unified latent space. (2) Graph Generator synthesizes graph data that are statistically similar to real outliers from latent space, and (3) Latent Diffusion Model learns the latent space distribution of real organic data by iterative denoising. Extensive experiments conducted on multiple datasets substantiate the effectiveness and efficiency of GODM. The case study further demonstrated the generation quality of our synthetic data. To foster accessibility and reproducibility, we encapsulate GODM into a plug-and-play package and release it at the Python Package Index (PyPI).         ",
    "url": "https://arxiv.org/abs/2312.17679",
    "authors": [
      "Kay Liu",
      "Hengrui Zhang",
      "Ziqing Hu",
      "Fangxin Wang",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2401.02398",
    "title": "Generating synthetic data for neural operators",
    "abstract": "           Numerous developments in the recent literature show the promising potential of deep learning in obtaining numerical solutions to partial differential equations (PDEs) beyond the reach of current numerical solvers. However, data-driven neural operators all suffer from a similar problem: the data needed to train a network depends on classical numerical solvers such as finite difference or finite element, among others. In this paper, we propose a different approach to generating synthetic functional training data that does not require solving a PDE numerically. We draw a large number $N$ of independent and identically distributed 'random functions' $u_j$ from the underlying solution space (e.g., $H_0^1(\\Omega)$) in which we know the solution lies according to classical theory. We then plug each such random candidate solution into the equation and get a corresponding right-hand side function $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as supervised training data for learning the underlying inverse problem $f \\rightarrow u$. This `backwards' approach to generating training data only requires derivative computations, in contrast to standard `forward' approaches, which require a numerical PDE solver, enabling us to generate many data points quickly and efficiently. While the idea is simple, we hope this method will expand the potential for developing neural PDE solvers that do not depend on classical numerical solvers.         ",
    "url": "https://arxiv.org/abs/2401.02398",
    "authors": [
      "Erisa Hasani",
      "Rachel A. Ward"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2401.11123",
    "title": "Uncertainty-aware Bridge based Mobile-Former Network for Event-based Pattern Recognition",
    "abstract": "           The mainstream human activity recognition (HAR) algorithms are developed based on RGB cameras, which are easily influenced by low-quality images (e.g., low illumination, motion blur). Meanwhile, the privacy protection issue caused by ultra-high definition (HD) RGB cameras aroused more and more people's attention. Inspired by the success of event cameras which perform better on high dynamic range, no motion blur, and low energy consumption, we propose to recognize human actions based on the event stream. We propose a lightweight uncertainty-aware information propagation based Mobile-Former network for efficient pattern recognition, which aggregates the MobileNet and Transformer network effectively. Specifically, we first embed the event images using a stem network into feature representations, then, feed them into uncertainty-aware Mobile-Former blocks for local and global feature learning and fusion. Finally, the features from MobileNet and Transformer branches are concatenated for pattern recognition. Extensive experiments on multiple event-based recognition datasets fully validated the effectiveness of our model. The source code of this work will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2401.11123",
    "authors": [
      "Haoxiang Yang",
      "Chengguo Yuan",
      "Yabin Zhu",
      "Lan Chen",
      "Xiao Wang",
      "Futian Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2401.13751",
    "title": "A Training Rate and Survival Heuristic for Inference and Robustness Evaluation (TRASHFIRE)",
    "abstract": "           Machine learning models -- deep neural networks in particular -- have performed remarkably well on benchmark datasets across a wide variety of domains. However, the ease of finding adversarial counter-examples remains a persistent problem when training times are measured in hours or days and the time needed to find a successful adversarial counter-example is measured in seconds. Much work has gone into generating and defending against these adversarial counter-examples, however the relative costs of attacks and defences are rarely discussed. Additionally, machine learning research is almost entirely guided by test/train metrics, but these would require billions of samples to meet industry standards. The present work addresses the problem of understanding and predicting how particular model hyper-parameters influence the performance of a model in the presence of an adversary. The proposed approach uses survival models, worst-case examples, and a cost-aware analysis to precisely and accurately reject a particular model change during routine model training procedures rather than relying on real-world deployment, expensive formal verification methods, or accurate simulations of very complicated systems (\\textit{e.g.}, digitally recreating every part of a car or a plane). Through an evaluation of many pre-processing techniques, adversarial counter-examples, and neural network configurations, the conclusion is that deeper models do offer marginal gains in survival times compared to more shallow counterparts. However, we show that those gains are driven more by the model inference time than inherent robustness properties. Using the proposed methodology, we show that ResNet is hopelessly insecure against even the simplest of white box attacks.         ",
    "url": "https://arxiv.org/abs/2401.13751",
    "authors": [
      "Charles Meyers",
      "Mohammad Reza Saleh Sedghpour",
      "Tommy L\u00f6fstedt",
      "Erik Elmroth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.12040",
    "title": "Attack Tree Generation via Process Mining",
    "abstract": "           Attack Trees are a graphical model of security used to study threat scenarios. While visually appealing and supported by solid theories and effective tools, one of their main drawbacks remains the amount of effort required by security experts to design them from scratch. This work aims to remedy this by providing a method for the automatic generation of Attack Trees from attack logs. The main original feature of our approach w.r.t existing ones is the use of Process Mining algorithms to synthesize Attack Trees, which allow users to customize the way a set of logs are summarized as an Attack Tree, for example by discarding statistically irrelevant events. Our approach is supported by a prototype that, apart from the derivation and translation of the model, provides the user with an Attack Tree in the RisQFLan format, a tool used for quantitative risk modeling and analysis with Attack Trees. We illustrate our approach with the case study of attacks on a communication protocol, produced by a state-of-the-art protocol analyzer.         ",
    "url": "https://arxiv.org/abs/2402.12040",
    "authors": [
      "Alyzia-Maria Konsta",
      "Gemma Di Federico",
      "Alberto Lluch Lafuente",
      "Andrea Burattin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Formal Languages and Automata Theory (cs.FL)"
    ]
  },
  {
    "id": "arXiv:2403.00381",
    "title": "Structured Deep Neural Network-Based Backstepping Trajectory Tracking Control for Lagrangian Systems",
    "abstract": "           Deep neural networks (DNN) are increasingly being used to learn controllers due to their excellent approximation capabilities. However, their black-box nature poses significant challenges to closed-loop stability guarantees and performance analysis. In this paper, we introduce a structured DNN-based controller for the trajectory tracking control of Lagrangian systems using backing techniques. By properly designing neural network structures, the proposed controller can ensure closed-loop stability for any compatible neural network parameters. In addition, improved control performance can be achieved by further optimizing neural network parameters. Besides, we provide explicit upper bounds on tracking errors in terms of controller parameters, which allows us to achieve the desired tracking performance by properly selecting the controller parameters. Furthermore, when system models are unknown, we propose an improved Lagrangian neural network (LNN) structure to learn the system dynamics and design the controller. We show that in the presence of model approximation errors and external disturbances, the closed-loop stability and tracking control performance can still be guaranteed. The effectiveness of the proposed approach is demonstrated through simulations.         ",
    "url": "https://arxiv.org/abs/2403.00381",
    "authors": [
      "Jiajun Qian",
      "Liang Xu",
      "Xiaoqiang Ren",
      "Xiaofan Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2403.14770",
    "title": "Beehive: A Flexible Network Stack for Direct-Attached Accelerators",
    "abstract": "           Direct-attached accelerators, where application accelerators are directly connected to the datacenter network via a hardware network stack, offer substantial benefits in terms of reduced latency, CPU overhead, and energy use. However, a key challenge is that modern datacenter network stacks are complex, with interleaved protocol layers, network management functions, and virtualization support. To operators, network feature agility, diagnostics, and manageability are often considered just as important as raw performance. By contrast, existing hardware network stacks only support basic protocols and are often difficult to extend since they use fixed processing pipelines. We propose Beehive, a new, open-source FPGA network stack for direct-attached accelerators designed to enable flexible and adaptive construction of complex network functionality in hardware. Application and network protocol elements are modularized as tiles over a network-on-chip substrate. Elements can be added or scaled up/down to match workload characteristics with minimal effort or changes to other elements. Flexible diagnostics and control are integral, with tooling to ensure deadlock safety. Our implementation interoperates with standard Linux TCP and UDP clients, with a 4x improvement in end-to-end RPC tail latency for Linux UDP clients versus a CPU-attached accelerator. Beehive is available at this https URL ",
    "url": "https://arxiv.org/abs/2403.14770",
    "authors": [
      "Katie Lim",
      "Matthew Giordano",
      "Theano Stavrinos",
      "Irene Zhang",
      "Jacob Nelson",
      "Baris Kasikci",
      "Tom Anderson"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2403.15365",
    "title": "A Transfer Attack to Image Watermarks",
    "abstract": "           Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.         ",
    "url": "https://arxiv.org/abs/2403.15365",
    "authors": [
      "Yuepeng Hu",
      "Zhengyuan Jiang",
      "Moyang Guo",
      "Neil Gong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.03916",
    "title": "Estimating mixed memberships in multi-layer networks",
    "abstract": "           Community detection in multi-layer networks has emerged as a crucial area of modern network analysis. However, conventional approaches often assume that nodes belong exclusively to a single community, which fails to capture the complex structure of real-world networks where nodes may belong to multiple communities simultaneously. To address this limitation, we propose novel spectral methods to estimate the common mixed memberships in the multi-layer mixed membership stochastic block model. The proposed methods leverage the eigen-decomposition of three aggregate matrices: the sum of adjacency matrices, the debiased sum of squared adjacency matrices, and the sum of squared adjacency matrices. We establish rigorous theoretical guarantees for the consistency of our methods. Specifically, we derive per-node error rates under mild conditions on network sparsity, demonstrating their consistency as the number of nodes and/or layers increases under the multi-layer mixed membership stochastic block model. Our theoretical results reveal that the method leveraging the sum of adjacency matrices generally performs poorer than the other two methods for mixed membership estimation in multi-layer networks. We conduct extensive numerical experiments to empirically validate our theoretical findings. For real-world multi-layer networks with unknown community information, we introduce two novel modularity metrics to quantify the quality of mixed membership community detection. Finally, we demonstrate the practical applications of our algorithms and modularity metrics by applying them to real-world multi-layer networks, demonstrating their effectiveness in extracting meaningful community structures.         ",
    "url": "https://arxiv.org/abs/2404.03916",
    "authors": [
      "Huan Qing"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2404.11256",
    "title": "MMCBE: Multi-modality Dataset for Crop Biomass Prediction and Beyond",
    "abstract": "           Crop biomass, a critical indicator of plant growth, health, and productivity, is invaluable for crop breeding programs and agronomic research. However, the accurate and scalable quantification of crop biomass remains inaccessible due to limitations in existing measurement methods. One of the obstacles impeding the advancement of current crop biomass prediction methodologies is the scarcity of publicly available datasets. Addressing this gap, we introduce a new dataset in this domain, i.e. Multi-modality dataset for crop biomass estimation (MMCBE). Comprising 216 sets of multi-view drone images, coupled with LiDAR point clouds, and hand-labelled ground truth, MMCBE represents the first multi-modality one in the field. This dataset aims to establish benchmark methods for crop biomass quantification and foster the development of vision-based approaches. We have rigorously evaluated state-of-the-art crop biomass estimation methods using MMCBE and ventured into additional potential applications, such as 3D crop reconstruction from drone imagery and novel-view rendering. With this publication, we are making our comprehensive dataset available to the broader community.         ",
    "url": "https://arxiv.org/abs/2404.11256",
    "authors": [
      "Xuesong Li",
      "Zeeshan Hayder",
      "Ali Zia",
      "Connor Cassidy",
      "Shiming Liu",
      "Warwick Stiller",
      "Eric Stone",
      "Warren Conaty",
      "Lars Petersson",
      "Vivien Rolland"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2405.05097",
    "title": "Biology-inspired joint distribution neurons based on Hierarchical Correlation Reconstruction allowing for multidirectional neural networks",
    "abstract": "           Biological neural networks seem qualitatively superior (e.g. in learning, flexibility, robustness) to current artificial like Multi-Layer Perceptron (MLP) or Kolmogorov-Arnold Network (KAN). Simultaneously, in contrast to them: biological have fundamentally multidirectional signal propagation~\\cite{axon}, also of probability distributions e.g. for uncertainty estimation, and are believed not being able to use standard backpropagation training~\\cite{backprop}. There are proposed novel artificial neurons based on HCR (Hierarchical Correlation Reconstruction) allowing to remove the above low level differences: with neurons containing local joint distribution model (of its connections), representing joint density on normalized variables as just linear combination of $(f_\\mathbf{j})$ orthonormal polynomials: $\\rho(\\mathbf{x})=\\sum_{\\mathbf{j}\\in B} a_\\mathbf{j} f_\\mathbf{j}(\\mathbf{x})$ for $\\mathbf{x} \\in [0,1]^d$ and $B$ some chosen basis, approaching complete description of joint distribution with basis growth. By various index summations of such $(a_\\mathbf{j})$ tensor as neuron parameters, we get simple formulas for e.g. conditional expected values for propagation in any direction, like $E[x|y,z]$, $E[y|x]$, which degenerate to KAN-like parametrization if restricting to pairwise dependencies. Such HCR network can also propagate probability distributions (also joint) like $\\rho(y,z|x)$. It also allows for additional training approaches, like direct $(a_\\mathbf{j})$ estimation, through tensor decomposition, or more biologically plausible information bottleneck training: layers directly influencing only neighbors, optimizing content to maximize information about the next layer, and minimizing about the previous to remove noise, extract crucial information.         ",
    "url": "https://arxiv.org/abs/2405.05097",
    "authors": [
      "Jarek Duda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2405.14529",
    "title": "AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2",
    "abstract": "           Recent advances in multimodal foundation models have set new standards in few-shot anomaly detection. This paper explores whether high-quality visual features alone are sufficient to rival existing state-of-the-art vision-language models. We affirm this by adapting DINOv2 for one-shot and few-shot anomaly detection, with a focus on industrial applications. We show that this approach does not only rival existing techniques but can even outmatch them in many settings. Our proposed vision-only approach, AnomalyDINO, is based on patch similarities and enables both image-level anomaly prediction and pixel-level anomaly segmentation. The approach is methodologically simple and training-free and, thus, does not require any additional data for fine-tuning or meta-learning. Despite its simplicity, AnomalyDINO achieves state-of-the-art results in one- and few-shot anomaly detection (e.g., pushing the one-shot performance on MVTec-AD from an AUROC of 93.1% to 96.6%). The reduced overhead, coupled with its outstanding few-shot performance, makes AnomalyDINO a strong candidate for fast deployment, e.g., in industrial contexts.         ",
    "url": "https://arxiv.org/abs/2405.14529",
    "authors": [
      "Simon Damm",
      "Mike Laszkiewicz",
      "Johannes Lederer",
      "Asja Fischer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.04170",
    "title": "Element-wise Multiplication Based Deeper Physics-Informed Neural Networks",
    "abstract": "           As a promising framework for resolving partial differential equations (PDEs), Physics-Informed Neural Networks (PINNs) have received widespread attention from industrial and scientific fields. However, lack of expressive ability and initialization pathology issues are found to prevent the application of PINNs in complex PDEs. In this work, we propose Deeper Physics-Informed Neural Network (Deeper-PINN) to resolve these issues. The element-wise multiplication operation is adopted to transform features into high-dimensional, non-linear spaces. Benefiting from element-wise multiplication operation, Deeper-PINNs can alleviate the initialization pathologies of PINNs and enhance the expressive capability of PINNs. The proposed structure is verified on various benchmarks. The results show that Deeper-PINNs can effectively resolve the initialization pathology and exhibit strong expressive ability.         ",
    "url": "https://arxiv.org/abs/2406.04170",
    "authors": [
      "Feilong Jiang",
      "Xiaonan Hou",
      "Min Xia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2406.06852",
    "title": "A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures",
    "abstract": "           Large Language Models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings. Despite the demonstrable efficacy of LLMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms. However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms.         ",
    "url": "https://arxiv.org/abs/2406.06852",
    "authors": [
      "Shuai Zhao",
      "Meihuizi Jia",
      "Zhongliang Guo",
      "Leilei Gan",
      "Xiaoyu Xu",
      "Xiaobao Wu",
      "Jie Fu",
      "Yichao Feng",
      "Fengjun Pan",
      "Luu Anh Tuan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.11548",
    "title": "AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation",
    "abstract": "           The ability to reflect on and correct failures is crucial for robotic systems to interact stably with real-life objects.Observing the generalization and reasoning capabilities of Multimodal Large Language Models (MLLMs), previous approaches have aimed to utilize these models to enhance robotic systems accordingly.However, these methods typically focus on high-level planning corrections using an additional MLLM, with limited utilization of failed samples to correct low-level contact poses. To address this gap, we propose an Autonomous Interactive Correction (AIC) MLLM, which makes use of previous low-level interaction experiences to correct SE(3) pose predictions. Specifically, AIC MLLM is initially fine-tuned to acquire both pose prediction and feedback prompt comprehension abilities.We carefully design two types of prompt instructions through interactions with objects: 1) visual masks to highlight unmovable parts for position correction, and 2)textual descriptions to indicate potential directions for rotation correction.During inference, a Feedback Information Extraction module is introduced to recognize the failure cause, allowing AIC MLLM to adaptively correct the pose prediction using the corresponding prompts. To further enhance manipulation stability, we devise a Test Time Adaptation strategy that enables AIC MLLM to better adapt to the current scene configuration.Finally, extensive experiments are conducted in both simulated and real-world environments to evaluate the proposed method. The results demonstrate that our AIC MLLM can efficiently correct failure samples by leveraging interaction experience prompts.Real-world demonstration can be found at this https URL ",
    "url": "https://arxiv.org/abs/2406.11548",
    "authors": [
      "Chuyan Xiong",
      "Chengyu Shen",
      "Xiaoqi Li",
      "Kaichen Zhou",
      "Jiaming Liu",
      "Ruiping Wang",
      "Hao Dong"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.18977",
    "title": "RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulation",
    "abstract": "           Utilizing Vision-Language Models (VLMs) for robotic manipulation represents a novel paradigm, aiming to enhance the model's ability to generalize to new objects and instructions. However, due to variations in camera specifications and mounting positions, existing methods exhibit significant performance disparities across different robotic platforms. To address this challenge, we propose RoboUniView in this paper, an innovative approach that decouples visual feature extraction from action learning. We first learn a unified view representation from multi-perspective views by pre-training on readily accessible data, and then derive actions from this unified view representation to control robotic manipulation. This unified view representation more accurately mirrors the physical world and is not constrained by the robotic platform's camera parameters. Thanks to this methodology, we achieve state-of-the-art performance on the demanding CALVIN benchmark, enhancing the success rate in the $D \\to D$ setting from 93.0% to 96.2%, and in the $ABC \\to D$ setting from 92.2% to 94.2%. Moreover, our model exhibits outstanding adaptability and flexibility: it maintains high performance under unseen camera parameters, can utilize multiple datasets with varying camera parameters, and is capable of joint cross-task learning across datasets. Code is provided for re-implementation. this https URL ",
    "url": "https://arxiv.org/abs/2406.18977",
    "authors": [
      "Fanfan Liu",
      "Feng Yan",
      "Liming Zheng",
      "Chengjian Feng",
      "Yiyang Huang",
      "Lin Ma"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.02403",
    "title": "Face Reconstruction Transfer Attack as Out-of-Distribution Generalization",
    "abstract": "           Understanding the vulnerability of face recognition systems to malicious attacks is of critical importance. Previous works have focused on reconstructing face images that can penetrate a targeted verification system. Even in the white-box scenario, however, naively reconstructed images misrepresent the identity information, hence the attacks are easily neutralized once the face system is updated or changed. In this paper, we aim to reconstruct face images which are capable of transferring face attacks on unseen encoders. We term this problem as Face Reconstruction Transfer Attack (FRTA) and show that it can be formulated as an out-of-distribution (OOD) generalization problem. Inspired by its OOD nature, we propose to solve FRTA by Averaged Latent Search and Unsupervised Validation with pseudo target (ALSUV). To strengthen the reconstruction attack on OOD unseen encoders, ALSUV reconstructs the face by searching the latent of amortized generator StyleGAN2 through multiple latent optimization, latent optimization trajectory averaging, and unsupervised validation with a pseudo target. We demonstrate the efficacy and generalization of our method on widely used face datasets, accompanying it with extensive ablation studies and visually, qualitatively, and quantitatively analyses. The source code will be released.         ",
    "url": "https://arxiv.org/abs/2407.02403",
    "authors": [
      "Yoon Gyo Jung",
      "Jaewoo Park",
      "Xingbo Dong",
      "Hojin Park",
      "Andrew Beng Jin Teoh",
      "Octavia Camps"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.11698",
    "title": "NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks",
    "abstract": "           Quantization has become increasingly pivotal in addressing the steadily increasing computational and memory requirements of Deep Neural Networks (DNNs). By reducing the number of bits used to represent weights and activations (typically from 32-bit floating-point to 16-bit or 8-bit integers), quantization reduces the memory footprint, energy consumption, and execution time of DNN models. However, traditional quantization methods typically focus on the inference of DNNs, while the training process still relies on floating-point operations. To date, only one work in the literature has addressed integer-only training for Multi-Layer Perceptron (MLP) architectures. This work introduces NITRO-D, a new framework for training arbitrarily deep integer-only Convolutional Neural Networks (CNNs) that operate entirely in the integer-only domain for both training and inference. NITRO-D is the first framework in the literature enabling the training of integer-only CNNs without the need to introduce a quantization scheme. Specifically, NITRO-D introduces a novel architecture integrating multiple integer local-loss blocks, which include the proposed NITRO Scaling Layer and the NITRO-ReLU activation function. Additionally, it introduces a novel integer-only learning algorithm derived from Local Error Signals (LES), utilizing IntegerSGD, an optimizer specifically designed to operate in an integer-only context. NITRO-D is implemented in an open-source Python library. Extensive experimental evaluations demonstrate its effectiveness across several state-of-the-art image recognition datasets. Results show significant performance improvements from 2.47% to 5.96% for integer-only MLP architectures over the state-of-the-art solution, and the capability of training integer-only CNN architectures with minimal accuracy degradation from -0.15% to -4.22% compared to floating-point LES.         ",
    "url": "https://arxiv.org/abs/2407.11698",
    "authors": [
      "Alberto Pirillo",
      "Luca Colombo",
      "Manuel Roveri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2407.14573",
    "title": "Trading Devil Final: Backdoor attack via Stock market and Bayesian Optimization",
    "abstract": "           Since the advent of generative artificial intelligence, every company and researcher has been rushing to develop their own generative models, whether commercial or not. Given the large number of users of these powerful new tools, there is currently no intrinsically verifiable way to explain from the ground up what happens when LLMs (large language models) learn. For example, those based on automatic speech recognition systems, which have to rely on huge and astronomical amounts of data collected from all over the web to produce fast and efficient results, In this article, we develop a backdoor attack called MarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is mainly based on modern stock market models. In order to show the possible vulnerabilities of speech-based transformers that may rely on LLMs.         ",
    "url": "https://arxiv.org/abs/2407.14573",
    "authors": [
      "Orson Mengara"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computational Finance (q-fin.CP)",
      "Pricing of Securities (q-fin.PR)",
      "Statistical Finance (q-fin.ST)"
    ]
  },
  {
    "id": "arXiv:2407.18134",
    "title": "$\\mathbb{X}$-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs",
    "abstract": "           Learning good representations involves capturing the diverse ways in which data samples relate. Contrastive loss - an objective matching related samples - underlies methods from self-supervised to multimodal learning. Contrastive losses, however, can be viewed more broadly as modifying a similarity graph to indicate how samples should relate in the embedding space. This view reveals a shortcoming in contrastive learning: the similarity graph is binary, as only one sample is the related positive sample. Crucially, similarities \\textit{across} samples are ignored. Based on this observation, we revise the standard contrastive loss to explicitly encode how a sample relates to others. We experiment with this new objective, called $\\mathbb{X}$-Sample Contrastive, to train vision models based on similarities in class or text caption descriptions. Our study spans three scales: ImageNet-1k with 1 million, CC3M with 3 million, and CC12M with 12 million samples. The representations learned via our objective outperform both contrastive self-supervised and vision-language models trained on the same data across a range of tasks. When training on CC12M, we outperform CLIP by $0.6\\%$ on both ImageNet and ImageNet Real. Our objective appears to work particularly well in lower-data regimes, with gains over CLIP of $16.8\\%$ on ImageNet and $18.1\\%$ on ImageNet Real when training with CC3M. Finally, our objective seems to encourage the model to learn representations that separate objects from their attributes and backgrounds, with gains of $3.3$-$5.6$\\% over CLIP on ImageNet9. We hope the proposed solution takes a small step towards developing richer learning objectives for understanding sample relations in foundation models.         ",
    "url": "https://arxiv.org/abs/2407.18134",
    "authors": [
      "Vlad Sobal",
      "Mark Ibrahim",
      "Randall Balestriero",
      "Vivien Cabannes",
      "Diane Bouchacourt",
      "Pietro Astolfi",
      "Kyunghyun Cho",
      "Yann LeCun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.21591",
    "title": "Simpler Optimal Sorting from a Directed Acyclic Graph",
    "abstract": "           Fredman proposed in 1976 the following algorithmic problem: Given are a ground set $X$, some partial order $P$ over $X$, and some comparison oracle $O_L$ that specifies a linear order $L$ over $X$ that extends $P$. A query to $O_L$ has as input distinct $x, x' \\in X$ and outputs whether $x <_L x'$ or vice versa. If we denote by $e(P)$ the number of linear extensions of $P$, then $\\log e(P)$ is a worst-case lower bound on the number of queries needed to output the sorted order of $X$. Fredman did not specify in what form the partial order is given. Haeupler, Hlad\u00edk, Iacono, Rozhon, Tarjan, and T\u011btek ('24) propose to assume as input a directed acyclic graph, $G$, with $m$ edges and $n=|X|$ vertices. Denote by $P_G$ the partial order induced by $G$. Algorithmic performance is measured in running time and the number of queries used, where they use $\\Theta(m + n + \\log e(P_G))$ time and $\\Theta(\\log e(P_G))$ queries to output $X$ in its sorted order. Their algorithm is worst-case optimal in terms of running time and queries, both. Their algorithm combines topological sorting with heapsort. Their analysis relies upon sophisticated counting arguments using entropy, recursively defined sets defined over the run of their algorithm, and vertices in the graph that they identify as bottlenecks for sorting. In this paper, we do away with sophistication. We show that when the input is a directed acyclic graph then the problem admits a simple solution using $\\Theta(m + n + \\log e(P_G))$ time and $\\Theta(\\log e(P_G))$ queries. Especially our proofs are much simpler as we avoid the usage of advanced charging arguments and data structures, and instead rely upon two brief observations.         ",
    "url": "https://arxiv.org/abs/2407.21591",
    "authors": [
      "Ivor van der Hoog",
      "Eva Rotenberg",
      "Daniel Rutschmann"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2408.02928",
    "title": "PGB: Benchmarking Differentially Private Synthetic Graph Generation Algorithms",
    "abstract": "           Differentially private graph analysis is a powerful tool for deriving insights from diverse graph data while protecting individual information. Designing private analytic algorithms for different graph queries often requires starting from scratch. In contrast, differentially private synthetic graph generation offers a general paradigm that supports one-time generation for multiple queries. Although a rich set of differentially private graph generation algorithms has been proposed, comparing them effectively remains challenging due to various factors, including differing privacy definitions, diverse graph datasets, varied privacy requirements, and multiple utility metrics. To this end, we propose PGB (Private Graph Benchmark), a comprehensive benchmark designed to enable researchers to compare differentially private graph generation algorithms fairly. We begin by identifying four essential elements of existing works as a 4-tuple: mechanisms, graph datasets, privacy requirements, and utility metrics. We discuss principles regarding these elements to ensure the comprehensiveness of a benchmark. Next, we present a benchmark instantiation that adheres to all principles, establishing a new method to evaluate existing and newly proposed graph generation algorithms. Through extensive theoretical and empirical analysis, we gain valuable insights into the strengths and weaknesses of prior algorithms. Our results indicate that there is no universal solution for all possible cases. Finally, we provide guidelines to help researchers select appropriate mechanisms for various scenarios.         ",
    "url": "https://arxiv.org/abs/2408.02928",
    "authors": [
      "Shang Liu",
      "Hao Du",
      "Yang Cao",
      "Bo Yan",
      "Jinfei Liu",
      "Masatoshi Yoshikawa"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2408.10555",
    "title": "GACL: Graph Attention Collaborative Learning for Temporal QoS Prediction",
    "abstract": "           Accurate prediction of temporal QoS is crucial for maintaining service reliability and enhancing user satisfaction in dynamic service-oriented environments. However, current methods often neglect high-order latent collaborative relationships and fail to dynamically adjust feature learning for specific user-service invocations, which are critical for precise feature extraction within each time slice. Moreover, the prevalent use of RNNs for modeling temporal feature evolution patterns is constrained by their inherent difficulty in managing long-range dependencies, thereby limiting the detection of long-term QoS trends across multiple time slices. These shortcomings dramatically degrade the performance of temporal QoS prediction. To address the two issues, we propose a novel Graph Attention Collaborative Learning (GACL) framework for temporal QoS prediction. Building on a dynamic user-service invocation graph to comprehensively model historical interactions, it designs a target-prompt graph attention network to extract deep latent features of users and services at each time slice, considering implicit target-neighboring collaborative relationships and historical QoS values. Additionally, a multi-layer Transformer encoder is introduced to uncover temporal feature evolution patterns, enhancing temporal QoS prediction. Extensive experiments on the WS-DREAM dataset demonstrate that GACL significantly outperforms state-of-the-art methods for temporal QoS prediction across multiple evaluation metrics, achieving the improvements of up to 38.80%.         ",
    "url": "https://arxiv.org/abs/2408.10555",
    "authors": [
      "Shengxiang Hu",
      "Guobing Zou",
      "Bofeng Zhang",
      "Shaogang Wu",
      "Shiyi Lin",
      "Yanglan Gan",
      "Yixin Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2408.12093",
    "title": "LLM-enhanced Scene Graph Learning for Household Rearrangement",
    "abstract": "           The household rearrangement task involves spotting misplaced objects in a scene and accommodate them with proper places. It depends both on common-sense knowledge on the objective side and human user preference on the subjective side. In achieving such task, we propose to mine object functionality with user preference alignment directly from the scene itself, without relying on human intervention. To do so, we work with scene graph representation and propose LLM-enhanced scene graph learning which transforms the input scene graph into an affordance-enhanced graph (AEG) with information-enhanced nodes and newly discovered edges (relations). In AEG, the nodes corresponding to the receptacle objects are augmented with context-induced affordance which encodes what kind of carriable objects can be placed on it. New edges are discovered with newly discovered non-local relations. With AEG, we perform task planning for scene rearrangement by detecting misplaced carriables and determining a proper placement for each of them. We test our method by implementing a tiding robot in simulator and perform evaluation on a new benchmark we build. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on misplacement detection and the following rearrangement planning.         ",
    "url": "https://arxiv.org/abs/2408.12093",
    "authors": [
      "Wenhao Li",
      "Zhiyuan Yu",
      "Qijin She",
      "Zhinan Yu",
      "Yuqing Lan",
      "Chenyang Zhu",
      "Ruizhen Hu",
      "Kai Xu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12419",
    "title": "4D Diffusion for Dynamic Protein Structure Prediction with Reference Guided Motion Alignment",
    "abstract": "           Protein structure prediction is pivotal for understanding the structure-function relationship of proteins, advancing biological research, and facilitating pharmaceutical development and experimental design. While deep learning methods and the expanded availability of experimental 3D protein structures have accelerated structure prediction, the dynamic nature of protein structures has received limited attention. This study introduces an innovative 4D diffusion model incorporating molecular dynamics (MD) simulation data to learn dynamic protein structures. Our approach is distinguished by the following components: (1) a unified diffusion model capable of generating dynamic protein structures, including both the backbone and side chains, utilizing atomic grouping and side-chain dihedral angle predictions; (2) a reference network that enhances structural consistency by integrating the latent embeddings of the initial 3D protein structures; and (3) a motion alignment module aimed at improving temporal structural coherence across multiple time steps. To our knowledge, this is the first diffusion-based model aimed at predicting protein trajectories across multiple time steps simultaneously. Validation on benchmark datasets demonstrates that our model exhibits high accuracy in predicting dynamic 3D structures of proteins containing up to 256 amino acids over 32 time steps, effectively capturing both local flexibility in stable states and significant conformational changes.         ",
    "url": "https://arxiv.org/abs/2408.12419",
    "authors": [
      "Kaihui Cheng",
      "Ce Liu",
      "Qingkun Su",
      "Jun Wang",
      "Liwei Zhang",
      "Yining Tang",
      "Yao Yao",
      "Siyu Zhu",
      "Yuan Qi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.14809",
    "title": "GSIFN: A Graph-Structured and Interlaced-Masked Multimodal Transformer-based Fusion Network for Multimodal Sentiment Analysis",
    "abstract": "           Multimodal Sentiment Analysis (MSA) leverages multiple data modals to analyze human sentiment. Existing MSA models generally employ cutting-edge multimodal fusion and representation learning-based methods to promote MSA capability. However, there are two key challenges: (i) in existing multimodal fusion methods, the decoupling of modal combinations and tremendous parameter redundancy, lead to insufficient fusion performance and efficiency; (ii) a challenging trade-off exists between representation capability and computational overhead in unimodal feature extractors and encoders. Our proposed GSIFN incorporates two main components to solve these problems: (i) a graph-structured and interlaced-masked multimodal Transformer. It adopts the Interlaced Mask mechanism to construct robust multimodal graph embedding, achieve all-modal-in-one Transformer-based fusion, and greatly reduce the computational overhead; (ii) a self-supervised learning framework with low computational overhead and high performance, which utilizes a parallelized LSTM with matrix memory to enhance non-verbal modal features for unimodal label generation. Evaluated on the MSA datasets CMU-MOSI, CMU-MOSEI, and CH-SIMS, GSIFN demonstrates superior performance with significantly lower computational overhead compared with previous state-of-the-art models.         ",
    "url": "https://arxiv.org/abs/2408.14809",
    "authors": [
      "Yijie Jin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.01230",
    "title": "CoLaNET -- A Spiking Neural Network with Columnar Layered Architecture for Classification",
    "abstract": "           In the present paper, I describe a spiking neural network (SNN) architecture which, can be used in wide range of supervised learning classification tasks. It is assumed, that all participating signals (the classified object description, correct class label and SNN decision) have spiking nature. The distinctive feature of this architecture is a combination of prototypical network structures corresponding to different classes and significantly distinctive instances of one class (=columns) and functionally differing populations of neurons inside columns (=layers). The other distinctive feature is a novel combination of anti-Hebbian and dopamine-modulated plasticity. The plasticity rules are local and do not use the backpropagation principle. Besides that, as in my previous studies, I was guided by the requirement that the all neuron/plasticity models should be easily implemented on modern neurochips. I illustrate the high performance of my network on a task related to model-based reinforcement learning, namely, evaluation of proximity of an external world state to the target state.         ",
    "url": "https://arxiv.org/abs/2409.01230",
    "authors": [
      "Mikhail Kiselev"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2409.02123",
    "title": "PuYun: Medium-Range Global Weather Forecasting Using Large Kernel Attention Convolutional Networks",
    "abstract": "           Accurate weather forecasting is essential for understanding and mitigating weather-related impacts. In this paper, we present PuYun, an autoregressive cascade model that leverages large kernel attention convolutional networks. The model's design inherently supports extended weather prediction horizons while broadening the effective receptive field. The integration of large kernel attention mechanisms within the convolutional layers enhances the model's capacity to capture fine-grained spatial details, thereby improving its predictive accuracy for meteorological phenomena. We introduce PuYun, comprising PuYun-Short for 0-5 day forecasts and PuYun-Medium for 5-10 day predictions. This approach enhances the accuracy of 10-day weather forecasting. Through evaluation, we demonstrate that PuYun-Short alone surpasses the performance of both GraphCast and FuXi-Short in generating accurate 10-day forecasts. Specifically, on the 10th day, PuYun-Short reduces the RMSE for Z500 to 720 $m^2/s^2$, compared to 732 $m^2/s^2$ for GraphCast and 740 $m^2/s^2$ for FuXi-Short. Additionally, the RMSE for T2M is reduced to 2.60 K, compared to 2.63 K for GraphCast and 2.65 K for FuXi-Short. Furthermore, when employing a cascaded approach by integrating PuYun-Short and PuYun-Medium, our method achieves superior results compared to the combined performance of FuXi-Short and FuXi-Medium. On the 10th day, the RMSE for Z500 is further reduced to 638 $m^2/s^2$, compared to 641 $m^2/s^2$ for FuXi. These findings underscore the effectiveness of our model ensemble in advancing medium-range weather prediction. Our training code and model will be open-sourced.         ",
    "url": "https://arxiv.org/abs/2409.02123",
    "authors": [
      "Shengchen Zhu",
      "Yiming Chen",
      "Peiying Yu",
      "Xiang Qu",
      "Yuxiao Zhou",
      "Yiming Ma",
      "Zhizhan Zhao",
      "Yukai Liu",
      "Hao Mi",
      "Bin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2409.03906",
    "title": "Analytical Optimized Traffic Flow Recovery for Large-scale Urban Transportation Network",
    "abstract": "           The implementation of intelligent transportation systems (ITS) has enhanced data collection in urban transportation through advanced traffic sensing devices. However, the high costs associated with installation and maintenance result in sparse traffic data coverage. To obtain complete, accurate, and high-resolution network-wide traffic flow data, this study introduces the Analytical Optimized Recovery (AOR) approach that leverages abundant GPS speed data alongside sparse flow data to estimate traffic flow in large-scale urban networks. The method formulates a constrained optimization framework that utilizes a quadratic objective function with l2 norm regularization terms to address the traffic flow recovery problem effectively and incorporates a Lagrangian relaxation technique to maintain non-negativity constraints. The effectiveness of this approach was validated in a large urban network in Shenzhen's Futian District using the Simulation of Urban MObility (SUMO) platform. Analytical results indicate that the method achieves low estimation errors, affirming its suitability for comprehensive traffic analysis in urban settings with limited sensor deployment.         ",
    "url": "https://arxiv.org/abs/2409.03906",
    "authors": [
      "Sicheng Fu",
      "Haotian Shi",
      "Shixiao Liang",
      "Xin Wang",
      "Bin Ran"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.04747",
    "title": "Explicit Mutual Information Maximization for Self-Supervised Learning",
    "abstract": "           Recently, self-supervised learning (SSL) has been extensively studied. Theoretically, mutual information maximization (MIM) is an optimal criterion for SSL, with a strong theoretical foundation in information theory. However, it is difficult to directly apply MIM in SSL since the data distribution is not analytically available in applications. In practice, many existing methods can be viewed as approximate implementations of the MIM criterion. This work shows that, based on the invariance property of MI, explicit MI maximization can be applied to SSL under a generic distribution assumption, i.e., a relaxed condition of the data distribution. We further illustrate this by analyzing the generalized Gaussian distribution. Based on this result, we derive a loss function based on the MIM criterion using only second-order statistics. We implement the new loss for SSL and demonstrate its effectiveness via extensive experiments.         ",
    "url": "https://arxiv.org/abs/2409.04747",
    "authors": [
      "Lele Chang",
      "Peilin Liu",
      "Qinghai Guo",
      "Fei Wen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.05587",
    "title": "DSDFormer: An Innovative Transformer-Mamba Framework for Robust High-Precision Driver Distraction Identification",
    "abstract": "           Driver distraction remains a leading cause of traffic accidents, posing a critical threat to road safety globally. As intelligent transportation systems evolve, accurate and real-time identification of driver distraction has become essential. However, existing methods struggle to capture both global contextual and fine-grained local features while contending with noisy labels in training datasets. To address these challenges, we propose DSDFormer, a novel framework that integrates the strengths of Transformer and Mamba architectures through a Dual State Domain Attention (DSDA) mechanism, enabling a balance between long-range dependencies and detailed feature extraction for robust driver behavior recognition. Additionally, we introduce Temporal Reasoning Confident Learning (TRCL), an unsupervised approach that refines noisy labels by leveraging spatiotemporal correlations in video sequences. Our model achieves state-of-the-art performance on the AUC-V1, AUC-V2, and 100-Driver datasets and demonstrates real-time processing efficiency on the NVIDIA Jetson AGX Orin platform. Extensive experimental results confirm that DSDFormer and TRCL significantly improve both the accuracy and robustness of driver distraction detection, offering a scalable solution to enhance road safety.         ",
    "url": "https://arxiv.org/abs/2409.05587",
    "authors": [
      "Junzhou Chen",
      "Zirui Zhang",
      "Jing Yu",
      "Heqiang Huang",
      "Ronghui Zhang",
      "Xuemiao Xu",
      "Bin Sheng",
      "Hong Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.06002",
    "title": "Enhanced Generative Data Augmentation for Semantic Segmentation via Stronger Guidance",
    "abstract": "           Data augmentation is a widely used technique for creating training data for tasks that require labeled data, such as semantic segmentation. This method benefits pixel-wise annotation tasks requiring much effort and intensive labor. Traditional data augmentation methods involve simple transformations like rotations and flips to create new images from existing ones. However, these new images may lack diversity along the main semantic axes in the data and not change high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable generative models offer a way to augment data for semantic segmentation tasks using a prompt and visual reference from the original image. However, using these models directly presents challenges, such as creating an effective prompt and visual reference to generate a synthetic image that accurately reflects the content and structure of the original. In this work, we introduce an effective data augmentation method for semantic segmentation using the Controllable Diffusion Model. Our proposed method includes efficient prompt generation using Class-Prompt Appending and Visual Prior Combination to enhance attention to labeled classes in real images. These techniques allow us to generate images that accurately depict segmented classes in the real image. In addition, we employ the class balancing algorithm to ensure efficiency when merging the synthetic and original images to generate balanced data for the training dataset. We evaluated our method on the PASCAL VOC datasets and found it highly effective for synthesizing images in semantic segmentation.         ",
    "url": "https://arxiv.org/abs/2409.06002",
    "authors": [
      "Quang-Huy Che",
      "Duc-Tri Le",
      "Vinh-Tiep Nguyen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.06259",
    "title": "ALSS-YOLO: An Adaptive Lightweight Channel Split and Shuffling Network for TIR Wildlife Detection in UAV Imagery",
    "abstract": "           Unmanned aerial vehicles (UAVs) equipped with thermal infrared (TIR) cameras play a crucial role in combating nocturnal wildlife poaching. However, TIR images often face challenges such as jitter, and wildlife overlap, necessitating UAVs to possess the capability to identify blurred and overlapping small targets. Current traditional lightweight networks deployed on UAVs struggle to extract features from blurry small targets. To address this issue, we developed ALSS-YOLO, an efficient and lightweight detector optimized for TIR aerial images. Firstly, we propose a novel Adaptive Lightweight Channel Split and Shuffling (ALSS) module. This module employs an adaptive channel split strategy to optimize feature extraction and integrates a channel shuffling mechanism to enhance information exchange between channels. This improves the extraction of blurry features, crucial for handling jitter-induced blur and overlapping targets. Secondly, we developed a Lightweight Coordinate Attention (LCA) module that employs adaptive pooling and grouped convolution to integrate feature information across dimensions. This module ensures lightweight operation while maintaining high detection precision and robustness against jitter and target overlap. Additionally, we developed a single-channel focus module to aggregate the width and height information of each channel into four-dimensional channel fusion, which improves the feature representation efficiency of infrared images. Finally, we modify the localization loss function to emphasize the loss value associated with small objects to improve localization accuracy. Extensive experiments on the BIRDSAI and ISOD TIR UAV wildlife datasets show that ALSS-YOLO achieves state-of-the-art performance, Our code is openly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.06259",
    "authors": [
      "Ang He",
      "Xiaobo Li",
      "Ximei Wu",
      "Chengyue Su",
      "Jing Chen",
      "Sheng Xu",
      "Xiaobin Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.06402",
    "title": "Symmetry Breaking in Neural Network Optimization: Insights from Input Dimension Expansion",
    "abstract": "           Understanding the mechanisms behind neural network optimization is crucial for improving network design and performance. While various optimization techniques have been developed, a comprehensive understanding of the underlying principles that govern these techniques remains elusive. Specifically, the role of symmetry breaking, a fundamental concept in physics, has not been fully explored in neural network optimization. This gap in knowledge limits our ability to design networks that are both efficient and effective. Here, we propose the symmetry breaking hypothesis to elucidate the significance of symmetry breaking in enhancing neural network optimization. We demonstrate that a simple input expansion can significantly improve network performance across various tasks, and we show that this improvement can be attributed to the underlying symmetry breaking mechanism. We further develop a metric to quantify the degree of symmetry breaking in neural networks, providing a practical approach to evaluate and guide network design. Our findings confirm that symmetry breaking is a fundamental principle that underpins various optimization techniques, including dropout, batch normalization, and equivariance. By quantifying the degree of symmetry breaking, our work offers a practical technique for performance enhancement and a metric to guide network design without the need for complete datasets and extensive training processes.         ",
    "url": "https://arxiv.org/abs/2409.06402",
    "authors": [
      "Jun-Jie Zhang",
      "Nan Cheng",
      "Fu-Peng Li",
      "Xiu-Cheng Wang",
      "Jian-Nan Chen",
      "Long-Gang Pang",
      "Deyu Meng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Mathematical Physics (math-ph)"
    ]
  },
  {
    "id": "arXiv:2409.07333",
    "title": "Joint Energy and SINR Coverage Probability in UAV Corridor-assisted RF-powered IoT Networks",
    "abstract": "           This letter studies the joint energy and signal-to-interference-plus-noise (SINR)-based coverage probability in Unmanned Aerial Vehicle (UAV)-assisted radio frequency (RF)-powered Internet of Things (IoT) networks. The UAVs are spatially distributed in an aerial corridor that is modeled as a one-dimensional (1D) binomial point process (BPP). By accurately capturing the line-of-sight (LoS) probability of a UAV through large-scale fading i) an exact form expression for the energy coverage probability is derived, and ii) a tight approximation for the overall coverage performance is obtained. Among several key findings, numerical results reveal the optimal number of deployed UAV-BSs that maximizes the joint coverage probability, as well as the optimal length of the UAV corridors when designing such UAV-assisted IoT networks.         ",
    "url": "https://arxiv.org/abs/2409.07333",
    "authors": [
      "Harris K. Armeniakos",
      "Petros S. Bithas",
      "Konstantinos Maliatsos",
      "Athanasios G. Kanatas"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2409.07409",
    "title": "Robust Robot Walker: Learning Agile Locomotion over Tiny Traps",
    "abstract": "           Quadruped robots must exhibit robust walking capabilities in practical applications. In this work, we propose a novel approach that enables quadruped robots to pass various small obstacles, or \"tiny traps\". Existing methods often rely on exteroceptive sensors, which can be unreliable for detecting such tiny traps. To overcome this limitation, our approach focuses solely on proprioceptive inputs. We introduce a two-stage training framework incorporating a contact encoder and a classification head to learn implicit representations of different traps. Additionally, we design a set of tailored reward functions to improve both the stability of training and the ease of deployment for goal-tracking tasks. To benefit further research, we design a new benchmark for tiny trap task. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness and robustness of our method. Project Page: this https URL ",
    "url": "https://arxiv.org/abs/2409.07409",
    "authors": [
      "Shaoting Zhu",
      "Runhan Huang",
      "Linzhan Mou",
      "Hang Zhao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2108.01240",
    "title": "Dynamic Prediction Model for NOx Emission of SCR System Based on Hybrid Data-driven Algorithms",
    "abstract": "           Aiming at the problem that delay time is difficult to determine and prediction accuracy is low in building prediction model of SCR system, a dynamic modeling scheme based on a hybrid of multiple data-driven algorithms was proposed. First, processed abnormal values and normalized the data. To improve the relevance of the input data, used MIC to estimate delay time and reconstructed production data. Then used combined feature selection method to determine input variables. To further mine data information, VMD was used to decompose input time series. Finally, established NOx emission prediction model combining ELM and EC model. Experimental results based on actual historical operating data show that the MAPE of predicted results is 2.61%. Model sensitivity analysis shows that besides the amount of ammonia injection, the inlet oxygen concentration and the flue gas temperature have a significant impact on NOx emission, which should be considered in SCR process control and optimization.         ",
    "url": "https://arxiv.org/abs/2108.01240",
    "authors": [
      "Zhenhao Tang",
      "Shikui Wang",
      "Shengxian Cao",
      "Yang Li",
      "Tao Shen"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2304.01823",
    "title": "The structure of quasi-transitive graphs avoiding a minor with applications to the domino problem",
    "abstract": "           An infinite graph is quasi-transitive if its vertex set has finitely many orbits under the action of its automorphism group. In this paper we obtain a structure theorem for locally finite quasi-transitive graphs avoiding a minor, which is reminiscent of the Robertson-Seymour Graph Minor Structure Theorem. We prove that every locally finite quasi-transitive graph $G$ avoiding a minor has a tree-decomposition whose torsos are finite or planar; moreover the tree-decomposition is canonical, i.e. invariant under the action of the automorphism group of $G$. As applications of this result, we prove the following. * Every locally finite quasi-transitive graph attains its Hadwiger number, that is, if such a graph contains arbitrarily large clique minors, then it contains an infinite clique minor. This extends a result of Thomassen (1992) who proved it in the 4-connected case and suggested that this assumption could be omitted. * Locally finite quasi-transitive graphs avoiding a minor are accessible (in the sense of Thomassen and Woess), which extends known results on planar graphs to any proper minor-closed family. * Minor-excluded finitely generated groups are accessible (in the group-theoretic sense) and finitely presented, which extends classical results on planar groups. * The domino problem is decidable in a minor-excluded finitely generated group if and only if the group is virtually free, which proves the minor-excluded case of a conjecture of Ballier and Stein (2018).         ",
    "url": "https://arxiv.org/abs/2304.01823",
    "authors": [
      "Louis Esperet",
      "Ugo Giocanti",
      "Cl\u00e9ment Legrand-Duchesne"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Dynamical Systems (math.DS)",
      "Group Theory (math.GR)"
    ]
  },
  {
    "id": "arXiv:2403.11694",
    "title": "Object Segmentation-Assisted Inter Prediction for Versatile Video Coding",
    "abstract": "           In modern video coding standards, block-based inter prediction is widely adopted, which brings high compression efficiency. However, in natural videos, there are usually multiple moving objects of arbitrary shapes, resulting in complex motion fields that are difficult to represent compactly. This problem has been tackled by more flexible block partitioning methods in the Versatile Video Coding (VVC) standard, but the more flexible partitions require more overhead bits to signal and still cannot be made arbitrarily shaped. To address this limitation, we propose an object segmentation-assisted inter prediction method (SAIP), where objects in the reference frames are segmented by some advanced technologies. With a proper indication, the object segmentation mask is translated from the reference frame to the current frame as the arbitrary-shaped partition of different regions without any extra signal. Using the segmentation mask, motion compensation is separately performed for different regions, achieving higher prediction accuracy. The segmentation mask is further used to code the motion vectors of different regions more efficiently. Moreover, the segmentation mask is considered in the joint rate-distortion optimization for motion estimation and partition estimation to derive the motion vector of different regions and partition more accurately. The proposed method is implemented into the VVC reference software, VTM version 12.0. Experimental results show that the proposed method achieves up to 1.98%, 1.14%, 0.79%, and on average 0.82%, 0.49%, 0.37% BD-rate reduction for common test sequences, under the Low-delay P, Low-delay B, and Random Access configurations, respectively.         ",
    "url": "https://arxiv.org/abs/2403.11694",
    "authors": [
      "Zhuoyuan Li",
      "Zikun Yuan",
      "Li Li",
      "Dong Liu",
      "Xiaohu Tang",
      "Feng Wu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.15176",
    "title": "Brain-aligning of semantic vectors improves neural decoding of visual stimuli",
    "abstract": "           The development of algorithms to accurately decode of neural information is a long-standing effort in the field of neuroscience. Brain decoding is typically employed by training machine learning models to map neural data onto a preestablished vector representation of stimulus features. These vectors are usually derived from image- and/or text-based feature spaces. Nonetheless, the intrinsic characteristics of these vectors might be fundamentally different than those encoded by the brain, limiting the ability of algorithms to accurately learn this mapping. To address this issue, here, we propose a representation learning framework, called brain-aligning of semantic vectors, that fine-tunes pretrained feature vectors to better align with the structure of neural representations of visual stimuli in the human brain. We trained this model with functional magnetic resonance imaging (fMRI) data representing 150 visual stimulus categories; then, we performed zero-shot brain decoding on 1) fMRI, 2) magnetoencephalography (MEG), and 3) electrocorticography (ECoG) data reflecting neural representations of visual stimuli. By using fMRI-based brain-aligned vectors, the zero-shot decoding accuracy all three neuroimaging datasets increased. This finding underscores the potential of leveraging a richer array of brainderived features to increase the performance of brain decoding algorithms.         ",
    "url": "https://arxiv.org/abs/2403.15176",
    "authors": [
      "Shirin Vafaei",
      "Ryohei Fukuma",
      "Takufumi Yanagisawa",
      "Huixiang Yang",
      "Satoru Oshino",
      "Naoki Tani",
      "Hui Ming Khoo",
      "Hidenori Sugano",
      "Yasushi Iimura",
      "Hiroharu Suzuki",
      "Madoka Nakajima",
      "Kentaro Tamura",
      "Haruhiko Kishima"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.09845",
    "title": "Towards understanding epoch-wise double descent in two-layer linear neural networks",
    "abstract": "           Epoch-wise double descent is the phenomenon where generalisation performance improves beyond the point of overfitting, resulting in a generalisation curve exhibiting two descents under the course of learning. Understanding the mechanisms driving this behaviour is crucial not only for understanding the generalisation behaviour of machine learning models in general, but also for employing conventional selection methods, such as the use of early stopping to mitigate overfitting. While we ultimately want to draw conclusions of more complex models, such as deep neural networks, a majority of theoretical results regarding the underlying cause of epoch-wise double descent are based on simple models, such as standard linear regression. In this paper, to take a step towards more complex models in theoretical analysis, we study epoch-wise double descent in two-layer linear neural networks. First, we derive a gradient flow for the linear two-layer model, that bridges the learning dynamics of the standard linear regression model, and the linear two-layer diagonal network with quadratic weights. Second, we identify additional factors of epoch-wise double descent emerging with the extra model layer, by deriving necessary conditions for the generalisation error to follow a double descent pattern. While epoch-wise double descent in linear regression has been attributed to differences in input variance, in the two-layer model, also the singular values of the input-output covariance matrix play an important role. This opens up for further questions regarding unidentified factors of epoch-wise double descent for truly deep models.         ",
    "url": "https://arxiv.org/abs/2407.09845",
    "authors": [
      "Amanda Olmin",
      "Fredrik Lindsten"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.04967",
    "title": "ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild",
    "abstract": "           The growing prominence of the field of audio deepfake detection is driven by its wide range of applications, notably in protecting the public from potential fraud and other malicious activities, prompting the need for greater attention and research in this area. The ADD 2023 challenge goes beyond binary real/fake classification by emulating real-world scenarios, such as the identification of manipulated intervals in partially fake audio and determining the source responsible for generating any fake audio, both with real-life implications, notably in audio forensics, law enforcement, and construction of reliable and trustworthy evidence. To further foster research in this area, in this article, we describe the dataset that was used in the fake game, manipulation region location and deepfake algorithm recognition tracks of the challenge. We also focus on the analysis of the technical methodologies by the top-performing participants in each task and note the commonalities and differences in their approaches. Finally, we discuss the current technical limitations as identified through the technical analysis, and provide a roadmap for future research directions. The dataset is available for download at this http URL.         ",
    "url": "https://arxiv.org/abs/2408.04967",
    "authors": [
      "Jiangyan Yi",
      "Chu Yuan Zhang",
      "Jianhua Tao",
      "Chenglong Wang",
      "Xinrui Yan",
      "Yong Ren",
      "Hao Gu",
      "Junzuo Zhou"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2408.05373",
    "title": "Evolutionary mechanisms that promote cooperation may not promote social welfare",
    "abstract": "           Understanding the emergence of prosocial behaviours among self-interested individuals is an important problem in many scientific disciplines. Various mechanisms have been proposed to explain the evolution of such behaviours, primarily seeking the conditions under which a given mechanism can induce highest levels of cooperation. As these mechanisms usually involve costs that alter individual payoffs, it is however possible that aiming for highest levels of cooperation might be detrimental for social welfare -- the later broadly defined as the total population payoff, taking into account all costs involved for inducing increased prosocial behaviours. Herein, by comparatively analysing the social welfare and cooperation levels obtained from stochastic evolutionary models of two well-established mechanisms of prosocial behaviour, namely, peer and institutional incentives, we demonstrate exactly that. We show that the objectives of maximising cooperation levels and the objectives of maximising social welfare are often misaligned. We argue for the need of adopting social welfare as the main optimisation objective when designing and implementing evolutionary mechanisms for social and collective goods.         ",
    "url": "https://arxiv.org/abs/2408.05373",
    "authors": [
      "Anh Han",
      "Manh Hong Duong",
      "Matjaz Perc"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ]
  },
  {
    "id": "arXiv:2408.09532",
    "title": "Deep Limit Model-free Prediction in Regression",
    "abstract": "           In this paper, we provide a novel Model-free approach based on Deep Neural Network (DNN) to accomplish point prediction and prediction interval under a general regression setting. Usually, people rely on parametric or non-parametric models to bridge dependent and independent variables (Y and X). However, this classical method relies heavily on the correct model specification. Even for the non-parametric approach, some additive form is often assumed. A newly proposed Model-free prediction principle sheds light on a prediction procedure without any model assumption. Previous work regarding this principle has shown better performance than other standard alternatives. Recently, DNN, one of the machine learning methods, has received increasing attention due to its great performance in practice. Guided by the Model-free prediction idea, we attempt to apply a fully connected forward DNN to map X and some appropriate reference random variable Z to Y. The targeted DNN is trained by minimizing a specially designed loss function so that the randomness of Y conditional on X is outsourced to Z through the trained DNN. Our method is more stable and accurate compared to other DNN-based counterparts, especially for optimal point predictions. With a specific prediction procedure, our prediction interval can capture the estimation variability so that it can render a better coverage rate for finite sample cases. The superior performance of our method is verified by simulation and empirical studies.         ",
    "url": "https://arxiv.org/abs/2408.09532",
    "authors": [
      "Kejin Wu",
      "Dimitris N. Politis"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  }
]