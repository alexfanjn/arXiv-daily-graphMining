[
  {
    "id": "arXiv:2409.17158",
    "title": "Cross Dataset Analysis and Network Architecture Repair for Autonomous Car Lane Detection",
    "abstract": "           Transfer Learning has become one of the standard methods to solve problems to overcome the isolated learning paradigm by utilizing knowledge acquired for one task to solve another related one. However, research needs to be done, to identify the initial steps before inducing transfer learning to applications for further verification and explainablity. In this research, we have performed cross dataset analysis and network architecture repair for the lane detection application in autonomous vehicles. Lane detection is an important aspect of autonomous vehicles driving assistance system. In most circumstances, modern deep-learning-based lane recognition systems are successful, but they struggle with lanes with complex topologies. The proposed architecture, ERFCondLaneNet is an enhancement to the CondlaneNet used for lane identification framework to solve the difficulty of detecting lane lines with complex topologies like dense, curved and fork lines. The newly proposed technique was tested on two common lane detecting benchmarks, CULane and CurveLanes respectively, and two different backbones, ResNet and ERFNet. The researched technique with ERFCondLaneNet, exhibited similar performance in comparison to ResnetCondLaneNet, while using 33% less features, resulting in a reduction of model size by 46%.         ",
    "url": "https://arxiv.org/abs/2409.17158",
    "authors": [
      "Parth Ganeriwala",
      "Siddhartha Bhattacharyya",
      "Raja Muthalagu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17173",
    "title": "A Multiple-Fill-in-the-Blank Exam Approach for Enhancing Zero-Resource Hallucination Detection in Large Language Models",
    "abstract": "           Large language models (LLMs) often fabricate a hallucinatory text. Several methods have been developed to detect such text by semantically comparing it with the multiple versions probabilistically regenerated. However, a significant issue is that if the storyline of each regenerated text changes, the generated texts become incomparable, which worsen detection accuracy. In this paper, we propose a hallucination detection method that incorporates a multiple-fill-in-the-blank exam approach to address this storyline-changing issue. First, our method creates a multiple-fill-in-the-blank exam by masking multiple objects from the original text. Second, prompts an LLM to repeatedly answer this exam. This approach ensures that the storylines of the exam answers align with the original ones. Finally, quantifies the degree of hallucination for each original sentence by scoring the exam answers, considering the potential for \\emph{hallucination snowballing} within the original text itself. Experimental results show that our method alone not only outperforms existing methods, but also achieves clearer state-of-the-art performance in the ensembles with existing methods.         ",
    "url": "https://arxiv.org/abs/2409.17173",
    "authors": [
      "Satoshi Munakata",
      "Taku Fukui",
      "Takao Mohri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17176",
    "title": "XDC Gasless Subnet: Gasless Subnet Staking dApp for XDC Network",
    "abstract": "           With a delegated proof-of-stake (XDPoS) consensus mechanism, the XDC Network is an enterprise-focused blockchain platform that combines the strength of public and private blockchains to provide quick transaction times, low energy consumption, and economical gas fees. XDC is designed for interoperability and supports decentralized apps (dApps) and integrates smoothly with financial systems. It is perfect for trade financing and tokenisation of physical assets because of its emphasis on security and scalability. However, there are a few critical issues that hamper wider acceptance and usability for certain high-frequency applications. This whitepaper introduces a novel and enthralling dApp for establishing a gasless subnet in which mainnet XDC can be staked to spin off a subnet that functions similarly to a non-crypto network, accepting currency fees on the XDC network. This would allow users to stake their tokens without incurring gas fees making the staking process more efficient, cost-effective, and simultaneously enhancing scalability. Performance evaluation of the dApp shows promising results in terms of throughput, latency, scalability, security, and cost efficiency. The use cases and applications of this approach along with challenges and ensuing solutions are included.         ",
    "url": "https://arxiv.org/abs/2409.17176",
    "authors": [
      "Mohuya Chakraborty",
      "Atul Khekade"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.17191",
    "title": "An Effective, Robust and Fairness-aware Hate Speech Detection Framework",
    "abstract": "           With the widespread online social networks, hate speeches are spreading faster and causing more damage than ever before. Existing hate speech detection methods have limitations in several aspects, such as handling data insufficiency, estimating model uncertainty, improving robustness against malicious attacks, and handling unintended bias (i.e., fairness). There is an urgent need for accurate, robust, and fair hate speech classification in online social networks. To bridge the gap, we design a data-augmented, fairness addressed, and uncertainty estimated novel framework. As parts of the framework, we propose Bidirectional Quaternion-Quasi-LSTM layers to balance effectiveness and efficiency. To build a generalized model, we combine five datasets collected from three platforms. Experiment results show that our model outperforms eight state-of-the-art methods under both no attack scenario and various attack scenarios, indicating the effectiveness and robustness of our model. We share our code along with combined dataset for better future research         ",
    "url": "https://arxiv.org/abs/2409.17191",
    "authors": [
      "Guanyi Mou",
      "Kyumin Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17192",
    "title": "Constrain Path Optimization on Time-Dependent Road Networks",
    "abstract": "           Time-Dependent Constrained Path Optimization (TD-CPO) takes the following input: (i) time-dependent (TD) road network, (ii) source ($s$), (iii) destination ($d$), (iv) departure time ($t$) and, (v) budget ($\\mathcal{B}$). In TD graph, each edge is characterized by a time-dependent arrival time and a score function. TD-CPO aims to determine a loopless path $s$--$d$ departing from $s$ at time $t$ and arriving at $d$ on or before $t+\\mathcal{B}$ while maximizing the score. TD-CPO has applications in urban navigation. TD-CPO is a variant of the Arc Orienteering Problem (AOP) known to be NP-hard in nature. The key computational challenge of TD-CPO is that we need to find the \"longest path\" in terms of score within the given budget constraint in a TD graph. Current works prune down the search space very aggressively. Thus, despite having low execution time, these algorithms often produce low-quality solutions. In contrast, our proposed approach $\\mathcal{SCOPE}$ efficiently solves TD-CPO by exploiting road networks' spatial and temporal properties. The inherent computational structure of $\\mathcal{SCOPE}$ enables trivial parallelization for improved performance. Our experiments indicate that $\\mathcal{SCOPE}$ produces superior quality solutions (nearly $2x$) compared to the state-of-the-art algorithm while having comparable running times. Furthermore, $\\mathcal{SCOPE}$ exhibits almost linear speedup as the number of CPUs (cores) increases (up to 24 CPUs).         ",
    "url": "https://arxiv.org/abs/2409.17192",
    "authors": [
      "Kousik Kumar Dutta",
      "Venkata M. V. Gunturi"
    ],
    "subjectives": [
      "Other Computer Science (cs.OH)"
    ]
  },
  {
    "id": "arXiv:2409.17203",
    "title": "AACLiteNet: A Lightweight Model for Detection of Fine-Grained Abdominal Aortic Calcification",
    "abstract": "           Cardiovascular Diseases (CVDs) are the leading cause of death worldwide, taking 17.9 million lives annually. Abdominal Aortic Calcification (AAC) is an established marker for CVD, which can be observed in lateral view Vertebral Fracture Assessment (VFA) scans, usually done for vertebral fracture detection. Early detection of AAC may help reduce the risk of developing clinical CVDs by encouraging preventive measures. Manual analysis of VFA scans for AAC measurement is time consuming and requires trained human assessors. Recently, efforts have been made to automate the process, however, the proposed models are either low in accuracy, lack granular level score prediction, or are too heavy in terms of inference time and memory footprint. Considering all these shortcomings of existing algorithms, we propose 'AACLiteNet', a lightweight deep learning model that predicts both cumulative and granular level AAC scores with high accuracy, and also has a low memory footprint, and computation cost (Floating Point Operations (FLOPs)). The AACLiteNet achieves a significantly improved one-vs-rest average accuracy of 85.94% as compared to the previous best 81.98%, with 19.88 times less computational cost and 2.26 times less memory footprint, making it implementable on portable computing devices.         ",
    "url": "https://arxiv.org/abs/2409.17203",
    "authors": [
      "Zaid Ilyas",
      "Afsah Saleem",
      "David Suter",
      "Siobhan Reid",
      "John Schousboe",
      "William Leslie",
      "Joshua Lewis",
      "Syed Zulqarnain Gilani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17208",
    "title": "2024 BRAVO Challenge Track 1 1st Place Report: Evaluating Robustness of Vision Foundation Models for Semantic Segmentation",
    "abstract": "           In this report, we present our solution for Track 1 of the 2024 BRAVO Challenge, where a model is trained on Cityscapes and its robustness is evaluated on several out-of-distribution datasets. Our solution leverages the powerful representations learned by vision foundation models, by attaching a simple segmentation decoder to DINOv2 and fine-tuning the entire model. This approach outperforms more complex existing approaches, and achieves 1st place in the challenge. Our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17208",
    "authors": [
      "Tommie Kerssies",
      "Daan de Geus",
      "Gijs Dubbelman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.17210",
    "title": "Neural Network Architecture Search Enabled Wide-Deep Learning (NAS-WD) for Spatially Heterogenous Property Awared Chicken Woody Breast Classification and Hardness Regression",
    "abstract": "           Due to intensive genetic selection for rapid growth rates and high broiler yields in recent years, the global poultry industry has faced a challenging problem in the form of woody breast (WB) conditions. This condition has caused significant economic losses as high as $200 million annually, and the root cause of WB has yet to be identified. Human palpation is the most common method of distinguishing a WB from others. However, this method is time-consuming and subjective. Hyperspectral imaging (HSI) combined with machine learning algorithms can evaluate the WB conditions of fillets in a non-invasive, objective, and high-throughput manner. In this study, 250 raw chicken breast fillet samples (normal, mild, severe) were taken, and spatially heterogeneous hardness distribution was first considered when designing HSI processing models. The study not only classified the WB levels from HSI but also built a regression model to correlate the spectral information with sample hardness data. To achieve a satisfactory classification and regression model, a neural network architecture search (NAS) enabled a wide-deep neural network model named NAS-WD, which was developed. In NAS-WD, NAS was first used to automatically optimize the network architecture and hyperparameters. The classification results show that NAS-WD can classify the three WB levels with an overall accuracy of 95%, outperforming the traditional machine learning model, and the regression correlation between the spectral data and hardness was 0.75, which performs significantly better than traditional regression models.         ",
    "url": "https://arxiv.org/abs/2409.17210",
    "authors": [
      "Chaitanya Pallerla",
      "Yihong Feng",
      "Casey M. Owens",
      "Ramesh Bahadur Bist",
      "Siavash Mahmoudi",
      "Pouya Sohrabipour",
      "Amirreza Davar",
      "Dongyi Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2409.17213",
    "title": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles",
    "abstract": "           Recent debates raised concerns that language models may favor certain viewpoints. But what if the solution is not to aim for a 'view from nowhere' but rather to leverage different viewpoints? We introduce Plurals, a system and Python library for pluralistic AI deliberation. Plurals consists of Agents (LLMs, optionally with personas) which deliberate within customizable Structures, with Moderators overseeing deliberation. Plurals is a generator of simulated social ensembles. Plurals integrates with government datasets to create nationally representative personas, includes deliberation templates inspired by democratic deliberation theory, and allows users to customize both information-sharing structures and deliberation behavior within Structures. Six case studies demonstrate fidelity to theoretical constructs and efficacy. Three randomized experiments show simulated focus groups produced output resonant with an online sample of the relevant audiences (chosen over zero-shot generation in 75% of trials). Plurals is both a paradigm and a concrete system for pluralistic AI. The Plurals library is available at this https URL and will be continually updated.         ",
    "url": "https://arxiv.org/abs/2409.17213",
    "authors": [
      "Joshua Ashkinaze",
      "Emily Fry",
      "Narendra Edara",
      "Eric Gilbert",
      "Ceren Budak"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2409.17221",
    "title": "Walker: Self-supervised Multiple Object Tracking by Walking on Temporal Appearance Graphs",
    "abstract": "           The supervision of state-of-the-art multiple object tracking (MOT) methods requires enormous annotation efforts to provide bounding boxes for all frames of all videos, and instance IDs to associate them through time. To this end, we introduce Walker, the first self-supervised tracker that learns from videos with sparse bounding box annotations, and no tracking labels. First, we design a quasi-dense temporal object appearance graph, and propose a novel multi-positive contrastive objective to optimize random walks on the graph and learn instance similarities. Then, we introduce an algorithm to enforce mutually-exclusive connective properties across instances in the graph, optimizing the learned topology for MOT. At inference time, we propose to associate detected instances to tracklets based on the max-likelihood transition state under motion-constrained bi-directional walks. Walker is the first self-supervised tracker to achieve competitive performance on MOT17, DanceTrack, and BDD100K. Remarkably, our proposal outperforms the previous self-supervised trackers even when drastically reducing the annotation requirements by up to 400x.         ",
    "url": "https://arxiv.org/abs/2409.17221",
    "authors": [
      "Mattia Segu",
      "Luigi Piccinelli",
      "Siyuan Li",
      "Luc Van Gool",
      "Fisher Yu",
      "Bernt Schiele"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17262",
    "title": "CROSS-GAiT: Cross-Attention-Based Multimodal Representation Fusion for Parametric Gait Adaptation in Complex Terrains",
    "abstract": "           We present CROSS-GAiT, a novel algorithm for quadruped robots that uses Cross Attention to fuse terrain representations derived from visual and time-series inputs, including linear accelerations, angular velocities, and joint efforts. These fused representations are used to adjust the robot's step height and hip splay, enabling adaptive gaits that respond dynamically to varying terrain conditions. We generate these terrain representations by processing visual inputs through a masked Vision Transformer (ViT) encoder and time-series data through a dilated causal convolutional encoder. The cross-attention mechanism then selects and integrates the most relevant features from each modality, combining terrain characteristics with robot dynamics for better-informed gait adjustments. CROSS-GAiT uses the combined representation to dynamically adjust gait parameters in response to varying and unpredictable terrains. We train CROSS-GAiT on data from diverse terrains, including asphalt, concrete, brick pavements, grass, dense vegetation, pebbles, gravel, and sand. Our algorithm generalizes well and adapts to unseen environmental conditions, enhancing real-time navigation performance. CROSS-GAiT was implemented on a Ghost Robotics Vision 60 robot and extensively tested in complex terrains with high vegetation density, uneven/unstable surfaces, sand banks, deformable substrates, etc. We observe at least a 7.04% reduction in IMU energy density and a 27.3% reduction in total joint effort, which directly correlates with increased stability and reduced energy usage when compared to state-of-the-art methods. Furthermore, CROSS-GAiT demonstrates at least a 64.5% increase in success rate and a 4.91% reduction in time to reach the goal in four complex scenarios. Additionally, the learned representations perform 4.48% better than the state-of-the-art on a terrain classification task.         ",
    "url": "https://arxiv.org/abs/2409.17262",
    "authors": [
      "Gershom Seneviratne",
      "Kasun Weerakoon",
      "Mohamed Elnoor",
      "Vignesh Rajgopal",
      "Harshavarthan Varatharajan",
      "Mohamed Khalid M Jaffar",
      "Jason Pusey",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.17270",
    "title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning",
    "abstract": "           Large Language Models (LLMs) have revolutionized natural language processing, yet they struggle with inconsistent reasoning, particularly in novel domains and complex logical sequences. This research introduces Proof of Thought, a framework that enhances the reliability and transparency of LLM outputs. Our approach bridges LLM-generated ideas with formal logic verification, employing a custom interpreter to convert LLM outputs into First Order Logic constructs for theorem prover scrutiny. Central to our method is an intermediary JSON-based Domain-Specific Language, which by design balances precise logical structures with intuitive human concepts. This hybrid representation enables both rigorous validation and accessible human comprehension of LLM reasoning processes. Key contributions include a robust type system with sort management for enhanced logical integrity, explicit representation of rules for clear distinction between factual and inferential knowledge, and a flexible architecture that allows for easy extension to various domain-specific applications. We demonstrate Proof of Thought's effectiveness through benchmarking on StrategyQA and a novel multimodal reasoning task, showing improved performance in open-ended scenarios. By providing verifiable and interpretable results, our technique addresses critical needs for AI system accountability and sets a foundation for human-in-the-loop oversight in high-stakes domains.         ",
    "url": "https://arxiv.org/abs/2409.17270",
    "authors": [
      "Debargha Ganguly",
      "Srinivasan Iyengar",
      "Vipin Chaudhary",
      "Shivkumar Kalyanaraman"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2409.17275",
    "title": "On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains",
    "abstract": "           Retrieval-Augmented Generation (RAG) has been empirically shown to enhance the performance of large language models (LLMs) in knowledge-intensive domains such as healthcare, finance, and legal contexts. Given a query, RAG retrieves relevant documents from a corpus and integrates them into the LLMs' generation process. In this study, we investigate the adversarial robustness of RAG, focusing specifically on examining the retrieval system. First, across 225 different setup combinations of corpus, retriever, query, and targeted information, we show that retrieval systems are vulnerable to universal poisoning attacks in medical Q\\&A. In such attacks, adversaries generate poisoned documents containing a broad spectrum of targeted information, such as personally identifiable information. When these poisoned documents are inserted into a corpus, they can be accurately retrieved by any users, as long as attacker-specified queries are used. To understand this vulnerability, we discovered that the deviation from the query's embedding to that of the poisoned document tends to follow a pattern in which the high similarity between the poisoned document and the query is retained, thereby enabling precise retrieval. Based on these findings, we develop a new detection-based defense to ensure the safe use of RAG. Through extensive experiments spanning various Q\\&A domains, we observed that our proposed method consistently achieves excellent detection rates in nearly all cases.         ",
    "url": "https://arxiv.org/abs/2409.17275",
    "authors": [
      "Xun Xian",
      "Ganghua Wang",
      "Xuan Bi",
      "Jayanth Srinivasa",
      "Ashish Kundu",
      "Charles Fleming",
      "Mingyi Hong",
      "Jie Ding"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Databases (cs.DB)",
      "Emerging Technologies (cs.ET)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17277",
    "title": "Building Real-time Awareness of Out-of-distribution in Trajectory Prediction for Autonomous Vehicles",
    "abstract": "           Trajectory prediction describes the motions of surrounding moving obstacles for an autonomous vehicle; it plays a crucial role in enabling timely decision-making, such as collision avoidance and trajectory replanning. Accurate trajectory planning is the key to reliable vehicle deployments in open-world environment, where unstructured obstacles bring in uncertainties that are impossible to fully capture by training data. For traditional machine learning tasks, such uncertainties are often addressed reasonably well via methods such as continual learning. On the one hand, naively applying those methods to trajectory prediction can result in continuous data collection and frequent model updates, which can be resource-intensive. On the other hand, the predicted trajectories can be far away from the true trajectories, leading to unsafe decision-making. In this paper, we aim to establish real-time awareness of out-of-distribution in trajectory prediction for autonomous vehicles. We focus on the challenging and practically relevant setting where the out-of-distribution is deceptive, that is, the one not easily detectable by human intuition. Drawing on the well-established techniques of sequential analysis, we build real-time awareness of out-of-distribution by monitoring prediction errors using the quickest change point detection (QCD). Our solutions are lightweight and can handle the occurrence of out-of-distribution at any time during trajectory prediction inference. Experimental results on multiple real-world datasets using a benchmark trajectory prediction model demonstrate the effectiveness of our methods.         ",
    "url": "https://arxiv.org/abs/2409.17277",
    "authors": [
      "Tongfei",
      "Taposh Banerjee",
      "Rui Liu",
      "Lili Su"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17279",
    "title": "SHEATH: Defending Horizontal Collaboration for Distributed CNNs against Adversarial Noise",
    "abstract": "           As edge computing and the Internet of Things (IoT) expand, horizontal collaboration (HC) emerges as a distributed data processing solution for resource-constrained devices. In particular, a convolutional neural network (CNN) model can be deployed on multiple IoT devices, allowing distributed inference execution for image recognition while ensuring model and data privacy. Yet, this distributed architecture remains vulnerable to adversaries who want to make subtle alterations that impact the model, even if they lack access to the entire model. Such vulnerabilities can have severe implications for various sectors, including healthcare, military, and autonomous systems. However, security solutions for these vulnerabilities have not been explored. This paper presents a novel framework for Secure Horizontal Edge with Adversarial Threat Handling (SHEATH) to detect adversarial noise and eliminate its effect on CNN inference by recovering the original feature maps. Specifically, SHEATH aims to address vulnerabilities without requiring complete knowledge of the CNN model in HC edge architectures based on sequential partitioning. It ensures data and model integrity, offering security against adversarial attacks in diverse HC environments. Our evaluations demonstrate SHEATH's adaptability and effectiveness across diverse CNN configurations.         ",
    "url": "https://arxiv.org/abs/2409.17279",
    "authors": [
      "Muneeba Asif",
      "Mohammad Kumail Kazmi",
      "Mohammad Ashiqur Rahman",
      "Syed Rafay Hasan",
      "Soamar Homsi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2409.17283",
    "title": "Investigating Privacy Attacks in the Gray-Box Setting to Enhance Collaborative Learning Schemes",
    "abstract": "           The notion that collaborative machine learning can ensure privacy by just withholding the raw data is widely acknowledged to be flawed. Over the past seven years, the literature has revealed several privacy attacks that enable adversaries to extract information about a model's training dataset by exploiting access to model parameters during or after training. In this work, we study privacy attacks in the gray-box setting, where the attacker has only limited access - in terms of view and actions - to the model. The findings of our investigation provide new insights for the development of privacy-preserving collaborative learning solutions. We deploy SmartCryptNN, a framework that tailors homomorphic encryption to protect the portions of the model posing higher privacy risks. Our solution offers a trade-off between privacy and efficiency, which varies based on the extent and selection of the model components we choose to protect. We explore it on dense neural networks, where through extensive evaluation of diverse datasets and architectures, we uncover instances where a favorable sweet spot in the trade-off can be achieved by safeguarding only a single layer of the network. In one of such instances, our approach trains ~4 times faster compared to fully encrypted solutions, while reducing membership leakage by 17.8 times compared to plaintext solutions.         ",
    "url": "https://arxiv.org/abs/2409.17283",
    "authors": [
      "Federico Mazzone",
      "Ahmad Al Badawi",
      "Yuriy Polyakov",
      "Maarten Everts",
      "Florian Hahn",
      "Andreas Peter"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.17285",
    "title": "SpoofCeleb: Speech Deepfake Detection and SASV In The Wild",
    "abstract": "           This paper introduces SpoofCeleb, a dataset designed for Speech Deepfake Detection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV), utilizing source data from real-world conditions and spoofing attacks generated by Text-To-Speech (TTS) systems also trained on the same real-world data. Robust recognition systems require speech data recorded in varied acoustic environments with different levels of noise to be trained. However, existing datasets typically include clean, high-quality recordings (bona fide data) due to the requirements for TTS training; studio-quality or well-recorded read speech is typically necessary to train TTS models. Existing SDD datasets also have limited usefulness for training SASV models due to insufficient speaker diversity. We present SpoofCeleb, which leverages a fully automated pipeline that processes the VoxCeleb1 dataset, transforming it into a suitable form for TTS training. We subsequently train 23 contemporary TTS systems. The resulting SpoofCeleb dataset comprises over 2.5 million utterances from 1,251 unique speakers, collected under natural, real-world conditions. The dataset includes carefully partitioned training, validation, and evaluation sets with well-controlled experimental protocols. We provide baseline results for both SDD and SASV tasks. All data, protocols, and baselines are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17285",
    "authors": [
      "Jee-weon Jung",
      "Yihan Wu",
      "Xin Wang",
      "Ji-Hoon Kim",
      "Soumi Maiti",
      "Yuta Matsunaga",
      "Hye-jin Shim",
      "Jinchuan Tian",
      "Nicholas Evans",
      "Joon Son Chung",
      "Wangyou Zhang",
      "Seyun Um",
      "Shinnosuke Takamichi",
      "Shinji Watanabe"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2409.17300",
    "title": "Neural Network Plasticity and Loss Sharpness",
    "abstract": "           In recent years, continual learning, a prediction setting in which the problem environment may evolve over time, has become an increasingly popular research field due to the framework's gearing towards complex, non-stationary objectives. Learning such objectives requires plasticity, or the ability of a neural network to adapt its predictions to a different task. Recent findings indicate that plasticity loss on new tasks is highly related to loss landscape sharpness in non-stationary RL frameworks. We explore the usage of sharpness regularization techniques, which seek out smooth minima and have been touted for their generalization capabilities in vanilla prediction settings, in efforts to combat plasticity loss. Our findings indicate that such techniques have no significant effect on reducing plasticity loss.         ",
    "url": "https://arxiv.org/abs/2409.17300",
    "authors": [
      "Max Koster",
      "Jude Kukla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17311",
    "title": "A Hybrid Quantum-Classical AI-Based Detection Strategy for Generative Adversarial Network-Based Deepfake Attacks on an Autonomous Vehicle Traffic Sign Classification System",
    "abstract": "           The perception module in autonomous vehicles (AVs) relies heavily on deep learning-based models to detect and identify various objects in their surrounding environment. An AV traffic sign classification system is integral to this module, which helps AVs recognize roadway traffic signs. However, adversarial attacks, in which an attacker modifies or alters the image captured for traffic sign recognition, could lead an AV to misrecognize the traffic signs and cause hazardous consequences. Deepfake presents itself as a promising technology to be used for such adversarial attacks, in which a deepfake traffic sign would replace a real-world traffic sign image before the image is fed to the AV traffic sign classification system. In this study, the authors present how a generative adversarial network-based deepfake attack can be crafted to fool the AV traffic sign classification systems. The authors developed a deepfake traffic sign image detection strategy leveraging hybrid quantum-classical neural networks (NNs). This hybrid approach utilizes amplitude encoding to represent the features of an input traffic sign image using quantum states, which substantially reduces the memory requirement compared to its classical counterparts. The authors evaluated this hybrid deepfake detection approach along with several baseline classical convolutional NNs on real-world and deepfake traffic sign images. The results indicate that the hybrid quantum-classical NNs for deepfake detection could achieve similar or higher performance than the baseline classical convolutional NNs in most cases while requiring less than one-third of the memory required by the shallowest classical convolutional NN considered in this study.         ",
    "url": "https://arxiv.org/abs/2409.17311",
    "authors": [
      "M Sabbir Salek",
      "Shaozhi Li",
      "Mashrur Chowdhury"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2409.17315",
    "title": "KIPPS: Knowledge infusion in Privacy Preserving Synthetic Data Generation",
    "abstract": "           The integration of privacy measures, including differential privacy techniques, ensures a provable privacy guarantee for the synthetic data. However, challenges arise for Generative Deep Learning models when tasked with generating realistic data, especially in critical domains such as Cybersecurity and Healthcare. Generative Models optimized for continuous data struggle to model discrete and non-Gaussian features that have domain constraints. Challenges increase when the training datasets are limited and not diverse. In such cases, generative models create synthetic data that repeats sensitive features, which is a privacy risk. Moreover, generative models face difficulties comprehending attribute constraints in specialized domains. This leads to the generation of unrealistic data that impacts downstream accuracy. To address these issues, this paper proposes a novel model, KIPPS, that infuses Domain and Regulatory Knowledge from Knowledge Graphs into Generative Deep Learning models for enhanced Privacy Preserving Synthetic data generation. The novel framework augments the training of generative models with supplementary context about attribute values and enforces domain constraints during training. This added guidance enhances the model's capacity to generate realistic and domain-compliant synthetic data. The proposed model is evaluated on real-world datasets, specifically in the domains of Cybersecurity and Healthcare, where domain constraints and rules add to the complexity of the data. Our experiments evaluate the privacy resilience and downstream accuracy of the model against benchmark methods, demonstrating its effectiveness in addressing the balance between privacy preservation and data accuracy in complex domains.         ",
    "url": "https://arxiv.org/abs/2409.17315",
    "authors": [
      "Anantaa Kotal",
      "Anupam Joshi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.17330",
    "title": "VL4AD: Vision-Language Models Improve Pixel-wise Anomaly Detection",
    "abstract": "           Semantic segmentation networks have achieved significant success under the assumption of independent and identically distributed data. However, these networks often struggle to detect anomalies from unknown semantic classes due to the limited set of visual concepts they are typically trained on. To address this issue, anomaly segmentation often involves fine-tuning on outlier samples, necessitating additional efforts for data collection, labeling, and model retraining. Seeking to avoid this cumbersome work, we take a different approach and propose to incorporate Vision-Language (VL) encoders into existing anomaly detectors to leverage the semantically broad VL pre-training for improved outlier awareness. Additionally, we propose a new scoring function that enables data- and training-free outlier supervision via textual prompts. The resulting VL4AD model, which includes max-logit prompt ensembling and a class-merging strategy, achieves competitive performance on widely used benchmark datasets, thereby demonstrating the potential of vision-language models for pixel-wise anomaly detection.         ",
    "url": "https://arxiv.org/abs/2409.17330",
    "authors": [
      "Liangyu Zhong",
      "Joachim Sicking",
      "Fabian H\u00fcger",
      "Hanno Gottschalk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17335",
    "title": "Non-asymptotic Convergence of Training Transformers for Next-token Prediction",
    "abstract": "           Transformers have achieved extraordinary success in modern machine learning due to their excellent ability to handle sequential data, especially in next-token prediction (NTP) tasks. However, the theoretical understanding of their performance in NTP is limited, with existing studies focusing mainly on asymptotic performance. This paper provides a fine-grained non-asymptotic analysis of the training dynamics of a one-layer transformer consisting of a self-attention module followed by a feed-forward layer. We first characterize the essential structural properties of training datasets for NTP using a mathematical framework based on partial orders. Then, we design a two-stage training algorithm, where the pre-processing stage for training the feed-forward layer and the main stage for training the attention layer exhibit fast convergence performance. Specifically, both layers converge sub-linearly to the direction of their corresponding max-margin solutions. We also show that the cross-entropy loss enjoys a linear convergence rate. Furthermore, we show that the trained transformer presents non-trivial prediction ability with dataset shift, which sheds light on the remarkable generalization performance of transformers. Our analysis technique involves the development of novel properties on the attention gradient and further in-depth analysis of how these properties contribute to the convergence of the training process. Our experiments further validate our theoretical findings.         ",
    "url": "https://arxiv.org/abs/2409.17335",
    "authors": [
      "Ruiquan Huang",
      "Yingbin Liang",
      "Jing Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2409.17340",
    "title": "Koopman-driven grip force prediction through EMG sensing",
    "abstract": "           Loss of hand function due to conditions like stroke or multiple sclerosis significantly impacts daily activities. Robotic rehabilitation provides tools to restore hand function, while novel methods based on surface electromyography (sEMG) enable the adaptation of the device's force output according to the user's condition, thereby improving rehabilitation outcomes. This study aims to achieve accurate force estimations during medium wrap grasps using a single sEMG sensor pair, thereby addressing the challenge of escalating sensor requirements for precise predictions. We conducted sEMG measurements on 13 subjects at two forearm positions, validating results with a hand dynamometer. We established flexible signal-processing steps, yielding high peak cross-correlations between the processed sEMG signal (representing meaningful muscle activity) and grip force. Influential parameters were subsequently identified through sensitivity analysis. Leveraging a novel data-driven Koopman operator theory-based approach and problem-specific data lifting techniques, we devised a methodology for the estimation and short-term prediction of grip force from processed sEMG signals. A weighted mean absolute percentage error (wMAPE) of approx. 5.5% was achieved for the estimated grip force, whereas predictions with a 0.5-second prediction horizon resulted in a wMAPE of approx. 17.9%. The methodology proved robust regarding precise electrode positioning, as the effect of sensing position on error metrics was non-significant. The algorithm executes exceptionally fast, processing, estimating, and predicting a 0.5-second sEMG signal batch in just approx. 30 ms, facilitating real-time implementation.         ",
    "url": "https://arxiv.org/abs/2409.17340",
    "authors": [
      "Tomislav Bazina",
      "Ervin Kamenar",
      "Maria Fonoberova",
      "Igor Mezi\u0107"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2409.17367",
    "title": "Implicit Neural Representations for Simultaneous Reduction and Continuous Reconstruction of Multi-Altitude Climate Data",
    "abstract": "           The world is moving towards clean and renewable energy sources, such as wind energy, in an attempt to reduce greenhouse gas emissions that contribute to global warming. To enhance the analysis and storage of wind data, we introduce a deep learning framework designed to simultaneously enable effective dimensionality reduction and continuous representation of multi-altitude wind data from discrete observations. The framework consists of three key components: dimensionality reduction, cross-modal prediction, and super-resolution. We aim to: (1) improve data resolution across diverse climatic conditions to recover high-resolution details; (2) reduce data dimensionality for more efficient storage of large climate datasets; and (3) enable cross-prediction between wind data measured at different heights. Comprehensive testing confirms that our approach surpasses existing methods in both super-resolution quality and compression efficiency.         ",
    "url": "https://arxiv.org/abs/2409.17367",
    "authors": [
      "Alif Bin Abdul Qayyum",
      "Xihaier Luo",
      "Nathan M. Urban",
      "Xiaoning Qian",
      "Byung-Jun Yoon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17368",
    "title": "EfiMon: A Process Analyser for Granular Power Consumption Prediction",
    "abstract": "           High-performance computing (HPC) and supercomputing are critical in Artificial Intelligence (AI) research, development, and deployment. The extensive use of supercomputers for training complex AI models, which can take from days to months, raises significant concerns about energy consumption and carbon emissions. Traditional methods for estimating the energy consumption of HPC workloads rely on metering reports from computing nodes power supply units, assuming exclusive use of the entire node. This assumption is increasingly untenable with the advent of next-generation supercomputers that share resources to accelerate workloads, as seen in initiatives like Acceleration as a Service (XaaS) and cloud computing. This paper introduces EfiMon, an agnostic and non-invasive tool designed to extract detailed information about process execution, including instructions executed within specific time windows and CPU and RAM usage. Additionally, it captures comprehensive system metrics, such as power consumption reported by CPU sockets and PSUs. This data enables the development of prediction models to estimate the energy consumption of individual processes without requiring isolation. Using a regression-based mathematical model, our tool is able to estimate single processes' power consumption in isolated and shared resource environments. In shared scenarios, the model demonstrates robust performance, deviating by a maximum of 2.2% on Intel-based machines and 4.4% on AMD systems compared to non-shared cases. This significant accuracy showcases EfiMon's potential for enhancing energy accounting in supercomputing, contributing to more efficient and energy-aware optimisation strategies in HPC.         ",
    "url": "https://arxiv.org/abs/2409.17368",
    "authors": [
      "Luis G. Le\u00f3n-Vega",
      "Niccol\u00f2 Tosato",
      "Stefano Cozzini"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2409.17369",
    "title": "Evaluation of Spectrum Sharing Algorithms for Networks with Heterogeneous Wireless Devices",
    "abstract": "           As highlighted in the National Spectrum Strategy, Dynamic Spectrum Access (DSA) is key for enabling 6G networks to meet the increasing demand for spectrum from various, heterogeneous emerging applications. In this paper, we consider heterogeneous wireless networks with multiple 6G base stations (BS) and a limited number of frequency bands available for transmission. Each BS is associated with a geographical location, a coverage area, and a bandwidth requirement. We assume that clients/UEs are within the corresponding BS's coverage area. To avoid interference, we impose that BSs with overlapping coverage areas must use different frequency bands. We address the challenging problem of efficiently allocating contiguous frequency bands to BSs while avoiding interference. Specifically, we define performance metrics that capture the feasibility of the frequency allocation task, the number of BSs that can be allocated within the limited frequency bands, and the amount of resources utilized by the network. Then, we consider five different DSA algorithms that prioritize BSs based on different features - one of these algorithms is known in the graph theory literature as Welsh-Powell graph colouring algorithm - and compare their performance using extensive simulations. Our results show that DSA algorithms that attempt to maximize the chances of obtaining a feasible frequency allocation - which have been widely studied in the literature - tend to under-perform in all other metrics.         ",
    "url": "https://arxiv.org/abs/2409.17369",
    "authors": [
      "Ankit Walishetti",
      "Igor Kadota",
      "Aidan Kim",
      "Colin Ward",
      "Eduardo Gutierrez",
      "Randall Berry"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2409.17370",
    "title": "The Overfocusing Bias of Convolutional Neural Networks: A Saliency-Guided Regularization Approach",
    "abstract": "           Despite transformers being considered as the new standard in computer vision, convolutional neural networks (CNNs) still outperform them in low-data regimes. Nonetheless, CNNs often make decisions based on narrow, specific regions of input images, especially when training data is limited. This behavior can severely compromise the model's generalization capabilities, making it disproportionately dependent on certain features that might not represent the broader context of images. While the conditions leading to this phenomenon remain elusive, the primary intent of this article is to shed light on this observed behavior of neural networks. Our research endeavors to prioritize comprehensive insight and to outline an initial response to this phenomenon. In line with this, we introduce Saliency Guided Dropout (SGDrop), a pioneering regularization approach tailored to address this specific issue. SGDrop utilizes attribution methods on the feature map to identify and then reduce the influence of the most salient features during training. This process encourages the network to diversify its attention and not focus solely on specific standout areas. Our experiments across several visual classification benchmarks validate SGDrop's role in enhancing generalization. Significantly, models incorporating SGDrop display more expansive attributions and neural activity, offering a more comprehensive view of input images in contrast to their traditionally trained counterparts.         ",
    "url": "https://arxiv.org/abs/2409.17370",
    "authors": [
      "David Bertoin",
      "Eduardo Hugo Sanchez",
      "Mehdi Zouitine",
      "Emmanuel Rachelson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17376",
    "title": "Optical Lens Attack on Deep Learning Based Monocular Depth Estimation",
    "abstract": "           Monocular Depth Estimation (MDE) plays a crucial role in vision-based Autonomous Driving (AD) systems. It utilizes a single-camera image to determine the depth of objects, facilitating driving decisions such as braking a few meters in front of a detected obstacle or changing lanes to avoid collision. In this paper, we investigate the security risks associated with monocular vision-based depth estimation algorithms utilized by AD systems. By exploiting the vulnerabilities of MDE and the principles of optical lenses, we introduce LensAttack, a physical attack that involves strategically placing optical lenses on the camera of an autonomous vehicle to manipulate the perceived object depths. LensAttack encompasses two attack formats: concave lens attack and convex lens attack, each utilizing different optical lenses to induce false depth perception. We begin by constructing a mathematical model of our attack, incorporating various attack parameters. Subsequently, we simulate the attack and evaluate its real-world performance in driving scenarios to demonstrate its effect on state-of-the-art MDE models. The results highlight the significant impact of LensAttack on the accuracy of depth estimation in AD systems.         ",
    "url": "https://arxiv.org/abs/2409.17376",
    "authors": [
      "Ce Zhou",
      "Qiben Yan",
      "Daniel Kent",
      "Guangjing Wang",
      "Ziqi Zhang",
      "Hayder Radha"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17379",
    "title": "Decentralized Nonlinear Model Predictive Control for Safe Collision Avoidance in Quadrotor Teams with Limited Detection Range",
    "abstract": "           Multi-quadrotor systems face significant challenges in decentralized control, particularly with safety and coordination under sensing and communication limitations. State-of-the-art methods leverage Control Barrier Functions (CBFs) to provide safety guarantees but often neglect actuation constraints and limited detection range. To address these gaps, we propose a novel decentralized Nonlinear Model Predictive Control (NMPC) that integrates Exponential CBFs (ECBFs) to enhance safety and optimality in multi-quadrotor systems. We provide both conservative and practical minimum bounds of the range that preserve the safety guarantees of the ECBFs. We validate our approach through extensive simulations with up to 10 quadrotors and 20 obstacles, as well as real-world experiments with 3 quadrotors. Results demonstrate the effectiveness of the proposed framework in realistic settings, highlighting its potential for reliable quadrotor teams operations.         ",
    "url": "https://arxiv.org/abs/2409.17379",
    "authors": [
      "Manohari Goarin",
      "Guanrui Li",
      "Alessandro Saviolo",
      "Giuseppe Loianno"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2409.17385",
    "title": "Data-efficient Trajectory Prediction via Coreset Selection",
    "abstract": "           Modern vehicles are equipped with multiple information-collection devices such as sensors and cameras, continuously generating a large volume of raw data. Accurately predicting the trajectories of neighboring vehicles is a vital component in understanding the complex driving environment. Yet, training trajectory prediction models is challenging in two ways. Processing the large-scale data is computation-intensive. Moreover, easy-medium driving scenarios often overwhelmingly dominate the dataset, leaving challenging driving scenarios such as dense traffic under-represented. For example, in the Argoverse motion prediction dataset, there are very few instances with $\\ge 50$ agents, while scenarios with $10 \\thicksim 20$ agents are far more common. In this paper, to mitigate data redundancy in the over-represented driving scenarios and to reduce the bias rooted in the data scarcity of complex ones, we propose a novel data-efficient training method based on coreset selection. This method strategically selects a small but representative subset of data while balancing the proportions of different scenario difficulties. To the best of our knowledge, we are the first to introduce a method capable of effectively condensing large-scale trajectory dataset, while achieving a state-of-the-art compression ratio. Notably, even when using only 50% of the Argoverse dataset, the model can be trained with little to no decline in performance. Moreover, the selected coreset maintains excellent generalization ability.         ",
    "url": "https://arxiv.org/abs/2409.17385",
    "authors": [
      "Ruining Yang",
      "Lili Su"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17386",
    "title": "Beyond Redundancy: Information-aware Unsupervised Multiplex Graph Structure Learning",
    "abstract": "           Unsupervised Multiplex Graph Learning (UMGL) aims to learn node representations on various edge types without manual labeling. However, existing research overlooks a key factor: the reliability of the graph structure. Real-world data often exhibit a complex nature and contain abundant task-irrelevant noise, severely compromising UMGL's performance. Moreover, existing methods primarily rely on contrastive learning to maximize mutual information across different graphs, limiting them to multiplex graph redundant scenarios and failing to capture view-unique task-relevant information. In this paper, we focus on a more realistic and challenging task: to unsupervisedly learn a fused graph from multiple graphs that preserve sufficient task-relevant information while removing task-irrelevant noise. Specifically, our proposed Information-aware Unsupervised Multiplex Graph Fusion framework (InfoMGF) uses graph structure refinement to eliminate irrelevant noise and simultaneously maximizes view-shared and view-unique task-relevant information, thereby tackling the frontier of non-redundant multiplex graph. Theoretical analyses further guarantee the effectiveness of InfoMGF. Comprehensive experiments against various baselines on different downstream tasks demonstrate its superior performance and robustness. Surprisingly, our unsupervised method even beats the sophisticated supervised approaches. The source code and datasets are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17386",
    "authors": [
      "Zhixiang Shen",
      "Shuo Wang",
      "Zhao Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2409.17392",
    "title": "Trading through Earnings Seasons using Self-Supervised Contrastive Representation Learning",
    "abstract": "           Earnings release is a key economic event in the financial markets and crucial for predicting stock movements. Earnings data gives a glimpse into how a company is doing financially and can hint at where its stock might go next. However, the irregularity of its release cycle makes it a challenge to incorporate this data in a medium-frequency algorithmic trading model and the usefulness of this data fades fast after it is released, making it tough for models to stay accurate over time. Addressing this challenge, we introduce the Contrastive Earnings Transformer (CET) model, a self-supervised learning approach rooted in Contrastive Predictive Coding (CPC), aiming to optimise the utilisation of earnings data. To ascertain its effectiveness, we conduct a comparative study of CET against benchmark models across diverse sectors. Our research delves deep into the intricacies of stock data, evaluating how various models, and notably CET, handle the rapidly changing relevance of earnings data over time and over different sectors. The research outcomes shed light on CET's distinct advantage in extrapolating the inherent value of earnings data over time. Its foundation on CPC allows for a nuanced understanding, facilitating consistent stock predictions even as the earnings data ages. This finding about CET presents a fresh approach to better use earnings data in algorithmic trading for predicting stock price trends.         ",
    "url": "https://arxiv.org/abs/2409.17392",
    "authors": [
      "Zhengxin Joseph Ye",
      "Bjoern Schuller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Trading and Market Microstructure (q-fin.TR)"
    ]
  },
  {
    "id": "arXiv:2409.17397",
    "title": "Severity Prediction in Mental Health: LLM-based Creation, Analysis, Evaluation of a Novel Multilingual Dataset",
    "abstract": "           Large Language Models (LLMs) are increasingly integrated into various medical fields, including mental health support systems. However, there is a gap in research regarding the effectiveness of LLMs in non-English mental health support applications. To address this problem, we present a novel multilingual adaptation of widely-used mental health datasets, translated from English into six languages (Greek, Turkish, French, Portuguese, German, and Finnish). This dataset enables a comprehensive evaluation of LLM performance in detecting mental health conditions and assessing their severity across multiple languages. By experimenting with GPT and Llama, we observe considerable variability in performance across languages, despite being evaluated on the same translated dataset. This inconsistency underscores the complexities inherent in multilingual mental health support, where language-specific nuances and mental health data coverage can affect the accuracy of the models. Through comprehensive error analysis, we emphasize the risks of relying exclusively on large language models (LLMs) in medical settings (e.g., their potential to contribute to misdiagnoses). Moreover, our proposed approach offers significant cost savings for multilingual tasks, presenting a major advantage for broad-scale implementation.         ",
    "url": "https://arxiv.org/abs/2409.17397",
    "authors": [
      "Konstantinos Skianis",
      "John Pavlopoulos",
      "A. Seza Do\u011fru\u00f6z"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17400",
    "title": "AgRegNet: A Deep Regression Network for Flower and Fruit Density Estimation, Localization, and Counting in Orchards",
    "abstract": "           One of the major challenges for the agricultural industry today is the uncertainty in manual labor availability and the associated cost. Automated flower and fruit density estimation, localization, and counting could help streamline harvesting, yield estimation, and crop-load management strategies such as flower and fruitlet thinning. This article proposes a deep regression-based network, AgRegNet, to estimate density, count, and location of flower and fruit in tree fruit canopies without explicit object detection or polygon annotation. Inspired by popular U-Net architecture, AgRegNet is a U-shaped network with an encoder-to-decoder skip connection and modified ConvNeXt-T as an encoder feature extractor. AgRegNet can be trained based on information from point annotation and leverages segmentation information and attention modules (spatial and channel) to highlight relevant flower and fruit features while suppressing non-relevant background features. Experimental evaluation in apple flower and fruit canopy images under an unstructured orchard environment showed that AgRegNet achieved promising accuracy as measured by Structural Similarity Index (SSIM), percentage Mean Absolute Error (pMAE) and mean Average Precision (mAP) to estimate flower and fruit density, count, and centroid location, respectively. Specifically, the SSIM, pMAE, and mAP values for flower images were 0.938, 13.7%, and 0.81, respectively. For fruit images, the corresponding values were 0.910, 5.6%, and 0.93. Since the proposed approach relies on information from point annotation, it is suitable for sparsely and densely located objects. This simplified technique will be highly applicable for growers to accurately estimate yields and decide on optimal chemical and mechanical flower thinning practices.         ",
    "url": "https://arxiv.org/abs/2409.17400",
    "authors": [
      "Uddhav Bhattarai",
      "Santosh Bhusal",
      "Qin Zhang",
      "Manoj Karkee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17403",
    "title": "Transient Adversarial 3D Projection Attacks on Object Detection in Autonomous Driving",
    "abstract": "           Object detection is a crucial task in autonomous driving. While existing research has proposed various attacks on object detection, such as those using adversarial patches or stickers, the exploration of projection attacks on 3D surfaces remains largely unexplored. Compared to adversarial patches or stickers, which have fixed adversarial patterns, projection attacks allow for transient modifications to these patterns, enabling a more flexible attack. In this paper, we introduce an adversarial 3D projection attack specifically targeting object detection in autonomous driving scenarios. We frame the attack formulation as an optimization problem, utilizing a combination of color mapping and geometric transformation models. Our results demonstrate the effectiveness of the proposed attack in deceiving YOLOv3 and Mask R-CNN in physical settings. Evaluations conducted in an indoor environment show an attack success rate of up to 100% under low ambient light conditions, highlighting the potential damage of our attack in real-world driving scenarios.         ",
    "url": "https://arxiv.org/abs/2409.17403",
    "authors": [
      "Ce Zhou",
      "Qiben Yan",
      "Sijia Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17414",
    "title": "Uniformly $hp$-stable elements for the elasticity complex",
    "abstract": "           For the discretization of symmetric, divergence-conforming stress tensors in continuum mechanics, we prove inf-sup stability bounds which are uniform in polynomial degree and mesh size for the Hu--Zhang finite element in two dimensions. This is achieved via an explicit construction of a bounded right inverse of the divergence operator, with the crucial component being the construction of bounded Poincar\u00e9 operators for the stress elasticity complex which are polynomial-preserving, in the Bernstein--Gelfand--Gelfand framework of the finite element exterior calculus. We also construct $hp$-bounded projection operators satisfying a commuting diagram property and $hp$-stable Hodge decompositions. Numerical examples are provided.         ",
    "url": "https://arxiv.org/abs/2409.17414",
    "authors": [
      "Francis R. A. Aznaran",
      "Kaibo Hu",
      "Charles Parker"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2409.17419",
    "title": "Pre-Finetuning with Impact Duration Awareness for Stock Movement Prediction",
    "abstract": "           Understanding the duration of news events' impact on the stock market is crucial for effective time-series forecasting, yet this facet is largely overlooked in current research. This paper addresses this research gap by introducing a novel dataset, the Impact Duration Estimation Dataset (IDED), specifically designed to estimate impact duration based on investor opinions. Our research establishes that pre-finetuning language models with IDED can enhance performance in text-based stock movement predictions. In addition, we juxtapose our proposed pre-finetuning task with sentiment analysis pre-finetuning, further affirming the significance of learning impact duration. Our findings highlight the promise of this novel research direction in stock movement prediction, offering a new avenue for financial forecasting. We also provide the IDED and pre-finetuned language models under the CC BY-NC-SA 4.0 license for academic use, fostering further exploration in this field.         ",
    "url": "https://arxiv.org/abs/2409.17419",
    "authors": [
      "Chr-Jr Chiu",
      "Chung-Chi Chen",
      "Hen-Hsen Huang",
      "Hsin-Hsi Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.17427",
    "title": "Stress Detection from Photoplethysmography in a Virtual Reality Environment",
    "abstract": "           Personalized virtual reality exposure therapy is a therapeutic practice that can adapt to an individual patient, leading to better health outcomes. Measuring a patient's mental state to adjust the therapy is a critical but difficult task. Most published studies use subjective methods to estimate a patient's mental state, which can be inaccurate. This article proposes a virtual reality exposure therapy (VRET) platform capable of assessing a patient's mental state using non-intrusive and widely available physiological signals such as photoplethysmography (PPG). In a case study, we evaluate how PPG signals can be used to detect two binary classifications: peaceful and stressful states. Sixteen healthy subjects were exposed to the two VR environments (relaxed and stressful). Using LOSO cross-validation, our best classification model could predict the two states with a 70.6% accuracy which outperforms many more complex approaches.         ",
    "url": "https://arxiv.org/abs/2409.17427",
    "authors": [
      "Athar Mahmoudi-Nejad",
      "Pierre Boulanger",
      "Matthew Guzdial"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2409.17430",
    "title": "A Hierarchical Gradient Tracking Algorithm for Mitigating Subnet-Drift in Fog Learning Networks",
    "abstract": "           Federated learning (FL) encounters scalability challenges when implemented over fog networks that do not follow FL's conventional star topology architecture. Semi-decentralized FL (SD-FL) has proposed a solution for device-to-device (D2D) enabled networks that divides model cooperation into two stages: at the lower stage, D2D communications is employed for local model aggregations within subnetworks (subnets), while the upper stage handles device-server (DS) communications for global model aggregations. However, existing SD-FL schemes are based on gradient diversity assumptions that become performance bottlenecks as data distributions become more heterogeneous. In this work, we develop semi-decentralized gradient tracking (SD-GT), the first SD-FL methodology that removes the need for such assumptions by incorporating tracking terms into device updates for each communication layer. Our analytical characterization of SD-GT reveals upper bounds on convergence for non-convex, convex, and strongly-convex problems. We show how the bounds enable the development of an optimization algorithm that navigates the performance-efficiency trade-off by tuning subnet sampling rate and D2D rounds for each global training interval. Our subsequent numerical evaluations demonstrate that SD-GT obtains substantial improvements in trained model quality and communication cost relative to baselines in SD-FL and gradient tracking on several datasets.         ",
    "url": "https://arxiv.org/abs/2409.17430",
    "authors": [
      "Evan Chen",
      "Shiqiang Wang",
      "Christopher G. Brinton"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2409.17433",
    "title": "HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and Dynamic Workflows",
    "abstract": "           Despite recent advancements in large language models (LLMs), their performance on complex reasoning problems requiring multi-step thinking and combining various skills is still limited. To address this, we propose a novel framework HDFlow for complex reasoning with LLMs that combines fast and slow thinking modes in an adaptive manner. Our approach consists of two key components: 1) a new approach for slow, deliberate reasoning called Dynamic Workflow, which automatically decomposes complex problems into more manageable sub-tasks and dynamically designs a workflow to assemble specialized LLM or symbolic reasoning tools to solve sub-tasks; 2) Hybrid Thinking, a general framework that dynamically combines fast and slow thinking based on problem complexity. Finally, we propose an easy-to-scale method for automatically synthesizing a large-scale dataset of 27K challenging reasoning problems for complex reasoning and a hybrid thinking tuning method that trains smaller LLMs on this dataset to internalize the fast/slow hybrid reasoning strategies. Experiments on four reasoning benchmark datasets demonstrate that our slow thinking with dynamic workflows significantly outperforms Chain-of-Thought, and hybrid thinking achieves the highest accuracy while providing an effective balance between computational efficiency and performance. Fine-tuning using our hybrid thinking approach also significantly boosts the complex reasoning capabilities of open-source language models. The results showcase the promise of slow thinking, dynamic workflows, and hybrid thinking in expanding the frontier of complex problem-solving with LLMs\\footnote{Code and data will be released at \\url{this https URL}.}.         ",
    "url": "https://arxiv.org/abs/2409.17433",
    "authors": [
      "Wenlin Yao",
      "Haitao Mi",
      "Dong Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17440",
    "title": "A Time Series is Worth Five Experts: Heterogeneous Mixture of Experts for Traffic Flow Prediction",
    "abstract": "           Accurate traffic prediction faces significant challenges, necessitating a deep understanding of both temporal and spatial cues and their complex interactions across multiple variables. Recent advancements in traffic prediction systems are primarily due to the development of complex sequence-centric models. However, existing approaches often embed multiple variables and spatial relationships at each time step, which may hinder effective variable-centric learning, ultimately leading to performance degradation in traditional traffic prediction tasks. To overcome these limitations, we introduce variable-centric and prior knowledge-centric modeling techniques. Specifically, we propose a Heterogeneous Mixture of Experts (TITAN) model for traffic flow prediction. TITAN initially consists of three experts focused on sequence-centric modeling. Then, designed a low-rank adaptive method, TITAN simultaneously enables variable-centric modeling. Furthermore, we supervise the gating process using a prior knowledge-centric modeling strategy to ensure accurate routing. Experiments on two public traffic network datasets, METR-LA and PEMS-BAY, demonstrate that TITAN effectively captures variable-centric dependencies while ensuring accurate routing. Consequently, it achieves improvements in all evaluation metrics, ranging from approximately 4.37\\% to 11.53\\%, compared to previous state-of-the-art (SOTA) models. The code is open at \\href{this https URL}{this https URL}.         ",
    "url": "https://arxiv.org/abs/2409.17440",
    "authors": [
      "Guangyu Wang",
      "Yujie Chen",
      "Ming Gao",
      "Zhiqiao Wu",
      "Jiafu Tang",
      "Jiabi Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17443",
    "title": "Cat-and-Mouse Satellite Dynamics: Divergent Adversarial Reinforcement Learning for Contested Multi-Agent Space Operations",
    "abstract": "           As space becomes increasingly crowded and contested, robust autonomous capabilities for multi-agent environments are gaining critical importance. Current autonomous systems in space primarily rely on optimization-based path planning or long-range orbital maneuvers, which have not yet proven effective in adversarial scenarios where one satellite is actively pursuing another. We introduce Divergent Adversarial Reinforcement Learning (DARL), a two-stage Multi-Agent Reinforcement Learning (MARL) approach designed to train autonomous evasion strategies for satellites engaged with multiple adversarial spacecraft. Our method enhances exploration during training by promoting diverse adversarial strategies, leading to more robust and adaptable evader models. We validate DARL through a cat-and-mouse satellite scenario, modeled as a partially observable multi-agent capture the flag game where two adversarial `cat' spacecraft pursue a single `mouse' evader. DARL's performance is compared against several benchmarks, including an optimization-based satellite path planner, demonstrating its ability to produce highly robust models for adversarial multi-agent space environments.         ",
    "url": "https://arxiv.org/abs/2409.17443",
    "authors": [
      "Cameron Mehlman",
      "Joseph Abramov",
      "Gregory Falco"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.17446",
    "title": "Efficient Federated Learning against Heterogeneous and Non-stationary Client Unavailability",
    "abstract": "           Addressing intermittent client availability is critical for the real-world deployment of federated learning algorithms. Most prior work either overlooks the potential non-stationarity in the dynamics of client unavailability or requires substantial memory/computation overhead. We study federated learning in the presence of heterogeneous and non-stationary client availability, which may occur when the deployment environments are uncertain or the clients are mobile. The impacts of the heterogeneity and non-stationarity in client unavailability can be significant, as we illustrate using FedAvg, the most widely adopted federated learning algorithm. We propose FedAPM, which includes novel algorithmic structures that (i) compensate for missed computations due to unavailability with only $O(1)$ additional memory and computation with respect to standard FedAvg, and (ii) evenly diffuse local updates within the federated learning system through implicit gossiping, despite being agnostic to non-stationary dynamics. We show that FedAPM converges to a stationary point of even non-convex objectives while achieving the desired linear speedup property. We corroborate our analysis with numerical experiments over diversified client unavailability dynamics on real-world data sets.         ",
    "url": "https://arxiv.org/abs/2409.17446",
    "authors": [
      "Ming Xiang",
      "Stratis Ioannidis",
      "Edmund Yeh",
      "Carlee Joe-Wong",
      "Lili Su"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2409.17453",
    "title": "AgMTR: Agent Mining Transformer for Few-shot Segmentation in Remote Sensing",
    "abstract": "           Few-shot Segmentation (FSS) aims to segment the interested objects in the query image with just a handful of labeled samples (i.e., support images). Previous schemes would leverage the similarity between support-query pixel pairs to construct the pixel-level semantic correlation. However, in remote sensing scenarios with extreme intra-class variations and cluttered backgrounds, such pixel-level correlations may produce tremendous mismatches, resulting in semantic ambiguity between the query foreground (FG) and background (BG) pixels. To tackle this problem, we propose a novel Agent Mining Transformer (AgMTR), which adaptively mines a set of local-aware agents to construct agent-level semantic correlation. Compared with pixel-level semantics, the given agents are equipped with local-contextual information and possess a broader receptive field. At this point, different query pixels can selectively aggregate the fine-grained local semantics of different agents, thereby enhancing the semantic clarity between query FG and BG pixels. Concretely, the Agent Learning Encoder (ALE) is first proposed to erect the optimal transport plan that arranges different agents to aggregate support semantics under different local regions. Then, for further optimizing the agents, the Agent Aggregation Decoder (AAD) and the Semantic Alignment Decoder (SAD) are constructed to break through the limited support set for mining valuable class-specific semantics from unlabeled data sources and the query image itself, respectively. Extensive experiments on the remote sensing benchmark iSAID indicate that the proposed method achieves state-of-the-art performance. Surprisingly, our method remains quite competitive when extended to more common natural scenarios, i.e., PASCAL-5i and COCO-20i.         ",
    "url": "https://arxiv.org/abs/2409.17453",
    "authors": [
      "Hanbo Bi",
      "Yingchao Feng",
      "Yongqiang Mao",
      "Jianning Pei",
      "Wenhui Diao",
      "Hongqi Wang",
      "Xian Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17467",
    "title": "What is the social benefit of hate speech detection research? A Systematic Review",
    "abstract": "           While NLP research into hate speech detection has grown exponentially in the last three decades, there has been minimal uptake or engagement from policy makers and non-profit organisations. We argue the absence of ethical frameworks have contributed to this rift between current practice and best practice. By adopting appropriate ethical frameworks, NLP researchers may enable the social impact potential of hate speech research. This position paper is informed by reviewing forty-eight hate speech detection systems associated with thirty-seven publications from different venues.         ",
    "url": "https://arxiv.org/abs/2409.17467",
    "authors": [
      "Sidney Gig-Jan Wong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.17474",
    "title": "Reducing and Exploiting Data Augmentation Noise through Meta Reweighting Contrastive Learning for Text Classification",
    "abstract": "           Data augmentation has shown its effectiveness in resolving the data-hungry problem and improving model's generalization ability. However, the quality of augmented data can be varied, especially compared with the raw/original data. To boost deep learning models' performance given augmented data/samples in text classification tasks, we propose a novel framework, which leverages both meta learning and contrastive learning techniques as parts of our design for reweighting the augmented samples and refining their feature representations based on their quality. As part of the framework, we propose novel weight-dependent enqueue and dequeue algorithms to utilize augmented samples' weight/quality information effectively. Through experiments, we show that our framework can reasonably cooperate with existing deep learning models (e.g., RoBERTa-base and Text-CNN) and augmentation techniques (e.g., Wordnet and Easydata) for specific supervised learning tasks. Experiment results show that our framework achieves an average of 1.6%, up to 4.3% absolute improvement on Text-CNN encoders and an average of 1.4%, up to 4.4% absolute improvement on RoBERTa-base encoders on seven GLUE benchmark datasets compared with the best baseline. We present an indepth analysis of our framework design, revealing the non-trivial contributions of our network components. Our code is publicly available for better reproducibility.         ",
    "url": "https://arxiv.org/abs/2409.17474",
    "authors": [
      "Guanyi Mou",
      "Yichuan Li",
      "Kyumin Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17475",
    "title": "On the Impact of Feature Heterophily on Link Prediction with Graph Neural Networks",
    "abstract": "           Heterophily, or the tendency of connected nodes in networks to have different class labels or dissimilar features, has been identified as challenging for many Graph Neural Network (GNN) models. While the challenges of applying GNNs for node classification when class labels display strong heterophily are well understood, it is unclear how heterophily affects GNN performance in other important graph learning tasks where class labels are not available. In this work, we focus on the link prediction task and systematically analyze the impact of heterophily in node features on GNN performance. Theoretically, we first introduce formal definitions of homophilic and heterophilic link prediction tasks, and present a theoretical framework that highlights the different optimizations needed for the respective tasks. We then analyze how different link prediction encoders and decoders adapt to varying levels of feature homophily and introduce designs for improved performance. Our empirical analysis on a variety of synthetic and real-world datasets confirms our theoretical insights and highlights the importance of adopting learnable decoders and GNN encoders with ego- and neighbor-embedding separation in message passing for link prediction tasks beyond homophily.         ",
    "url": "https://arxiv.org/abs/2409.17475",
    "authors": [
      "Jiong Zhu",
      "Gaotang Li",
      "Yao-An Yang",
      "Jing Zhu",
      "Xuehao Cui",
      "Danai Koutra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17476",
    "title": "Improving the Shortest Plank: Vulnerability-Aware Adversarial Training for Robust Recommender System",
    "abstract": "           Recommender systems play a pivotal role in mitigating information overload in various fields. Nonetheless, the inherent openness of these systems introduces vulnerabilities, allowing attackers to insert fake users into the system's training data to skew the exposure of certain items, known as poisoning attacks. Adversarial training has emerged as a notable defense mechanism against such poisoning attacks within recommender systems. Existing adversarial training methods apply perturbations of the same magnitude across all users to enhance system robustness against attacks. Yet, in reality, we find that attacks often affect only a subset of users who are vulnerable. These perturbations of indiscriminate magnitude make it difficult to balance effective protection for vulnerable users without degrading recommendation quality for those who are not affected. To address this issue, our research delves into understanding user vulnerability. Considering that poisoning attacks pollute the training data, we note that the higher degree to which a recommender system fits users' training data correlates with an increased likelihood of users incorporating attack information, indicating their vulnerability. Leveraging these insights, we introduce the Vulnerability-aware Adversarial Training (VAT), designed to defend against poisoning attacks in recommender systems. VAT employs a novel vulnerability-aware function to estimate users' vulnerability based on the degree to which the system fits them. Guided by this estimation, VAT applies perturbations of adaptive magnitude to each user, not only reducing the success ratio of attacks but also preserving, and potentially enhancing, the quality of recommendations. Comprehensive experiments confirm VAT's superior defensive capabilities across different recommendation models and against various types of attacks.         ",
    "url": "https://arxiv.org/abs/2409.17476",
    "authors": [
      "Kaike Zhang",
      "Qi Cao",
      "Yunfan Wu",
      "Fei Sun",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2409.17480",
    "title": "What Would Happen Next? Predicting Consequences from An Event Causality Graph",
    "abstract": "           Existing script event prediction task forcasts the subsequent event based on an event script chain. However, the evolution of historical events are more complicated in real world scenarios and the limited information provided by the event script chain also make it difficult to accurately predict subsequent events. This paper introduces a Causality Graph Event Prediction(CGEP) task that forecasting consequential event based on an Event Causality Graph (ECG). We propose a Semantic Enhanced Distance-sensitive Graph Prompt Learning (SeDGPL) Model for the CGEP task. In SeDGPL, (1) we design a Distance-sensitive Graph Linearization (DsGL) module to reformulate the ECG into a graph prompt template as the input of a PLM; (2) propose an Event-Enriched Causality Encoding (EeCE) module to integrate both event contextual semantic and graph schema information; (3) propose a Semantic Contrast Event Prediction (ScEP) module to enhance the event representation among numerous candidate events and predict consequential event following prompt learning paradigm. %We construct two CGEP datasets based on existing MAVEN-ERE and ESC corpus for experiments. Experiment results validate our argument our proposed SeDGPL model outperforms the advanced competitors for the CGEP task.         ",
    "url": "https://arxiv.org/abs/2409.17480",
    "authors": [
      "Chuanhong Zhan",
      "Wei Xiang",
      "Chao Liang",
      "Bang Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17483",
    "title": "Heterogeneous Hyper-Graph Neural Networks for Context-aware Human Activity Recognition",
    "abstract": "           Context-aware Human Activity Recognition (CHAR) is challenging due to the need to recognize the user's current activity from signals that vary significantly with contextual factors such as phone placements and the varied styles with which different users perform the same activity. In this paper, we argue that context-aware activity visit patterns in realistic in-the-wild data can equivocally be considered as a general graph representation learning task. We posit that exploiting underlying graphical patterns in CHAR data can improve CHAR task performance and representation learning. Building on the intuition that certain activities are frequently performed with the phone placed in certain positions, we focus on the context-aware human activity problem of recognizing the <Activity, Phone Placement> tuple. We demonstrate that CHAR data has an underlying graph structure that can be viewed as a heterogenous hypergraph that has multiple types of nodes and hyperedges (an edge connecting more than two nodes). Subsequently, learning <Activity, Phone Placement> representations becomes a graph node representation learning problem. After task transformation, we further propose a novel Heterogeneous HyperGraph Neural Network architecture for Context-aware Human Activity Recognition (HHGNN-CHAR), with three types of heterogeneous nodes (user, phone placement, and activity). Connections between all types of nodes are represented by hyperedges. Rigorous evaluation demonstrated that on an unscripted, in-the-wild CHAR dataset, our proposed framework significantly outperforms state-of-the-art (SOTA) baselines including CHAR models that do not exploit graphs, and GNN variants that do not incorporate heterogeneous nodes or hyperedges with overall improvements 14.04% on Matthews Correlation Coefficient (MCC) and 7.01% on Macro F1 scores.         ",
    "url": "https://arxiv.org/abs/2409.17483",
    "authors": [
      "Wen Ge",
      "Guanyi Mou",
      "Emmanuel O. Agu",
      "Kyumin Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17485",
    "title": "Revisiting Deep Ensemble Uncertainty for Enhanced Medical Anomaly Detection",
    "abstract": "           Medical anomaly detection (AD) is crucial in pathological identification and localization. Current methods typically rely on uncertainty estimation in deep ensembles to detect anomalies, assuming that ensemble learners should agree on normal samples while exhibiting disagreement on unseen anomalies in the output space. However, these methods may suffer from inadequate disagreement on anomalies or diminished agreement on normal samples. To tackle these issues, we propose D2UE, a Diversified Dual-space Uncertainty Estimation framework for medical anomaly detection. To effectively balance agreement and disagreement for anomaly detection, we propose Redundancy-Aware Repulsion (RAR), which uses a similarity kernel that remains invariant to both isotropic scaling and orthogonal transformations, explicitly promoting diversity in learners' feature space. Moreover, to accentuate anomalous regions, we develop Dual-Space Uncertainty (DSU), which utilizes the ensemble's uncertainty in input and output spaces. In input space, we first calculate gradients of reconstruction error with respect to input images. The gradients are then integrated with reconstruction outputs to estimate uncertainty for inputs, enabling effective anomaly discrimination even when output space disagreement is minimal. We conduct a comprehensive evaluation of five medical benchmarks with different backbones. Experimental results demonstrate the superiority of our method to state-of-the-art methods and the effectiveness of each component in our framework. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17485",
    "authors": [
      "Yi Gu",
      "Yi Lin",
      "Kwang-Ting Cheng",
      "Hao Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17494",
    "title": "From Graphs to Words: A Computer-Assisted Framework for the Production of Accessible Text Descriptions",
    "abstract": "           In the digital landscape, the ubiquity of data visualizations in media underscores the necessity for accessibility to ensure inclusivity for all users, including those with visual impairments. Current visual content often fails to cater to the needs of screen reader users due to the absence of comprehensive textual descriptions. To address this gap, we propose in this paper a framework designed to empower media content creators to transform charts into descriptive narratives. This tool not only facilitates the understanding of complex visual data through text but also fosters a broader awareness of accessibility in digital content creation. Through the application of this framework, users can interpret and convey the insights of data visualizations more effectively, accommodating a diverse audience. Our evaluations reveal that this tool not only enhances the comprehension of data visualizations but also promotes new perspectives on the represented data, thereby broadening the interpretative possibilities for all users.         ",
    "url": "https://arxiv.org/abs/2409.17494",
    "authors": [
      "Qiang Xu",
      "Thomas Hurtut"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2409.17500",
    "title": "GLinSAT: The General Linear Satisfiability Neural Network Layer By Accelerated Gradient Descent",
    "abstract": "           Ensuring that the outputs of neural networks satisfy specific constraints is crucial for applying neural networks to real-life decision-making problems. In this paper, we consider making a batch of neural network outputs satisfy bounded and general linear constraints. We first reformulate the neural network output projection problem as an entropy-regularized linear programming problem. We show that such a problem can be equivalently transformed into an unconstrained convex optimization problem with Lipschitz continuous gradient according to the duality theorem. Then, based on an accelerated gradient descent algorithm with numerical performance enhancement, we present our architecture, GLinSAT, to solve the problem. To the best of our knowledge, this is the first general linear satisfiability layer in which all the operations are differentiable and matrix-factorization-free. Despite the fact that we can explicitly perform backpropagation based on automatic differentiation mechanism, we also provide an alternative approach in GLinSAT to calculate the derivatives based on implicit differentiation of the optimality condition. Experimental results on constrained traveling salesman problems, partial graph matching with outliers, predictive portfolio allocation and power system unit commitment demonstrate the advantages of GLinSAT over existing satisfiability layers.         ",
    "url": "https://arxiv.org/abs/2409.17500",
    "authors": [
      "Hongtai Zeng",
      "Chao Yang",
      "Yanzhen Zhou",
      "Cheng Yang",
      "Qinglai Guo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2409.17504",
    "title": "HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection",
    "abstract": "           The surge in applications of large language models (LLMs) has prompted concerns about the generation of misleading or fabricated information, known as hallucinations. Therefore, detecting hallucinations has become critical to maintaining trust in LLM-generated content. A primary challenge in learning a truthfulness classifier is the lack of a large amount of labeled truthful and hallucinated data. To address the challenge, we introduce HaloScope, a novel learning framework that leverages the unlabeled LLM generations in the wild for hallucination detection. Such unlabeled data arises freely upon deploying LLMs in the open world, and consists of both truthful and hallucinated information. To harness the unlabeled data, we present an automated membership estimation score for distinguishing between truthful and untruthful generations within unlabeled mixture data, thereby enabling the training of a binary truthfulness classifier on top. Importantly, our framework does not require extra data collection and human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiments show that HaloScope can achieve superior hallucination detection performance, outperforming the competitive rivals by a significant margin. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17504",
    "authors": [
      "Xuefeng Du",
      "Chaowei Xiao",
      "Yixuan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.17513",
    "title": "Comparing Unidirectional, Bidirectional, and Word2vec Models for Discovering Vulnerabilities in Compiled Lifted Code",
    "abstract": "           Ransomware and other forms of malware cause significant financial and operational damage to organizations by exploiting long-standing and often difficult-to-detect software vulnerabilities. To detect vulnerabilities such as buffer overflows in compiled code, this research investigates the application of unidirectional transformer-based embeddings, specifically GPT-2. Using a dataset of LLVM functions, we trained a GPT-2 model to generate embeddings, which were subsequently used to build LSTM neural networks to differentiate between vulnerable and non-vulnerable code. Our study reveals that embeddings from the GPT-2 model significantly outperform those from bidirectional models of BERT and RoBERTa, achieving an accuracy of 92.5% and an F1-score of 89.7%. LSTM neural networks were developed with both frozen and unfrozen embedding model layers. The model with the highest performance was achieved when the embedding layers were unfrozen. Further, the research finds that, in exploring the impact of different optimizers within this domain, the SGD optimizer demonstrates superior performance over Adam. Overall, these findings reveal important insights into the potential of unidirectional transformer-based approaches in enhancing cybersecurity defenses.         ",
    "url": "https://arxiv.org/abs/2409.17513",
    "authors": [
      "Gary A. McCully",
      "John D. Hastings",
      "Shengjie Xu",
      "Adam Fortier"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2409.17524",
    "title": "JoyType: A Robust Design for Multilingual Visual Text Creation",
    "abstract": "           Generating images with accurately represented text, especially in non-Latin languages, poses a significant challenge for diffusion models. Existing approaches, such as the integration of hint condition diagrams via auxiliary networks (e.g., ControlNet), have made strides towards addressing this issue. However, diffusion models often fall short in tasks requiring controlled text generation, such as specifying particular fonts or producing text in small fonts. In this paper, we introduce a novel approach for multilingual visual text creation, named JoyType, designed to maintain the font style of text during the image generation process. Our methodology begins with assembling a training dataset, JoyType-1M, comprising 1 million pairs of data. Each pair includes an image, its description, and glyph instructions corresponding to the font style within the image. We then developed a text control network, Font ControlNet, tasked with extracting font style information to steer the image generation. To further enhance our model's ability to maintain font style, notably in generating small-font text, we incorporated a multi-layer OCR-aware loss into the diffusion process. This enhancement allows JoyType to direct text rendering using low-level descriptors. Our evaluations, based on both visual and accuracy metrics, demonstrate that JoyType significantly outperforms existing state-of-the-art methods. Additionally, JoyType can function as a plugin, facilitating the creation of varied image styles in conjunction with other stable diffusion models on HuggingFace and CivitAI. Our project is open-sourced on this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17524",
    "authors": [
      "Chao Li",
      "Chen Jiang",
      "Xiaolong Liu",
      "Jun Zhao",
      "Guoxin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17526",
    "title": "Drone Stereo Vision for Radiata Pine Branch Detection and Distance Measurement: Integrating SGBM and Segmentation Models",
    "abstract": "           Manual pruning of radiata pine trees presents significant safety risks due to their substantial height and the challenging terrains in which they thrive. To address these risks, this research proposes the development of a drone-based pruning system equipped with specialized pruning tools and a stereo vision camera, enabling precise detection and trimming of branches. Deep learning algorithms, including YOLO and Mask R-CNN, are employed to ensure accurate branch detection, while the Semi-Global Matching algorithm is integrated to provide reliable distance estimation. The synergy between these techniques facilitates the precise identification of branch locations and enables efficient, targeted pruning. Experimental results demonstrate that the combined implementation of YOLO and SGBM enables the drone to accurately detect branches and measure their distances from the drone. This research not only improves the safety and efficiency of pruning operations but also makes a significant contribution to the advancement of drone technology in the automation of agricultural and forestry practices, laying a foundational framework for further innovations in environmental management.         ",
    "url": "https://arxiv.org/abs/2409.17526",
    "authors": [
      "Yida Lin",
      "Bing Xue",
      "Mengjie Zhang",
      "Sam Schofield",
      "Richard Green"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17527",
    "title": "Data Proportion Detection for Optimized Data Management for Large Language Models",
    "abstract": "           Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks and domains, with data preparation playing a critical role in achieving these results. Pre-training data typically combines information from multiple domains. To maximize performance when integrating data from various domains, determining the optimal data proportion is essential. However, state-of-the-art (SOTA) LLMs rarely disclose details about their pre-training data, making it difficult for researchers to identify ideal data proportions. In this paper, we introduce a new topic, \\textit{data proportion detection}, which enables the automatic estimation of pre-training data proportions by analyzing the generated outputs of LLMs. We provide rigorous theoretical proofs, practical algorithms, and preliminary experimental results for data proportion detection. Based on these findings, we offer valuable insights into the challenges and future directions for effective data proportion detection and data management.         ",
    "url": "https://arxiv.org/abs/2409.17527",
    "authors": [
      "Hao Liang",
      "Keshi Zhao",
      "Yajie Yang",
      "Bin Cui",
      "Guosheng Dong",
      "Zenan Zhou",
      "Wentao Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.17535",
    "title": "Privacy-Preserving Redaction of Diagnosis Data through Source Code Analysis",
    "abstract": "           Protecting sensitive information in diagnostic data such as logs, is a critical concern in the industrial software diagnosis and debugging process. While there are many tools developed to automatically redact the logs for identifying and removing sensitive information, they have severe limitations which can cause either over redaction and loss of critical diagnostic information (false positives), or disclosure of sensitive information (false negatives), or both. To address the problem, in this paper, we argue for a source code analysis approach for log redaction. To identify a log message containing sensitive information, our method locates the corresponding log statement in the source code with logger code augmentation, and checks if the log statement outputs data from sensitive sources by using the data flow graph built from the source code. Appropriate redaction rules are further applied depending on the sensitiveness of the data sources to preserve the privacy information in the logs. We conducted experimental evaluation and comparison with other popular baselines. The results demonstrate that our approach can significantly improve the detection precision of the sensitive information and reduce both false positives and negatives.         ",
    "url": "https://arxiv.org/abs/2409.17535",
    "authors": [
      "Lixi Zhou",
      "Lei Yu",
      "Jia Zou",
      "Hong Min"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.17536",
    "title": "MUSE: Integrating Multi-Knowledge for Knowledge Graph Completion",
    "abstract": "           Knowledge Graph Completion (KGC) aims to predict the missing [relation] part of (head entity)--[relation]->(tail entity) triplet. Most existing KGC methods focus on single features (e.g., relation types) or sub-graph aggregation. However, they do not fully explore the Knowledge Graph (KG) features and neglect the guidance of external semantic knowledge. To address these shortcomings, we propose a knowledge-aware reasoning model (MUSE), which designs a novel multi-knowledge representation learning mechanism for missing relation prediction. Our model develops a tailored embedding space through three parallel components: 1) Prior Knowledge Learning for enhancing the triplets' semantic representation by fine-tuning BERT; 2) Context Message Passing for enhancing the context messages of KG; 3) Relational Path Aggregation for enhancing the path representation from the head entity to the tail entity. The experimental results show that MUSE significantly outperforms other baselines on four public datasets, achieving over 5.50% H@1 improvement and 4.20% MRR improvement on the NELL995 dataset. The code and datasets will be released via this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17536",
    "authors": [
      "Pengjie Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.17538",
    "title": "On the Implicit Relation Between Low-Rank Adaptation and Differential Privacy",
    "abstract": "           A significant approach in natural language processing involves large-scale pre-training on general domain data followed by adaptation to specific tasks or domains. As models grow in size, full fine-tuning all parameters becomes increasingly impractical. To address this, some methods for low-rank task adaptation of language models have been proposed, e.g. LoRA and FLoRA. These methods keep the pre-trained model weights fixed and incorporate trainable low-rank decomposition matrices into some layers of the transformer architecture, called adapters. This approach significantly reduces the number of trainable parameters required for downstream tasks compared to full fine-tuning all parameters. In this work, we look at low-rank adaptation from the lens of data privacy. We show theoretically that the low-rank adaptation used in LoRA and FLoRA is equivalent to injecting some random noise into the batch gradients w.r.t the adapter parameters coming from their full fine-tuning, and we quantify the variance of the injected noise. By establishing a Berry-Esseen type bound on the total variation distance between the noise distribution and a Gaussian distribution with the same variance, we show that the dynamics of LoRA and FLoRA are very close to differentially private full fine-tuning the adapters, which suggests that low-rank adaptation implicitly provides privacy w.r.t the fine-tuning data. Finally, using Johnson-Lindenstrauss lemma, we show that when augmented with gradient clipping, low-rank adaptation is almost equivalent to differentially private full fine-tuning adapters with a fixed noise scale.         ",
    "url": "https://arxiv.org/abs/2409.17538",
    "authors": [
      "Saber Malekmohammadi",
      "Golnoosh Farnadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.17541",
    "title": "Swapping-Centric Neural Recording Systems",
    "abstract": "           Neural interfaces read the activity of biological neurons to help advance the neurosciences and offer treatment options for severe neurological diseases. The total number of neurons that are now being recorded using multi-electrode interfaces is doubling roughly every 4-6 years \\cite{Stevenson2011}. However, processing this exponentially-growing data in real-time under strict power-constraints puts an exorbitant amount of pressure on both compute and storage within traditional neural recording systems. Existing systems deploy various accelerators for better performance-per-watt while also integrating NVMs for data querying and better treatment decisions. These accelerators have direct access to a limited amount of fast SRAM-based memory that is unable to manage the growing data rates. Swapping to the NVM becomes inevitable; however, naive approaches are unable to complete during the refractory period of a neuron -- i.e., a few milliseconds -- which disrupts timely disease treatment. We propose co-designing accelerators and storage, with swapping as a primary design goal, using theoretical and practical models of compute and storage respectively to overcome these limitations.         ",
    "url": "https://arxiv.org/abs/2409.17541",
    "authors": [
      "Muhammed Ugur",
      "Raghavendra Pradyumna Pothukuchi",
      "Abhishek Bhattacharjee"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2409.17549",
    "title": "Canonical Representation and Force-Based Pretraining of 3D Tactile for Dexterous Visuo-Tactile Policy Learning",
    "abstract": "           Tactile sensing plays a vital role in enabling robots to perform fine-grained, contact-rich tasks. However, the high dimensionality of tactile data, due to the large coverage on dexterous hands, poses significant challenges for effective tactile feature learning, especially for 3D tactile data, as there are no large standardized datasets and no strong pretrained backbones. To address these challenges, we propose a novel canonical representation that reduces the difficulty of 3D tactile feature learning and further introduces a force-based self-supervised pretraining task to capture both local and net force features, which are crucial for dexterous manipulation. Our method achieves an average success rate of 78% across four fine-grained, contact-rich dexterous manipulation tasks in real-world experiments, demonstrating effectiveness and robustness compared to other methods. Further analysis shows that our method fully utilizes both spatial and force information from 3D tactile data to accomplish the tasks. The videos can be viewed at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17549",
    "authors": [
      "Tianhao Wu",
      "Jinzhou Li",
      "Jiyao Zhang",
      "Mingdong Wu",
      "Hao Dong"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.17552",
    "title": "Expression Rates of Neural Operators for Linear Elliptic PDEs in Polytopes",
    "abstract": "           We study the approximation rates of a class of deep neural network approximations of operators, which arise as data-to-solution maps $\\mathcal{G}^\\dagger$ of linear elliptic partial differential equations (PDEs), and act between pairs $X,Y$ of suitable infinite-dimensional spaces. We prove expression rate bounds for approximate neural operators $\\mathcal{G}$ with the structure $\\mathcal{G} = \\mathcal{R} \\circ \\mathcal{A} \\circ \\mathcal{E}$, with linear encoders $\\mathcal{E}$ and decoders $\\mathcal{R}$. The constructive proofs are via a recurrent NN structure obtained by unrolling exponentially convergent, self-consistent (``Richardson'') iterations. We bound the operator approximation error with respect to the linear Kolmogorov $N$-widths of the data and solution sets and in terms of the size of the approximation network. We prove expression rate bounds for approximate, neural solution operators emulating the coefficient-to-solution maps for elliptic PDEs set in $d$-dimensional polytopes, with $d\\in\\{2,3\\}$, and subject to Dirichlet-, Neumann- or mixed boundary conditions. Exploiting weighted norm characterizations of the solution sets of elliptic PDEs in polytopes, we show algebraic rates of expression for problems with data with finite regularity, and exponential operator expression rates for analytic data.         ",
    "url": "https://arxiv.org/abs/2409.17552",
    "authors": [
      "Carlo Marcati",
      "Christoph Schwab"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2409.17566",
    "title": "Flexiffusion: Segment-wise Neural Architecture Search for Flexible Denoising Schedule",
    "abstract": "           Diffusion models are cutting-edge generative models adept at producing diverse, high-quality images. Despite their effectiveness, these models often require significant computational resources owing to their numerous sequential denoising steps and the significant inference cost of each step. Recently, Neural Architecture Search (NAS) techniques have been employed to automatically search for faster generation processes. However, NAS for diffusion is inherently time-consuming as it requires estimating thousands of diffusion models to search for the optimal one. In this paper, we introduce Flexiffusion, a novel training-free NAS paradigm designed to accelerate diffusion models by concurrently optimizing generation steps and network structures. Specifically, we partition the generation process into isometric step segments, each sequentially composed of a full step, multiple partial steps, and several null steps. The full step computes all network blocks, while the partial step involves part of the blocks, and the null step entails no computation. Flexiffusion autonomously explores flexible step combinations for each segment, substantially reducing search costs and enabling greater acceleration compared to the state-of-the-art (SOTA) method for diffusion models. Our searched models reported speedup factors of $2.6\\times$ and $1.5\\times$ for the original LDM-4-G and the SOTA, respectively. The factors for Stable Diffusion V1.5 and the SOTA are $5.1\\times$ and $2.0\\times$. We also verified the performance of Flexiffusion on multiple datasets, and positive experiment results indicate that Flexiffusion can effectively reduce redundancy in diffusion models.         ",
    "url": "https://arxiv.org/abs/2409.17566",
    "authors": [
      "Hongtao Huang",
      "Xiaojun Chang",
      "Lina Yao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17568",
    "title": "Showing Many Labels in Multi-label Classification Models: An Empirical Study of Adversarial Examples",
    "abstract": "           With the rapid development of Deep Neural Networks (DNNs), they have been applied in numerous fields. However, research indicates that DNNs are susceptible to adversarial examples, and this is equally true in the multi-label domain. To further investigate multi-label adversarial examples, we introduce a novel type of attacks, termed \"Showing Many Labels\". The objective of this attack is to maximize the number of labels included in the classifier's prediction results. In our experiments, we select nine attack algorithms and evaluate their performance under \"Showing Many Labels\". Eight of the attack algorithms were adapted from the multi-class environment to the multi-label environment, while the remaining one was specifically designed for the multi-label environment. We choose ML-LIW and ML-GCN as target models and train them on four popular multi-label datasets: VOC2007, VOC2012, NUS-WIDE, and COCO. We record the success rate of each algorithm when it shows the expected number of labels in eight different scenarios. Experimental results indicate that under the \"Showing Many Labels\", iterative attacks perform significantly better than one-step attacks. Moreover, it is possible to show all labels in the dataset.         ",
    "url": "https://arxiv.org/abs/2409.17568",
    "authors": [
      "Yujiang Liu",
      "Wenjian Luo",
      "Zhijian Chen",
      "Muhammad Luqman Naseem"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17589",
    "title": "Improving Fast Adversarial Training via Self-Knowledge Guidance",
    "abstract": "           Adversarial training has achieved remarkable advancements in defending against adversarial attacks. Among them, fast adversarial training (FAT) is gaining attention for its ability to achieve competitive robustness with fewer computing resources. Existing FAT methods typically employ a uniform strategy that optimizes all training data equally without considering the influence of different examples, which leads to an imbalanced optimization. However, this imbalance remains unexplored in the field of FAT. In this paper, we conduct a comprehensive study of the imbalance issue in FAT and observe an obvious class disparity regarding their performances. This disparity could be embodied from a perspective of alignment between clean and robust accuracy. Based on the analysis, we mainly attribute the observed misalignment and disparity to the imbalanced optimization in FAT, which motivates us to optimize different training data adaptively to enhance robustness. Specifically, we take disparity and misalignment into consideration. First, we introduce self-knowledge guided regularization, which assigns differentiated regularization weights to each class based on its training state, alleviating class disparity. Additionally, we propose self-knowledge guided label relaxation, which adjusts label relaxation according to the training accuracy, alleviating the misalignment and improving robustness. By combining these methods, we formulate the Self-Knowledge Guided FAT (SKG-FAT), leveraging naturally generated knowledge during training to enhance the adversarial robustness without compromising training efficiency. Extensive experiments on four standard datasets demonstrate that the SKG-FAT improves the robustness and preserves competitive clean accuracy, outperforming the state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2409.17589",
    "authors": [
      "Chengze Jiang",
      "Junkai Wang",
      "Minjing Dong",
      "Jie Gui",
      "Xinli Shi",
      "Yuan Cao",
      "Yuan Yan Tang",
      "James Tin-Yau Kwok"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17592",
    "title": "Deep Manifold Part 1: Anatomy of Neural Network Manifold",
    "abstract": "           Based on the numerical manifold method principle, we developed a mathematical framework of a neural network manifold: Deep Manifold and discovered that neural networks: 1) is numerical computation combining forward and inverse; 2) have near infinite degrees of freedom; 3) exponential learning capacity with depth; 4) have self-progressing boundary conditions; 5) has training hidden bottleneck. We also define two concepts: neural network learning space and deep manifold space and introduce two concepts: neural network intrinsic pathway and fixed point. We raise three fundamental questions: 1). What is the training completion definition; 2). where is the deep learning convergence point (neural network fixed point); 3). How important is token timestamp in training data given negative time is critical in inverse problem.         ",
    "url": "https://arxiv.org/abs/2409.17592",
    "authors": [
      "Max Y. Ma",
      "Gen-Hua Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17598",
    "title": "Freeze and Learn: Continual Learning with Selective Freezing for Speech Deepfake Detection",
    "abstract": "           In speech deepfake detection, one of the critical aspects is developing detectors able to generalize on unseen data and distinguish fake signals across different datasets. Common approaches to this challenge involve incorporating diverse data into the training process or fine-tuning models on unseen datasets. However, these solutions can be computationally demanding and may lead to the loss of knowledge acquired from previously learned data. Continual learning techniques offer a potential solution to this problem, allowing the models to learn from unseen data without losing what they have already learned. Still, the optimal way to apply these algorithms for speech deepfake detection remains unclear, and we do not know which is the best way to apply these algorithms to the developed models. In this paper we address this aspect and investigate whether, when retraining a speech deepfake detector, it is more effective to apply continual learning across the entire model or to update only some of its layers while freezing others. Our findings, validated across multiple models, indicate that the most effective approach among the analyzed ones is to update only the weights of the initial layers, which are responsible for processing the input features of the detector.         ",
    "url": "https://arxiv.org/abs/2409.17598",
    "authors": [
      "Davide Salvi",
      "Viola Negroni",
      "Luca Bondi",
      "Paolo Bestagini",
      "Stefano Tubaro"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2409.17601",
    "title": "TA-Cleaner: A Fine-grained Text Alignment Backdoor Defense Strategy for Multimodal Contrastive Learning",
    "abstract": "           Pre-trained large models for multimodal contrastive learning, such as CLIP, have been widely recognized in the industry as highly susceptible to data-poisoned backdoor attacks. This poses significant risks to downstream model training. In response to such potential threats, finetuning offers a simpler and more efficient defense choice compared to retraining large models with augmented data. In the supervised learning domain, fine-tuning defense strategies can achieve excellent defense performance. However, in the unsupervised and semi-supervised domain, we find that when CLIP faces some complex attack techniques, the existing fine-tuning defense strategy, CleanCLIP, has some limitations on defense performance. The synonym substitution of its text-augmentation is insufficient to enhance the text feature space. To compensate for this weakness, we improve it by proposing a fine-grained \\textbf{T}ext \\textbf{A}lignment \\textbf{C}leaner (TA-Cleaner) to cut off feature connections of backdoor triggers. We randomly select a few samples for positive and negative subtext generation at each epoch of CleanCLIP, and align the subtexts to the images to strengthen the text self-supervision. We evaluate the effectiveness of our TA-Cleaner against six attack algorithms and conduct comprehensive zero-shot classification tests on ImageNet1K. Our experimental results demonstrate that TA-Cleaner achieves state-of-the-art defensiveness among finetuning-based defense techniques. Even when faced with the novel attack technique BadCLIP, our TA-Cleaner outperforms CleanCLIP by reducing the ASR of Top-1 and Top-10 by 52.02\\% and 63.88\\%, respectively.         ",
    "url": "https://arxiv.org/abs/2409.17601",
    "authors": [
      "Yuan Xun",
      "Siyuan Liang",
      "Xiaojun Jia",
      "Xinwei Liu",
      "Xiaochun Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17608",
    "title": "Appearance Blur-driven AutoEncoder and Motion-guided Memory Module for Video Anomaly Detection",
    "abstract": "           Video anomaly detection (VAD) often learns the distribution of normal samples and detects the anomaly through measuring significant deviations, but the undesired generalization may reconstruct a few anomalies thus suppressing the deviations. Meanwhile, most VADs cannot cope with cross-dataset validation for new target domains, and few-shot methods must laboriously rely on model-tuning from the target domain to complete domain adaptation. To address these problems, we propose a novel VAD method with a motion-guided memory module to achieve cross-dataset validation with zero-shot. First, we add Gaussian blur to the raw appearance images, thereby constructing the global pseudo-anomaly, which serves as the input to the network. Then, we propose multi-scale residual channel attention to deblur the pseudo-anomaly in normal samples. Next, memory items are obtained by recording the motion features in the training phase, which are used to retrieve the motion features from the raw information in the testing phase. Lastly, our method can ignore the blurred real anomaly through attention and rely on motion memory items to increase the normality gap between normal and abnormal motion. Extensive experiments on three benchmark datasets demonstrate the effectiveness of the proposed method. Compared with cross-domain methods, our method achieves competitive performance without adaptation during testing.         ",
    "url": "https://arxiv.org/abs/2409.17608",
    "authors": [
      "Jiahao Lyu",
      "Minghua Zhao",
      "Jing Hu",
      "Xuewen Huang",
      "Shuangli Du",
      "Cheng Shi",
      "Zhiyong Lv"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17622",
    "title": "Neural P$^3$M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs",
    "abstract": "           Geometric graph neural networks (GNNs) have emerged as powerful tools for modeling molecular geometry. However, they encounter limitations in effectively capturing long-range interactions in large molecular systems. To address this challenge, we introduce Neural P$^3$M, a versatile enhancer of geometric GNNs to expand the scope of their capabilities by incorporating mesh points alongside atoms and reimaging traditional mathematical operations in a trainable manner. Neural P$^3$M exhibits flexibility across a wide range of molecular systems and demonstrates remarkable accuracy in predicting energies and forces, outperforming on benchmarks such as the MD22 dataset. It also achieves an average improvement of 22% on the OE62 dataset while integrating with various architectures.         ",
    "url": "https://arxiv.org/abs/2409.17622",
    "authors": [
      "Yusong Wang",
      "Chaoran Cheng",
      "Shaoning Li",
      "Yuxuan Ren",
      "Bin Shao",
      "Ge Liu",
      "Pheng-Ann Heng",
      "Nanning Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17623",
    "title": "Fully Dynamic Graph Algorithms with Edge Differential Privacy",
    "abstract": "           We study differentially private algorithms for analyzing graphs in the challenging setting of continual release with fully dynamic updates, where edges are inserted and deleted over time, and the algorithm is required to update the solution at every time step. Previous work has presented differentially private algorithms for many graph problems that can handle insertions only or deletions only (called partially dynamic algorithms) and obtained some hardness results for the fully dynamic setting. The only algorithms in the latter setting were for the edge count, given by Fichtenberger, Henzinger, and Ost (ESA 21), and for releasing the values of all graph cuts, given by Fichtenberger, Henzinger, and Upadhyay (ICML 23). We provide the first differentially private and fully dynamic graph algorithms for several other fundamental graph statistics (including the triangle count, the number of connected components, the size of the maximum matching, and the degree histogram), analyze their error and show strong lower bounds on the error for all algorithms in this setting. We study two variants of edge differential privacy for fully dynamic graph algorithms: event-level and item-level. We give upper and lower bounds on the error of both event-level and item-level fully dynamic algorithms for several fundamental graph problems. No fully dynamic algorithms that are private at the item-level (the more stringent of the two notions) were known before. In the case of item-level privacy, for several problems, our algorithms match our lower bounds.         ",
    "url": "https://arxiv.org/abs/2409.17623",
    "authors": [
      "Sofya Raskhodnikova",
      "Teresa Anna Steiner"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.17628",
    "title": "Convolutional Signal Propagation: A Simple Scalable Algorithm for Hypergraphs",
    "abstract": "           Last decade has seen the emergence of numerous methods for learning on graphs, particularly Graph Neural Networks (GNNs). These methods, however, are often not directly applicable to more complex structures like bipartite graphs (equivalent to hypergraphs), which represent interactions among two entity types (e.g. a user liking a movie). This paper proposes Convolutional Signal Propagation (CSP), a non-parametric simple and scalable method that natively operates on bipartite graphs (hypergraphs) and can be implemented with just a few lines of code. After defining CSP, we demonstrate its relationship with well-established methods like label propagation, Naive Bayes, and Hypergraph Convolutional Networks. We evaluate CSP against several reference methods on real-world datasets from multiple domains, focusing on retrieval and classification tasks. Our results show that CSP offers competitive performance while maintaining low computational complexity, making it an ideal first choice as a baseline for hypergraph node classification and retrieval. Moreover, despite operating on hypergraphs, CSP achieves good results in tasks typically not associated with hypergraphs, such as natural language processing.         ",
    "url": "https://arxiv.org/abs/2409.17628",
    "authors": [
      "Pavel Proch\u00e1zka",
      "Marek D\u011bdi\u010d",
      "Luk\u00e1\u0161 Bajer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17629",
    "title": "Hand-object reconstruction via interaction-aware graph attention mechanism",
    "abstract": "           Estimating the poses of both a hand and an object has become an important area of research due to the growing need for advanced vision computing. The primary challenge involves understanding and reconstructing how hands and objects interact, such as contact and physical plausibility. Existing approaches often adopt a graph neural network to incorporate spatial information of hand and object meshes. However, these approaches have not fully exploited the potential of graphs without modification of edges within and between hand- and object-graphs. We propose a graph-based refinement method that incorporates an interaction-aware graph-attention mechanism to account for hand-object interactions. Using edges, we establish connections among closely correlated nodes, both within individual graphs and across different graphs. Experiments demonstrate the effectiveness of our proposed method with notable improvements in the realm of physical plausibility.         ",
    "url": "https://arxiv.org/abs/2409.17629",
    "authors": [
      "Taeyun Woo",
      "Tae-Kyun Kim",
      "Jinah Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17637",
    "title": "Intervention strategies for misinformation sharing on social media: A bibliometric analysis",
    "abstract": "           Widely distributed misinformation shared across social media channels is a pressing issue that poses a significant threat to many aspects of society's well-being. Inaccurate shared information causes confusion, can adversely affect mental health, and can lead to mis-informed decision-making. Therefore, it is important to implement proactive measures to intervene and curb the spread of misinformation where possible. This has prompted scholars to investigate a variety of intervention strategies for misinformation sharing on social media. This study explores the typology of intervention strategies for addressing misinformation sharing on social media, identifying 4 important clusters - cognition-based, automated-based, information-based, and hybrid-based. The literature selection process utilized the PRISMA method to ensure a systematic and comprehensive analysis of relevant literature while maintaining transparency and reproducibility. A total of 139 articles published from 2013-2023 were then analyzed. Meanwhile, bibliometric analyses were conducted using performance analysis and science mapping techniques for the typology development. A comparative analysis of the typology was conducted to reveal patterns and evolution in the field. This provides valuable insights for both theory and practical applications. Overall, the study concludes that scholarly contributions to scientific research and publication help to address research gaps and expand knowledge in this field. Understanding the evolution of intervention strategies for misinformation sharing on social media can support future research that contributes to the development of more effective and sustainable solutions to this persistent problem.         ",
    "url": "https://arxiv.org/abs/2409.17637",
    "authors": [
      "Juanita Zainudin",
      "Nazlena Mohamad Ali",
      "Alan F. Smeaton",
      "Mohamad Taha Ijab"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2409.17642",
    "title": "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic Self-Disclosure",
    "abstract": "           Large language model (LLM)-based AI delegates are increasingly utilized to act on behalf of users, assisting them with a wide range of tasks through conversational interfaces. Despite their advantages, concerns arise regarding the potential risk of privacy leaks, particularly in scenarios involving social interactions. While existing research has focused on protecting privacy by limiting the access of AI delegates to sensitive user information, many social scenarios require disclosing private details to achieve desired outcomes, necessitating a balance between privacy protection and disclosure. To address this challenge, we conduct a pilot study to investigate user preferences for AI delegates across various social relations and task scenarios, and then propose a novel AI delegate system that enables privacy-conscious self-disclosure. Our user study demonstrates that the proposed AI delegate strategically protects privacy, pioneering its use in diverse and dynamic social interactions.         ",
    "url": "https://arxiv.org/abs/2409.17642",
    "authors": [
      "Xi Chen",
      "Zhiyang Zhang",
      "Fangkai Yang",
      "Xiaoting Qin",
      "Chao Du",
      "Xi Cheng",
      "Hangxin Liu",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Qi Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2409.17647",
    "title": "MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning",
    "abstract": "           Video causal reasoning aims to achieve a high-level understanding of video content from a causal perspective. However, current video reasoning tasks are limited in scope, primarily executed in a question-answering paradigm and focusing on short videos containing only a single event and simple causal relationships, lacking comprehensive and structured causality analysis for videos with multiple events. To fill this gap, we introduce a new task and dataset, Multi-Event Causal Discovery (MECD). It aims to uncover the causal relationships between events distributed chronologically across long videos. Given visual segments and textual descriptions of events, MECD requires identifying the causal associations between these events to derive a comprehensive, structured event-level video causal diagram explaining why and how the final result event occurred. To address MECD, we devise a novel framework inspired by the Granger Causality method, using an efficient mask-based event prediction model to perform an Event Granger Test, which estimates causality by comparing the predicted result event when premise events are masked versus unmasked. Furthermore, we integrate causal inference techniques such as front-door adjustment and counterfactual inference to address challenges in MECD like causality confounding and illusory causality. Experiments validate the effectiveness of our framework in providing causal relationships in multi-event videos, outperforming GPT-4o and VideoLLaVA by 5.7% and 4.1%, respectively.         ",
    "url": "https://arxiv.org/abs/2409.17647",
    "authors": [
      "Tieyuan Chen",
      "Huabin Liu",
      "Tianyao He",
      "Yihang Chen",
      "Chaofan Gan",
      "Xiao Ma",
      "Cheng Zhong",
      "Yang Zhang",
      "Yingxue Wang",
      "Hui Lin",
      "Weiyao Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17649",
    "title": "Provable Performance Guarantees of Copy Detection Patterns",
    "abstract": "           Copy Detection Patterns (CDPs) are crucial elements in modern security applications, playing a vital role in safeguarding industries such as food, pharmaceuticals, and cosmetics. Current performance evaluations of CDPs predominantly rely on empirical setups using simplistic metrics like Hamming distances or Pearson correlation. These methods are often inadequate due to their sensitivity to distortions, degradation, and their limitations to stationary statistics of printing and imaging. Additionally, machine learning-based approaches suffer from distribution biases and fail to generalize to unseen counterfeit samples. Given the critical importance of CDPs in preventing counterfeiting, including the counterfeit vaccines issue highlighted during the COVID-19 pandemic, there is an urgent need for provable performance guarantees across various criteria. This paper aims to establish a theoretical framework to derive optimal criteria for the analysis, optimization, and future development of CDP authentication technologies, ensuring their reliability and effectiveness in diverse security scenarios.         ",
    "url": "https://arxiv.org/abs/2409.17649",
    "authors": [
      "Joakim Tutt",
      "Slava Voloshynovskiy"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17652",
    "title": "FactorSim: Generative Simulation via Factorized Representation",
    "abstract": "           Generating simulations to train intelligent agents in game-playing and robotics from natural language input, from user input or task documentation, remains an open-ended challenge. Existing approaches focus on parts of this challenge, such as generating reward functions or task hyperparameters. Unlike previous work, we introduce FACTORSIM that generates full simulations in code from language input that can be used to train agents. Exploiting the structural modularity specific to coded simulations, we propose to use a factored partially observable Markov decision process representation that allows us to reduce context dependence during each step of the generation. For evaluation, we introduce a generative simulation benchmark that assesses the generated simulation code's accuracy and effectiveness in facilitating zero-shot transfers in reinforcement learning settings. We show that FACTORSIM outperforms existing methods in generating simulations regarding prompt alignment (e.g., accuracy), zero-shot transfer abilities, and human evaluation. We also demonstrate its effectiveness in generating robotic tasks.         ",
    "url": "https://arxiv.org/abs/2409.17652",
    "authors": [
      "Fan-Yun Sun",
      "S. I. Harini",
      "Angela Yi",
      "Yihan Zhou",
      "Alex Zook",
      "Jonathan Tremblay",
      "Logan Cross",
      "Jiajun Wu",
      "Nick Haber"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.17656",
    "title": "Prototype based Masked Audio Model for Self-Supervised Learning of Sound Event Detection",
    "abstract": "           A significant challenge in sound event detection (SED) is the effective utilization of unlabeled data, given the limited availability of labeled data due to high annotation costs. Semi-supervised algorithms rely on labeled data to learn from unlabeled data, and the performance is constrained by the quality and size of the former. In this paper, we introduce the Prototype based Masked Audio Model~(PMAM) algorithm for self-supervised representation learning in SED, to better exploit unlabeled data. Specifically, semantically rich frame-level pseudo labels are constructed from a Gaussian mixture model (GMM) based prototypical distribution modeling. These pseudo labels supervise the learning of a Transformer-based masked audio model, in which binary cross-entropy loss is employed instead of the widely used InfoNCE loss, to provide independent loss contributions from different prototypes, which is important in real scenarios in which multiple labels may apply to unsupervised data frames. A final stage of fine-tuning with just a small amount of labeled data yields a very high performing SED model. On like-for-like tests using the DESED task, our method achieves a PSDS1 score of 62.5\\%, surpassing current state-of-the-art models and demonstrating the superiority of the proposed technique.         ",
    "url": "https://arxiv.org/abs/2409.17656",
    "authors": [
      "Pengfei Cai",
      "Yan Song",
      "Nan Jiang",
      "Qing Gu",
      "Ian McLoughlin"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2409.17673",
    "title": "Cross-lingual Human-Preference Alignment for Neural Machine Translation with Direct Quality Optimization",
    "abstract": "           Reinforcement Learning from Human Feedback (RLHF) and derivative techniques like Direct Preference Optimization (DPO) are task-alignment algorithms used to repurpose general, foundational models for specific tasks. We show that applying task-alignment to neural machine translation (NMT) addresses an existing task--data mismatch in NMT, leading to improvements across all languages of a multilingual model, even when task-alignment is only applied to a subset of those languages. We do so by introducing Direct Quality Optimization (DQO), a variant of DPO leveraging a pre-trained translation quality estimation model as a proxy for human preferences, and verify the improvements with both automatic metrics and human evaluation.         ",
    "url": "https://arxiv.org/abs/2409.17673",
    "authors": [
      "Kaden Uhlig",
      "Joern Wuebker",
      "Raphael Reinauer",
      "John DeNero"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.17674",
    "title": "Self-Supervised Learning of Deviation in Latent Representation for Co-speech Gesture Video Generation",
    "abstract": "           Gestures are pivotal in enhancing co-speech communication. While recent works have mostly focused on point-level motion transformation or fully supervised motion representations through data-driven approaches, we explore the representation of gestures in co-speech, with a focus on self-supervised representation and pixel-level motion deviation, utilizing a diffusion model which incorporates latent motion features. Our approach leverages self-supervised deviation in latent representation to facilitate hand gestures generation, which are crucial for generating realistic gesture videos. Results of our first experiment demonstrate that our method enhances the quality of generated videos, with an improvement from 2.7 to 4.5% for FGD, DIV, and FVD, and 8.1% for PSNR, 2.5% for SSIM over the current state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2409.17674",
    "authors": [
      "Huan Yang",
      "Jiahui Chen",
      "Chaofan Ding",
      "Runhua Shi",
      "Siyu Xiong",
      "Qingqi Hong",
      "Xiaoqi Mo",
      "Xinhan Di"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17678",
    "title": "Modeling the Popularity of Events on Web by Sparsity and Mutual-Excitation Guided Graph Neural Network",
    "abstract": "           The content of a webpage described or posted an event in the cyberspace inevitably reflects viewpoints, values and trends of the physical society. Mapping an event on web to the popularity score plays a pivot role to sense the social trends from the cyberspace. However, the complex semantic correspondence between texts and images, as well as the implicit text-image-popularity mapping mechanics pose a significant challenge to this non-trivial task. In this paper, we address this problem from a viewpoint of understanding the interpretable mapping mechanics. Concretely, we organize the keywords from different events into an unified graph. The unified graph facilitates to model the popularity of events via two-level mappings, i.e., the self excitation and the mutual excitation. The self-excitation assumes that each keyword forms the popularity while the mutual-excitation models that two keywords would excite each other to determine the popularity of an event. Specifically, we use Graph Neural Network (GNN) as the backbone to model the self-excitation, the mutual excitation and the context of images into a sparse and deep factor model. Besides, to our best knowledge, we release a challenge web event dataset for the popularity prediction task. The experimental results on three public datasets demonstrate that our method achieves significant improvements and outperforms the state-of-the-art methods. Dataset is publicly available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17678",
    "authors": [
      "Jiaxin Deng",
      "Linlin Jia",
      "Junbiao Pang",
      "Qingming Huang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2409.17681",
    "title": "Computation Pre-Offloading for MEC-Enabled Vehicular Networks via Trajectory Prediction",
    "abstract": "           Task offloading is of paramount importance to efficiently orchestrate vehicular wireless networks, necessitating the availability of information regarding the current network status and computational resources. However, due to the mobility of the vehicles and the limited computational resources for performing task offloading in near-real-time, such schemes may require high latency, thus, become even infeasible. To address this issue, in this paper, we present a Trajectory Prediction-based Pre-offloading Decision (TPPD) algorithm for analyzing the historical trajectories of vehicles to predict their future coordinates, thereby allowing for computational resource allocation in advance. We first utilize the Long Short-Term Memory (LSTM) network model to predict each vehicle's movement trajectory. Then, based on the task requirements and the predicted trajectories, we devise a dynamic resource allocation algorithm using a Double Deep Q-Network (DDQN) that enables the edge server to minimize task processing delay, while ensuring effective utilization of the available computational resources. Our simulation results verify the effectiveness of the proposed approach, showcasing that, as compared with traditional real-time task offloading strategies, the proposed TPPD algorithm significantly reduces task processing delay while improving resource utilization.         ",
    "url": "https://arxiv.org/abs/2409.17681",
    "authors": [
      "Ting Zhang",
      "Bo Yang",
      "Zhiwen Yu",
      "Xuelin Cao",
      "George C. Alexandropoulos",
      "Yan Zhang",
      "Chau Yuen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2409.17687",
    "title": "Graph Edit Distance with General Costs Using Neural Set Divergence",
    "abstract": "           Graph Edit Distance (GED) measures the (dis-)similarity between two given graphs, in terms of the minimum-cost edit sequence that transforms one graph to the other. However, the exact computation of GED is NP-Hard, which has recently motivated the design of neural methods for GED estimation. However, they do not explicitly account for edit operations with different costs. In response, we propose GRAPHEDX, a neural GED estimator that can work with general costs specified for the four edit operations, viz., edge deletion, edge addition, node deletion and node addition. We first present GED as a quadratic assignment problem (QAP) that incorporates these four costs. Then, we represent each graph as a set of node and edge embeddings and use them to design a family of neural set divergence surrogates. We replace the QAP terms corresponding to each operation with their surrogates. Computing such neural set divergence require aligning nodes and edges of the two graphs. We learn these alignments using a Gumbel-Sinkhorn permutation generator, additionally ensuring that the node and edge alignments are consistent with each other. Moreover, these alignments are cognizant of both the presence and absence of edges between node-pairs. Experiments on several datasets, under a variety of edit cost settings, show that GRAPHEDX consistently outperforms state-of-the-art methods and heuristics in terms of prediction error.         ",
    "url": "https://arxiv.org/abs/2409.17687",
    "authors": [
      "Eeshaan Jain",
      "Indradyumna Roy",
      "Saswat Meher",
      "Soumen Chakrabarti",
      "Abir De"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17688",
    "title": "HPC acceleration of large (min, +) matrix products to compute domination-type parameters in graphs",
    "abstract": "           The computation of the domination-type parameters is a challenging problem in Cartesian product graphs. We present an algorithmic method to compute the $2$-domination number of the Cartesian product of a path with small order and any cycle, involving the $(\\min,+)$ matrix product. We establish some theoretical results that provide the algorithms necessary to compute that parameter, and the main challenge to run such algorithms comes from the large size of the matrices used, which makes it necessary to improve the techniques to handle these objects. We analyze the performance of the algorithms on modern multicore CPUs and on GPUs and we show the advantages over the sequential implementation. The use of these platforms allows us to compute the $2$-domination number of cylinders such that their paths have at most $12$ vertices.         ",
    "url": "https://arxiv.org/abs/2409.17688",
    "authors": [
      "E.M. Garz\u00f3n",
      "J.A. Mart\u00ednez",
      "J.J. Moreno",
      "M.L. Puertas"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2409.17693",
    "title": "Spatial embedding promotes a specific form of modularity with low entropy and heterogeneous spectral dynamics",
    "abstract": "           Understanding how biological constraints shape neural computation is a central goal of computational neuroscience. Spatially embedded recurrent neural networks provide a promising avenue to study how modelled constraints shape the combined structural and functional organisation of networks over learning. Prior work has shown that spatially embedded systems like this can combine structure and function into single artificial models during learning. But it remains unclear precisely how, in general, structural constraints bound the range of attainable configurations. In this work, we show that it is possible to study these restrictions through entropic measures of the neural weights and eigenspectrum, across both rate and spiking neural networks. Spatial embedding, in contrast to baseline models, leads to networks with a highly specific low entropy modularity where connectivity is readily interpretable given the known spatial and communication constraints acting on them. Crucially, these networks also demonstrate systematically modulated spectral dynamics, revealing how they exploit heterogeneity in their function to overcome the constraints imposed on their structure. This work deepens our understanding of constrained learning in neural networks, across coding schemes and tasks, where solutions to simultaneous structural and functional objectives must be accomplished in tandem.         ",
    "url": "https://arxiv.org/abs/2409.17693",
    "authors": [
      "Cornelia Sheeran",
      "Andrew S. Ham",
      "Duncan E. Astle",
      "Jascha Achterberg",
      "Danyal Akarca"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2409.17699",
    "title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks",
    "abstract": "           The proliferation of Large Language Models (LLMs) in diverse applications underscores the pressing need for robust security measures to thwart potential jailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger data integrity and user privacy. Guardrails serve as crucial protective mechanisms against such threats, but existing models often fall short in terms of both detection accuracy, and computational efficiency. This paper advocates for the significance of jailbreak attack prevention on LLMs, and emphasises the role of input guardrails in safeguarding these models. We introduce MoJE (Mixture of Jailbreak Expert), a novel guardrail architecture designed to surpass current limitations in existing state-of-the-art guardrails. By employing simple linguistic statistical techniques, MoJE excels in detecting jailbreak attacks while maintaining minimal computational overhead during model inference. Through rigorous experimentation, MoJE demonstrates superior performance capable of detecting 90% of the attacks without compromising benign prompts, enhancing LLMs security against jailbreak attacks.         ",
    "url": "https://arxiv.org/abs/2409.17699",
    "authors": [
      "Giandomenico Cornacchia",
      "Giulio Zizzo",
      "Kieran Fraser",
      "Muhammad Zaid Hamed",
      "Ambrish Rawat",
      "Mark Purcell"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17700",
    "title": "Demystifying Privacy in 5G Stand Alone Networks",
    "abstract": "           Ensuring user privacy remains critical in mobile networks, particularly with the rise of connected devices and denser 5G infrastructure. Privacy concerns have persisted across 2G, 3G, and 4G/LTE networks. Recognizing these concerns, the 3rd Generation Partnership Project (3GPP) has made privacy enhancements in 5G Release 15. However, the extent of operator adoption remains unclear, especially as most networks operate in 5G Non Stand Alone (NSA) mode, relying on 4G Core Networks. This study provides the first qualitative and experimental comparison between 5G NSA and Stand Alone (SA) in real operator networks, focusing on privacy enhancements addressing top eight pre-5G attacks based on recent academic literature. Additionally, it evaluates the privacy levels of OpenAirInterface (OAI), a leading open-source software for 5G, against real network deployments for the same attacks. The analysis reveals two new 5G privacy vulnerabilities, underscoring the need for further research and stricter standards.         ",
    "url": "https://arxiv.org/abs/2409.17700",
    "authors": [
      "Stavros Eleftherakis",
      "Timothy Otim",
      "Giuseppe Santaromita",
      "Almudena Diaz Zayas",
      "Domenico Giustiniano",
      "Nicolas Kourtellis"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2409.17705",
    "title": "On the Output Redundancy of LTI Systems: A Geometric Approach with Application to Privacy",
    "abstract": "           This paper examines the properties of output-redundant systems, that is, systems possessing a larger number of outputs than inputs, through the lenses of the geometric approach of Wonham et al. We begin by formulating a simple output allocation synthesis problem, which involves ``concealing\" input information from a malicious eavesdropper having access to the system output, while still allowing for a legitimate user to reconstruct it. It is shown that the solvability of this problem requires the availability of a redundant set of outputs. This very problem is instrumental to unveiling the fundamental geometric properties of output-redundant systems, which form the basis for our subsequent constructions and results. As a direct application, we demonstrate how output allocation can be employed to effectively protect the information of input information from certain output eavesdroppers with guaranteed results.         ",
    "url": "https://arxiv.org/abs/2409.17705",
    "authors": [
      "Guitao Yang",
      "Alexander J. Gallo",
      "Angelo Barboni",
      "Riccardo M.G. Ferrari",
      "Andrea Serrani",
      "Thomas Parisini"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.17729",
    "title": "Neural Implicit Representation for Highly Dynamic LiDAR Mapping and Odometry",
    "abstract": "           Recent advancements in Simultaneous Localization and Mapping (SLAM) have increasingly highlighted the robustness of LiDAR-based techniques. At the same time, Neural Radiance Fields (NeRF) have introduced new possibilities for 3D scene reconstruction, exemplified by SLAM systems. Among these, NeRF-LOAM has shown notable performance in NeRF-based SLAM applications. However, despite its strengths, these systems often encounter difficulties in dynamic outdoor environments due to their inherent static assumptions. To address these limitations, this paper proposes a novel method designed to improve reconstruction in highly dynamic outdoor scenes. Based on NeRF-LOAM, the proposed approach consists of two primary components. First, we separate the scene into static background and dynamic foreground. By identifying and excluding dynamic elements from the mapping process, this segmentation enables the creation of a dense 3D map that accurately represents the static background only. The second component extends the octree structure to support multi-resolution representation. This extension not only enhances reconstruction quality but also aids in the removal of dynamic objects identified by the first module. Additionally, Fourier feature encoding is applied to the sampled points, capturing high-frequency information and leading to more complete reconstruction results. Evaluations on various datasets demonstrate that our method achieves more competitive results compared to current state-of-the-art approaches.         ",
    "url": "https://arxiv.org/abs/2409.17729",
    "authors": [
      "Qi Zhang",
      "He Wang",
      "Ru Li",
      "Wenbin Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17731",
    "title": "Robust Ladder Climbing with a Quadrupedal Robot",
    "abstract": "           Quadruped robots are proliferating in industrial environments where they carry sensor suites and serve as autonomous inspection platforms. Despite the advantages of legged robots over their wheeled counterparts on rough and uneven terrain, they are still yet to be able to reliably negotiate ubiquitous features of industrial infrastructure: ladders. Inability to traverse ladders prevents quadrupeds from inspecting dangerous locations, puts humans in harm's way, and reduces industrial site productivity. In this paper, we learn quadrupedal ladder climbing via a reinforcement learning-based control policy and a complementary hooked end-effector. We evaluate the robustness in simulation across different ladder inclinations, rung geometries, and inter-rung spacings. On hardware, we demonstrate zero-shot transfer with an overall 90% success rate at ladder angles ranging from 70\u00b0 to 90\u00b0, consistent climbing performance during unmodeled perturbations, and climbing speeds 232x faster than the state of the art. This work expands the scope of industrial quadruped robot applications beyond inspection on nominal terrains to challenging infrastructural features in the environment, highlighting synergies between robot morphology and control policy when performing complex skills. More information can be found at the project website: this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17731",
    "authors": [
      "Dylan Vogel",
      "Robert Baines",
      "Joseph Church",
      "Julian Lotzer",
      "Karl Werner",
      "Marco Hutter"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.17742",
    "title": "TADAR: Thermal Array-based Detection and Ranging for Privacy-Preserving Human Sensing",
    "abstract": "           Human sensing has gained increasing attention in various applications. Among the available technologies, visual images offer high accuracy, while sensing on the RF spectrum preserves privacy, creating a conflict between imaging resolution and privacy preservation. In this paper, we explore thermal array sensors as an emerging modality that strikes an excellent resolution-privacy balance for ubiquitous sensing. To this end, we present TADAR, the first multi-user Thermal Array-based Detection and Ranging system that estimates the inherently missing range information, extending thermal array outputs from 2D thermal pixels to 3D depths and empowering them as a promising modality for ubiquitous privacy-preserving human sensing. We prototype TADAR using a single commodity thermal array sensor and conduct extensive experiments in different indoor environments. Our results show that TADAR achieves a mean F1 score of 88.8% for multi-user detection and a mean accuracy of 32.0 cm for multi-user ranging, which further improves to 20.1 cm for targets located within 3 m. We conduct two case studies on fall detection and occupancy estimation to showcase the potential applications of TADAR. We hope TADAR will inspire the vast community to explore new directions of thermal array sensing, beyond wireless and acoustic sensing. TADAR is open-sourced on GitHub: this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17742",
    "authors": [
      "Xie Zhang",
      "Chenshu Wu"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2409.17744",
    "title": "Privacy for Quantum Annealing. Attack on Spin Reversal Transformations in the case of cryptanalysis",
    "abstract": "           This paper demonstrates that applying spin reversal transformations (SRT), commonly known as a sufficient method for privacy enhancing in problems solved using quantum annealing, does not guarantee privacy for all possible problems. We show how to recover the original problem from the Ising problem obtained using SRT when the resulting problem in Ising form represents the algebraic attack on the $E_0$ stream cipher. A small example is used to illustrate how to retrieve the original problem from the one transformed by SRT. Moreover, it is shown that our method is efficient even for full-scale problems.         ",
    "url": "https://arxiv.org/abs/2409.17744",
    "authors": [
      "Mateusz Le\u015bniak",
      "Micha\u0142 Wro\u0144ski"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.17756",
    "title": "Stackelberg Attack on Protocol Fee Governance",
    "abstract": "           We establish a Stackelberg attack by Liquidity Providers against Governance of an AMM, leveraging forking and commitments through a Grim Forker smart contract. We produce a dynamic, block-by-block model of AMM reserves and trading volume in the presence of competing forks, derive equilibrium conditions in the presence of protocol fees, and analyze Stackelberg equilibria with smart contract moves.         ",
    "url": "https://arxiv.org/abs/2409.17756",
    "authors": [
      "Alexandre Hajjar"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2409.17758",
    "title": "Adapting Deep Variational Bayes Filter for Enhanced Confidence Estimation in Finite Element Method Integrated Networks (FEMIN)",
    "abstract": "           The Finite Element Method (FEM) is a widely used technique for simulating crash scenarios with high accuracy and reliability. To reduce the significant computational costs associated with FEM, the Finite Element Method Integrated Networks (FEMIN) framework integrates neural networks (NNs) with FEM solvers. However, this integration can introduce errors and deviations from full-FEM simulations, highlighting the need for an additional metric to assess prediction confidence, especially when no ground truth data is available. In this study, we adapt the Deep Variational Bayes Filter (DVBF) to the FEMIN framework, incorporating a probabilistic approach to provide qualitative insights into prediction confidence during FEMIN simulations. The adaptation involves using the learned transition model for a predictive decoding step, generating a preliminary force prediction. This predictive force is used alongside the displacement and the velocity data from the FEM solver as input for the encoder model. The decoder reconstructs the likelihood distribution based on the posterior. The mean force of this distribution is applied to the FEM solver, while the predicted standard deviation can be used for uncertainty estimation. Our findings demonstrate that the DVBF outperforms deterministic NN architectures in terms of accuracy. Furthermore, the standard deviation derived from the decoder serves as a valuable qualitative metric for assessing the confidence in FEMIN simulations. This approach enhances the robustness of FEMIN by providing a measure of reliability alongside the simulation results.         ",
    "url": "https://arxiv.org/abs/2409.17758",
    "authors": [
      "Simon Thel",
      "Lars Greve",
      "Maximilian Karl",
      "Patrick van der Smagt"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2409.17774",
    "title": "Faithfulness and the Notion of Adversarial Sensitivity in NLP Explanations",
    "abstract": "           Faithfulness is arguably the most critical metric to assess the reliability of explainable AI. In NLP, current methods for faithfulness evaluation are fraught with discrepancies and biases, often failing to capture the true reasoning of models. We introduce Adversarial Sensitivity as a novel approach to faithfulness evaluation, focusing on the explainer's response when the model is under adversarial attack. Our method accounts for the faithfulness of explainers by capturing sensitivity to adversarial input changes. This work addresses significant limitations in existing evaluation techniques, and furthermore, quantifies faithfulness from a crucial yet underexplored paradigm.         ",
    "url": "https://arxiv.org/abs/2409.17774",
    "authors": [
      "Supriya Manna",
      "Niladri Sett"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17786",
    "title": "Predicting the Stay Length of Patients in Hospitals using Convolutional Gated Recurrent Deep Learning Model",
    "abstract": "           Predicting hospital length of stay (LoS) stands as a critical factor in shaping public health strategies. This data serves as a cornerstone for governments to discern trends, patterns, and avenues for enhancing healthcare delivery. In this study, we introduce a robust hybrid deep learning model, a combination of Multi-layer Convolutional (CNNs) deep learning, Gated Recurrent Units (GRU), and Dense neural networks, that outperforms 11 conventional and state-of-the-art Machine Learning (ML) and Deep Learning (DL) methodologies in accurately forecasting inpatient hospital stay duration. Our investigation delves into the implementation of this hybrid model, scrutinising variables like geographic indicators tied to caregiving institutions, demographic markers encompassing patient ethnicity, race, and age, as well as medical attributes such as the CCS diagnosis code, APR DRG code, illness severity metrics, and hospital stay duration. Statistical evaluations reveal the pinnacle LoS accuracy achieved by our proposed model (CNN-GRU-DNN), which averages at 89% across a 10-fold cross-validation test, surpassing LSTM, BiLSTM, GRU, and Convolutional Neural Networks (CNNs) by 19%, 18.2%, 18.6%, and 7%, respectively. Accurate LoS predictions not only empower hospitals to optimise resource allocation and curb expenses associated with prolonged stays but also pave the way for novel strategies in hospital stay management. This avenue holds promise for catalysing advancements in healthcare research and innovation, inspiring a new era of precision-driven healthcare practices.         ",
    "url": "https://arxiv.org/abs/2409.17786",
    "authors": [
      "Mehdi Neshat",
      "Michael Phipps",
      "Chris A. Browne",
      "Nicole T. Vargas",
      "Seyedali Mirjalili"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17788",
    "title": "Ophthalmic Biomarker Detection with Parallel Prediction of Transformer and Convolutional Architecture",
    "abstract": "           Ophthalmic diseases represent a significant global health issue, necessitating the use of advanced precise diagnostic tools. Optical Coherence Tomography (OCT) imagery which offers high-resolution cross-sectional images of the retina has become a pivotal imaging modality in ophthalmology. Traditionally physicians have manually detected various diseases and biomarkers from such diagnostic imagery. In recent times, deep learning techniques have been extensively used for medical diagnostic tasks enabling fast and precise diagnosis. This paper presents a novel approach for ophthalmic biomarker detection using an ensemble of Convolutional Neural Network (CNN) and Vision Transformer. While CNNs are good for feature extraction within the local context of the image, transformers are known for their ability to extract features from the global context of the image. Using an ensemble of both techniques allows us to harness the best of both worlds. Our method has been implemented on the OLIVES dataset to detect 6 major biomarkers from the OCT images and shows significant improvement of the macro averaged F1 score on the dataset.         ",
    "url": "https://arxiv.org/abs/2409.17788",
    "authors": [
      "Md. Touhidul Islam",
      "Md. Abtahi Majeed Chowdhury",
      "Mahmudul Hasan",
      "Asif Quadir",
      "Lutfa Aktar"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17790",
    "title": "CASPFormer: Trajectory Prediction from BEV Images with Deformable Attention",
    "abstract": "           Motion prediction is an important aspect for Autonomous Driving (AD) and Advance Driver Assistance Systems (ADAS). Current state-of-the-art motion prediction methods rely on High Definition (HD) maps for capturing the surrounding context of the ego vehicle. Such systems lack scalability in real-world deployment as HD maps are expensive to produce and update in real-time. To overcome this issue, we propose Context Aware Scene Prediction Transformer (CASPFormer), which can perform multi-modal motion prediction from rasterized Bird-Eye-View (BEV) images. Our system can be integrated with any upstream perception module that is capable of generating BEV images. Moreover, CASPFormer directly decodes vectorized trajectories without any postprocessing. Trajectories are decoded recurrently using deformable attention, as it is computationally efficient and provides the network with the ability to focus its attention on the important spatial locations of the BEV images. In addition, we also address the issue of mode collapse for generating multiple scene-consistent trajectories by incorporating learnable mode queries. We evaluate our model on the nuScenes dataset and show that it reaches state-of-the-art across multiple metrics         ",
    "url": "https://arxiv.org/abs/2409.17790",
    "authors": [
      "Harsh Yadav",
      "Maximilian Schaefer",
      "Kun Zhao",
      "Tobias Meisen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17791",
    "title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness",
    "abstract": "           Recently, there has been significant interest in replacing the reward model in Reinforcement Learning with Human Feedback (RLHF) methods for Large Language Models (LLMs), such as Direct Preference Optimization (DPO) and its variants. These approaches commonly use a binary cross-entropy mechanism on pairwise samples, i.e., minimizing and maximizing the loss based on preferred or dis-preferred responses, respectively. However, while this training strategy omits the reward model, it also overlooks the varying preference degrees within different responses. We hypothesize that this is a key factor hindering LLMs from sufficiently understanding human preferences. To address this problem, we propose a novel Self-supervised Preference Optimization (SPO) framework, which constructs a self-supervised preference degree loss combined with the alignment loss, thereby helping LLMs improve their ability to understand the degree of preference. Extensive experiments are conducted on two widely used datasets of different tasks. The results demonstrate that SPO can be seamlessly integrated with existing preference optimization methods and significantly boost their performance to achieve state-of-the-art performance. We also conduct detailed analyses to offer comprehensive insights into SPO, which verifies its effectiveness. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17791",
    "authors": [
      "Jian Li",
      "Haojing Huang",
      "Yujia Zhang",
      "Pengfei Xu",
      "Xi Chen",
      "Rui Song",
      "Lida Shi",
      "Jingwen Wang",
      "Hao Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17795",
    "title": "Physics-driven complex relaxation for multi-body systems of SPH method",
    "abstract": "           In the smoothed particle dynamics (SPH) method, the characteristics of a target particle are interpolated based on the information from its neighboring particles. Consequently, a uniform initial distribution of particles significantly enhances the accuracy of SPH calculations. This aspect is particularly critical in Eulerian SPH, where particles are stationary throughout the simulation. To address this, we introduce a physics-driven complex relaxation method for multi-body systems. Through a series of two-dimensional and three-dimensional case studies, we demonstrate that this method is capable of achieving a globally uniform particle distribution, especially at the interfaces between contacting bodies, and ensuring improved zero-order consistency. Moreover, the effectiveness and reliability of the complex relaxation method in enhancing the accuracy of physical simulations are further validated.         ",
    "url": "https://arxiv.org/abs/2409.17795",
    "authors": [
      "Chenxi Zhao",
      "Yongchuan Yu",
      "Oskar J. Haidn",
      "Xiangyu Hu"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2409.17800",
    "title": "Bias Assessment and Data Drift Detection in Medical Image Analysis: A Survey",
    "abstract": "           Machine Learning (ML) models have gained popularity in medical imaging analysis given their expert level performance in many medical domains. To enhance the trustworthiness, acceptance, and regulatory compliance of medical imaging models and to facilitate their integration into clinical settings, we review and categorise methods for ensuring ML reliability, both during development and throughout the model's lifespan. Specifically, we provide an overview of methods assessing models' inner-workings regarding bias encoding and detection of data drift for disease classification models. Additionally, to evaluate the severity in case of a significant drift, we provide an overview of the methods developed for classifier accuracy estimation in case of no access to ground truth labels. This should enable practitioners to implement methods ensuring reliable ML deployment and consistent prediction performance over time.         ",
    "url": "https://arxiv.org/abs/2409.17800",
    "authors": [
      "Andrea Prenner",
      "Bernhard Kainz"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2409.17834",
    "title": "PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent Representation MOdification",
    "abstract": "           Due to their substantial sizes, large language models (LLMs) are typically deployed within a single-backbone multi-tenant framework. In this setup, a single instance of an LLM backbone must cater to multiple users or tasks through the application of various parameter-efficient fine-tuning (PEFT) models. Despite the availability of numerous effective PEFT techniques such as LoRA, there remains a need for a PEFT approach that achieves both high efficiency during inference and competitive performance on downstream tasks. In this research, we introduce a new and straightforward PEFT methodology named \\underline{P}rompt D\\underline{E}pen\\underline{D}ent \\underline{R}epresentation M\\underline{O}dification (PEDRO). The proposed method involves integrating a lightweight vector generator into each Transformer layer, which generates vectors contingent upon the input prompts. These vectors then modify the hidden representations created by the LLM through a dot product operation, thereby influencing the semantic output and generated content of the model. Extensive experimentation across a variety of tasks indicates that: (a) PEDRO surpasses recent PEFT benchmarks when using a similar number of tunable parameters. (b) Under the single-backbone multi-tenant deployment model, PEDRO exhibits superior efficiency compared to LoRA, indicating significant industrial potential.         ",
    "url": "https://arxiv.org/abs/2409.17834",
    "authors": [
      "Tianfang Xie",
      "Tianjing Li",
      "Wei Zhu",
      "Wei Han",
      "Yi Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.17836",
    "title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards General Neural Parameter Prior Models",
    "abstract": "           Despite the widespread use of statistical prior models in various fields, such models for neural network gradients have long been overlooked. The inherent challenge stems from their high-dimensional structures and complex interdependencies, which complicate effective modeling. In this work, we demonstrate the potential of large language models (LLMs) to act as gradient priors in a zero-shot setting. We examine the property by considering lossless gradient compression -- a critical application in distributed learning -- that depends heavily on precise probability modeling. To achieve this, we introduce LM-GC, a novel method that integrates LLMs with arithmetic coding. Our technique converts plain gradients into text-like formats, enhancing token efficiency by up to 38 times compared to their plain representations. We ensure that this data conversion maintains a close alignment with the structure of plain gradients and the symbols commonly recognized by LLMs. Our experiments indicate that LM-GC surpasses existing state-of-the-art lossless compression methods, improving compression rates by 10\\% up to 17.2\\% across various datasets and architectures. Additionally, our approach shows promising compatibility with lossy compression techniques such as quantization and sparsification. These findings highlight the significant potential of LLMs as a model for effectively handling gradients. We will release the source code upon publication.         ",
    "url": "https://arxiv.org/abs/2409.17836",
    "authors": [
      "Hui-Po Wang",
      "Mario Fritz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17840",
    "title": "Detecting and Measuring Confounding Using Causal Mechanism Shifts",
    "abstract": "           Detecting and measuring confounding effects from data is a key challenge in causal inference. Existing methods frequently assume causal sufficiency, disregarding the presence of unobserved confounding variables. Causal sufficiency is both unrealistic and empirically untestable. Additionally, existing methods make strong parametric assumptions about the underlying causal generative process to guarantee the identifiability of confounding variables. Relaxing the causal sufficiency and parametric assumptions and leveraging recent advancements in causal discovery and confounding analysis with non-i.i.d. data, we propose a comprehensive approach for detecting and measuring confounding. We consider various definitions of confounding and introduce tailored methodologies to achieve three objectives: (i) detecting and measuring confounding among a set of variables, (ii) separating observed and unobserved confounding effects, and (iii) understanding the relative strengths of confounding bias between different sets of variables. We present useful properties of a confounding measure and present measures that satisfy those properties. Empirical results support the theoretical analysis.         ",
    "url": "https://arxiv.org/abs/2409.17840",
    "authors": [
      "Abbavaram Gowtham Reddy",
      "Vineeth N Balasubramanian"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17841",
    "title": "Machine Learning-based vs Deep Learning-based Anomaly Detection in Multivariate Time Series for Spacecraft Attitude Sensors",
    "abstract": "           In the framework of Failure Detection, Isolation and Recovery (FDIR) on spacecraft, new AI-based approaches are emerging in the state of the art to overcome the limitations commonly imposed by traditional threshold checking. The present research aims at characterizing two different approaches to the problem of stuck values detection in multivariate time series coming from spacecraft attitude sensors. The analysis reveals the performance differences in the two approaches, while commenting on their interpretability and generalization to different scenarios.         ",
    "url": "https://arxiv.org/abs/2409.17841",
    "authors": [
      "R. Gallon",
      "F. Schiemenz",
      "A. Krstova",
      "A. Menicucci",
      "E. Gill"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.17843",
    "title": "Auction-based Adaptive Resource Allocation Optimization in Dense IoT Networks",
    "abstract": "           The rapid pervasivity of the Internet of Things (IoT) calls for an autonomous and efficient resource management framework to seamlessly register and discover facilities and services. Cloud-Fog-Automation (CFA) standards provide a robust foundation for multi-tiered wireless architectures, enhancing cyber-physical system performance with advanced abstractions. This work is for resource allocation optimization in IoT networks, particularly in power management and time-frequency spreading techniques, ensuring deterministic connectivity, networked computing, and intelligent control systems. Auction game theory is pivotal in managing resource allocation in densely populated, high-demand IoT networks. By employing sealed-bid auctions based on Bayesian game theory, the uncertainties in individual hypotheses and channel states among IoT entities are effectively mitigated. A novel dispersion metric optimization further enhances the coordination of layer-specific IoT uplinks, enabling ultra-reliable, low-latency (URLLC) communication. Numerical results demonstrate the superior performance of this resilient architecture, achieving fair resource allocation with minimal power consumption and robust performance in unsecured scenarios.         ",
    "url": "https://arxiv.org/abs/2409.17843",
    "authors": [
      "Nirmal D. Wickramasinghe",
      "John Dooley",
      "Dirk Pesch",
      "Indrakshi Dey"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2409.17864",
    "title": "A Multimodal Single-Branch Embedding Network for Recommendation in Cold-Start and Missing Modality Scenarios",
    "abstract": "           Most recommender systems adopt collaborative filtering (CF) and provide recommendations based on past collective interactions. Therefore, the performance of CF algorithms degrades when few or no interactions are available, a scenario referred to as cold-start. To address this issue, previous work relies on models leveraging both collaborative data and side information on the users or items. Similar to multimodal learning, these models aim at combining collaborative and content representations in a shared embedding space. In this work we propose a novel technique for multimodal recommendation, relying on a multimodal Single-Branch embedding network for Recommendation (SiBraR). Leveraging weight-sharing, SiBraR encodes interaction data as well as multimodal side information using the same single-branch embedding network on different modalities. This makes SiBraR effective in scenarios of missing modality, including cold start. Our extensive experiments on large-scale recommendation datasets from three different recommendation domains (music, movie, and e-commerce) and providing multimodal content information (audio, text, image, labels, and interactions) show that SiBraR significantly outperforms CF as well as state-of-the-art content-based RSs in cold-start scenarios, and is competitive in warm scenarios. We show that SiBraR's recommendations are accurate in missing modality scenarios, and that the model is able to map different modalities to the same region of the shared embedding space, hence reducing the modality gap.         ",
    "url": "https://arxiv.org/abs/2409.17864",
    "authors": [
      "Christian Ganh\u00f6r",
      "Marta Moscati",
      "Anna Hausberger",
      "Shah Nawaz",
      "Markus Schedl"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2409.17872",
    "title": "A method for identifying causality in the response of nonlinear dynamical systems",
    "abstract": "           Predicting the response of nonlinear dynamical systems subject to random, broadband excitation is important across a range of scientific disciplines, such as structural dynamics and neuroscience. Building data-driven models requires experimental measurements of the system input and output, but it can be difficult to determine whether inaccuracies in the model stem from modelling errors or noise. This paper presents a novel method to identify the causal component of the input-output data from measurements of a system in the presence of output noise, as a function of frequency, without needing a high fidelity model. An output prediction, calculated using an available model, is optimally combined with noisy measurements of the output to predict the input to the system. The parameters of the algorithm balance the two output signals and are utilised to calculate a nonlinear coherence metric as a measure of causality. This method is applicable to a broad class of nonlinear dynamical systems. There are currently no solutions to this problem in the absence of a complete benchmark model.         ",
    "url": "https://arxiv.org/abs/2409.17872",
    "authors": [
      "Joseph Massingham",
      "Ole Nielsen",
      "Tore Butlin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17885",
    "title": "Sentiment Analysis of ML Projects: Bridging Emotional Intelligence and Code Quality",
    "abstract": "           This study explores the intricate relationship between sentiment analysis (SA) and code quality within machine learning (ML) projects, illustrating how the emotional dynamics of developers affect the technical and functional attributes of software projects. Recognizing the vital role of developer sentiments, this research employs advanced sentiment analysis techniques to scrutinize affective states from textual interactions such as code comments, commit messages, and issue discussions within high-profile ML projects. By integrating a comprehensive dataset of popular ML repositories, this analysis applies a blend of rule-based, machine learning, and hybrid sentiment analysis methodologies to systematically quantify sentiment scores. The emotional valence expressed by developers is then correlated with a spectrum of code quality indicators, including the prevalence of bugs, vulnerabilities, security hotspots, code smells, and duplication instances. Findings from this study distinctly illustrate that positive sentiments among developers are strongly associated with superior code quality metrics manifested through reduced bugs and lower incidence of code smells. This relationship underscores the importance of fostering positive emotional environments to enhance productivity and code craftsmanship. Conversely, the analysis reveals that negative sentiments correlate with an uptick in code issues, particularly increased duplication and heightened security risks, pointing to the detrimental effects of adverse emotional conditions on project health.         ",
    "url": "https://arxiv.org/abs/2409.17885",
    "authors": [
      "Md Shoaib Ahmed",
      "Dongyoung Park",
      "Nasir U. Eisty"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2409.17886",
    "title": "Upper-Body Pose-based Gaze Estimation for Privacy-Preserving 3D Gaze Target Detection",
    "abstract": "           Gaze Target Detection (GTD), i.e., determining where a person is looking within a scene from an external viewpoint, is a challenging task, particularly in 3D space. Existing approaches heavily rely on analyzing the person's appearance, primarily focusing on their face to predict the gaze target. This paper presents a novel approach to tackle this problem by utilizing the person's upper-body pose and available depth maps to extract a 3D gaze direction and employing a multi-stage or an end-to-end pipeline to predict the gazed target. When predicted accurately, the human body pose can provide valuable information about the head pose, which is a good approximation of the gaze direction, as well as the position of the arms and hands, which are linked to the activity the person is performing and the objects they are likely focusing on. Consequently, in addition to performing gaze estimation in 3D, we are also able to perform GTD simultaneously. We demonstrate state-of-the-art results on the most comprehensive publicly accessible 3D gaze target detection dataset without requiring images of the person's face, thus promoting privacy preservation in various application contexts. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17886",
    "authors": [
      "Andrea Toaiari",
      "Vittorio Murino",
      "Marco Cristani",
      "Cigdem Beyan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17895",
    "title": "Self-supervised Monocular Depth Estimation with Large Kernel Attention",
    "abstract": "           Self-supervised monocular depth estimation has emerged as a promising approach since it does not rely on labeled training data. Most methods combine convolution and Transformer to model long-distance dependencies to estimate depth accurately. However, Transformer treats 2D image features as 1D sequences, and positional encoding somewhat mitigates the loss of spatial information between different feature blocks, tending to overlook channel features, which limit the performance of depth estimation. In this paper, we propose a self-supervised monocular depth estimation network to get finer details. Specifically, we propose a decoder based on large kernel attention, which can model long-distance dependencies without compromising the two-dimension structure of features while maintaining feature channel adaptivity. In addition, we introduce a up-sampling module to accurately recover the fine details in the depth map. Our method achieves competitive results on the KITTI dataset.         ",
    "url": "https://arxiv.org/abs/2409.17895",
    "authors": [
      "Xuezhi Xiang",
      "Yao Wang",
      "Lei Zhang",
      "Denis Ombati",
      "Himaloy Himu",
      "Xiantong Zhen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17906",
    "title": "Graph Reasoning with Large Language Models via Pseudo-code Prompting",
    "abstract": "           Large language models (LLMs) have recently achieved remarkable success in various reasoning tasks in the field of natural language processing. This success of LLMs has also motivated their use in graph-related tasks. Among others, recent work has explored whether LLMs can solve graph problems such as counting the number of connected components of a graph or computing the shortest path distance between two nodes. Although LLMs possess preliminary graph reasoning abilities, they might still struggle to solve some seemingly simple problems. In this paper, we investigate whether prompting via pseudo-code instructions can improve the performance of LLMs in solving graph problems. Our experiments demonstrate that using pseudo-code instructions generally improves the performance of all considered LLMs. The graphs, pseudo-code prompts, and evaluation code are publicly available.         ",
    "url": "https://arxiv.org/abs/2409.17906",
    "authors": [
      "Konstantinos Skianis",
      "Giannis Nikolentzos",
      "Michalis Vazirgiannis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17924",
    "title": "Neural Light Spheres for Implicit Image Stitching and View Synthesis",
    "abstract": "           Challenging to capture, and challenging to display on a cellphone screen, the panorama paradoxically remains both a staple and underused feature of modern mobile camera applications. In this work we address both of these challenges with a spherical neural light field model for implicit panoramic image stitching and re-rendering; able to accommodate for depth parallax, view-dependent lighting, and local scene motion and color changes during capture. Fit during test-time to an arbitrary path panoramic video capture -- vertical, horizontal, random-walk -- these neural light spheres jointly estimate the camera path and a high-resolution scene reconstruction to produce novel wide field-of-view projections of the environment. Our single-layer model avoids expensive volumetric sampling, and decomposes the scene into compact view-dependent ray offset and color components, with a total model size of 80 MB per scene, and real-time (50 FPS) rendering at 1080p resolution. We demonstrate improved reconstruction quality over traditional image stitching and radiance field methods, with significantly higher tolerance to scene motion and non-ideal capture settings.         ",
    "url": "https://arxiv.org/abs/2409.17924",
    "authors": [
      "Ilya Chugunov",
      "Amogh Joshi",
      "Kiran Murthy",
      "Francois Bleibel",
      "Felix Heide"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17931",
    "title": "Intelligent Energy Management: Remaining Useful Life Prediction and Charging Automation System Comprised of Deep Learning and the Internet of Things",
    "abstract": "           Remaining Useful Life (RUL) of battery is an important parameter to know the battery's remaining life and need for recharge. The goal of this research project is to develop machine learning-based models for the battery RUL dataset. Different ML models are developed to classify the RUL of the vehicle, and the IoT (Internet of Things) concept is simulated for automating the charging system and managing any faults aligning. The graphs plotted depict the relationship between various vehicle parameters using the Blynk IoT platform. Results show that the catboost, Multi-Layer Perceptron (MLP), Gated Recurrent Unit (GRU), and hybrid model developed could classify RUL into three classes with 99% more accuracy. The data is fed using the tkinter GUI for simulating artificial intelligence (AI)-based charging, and with a pyserial backend, data can be entered into the Esp-32 microcontroller for making charge discharge possible with the model's predictions. Also, with an IoT system, the charging can be disconnected, monitored, and analyzed for automation. The results show that an accuracy of 99% can be obtained on models MLP, catboost model and similar accuracy on GRU model can be obtained, and finally relay-based triggering can be made by prediction through the model used for automating the charging and energy-saving mechanism. By showcasing an exemplary Blynk platform-based monitoring and automation phenomenon, we further present innovative ways of monitoring parameters and automating the system.         ",
    "url": "https://arxiv.org/abs/2409.17931",
    "authors": [
      "Biplov Paneru",
      "Bishwash Paneru",
      "DP Sharma Mainali"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.17941",
    "title": "Perturb, Attend, Detect and Localize (PADL): Robust Proactive Image Defense",
    "abstract": "           Image manipulation detection and localization have received considerable attention from the research community given the blooming of Generative Models (GMs). Detection methods that follow a passive approach may overfit to specific GMs, limiting their application in real-world scenarios, due to the growing diversity of generative models. Recently, approaches based on a proactive framework have shown the possibility of dealing with this limitation. However, these methods suffer from two main limitations, which raises concerns about potential vulnerabilities: i) the manipulation detector is not robust to noise and hence can be easily fooled; ii) the fact that they rely on fixed perturbations for image protection offers a predictable exploit for malicious attackers, enabling them to reverse-engineer and evade detection. To overcome this issue we propose PADL, a new solution able to generate image-specific perturbations using a symmetric scheme of encoding and decoding based on cross-attention, which drastically reduces the possibility of reverse engineering, even when evaluated with adaptive attack [31]. Additionally, PADL is able to pinpoint manipulated areas, facilitating the identification of specific regions that have undergone alterations, and has more generalization power than prior art on held-out generative models. Indeed, although being trained only on an attribute manipulation GAN model [15], our method generalizes to a range of unseen models with diverse architectural designs, such as StarGANv2, BlendGAN, DiffAE, StableDiffusion and StableDiffusionXL. Additionally, we introduce a novel evaluation protocol, which offers a fair evaluation of localisation performance in function of detection accuracy and better captures real-world scenarios.         ",
    "url": "https://arxiv.org/abs/2409.17941",
    "authors": [
      "Filippo Bartolucci",
      "Iacopo Masi",
      "Giuseppe Lisanti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17945",
    "title": "Modular Autonomous Vehicle in Heterogeneous Traffic Flow: Modeling, Simulation, and Implication",
    "abstract": "           Modular autonomous vehicles (MAVs) represent a groundbreaking concept that integrates modularity into the ongoing development of autonomous vehicles. This innovative design introduces unique features to traffic flow, allowing multiple modules to seamlessly join together and operate collectively. To understand the traffic flow characteristics involving these vehicles and their collective operations, this study established a modeling framework specifically designed to simulate their behavior within traffic flow. The mixed traffic flow, incorporating arbitrarily formed trains of various modular sizes, is modeled and studied. Simulations are conducted under varying levels of traffic demand and penetration rates to examine the traffic flow dynamics in the presence of these vehicles and their operations. The microscopic trajectories, MAV train compositions, and macroscopic fundamental diagrams of the mixed traffic flow are analyzed. The simulation findings indicate that integrating MAVs and their collective operations can substantially enhance capacity, with the extent of improvement depending on the penetration rate in mixed traffic flow. Notably, the capacity nearly doubles when the penetration rate exceeds 75%. Furthermore, their presence significantly influences and regulates the free-flow speed of the mixed traffic. Particularly, when variations in operational speed limits exist between the MAVs and the background traffic, the mixed traffic adjusts to the operating velocity of these vehicles. This study provides insights into potential future traffic flow systems incorporating emerging MAV technologies.         ",
    "url": "https://arxiv.org/abs/2409.17945",
    "authors": [
      "Lanhang Ye",
      "Toshiyuki Yamamoto"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2409.17946",
    "title": "Weak-To-Strong Backdoor Attacks for LLMs with Contrastive Knowledge Distillation",
    "abstract": "           Despite being widely applied due to their exceptional capabilities, Large Language Models (LLMs) have been proven to be vulnerable to backdoor attacks. These attacks introduce targeted vulnerabilities into LLMs by poisoning training samples and full-parameter fine-tuning. However, this kind of backdoor attack is limited since they require significant computational resources, especially as the size of LLMs increases. Besides, parameter-efficient fine-tuning (PEFT) offers an alternative but the restricted parameter updating may impede the alignment of triggers with target labels. In this study, we first verify that backdoor attacks with PEFT may encounter challenges in achieving feasible performance. To address these issues and improve the effectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack algorithm from weak to strong based on contrastive knowledge distillation (W2SAttack). Specifically, we poison small-scale language models through full-parameter fine-tuning to serve as the teacher model. The teacher model then covertly transfers the backdoor to the large-scale student model through contrastive knowledge distillation, which employs PEFT. Theoretical analysis reveals that W2SAttack has the potential to augment the effectiveness of backdoor attacks. We demonstrate the superior performance of W2SAttack on classification tasks across four language models, four backdoor attack algorithms, and two different architectures of teacher models. Experimental results indicate success rates close to 100% for backdoor attacks targeting PEFT.         ",
    "url": "https://arxiv.org/abs/2409.17946",
    "authors": [
      "Shuai Zhao",
      "Leilei Gan",
      "Zhongliang Guo",
      "Xiaobao Wu",
      "Luwei Xiao",
      "Xiaoyu Xu",
      "Cong-Duy Nguyen",
      "Luu Anh Tuan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.17951",
    "title": "Spatial Hierarchy and Temporal Attention Guided Cross Masking for Self-supervised Skeleton-based Action Recognition",
    "abstract": "           In self-supervised skeleton-based action recognition, the mask reconstruction paradigm is gaining interest in enhancing model refinement and robustness through effective masking. However, previous works primarily relied on a single masking criterion, resulting in the model overfitting specific features and overlooking other effective information. In this paper, we introduce a hierarchy and attention guided cross-masking framework (HA-CM) that applies masking to skeleton sequences from both spatial and temporal perspectives. Specifically, in spatial graphs, we utilize hyperbolic space to maintain joint distinctions and effectively preserve the hierarchical structure of high-dimensional skeletons, employing joint hierarchy as the masking criterion. In temporal flows, we substitute traditional distance metrics with the global attention of joints for masking, addressing the convergence of distances in high-dimensional space and the lack of a global perspective. Additionally, we incorporate cross-contrast loss based on the cross-masking framework into the loss function to enhance the model's learning of instance-level features. HA-CM shows efficiency and universality on three public large-scale datasets, NTU-60, NTU-120, and PKU-MMD. The source code of our HA-CM is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17951",
    "authors": [
      "Xinpeng Yin",
      "Wenming Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17963",
    "title": "CNCA: Toward Customizable and Natural Generation of Adversarial Camouflage for Vehicle Detectors",
    "abstract": "           Prior works on physical adversarial camouflage against vehicle detectors mainly focus on the effectiveness and robustness of the attack. The current most successful methods optimize 3D vehicle texture at a pixel level. However, this results in conspicuous and attention-grabbing patterns in the generated camouflage, which humans can easily identify. To address this issue, we propose a Customizable and Natural Camouflage Attack (CNCA) method by leveraging an off-the-shelf pre-trained diffusion model. By sampling the optimal texture image from the diffusion model with a user-specific text prompt, our method can generate natural and customizable adversarial camouflage while maintaining high attack performance. With extensive experiments on the digital and physical worlds and user studies, the results demonstrate that our proposed method can generate significantly more natural-looking camouflage than the state-of-the-art baselines while achieving competitive attack performance. Our code is available at \\href{this https URL}{this https URL}         ",
    "url": "https://arxiv.org/abs/2409.17963",
    "authors": [
      "Linye Lyu",
      "Jiawei Zhou",
      "Daojing He",
      "Yu Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17977",
    "title": "Cross-Modality Attack Boosted by Gradient-Evolutionary Multiform Optimization",
    "abstract": "           In recent years, despite significant advancements in adversarial attack research, the security challenges in cross-modal scenarios, such as the transferability of adversarial attacks between infrared, thermal, and RGB images, have been overlooked. These heterogeneous image modalities collected by different hardware devices are widely prevalent in practical applications, and the substantial differences between modalities pose significant challenges to attack transferability. In this work, we explore a novel cross-modal adversarial attack strategy, termed multiform attack. We propose a dual-layer optimization framework based on gradient-evolution, facilitating efficient perturbation transfer between modalities. In the first layer of optimization, the framework utilizes image gradients to learn universal perturbations within each modality and employs evolutionary algorithms to search for shared perturbations with transferability across different modalities through secondary optimization. Through extensive testing on multiple heterogeneous datasets, we demonstrate the superiority and robustness of Multiform Attack compared to existing techniques. This work not only enhances the transferability of cross-modal adversarial attacks but also provides a new perspective for understanding security vulnerabilities in cross-modal systems.         ",
    "url": "https://arxiv.org/abs/2409.17977",
    "authors": [
      "Yunpeng Gong",
      "Qingyuan Zeng",
      "Dejun Xu",
      "Zhenzhong Wang",
      "Min Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17986",
    "title": "Supra-Laplacian Encoding for Transformer on Dynamic Graphs",
    "abstract": "           Fully connected Graph Transformers (GT) have rapidly become prominent in the static graph community as an alternative to Message-Passing models, which suffer from a lack of expressivity, oversquashing, and under-reaching. However, in a dynamic context, by interconnecting all nodes at multiple snapshots with self-attention, GT loose both structural and temporal information. In this work, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs (SLATE), a new spatio-temporal encoding to leverage the GT architecture while keeping spatio-temporal information. Specifically, we transform Discrete Time Dynamic Graphs into multi-layer graphs and take advantage of the spectral properties of their associated supra-Laplacian matrix. Our second contribution explicitly model nodes' pairwise relationships with a cross-attention mechanism, providing an accurate edge representation for dynamic link prediction. SLATE outperforms numerous state-of-the-art methods based on Message-Passing Graph Neural Networks combined with recurrent models (e.g LSTM), and Dynamic Graph Transformers, on 9 datasets. Code and instructions to reproduce our results will be open-sourced.         ",
    "url": "https://arxiv.org/abs/2409.17986",
    "authors": [
      "Yannis Karmim",
      "Marc Lafon",
      "Rapha\u00ebl Fournier S'niehotta",
      "Nicolas Thome"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17990",
    "title": "Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models",
    "abstract": "           This paper proposes temporally aligned Large Language Models (LLMs) as a tool for longitudinal analysis of social media data. We fine-tune Temporal Adapters for Llama 3 8B on full timelines from a panel of British Twitter users, and extract longitudinal aggregates of emotions and attitudes with established questionnaires. We validate our estimates against representative British survey data and find strong positive, significant correlations for several collective emotions. The obtained estimates are robust across multiple training seeds and prompt formulations, and in line with collective emotions extracted using a traditional classification model trained on labeled data. To the best of our knowledge, this is the first work to extend the analysis of affect in LLMs to a longitudinal setting through Temporal Adapters. Our work enables new approaches towards the longitudinal analysis of social media data.         ",
    "url": "https://arxiv.org/abs/2409.17990",
    "authors": [
      "Georg Ahnert",
      "Max Pellert",
      "David Garcia",
      "Markus Strohmaier"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.17993",
    "title": "InterNet: Unsupervised Cross-modal Homography Estimation Based on Interleaved Modality Transfer and Self-supervised Homography Prediction",
    "abstract": "           We propose a novel unsupervised cross-modal homography estimation framework, based on interleaved modality transfer and self-supervised homography prediction, named InterNet. InterNet integrates modality transfer and self-supervised homography estimation, introducing an innovative interleaved optimization framework to alternately promote both components. The modality transfer gradually narrows the modality gaps, facilitating the self-supervised homography estimation to fully leverage the synthetic intra-modal data. The self-supervised homography estimation progressively achieves reliable predictions, thereby providing robust cross-modal supervision for the modality transfer. To further boost the estimation accuracy, we also formulate a fine-grained homography feature loss to improve the connection between two components. Furthermore, we employ a simple yet effective distillation training technique to reduce model parameters and improve cross-domain generalization ability while maintaining comparable performance. Experiments reveal that InterNet achieves the state-of-the-art (SOTA) performance among unsupervised methods, and even outperforms many supervised methods such as MHN and LocalTrans.         ",
    "url": "https://arxiv.org/abs/2409.17993",
    "authors": [
      "Junchen Yu",
      "Si-Yuan Cao",
      "Runmin Zhang",
      "Chenghao Zhang",
      "Jianxin Hu",
      "Zhu Yu",
      "Hui-liang Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17994",
    "title": "CRoP: Context-wise Robust Static Human-Sensing Personalization",
    "abstract": "           The advancement in deep learning and internet-of-things have led to diverse human sensing applications. However, distinct patterns in human sensing, influenced by various factors or contexts, challenge generic neural network model's performance due to natural distribution shifts. To address this, personalization tailors models to individual users. Yet most personalization studies overlook intra-user heterogeneity across contexts in sensory data, limiting intra-user generalizability. This limitation is especially critical in clinical applications, where limited data availability hampers both generalizability and personalization. Notably, intra-user sensing attributes are expected to change due to external factors such as treatment progression, further complicating the this http URL work introduces CRoP, a novel static personalization approach using an off-the-shelf pre-trained model and pruning to optimize personalization and generalization. CRoP shows superior personalization effectiveness and intra-user robustness across four human-sensing datasets, including two from real-world health domains, highlighting its practical and social impact. Additionally, to support CRoP's generalization ability and design choices, we provide empirical justification through gradient inner product analysis, ablation studies, and comparisons against state-of-the-art baselines.         ",
    "url": "https://arxiv.org/abs/2409.17994",
    "authors": [
      "Sawinder Kaur",
      "Avery Gump",
      "Jingyu Xin",
      "Yi Xiao",
      "Harshit Sharma",
      "Nina R Benway",
      "Jonathan L Preston",
      "Asif Salekin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.18013",
    "title": "Spatiotemporal Learning on Cell-embedded Graphs",
    "abstract": "           Data-driven simulation of physical systems has recently kindled significant attention, where many neural models have been developed. In particular, mesh-based graph neural networks (GNNs) have demonstrated significant potential in predicting spatiotemporal dynamics across arbitrary geometric domains. However, the existing node-edge message passing mechanism in GNNs limits the model's representation learning ability. In this paper, we proposed a cell-embedded GNN model (aka CeGNN) to learn spatiotemporal dynamics with lifted performance. Specifically, we introduce a learnable cell attribution to the node-edge message passing process, which better captures the spatial dependency of regional features. Such a strategy essentially upgrades the local aggregation scheme from the first order (e.g., from edge to node) to a higher order (e.g., from volume to edge and then to node), which takes advantage of volumetric information in message passing. Meanwhile, a novel feature-enhanced block is designed to further improve the performance of CeGNN and relieve the over-smoothness problem, via treating the latent features as basis functions. The extensive experiments on various PDE systems and one real-world dataset demonstrate that CeGNN achieves superior performance compared with other baseline models, particularly reducing the prediction error with up to 1 orders of magnitude on several PDE systems.         ",
    "url": "https://arxiv.org/abs/2409.18013",
    "authors": [
      "Yuan Mi",
      "Hao Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.18016",
    "title": "Relating Superconducting Optoelectronic Networks to Classical Neurodynamics",
    "abstract": "           The circuits comprising superconducting optoelectronic synapses, dendrites, and neurons are described by numerically cumbersome and formally opaque coupled differential equations. Reference 1 showed that a phenomenological model of superconducting loop neurons eliminates the need to solve the Josephson circuit equations that describe synapses and dendrites. The initial goal of the model was to decrease the time required for simulations, yet an additional benefit of the model was increased transparency of the underlying neural circuit operations and conceptual clarity regarding the connection of loop neurons to other physical systems. Whereas the original model simplified the treatment of the Josephson-junction dynamics, essentially by only considering low-pass versions of the dendritic outputs, the model resorted to an awkward treatment of spikes generated by semiconductor transmitter circuits that required explicitly checking for threshold crossings and distinct treatment of time steps wherein somatic threshold is reached. Here we extend that model to simplify the treatment of spikes coming from somas, again making use of the fact that in neural systems the downstream recipients of spike events almost always perform low-pass filtering. We provide comparisons between the first and second phenomenological models, quantifying the accuracy of the additional approximations. We identify regions of circuit parameter space in which the extended model works well and regions where it works poorly. For some circuit parameters it is possible to represent the downstream dendritic response to a single spike as well as coincidences or sequences of spikes, indicating the model is not simply a reduction to rate coding. The governing equations are shown to be nearly identical to those ubiquitous in the neuroscience literature for modeling leaky-integrator dendrites and neurons.         ",
    "url": "https://arxiv.org/abs/2409.18016",
    "authors": [
      "Jeffrey M. Shainline",
      "Bryce A. Primavera",
      "Ryan O'Loughlin"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2409.18023",
    "title": "DARE: Diverse Visual Question Answering with Robustness Evaluation",
    "abstract": "           Vision Language Models (VLMs) extend remarkable capabilities of text-only large language models and vision-only models, and are able to learn from and process multi-modal vision-text input. While modern VLMs perform well on a number of standard image classification and image-text matching tasks, they still struggle with a number of crucial vision-language (VL) reasoning abilities such as counting and spatial reasoning. Moreover, while they might be very brittle to small variations in instructions and/or evaluation protocols, existing benchmarks fail to evaluate their robustness (or rather the lack of it). In order to couple challenging VL scenarios with comprehensive robustness evaluation, we introduce DARE, Diverse Visual Question Answering with Robustness Evaluation, a carefully created and curated multiple-choice VQA benchmark. DARE evaluates VLM performance on five diverse categories and includes four robustness-oriented evaluations based on the variations of: prompts, the subsets of answer options, the output format and the number of correct answers. Among a spectrum of other findings, we report that state-of-the-art VLMs still struggle with questions in most categories and are unable to consistently deliver their peak performance across the tested robustness evaluations. The worst case performance across the subsets of options is up to 34% below the performance in the standard case. The robustness of the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match the closed-source models such as GPT-4 and Gemini, but even the latter remain very brittle to different variations.         ",
    "url": "https://arxiv.org/abs/2409.18023",
    "authors": [
      "Hannah Sterz",
      "Jonas Pfeiffer",
      "Ivan Vuli\u0107"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.18025",
    "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
    "abstract": "           Large language models are finetuned to refuse questions about hazardous knowledge, but these protections can often be bypassed. Unlearning methods aim at completely removing hazardous capabilities from models and make them inaccessible to adversaries. This work challenges the fundamental differences between unlearning and traditional safety post-training from an adversarial perspective. We demonstrate that existing jailbreak methods, previously reported as ineffective against unlearning, can be successful when applied carefully. Furthermore, we develop a variety of adaptive methods that recover most supposedly unlearned capabilities. For instance, we show that finetuning on 10 unrelated examples or removing specific directions in the activation space can recover most hazardous capabilities for models edited with RMU, a state-of-the-art unlearning method. Our findings challenge the robustness of current unlearning approaches and question their advantages over safety training.         ",
    "url": "https://arxiv.org/abs/2409.18025",
    "authors": [
      "Jakub \u0141ucki",
      "Boyi Wei",
      "Yangsibo Huang",
      "Peter Henderson",
      "Florian Tram\u00e8r",
      "Javier Rando"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.18026",
    "title": "ReliOcc: Towards Reliable Semantic Occupancy Prediction via Uncertainty Learning",
    "abstract": "           Vision-centric semantic occupancy prediction plays a crucial role in autonomous driving, which requires accurate and reliable predictions from low-cost sensors. Although having notably narrowed the accuracy gap with LiDAR, there is still few research effort to explore the reliability in predicting semantic occupancy from camera. In this paper, we conduct a comprehensive evaluation of existing semantic occupancy prediction models from a reliability perspective for the first time. Despite the gradual alignment of camera-based models with LiDAR in term of accuracy, a significant reliability gap persists. To addresses this concern, we propose ReliOcc, a method designed to enhance the reliability of camera-based occupancy networks. ReliOcc provides a plug-and-play scheme for existing models, which integrates hybrid uncertainty from individual voxels with sampling-based noise and relative voxels through mix-up learning. Besides, an uncertainty-aware calibration strategy is devised to further enhance model reliability in offline mode. Extensive experiments under various settings demonstrate that ReliOcc significantly enhances model reliability while maintaining the accuracy of both geometric and semantic predictions. Importantly, our proposed approach exhibits robustness to sensor failures and out of domain noises during inference.         ",
    "url": "https://arxiv.org/abs/2409.18026",
    "authors": [
      "Song Wang",
      "Zhongdao Wang",
      "Jiawei Yu",
      "Wentong Li",
      "Bailan Feng",
      "Junbo Chen",
      "Jianke Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.18028",
    "title": "Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective",
    "abstract": "           A common practice in large language model (LLM) usage for complex analytical tasks such as code generation, is to sample a solution for the entire task within the model's context window. Previous works have shown that subtask decomposition within the model's context (chain of thought), is beneficial for solving such tasks. In this work, we point a limitation of LLMs' ability to perform several sub-tasks within the same context window - an in-context hardness of composition, pointing to an advantage for distributing a decomposed problem in a multi-agent system of LLMs. The hardness of composition is quantified by a generation complexity metric, i.e., the number of LLM generations required to sample at least one correct solution. We find a gap between the generation complexity of solving a compositional problem within the same context relative to distributing it among multiple agents, that increases exponentially with the solution's length. We prove our results theoretically and demonstrate them empirically.         ",
    "url": "https://arxiv.org/abs/2409.18028",
    "authors": [
      "Yotam Wolf",
      "Binyamin Rothberg",
      "Dorin Shteyman",
      "Amnon Shashua"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.18033",
    "title": "Automated Detection and Analysis of Power Words in Persuasive Text Using Natural Language Processing",
    "abstract": "           Power words are terms that evoke strong emotional responses and significantly influence readers' behavior, playing a crucial role in fields like marketing, politics, and motivational writing. This study proposes a methodology for the automated detection and analysis of power words in persuasive text using a custom lexicon and the TextBlob library in Python. By identifying the presence and frequency of power words within a given text, we aim to classify and analyze their impact on sentiment and reader engagement. This research examines diverse datasets across various domains to provide insights into the effectiveness of power words, offering practical applications for content creators, advertisers, and policymakers.         ",
    "url": "https://arxiv.org/abs/2409.18033",
    "authors": [
      "Sahil Garje"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.18055",
    "title": "Visual Data Diagnosis and Debiasing with Concept Graphs",
    "abstract": "           The widespread success of deep learning models today is owed to the curation of extensive datasets significant in size and complexity. However, such models frequently pick up inherent biases in the data during the training process, leading to unreliable predictions. Diagnosing and debiasing datasets is thus a necessity to ensure reliable model performance. In this paper, we present CONBIAS, a novel framework for diagnosing and mitigating Concept co-occurrence Biases in visual datasets. CONBIAS represents visual datasets as knowledge graphs of concepts, enabling meticulous analysis of spurious concept co-occurrences to uncover concept imbalances across the whole dataset. Moreover, we show that by employing a novel clique-based concept balancing strategy, we can mitigate these imbalances, leading to enhanced performance on downstream tasks. Extensive experiments show that data augmentation based on a balanced concept distribution augmented by CONBIAS improves generalization performance across multiple datasets compared to state-of-the-art methods. We will make our code and data publicly available.         ",
    "url": "https://arxiv.org/abs/2409.18055",
    "authors": [
      "Rwiddhi Chakraborty",
      "Yinong Wang",
      "Jialu Gao",
      "Runkai Zheng",
      "Cheng Zhang",
      "Fernando De la Torre"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.18057",
    "title": "LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field",
    "abstract": "           Recent works have shown that neural radiance fields (NeRFs) on top of parametric models have reached SOTA quality to build photorealistic head avatars from a monocular video. However, one major limitation of the NeRF-based avatars is the slow rendering speed due to the dense point sampling of NeRF, preventing them from broader utility on resource-constrained devices. We introduce LightAvatar, the first head avatar model based on neural light fields (NeLFs). LightAvatar renders an image from 3DMM parameters and a camera pose via a single network forward pass, without using mesh or volume rendering. The proposed approach, while being conceptually appealing, poses a significant challenge towards real-time efficiency and training stability. To resolve them, we introduce dedicated network designs to obtain proper representations for the NeLF model and maintain a low FLOPs budget. Meanwhile, we tap into a distillation-based training strategy that uses a pretrained avatar model as teacher to synthesize abundant pseudo data for training. A warping field network is introduced to correct the fitting error in the real data so that the model can learn better. Extensive experiments suggest that our method can achieve new SOTA image quality quantitatively or qualitatively, while being significantly faster than the counterparts, reporting 174.1 FPS (512x512 resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.         ",
    "url": "https://arxiv.org/abs/2409.18057",
    "authors": [
      "Huan Wang",
      "Feitong Tan",
      "Ziqian Bai",
      "Yinda Zhang",
      "Shichen Liu",
      "Qiangeng Xu",
      "Menglei Chai",
      "Anish Prabhu",
      "Rohit Pandey",
      "Sean Fanello",
      "Zeng Huang",
      "Yun Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.18062",
    "title": "Efficient Approximation of Centrality Measures in Uncertain Graphs",
    "abstract": "           In this thesis I propose an algorithm to heuristically calculate different distance measures on uncertain graphs (i.e. graphs where edges only exist with a certain probability) and apply this to the heuristic calculation of harmonic closeness centrality. This approach is mainly based on previous work on the calculation of distance measures by Potamias et al. and on a heuristic algorithm for betweenness centrality by Chenxu Wang and Ziyuan Lin. I extend on their research by using the concept of possible shortest paths, applying them to the afformentioned distances. To the best of my knowledge, this algorithmic approach has never been studied before. I will compare my heuristic results for harmonic closeness against the Monte Carlo method both in runtime and accuracy. Similarly, I will conduct new experiments on the betweenness centrality heuristic proposed y Chenxu Wang and Ziyuan Lin to test its efficacy on a bigger variety of instances. Finally, I will test both of these algorithms on large scale graphs to evaluate the scalability of their runtime.         ",
    "url": "https://arxiv.org/abs/2409.18062",
    "authors": [
      "Daniel Ketels"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2409.18084",
    "title": "GSON: A Group-based Social Navigation Framework with Large Multimodal Model",
    "abstract": "           As the number of service robots and autonomous vehicles in human-centered environments grows, their requirements go beyond simply navigating to a destination. They must also take into account dynamic social contexts and ensure respect and comfort for others in shared spaces, which poses significant challenges for perception and planning. In this paper, we present a group-based social navigation framework GSON to enable mobile robots to perceive and exploit the social group of their surroundings by leveling the visual reasoning capability of the Large Multimodal Model (LMM). For perception, we apply visual prompting techniques to zero-shot extract the social relationship among pedestrians and combine the result with a robust pedestrian detection and tracking pipeline to alleviate the problem of low inference speed of the LMM. Given the perception result, the planning system is designed to avoid disrupting the current social structure. We adopt a social structure-based mid-level planner as a bridge between global path planning and local motion planning to preserve the global context and reactive response. The proposed method is validated on real-world mobile robot navigation tasks involving complex social structure understanding and reasoning. Experimental results demonstrate the effectiveness of the system in these scenarios compared with several baselines.         ",
    "url": "https://arxiv.org/abs/2409.18084",
    "authors": [
      "Shangyi Luo",
      "Ji Zhu",
      "Peng Sun",
      "Yuhong Deng",
      "Cunjun Yu",
      "Anxing Xiao",
      "Xueqian Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.18094",
    "title": "Mobility in Age-Based Gossip Networks",
    "abstract": "           We consider a gossiping network where a source forwards updates to a set of $n$ gossiping nodes that are placed in an arbitrary graph structure and gossip with their neighbors. In this paper, we analyze how mobility of nodes affects the freshness of nodes in the gossiping network. To model mobility, we let nodes randomly exchange positions with other nodes in the network. The position of the node determines how the node interacts with the rest of the network. In order to quantify information freshness, we use the version age of information metric. We use the stochastic hybrid system (SHS) framework to derive recursive equations to find the version age for a set of positions in the network in terms of the version ages of sets of positions that are one larger or of the same size. We use these recursive equations to find an upper bound for the average version age of a node in two example networks. We show that mobility can decrease the version age of nodes in a disconnected network from linear scaling in $n$ to at most square root scaling and even to constant scaling in some cases. We perform numerical simulations to analyze how mobility affects the version age of different positions in the network and also show that the upper bounds obtained for the example networks are tight.         ",
    "url": "https://arxiv.org/abs/2409.18094",
    "authors": [
      "Arunabh Srivastava",
      "Sennur Ulukus"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Social and Information Networks (cs.SI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2409.18100",
    "title": "Self-supervised Pretraining for Cardiovascular Magnetic Resonance Cine Segmentation",
    "abstract": "           Self-supervised pretraining (SSP) has shown promising results in learning from large unlabeled datasets and, thus, could be useful for automated cardiovascular magnetic resonance (CMR) short-axis cine segmentation. However, inconsistent reports of the benefits of SSP for segmentation have made it difficult to apply SSP to CMR. Therefore, this study aimed to evaluate SSP methods for CMR cine segmentation. To this end, short-axis cine stacks of 296 subjects (90618 2D slices) were used for unlabeled pretraining with four SSP methods; SimCLR, positional contrastive learning, DINO, and masked image modeling (MIM). Subsets of varying numbers of subjects were used for supervised fine-tuning of 2D models for each SSP method, as well as to train a 2D baseline model from scratch. The fine-tuned models were compared to the baseline using the 3D Dice similarity coefficient (DSC) in a test dataset of 140 subjects. The SSP methods showed no performance gains with the largest supervised fine-tuning subset compared to the baseline (DSC = 0.89). When only 10 subjects (231 2D slices) are available for supervised training, SSP using MIM (DSC = 0.86) improves over training from scratch (DSC = 0.82). This study found that SSP is valuable for CMR cine segmentation when labeled training data is scarce, but does not aid state-of-the-art deep learning methods when ample labeled data is available. Moreover, the choice of SSP method is important. The code is publicly available at: this https URL ",
    "url": "https://arxiv.org/abs/2409.18100",
    "authors": [
      "Rob A. J. de Mooij",
      "Josien P. W. Pluim",
      "Cian M. Scannell"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.18109",
    "title": "Canonical labelling of sparse random graphs",
    "abstract": "           We show that if $p=O(1/n)$, then the Erd\u0151s-R\u00e9nyi random graph $G(n,p)$ with high probability admits a canonical labeling computable in time $O(n\\log n)$. Combined with the previous results on the canonization of random graphs, this implies that $G(n,p)$ with high probability admits a polynomial-time canonical labeling whatever the edge probability function $p$. Our algorithm combines the standard color refinement routine with simple post-processing based on the classical linear-time tree canonization. Noteworthy, our analysis of how well color refinement performs in this setting allows us to complete the description of the automorphism group of the 2-core of $G(n,p)$.         ",
    "url": "https://arxiv.org/abs/2409.18109",
    "authors": [
      "Oleg Verbitsky",
      "Maksim Zhukovskii"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2409.18118",
    "title": "Slowly Scaling Per-Record Differential Privacy",
    "abstract": "           We develop formal privacy mechanisms for releasing statistics from data with many outlying values, such as income data. These mechanisms ensure that a per-record differential privacy guarantee degrades slowly in the protected records' influence on the statistics being released. Formal privacy mechanisms generally add randomness, or \"noise,\" to published statistics. If a noisy statistic's distribution changes little with the addition or deletion of a single record in the underlying dataset, an attacker looking at this statistic will find it plausible that any particular record was present or absent, preserving the records' privacy. More influential records -- those whose addition or deletion would change the statistics' distribution more -- typically suffer greater privacy loss. The per-record differential privacy framework quantifies these record-specific privacy guarantees, but existing mechanisms let these guarantees degrade rapidly (linearly or quadratically) with influence. While this may be acceptable in cases with some moderately influential records, it results in unacceptably high privacy losses when records' influence varies widely, as is common in economic data. We develop mechanisms with privacy guarantees that instead degrade as slowly as logarithmically with influence. These mechanisms allow for the accurate, unbiased release of statistics, while providing meaningful protection for highly influential records. As an example, we consider the private release of sums of unbounded establishment data such as payroll, where our mechanisms extend meaningful privacy protection even to very large establishments. We evaluate these mechanisms empirically and demonstrate their utility.         ",
    "url": "https://arxiv.org/abs/2409.18118",
    "authors": [
      "Brian Finley",
      "Anthony M Caruso",
      "Justin C Doty",
      "Ashwin Machanavajjhala",
      "Mikaela R Meyer",
      "David Pujol",
      "William Sexton",
      "Zachary Terner"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2409.18124",
    "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
    "abstract": "           Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also significantly enhances efficiency, being hundreds of times faster than most existing diffusion-based methods.         ",
    "url": "https://arxiv.org/abs/2409.18124",
    "authors": [
      "Jing He",
      "Haodong Li",
      "Wei Yin",
      "Yixun Liang",
      "Leheng Li",
      "Kaiqiang Zhou",
      "Hongbo Liu",
      "Bingbing Liu",
      "Ying-Cong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17178",
    "title": "MODEL&CO: Exoplanet detection in angular differential imaging by learning across multiple observations",
    "abstract": "           Direct imaging of exoplanets is particularly challenging due to the high contrast between the planet and the star luminosities, and their small angular separation. In addition to tailored instrumental facilities implementing adaptive optics and coronagraphy, post-processing methods combining several images recorded in pupil tracking mode are needed to attenuate the nuisances corrupting the signals of interest. Most of these post-processing methods build a model of the nuisances from the target observations themselves, resulting in strongly limited detection sensitivity at short angular separations due to the lack of angular diversity. To address this issue, we propose to build the nuisance model from an archive of multiple observations by leveraging supervised deep learning techniques. The proposed approach casts the detection problem as a reconstruction task and captures the structure of the nuisance from two complementary representations of the data. Unlike methods inspired by reference differential imaging, the proposed model is highly non-linear and does not resort to explicit image-to-image similarity measurements and subtractions. The proposed approach also encompasses statistical modeling of learnable spatial features. The latter is beneficial to improve both the detection sensitivity and the robustness against heterogeneous data. We apply the proposed algorithm to several datasets from the VLT/SPHERE instrument, and demonstrate a superior precision-recall trade-off compared to the PACO algorithm. Interestingly, the gain is especially important when the diversity induced by ADI is the most limited, thus supporting the ability of the proposed approach to learn information across multiple observations.         ",
    "url": "https://arxiv.org/abs/2409.17178",
    "authors": [
      "Th\u00e9o Bodrito",
      "Olivier Flasseur",
      "Julien Mairal",
      "Jean Ponce",
      "Maud Langlois",
      "Anne-Marie Lagrange"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2409.17189",
    "title": "Decentralized Federated Learning with Gradient Tracking over Time-Varying Directed Networks",
    "abstract": "           We investigate the problem of agent-to-agent interaction in decentralized (federated) learning over time-varying directed graphs, and, in doing so, propose a consensus-based algorithm called DSGTm-TV. The proposed algorithm incorporates gradient tracking and heavy-ball momentum to distributively optimize a global objective function, while preserving local data privacy. Under DSGTm-TV, agents will update local model parameters and gradient estimates using information exchange with neighboring agents enabled through row- and column-stochastic mixing matrices, which we show guarantee both consensus and optimality. Our analysis establishes that DSGTm-TV exhibits linear convergence to the exact global optimum when exact gradient information is available, and converges in expectation to a neighborhood of the global optimum when employing stochastic gradients. Moreover, in contrast to existing methods, DSGTm-TV preserves convergence for networks with uncoordinated stepsizes and momentum parameters, for which we provide explicit bounds. These results enable agents to operate in a fully decentralized manner, independently optimizing their local hyper-parameters. We demonstrate the efficacy of our approach via comparisons with state-of-the-art baselines on real-world image classification and natural language processing tasks.         ",
    "url": "https://arxiv.org/abs/2409.17189",
    "authors": [
      "Duong Thuy Anh Nguyen",
      "Su Wang",
      "Duong Tung Nguyen",
      "Angelia Nedich",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17228",
    "title": "Disk2Planet: A Robust and Automated Machine Learning Tool for Parameter Inference in Disk-Planet Systems",
    "abstract": "           We introduce Disk2Planet, a machine learning-based tool to infer key parameters in disk-planet systems from observed protoplanetary disk structures. Disk2Planet takes as input the disk structures in the form of two-dimensional density and velocity maps, and outputs disk and planet properties, that is, the Shakura--Sunyaev viscosity, the disk aspect ratio, the planet--star mass ratio, and the planet's radius and azimuth. We integrate the Covariance Matrix Adaptation Evolution Strategy (CMA--ES), an evolutionary algorithm tailored for complex optimization problems, and the Protoplanetary Disk Operator Network (PPDONet), a neural network designed to predict solutions of disk--planet interactions. Our tool is fully automated and can retrieve parameters in one system in three minutes on an Nvidia A100 graphics processing unit. We empirically demonstrate that our tool achieves percent-level or higher accuracy, and is able to handle missing data and unknown levels of noise.         ",
    "url": "https://arxiv.org/abs/2409.17228",
    "authors": [
      "Shunyuan Mao",
      "Ruobing Dong",
      "Kwang Moo Yi",
      "Lu Lu",
      "Sifan Wang",
      "Paris Perdikaris"
    ],
    "subjectives": [
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17276",
    "title": "Multiview Canonical Correlation Analysis for Automatic Pathological Speech Detection",
    "abstract": "           Recently proposed automatic pathological speech detection approaches rely on spectrogram input representations or wav2vec2 embeddings. These representations may contain pathology irrelevant uncorrelated information, such as changing phonetic content or variations in speaking style across time, which can adversely affect classification performance. To address this issue, we propose to use Multiview Canonical Correlation Analysis (MCCA) on these input representations prior to automatic pathological speech detection. Our results demonstrate that unlike other dimensionality reduction techniques, the use of MCCA leads to a considerable improvement in pathological speech detection performance by eliminating uncorrelated information present in the input representations. Employing MCCA with traditional classifiers yields a comparable or higher performance than using sophisticated architectures, while preserving the representation structure and providing interpretability.         ",
    "url": "https://arxiv.org/abs/2409.17276",
    "authors": [
      "Yacouba Kaloga",
      "Shakeel A. Sheikh",
      "Ina Kodrasi"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2409.17298",
    "title": "Sparsity, Regularization and Causality in Agricultural Yield: The Case of Paddy Rice in Peru",
    "abstract": "           This study introduces a novel approach that integrates agricultural census data with remotely sensed time series to develop precise predictive models for paddy rice yield across various regions of Peru. By utilizing sparse regression and Elastic-Net regularization techniques, the study identifies causal relationships between key remotely sensed variables-such as NDVI, precipitation, and temperature-and agricultural yield. To further enhance prediction accuracy, the first- and second-order dynamic transformations (velocity and acceleration) of these variables are applied, capturing non-linear patterns and delayed effects on yield. The findings highlight the improved predictive performance when combining regularization techniques with climatic and geospatial variables, enabling more precise forecasts of yield variability. The results confirm the existence of causal relationships in the Granger sense, emphasizing the value of this methodology for strategic agricultural management. This contributes to more efficient and sustainable production in paddy rice cultivation.         ",
    "url": "https://arxiv.org/abs/2409.17298",
    "authors": [
      "Rita Rocio Guzman-Lopez",
      "Luis Huamanchumo",
      "Kevin Fernandez",
      "Oscar Cutipa-Luque",
      "Yhon Tiahuallpa",
      "Helder Rojas"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2409.17306",
    "title": "Bounds on the Complete Forcing Number of Graphs",
    "abstract": "           A forcing set for a perfect matching of a graph is defined as a subset of the edges of that perfect matching such that there exists a unique perfect matching containing it. A complete forcing set for a graph is a subset of its edges, such that it intersects the edges of every perfect matching in a forcing set of that perfect matching. The size of a smallest complete forcing set of a graph is called the complete forcing number of the graph. In this paper, we derive new upper bounds for the complete forcing number of graphs in terms of other graph theoretical parameters such as the degeneracy or the spectral radius of the graph. We show that for graphs with the number of edges more than some constant times the number of vertices, our result outperforms the best known upper bound for the complete forcing number. For the set of edge-transitive graphs, we present a lower bound for the complete forcing number in terms of maximum forcing number. This result in particular is applied to the hypercube graphs and Cartesian powers of even cycles.         ",
    "url": "https://arxiv.org/abs/2409.17306",
    "authors": [
      "Javad B. Ebrahimi",
      "Aref Nemayande",
      "Elahe Tohidi"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2409.17364",
    "title": "Exploring synthetic data for cross-speaker style transfer in style representation based TTS",
    "abstract": "           Incorporating cross-speaker style transfer in text-to-speech (TTS) models is challenging due to the need to disentangle speaker and style information in audio. In low-resource expressive data scenarios, voice conversion (VC) can generate expressive speech for target speakers, which can then be used to train the TTS model. However, the quality and style transfer ability of the VC model are crucial for the overall TTS model quality. In this work, we explore the use of synthetic data generated by a VC model to assist the TTS model in cross-speaker style transfer tasks. Additionally, we employ pre-training of the style encoder using timbre perturbation and prototypical angular loss to mitigate speaker leakage. Our results show that using VC synthetic data can improve the naturalness and speaker similarity of TTS in cross-speaker scenarios. Furthermore, we extend this approach to a cross-language scenario, enhancing accent transfer.         ",
    "url": "https://arxiv.org/abs/2409.17364",
    "authors": [
      "Lucas H. Ueda",
      "Leonardo B. de M. M. Marques",
      "Fl\u00e1vio O. Sim\u00f5es",
      "M\u00e1rio U. Neto",
      "Fernando Runstein",
      "Bianca Dal B\u00f3",
      "Paula D. P. Costa"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2409.17421",
    "title": "Solar Active Regions Emergence Prediction Using Long Short-Term Memory Networks",
    "abstract": "           We developed Long Short-Term Memory (LSTM) models to predict the formation of active regions (ARs) on the solar surface. Using the Doppler shift velocity, the continuum intensity, and the magnetic field observations from the Solar Dynamics Observatory (SDO) Helioseismic and Magnetic Imager (HMI), we have created time-series datasets of acoustic power and magnetic flux, which are used to train LSTM models on predicting continuum intensity, 12 hours in advance. These novel machine learning (ML) models are able to capture variations of the acoustic power density associated with upcoming magnetic flux emergence and continuum intensity decrease. Testing of the models' performance was done on data for 5 ARs, unseen from the models during training. Model 8, the best performing model trained, was able to make a successful prediction of emergence for all testing active regions in an experimental setting and three of them in an operational. The model predicted the emergence of AR11726, AR13165, and AR13179 respectively 10, 29, and 5 hours in advance, and variations of this model achieved average RMSE values of 0.11 for both active and quiet areas on the solar disc. This work sets the foundations for ML-aided prediction of solar ARs.         ",
    "url": "https://arxiv.org/abs/2409.17421",
    "authors": [
      "Spiridon Kasapis",
      "Irina N. Kitiashvili",
      "Alexander G. Kosovichev",
      "John T. Stefan"
    ],
    "subjectives": [
      "Solar and Stellar Astrophysics (astro-ph.SR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17488",
    "title": "Optimal control of stochastic reaction networks with entropic control cost and emergence of mode-switching strategies",
    "abstract": "           Controlling the stochastic dynamics of biological populations is a challenge that arises across various biological contexts. However, these dynamics are inherently nonlinear and involve a discrete state space, i.e., the number of molecules, cells, or organisms. Additionally, the possibility of extinction has a significant impact on both the dynamics and control strategies, particularly when the population size is small. These factors hamper the direct application of conventional control theories to biological systems. To address these challenges, we formulate the optimal control problem for stochastic population dynamics by utilizing a control cost function based on the Kullback-Leibler divergence. This approach naturally accounts for population-specific factors and simplifies the complex nonlinear Hamilton-Jacobi-Bellman equation into a linear form, facilitating efficient computation of optimal solutions. We demonstrate the effectiveness of our approach by applying it to the control of interacting random walkers, Moran processes, and SIR models, and observe the mode-switching phenomena in the control strategies. Our approach provides new opportunities for applying control theory to a wide range of biological problems.         ",
    "url": "https://arxiv.org/abs/2409.17488",
    "authors": [
      "Shuhei A. Horiguchi",
      "Tetsuya J. Kobayashi"
    ],
    "subjectives": [
      "Populations and Evolution (q-bio.PE)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)",
      "Biological Physics (physics.bio-ph)",
      "Molecular Networks (q-bio.MN)"
    ]
  },
  {
    "id": "arXiv:2409.17503",
    "title": "Shape-intensity knowledge distillation for robust medical image segmentation",
    "abstract": "           Many medical image segmentation methods have achieved impressive results. Yet, most existing methods do not take into account the shape-intensity prior information. This may lead to implausible segmentation results, in particular for images of unseen datasets. In this paper, we propose a novel approach to incorporate joint shape-intensity prior information into the segmentation network. Specifically, we first train a segmentation network (regarded as the teacher network) on class-wise averaged training images to extract valuable shape-intensity information, which is then transferred to a student segmentation network with the same network architecture as the teacher via knowledge distillation. In this way, the student network regarded as the final segmentation model can effectively integrate the shape-intensity prior information, yielding more accurate segmentation results. Despite its simplicity, experiments on five medical image segmentation tasks of different modalities demonstrate that the proposed Shape-Intensity Knowledge Distillation (SIKD) consistently improves several baseline models (including recent MaxStyle and SAMed) under intra-dataset evaluation, and significantly improves the cross-dataset generalization ability. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17503",
    "authors": [
      "Wenhui Dong",
      "Bo Du",
      "Yongchao Xu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.17510",
    "title": "NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human Connectomes",
    "abstract": "           Although modern imaging technologies allow us to study connectivity between two distinct brain regions in-vivo, an in-depth understanding of how anatomical structure supports brain function and how spontaneous functional fluctuations emerge remarkable cognition is still elusive. Meanwhile, tremendous efforts have been made in the realm of machine learning to establish the nonlinear mapping between neuroimaging data and phenotypic traits. However, the absence of neuroscience insight in the current approaches poses significant challenges in understanding cognitive behavior from transient neural activities. To address this challenge, we put the spotlight on the coupling mechanism of structural connectivity (SC) and functional connectivity (FC) by formulating such network neuroscience question into an expressive graph representation learning problem for high-order topology. Specifically, we introduce the concept of topological detour to characterize how a ubiquitous instance of FC (direct link) is supported by neural pathways (detour) physically wired by SC, which forms a cyclic loop interacted by brain structure and function. In the clich\u00e9 of machine learning, the multi-hop detour pathway underlying SC-FC coupling allows us to devise a novel multi-head self-attention mechanism within Transformer to capture multi-modal feature representation from paired graphs of SC and FC. Taken together, we propose a biological-inspired deep model, coined as NeuroPath, to find putative connectomic feature representations from the unprecedented amount of neuroimages, which can be plugged into various downstream applications such as task recognition and disease diagnosis. We have evaluated NeuroPath on large-scale public datasets including HCP and UK Biobank under supervised and zero-shot learning, where the state-of-the-art performance by our NeuroPath indicates great potential in network neuroscience.         ",
    "url": "https://arxiv.org/abs/2409.17510",
    "authors": [
      "Ziquan Wei",
      "Tingting Dan",
      "Jiaqi Ding",
      "Paul J Laurienti",
      "Guorong Wu"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17544",
    "title": "Optimizing the Induced Correlation in Omnibus Joint Graph Embeddings",
    "abstract": "           Theoretical and empirical evidence suggests that joint graph embedding algorithms induce correlation across the networks in the embedding space. In the Omnibus joint graph embedding framework, previous results explicitly delineated the dual effects of the algorithm-induced and model-inherent correlations on the correlation across the embedded networks. Accounting for and mitigating the algorithm-induced correlation is key to subsequent inference, as sub-optimal Omnibus matrix constructions have been demonstrated to lead to loss in inference fidelity. This work presents the first efforts to automate the Omnibus construction in order to address two key questions in this joint embedding framework: the correlation-to-OMNI problem and the flat correlation problem. In the flat correlation problem, we seek to understand the minimum algorithm-induced flat correlation (i.e., the same across all graph pairs) produced by a generalized Omnibus embedding. Working in a subspace of the fully general Omnibus matrices, we prove both a lower bound for this flat correlation and that the classical Omnibus construction induces the maximal flat correlation. In the correlation-to-OMNI problem, we present an algorithm -- named corr2Omni -- that, from a given matrix of estimated pairwise graph correlations, estimates the matrix of generalized Omnibus weights that induces optimal correlation in the embedding space. Moreover, in both simulated and real data settings, we demonstrate the increased effectiveness of our corr2Omni algorithm versus the classical Omnibus construction.         ",
    "url": "https://arxiv.org/abs/2409.17544",
    "authors": [
      "Konstantinos Pantazis",
      "Michael Trosset",
      "William N. Frost",
      "Carey E. Priebe",
      "Vince Lyzinski"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2409.17583",
    "title": "Let the Quantum Creep In: Designing Quantum Neural Network Models by Gradually Swapping Out Classical Components",
    "abstract": "           Artificial Intelligence (AI), with its multiplier effect and wide applications in multiple areas, could potentially be an important application of quantum computing. Since modern AI systems are often built on neural networks, the design of quantum neural networks becomes a key challenge in integrating quantum computing into AI. To provide a more fine-grained characterisation of the impact of quantum components on the performance of neural networks, we propose a framework where classical neural network layers are gradually replaced by quantum layers that have the same type of input and output while keeping the flow of information between layers unchanged, different from most current research in quantum neural network, which favours an end-to-end quantum model. We start with a simple three-layer classical neural network without any normalisation layers or activation functions, and gradually change the classical layers to the corresponding quantum versions. We conduct numerical experiments on image classification datasets such as the MNIST, FashionMNIST and CIFAR-10 datasets to demonstrate the change of performance brought by the systematic introduction of quantum components. Through this framework, our research sheds new light on the design of future quantum neural network models where it could be more favourable to search for methods and frameworks that harness the advantages from both the classical and quantum worlds.         ",
    "url": "https://arxiv.org/abs/2409.17583",
    "authors": [
      "Peiyong Wang",
      "Casey. R. Myers",
      "Lloyd C. L. Hollenberg",
      "Udaya Parampalli"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17591",
    "title": "Conjugate Bayesian Two-step Change Point Detection for Hawkes Process",
    "abstract": "           The Bayesian two-step change point detection method is popular for the Hawkes process due to its simplicity and intuitiveness. However, the non-conjugacy between the point process likelihood and the prior requires most existing Bayesian two-step change point detection methods to rely on non-conjugate inference methods. These methods lack analytical expressions, leading to low computational efficiency and impeding timely change point detection. To address this issue, this work employs data augmentation to propose a conjugate Bayesian two-step change point detection method for the Hawkes process, which proves to be more accurate and efficient. Extensive experiments on both synthetic and real data demonstrate the superior effectiveness and efficiency of our method compared to baseline methods. Additionally, we conduct ablation studies to explore the robustness of our method concerning various hyperparameters. Our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17591",
    "authors": [
      "Zeyue Zhang",
      "Xiaoling Lu",
      "Feng Zhou"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17658",
    "title": "Powers of large matrices on GPU platforms to compute the Roman domination number of cylindrical graphs",
    "abstract": "           The Roman domination in a graph $G$ is a variant of the classical domination, defined by means of a so-called Roman domination function $f\\colon V(G)\\to \\{0,1,2\\}$ such that if $f(v)=0$ then, the vertex $v$ is adjacent to at least one vertex $w$ with $f(w)=2$. The weight $f(G)$ of a Roman dominating function of $G$ is the sum of the weights of all vertices of $G$, that is, $f(G)=\\sum_{u\\in V(G)}f(u)$. The Roman domination number $\\gamma_R(G)$ is the minimum weight of a Roman dominating function of $G$. In this paper we propose algorithms to compute this parameter involving the $(\\min,+)$ powers of large matrices with high computational requirements and the GPU (Graphics Processing Unit) allows us to accelerate such operations. Specific routines have been developed to efficiently compute the $(\\min ,+)$ product on GPU architecture, taking advantage of its computational power. These algorithms allow us to compute the Roman domination number of cylindrical graphs $P_m\\Box C_n$ i.e., the Cartesian product of a path and a cycle, in cases $m=7,8,9$, $ n\\geq 3$ and $m\\geq $10$, n\\equiv 0\\pmod 5$. Moreover, we provide a lower bound for the remaining cases $m\\geq 10, n\\not\\equiv 0\\pmod 5$.         ",
    "url": "https://arxiv.org/abs/2409.17658",
    "authors": [
      "J.A. Mart\u00ednez",
      "E.M. Garz\u00f3n",
      "M.L. Puertas"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2409.17669",
    "title": "Impact of opinion formation phenomena in epidemic dynamics: kinetic modeling on networks",
    "abstract": "           After the recent COVID-19 outbreaks, it became increasingly evident that individuals' thoughts and beliefs can have a strong impact on disease transmission. It becomes therefore important to understand how information and opinions on protective measures evolve during epidemics. To this end, incorporating the impact of social media is essential to take into account the hierarchical structure of these platforms. In this context, we present a novel approach to take into account the interplay between infectious disease dynamics and socially-structured opinion dynamics. Our work extends a conventional compartmental framework including behavioral attitudes in shaping public opinion and promoting the adoption of protective measures under the influence of different degrees of connectivity. The proposed approach is capable to reproduce the emergence of epidemic waves. Specifically, it provides a clear link between the social influence of highly connected individuals and the epidemic dynamics. Through a heterogeneity of numerical tests we show how this comprehensive framework offers a more nuanced understanding of epidemic dynamics in the context of modern information dissemination and social behavior.         ",
    "url": "https://arxiv.org/abs/2409.17669",
    "authors": [
      "Giacomo Albi",
      "Elisa Calzola",
      "Giacomo Dimarco",
      "Mattia Zanella"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2409.17721",
    "title": "Improving the Vector Basis Neural Network for RANS Equations Using Separate Trainings",
    "abstract": "           We present a new data-driven turbulence model for Reynolds-averaged Navier-Stokes equations called $\\nu_t$-Vector Basis Neural Network. This new model, grounded on the already existing Vector Basis Neural Network, predicts separately the turbulent viscosity $\\nu_t$ and the contribution of the Reynolds force vector that is not already accounted in $\\nu_t$. Numerical experiments on the flow in a Square Duct show the better accuracy of the new model compared to the reference one.         ",
    "url": "https://arxiv.org/abs/2409.17721",
    "authors": [
      "Davide Oberto"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2409.17852",
    "title": "AMARO: All Heavy-Atom Transferable Neural Network Potentials of Protein Thermodynamics",
    "abstract": "           All-atom molecular simulations offer detailed insights into macromolecular phenomena, but their substantial computational cost hinders the exploration of complex biological processes. We introduce Advanced Machine-learning Atomic Representation Omni-force-field (AMARO), a new neural network potential (NNP) that combines an O(3)-equivariant message-passing neural network architecture, TensorNet, with a coarse-graining map that excludes hydrogen atoms. AMARO demonstrates the feasibility of training coarser NNP, without prior energy terms, to run stable protein dynamics with scalability and generalization capabilities.         ",
    "url": "https://arxiv.org/abs/2409.17852",
    "authors": [
      "Antonio Mirarchi",
      "Raul P. Pelaez",
      "Guillem Simeon",
      "Gianni De Fabritiis"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)",
      "Biological Physics (physics.bio-ph)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2409.17858",
    "title": "How Feature Learning Can Improve Neural Scaling Laws",
    "abstract": "           We develop a solvable model of neural scaling laws beyond the kernel limit. Theoretical analysis of this model shows how performance scales with model size, training time, and the total amount of available data. We identify three scaling regimes corresponding to varying task difficulties: hard, easy, and super easy tasks. For easy and super-easy target functions, which lie in the reproducing kernel Hilbert space (RKHS) defined by the initial infinite-width Neural Tangent Kernel (NTK), the scaling exponents remain unchanged between feature learning and kernel regime models. For hard tasks, defined as those outside the RKHS of the initial NTK, we demonstrate both analytically and empirically that feature learning can improve scaling with training time and compute, nearly doubling the exponent for hard tasks. This leads to a different compute optimal strategy to scale parameters and training time in the feature learning regime. We support our finding that feature learning improves the scaling law for hard tasks but not for easy and super-easy tasks with experiments of nonlinear MLPs fitting functions with power-law Fourier spectra on the circle and CNNs learning vision tasks.         ",
    "url": "https://arxiv.org/abs/2409.17858",
    "authors": [
      "Blake Bordelon",
      "Alexander Atanasov",
      "Cengiz Pehlevan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17899",
    "title": "Revisiting Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations",
    "abstract": "           Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Frechet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems.         ",
    "url": "https://arxiv.org/abs/2409.17899",
    "authors": [
      "Yujia Sun",
      "Zeyu Zhao",
      "Korin Richmond",
      "Yuanchao Li"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2409.17907",
    "title": "PhantomLiDAR: Cross-modality Signal Injection Attacks against LiDAR",
    "abstract": "           LiDAR (Light Detection and Ranging) is a pivotal sensor for autonomous driving, offering precise 3D spatial information. Previous signal attacks against LiDAR systems mainly exploit laser signals. In this paper, we investigate the possibility of cross-modality signal injection attacks, i.e., injecting intentional electromagnetic interference (IEMI) to manipulate LiDAR output. Our insight is that the internal modules of a LiDAR, i.e., the laser receiving circuit, the monitoring sensors, and the beam-steering modules, even with strict electromagnetic compatibility (EMC) testing, can still couple with the IEMI attack signals and result in the malfunction of LiDAR systems. Based on the above attack surfaces, we propose the PhantomLiDAR attack, which manipulates LiDAR output in terms of Points Interference, Points Injection, Points Removal, and even LiDAR Power-Off. We evaluate and demonstrate the effectiveness of PhantomLiDAR with both simulated and real-world experiments on five COTS LiDAR systems. We also conduct feasibility experiments in real-world moving scenarios. We provide potential defense measures that can be implemented at both the sensor level and the vehicle system level to mitigate the risks associated with IEMI attacks. Video demonstrations can be viewed at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.17907",
    "authors": [
      "Zizhi Jin",
      "Qinhong Jiang",
      "Xuancun Lu",
      "Chen Yan",
      "Xiaoyu Ji",
      "Wenyuan Xu"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.17909",
    "title": "Unveiling the Potential of Graph Neural Networks in SME Credit Risk Assessment",
    "abstract": "           This paper takes the graph neural network as the technical framework, integrates the intrinsic connections between enterprise financial indicators, and proposes a model for enterprise credit risk assessment. The main research work includes: Firstly, based on the experience of predecessors, we selected 29 enterprise financial data indicators, abstracted each indicator as a vertex, deeply analyzed the relationships between the indicators, constructed a similarity matrix of indicators, and used the maximum spanning tree algorithm to achieve the graph structure mapping of enterprises; secondly, in the representation learning phase of the mapped graph, a graph neural network model was built to obtain its embedded representation. The feature vector of each node was expanded to 32 dimensions, and three GraphSAGE operations were performed on the graph, with the results pooled using the Pool operation, and the final output of three feature vectors was averaged to obtain the graph's embedded representation; finally, a classifier was constructed using a two-layer fully connected network to complete the prediction task. Experimental results on real enterprise data show that the model proposed in this paper can well complete the multi-level credit level estimation of enterprises. Furthermore, the tree-structured graph mapping deeply portrays the intrinsic connections of various indicator data of the company, and according to the ROC and other evaluation criteria, the model's classification effect is significant and has good \"robustness\".         ",
    "url": "https://arxiv.org/abs/2409.17909",
    "authors": [
      "Bingyao Liu",
      "Iris Li",
      "Jianhua Yao",
      "Yuan Chen",
      "Guanming Huang",
      "Jiajing Wang"
    ],
    "subjectives": [
      "Risk Management (q-fin.RM)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.17938",
    "title": "Error bounds for Physics Informed Neural Networks in Nonlinear Schr\\\"odinger equations placed on unbounded domains",
    "abstract": "           We consider the subcritical nonlinear Schr\u00f6dinger (NLS) in dimension one posed on the unbounded real line. Several previous works have considered the deep neural network approximation of NLS solutions from the numerical and theoretical point of view in the case of bounded domains. In this paper, we introduce a new PINNs method to treat the case of unbounded domains and provide rigorous bounds on the associated approximation error in terms of the energy and Strichartz norms, provided a reasonable integration scheme is available. Applications to traveling waves, breathers and solitons, as well as numerical experiments confirming the validity of the approximation are also provided as well.         ",
    "url": "https://arxiv.org/abs/2409.17938",
    "authors": [
      "Miguel \u00c1. Alejo",
      "Lucrezia Cossetti",
      "Luca Fanelli",
      "Claudio Mu\u00f1oz",
      "Nicol\u00e1s Valenzuela"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2409.18032",
    "title": "FlowBench: A Large Scale Benchmark for Flow Simulation over Complex Geometries",
    "abstract": "           Simulating fluid flow around arbitrary shapes is key to solving various engineering problems. However, simulating flow physics across complex geometries remains numerically challenging and computationally resource-intensive, particularly when using conventional PDE solvers. Machine learning methods offer attractive opportunities to create fast and adaptable PDE solvers. However, benchmark datasets to measure the performance of such methods are scarce, especially for flow physics across complex geometries. We introduce FlowBench, a dataset for neural simulators with over 10K samples, which is currently larger than any publicly available flow physics dataset. FlowBench contains flow simulation data across complex geometries (\\textit{parametric vs. non-parametric}), spanning a range of flow conditions (\\textit{Reynolds number and Grashoff number}), capturing a diverse array of flow phenomena (\\textit{steady vs. transient; forced vs. free convection}), and for both 2D and 3D. FlowBench contains over 10K data samples, with each sample the outcome of a fully resolved, direct numerical simulation using a well-validated simulator framework designed for modeling transport phenomena in complex geometries. For each sample, we include velocity, pressure, and temperature field data at 3 different resolutions and several summary statistics features of engineering relevance (such as coefficients of lift and drag, and Nusselt numbers). %Additionally, we include masks and signed distance fields for each shape. We envision that FlowBench will enable evaluating the interplay between complex geometry, coupled flow phenomena, and data sufficiency on the performance of current, and future, neural PDE solvers. We enumerate several evaluation metrics to help rank order the performance of neural PDE solvers. We benchmark the performance of several baseline methods including FNO, CNO, WNO, and DeepONet.         ",
    "url": "https://arxiv.org/abs/2409.18032",
    "authors": [
      "Ronak Tali",
      "Ali Rabeh",
      "Cheng-Hau Yang",
      "Mehdi Shadkhah",
      "Samundra Karki",
      "Abhisek Upadhyaya",
      "Suriya Dhakshinamoorthy",
      "Marjan Saadati",
      "Soumik Sarkar",
      "Adarsh Krishnamurthy",
      "Chinmay Hegde",
      "Aditya Balu",
      "Baskar Ganapathysubramanian"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:1912.01181",
    "title": "Learning Multi-resolution Graph Edge Embedding for Discovering Brain Network Dysfunction in Neurological Disorders",
    "abstract": "           Tremendous recent literature show that associations between different brain regions, i.e., brain connectivity, provide early symptoms of neurological disorders. Despite significant efforts made for graph neural network (GNN) techniques, their focus on graph nodes makes the state-of-the-art GNN methods not suitable for classifying brain connectivity as graphs where the objective is to characterize disease-relevant network dysfunction patterns on graph links. To address this issue, we propose Multi-resolution Edge Network (MENET) to detect disease-specific connectomic benchmarks with high discrimination power across diagnostic categories. The core of MENET is a novel graph edge-wise transform that we propose, which allows us to capture multi-resolution ``connectomic'' features. Using a rich set of the connectomic features, we devise a graph learning framework to jointly select discriminative edges and assign diagnostic labels for graphs. Experiments on two real datasets show that MENET accurately predicts diagnostic labels and identify brain connectivities highly associated with neurological disorders such as Alzheimer's Disease and Attention-Deficit/Hyperactivity Disorder.         ",
    "url": "https://arxiv.org/abs/1912.01181",
    "authors": [
      "Xin Ma",
      "Guorong Wu",
      "Seong Jae Hwang",
      "Won Hwa Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2304.10783",
    "title": "Denial-of-Service or Fine-Grained Control: Towards Flexible Model Poisoning Attacks on Federated Learning",
    "abstract": "           Federated learning (FL) is vulnerable to poisoning attacks, where adversaries corrupt the global aggregation results and cause denial-of-service (DoS). Unlike recent model poisoning attacks that optimize the amplitude of malicious perturbations along certain prescribed directions to cause DoS, we propose a Flexible Model Poisoning Attack (FMPA) that can achieve versatile attack goals. We consider a practical threat scenario where no extra knowledge about the FL system (e.g., aggregation rules or updates on benign devices) is available to adversaries. FMPA exploits the global historical information to construct an estimator that predicts the next round of the global model as a benign reference. It then fine-tunes the reference model to obtain the desired poisoned model with low accuracy and small perturbations. Besides the goal of causing DoS, FMPA can be naturally extended to launch a fine-grained controllable attack, making it possible to precisely reduce the global accuracy. Armed with precise control, malicious FL service providers can gain advantages over their competitors without getting noticed, hence opening a new attack surface in FL other than DoS. Even for the purpose of DoS, experiments show that FMPA significantly decreases the global accuracy, outperforming six state-of-the-art attacks.         ",
    "url": "https://arxiv.org/abs/2304.10783",
    "authors": [
      "Hangtao Zhang",
      "Zeming Yao",
      "Leo Yu Zhang",
      "Shengshan Hu",
      "Chao Chen",
      "Alan Liew",
      "Zhetao Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2306.14545",
    "title": "Time and State Dependent Neural Delay Differential Equations",
    "abstract": "           Discontinuities and delayed terms are encountered in the governing equations of a large class of problems ranging from physics and engineering to medicine and economics. These systems cannot be properly modelled and simulated with standard Ordinary Differential Equations (ODE), or data-driven approximations such as Neural Ordinary Differential Equations (NODE). To circumvent this issue, latent variables are typically introduced to solve the dynamics of the system in a higher dimensional space and obtain the solution as a projection to the original space. However, this solution lacks physical interpretability. In contrast, Delay Differential Equations (DDEs), and their data-driven approximated counterparts, naturally appear as good candidates to characterize such systems. In this work we revisit the recently proposed Neural DDE by introducing Neural State-Dependent DDE (SDDDE), a general and flexible framework that can model multiple and state- and time-dependent delays. We show that our method is competitive and outperforms other continuous-class models on a wide variety of delayed dynamical systems. Code is available at the repository \\href{this https URL}{here}.         ",
    "url": "https://arxiv.org/abs/2306.14545",
    "authors": [
      "Thibault Monsel",
      "Onofrio Semeraro",
      "Lionel Mathelin",
      "Guillaume Charpiat"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2309.01698",
    "title": "Robust Online Classification: From Estimation to Denoising",
    "abstract": "           We study online classification of features into labels with general hypothesis classes. In our setting, true labels are determined by some function within the hypothesis class but are corrupted by unknown stochastic noise, and the features are generated adversarially. Predictions are made using observed noisy labels and noiseless features, while the performance is measured via minimax risk when comparing against true labels. The noise mechanism is modeled via a general noise kernel that specifies, for any individual data point, a set of distributions from which the actual noisy label distribution is chosen. We show that minimax risk is tightly characterized (up to a logarithmic factor of the hypothesis class size) by the Hellinger gap of the noisy label distributions induced by the kernel, independent of other properties such as the means and variances of the noise. Our main technique is based on a novel reduction to an online comparison scheme of two hypotheses, along with a new conditional version of Le Cam-Birg\u00e9 testing suitable for online settings. Our work provides the first comprehensive characterization for noisy online classification with guarantees with respect to the ground truth while addressing general noisy observations.         ",
    "url": "https://arxiv.org/abs/2309.01698",
    "authors": [
      "Changlong Wu",
      "Ananth Grama",
      "Wojciech Szpankowski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2309.13743",
    "title": "Robust Adaptive MPC Using Uncertainty Compensation",
    "abstract": "           This paper presents an uncertainty compensation-based robust adaptive model predictive control (MPC) framework for linear systems with both matched and unmatched nonlinear uncertainties subject to both state and input constraints. In particular, the proposed control framework leverages an L1 adaptive controller (L1AC) to compensate for the matched uncertainties and to provide guaranteed uniform bounds on the error between the states and control inputs of the actual system and those of a nominal i.e., uncertainty-free, system. The performance bounds provided by the L1AC are then used to tighten the state and control constraints of the actual system, and a model predictive controller is designed for the nominal system with the tightened constraints. The proposed control framework, which we denote as uncertainty compensation-based MPC (UC-MPC), guarantees constraint satisfaction and achieves improved performance compared with existing methods. Simulation results on a flight control example demonstrate the benefits of the proposed framework.         ",
    "url": "https://arxiv.org/abs/2309.13743",
    "authors": [
      "Ran Tao",
      "Pan Zhao",
      "Ilya Kolmanovsky",
      "Naira Hovakimyan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2309.16928",
    "title": "Learning to Receive Help: Intervention-Aware Concept Embedding Models",
    "abstract": "           Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories at train-time. This conditions IntCEMs to effectively select and receive concept interventions when deployed at test-time. Our experiments show that IntCEMs significantly outperform state-of-the-art concept-interpretable models when provided with test-time concept interventions, demonstrating the effectiveness of our approach.         ",
    "url": "https://arxiv.org/abs/2309.16928",
    "authors": [
      "Mateo Espinosa Zarlenga",
      "Katherine M. Collins",
      "Krishnamurthy Dvijotham",
      "Adrian Weller",
      "Zohreh Shams",
      "Mateja Jamnik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.06379",
    "title": "Understanding the Expressivity and Trainability of Fourier Neural Operator: A Mean-Field Perspective",
    "abstract": "           In this paper, we explores the expressivity and trainability of the Fourier Neural Operator (FNO). We establish a mean-field theory for the FNO, analyzing the behavior of the random FNO from an edge of chaos perspective. Our investigation into the expressivity of a random FNO involves examining the ordered-chaos phase transition of the network based on the weight distribution. This phase transition demonstrates characteristics unique to the FNO, induced by mode truncation, while also showcasing similarities to those of densely connected networks. Furthermore, we identify a connection between expressivity and trainability: the ordered and chaotic phases correspond to regions of vanishing and exploding gradients, respectively. This finding provides a practical prerequisite for the stable training of the FNO. Our experimental results corroborate our theoretical findings.         ",
    "url": "https://arxiv.org/abs/2310.06379",
    "authors": [
      "Takeshi Koshizuka",
      "Masahiro Fujisawa",
      "Yusuke Tanaka",
      "Issei Sato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.09802",
    "title": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs",
    "abstract": "           Two lines of approaches are adopted for complex reasoning with LLMs. One line of work prompts LLMs with various reasoning structures, while the structural outputs can be naturally regarded as intermediate reasoning steps. Another line of work adopt LLM-free declarative solvers to do the reasoning task, rendering higher reasoning accuracy but lacking interpretability due to the black-box nature of the solvers. Aiming to resolve the trade-off between answer accuracy and interpretability, we present a simple extension to the latter line of work. Specifically, we showcase that the intermediate search logs generated by Prolog interpreters can be accessed and interpreted into human-readable reasoning proofs. As long as LLMs correctly translate problem descriptions into Prolog representations, the corresponding reasoning proofs are ensured to be causal and reliable. On two logical reasoning and one arithmetic reasoning datasets, our framework obtains significant improvements in terms of both answer accuracy and reasoning proof accuracy. Our code is released at this https URL ",
    "url": "https://arxiv.org/abs/2311.09802",
    "authors": [
      "Sen Yang",
      "Xin Li",
      "Leyang Cui",
      "Lidong Bing",
      "Wai Lam"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2312.10743",
    "title": "A Unified Framework for Multi-Domain CTR Prediction via Large Language Models",
    "abstract": "           Click-Through Rate (CTR) prediction is a crucial task in online recommendation platforms as it involves estimating the probability of user engagement with advertisements or items by clicking on them. Given the availability of various services like online shopping, ride-sharing, food delivery, and professional services on commercial platforms, recommendation systems in these platforms are required to make CTR predictions across multiple domains rather than just a single domain. However, multi-domain click-through rate (MDCTR) prediction remains a challenging task in online recommendation due to the complex mutual influence between domains. Traditional MDCTR models typically encode domains as discrete identifiers, ignoring rich semantic information underlying. Consequently, they can hardly generalize to new domains. Besides, existing models can be easily dominated by some specific domains, which results in significant performance drops in the other domains (i.e. the \"seesaw phenomenon\"). In this paper, we propose a novel solution Uni-CTR to address the above challenges. Uni-CTR leverages a backbone Large Language Model (LLM) to learn layer-wise semantic representations that capture commonalities between domains. Uni-CTR also uses several domain-specific networks to capture the characteristics of each domain. Note that we design a masked loss strategy so that these domain-specific networks are decoupled from backbone LLM. This allows domain-specific networks to remain unchanged when incorporating new or removing domains, thereby enhancing the flexibility and scalability of the system significantly. Experimental results on three public datasets show that Uni-CTR outperforms the state-of-the-art (SOTA) MDCTR models significantly. Furthermore, Uni-CTR demonstrates remarkable effectiveness in zero-shot prediction. We have applied Uni-CTR in industrial scenarios, confirming its efficiency.         ",
    "url": "https://arxiv.org/abs/2312.10743",
    "authors": [
      "Zichuan Fu",
      "Xiangyang Li",
      "Chuhan Wu",
      "Yichao Wang",
      "Kuicai Dong",
      "Xiangyu Zhao",
      "Mengchen Zhao",
      "Huifeng Guo",
      "Ruiming Tang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2312.15268",
    "title": "Manydepth2: Motion-Aware Self-Supervised Monocular Depth Estimation in Dynamic Scenes",
    "abstract": "           Despite advancements in self-supervised monocular depth estimation, challenges persist in dynamic scenarios due to the dependence on assumptions about a static world. In this paper, we present Manydepth2, a Motion-Guided Cost Volume Depth Net, to achieve precise depth estimation for both dynamic objects and static backgrounds, all while maintaining computational efficiency. To tackle the challenges posed by dynamic content, we incorporate optical flow and coarse monocular depth to create a novel static reference frame. This frame is then utilized to build a motion-guided cost volume in collaboration with the target frame. Additionally, to enhance the accuracy and resilience of the network structure, we introduce an attention-based depth net architecture to effectively integrate information from feature maps with varying resolutions. Compared to methods with similar computational costs, Manydepth2 achieves a significant reduction of approximately five percent in root-mean-square error for self-supervised monocular depth estimation on the KITTI-2015 dataset. The code could be found: this https URL ",
    "url": "https://arxiv.org/abs/2312.15268",
    "authors": [
      "Kaichen Zhou",
      "Jia-Wang Bian",
      "Qian Xie",
      "Jian-Qing Zheng",
      "Niki Trigoni",
      "Andrew Markham"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.06532",
    "title": "Generative Adversarial Model-Based Optimization via Source Critic Regularization",
    "abstract": "           Offline model-based optimization seeks to optimize against a learned surrogate model without querying the true oracle objective function during optimization. Such tasks are commonly encountered in protein design, robotics, and clinical medicine where evaluating the oracle function is prohibitively expensive. However, inaccurate surrogate model predictions are frequently encountered along offline optimization trajectories. To address this limitation, we propose generative adversarial model-based optimization using adaptive source critic regularization (aSCR) -- a task- and optimizer- agnostic framework for constraining the optimization trajectory to regions of the design space where the surrogate function is reliable. We propose a computationally tractable algorithm to dynamically adjust the strength of this constraint, and show how leveraging aSCR with standard Bayesian optimization outperforms existing methods on a suite of offline generative design tasks. Our code is available at this https URL ",
    "url": "https://arxiv.org/abs/2402.06532",
    "authors": [
      "Michael S. Yao",
      "Yimeng Zeng",
      "Hamsa Bastani",
      "Jacob Gardner",
      "James C. Gee",
      "Osbert Bastani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.12844",
    "title": "ICON: Improving Inter-Report Consistency in Radiology Report Generation via Lesion-aware Mixup Augmentation",
    "abstract": "           Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation. Aiming to enhance the system's ability to capture similarities in semantically equivalent lesions, our approach first involves extracting lesions from input images and examining their characteristics. Then, we introduce a lesion-aware mixup technique to ensure that the representations of the semantically equivalent lesions align with the same attributes, achieved through a linear combination during the training phase. Extensive experiments on three publicly available chest X-ray datasets verify the effectiveness of our approach, both in terms of improving the consistency and accuracy of the generated reports.         ",
    "url": "https://arxiv.org/abs/2402.12844",
    "authors": [
      "Wenjun Hou",
      "Yi Cheng",
      "Kaishuai Xu",
      "Yan Hu",
      "Wenjie Li",
      "Jiang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.18401",
    "title": "DevPhish: Exploring Social Engineering in Software Supply Chain Attacks on Developers",
    "abstract": "           The Software Supply Chain (SSC) has captured considerable attention from attackers seeking to infiltrate systems and undermine organizations. There is evidence indicating that adversaries utilize Social Engineering (SocE) techniques specifically aimed at software developers. That is, they interact with developers at critical steps in the Software Development Life Cycle (SDLC), such as accessing Github repositories, incorporating code dependencies, and obtaining approval for Pull Requests (PR) to introduce malicious code. This paper aims to comprehensively explore the existing and emerging SocE tactics employed by adversaries to trick Software Engineers (SWEs) into delivering malicious software. By analyzing a diverse range of resources, which encompass established academic literature and real-world incidents, the paper systematically presents an overview of these manipulative strategies within the realm of the SSC. Such insights prove highly beneficial for threat modeling and security gap analysis.         ",
    "url": "https://arxiv.org/abs/2402.18401",
    "authors": [
      "Hossein Siadati",
      "Sima Jafarikhah",
      "Elif Sahin",
      "Terrence Brent Hernandez",
      "Elijah Lorenzo Tripp",
      "Denis Khryashchev"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2404.00769",
    "title": "An Active Perception Game for Robust Information Gathering",
    "abstract": "           Active perception approaches select future viewpoints by using some estimate of the information gain. An inaccurate estimate can be detrimental in critical situations, e.g., locating a person in distress. However the true information gained can only be calculated post hoc, i.e., after the observation is realized. We present an approach for estimating the discrepancy between the information gain (which is the average over putative future observations) and the true information gain. The key idea is to analyze the mathematical relationship between active perception and the estimation error of the information gain in a game-theoretic setting. Using this, we develop an online estimation approach that achieves sub-linear regret (in the number of time-steps) for the estimation of the true information gain and reduces the sub-optimality of active perception systems. We demonstrate our approach for active perception using a comprehensive set of experiments on: (a) different types of environments, including a quadrotor in a photorealistic simulation, real-world robotic data, and real-world experiments with ground robots exploring indoor and outdoor scenes; (b) different types of robotic perception data; and (c) different map representations. On average, our approach reduces information gain estimation errors by 42%, increases the information gain by 7%, PSNR by 5%, and semantic accuracy (measured as the number of objects that are localized correctly) by 6%. In real-world experiments with a Jackal ground robot, our approach demonstrated complex trajectories to explore occluded regions.         ",
    "url": "https://arxiv.org/abs/2404.00769",
    "authors": [
      "Siming He",
      "Yuezhan Tao",
      "Igor Spasojevic",
      "Vijay Kumar",
      "Pratik Chaudhari"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2404.09486",
    "title": "MMCode: Benchmarking Multimodal Large Language Models for Code Generation with Visually Rich Programming Problems",
    "abstract": "           Programming often involves converting detailed and complex specifications into code, a process during which developers typically utilize visual aids to more effectively convey concepts. While recent developments in Large Multimodal Models have demonstrated remarkable abilities in visual reasoning and mathematical tasks, there is little work on investigating whether these models can effectively interpret visual elements for code generation. To this end, we present MMCode, the first multi-modal coding dataset for evaluating algorithmic problem-solving skills in visually rich contexts. MMCode contains 3,548 questions and 6,620 images collected from real-world programming challenges harvested from 10 code competition websites, presenting significant challenges due to the extreme demand for reasoning abilities. Our experiment results show that current state-of-the-art models struggle to solve these problems. The results highlight the lack of powerful vision-code models, and we hope MMCode can serve as an inspiration for future works in this domain. The data and code are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2404.09486",
    "authors": [
      "Kaixin Li",
      "Yuchen Tian",
      "Qisheng Hu",
      "Ziyang Luo",
      "Zhiyong Huang",
      "Jing Ma"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2404.17080",
    "title": "Solving the Graph Burning Problem for Large Graphs",
    "abstract": "           We propose an exact algorithm for the Graph Burning Problem ($\\texttt{GBP}$), an NP-hard optimization problem that models the spread of influence on social networks. Given a graph $G$ with vertex set $V$, the objective is to find a sequence of $k$ vertices in $V$, namely, $v_1, v_2, \\dots, v_k$, such that $k$ is minimum and $\\bigcup_{i = 1}^{k} \\{u\\! \\in\\! V\\! : d(u, v_i) \\leq k - i\\} = V$, where $d(u,v)$ denotes the distance between $u$ and $v$. We formulate the problem as a set covering integer programming model and design a row generation algorithm for the $\\texttt{GBP}$. Our method exploits the fact that a very small number of covering constraints is often sufficient for solving the integer model, allowing the corresponding rows to be generated on demand. To date, the most efficient exact algorithm for the $\\texttt{GBP}$, denoted here by $\\texttt{GDCA}$, is able to obtain optimal solutions for graphs with up to 14,000 vertices within two hours of execution. In comparison, our algorithm finds provably optimal solutions approximately 236 times faster, on average, than $\\texttt{GDCA}$. For larger graphs, memory space becomes a limiting factor for $\\texttt{GDCA}$. Our algorithm, however, solves real-world instances with almost 200,000 vertices in less than 35 seconds, increasing the size of graphs for which optimal solutions are known by a factor of 14.         ",
    "url": "https://arxiv.org/abs/2404.17080",
    "authors": [
      "Felipe de Carvalho Pereira",
      "Pedro Jussieu de Rezende",
      "Tallys Yunes",
      "Luiz Fernando Batista Morato"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2404.19454",
    "title": "Augmented neural forms with parametric boundary-matching operators for solving ordinary differential equations",
    "abstract": "           Approximating solutions of ordinary and partial differential equations constitutes a significant challenge. Based on functional expressions that inherently depend on neural networks, neural forms are specifically designed to precisely satisfy the prescribed initial or boundary conditions of the problem, while providing the approximate solutions in closed form. Departing from the important class of ordinary differential equations, the present work aims to refine and validate the neural forms methodology, paving the ground for further developments in more challenging fields. The main contributions are as follows. First, it introduces a formalism for systematically crafting proper neural forms with adaptable boundary matches that are amenable to optimization. Second, it describes a novel technique for converting problems with Neumann or Robin conditions into equivalent problems with parametric Dirichlet conditions. Third, it outlines a method for determining an upper bound on the absolute deviation from the exact solution. The proposed augmented neural forms approach was tested on a set of diverse problems, encompassing first- and second-order ordinary differential equations, as well as first-order systems. Stiff differential equations have been considered as well. The resulting solutions were subjected to assessment against existing exact solutions, solutions derived through the common penalized neural method, and solutions obtained via contemporary numerical analysis methods. The reported results demonstrate that the augmented neural forms not only satisfy the boundary and initial conditions exactly, but also provide closed-form solutions that facilitate high-quality interpolation and controllable overall precision. These attributes are essential for expanding the application field of neural forms to more challenging problems that are described by partial differential equations.         ",
    "url": "https://arxiv.org/abs/2404.19454",
    "authors": [
      "Adam D. Kypriadis",
      "Isaac E. Lagaris",
      "Aristidis Likas",
      "Konstantinos E. Parsopoulos"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2405.07865",
    "title": "AnoVox: A Benchmark for Multimodal Anomaly Detection in Autonomous Driving",
    "abstract": "           The scale-up of autonomous vehicles depends heavily on their ability to deal with anomalies, such as rare objects on the road. In order to handle such situations, it is necessary to detect anomalies in the first place. Anomaly detection for autonomous driving has made great progress in the past years but suffers from poorly designed benchmarks with a strong focus on camera data. In this work, we propose AnoVox, the largest benchmark for ANOmaly detection in autonomous driving to date. AnoVox incorporates large-scale multimodal sensor data and spatial VOXel ground truth, allowing for the comparison of methods independent of their used sensor. We propose a formal definition of normality and provide a compliant training dataset. AnoVox is the first benchmark to contain both content and temporal anomalies.         ",
    "url": "https://arxiv.org/abs/2405.07865",
    "authors": [
      "Daniel Bogdoll",
      "Iramm Hamdard",
      "Lukas Namgyu R\u00f6\u00dfler",
      "Felix Geisler",
      "Muhammed Bayram",
      "Felix Wang",
      "Jan Imhof",
      "Miguel de Campos",
      "Anushervon Tabarov",
      "Yitian Yang",
      "Hanno Gottschalk",
      "J. Marius Z\u00f6llner"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2405.11658",
    "title": "A Starting Point for Dynamic Community Detection with Leiden Algorithm",
    "abstract": "           Real-world graphs often evolve over time, making community or cluster detection a crucial task. In this technical report, we extend three dynamic approaches - Naive-dynamic (ND), Delta-screening (DS), and Dynamic Frontier (DF) - to our multicore implementation of the Leiden algorithm, known for its high-quality community detection. Our experiments, conducted on a server with a 64-core AMD EPYC-7742 processor, show that ND, DS, and DF Leiden achieve average speedups of 1.37x, 1.47x, and 1.98x on large graphs with random batch updates, and 1.07x, 1.10x, and 1.13x on real-world dynamic graphs, compared to the Static Leiden algorithm. To our knowledge, this is the first attempt to apply dynamic approaches to the Leiden algorithm. We hope these early results pave the way for further development of dynamic approaches for evolving graphs.         ",
    "url": "https://arxiv.org/abs/2405.11658",
    "authors": [
      "Subhajit Sahu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2405.15059",
    "title": "Message-Passing Monte Carlo: Generating low-discrepancy point sets via Graph Neural Networks",
    "abstract": "           Discrepancy is a well-known measure for the irregularity of the distribution of a point set. Point sets with small discrepancy are called low-discrepancy and are known to efficiently fill the space in a uniform manner. Low-discrepancy points play a central role in many problems in science and engineering, including numerical integration, computer vision, machine perception, computer graphics, machine learning, and simulation. In this work, we present the first machine learning approach to generate a new class of low-discrepancy point sets named Message-Passing Monte Carlo (MPMC) points. Motivated by the geometric nature of generating low-discrepancy point sets, we leverage tools from Geometric Deep Learning and base our model on Graph Neural Networks. We further provide an extension of our framework to higher dimensions, which flexibly allows the generation of custom-made points that emphasize the uniformity in specific dimensions that are primarily important for the particular problem at hand. Finally, we demonstrate that our proposed model achieves state-of-the-art performance superior to previous methods by a significant margin. In fact, MPMC points are empirically shown to be either optimal or near-optimal with respect to the discrepancy for low dimension and small number of points, i.e., for which the optimal discrepancy can be determined. Code for generating MPMC points can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2405.15059",
    "authors": [
      "T. Konstantin Rusch",
      "Nathan Kirk",
      "Michael M. Bronstein",
      "Christiane Lemieux",
      "Daniela Rus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.05504",
    "title": "G-Transformer: Counterfactual Outcome Prediction under Dynamic and Time-varying Treatment Regimes",
    "abstract": "           In the context of medical decision making, counterfactual prediction enables clinicians to predict treatment outcomes of interest under alternative courses of therapeutic actions given observed patient history. In this work, we present G-Transformer for counterfactual outcome prediction under dynamic and time-varying treatment strategies. Our approach leverages a Transformer architecture to capture complex, long-range dependencies in time-varying covariates while enabling g-computation, a causal inference method for estimating the effects of dynamic treatment regimes. Specifically, we use a Transformer-based encoder architecture to estimate the conditional distribution of relevant covariates given covariate and treatment history at each time point, then produces Monte Carlo estimates of counterfactual outcomes by simulating forward patient trajectories under treatment strategies of interest. We evaluate G-Transformer extensively using two simulated longitudinal datasets from mechanistic models, and a real-world sepsis ICU dataset from MIMIC-IV. G-Transformer outperforms both classical and state-of-the-art counterfactual prediction models in these settings. To the best of our knowledge, this is the first Transformer-based architecture that supports g-computation for counterfactual outcome prediction under dynamic and time-varying treatment strategies.         ",
    "url": "https://arxiv.org/abs/2406.05504",
    "authors": [
      "Hong Xiong",
      "Feng Wu",
      "Leon Deng",
      "Megan Su",
      "Li-wei H Lehman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.11553",
    "title": "The Susceptibility Paradox in Online Social Influence",
    "abstract": "           Understanding susceptibility to online influence is crucial for mitigating the spread of misinformation and protecting vulnerable audiences. This paper investigates susceptibility to influence within social networks, focusing on the differential effects of influence-driven versus spontaneous behaviors on user content adoption. Our analysis reveals that influence-driven adoption exhibits high homophily, indicating that individuals prone to influence often connect with similarly susceptible peers, thereby reinforcing peer influence dynamics, whereas spontaneous adoption shows significant but lower homophily. Additionally, we extend the Generalized Friendship Paradox to influence-driven behaviors, demonstrating that users' friends are generally more susceptible to influence than the users themselves, de facto establishing the notion of Susceptibility Paradox in online social influence. This pattern does not hold for spontaneous behaviors, where friends exhibit fewer spontaneous adoptions. We find that susceptibility to influence can be predicted using friends' susceptibility alone, while predicting spontaneous adoption requires additional features, such as user metadata. These findings highlight the complex interplay between user engagement and characteristics in spontaneous content adoption. Our results provide new insights into social influence mechanisms and offer implications for designing more effective moderation strategies to protect vulnerable audiences.         ",
    "url": "https://arxiv.org/abs/2406.11553",
    "authors": [
      "Luca Luceri",
      "Jinyi Ye",
      "Julie Jiang",
      "Emilio Ferrara"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2406.16959",
    "title": "Recurrent Stochastic Configuration Networks for Temporal Data Analytics",
    "abstract": "           Temporal data modelling techniques with neural networks are useful in many domain applications, including time-series forecasting and control engineering. This paper aims at developing a recurrent version of stochastic configuration networks (RSCNs) for problem solving, where we have no underlying assumption on the dynamic orders of the input variables. Given a collection of historical data, we first build an initial RSCN model in the light of a supervisory mechanism, followed by an online update of the output weights by using a projection algorithm. Some theoretical results are established, including the echo state property, the universal approximation property of RSCNs for both the offline and online learnings, and the convergence of the output weights. The proposed RSCN model is remarkably distinguished from the well-known echo state networks (ESNs) in terms of the way of assigning the input random weight matrix and a special structure of the random feedback matrix. A comprehensive comparison study among the long short-term memory (LSTM) network, the original ESN, and several state-of-the-art ESN methods such as the simple cycle reservoir (SCR), the polynomial ESN (PESN), the leaky-integrator ESN (LIESN) and RSCN is carried out. Numerical results clearly indicate that the proposed RSCN performs favourably over all of the datasets.         ",
    "url": "https://arxiv.org/abs/2406.16959",
    "authors": [
      "Dianhui Wang",
      "Gang Dang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.17255",
    "title": "MPCODER: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning",
    "abstract": "           Large Language Models (LLMs) have demonstrated great potential for assisting developers in their daily development. However, most research focuses on generating correct code, how to use LLMs to generate personalized code has seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user Personalized Code Generator) to generate personalized code for multiple users. To better learn coding style features, we utilize explicit coding style residual learning to capture the syntax code style standards and implicit style learning to capture the semantic code style conventions. We train a multi-user style adapter to better differentiate the implicit feature representations of different users through contrastive learning, ultimately enabling personalized code generation for multiple users. We further propose a novel evaluation metric for estimating similarities between codes of different coding styles. The experimental results show the effectiveness of our approach for this novel task.         ",
    "url": "https://arxiv.org/abs/2406.17255",
    "authors": [
      "Zhenlong Dai",
      "Chang Yao",
      "WenKang Han",
      "Ying Yuan",
      "Zhipeng Gao",
      "Jingyuan Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.19234",
    "title": "Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation",
    "abstract": "           Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that mitigates issues such as hallucinations and knowledge staleness in Large Language Models (LLMs) by retrieving relevant knowledge from an external database to assist in content generation. Existing research has demonstrated potential privacy risks associated with the LLMs of RAG. However, the privacy risks posed by the integration of an external database, which often contains sensitive data such as medical records or personal identities, have remained largely unexplored. In this paper, we aim to bridge this gap by focusing on membership privacy of RAG's external database, with the aim of determining whether a given sample is part of the RAG's database. Our basic idea is that if a sample is in the external database, it will exhibit a high degree of semantic similarity to the text generated by the RAG system. We present S$^2$MIA, a \\underline{M}embership \\underline{I}nference \\underline{A}ttack that utilizes the \\underline{S}emantic \\underline{S}imilarity between a given sample and the content generated by the RAG system. With our proposed S$^2$MIA, we demonstrate the potential to breach the membership privacy of the RAG database. Extensive experiment results demonstrate that S$^2$MIA can achieve a strong inference performance compared with five existing MIAs, and is able to escape from the protection of three representative defenses.         ",
    "url": "https://arxiv.org/abs/2406.19234",
    "authors": [
      "Yuying Li",
      "Gaoyang Liu",
      "Chen Wang",
      "Yang Yang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.00312",
    "title": "UDC: A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems",
    "abstract": "           Single-stage neural combinatorial optimization solvers have achieved near-optimal results on various small-scale combinatorial optimization (CO) problems without needing expert knowledge. However, these solvers exhibit significant performance degradation when applied to large-scale CO problems. Recently, two-stage neural methods with divide-and-conquer strategies have shown efficiency in addressing large-scale CO problems. Nevertheless, the performance of these methods highly relies on problem-specific heuristics in either the divide or the conquer procedure, which limits their applicability to general CO problems. Moreover, these methods employ separate training schemes and ignore the interdependencies between the dividing and conquering strategies, which often leads to sub-optimal solutions. To tackle these drawbacks, this article develops a unified neural divide-and-conquer framework (i.e., UDC) for solving general large-scale CO problems. UDC offers a Divide-Conquer-Reunion (DCR) training method to eliminate the negative impact of a sub-optimal dividing policy. Employing a high-efficiency Graph Neural Network (GNN) for global instance dividing and a fixed-length sub-path solver for conquering divided sub-problems, the proposed UDC framework demonstrates extensive applicability, achieving superior performance in 10 representative large-scale CO problems. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.00312",
    "authors": [
      "Zhi Zheng",
      "Changliang Zhou",
      "Tong Xialiang",
      "Mingxuan Yuan",
      "Zhenkun Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2407.17896",
    "title": "SR-CurvANN: Advancing 3D Surface Reconstruction through Curvature-Aware Neural Networks",
    "abstract": "           Incomplete or missing data in three-dimensional (3D) models can lead to erroneous or flawed renderings, limiting their usefulness in applications such as visualization, geometric computation, and 3D printing. Conventional surface-repair techniques often fail to infer complex geometric details in missing areas. Neural networks successfully address hole-filling tasks in 2D images using inpainting techniques. The combination of surface reconstruction algorithms, guided by the model's curvature properties and the creativity of neural networks in the inpainting processes should provide realistic results in the hole completion task. In this paper, we propose a novel method entitled SR-CurvANN (Surface Reconstruction Based on Curvature-Aware Neural Networks) that incorporates neural network-based 2D inpainting to effectively reconstruct 3D surfaces. We train the neural networks with images that represent planar representations of the curvature at vertices of hundreds of 3D models. Once the missing areas have been inferred, a coarse-to-fine surface deformation process ensures that the surface fits the reconstructed curvature image. Our proposal makes it possible to learn and generalize patterns from a wide variety of training 3D models, generating comprehensive inpainted curvature images and surfaces. Experiments conducted on 959 models with several holes have demonstrated that SR-CurvANN excels in the shape completion process, filling holes with a remarkable level of realism and precision.         ",
    "url": "https://arxiv.org/abs/2407.17896",
    "authors": [
      "Marina Hern\u00e1ndez-Bautista",
      "Francisco J. Melero"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.18789",
    "title": "Granularity is crucial when applying differential privacy to text: An investigation for neural machine translation",
    "abstract": "           Applying differential privacy (DP) by means of the DP-SGD algorithm to protect individual data points during training is becoming increasingly popular in NLP. However, the choice of granularity at which DP is applied is often neglected. For example, neural machine translation (NMT) typically operates on the sentence-level granularity. From the perspective of DP, this setup assumes that each sentence belongs to a single person and any two sentences in the training dataset are independent. This assumption is however violated in many real-world NMT datasets, e.g., those including dialogues. For proper application of DP we thus must shift from sentences to entire documents. In this paper, we investigate NMT at both the sentence and document levels, analyzing the privacy/utility trade-off for both scenarios, and evaluating the risks of not using the appropriate privacy granularity in terms of leaking personally identifiable information (PII). Our findings indicate that the document-level NMT system is more resistant to membership inference attacks, emphasizing the significance of using the appropriate granularity when working with DP.         ",
    "url": "https://arxiv.org/abs/2407.18789",
    "authors": [
      "Doan Nam Long Vu",
      "Timour Igamberdiev",
      "Ivan Habernal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.03944",
    "title": "Improving Fast Adversarial Training Paradigm: An Example Taxonomy Perspective",
    "abstract": "           While adversarial training is an effective defense method against adversarial attacks, it notably increases the training cost. To this end, fast adversarial training (FAT) is presented for efficient training and has become a hot research topic. However, FAT suffers from catastrophic overfitting, which leads to a performance drop compared with multi-step adversarial training. However, the cause of catastrophic overfitting remains unclear and lacks exploration. In this paper, we present an example taxonomy in FAT, which identifies that catastrophic overfitting is caused by the imbalance between the inner and outer optimization in FAT. Furthermore, we investigated the impact of varying degrees of training loss, revealing a correlation between training loss and catastrophic overfitting. Based on these observations, we redesign the loss function in FAT with the proposed dynamic label relaxation to concentrate the loss range and reduce the impact of misclassified examples. Meanwhile, we introduce batch momentum initialization to enhance the diversity to prevent catastrophic overfitting in an efficient manner. Furthermore, we also propose Catastrophic Overfitting aware Loss Adaptation (COLA), which employs a separate training strategy for examples based on their loss degree. Our proposed method, named example taxonomy aware FAT (ETA), establishes an improved paradigm for FAT. Experiment results demonstrate our ETA achieves state-of-the-art performance. Comprehensive experiments on four standard datasets demonstrate the competitiveness of our proposed method.         ",
    "url": "https://arxiv.org/abs/2408.03944",
    "authors": [
      "Jie Gui",
      "Chengze Jiang",
      "Minjing Dong",
      "Kun Tong",
      "Xinli Shi",
      "Yuan Yan Tang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.05968",
    "title": "Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large Language Models with Ex-Post Dataset Construction",
    "abstract": "           The rise of Large Language Models (LLMs) has triggered legal and ethical concerns, especially regarding the unauthorized use of copyrighted materials in their training datasets. This has led to lawsuits against tech companies accused of using protected content without permission. Membership Inference Attacks (MIAs) aim to detect whether specific documents were used in a given LLM pretraining, but their effectiveness is undermined by biases such as time-shifts and n-gram overlaps. This paper addresses the evaluation of MIAs on LLMs with partially inferable training sets, under the ex-post hypothesis, which acknowledges inherent distributional biases between members and non-members datasets. We propose and validate algorithms to create ``non-biased'' and ``non-classifiable'' datasets for fairer MIA assessment. Experiments using the Gutenberg dataset on OpenLamma and Pythia show that neutralizing known biases alone is insufficient. Our methods produce non-biased ex-post datasets with AUC-ROC scores comparable to those previously obtained on genuinely random datasets, validating our approach. Globally, MIAs yield results close to random, with only one being effective on both random and our datasets, but its performance decreases when bias is removed.         ",
    "url": "https://arxiv.org/abs/2408.05968",
    "authors": [
      "C\u00e9dric Eichler",
      "Nathan Champeil",
      "Nicolas Anciaux",
      "Alexandra Bensamoun",
      "Heber Hwang Arcolezi",
      "Jos\u00e9 Maria De Fuentes"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2408.06190",
    "title": "FruitNeRF: A Unified Neural Radiance Field based Fruit Counting Framework",
    "abstract": "           We introduce FruitNeRF, a unified novel fruit counting framework that leverages state-of-the-art view synthesis methods to count any fruit type directly in 3D. Our framework takes an unordered set of posed images captured by a monocular camera and segments fruit in each image. To make our system independent of the fruit type, we employ a foundation model that generates binary segmentation masks for any fruit. Utilizing both modalities, RGB and semantic, we train a semantic neural radiance field. Through uniform volume sampling of the implicit Fruit Field, we obtain fruit-only point clouds. By applying cascaded clustering on the extracted point cloud, our approach achieves precise fruit this http URL use of neural radiance fields provides significant advantages over conventional methods such as object tracking or optical flow, as the counting itself is lifted into 3D. Our method prevents double counting fruit and avoids counting irrelevant this http URL evaluate our methodology using both real-world and synthetic datasets. The real-world dataset consists of three apple trees with manually counted ground truths, a benchmark apple dataset with one row and ground truth fruit location, while the synthetic dataset comprises various fruit types including apple, plum, lemon, pear, peach, and this http URL, we assess the performance of fruit counting using the foundation model compared to a U-Net.         ",
    "url": "https://arxiv.org/abs/2408.06190",
    "authors": [
      "Lukas Meyer",
      "Andreas Gilson",
      "Ute Schmid",
      "Marc Stamminger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.10672",
    "title": "Neural Exploratory Landscape Analysis",
    "abstract": "           Recent research in Meta-Black-Box Optimization (MetaBBO) have shown that meta-trained neural networks can effectively guide the design of black-box optimizers, significantly reducing the need for expert tuning and delivering robust performance across complex problem distributions. Despite their success, a paradox remains: MetaBBO still rely on human-crafted Exploratory Landscape Analysis features to inform the meta-level agent about the low-level optimization progress. To address the gap, this paper proposes Neural Exploratory Landscape Analysis (NeurELA), a novel framework that dynamically profiles landscape features through a two-stage, attention-based neural network, executed in an entirely end-to-end fashion. NeurELA is pre-trained over a variety of MetaBBO algorithms using a multi-task neuroevolution strategy. Extensive experiments show that NeurELA achieves consistently superior performance when integrated into different and even unseen MetaBBO tasks and can be efficiently fine-tuned for further performance boost. This advancement marks a pivotal step in making MetaBBO algorithms more autonomous and broadly this http URL source code of NeurELA can be accessed at this https URL.         ",
    "url": "https://arxiv.org/abs/2408.10672",
    "authors": [
      "Zeyuan Ma",
      "Jiacheng Chen",
      "Hongshu Guo",
      "Yue-Jiao Gong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2408.13689",
    "title": "Decentralised Variational Inference Frameworks for Multi-object Tracking on Sensor Network",
    "abstract": "           This paper tackles the challenge of multi-sensor multi-object tracking by proposing various decentralised Variational Inference (VI) schemes that match the tracking performance of centralised sensor fusion with only local message exchanges among neighboring sensors. We first establish a centralised VI sensor fusion scheme as a benchmark and analyse the limitations of its decentralised counterpart, which requires sensors to await consensus at each VI iteration. Therefore, we propose a decentralised gradient-based VI framework that optimises the Locally Maximised Evidence Lower Bound (LM-ELBO) instead of the standard ELBO, which reduces the parameter search space and enables faster convergence, making it particularly beneficial for decentralised this http URL proposed framework is inherently self-evolving, improving with advancements in decentralised optimisation techniques for convergence guarantees and efficiency. Further, we enhance the convergence speed of proposed decentralised schemes using natural gradients and gradient tracking strategies. Results verify that our decentralised VI schemes are empirically equivalent to centralised fusion in tracking performance. Notably, the decentralised natural gradient VI method is the most communication-efficient, with communication costs comparable to suboptimal decentralised strategies while delivering notably higher tracking accuracy.         ",
    "url": "https://arxiv.org/abs/2408.13689",
    "authors": [
      "Qing Li",
      "Runze Gan",
      "Simon Godsill"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2408.15953",
    "title": "Modeling and Analyzing the Influence of Non-Item Pages on Sequential Next-Item Prediction",
    "abstract": "           Analyzing sequences of interactions between users and items, sequential recommendation models can learn user intent and make predictions about the next item. Next to item interactions, most systems also have interactions with what we call non-item pages: these pages are not related to specific items but still can provide insights of the user's interests, as, for example, navigation pages. We therefore propose a general way to include these non-item pages in sequential recommendation models to enhance next-item prediction. First, we demonstrate the influence of non-item pages on following interactions with the hypotheses testing framework HypTrails and propose methods for representing non-item pages in sequential recommendation models. Subsequently, we adapt popular sequential recommender models to integrate non-item pages and investigate their performance with different item representation strategies as well as their ability to handle noisy data. To show the general capabilities of the models to integrate non-item pages, we create a synthetic dataset for a controlled setting and then evaluate the improvements from including non-item pages on two real-world datasets. Our results show that non-item pages are a valuable source of information, and incorporating them in sequential recommendation models increases the performance of next-item prediction across all analyzed model architectures.         ",
    "url": "https://arxiv.org/abs/2408.15953",
    "authors": [
      "Elisabeth Fischer",
      "Albin Zehe",
      "Andreas Hotho",
      "Daniel Schl\u00f6r"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.03175",
    "title": "Data-based approaches to learning and control by similarity between heterogeneous systems",
    "abstract": "           This paper proposes basic definitions of similarity and similarity indexes between admissible behaviors of heterogeneous host and guest systems and further presents a similarity-based learning control framework by exploiting the offline sampled data. By exploring helpful geometric properties of the admissible behavior and decomposing it into the subspace and offset components, the similarity indexes between two admissible behaviors are defined as the principal angles between their corresponding subspace components. By reconstructing the admissible behaviors leveraging sampled data, an efficient strategy for calculating the similarity indexes is developed, based on which a similarity-based learning control framework is proposed. It is shown that, with the application of similarity-based learning control, the host system can directly accomplish the same control tasks by utilizing the successful experience provided by the guest system, without having to undergo the trial-and-error process. All results in this paper are supported by simulation examples.         ",
    "url": "https://arxiv.org/abs/2409.03175",
    "authors": [
      "Chenchao Wang",
      "Deyuan Meng"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.04428",
    "title": "Hybrid Spiking Neural Networks for Low-Power Intra-Cortical Brain-Machine Interfaces",
    "abstract": "           Intra-cortical brain-machine interfaces (iBMIs) have the potential to dramatically improve the lives of people with paraplegia by restoring their ability to perform daily activities. However, current iBMIs suffer from scalability and mobility limitations due to bulky hardware and wiring. Wireless iBMIs offer a solution but are constrained by a limited data rate. To overcome this challenge, we are investigating hybrid spiking neural networks for embedded neural decoding in wireless iBMIs. The networks consist of a temporal convolution-based compression followed by recurrent processing and a final interpolation back to the original sequence length. As recurrent units, we explore gated recurrent units (GRUs), leaky integrate-and-fire (LIF) neurons, and a combination of both - spiking GRUs (sGRUs) and analyze their differences in terms of accuracy, footprint, and activation sparsity. To that end, we train decoders on the \"Nonhuman Primate Reaching with Multichannel Sensorimotor Cortex Electrophysiology\" dataset and evaluate it using the NeuroBench framework, targeting both tracks of the IEEE BioCAS Grand Challenge on Neural Decoding. Our approach achieves high accuracy in predicting velocities of primate reaching movements from multichannel primary motor cortex recordings while maintaining a low number of synaptic operations, surpassing the current baseline models in the NeuroBench framework. This work highlights the potential of hybrid neural networks to facilitate wireless iBMIs with high decoding precision and a substantial increase in the number of monitored neurons, paving the way toward more advanced neuroprosthetic technologies.         ",
    "url": "https://arxiv.org/abs/2409.04428",
    "authors": [
      "Alexandru Vasilache",
      "Jann Krausse",
      "Klaus Knobloch",
      "Juergen Becker"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2409.12014",
    "title": "BRDF-NeRF: Neural Radiance Fields with Optical Satellite Images and BRDF Modelling",
    "abstract": "           Neural radiance fields (NeRF) have gained prominence as a machine learning technique for representing 3D scenes and estimating the bidirectional reflectance distribution function (BRDF) from multiple images. However, most existing research has focused on close-range imagery, typically modeling scene surfaces with simplified Microfacet BRDF models, which are often inadequate for representing complex Earth surfaces. Furthermore, NeRF approaches generally require large sets of simultaneously captured images for high-quality surface depth reconstruction - a condition rarely met in satellite imaging. To overcome these challenges, we introduce BRDF-NeRF, which incorporates the physically-based semi-empirical Rahman-Pinty-Verstraete (RPV) BRDF model, known to better capture the reflectance properties of natural surfaces. Additionally, we propose guided volumetric sampling and depth supervision to enable radiance field modeling with a minimal number of views. Our method is evaluated on two satellite datasets: (1) Djibouti, captured at varying viewing angles within a single epoch with a fixed Sun position, and (2) Lanzhou, captured across multiple epochs with different Sun positions and viewing angles. Using only three to four satellite images for training, BRDF-NeRF successfully synthesizes novel views from unseen angles and generates high-quality digital surface models (DSMs).         ",
    "url": "https://arxiv.org/abs/2409.12014",
    "authors": [
      "Lulin Zhang",
      "Ewelina Rupnik",
      "Tri Dung Nguyen",
      "St\u00e9phane Jacquemoud",
      "Yann Klinger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.13503",
    "title": "SatFed: A Resource-Efficient LEO Satellite-Assisted Heterogeneous Federated Learning Framework",
    "abstract": "           Traditional federated learning (FL) frameworks rely heavily on terrestrial networks, where coverage limitations and increasing bandwidth congestion significantly hinder model convergence. Fortunately, the advancement of low-Earth orbit (LEO) satellite networks offers promising new communication avenues to augment traditional terrestrial FL. Despite this potential, the limited satellite-ground communication bandwidth and the heterogeneous operating environments of ground devices-including variations in data, bandwidth, and computing power-pose substantial challenges for effective and robust satellite-assisted FL. To address these challenges, we propose SatFed, a resource-efficient satellite-assisted heterogeneous FL framework. SatFed implements freshness-based model prioritization queues to optimize the use of highly constrained satellite-ground bandwidth, ensuring the transmission of the most critical models. Additionally, a multigraph is constructed to capture real-time heterogeneous relationships between devices, including data distribution, terrestrial bandwidth, and computing capability. This multigraph enables SatFed to aggregate satellite-transmitted models into peer guidance, enhancing local training in heterogeneous environments. Extensive experiments with real-world LEO satellite networks demonstrate that SatFed achieves superior performance and robustness compared to state-of-the-art benchmarks.         ",
    "url": "https://arxiv.org/abs/2409.13503",
    "authors": [
      "Yuxin Zhang",
      "Zheng Lin",
      "Zhe Chen",
      "Zihan Fang",
      "Wenjun Zhu",
      "Xianhao Chen",
      "Jin Zhao",
      "Yue Gao"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.13590",
    "title": "Toward Interactive Optimization of Source Code Differences: An Empirical Study of Its Performance",
    "abstract": "           A source code difference (diff) indicates changes made by comparing new and old source codes, and it can be utilized in code reviews to help developers understand the changes made to the code. Although many diff generation methods have been proposed, existing automatic methods may generate nonoptimal diffs, hindering reviewers from understanding the changes. In this paper, we propose an interactive approach to optimize diffs. Users can provide feedback for the points of a diff that should not be matched but are or parts that should be matched but are not. The edit graph is updated based on this feedback, enabling users to obtain a more optimal diff. We simulated our proposed method by applying a search algorithm to empirically assess the number of feedback instances required and the amount of diff optimization resulting from the feedback to investigate the potential of this approach. The results of 23 GitHub projects confirm that 92% of nonoptimal diffs can be addressed with less than four feedback actions in the ideal case.         ",
    "url": "https://arxiv.org/abs/2409.13590",
    "authors": [
      "Tsukasa Yagi",
      "Shinpei Hayashi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2409.14109",
    "title": "Vision-Language Models Assisted Unsupervised Video Anomaly Detection",
    "abstract": "           Video anomaly detection is a subject of great interest across industrial and academic domains due to its crucial role in computer vision applications. However, the inherent unpredictability of anomalies and the scarcity of anomaly samples present significant challenges for unsupervised learning methods. To overcome the limitations of unsupervised learning, which stem from a lack of comprehensive prior knowledge about anomalies, we propose VLAVAD (Video-Language Models Assisted Anomaly Detection). Our method employs a cross-modal pre-trained model that leverages the inferential capabilities of large language models (LLMs) in conjunction with a Selective-Prompt Adapter (SPA) for selecting semantic space. Additionally, we introduce a Sequence State Space Module (S3M) that detects temporal inconsistencies in semantic features. By mapping high-dimensional visual features to low-dimensional semantic ones, our method significantly enhance the interpretability of unsupervised anomaly detection. Our proposed approach effectively tackles the challenge of detecting elusive anomalies that are hard to discern over periods, achieving SOTA on the challenging ShanghaiTech dataset.         ",
    "url": "https://arxiv.org/abs/2409.14109",
    "authors": [
      "Yalong Jiang",
      "Liquan Mao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.14500",
    "title": "TabGraphs: A Benchmark and Strong Baselines for Learning on Graphs with Tabular Node Features",
    "abstract": "           Tabular machine learning is an important field for industry and science. In this field, table rows are usually treated as independent data samples, but additional information about relations between them is sometimes available and can be used to improve predictive performance. Such information can be naturally modeled with a graph, thus tabular machine learning may benefit from graph machine learning methods. However, graph machine learning models are typically evaluated on datasets with homogeneous node features, which have little in common with heterogeneous mixtures of numerical and categorical features present in tabular datasets. Thus, there is a critical difference between the data used in tabular and graph machine learning studies, which does not allow one to understand how successfully graph models can be transferred to tabular data. To bridge this gap, we propose a new benchmark of diverse graphs with heterogeneous tabular node features and realistic prediction tasks. We use this benchmark to evaluate a vast set of models, including simple methods previously overlooked in the literature. Our experiments show that graph neural networks (GNNs) can indeed often bring gains in predictive performance for tabular data, but standard tabular models also can be adapted to work with graph data by using simple feature preprocessing, which sometimes enables them to compete with and even outperform GNNs. Based on our empirical study, we provide insights for researchers and practitioners in both tabular and graph machine learning fields.         ",
    "url": "https://arxiv.org/abs/2409.14500",
    "authors": [
      "Gleb Bazhenov",
      "Oleg Platonov",
      "Liudmila Prokhorenkova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.14552",
    "title": "Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training",
    "abstract": "           Emojis have gained immense popularity on social platforms, serving as a common means to supplement or replace text. However, existing data mining approaches generally either completely ignore or simply treat emojis as ordinary Unicode characters, which may limit the model's ability to grasp the rich semantic information in emojis and the interaction between emojis and texts. Thus, it is necessary to release the emoji's power in social media data mining. To this end, we first construct a heterogeneous graph consisting of three types of nodes, i.e. post, word and emoji nodes to improve the representation of different elements in posts. The edges are also well-defined to model how these three elements interact with each other. To facilitate the sharing of information among post, word and emoji nodes, we propose a graph pre-train framework for text and emoji co-modeling, which contains two graph pre-training tasks: node-level graph contrastive learning and edge-level link reconstruction learning. Extensive experiments on the Xiaohongshu and Twitter datasets with two types of downstream tasks demonstrate that our approach proves significant improvement over previous strong baseline methods.         ",
    "url": "https://arxiv.org/abs/2409.14552",
    "authors": [
      "Zhou Zhang",
      "Dongzeng Tan",
      "Jiaan Wang",
      "Yilong Chen",
      "Jiarong Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.14816",
    "title": "VARADE: a Variational-based AutoRegressive model for Anomaly Detection on the Edge",
    "abstract": "           Detecting complex anomalies on massive amounts of data is a crucial task in Industry 4.0, best addressed by deep learning. However, available solutions are computationally demanding, requiring cloud architectures prone to latency and bandwidth issues. This work presents VARADE, a novel solution implementing a light autoregressive framework based on variational inference, which is best suited for real-time execution on the edge. The proposed approach was validated on a robotic arm, part of a pilot production line, and compared with several state-of-the-art algorithms, obtaining the best trade-off between anomaly detection accuracy, power consumption and inference frequency on two different edge platforms.         ",
    "url": "https://arxiv.org/abs/2409.14816",
    "authors": [
      "Alessio Mascolini",
      "Sebastiano Gaiardelli",
      "Francesco Ponzio",
      "Nicola Dall'Ora",
      "Enrico Macii",
      "Sara Vinco",
      "Santa Di Cataldo",
      "Franco Fummi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.15228",
    "title": "A Comprehensive Framework for Evaluating API-oriented Code Generation in Large Language Models",
    "abstract": "           Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as powerful tools for code generation, significantly enhancing productivity and accelerating software development. However, existing benchmarks primarily focus on general code generation without considering API-oriented code generation, i.e., generating code that invokes APIs from specific libraries. Given the growing demand for API-oriented code generation, there is a pressing need for a systematic and automated approach to evaluate LLM on API-oriented code generation. To address this gap, we propose AutoAPIEval, a lightweight and automated framework designed to evaluate the capabilities of LLMs in API-oriented code generation. Our framework works with any library that provides API documentation and focuses on two unit tasks: API recommendation and code example generation, along with four metrics to evaluate the generated APIs and code examples, such as the proportion of incorrect API recommendations for Task 1, and the proportion of code examples where no specific API is invoked and uncompilable/unexecutable code examples for Task 2. In addition, we conducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder) and Java Runtime Environment 8 to demonstrate the framework's effectiveness. Our findings reveal substantial variability in LLM performance across tasks, with ChatGPT adhering better to instructions, while sharing similar effectiveness in code example generation with its counterparts (i.e., MagiCoder and DeekSeek Coder). We also identify key factors associated with code quality, such as API popularity and model confidence, and build classifiers that achieve high accuracy in detecting incorrect API recommendations and erroneous code examples. Retrieval-augmented generation enhances the quality of code generated by LLMs, though its effectiveness varies across different LLMs.         ",
    "url": "https://arxiv.org/abs/2409.15228",
    "authors": [
      "Yixi Wu",
      "Pengfei He",
      "Zehao Wang",
      "Shaowei Wang",
      "Yuan Tian",
      "Tse-Hsun Chen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.15246",
    "title": "On-Air Deep Learning Integrated Semantic Inference Models for Enhanced Earth Observation Satellite Networks",
    "abstract": "           Earth Observation (EO) systems play a crucial role in achieving Sustainable Development Goals by collecting and analyzing vital global data through satellite networks. These systems are essential for tasks like mapping, disaster monitoring, and resource management, but they face challenges in processing and transmitting large volumes of EO data, especially in specialized fields such as agriculture and real-time disaster response. Domain-adapted Large Language Models (LLMs) provide a promising solution by facilitating data fusion between extensive EO data and semantic EO data. By improving integration and interpretation of diverse datasets, LLMs address the challenges of processing specialized information in agriculture and disaster response applications. This fusion enhances the accuracy and relevance of transmitted data. This paper presents a framework for semantic communication in EO satellite networks, aimed at improving data transmission efficiency and overall system performance through cognitive processing techniques. The proposed system employs Discrete-Task-Oriented Source-Channel Coding (DT-JSCC) and Semantic Data Augmentation (SA) to focus on relevant information while minimizing communication overhead. By integrating cognitive semantic processing and inter-satellite links, the framework enhances the analysis and transmission of multispectral satellite imagery, improving object detection, pattern recognition, and real-time decision-making. The introduction of Cognitive Semantic Augmentation (CSA) allows satellites to process and transmit semantic information, boosting adaptability to changing environments and application needs. This end-to-end architecture is tailored for next-generation satellite networks, such as those supporting 6G, and demonstrates significant improvements in efficiency and accuracy.         ",
    "url": "https://arxiv.org/abs/2409.15246",
    "authors": [
      "Hong-fu Chou",
      "Vu Nguyen Ha",
      "Prabhu Thiruvasagam",
      "Thanh-Dung Le",
      "Geoffrey Eappen",
      "Ti Ti Nguyen",
      "Luis M. Garces-Socarras",
      "Jorge L. Gonzalez-Rios",
      "Juan Carlos Merlano-Duncan",
      "Symeon Chatzinotas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2409.15600",
    "title": "Polyatomic Complexes: A topologically-informed learning representation for atomistic systems",
    "abstract": "           Developing robust representations of chemical structures that enable models to learn topological inductive biases is challenging. In this manuscript, we present a representation of atomistic systems. We begin by proving that our representation satisfies all structural, geometric, efficiency, and generalizability constraints. Afterward, we provide a general algorithm to encode any atomistic system. Finally, we report performance comparable to state-of-the-art methods on numerous tasks. We open-source all code and datasets. The code and data are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.15600",
    "authors": [
      "Rahul Khorana",
      "Marcus Noack",
      "Jin Qian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2409.16225",
    "title": "VideoPatchCore: An Effective Method to Memorize Normality for Video Anomaly Detection",
    "abstract": "           Video anomaly detection (VAD) is a crucial task in video analysis and surveillance within computer vision. Currently, VAD is gaining attention with memory techniques that store the features of normal frames. The stored features are utilized for frame reconstruction, identifying an abnormality when a significant difference exists between the reconstructed and input frames. However, this approach faces several challenges due to the simultaneous optimization required for both the memory and encoder-decoder model. These challenges include increased optimization difficulty, complexity of implementation, and performance variability depending on the memory size. To address these challenges,we propose an effective memory method for VAD, called VideoPatchCore. Inspired by PatchCore, our approach introduces a structure that prioritizes memory optimization and configures three types of memory tailored to the characteristics of video data. This method effectively addresses the limitations of existing memory-based methods, achieving good performance comparable to state-of-the-art methods. Furthermore, our method requires no training and is straightforward to implement, making VAD tasks more accessible. Our code is available online at this http URL.         ",
    "url": "https://arxiv.org/abs/2409.16225",
    "authors": [
      "Sunghyun Ahn",
      "Youngwan Jo",
      "Kijung Lee",
      "Sanghyun Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.13387",
    "title": "Assumption violations in causal discovery and the robustness of score matching",
    "abstract": "           When domain knowledge is limited and experimentation is restricted by ethical, financial, or time constraints, practitioners turn to observational causal discovery methods to recover the causal structure, exploiting the statistical properties of their data. Because causal discovery without further assumptions is an ill-posed problem, each algorithm comes with its own set of usually untestable assumptions, some of which are hard to meet in real datasets. Motivated by these considerations, this paper extensively benchmarks the empirical performance of recent causal discovery methods on observational i.i.d. data generated under different background conditions, allowing for violations of the critical assumptions required by each selected approach. Our experimental findings show that score matching-based methods demonstrate surprising performance in the false positive and false negative rate of the inferred graph in these challenging scenarios, and we provide theoretical insights into their performance. This work is also the first effort to benchmark the stability of causal discovery algorithms with respect to the values of their hyperparameters. Finally, we hope this paper will set a new standard for the evaluation of causal discovery methods and can serve as an accessible entry point for practitioners interested in the field, highlighting the empirical implications of different algorithm choices.         ",
    "url": "https://arxiv.org/abs/2310.13387",
    "authors": [
      "Francesco Montagna",
      "Atalanti A. Mastakouri",
      "Elias Eulig",
      "Nicoletta Noceti",
      "Lorenzo Rosasco",
      "Dominik Janzing",
      "Bryon Aragam",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.17380",
    "title": "2D and 3D Deep Learning Models for MRI-based Parkinson's Disease Classification: A Comparative Analysis of Convolutional Kolmogorov-Arnold Networks, Convolutional Neural Networks, and Graph Convolutional Networks",
    "abstract": "           Parkinson's Disease (PD) diagnosis remains challenging. This study applies Convolutional Kolmogorov-Arnold Networks (ConvKANs), integrating learnable spline-based activation functions into convolutional layers, for PD classification using structural MRI. The first 3D implementation of ConvKANs for medical imaging is presented, comparing their performance to Convolutional Neural Networks (CNNs) and Graph Convolutional Networks (GCNs) across three open-source datasets. Isolated analyses assessed performance within individual datasets, using cross-validation techniques. Holdout analyses evaluated cross-dataset generalizability by training models on two datasets and testing on the third, mirroring real-world clinical scenarios. In isolated analyses, 2D ConvKANs achieved the highest AUC of 0.99 (95% CI: 0.98-0.99) on the PPMI dataset, outperforming 2D CNNs (AUC: 0.97, p = 0.0092). 3D models showed promise, with 3D CNN and 3D ConvKAN reaching an AUC of 0.85 on PPMI. In holdout analyses, 3D ConvKAN demonstrated superior generalization, achieving an AUC of 0.85 on early-stage PD data. GCNs underperformed in 2D but improved in 3D implementations. These findings highlight ConvKANs' potential for PD detection, emphasize the importance of 3D analysis in capturing subtle brain changes, and underscore cross-dataset generalization challenges. This study advances AI-assisted PD diagnosis using structural MRI and emphasizes the need for larger-scale validation.         ",
    "url": "https://arxiv.org/abs/2407.17380",
    "authors": [
      "Salil B Patel",
      "Vicky Goh",
      "James F FitzGerald",
      "Chrystalina A Antoniades"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2408.13818",
    "title": "HER2 and FISH Status Prediction in Breast Biopsy H&E-Stained Images Using Deep Learning",
    "abstract": "           The current standard for detecting human epidermal growth factor receptor 2 (HER2) status in breast cancer patients relies on HER2 amplification, identified through fluorescence in situ hybridization (FISH) or immunohistochemistry (IHC). However, hematoxylin and eosin (H\\&E) tumor stains are more widely available, and accurately predicting HER2 status using H\\&E could reduce costs and expedite treatment selection. Deep Learning algorithms for H&E have shown effectiveness in predicting various cancer features and clinical outcomes, including moderate success in HER2 status prediction. In this work, we employed a customized weak supervision classification technique combined with MoCo-v2 contrastive learning to predict HER2 status. We trained our pipeline on 182 publicly available H&E Whole Slide Images (WSIs) from The Cancer Genome Atlas (TCGA), for which annotations by the pathology team at Yale School of Medicine are publicly available. Our pipeline achieved an Area Under the Curve (AUC) of 0.85 across four different test folds. Additionally, we tested our model on 44 H&E slides from the TCGA-BRCA dataset, which had an HER2 score of 2+ and included corresponding HER2 status and FISH test results. These cases are considered equivocal for IHC, requiring an expensive FISH test on their IHC slides for disambiguation. Our pipeline demonstrated an AUC of 0.81 on these challenging H&E slides. Reducing the need for FISH test can have significant implications in cancer treatment equity for underserved populations.         ",
    "url": "https://arxiv.org/abs/2408.13818",
    "authors": [
      "Ardhendu Sekhar",
      "Vrinda Goel",
      "Garima Jain",
      "Abhijeet Patil",
      "Ravi Kant Gupta",
      "Tripti Bameta",
      "Swapnil Rane",
      "Amit Sethi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.02457",
    "title": "On Oriented Diameter of Power Graphs",
    "abstract": "           In this paper, we study the oriented diameter of power graphs of groups. We show that a $2$-edge connected power graph of a finite group has oriented diameter at most $4$. We prove that the power graph of a cyclic group of order $n$ has oriented diameter $2$ for all $n\\neq 1,2,4,6$. For non-cyclic finite nilpotent groups, we show that the oriented diameter of corresponding power graphs is at least $3$. Moreover, we provide necessary and sufficient conditions for the oriented diameter of 2-edge connected power graphs of finite non-cyclic nilpotent groups to be either $3$ or $4$. This, in turn, gives an algorithm for computing the oriented diameter of the power graph of a given nilpotent group that runs in time polynomial in the size of the group.         ",
    "url": "https://arxiv.org/abs/2409.02457",
    "authors": [
      "Deepu Benson",
      "Bireswar Das",
      "Dipan Dey",
      "Jinia Ghosh"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2409.16106",
    "title": "Scenario of Use Scheme: Threat Model Specification for Speaker Privacy Protection in the Medical Domain",
    "abstract": "           Speech recordings are being more frequently used to detect and monitor disease, leading to privacy concerns. Beyond cryptography, protection of speech can be addressed by approaches, such as perturbation, disentanglement, and re-synthesis, that eliminate sensitive information of the speaker, leaving the information necessary for medical analysis purposes. In order for such privacy protective approaches to be developed, clear and systematic specifications of assumptions concerning medical settings and the needs of medical professionals are necessary. In this paper, we propose a Scenario of Use Scheme that incorporates an Attacker Model, which characterizes the adversary against whom the speaker's privacy must be defended, and a Protector Model, which specifies the defense. We discuss the connection of the scheme with previous work on speech privacy. Finally, we present a concrete example of a specified Scenario of Use and a set of experiments about protecting speaker data against gender inference attacks while maintaining utility for Parkinson's detection.         ",
    "url": "https://arxiv.org/abs/2409.16106",
    "authors": [
      "Mehtab Ur Rahman",
      "Martha Larson",
      "Louis ten Bosch",
      "Cristian Tejedor-Garc\u00eda"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Sound (cs.SD)"
    ]
  }
]