[
  {
    "id": "arXiv:2507.07134",
    "title": "BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks",
    "abstract": "           The pervasive issue of bias in AI presents a significant challenge to painting classification, and is getting more serious as these systems become increasingly integrated into tasks like art curation and restoration. Biases, often arising from imbalanced datasets where certain artistic styles dominate, compromise the fairness and accuracy of model predictions, i.e., classifiers are less accurate on rarely seen paintings. While prior research has made strides in improving classification performance, it has largely overlooked the critical need to address these underlying biases, that is, when dealing with out-of-distribution (OOD) data. Our insight highlights the necessity of a more robust approach to bias mitigation in AI models for art classification on biased training data. We propose a novel OOD-informed model bias adaptive sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It addresses these challenges by dynamically adjusting temperature scaling and sampling probabilities, thereby promoting a more equitable representation of all classes. We evaluate our proposed approach to the KaoKore and PACS datasets, focusing on the model's ability to reduce class-wise bias. We further propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to assess class-wise separation and per-class bias reduction. Our method demonstrates the ability to balance high performance with fairness, making it a robust solution for unbiasing AI models in the art domain.         ",
    "url": "https://arxiv.org/abs/2507.07134",
    "authors": [
      "Mridula Vijendran",
      "Shuang Chen",
      "Jingjing Deng",
      "Hubert P. H. Shum"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07138",
    "title": "GNNs Meet Sequence Models Along the Shortest-Path: an Expressive Method for Link Prediction",
    "abstract": "           Graph Neural Networks (GNNs) often struggle to capture the link-specific structural patterns crucial for accurate link prediction, as their node-centric message-passing schemes overlook the subgraph structures connecting a pair of nodes. Existing methods to inject such structural context either incur high computational cost or rely on simplistic heuristics (e.g., common neighbor counts) that fail to model multi-hop dependencies. We introduce SP4LP (Shortest Path for Link Prediction), a novel framework that combines GNN-based node encodings with sequence modeling over shortest paths. Specifically, SP4LP first applies a GNN to compute representations for all nodes, then extracts the shortest path between each candidate node pair and processes the resulting sequence of node embeddings using a sequence model. This design enables SP4LP to capture expressive multi-hop relational patterns with computational efficiency. Empirically, SP4LP achieves state-of-the-art performance across link prediction benchmarks. Theoretically, we prove that SP4LP is strictly more expressive than standard message-passing GNNs and several state-of-the-art structural features methods, establishing it as a general and principled approach for link prediction in graphs.         ",
    "url": "https://arxiv.org/abs/2507.07138",
    "authors": [
      "Francesco Ferrini",
      "Veronica Lachi",
      "Antonio Longa",
      "Bruno Lepri",
      "Andrea Passerini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07139",
    "title": "Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning",
    "abstract": "           Recent advances in image generation models (IGMs), particularly diffusion-based architectures such as Stable Diffusion (SD), have markedly enhanced the quality and diversity of AI-generated visual content. However, their generative capability has also raised significant ethical, legal, and societal concerns, including the potential to produce harmful, misleading, or copyright-infringing content. To mitigate these concerns, machine unlearning (MU) emerges as a promising solution by selectively removing undesirable concepts from pretrained models. Nevertheless, the robustness and effectiveness of existing unlearning techniques remain largely unexplored, particularly in the presence of multi-modal adversarial inputs. To bridge this gap, we propose Recall, a novel adversarial framework explicitly designed to compromise the robustness of unlearned IGMs. Unlike existing approaches that predominantly rely on adversarial text prompts, Recall exploits the intrinsic multi-modal conditioning capabilities of diffusion models by efficiently optimizing adversarial image prompts with guidance from a single semantically relevant reference image. Extensive experiments across ten state-of-the-art unlearning methods and diverse tasks show that Recall consistently outperforms existing baselines in terms of adversarial effectiveness, computational efficiency, and semantic fidelity with the original textual prompt. These findings reveal critical vulnerabilities in current unlearning mechanisms and underscore the need for more robust solutions to ensure the safety and reliability of generative models. Code and data are publicly available at \\textcolor{blue}{this https URL}.         ",
    "url": "https://arxiv.org/abs/2507.07139",
    "authors": [
      "Renyang Liu",
      "Guanlin Li",
      "Tianwei Zhang",
      "See-Kiong Ng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07141",
    "title": "Str-GCL: Structural Commonsense Driven Graph Contrastive Learning",
    "abstract": "           Graph Contrastive Learning (GCL) is a widely adopted approach in self-supervised graph representation learning, applying contrastive objectives to produce effective representations. However, current GCL methods primarily focus on capturing implicit semantic relationships, often overlooking the structural commonsense embedded within the graph's structure and attributes, which contains underlying knowledge crucial for effective representation learning. Due to the lack of explicit information and clear guidance in general graph, identifying and integrating such structural commonsense in GCL poses a significant challenge. To address this gap, we propose a novel framework called Structural Commonsense Unveiling in Graph Contrastive Learning (Str-GCL). Str-GCL leverages first-order logic rules to represent structural commonsense and explicitly integrates them into the GCL framework. It introduces topological and attribute-based rules without altering the original graph and employs a representation alignment mechanism to guide the encoder in effectively capturing this commonsense. To the best of our knowledge, this is the first attempt to directly incorporate structural commonsense into GCL. Extensive experiments demonstrate that Str-GCL outperforms existing GCL methods, providing a new perspective on leveraging structural commonsense in graph representation learning.         ",
    "url": "https://arxiv.org/abs/2507.07141",
    "authors": [
      "Dongxiao He",
      "Yongqi Huang",
      "Jitao Zhao",
      "Xiaobao Wang",
      "Zhen Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07144",
    "title": "M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure",
    "abstract": "           As cloud services become increasingly integral to modern IT infrastructure, ensuring hardware reliability is essential to sustain high-quality service. Memory failures pose a significant threat to overall system stability, making accurate failure prediction through the analysis of memory error logs (i.e., Correctable Errors) imperative. Existing memory failure prediction approaches have notable limitations: rule-based expert models suffer from limited generalizability and low recall rates, while automated feature extraction methods exhibit suboptimal performance. To address these limitations, we propose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction framework designed to enhance the reliability and availability of cloud infrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level binary matrix representations and introduces a Binary Spatial Feature Extractor (BSFE) to automatically extract high-order features at both DIMM-level and bit-level. Building upon the BSFE outputs, we develop a dual-path temporal modeling architecture: 1) a time-patch module that aggregates multi-level features within observation windows, and 2) a time-point module that employs interpretable rule-generation trees trained on bit-level patterns. Experiments on both benchmark datasets and real-world deployment show the superiority of M$^2$-MFP as it outperforms existing state-of-the-art methods by significant margins. Code and data are available at this repository: this https URL.         ",
    "url": "https://arxiv.org/abs/2507.07144",
    "authors": [
      "Hongyi Xie",
      "Min Zhou",
      "Qiao Yu",
      "Jialiang Yu",
      "Zhenli Sheng",
      "Hong Xie",
      "Defu Lian"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2507.07145",
    "title": "CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs",
    "abstract": "           The rapid scaling of Large Language Models (LLMs) elevates inference costs and compounds substantial deployment barriers. While quantization to 8 or 4 bits mitigates this, sub-3-bit methods face severe accuracy, scalability, and efficiency degradation. We propose Convolutional Code Quantization (CCQ), an inference-optimized quantization approach compressing LLMs to 2.0-2.75 bits with minimal accuracy loss. Departing from error-prone scalar quantization or slow vector quantization, CCQ integrates a hardware-aware bit-shift encoding and decoding solution with Convolutional Code, Hybrid Encoding, and Code Cluster, jointly overcoming accuracy-speed bottlenecks. We construct a lookup-free encoding space, enabling a linear mapping between the codebook and weight vectors, thereby optimizing inference performance. Meanwhile, by drawing on the concept of data mapping from vector quantization, we minimize the performance degradation of the model under extremely low-bit conditions. Experiments demonstrate that CCQ achieves outstanding performance on LLMs across various benchmarks. We compress DeepSeek-V3 (671B total parameters) to 184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE 4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B model and inference engine have been open-sourced.         ",
    "url": "https://arxiv.org/abs/2507.07145",
    "authors": [
      "Zhaojing Zhou",
      "Xunchao Li",
      "Minghao Li",
      "Handi Zhang",
      "Haoshuang Wang",
      "Wenbin Chang",
      "Yiqun Liu",
      "Qingqing Dang",
      "Dianhai Yu",
      "Yanjun Ma",
      "Haifeng Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07151",
    "title": "Robust Multimodal Large Language Models Against Modality Conflict",
    "abstract": "           Despite the impressive capabilities of multimodal large language models (MLLMs) in vision-language tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in a dilemma and directly lead to hallucinations. We formally define the modality conflict and construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised fine-tuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs.         ",
    "url": "https://arxiv.org/abs/2507.07151",
    "authors": [
      "Zongmeng Zhang",
      "Wengang Zhou",
      "Jie Zhao",
      "Houqiang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.07153",
    "title": "Aerial Maritime Vessel Detection and Identification",
    "abstract": "           Autonomous maritime surveillance and target vessel identification in environments where Global Navigation Satellite Systems (GNSS) are not available is critical for a number of applications such as search and rescue and threat detection. When the target vessel is only described by visual cues and its last known position is not available, unmanned aerial vehicles (UAVs) must rely solely on on-board vision to scan a large search area under strict computational constraints. To address this challenge, we leverage the YOLOv8 object detection model to detect all vessels in the field of view. We then apply feature matching and hue histogram distance analysis to determine whether any detected vessel corresponds to the target. When found, we localize the target using simple geometric principles. We demonstrate the proposed method in real-world experiments during the MBZIRC2023 competition, integrated into a fully autonomous system with GNSS-denied navigation. We also evaluate the impact of perspective on detection accuracy and localization precision and compare it with the oracle approach.         ",
    "url": "https://arxiv.org/abs/2507.07153",
    "authors": [
      "Antonella Barisic Kulas",
      "Frano Petric",
      "Stjepan Bogdan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.07154",
    "title": "CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation",
    "abstract": "           Accurate segmentation of polyps from colonoscopy images is crucial for the early diagnosis and treatment of colorectal cancer. Most existing deep learning-based polyp segmentation methods adopt an Encoder-Decoder architecture, and some utilize multi-task frameworks that incorporate auxiliary tasks such as classification to enhance segmentation performance. However, these approaches often require additional labeled data and rely on task similarity, which can limit their generalizability. To address these challenges, we propose CL-Polyp, a contrastive learning-enhanced polyp segmentation network. Our method leverages contrastive learning to improve the encoder's ability to extract discriminative features by contrasting positive and negative sample pairs derived from polyp images. This self-supervised strategy enhances visual representation without requiring additional annotations. In addition, we introduce two lightweight and effective modules: the Modified Atrous Spatial Pyramid Pooling (MASPP) module for better multi-scale feature fusion, and the Channel Concatenate and Element Add (CA) module to fuse low-level and upsampled features for improved boundary reconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG, CVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp consistently outperforms state-of-the-art methods. Specifically, it improves the IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets, respectively, validating its effectiveness in clinical polyp segmentation tasks.         ",
    "url": "https://arxiv.org/abs/2507.07154",
    "authors": [
      "Desheng Li",
      "Chaoliang Liu",
      "Zhiyong Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07197",
    "title": "Combining Pre-Trained Models for Enhanced Feature Representation in Reinforcement Learning",
    "abstract": "           The recent focus and release of pre-trained models have been a key components to several advancements in many fields (e.g. Natural Language Processing and Computer Vision), as a matter of fact, pre-trained models learn disparate latent embeddings sharing insightful representations. On the other hand, Reinforcement Learning (RL) focuses on maximizing the cumulative reward obtained via agent's interaction with the environment. RL agents do not have any prior knowledge about the world, and they either learn from scratch an end-to-end mapping between the observation and action spaces or, in more recent works, are paired with monolithic and computationally expensive Foundational Models. How to effectively combine and leverage the hidden information of different pre-trained models simultaneously in RL is still an open and understudied question. In this work, we propose Weight Sharing Attention (WSA), a new architecture to combine embeddings of multiple pre-trained models to shape an enriched state representation, balancing the tradeoff between efficiency and performance. We run an extensive comparison between several combination modes showing that WSA obtains comparable performance on multiple Atari games compared to end-to-end models. Furthermore, we study the generalization capabilities of this approach and analyze how scaling the number of models influences agents' performance during and after training.         ",
    "url": "https://arxiv.org/abs/2507.07197",
    "authors": [
      "Elia Piccoli",
      "Malio Li",
      "Giacomo Carf\u00ec",
      "Vincenzo Lomonaco",
      "Davide Bacciu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07216",
    "title": "Bias-Aware Mislabeling Detection via Decoupled Confident Learning",
    "abstract": "           Reliable data is a cornerstone of modern organizational systems. A notable data integrity challenge stems from label bias, which refers to systematic errors in a label, a covariate that is central to a quantitative analysis, such that its quality differs across social groups. This type of bias has been conceptually and empirically explored and is widely recognized as a pressing issue across critical domains. However, effective methodologies for addressing it remain scarce. In this work, we propose Decoupled Confident Learning (DeCoLe), a principled machine learning based framework specifically designed to detect mislabeled instances in datasets affected by label bias, enabling bias aware mislabelling detection and facilitating data quality improvement. We theoretically justify the effectiveness of DeCoLe and evaluate its performance in the impactful context of hate speech detection, a domain where label bias is a well documented challenge. Empirical results demonstrate that DeCoLe excels at bias aware mislabeling detection, consistently outperforming alternative approaches for label error detection. Our work identifies and addresses the challenge of bias aware mislabeling detection and offers guidance on how DeCoLe can be integrated into organizational data management practices as a powerful tool to enhance data reliability.         ",
    "url": "https://arxiv.org/abs/2507.07216",
    "authors": [
      "Yunyi Li",
      "Maria De-Arteaga",
      "Maytal Saar-Tsechansky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2507.07237",
    "title": "Towards Robust Surrogate Models: Benchmarking Machine Learning Approaches to Expediting Phase Field Simulations of Brittle Fracture",
    "abstract": "           Data driven approaches have the potential to make modeling complex, nonlinear physical phenomena significantly more computationally tractable. For example, computational modeling of fracture is a core challenge where machine learning techniques have the potential to provide a much needed speedup that would enable progress in areas such as mutli-scale modeling and uncertainty quantification. Currently, phase field modeling (PFM) of fracture is one such approach that offers a convenient variational formulation to model crack nucleation, branching and propagation. To date, machine learning techniques have shown promise in approximating PFM simulations. However, most studies rely on overly simple benchmarks that do not reflect the true complexity of the fracture processes where PFM excels as a method. To address this gap, we introduce a challenging dataset based on PFM simulations designed to benchmark and advance ML methods for fracture modeling. This dataset includes three energy decomposition methods, two boundary conditions, and 1,000 random initial crack configurations for a total of 6,000 simulations. Each sample contains 100 time steps capturing the temporal evolution of the crack field. Alongside this dataset, we also implement and evaluate Physics Informed Neural Networks (PINN), Fourier Neural Operators (FNO) and UNet models as baselines, and explore the impact of ensembling strategies on prediction accuracy. With this combination of our dataset and baseline models drawn from the literature we aim to provide a standardized and challenging benchmark for evaluating machine learning approaches to solid mechanics. Our results highlight both the promise and limitations of popular current models, and demonstrate the utility of this dataset as a testbed for advancing machine learning in fracture mechanics research.         ",
    "url": "https://arxiv.org/abs/2507.07237",
    "authors": [
      "Erfan Hamdi",
      "Emma Lejeune"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2507.07244",
    "title": "Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis",
    "abstract": "           In the ever-evolving landscape of cybersecurity, the rapid identification and mitigation of Advanced Persistent Threats (APTs) is crucial. Security practitioners rely on detailed threat reports to understand the tactics, techniques, and procedures (TTPs) employed by attackers. However, manually extracting attack testflows from these reports requires elusive knowledge and is time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a novel solution leveraging language models (i.e., BERT) and Natural Language Processing (NLP) techniques to automate the extraction of attack testflows from unstructured threat reports. FLOWGUARDIAN systematically analyzes and contextualizes security events, reconstructs attack sequences, and then generates comprehensive testflows. This automated approach not only saves time and reduces human error but also ensures comprehensive coverage and robustness in cybersecurity testing. Empirical validation using public threat reports demonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing the capabilities of security teams in proactive threat hunting and incident response.         ",
    "url": "https://arxiv.org/abs/2507.07244",
    "authors": [
      "Faissal Ahmadou",
      "Sepehr Ghaffarzadegan",
      "Boubakr Nour",
      "Makan Pourzandi",
      "Mourad Debbabi",
      "Chadi Assi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2507.07258",
    "title": "FedP3E: Privacy-Preserving Prototype Exchange for Non-IID IoT Malware Detection in Cross-Silo Federated Learning",
    "abstract": "           As IoT ecosystems continue to expand across critical sectors, they have become prominent targets for increasingly sophisticated and large-scale malware attacks. The evolving threat landscape, combined with the sensitive nature of IoT-generated data, demands detection frameworks that are both privacy-preserving and resilient to data heterogeneity. Federated Learning (FL) offers a promising solution by enabling decentralized model training without exposing raw data. However, standard FL algorithms such as FedAvg and FedProx often fall short in real-world deployments characterized by class imbalance and non-IID data distributions -- particularly in the presence of rare or disjoint malware classes. To address these challenges, we propose FedP3E (Privacy-Preserving Prototype Exchange), a novel FL framework that supports indirect cross-client representation sharing while maintaining data privacy. Each client constructs class-wise prototypes using Gaussian Mixture Models (GMMs), perturbs them with Gaussian noise, and transmits only these compact summaries to the server. The aggregated prototypes are then distributed back to clients and integrated into local training, supported by SMOTE-based augmentation to enhance representation of minority malware classes. Rather than relying solely on parameter averaging, our prototype-driven mechanism enables clients to enrich their local models with complementary structural patterns observed across the federation -- without exchanging raw data or gradients. This targeted strategy reduces the adverse impact of statistical heterogeneity with minimal communication overhead. We evaluate FedP3E on the N-BaIoT dataset under realistic cross-silo scenarios with varying degrees of data imbalance.         ",
    "url": "https://arxiv.org/abs/2507.07258",
    "authors": [
      "Rami Darwish",
      "Mahmoud Abdelsalam",
      "Sajad Khorsandroo",
      "Kaushik Roy"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07259",
    "title": "Exploiting Edge Features for Transferable Adversarial Attacks in Distributed Machine Learning",
    "abstract": "           As machine learning models become increasingly deployed across the edge of internet of things environments, a partitioned deep learning paradigm in which models are split across multiple computational nodes introduces a new dimension of security risk. Unlike traditional inference setups, these distributed pipelines span the model computation across heterogeneous nodes and communication layers, thereby exposing a broader attack surface to potential adversaries. Building on these motivations, this work explores a previously overlooked vulnerability: even when both the edge and cloud components of the model are inaccessible (i.e., black-box), an adversary who intercepts the intermediate features transmitted between them can still pose a serious threat. We demonstrate that, under these mild and realistic assumptions, an attacker can craft highly transferable proxy models, making the entire deep learning system significantly more vulnerable to evasion attacks. In particular, the intercepted features can be effectively analyzed and leveraged to distill surrogate models capable of crafting highly transferable adversarial examples against the target model. To this end, we propose an exploitation strategy specifically designed for distributed settings, which involves reconstructing the original tensor shape from vectorized transmitted features using simple statistical analysis, and adapting surrogate architectures accordingly to enable effective feature distillation. A comprehensive and systematic experimental evaluation has been conducted to demonstrate that surrogate models trained with the proposed strategy, i.e., leveraging intermediate features, tremendously improve the transferability of adversarial attacks. These findings underscore the urgent need to account for intermediate feature leakage in the design of secure distributed deep learning systems.         ",
    "url": "https://arxiv.org/abs/2507.07259",
    "authors": [
      "Giulio Rossolini",
      "Fabio Brau",
      "Alessandro Biondi",
      "Battista Biggio",
      "Giorgio Buttazzo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07261",
    "title": "Robust Multimodal Learning Framework For Intake Gesture Detection Using Contactless Radar and Wearable IMU Sensors",
    "abstract": "           Automated food intake gesture detection plays a vital role in dietary monitoring, enabling objective and continuous tracking of eating behaviors to support better health outcomes. Wrist-worn inertial measurement units (IMUs) have been widely used for this task with promising results. More recently, contactless radar sensors have also shown potential. This study explores whether combining wearable and contactless sensing modalities through multimodal learning can further improve detection performance. We also address a major challenge in multimodal learning: reduced robustness when one modality is missing. To this end, we propose a robust multimodal temporal convolutional network with cross-modal attention (MM-TCN-CMA), designed to integrate IMU and radar data, enhance gesture detection, and maintain performance under missing modality conditions. A new dataset comprising 52 meal sessions (3,050 eating gestures and 797 drinking gestures) from 52 participants is developed and made publicly available. Experimental results show that the proposed framework improves the segmental F1-score by 4.3% and 5.2% over unimodal Radar and IMU models, respectively. Under missing modality scenarios, the framework still achieves gains of 1.3% and 2.4% for missing radar and missing IMU inputs. This is the first study to demonstrate a robust multimodal learning framework that effectively fuses IMU and radar data for food intake gesture detection.         ",
    "url": "https://arxiv.org/abs/2507.07261",
    "authors": [
      "Chunzhuo Wang",
      "Hans Hallez",
      "Bart Vanrumste"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2507.07270",
    "title": "Audio-Visual Speech Separation via Bottleneck Iterative Network",
    "abstract": "           Integration of information from non-auditory cues can significantly improve the performance of speech-separation models. Often such models use deep modality-specific networks to obtain unimodal features, and risk being too costly or lightweight but lacking capacity. In this work, we present an iterative representation refinement approach called Bottleneck Iterative Network (BIN), a technique that repeatedly progresses through a lightweight fusion block, while bottlenecking fusion representations by fusion tokens. This helps improve the capacity of the model, while avoiding major increase in model size and balancing between the model performance and training cost. We test BIN on challenging noisy audio-visual speech separation tasks, and show that our approach consistently outperforms state-of-the-art benchmark models with respect to SI-SDRi on NTCD-TIMIT and LRS3+WHAM! datasets, while simultaneously achieving a reduction of more than 50% in training and GPU inference time across nearly all settings.         ",
    "url": "https://arxiv.org/abs/2507.07270",
    "authors": [
      "Sidong Zhang",
      "Shiv Shankar",
      "Trang Nguyen",
      "Andrea Fanelli",
      "Madalina Fiterau"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2507.07280",
    "title": "The Impact of Background Speech on Interruption Detection in Collaborative Groups",
    "abstract": "           Interruption plays a crucial role in collaborative learning, shaping group interactions and influencing knowledge construction. AI-driven support can assist teachers in monitoring these interactions. However, most previous work on interruption detection and interpretation has been conducted in single-conversation environments with relatively clean audio. AI agents deployed in classrooms for collaborative learning within small groups will need to contend with multiple concurrent conversations -- in this context, overlapping speech will be ubiquitous, and interruptions will need to be identified in other ways. In this work, we analyze interruption detection in single-conversation and multi-group dialogue settings. We then create a state-of-the-art method for interruption identification that is robust to overlapping speech, and thus could be deployed in classrooms. Further, our work highlights meaningful linguistic and prosodic information about how interruptions manifest in collaborative group interactions. Our investigation also paves the way for future works to account for the influence of overlapping speech from multiple groups when tracking group dialog.         ",
    "url": "https://arxiv.org/abs/2507.07280",
    "authors": [
      "Mariah Bradford",
      "Nikhil Krishnaswamy",
      "Nathaniel Blanchard"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.07284",
    "title": "A Robust, Open-Source Framework for Spiking Neural Networks on Low-End FPGAs",
    "abstract": "           As the demand for compute power in traditional neural networks has increased significantly, spiking neural networks (SNNs) have emerged as a potential solution to increasingly power-hungry neural networks. By operating on 0/1 spikes emitted by neurons instead of arithmetic multiply-and-accumulate operations, SNNs propagate information temporally and spatially, allowing for more efficient compute power. To this end, many architectures for accelerating and simulating SNNs have been developed, including Loihi, TrueNorth, and SpiNNaker. However, these chips are largely inaccessible to the wider community. Field programmable gate arrays (FPGAs) have been explored to serve as a middle ground between neuromorphic and non-neuromorphic hardware, but many proposed architectures require expensive high-end FPGAs or target a single SNN topology. This paper presents a framework consisting of a robust SNN acceleration architecture and a Pytorch-based SNN model compiler. Targeting any-to-any and/or fully connected SNNs, the FPGA architecture features a synaptic array that tiles across the SNN to propagate spikes. The architecture targets low-end FPGAs and requires very little (6358 LUT, 40.5 BRAM) resources. The framework, tested on a low-end Xilinx Artix-7 FPGA at 100 MHz, achieves competitive speed in recognizing MNIST digits (0.52 ms/img). Further experiments also show accurate simulation of hand coded any-to-any spiking neural networks on toy problems. All code and setup instructions are available at this https URL}{\\texttt{this https URL.         ",
    "url": "https://arxiv.org/abs/2507.07284",
    "authors": [
      "Andrew Fan",
      "Simon D. Levy"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2507.07288",
    "title": "Natural Evolutionary Search meets Probabilistic Numerics",
    "abstract": "           Zeroth-order local optimisation algorithms are essential for solving real-valued black-box optimisation problems. Among these, Natural Evolution Strategies (NES) represent a prominent class, particularly well-suited for scenarios where prior distributions are available. By optimising the objective function in the space of search distributions, NES algorithms naturally integrate prior knowledge during initialisation, making them effective in settings such as semi-supervised learning and user-prior belief frameworks. However, due to their reliance on random sampling and Monte Carlo estimates, NES algorithms can suffer from limited sample efficiency. In this paper, we introduce a novel class of algorithms, termed Probabilistic Natural Evolutionary Strategy Algorithms (ProbNES), which enhance the NES framework with Bayesian quadrature. We show that ProbNES algorithms consistently outperforms their non-probabilistic counterparts as well as global sample efficient methods such as Bayesian Optimisation (BO) or $\\pi$BO across a wide range of tasks, including benchmark test functions, data-driven optimisation tasks, user-informed hyperparameter tuning tasks and locomotion tasks.         ",
    "url": "https://arxiv.org/abs/2507.07288",
    "authors": [
      "Pierre Osselin",
      "Masaki Adachi",
      "Xiaowen Dong",
      "Michael A. Osborne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07298",
    "title": "Multilayer GNN for Predictive Maintenance and Clustering in Power Grids",
    "abstract": "           Unplanned power outages cost the US economy over $150 billion annually, partly due to predictive maintenance (PdM) models that overlook spatial, temporal, and causal dependencies in grid failures. This study introduces a multilayer Graph Neural Network (GNN) framework to enhance PdM and enable resilience-based substation clustering. Using seven years of incident data from Oklahoma Gas & Electric (292,830 records across 347 substations), the framework integrates Graph Attention Networks (spatial), Graph Convolutional Networks (temporal), and Graph Isomorphism Networks (causal), fused through attention-weighted embeddings. Our model achieves a 30-day F1-score of 0.8935 +/- 0.0258, outperforming XGBoost and Random Forest by 3.2% and 2.7%, and single-layer GNNs by 10 to 15 percent. Removing the causal layer drops performance to 0.7354 +/- 0.0418. For resilience analysis, HDBSCAN clustering on HierarchicalRiskGNN embeddings identifies eight operational risk groups. The highest-risk cluster (Cluster 5, 44 substations) shows 388.4 incidents/year and 602.6-minute recovery time, while low-risk groups report fewer than 62 incidents/year. ANOVA (p < 0.0001) confirms significant inter-cluster separation. Our clustering outperforms K-Means and Spectral Clustering with a Silhouette Score of 0.626 and Davies-Bouldin index of 0.527. This work supports proactive grid management through improved failure prediction and risk-aware substation clustering.         ",
    "url": "https://arxiv.org/abs/2507.07298",
    "authors": [
      "Muhammad Kazim",
      "Harun Pirim",
      "Chau Le",
      "Trung Le",
      "Om Prakash Yadav"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07319",
    "title": "Probability-Raising Causality for Uncertain Parametric Markov Decision Processes with PAC Guarantees",
    "abstract": "           Recent decision-making systems are increasingly complicated, making it crucial to verify and understand their behavior for a given specification. A promising approach is to comprehensively explain undesired behavior in the systems modeled by Markov decision processes (MDPs) through formal verification and causal reasoning. However, the reliable explanation using model-based probabilistic causal analysis has not been explored when the MDP's transition probabilities are uncertain. This paper proposes a method to identify potential causes of undesired behaviors in an uncertain parametric MDP (upMDP) using parameter sampling, model checking, and a set covering for the samples. A cause is defined as a subset of states based on a probability-raising principle. We show that the probability of each identified subset being a cause exceeds a specified threshold. Further, a lower bound of the probability that the undesired paths visit the subsets is maximized as much as possible while satisfying a nonredundancy condition. While computing these probabilities is complicated, this study derives probabilistically approximately correct lower bounds of both probabilities by the sampling. We demonstrate the effectiveness of the proposed method through a path-planning scenario.         ",
    "url": "https://arxiv.org/abs/2507.07319",
    "authors": [
      "Ryohei Oura",
      "yuji Ito"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.07320",
    "title": "Optimizing Communication and Device Clustering for Clustered Federated Learning with Differential Privacy",
    "abstract": "           In this paper, a secure and communication-efficient clustered federated learning (CFL) design is proposed. In our model, several base stations (BSs) with heterogeneous task-handling capabilities and multiple users with non-independent and identically distributed (non-IID) data jointly perform CFL training incorporating differential privacy (DP) techniques. Since each BS can process only a subset of the learning tasks and has limited wireless resource blocks (RBs) to allocate to users for federated learning (FL) model parameter transmission, it is necessary to jointly optimize RB allocation and user scheduling for CFL performance optimization. Meanwhile, our considered CFL method requires devices to use their limited data and FL model information to determine their task identities, which may introduce additional communication overhead. We formulate an optimization problem whose goal is to minimize the training loss of all learning tasks while considering device clustering, RB allocation, DP noise, and FL model transmission delay. To solve the problem, we propose a novel dynamic penalty function assisted value decomposed multi-agent reinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to independently determine their connected users, RBs, and DP noise of the connected users but jointly minimize the training loss of all learning tasks across all BSs. Different from the existing MARL methods that assign a large penalty for invalid actions, we propose a novel penalty assignment scheme that assigns penalty depending on the number of devices that cannot meet communication constraints (e.g., delay), which can guide the MARL scheme to quickly find valid actions, thus improving the convergence speed. Simulation results show that the DPVD-MARL can improve the convergence rate by up to 20% and the ultimate accumulated rewards by 15% compared to independent Q-learning.         ",
    "url": "https://arxiv.org/abs/2507.07320",
    "authors": [
      "Dongyu Wei",
      "Xiaoren Xu",
      "Shiwen Mao",
      "Mingzhe Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07335",
    "title": "Leveraging Manifold Embeddings for Enhanced Graph Transformer Representations and Learning",
    "abstract": "           Graph transformers typically embed every node in a single Euclidean space, blurring heterogeneous topologies. We prepend a lightweight Riemannian mixture-of-experts layer that routes each node to various kinds of manifold, mixture of spherical, flat, hyperbolic - best matching its local structure. These projections provide intrinsic geometric explanations to the latent space. Inserted into a state-of-the-art ensemble graph transformer, this projector lifts accuracy by up to 3% on four node-classification benchmarks. The ensemble makes sure that both euclidean and non-euclidean features are captured. Explicit, geometry-aware projection thus sharpens predictive power while making graph representations more interpretable.         ",
    "url": "https://arxiv.org/abs/2507.07335",
    "authors": [
      "Ankit Jyothish",
      "Ali Jannesari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07359",
    "title": "Goal-Oriented Sequential Bayesian Experimental Design for Causal Learning",
    "abstract": "           We present GO-CBED, a goal-oriented Bayesian framework for sequential causal experimental design. Unlike conventional approaches that select interventions aimed at inferring the full causal model, GO-CBED directly maximizes the expected information gain (EIG) on user-specified causal quantities of interest, enabling more targeted and efficient experimentation. The framework is both non-myopic, optimizing over entire intervention sequences, and goal-oriented, targeting only model aspects relevant to the causal query. To address the intractability of exact EIG computation, we introduce a variational lower bound estimator, optimized jointly through a transformer-based policy network and normalizing flow-based variational posteriors. The resulting policy enables real-time decision-making via an amortized network. We demonstrate that GO-CBED consistently outperforms existing baselines across various causal reasoning and discovery tasks-including synthetic structural causal models and semi-synthetic gene regulatory networks-particularly in settings with limited experimental budgets and complex causal mechanisms. Our results highlight the benefits of aligning experimental design objectives with specific research goals and of forward-looking sequential planning.         ",
    "url": "https://arxiv.org/abs/2507.07359",
    "authors": [
      "Zheyu Zhang",
      "Jiayuan Dong",
      "Jie Liu",
      "Xun Huan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2507.07373",
    "title": "Atherosclerosis through Hierarchical Explainable Neural Network Analysis",
    "abstract": "           In this work, we study the problem pertaining to personalized classification of subclinical atherosclerosis by developing a hierarchical graph neural network framework to leverage two characteristic modalities of a patient: clinical features within the context of the cohort, and molecular data unique to individual patients. Current graph-based methods for disease classification detect patient-specific molecular fingerprints, but lack consistency and comprehension regarding cohort-wide features, which are an essential requirement for understanding pathogenic phenotypes across diverse atherosclerotic trajectories. Furthermore, understanding patient subtypes often considers clinical feature similarity in isolation, without integration of shared pathogenic interdependencies among patients. To address these challenges, we introduce ATHENA: Atherosclerosis Through Hierarchical Explainable Neural Network Analysis, which constructs a novel hierarchical network representation through integrated modality learning; subsequently, it optimizes learned patient-specific molecular fingerprints that reflect individual omics data, enforcing consistency with cohort-wide patterns. With a primary clinical dataset of 391 patients, we demonstrate that this heterogeneous alignment of clinical features with molecular interaction patterns has significantly boosted subclinical atherosclerosis classification performance across various baselines by up to 13% in area under the receiver operating curve (AUC) and 20% in F1 score. Taken together, ATHENA enables mechanistically-informed patient subtype discovery through explainable AI (XAI)-driven subnetwork clustering; this novel integration framework strengthens personalized intervention strategies, thereby improving the prediction of atherosclerotic disease progression and management of their clinical actionable outcomes.         ",
    "url": "https://arxiv.org/abs/2507.07373",
    "authors": [
      "Irsyad Adam",
      "Steven Swee",
      "Erika Yilin",
      "Ethan Ji",
      "William Speier",
      "Dean Wang",
      "Alex Bui",
      "Wei Wang",
      "Karol Watson",
      "Peipei Ping"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07388",
    "title": "GRIT: Graph Transformer For Internal Ice Layer Thickness Prediction",
    "abstract": "           Gaining a deeper understanding of the thickness and variability of internal ice layers in Radar imagery is essential in monitoring the snow accumulation, better evaluating ice dynamics processes, and minimizing uncertainties in climate models. Radar sensors, capable of penetrating ice, capture detailed radargram images of internal ice layers. In this work, we introduce GRIT, graph transformer for ice layer thickness. GRIT integrates an inductive geometric graph learning framework with an attention mechanism, designed to map the relationships between shallow and deeper ice layers. Compared to baseline graph neural networks, GRIT demonstrates consistently lower prediction errors. These results highlight the attention mechanism's effectiveness in capturing temporal changes across ice layers, while the graph transformer combines the strengths of transformers for learning long-range dependencies with graph neural networks for capturing spatial patterns, enabling robust modeling of complex spatiotemporal dynamics.         ",
    "url": "https://arxiv.org/abs/2507.07388",
    "authors": [
      "Zesheng Liu",
      "Maryam Rahnemoonfar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07389",
    "title": "ST-GRIT: Spatio-Temporal Graph Transformer For Internal Ice Layer Thickness Prediction",
    "abstract": "           Understanding the thickness and variability of internal ice layers in radar imagery is crucial for monitoring snow accumulation, assessing ice dynamics, and reducing uncertainties in climate models. Radar sensors, capable of penetrating ice, provide detailed radargram images of these internal layers. In this work, we present ST-GRIT, a spatio-temporal graph transformer for ice layer thickness, designed to process these radargrams and capture the spatiotemporal relationships between shallow and deep ice layers. ST-GRIT leverages an inductive geometric graph learning framework to extract local spatial features as feature embeddings and employs a series of temporal and spatial attention blocks separately to model long-range dependencies effectively in both dimensions. Experimental evaluation on radargram data from the Greenland ice sheet demonstrates that ST-GRIT consistently outperforms current state-of-the-art methods and other baseline graph neural networks by achieving lower root mean-squared error. These results highlight the advantages of self-attention mechanisms on graphs over pure graph neural networks, including the ability to handle noise, avoid oversmoothing, and capture long-range dependencies. Moreover, the use of separate spatial and temporal attention blocks allows for distinct and robust learning of spatial relationships and temporal patterns, providing a more comprehensive and effective approach.         ",
    "url": "https://arxiv.org/abs/2507.07389",
    "authors": [
      "Zesheng Liu",
      "Maryam Rahnemoonfar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07393",
    "title": "KeyRe-ID: Keypoint-Guided Person Re-Identification using Part-Aware Representation in Videos",
    "abstract": "           We propose \\textbf{KeyRe-ID}, a keypoint-guided video-based person re-identification framework consisting of global and local branches that leverage human keypoints for enhanced spatiotemporal representation learning. The global branch captures holistic identity semantics through Transformer-based temporal aggregation, while the local branch dynamically segments body regions based on keypoints to generate fine-grained, part-aware features. Extensive experiments on MARS and iLIDS-VID benchmarks demonstrate state-of-the-art performance, achieving 91.73\\% mAP and 97.32\\% Rank-1 accuracy on MARS, and 96.00\\% Rank-1 and 100.0\\% Rank-5 accuracy on iLIDS-VID. The code for this work will be publicly available on GitHub upon publication.         ",
    "url": "https://arxiv.org/abs/2507.07393",
    "authors": [
      "Jinseong Kim",
      "Junghoon Song",
      "Gyeongseon Baek",
      "Byeongjoon Noh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07405",
    "title": "HGMP:Heterogeneous Graph Multi-Task Prompt Learning",
    "abstract": "           The pre-training and fine-tuning methods have gained widespread attention in the field of heterogeneous graph neural networks due to their ability to leverage large amounts of unlabeled data during the pre-training phase, allowing the model to learn rich structural features. However, these methods face the issue of a mismatch between the pre-trained model and downstream tasks, leading to suboptimal performance in certain application scenarios. Prompt learning methods have emerged as a new direction in heterogeneous graph tasks, as they allow flexible adaptation of task representations to address target inconsistency. Building on this idea, this paper proposes a novel multi-task prompt framework for the heterogeneous graph domain, named HGMP. First, to bridge the gap between the pre-trained model and downstream tasks, we reformulate all downstream tasks into a unified graph-level task format. Next, we address the limitations of existing graph prompt learning methods, which struggle to integrate contrastive pre-training strategies in the heterogeneous graph domain. We design a graph-level contrastive pre-training strategy to better leverage heterogeneous information and enhance performance in multi-task scenarios. Finally, we introduce heterogeneous feature prompts, which enhance model performance by refining the representation of input graph features. Experimental results on public datasets show that our proposed method adapts well to various tasks and significantly outperforms baseline methods.         ",
    "url": "https://arxiv.org/abs/2507.07405",
    "authors": [
      "Pengfei Jiao",
      "Jialong Ni",
      "Di Jin",
      "Xuan Guo",
      "Huan Liu",
      "Hongjiang Chen",
      "Yanxian Bi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07406",
    "title": "Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models",
    "abstract": "           Phishing attacks are becoming increasingly sophisticated, underscoring the need for detection systems that strike a balance between high accuracy and computational efficiency. This paper presents a comparative evaluation of traditional Machine Learning (ML), Deep Learning (DL), and quantized small-parameter Large Language Models (LLMs) for phishing detection. Through experiments on a curated dataset, we show that while LLMs currently underperform compared to ML and DL methods in terms of raw accuracy, they exhibit strong potential for identifying subtle, context-based phishing cues. We also investigate the impact of zero-shot and few-shot prompting strategies, revealing that LLM-rephrased emails can significantly degrade the performance of both ML and LLM-based detectors. Our benchmarking highlights that models like DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above 80%, using only 17GB of VRAM, supporting their viability for cost-efficient deployment. We further assess the models' adversarial robustness and cost-performance tradeoffs, and demonstrate how lightweight LLMs can provide concise, interpretable explanations to support real-time decision-making. These findings position optimized LLMs as promising components in phishing defence systems and offer a path forward for integrating explainable, efficient AI into modern cybersecurity frameworks.         ",
    "url": "https://arxiv.org/abs/2507.07406",
    "authors": [
      "Jikesh Thapa",
      "Gurrehmat Chahal",
      "Serban Voinea Gabreanu",
      "Yazan Otoum"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07413",
    "title": "Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT Networks",
    "abstract": "           This paper presents a novel approach to intrusion detection by integrating traditional signature-based methods with the contextual understanding capabilities of the GPT-2 Large Language Model (LLM). As cyber threats become increasingly sophisticated, particularly in distributed, heterogeneous, and resource-constrained environments such as those enabled by the Internet of Things (IoT), the need for dynamic and adaptive Intrusion Detection Systems (IDSs) becomes increasingly urgent. While traditional methods remain effective for detecting known threats, they often fail to recognize new and evolving attack patterns. In contrast, GPT-2 excels at processing unstructured data and identifying complex semantic relationships, making it well-suited to uncovering subtle, zero-day attack vectors. We propose a hybrid IDS framework that merges the robustness of signature-based techniques with the adaptability of GPT-2-driven semantic analysis. Experimental evaluations on a representative intrusion dataset demonstrate that our model enhances detection accuracy by 6.3%, reduces false positives by 9.0%, and maintains near real-time responsiveness. These results affirm the potential of language model integration to build intelligent, scalable, and resilient cybersecurity defences suited for modern connected environments.         ",
    "url": "https://arxiv.org/abs/2507.07413",
    "authors": [
      "Mohammad F. Al-Hammouri",
      "Yazan Otoum",
      "Rasha Atwa",
      "Amiya Nayak"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07414",
    "title": "GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation",
    "abstract": "           Time, cost, and energy efficiency are critical considerations in Deep-Learning (DL), particularly when processing long texts. Transformers, which represent the current state of the art, exhibit quadratic computational complexity relative to input length, making them inefficient for extended documents. This study introduces a novel model architecture that combines Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated with a real-time, end-to-end graph generation mechanism. The model processes compact batches of character-level inputs without requiring padding or truncation. To enhance performance while maintaining high speed and efficiency, the model incorporates information from Large Language Models (LLMs), such as token embeddings and sentiment polarities, through efficient dictionary lookups. It captures local contextual patterns using CNNs, expands local receptive fields via lattice-based graph structures, and employs small-world graphs to aggregate document-level information. The generated graphs exhibit structural properties indicative of meaningful semantic organization, with an average clustering coefficient of approximately 0.45 and an average shortest path length ranging between 4 and 5. The model is evaluated across multiple text classification tasks, including sentiment analysis and news-categorization, and is compared against state-of-the-art models. Experimental results confirm the proposed model's efficiency and competitive performance.         ",
    "url": "https://arxiv.org/abs/2507.07414",
    "authors": [
      "Fardin Rastakhiz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07417",
    "title": "May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks",
    "abstract": "           A popular class of defenses against prompt injection attacks on large language models (LLMs) relies on fine-tuning the model to separate instructions and data, so that the LLM does not follow instructions that might be present with data. There are several academic systems and production-level implementations of this idea. We evaluate the robustness of this class of prompt injection defenses in the whitebox setting by constructing strong optimization-based attacks and showing that the defenses do not provide the claimed security properties. Specifically, we construct a novel attention-based attack algorithm for text-based LLMs and apply it to two recent whitebox defenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks with success rates of up to 70% with modest increase in attacker budget in terms of tokens. Our findings make fundamental progress towards understanding the robustness of prompt injection defenses in the whitebox setting. We release our code and attacks at this https URL ",
    "url": "https://arxiv.org/abs/2507.07417",
    "authors": [
      "Nishit V. Pandya",
      "Andrey Labunets",
      "Sicun Gao",
      "Earlence Fernandes"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.07421",
    "title": "SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data",
    "abstract": "           Eviction is a significant yet understudied social determinants of health (SDoH), linked to housing instability, unemployment, and mental health. While eviction appears in unstructured electronic health records (EHRs), it is rarely coded in structured fields, limiting downstream applications. We introduce SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop annotation, and automated prompt optimization (APO) to extract eviction statuses from clinical notes. Using this pipeline, we created the largest public eviction-related SDoH dataset to date, comprising 14 fine-grained categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%), GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling cost-effective deployment across various model sizes. The pipeline reduces annotation effort by over 80%, accelerates dataset creation, enables scalable eviction detection, and generalizes to other information extraction tasks.         ",
    "url": "https://arxiv.org/abs/2507.07421",
    "authors": [
      "Zonghai Yao",
      "Youxia Zhao",
      "Avijit Mitra",
      "David A. Levy",
      "Emily Druhl",
      "Jack Tsai",
      "Hong Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07432",
    "title": "Neural networks leverage nominally quantum and post-quantum representations",
    "abstract": "           We show that deep neural networks, including transformers and RNNs, pretrained as usual on next-token prediction, intrinsically discover and represent beliefs over 'quantum' and 'post-quantum' low-dimensional generative models of their training data -- as if performing iterative Bayesian updates over the latent state of this world model during inference as they observe more context. Notably, neural nets easily find these representation whereas there is no finite classical circuit that would do the job. The corresponding geometric relationships among neural activations induced by different input sequences are found to be largely independent of neural-network architecture. Each point in this geometry corresponds to a history-induced probability density over all possible futures, and the relative displacement of these points reflects the difference in mechanism and magnitude for how these distinct pasts affect the future.         ",
    "url": "https://arxiv.org/abs/2507.07432",
    "authors": [
      "Paul M. Riechers",
      "Thomas J. Elliott",
      "Adam S. Shai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2507.07436",
    "title": "When Graph Contrastive Learning Backfires: Spectral Vulnerability and Defense in Recommendation",
    "abstract": "           Graph Contrastive Learning (GCL) has demonstrated substantial promise in enhancing the robustness and generalization of recommender systems, particularly by enabling models to leverage large-scale unlabeled data for improved representation learning. However, in this paper, we reveal an unexpected vulnerability: the integration of GCL inadvertently increases the susceptibility of a recommender to targeted promotion attacks. Through both theoretical investigation and empirical validation, we identify the root cause as the spectral smoothing effect induced by contrastive optimization, which disperses item embeddings across the representation space and unintentionally enhances the exposure of target items. Building on this insight, we introduce CLeaR, a bi-level optimization attack method that deliberately amplifies spectral smoothness, enabling a systematic investigation of the susceptibility of GCL-based recommendation models to targeted promotion attacks. Our findings highlight the urgent need for robust countermeasures; in response, we further propose SIM, a spectral irregularity mitigation framework designed to accurately detect and suppress targeted items without compromising model performance. Extensive experiments on multiple benchmark datasets demonstrate that, compared to existing targeted promotion attacks, GCL-based recommendation models exhibit greater susceptibility when evaluated with CLeaR, while SIM effectively mitigates these vulnerabilities.         ",
    "url": "https://arxiv.org/abs/2507.07436",
    "authors": [
      "Zongwei Wang",
      "Min Gao",
      "Junliang Yu",
      "Shazia Sadiq",
      "Hongzhi Yin",
      "Ling Liu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2507.07437",
    "title": "PHandover: Parallel Handover in Mobile Satellite Network",
    "abstract": "           The construction of Low Earth Orbit (LEO) satellite constellations has recently attracted tremendous attention from both academia and industry. The 5G and 6G standards have identified LEO satellite networks as a key component of future mobile networks. However, due to the high-speed movement of satellites, ground terminals often experience frequent and high-latency handovers, which significantly deteriorate the performance of latency-sensitive applications. To address this challenge, we propose a parallel handover mechanism for mobile satellite networks that can considerably reduce handover latency. The main idea is to employ plan-based handovers instead of measurement-based handovers to avoid interactions between the access and core networks, thereby eliminating the significant time overhead associated with traditional handover procedures. Specifically, we introduce a novel network function named the Satellite Synchronized Function (SSF), which is designed to be fully compliant with the standard 5G core network. In addition, we propose a machine learning model for signal strength prediction, coupled with an efficient handover scheduling algorithm. We have conducted extensive experiments, and the results demonstrate that our proposed handover scheme can reduce handover latency by 21\\times compared to the standard NTN handover scheme and two other existing handover approaches, along with significant improvements in network stability and user-level performance.         ",
    "url": "https://arxiv.org/abs/2507.07437",
    "authors": [
      "Jiasheng Wu",
      "Shaojie Su",
      "Wenjun Zhu",
      "Xiong Wang",
      "Jingjing Zhang",
      "Xingqiu He",
      "Yue Gao"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2507.07438",
    "title": "Algorithmic Complexity Attacks on All Learned Cardinality Estimators: A Data-centric Approach",
    "abstract": "           Learned cardinality estimators show promise in query cardinality prediction, yet they universally exhibit fragility to training data drifts, posing risks for real-world deployment. This work is the first to theoretical investigate how minimal data-level drifts can maximally degrade the accuracy of learned estimators. We propose data-centric algorithmic complexity attacks against learned estimators in a black-box setting, proving that finding the optimal attack strategy is NP-Hard. To address this, we design a polynomial-time approximation algorithm with a $(1-\\kappa)$ approximation ratio. Extensive experiments demonstrate our attack's effectiveness: on STATS-CEB and IMDB-JOB benchmarks, modifying just 0.8\\% of training tuples increases the 90th percentile Qerror by three orders of magnitude and raises end-to-end processing time by up to 20$\\times$. Our work not only reveals critical vulnerabilities in deployed learned estimators but also provides the first unified worst-case theoretical analysis of their fragility under data updates. Additionally, we identify two countermeasures to mitigate such black-box attacks, offering insights for developing robust learned database optimizers.         ",
    "url": "https://arxiv.org/abs/2507.07438",
    "authors": [
      "Yingze Li",
      "Xianglong Liu",
      "Dong Wang",
      "Zixuan Wang",
      "Hongzhi Wang",
      "Kaixing Zhang",
      "Yiming Guan"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2507.07440",
    "title": "Self-supervised Learning of Latent Space Dynamics",
    "abstract": "           Modeling the dynamic behavior of deformable objects is crucial for creating realistic digital worlds. While conventional simulations produce high-quality motions, their computational costs are often prohibitive. Subspace simulation techniques address this challenge by restricting deformations to a lower-dimensional space, improving performance while maintaining visually compelling results. However, even subspace methods struggle to meet the stringent performance demands of portable devices such as virtual reality headsets and mobile platforms. To overcome this limitation, we introduce a novel subspace simulation framework powered by a neural latent-space integrator. Our approach leverages self-supervised learning to enhance inference stability and generalization. By operating entirely within latent space, our method eliminates the need for full-space computations, resulting in a highly efficient method well-suited for deployment on portable devices. We demonstrate the effectiveness of our approach on challenging examples involving rods, shells, and solids, showcasing its versatility and potential for widespread adoption.         ",
    "url": "https://arxiv.org/abs/2507.07440",
    "authors": [
      "Yue Li",
      "Gene Wei-Chin Lin",
      "Egor Larionov",
      "Aljaz Bozic",
      "Doug Roble",
      "Ladislav Kavan",
      "Stelian Coros",
      "Bernhard Thomaszewski",
      "Tuur Stuyck",
      "Hsiao-yu Chen"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2507.07443",
    "title": "Dual Semantic-Aware Network for Noise Suppressed Ultrasound Video Segmentation",
    "abstract": "           Ultrasound imaging is a prevalent diagnostic tool known for its simplicity and non-invasiveness. However, its inherent characteristics often introduce substantial noise, posing considerable challenges for automated lesion or organ segmentation in ultrasound video sequences. To address these limitations, we propose the Dual Semantic-Aware Network (DSANet), a novel framework designed to enhance noise robustness in ultrasound video segmentation by fostering mutual semantic awareness between local and global features. Specifically, we introduce an Adjacent-Frame Semantic-Aware (AFSA) module, which constructs a channel-wise similarity matrix to guide feature fusion across adjacent frames, effectively mitigating the impact of random noise without relying on pixel-level relationships. Additionally, we propose a Local-and-Global Semantic-Aware (LGSA) module that reorganizes and fuses temporal unconditional local features, which capture spatial details independently at each frame, with conditional global features that incorporate temporal context from adjacent frames. This integration facilitates multi-level semantic representation, significantly improving the model's resilience to noise interference. Extensive evaluations on four benchmark datasets demonstrate that DSANet substantially outperforms state-of-the-art methods in segmentation accuracy. Moreover, since our model avoids pixel-level feature dependencies, it achieves significantly higher inference FPS than video-based methods, and even surpasses some image-based models. Code can be found in \\href{this https URL}{DSANet}         ",
    "url": "https://arxiv.org/abs/2507.07443",
    "authors": [
      "Ling Zhou",
      "Runtian Yuan",
      "Yi Liu",
      "Yuejie Zhang",
      "Rui Feng",
      "Shang Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07453",
    "title": "Bluish Veil Detection and Lesion Classification using Custom Deep Learnable Layers with Explainable Artificial Intelligence (XAI)",
    "abstract": "           Melanoma, one of the deadliest types of skin cancer, accounts for thousands of fatalities globally. The bluish, blue-whitish, or blue-white veil (BWV) is a critical feature for diagnosing melanoma, yet research into detecting BWV in dermatological images is limited. This study utilizes a non-annotated skin lesion dataset, which is converted into an annotated dataset using a proposed imaging algorithm based on color threshold techniques on lesion patches and color palettes. A Deep Convolutional Neural Network (DCNN) is designed and trained separately on three individual and combined dermoscopic datasets, using custom layers instead of standard activation function layers. The model is developed to categorize skin lesions based on the presence of BWV. The proposed DCNN demonstrates superior performance compared to conventional BWV detection models across different datasets. The model achieves a testing accuracy of 85.71% on the augmented PH2 dataset, 95.00% on the augmented ISIC archive dataset, 95.05% on the combined augmented (PH2+ISIC archive) dataset, and 90.00% on the Derm7pt dataset. An explainable artificial intelligence (XAI) algorithm is subsequently applied to interpret the DCNN's decision-making process regarding BWV detection. The proposed approach, coupled with XAI, significantly improves the detection of BWV in skin lesions, outperforming existing models and providing a robust tool for early melanoma diagnosis.         ",
    "url": "https://arxiv.org/abs/2507.07453",
    "authors": [
      "M. A. Rasel",
      "Sameem Abdul Kareem",
      "Zhenli Kwan",
      "Shin Shen Yong",
      "Unaizah Obaidellah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07481",
    "title": "Energy Transfer and Data Collection from Batteryless Sensors in Low-altitude Wireless Networks",
    "abstract": "           The integration of wireless power transfer (WPT) with Internet of Things (IoT) offers promising solutions for sensing applications, but faces significant challenges when deployed in hard-to-access areas such as high-temperature environments. In such extreme conditions, traditional fixed WPT infrastructure cannot be safely installed, and batteries rapidly degrade due to hardware failures. In this paper, we propose an uncrewed aerial vehicle (UAV)-assisted data collection and WPT framework for batteryless sensor (BLS) networks deployed in these challenging environments. Specifically, we consider a practical scenario where a UAV first transfers energy to BLS nodes via WPT, enabling these nodes to subsequently transmit their collected data to the UAV through orthogonal frequency-division multiple access (OFDMA). Then, we formulate a multi-objective optimization problem that aims to maximize the fair data collection volume while minimizing the UAV energy consumption through joint optimization of transmit power allocation and flight trajectory planning. Due to the non-convex nature and dynamic characteristics of this problem, conventional optimization methods prove inadequate. To address these challenges, we propose an enhanced soft actor-critic algorithm with parameter-free attention, prioritized experience replay, and value-based reward centering (SAC-PPV), thereby improving the exploration efficiency and learning stability of the algorithm in complex WPT scenarios. Simulation results demonstrate that the proposed approach consistently outperforms benchmark algorithms under various network configurations.         ",
    "url": "https://arxiv.org/abs/2507.07481",
    "authors": [
      "Wen Zhang",
      "Aimin Wang",
      "Jiahui Li",
      "Geng Sun",
      "Jiacheng Wang",
      "Weijie Yuan",
      "Dusit Niyato"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2507.07495",
    "title": "PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving",
    "abstract": "           Recently, decomposing complex problems into simple subtasks--a crucial part of human-like natural planning--to solve the given problem has significantly boosted the performance of large language models (LLMs). However, leveraging such planning structures during post-training to boost the performance of smaller open-source LLMs remains underexplored. Motivated by this, we introduce PLAN-TUNING, a unified post-training framework that (i) distills synthetic task decompositions (termed \"planning trajectories\") from large-scale LLMs and (ii) fine-tunes smaller models via supervised and reinforcement-learning objectives designed to mimic these planning processes to improve complex reasoning. On GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by an average $\\sim7\\%$. Furthermore, plan-tuned models show better generalization capabilities on out-of-domain datasets, with average $\\sim10\\%$ and $\\sim12\\%$ performance improvements on OlympiadBench and AIME 2024, respectively. Our detailed analysis demonstrates how planning trajectories improves complex reasoning capabilities, showing that PLAN-TUNING is an effective strategy for improving task-specific performance of smaller LLMs.         ",
    "url": "https://arxiv.org/abs/2507.07495",
    "authors": [
      "Mihir Parmar",
      "Palash Goyal",
      "Xin Liu",
      "Yiwen Song",
      "Mingyang Ling",
      "Chitta Baral",
      "Hamid Palangi",
      "Tomas Pfister"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07498",
    "title": "Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code",
    "abstract": "           Enhancing reasoning capabilities remains a central focus in the LLM reasearch community. A promising direction involves requiring models to simulate code execution step-by-step to derive outputs for given inputs. However, as code is often designed for large-scale systems, direct application leads to over-reliance on complex data structures and algorithms, even for simple cases, resulting in overfitting to algorithmic patterns rather than core reasoning structures. To address this, we propose TeaR, which aims at teaching LLMs to reason better. TeaR leverages careful data curation and reinforcement learning to guide models in discovering optimal reasoning paths through code-related tasks, thereby improving general reasoning abilities. We conduct extensive experiments using two base models and three long-CoT distillation models, with model sizes ranging from 1.5 billion to 32 billion parameters, and across 17 benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results consistently show significant performance improvements. Notably, TeaR achieves a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.         ",
    "url": "https://arxiv.org/abs/2507.07498",
    "authors": [
      "Keqin Bao",
      "Nuo Chen",
      "Xiaoyuan Li",
      "Binyuan Hui",
      "Bowen Yu",
      "Fuli Feng",
      "Junyang Lin",
      "Xiangnan He",
      "Dayiheng Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07515",
    "title": "GGMotion: Group Graph Dynamics-Kinematics Networks for Human Motion Prediction",
    "abstract": "           Human motion is a continuous physical process in 3D space, governed by complex dynamic and kinematic constraints. Existing methods typically represent the human pose as an abstract graph structure, neglecting the intrinsic physical dependencies between joints, which increases learning difficulty and makes the model prone to generating unrealistic motions. In this paper, we propose GGMotion, a group graph dynamics-kinematics network that models human topology in groups to better leverage dynamics and kinematics priors. To preserve the geometric equivariance in 3D space, we propose a novel radial field for the graph network that captures more comprehensive spatio-temporal dependencies by aggregating joint features through spatial and temporal edges. Inter-group and intra-group interaction modules are employed to capture the dependencies of joints at different scales. Combined with equivariant multilayer perceptrons (MLP), joint position features are updated in each group through parallelized dynamics-kinematics propagation to improve physical plausibility. Meanwhile, we introduce an auxiliary loss to supervise motion priors during training. Extensive experiments on three standard benchmarks, including Human3.6M, CMU-Mocap, and 3DPW, demonstrate the effectiveness and superiority of our approach, achieving a significant performance margin in short-term motion prediction. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.07515",
    "authors": [
      "Shuaijin Wan",
      "Huaijiang Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07522",
    "title": "NLGCL: Naturally Existing Neighbor Layers Graph Contrastive Learning for Recommendation",
    "abstract": "           Graph Neural Networks (GNNs) are widely used in collaborative filtering to capture high-order user-item relationships. To address the data sparsity problem in recommendation systems, Graph Contrastive Learning (GCL) has emerged as a promising paradigm that maximizes mutual information between contrastive views. However, existing GCL methods rely on augmentation techniques that introduce semantically irrelevant noise and incur significant computational and storage costs, limiting effectiveness and efficiency. To overcome these challenges, we propose NLGCL, a novel contrastive learning framework that leverages naturally contrastive views between neighbor layers within GNNs. By treating each node and its neighbors in the next layer as positive pairs, and other nodes as negatives, NLGCL avoids augmentation-based noise while preserving semantic relevance. This paradigm eliminates costly view construction and storage, making it computationally efficient and practical for real-world scenarios. Extensive experiments on four public datasets demonstrate that NLGCL outperforms state-of-the-art baselines in effectiveness and efficiency.         ",
    "url": "https://arxiv.org/abs/2507.07522",
    "authors": [
      "Jinfeng Xu",
      "Zheyu Chen",
      "Shuo Yang",
      "Jinze Li",
      "Hewei Wang",
      "Wei Wang",
      "Xiping Hu",
      "Edith Ngai"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2507.07526",
    "title": "DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel Spectrogram Reconstruction",
    "abstract": "           Decoding speech from brain signals is a challenging research problem. Although existing technologies have made progress in reconstructing the mel spectrograms of auditory stimuli at the word or letter level, there remain core challenges in the precise reconstruction of minute-level continuous imagined speech: traditional models struggle to balance the efficiency of temporal dependency modeling and information retention in long-sequence decoding. To address this issue, this paper proposes the Dynamic Multiscale Fusion Network (DMF2Mel), which consists of four core components: the Dynamic Contrastive Feature Aggregation Module (DC-FAM), the Hierarchical Attention-Guided Multi-Scale Network (HAMS-Net), the SplineMap attention mechanism, and the bidirectional state space module (convMamba). Specifically, the DC-FAM separates speech-related \"foreground features\" from noisy \"background features\" through local convolution and global attention mechanisms, effectively suppressing interference and enhancing the representation of transient signals. HAMS-Net, based on the U-Net framework,achieves cross-scale fusion of high-level semantics and low-level details. The SplineMap attention mechanism integrates the Adaptive Gated Kolmogorov-Arnold Network (AGKAN) to combine global context modeling with spline-based local fitting. The convMamba captures long-range temporal dependencies with linear complexity and enhances nonlinear dynamic modeling capabilities. Results on the SparrKULee dataset show that DMF2Mel achieves a Pearson correlation coefficient of 0.074 in mel spectrogram reconstruction for known subjects (a 48% improvement over the baseline) and 0.048 for unknown subjects (a 35% improvement over the baseline).Code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2507.07526",
    "authors": [
      "Cunhang Fan",
      "Sheng Zhang",
      "Jingjing Zhang",
      "Enrui Liu",
      "Xinhui Li",
      "Minggang Zhao",
      "Zhao Lv"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2507.07532",
    "title": "Neural Concept Verifier: Scaling Prover-Verifier Games via Concept Encodings",
    "abstract": "           While Prover-Verifier Games (PVGs) offer a promising path toward verifiability in nonlinear classification models, they have not yet been applied to complex inputs such as high-dimensional images. Conversely, Concept Bottleneck Models (CBMs) effectively translate such data into interpretable concepts but are limited by their reliance on low-capacity linear predictors. In this work, we introduce the Neural Concept Verifier (NCV), a unified framework combining PVGs with concept encodings for interpretable, nonlinear classification in high-dimensional settings. NCV achieves this by utilizing recent minimally supervised concept discovery models to extract structured concept encodings from raw inputs. A prover then selects a subset of these encodings, which a verifier -- implemented as a nonlinear predictor -- uses exclusively for decision-making. Our evaluations show that NCV outperforms CBM and pixel-based PVG classifier baselines on high-dimensional, logically complex datasets and also helps mitigate shortcut behavior. Overall, we demonstrate NCV as a promising step toward performative, verifiable AI.         ",
    "url": "https://arxiv.org/abs/2507.07532",
    "authors": [
      "Berkant Turan",
      "Suhrab Asadulla",
      "David Steinmann",
      "Wolfgang Stammer",
      "Sebastian Pokutta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07535",
    "title": "A Fragmentation-Aware Adaptive Bilevel Search Framework for Service Mapping in Computing Power Networks",
    "abstract": "           Computing Power Network (CPN) unifies wide-area computing resources through coordinated network control, while cloud-native abstractions enable flexible resource orchestration and on-demand service provisioning atop the elastic infrastructure CPN provides. However, current approaches fall short of fully integrating computing resources via network-enabled coordination as envisioned by CPN. In particular, optimally mapping services to an underlying infrastructure to maximize resource efficiency and service satisfaction remains challenging. To overcome this challenge, we formally define the service mapping problem in CPN, establish its theoretical intractability, and identify key challenges in practical optimization. We propose Adaptive Bilevel Search (ABS), a modular framework featuring (1) graph partitioning-based reformulation to capture variable coupling, (2) a bilevel optimization architecture for efficient global exploration with local optimality guarantees, and (3) fragmentation-aware evaluation for global performance guidance. Implemented using distributed particle swarm optimization, ABS is extensively evaluated across diverse CPN scenarios, consistently outperforming existing approaches. Notably, in complex scenarios, ABS achieves up to 73.2% higher computing resource utilization and a 60.2% higher service acceptance ratio compared to the best-performing baseline.         ",
    "url": "https://arxiv.org/abs/2507.07535",
    "authors": [
      "Jingzhao Xie",
      "Zhenglian Li",
      "Gang Sun",
      "Long Luo",
      "Hongfang Yu",
      "Dusit Niyato"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2507.07559",
    "title": "Real-Time Decorrelation-Based Anomaly Detection for Multivariate Time Series",
    "abstract": "           Anomaly detection (AD) plays a vital role across a wide range of real-world domains by identifying data instances that deviate from expected patterns, potentially signaling critical events such as system failures, fraudulent activities, or rare medical conditions. The demand for real-time AD has surged with the rise of the (Industrial) Internet of Things, where massive volumes of multivariate sensor data must be processed instantaneously. Real-time AD requires methods that not only handle high-dimensional streaming data but also operate in a single-pass manner, without the burden of storing historical instances, thereby ensuring minimal memory usage and fast decision-making. We propose DAD, a novel real-time decorrelation-based anomaly detection method for multivariate time series, based on an online decorrelation learning approach. Unlike traditional proximity-based or reconstruction-based detectors that process entire data or windowed instances, DAD dynamically learns and monitors the correlation structure of data sample by sample in a single pass, enabling efficient and effective detection. To support more realistic benchmarking practices, we also introduce a practical hyperparameter tuning strategy tailored for real-time anomaly detection scenarios. Extensive experiments on widely used benchmark datasets demonstrate that DAD achieves the most consistent and superior performance across diverse anomaly types compared to state-of-the-art methods. Crucially, its robustness to increasing dimensionality makes it particularly well-suited for real-time, high-dimensional data streams. Ultimately, DAD not only strikes an optimal balance between detection efficacy and computational efficiency but also sets a new standard for real-time, memory-constrained anomaly detection.         ",
    "url": "https://arxiv.org/abs/2507.07559",
    "authors": [
      "Amirhossein Sadough",
      "Mahyar Shahsavari",
      "Mark Wijtvliet",
      "Marcel van Gerven"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.07565",
    "title": "Secure Cooperative Gradient Coding: Optimality, Reliability, and Global Privacy",
    "abstract": "           This paper studies privacy-sensitive federated learning (FL) with unreliable communication, focusing on secure aggregation and straggler mitigation. While secure aggregation cryptographically reconstructs the global model without exposing client updates, random link failures disrupt its key coordination, degrading model accuracy. Moreover, unreliable communication can lead to objective inconsistency, causing the global model to converge to arbitrary, sub-optimal points far from the intended optimum. This paper proposes Secure Cooperative Gradient Coding (SecCoGC), a practical solution that achieves secure aggregation with arbitrarily strong privacy guarantees and robust straggler mitigation under unreliable communication. SecCoGC operates natively in the real field, making it directly applicable to practical deployments. To ensure equitable privacy protection across clients, we further introduce Fair-SecCoGC, an extension that enforces fairness in the level of privacy offered to all users. To conclude, this paper formally formulates the problem of secure aggregation in the real field and presents both general and computationally efficient key construction methods. Moreover, it provides a comprehensive privacy analysis under Local Mutual Information Privacy (LMIP) and Local Differential Privacy (LDP) across all protocol layers. Robustness and convergence properties are also rigorously analyzed. Finally, extensive simulations are performed across diverse network conditions and benchmark datasets to validate the effectiveness of the proposed methods. The results show that SecCoGC achieves strong robustness to unreliable communication under arbitrarily strong privacy guarantees. It outperforms existing privacy-preserving methods with performance gains of up to 20\\%-70\\%.         ",
    "url": "https://arxiv.org/abs/2507.07565",
    "authors": [
      "Shudi Weng"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2507.07579",
    "title": "NexViTAD: Few-shot Unsupervised Cross-Domain Defect Detection via Vision Foundation Models and Multi-Task Learning",
    "abstract": "           This paper presents a novel few-shot cross-domain anomaly detection framework, Nexus Vision Transformer for Anomaly Detection (NexViTAD), based on vision foundation models, which effectively addresses domain-shift challenges in industrial anomaly detection through innovative shared subspace projection mechanisms and multi-task learning (MTL) module. The main innovations include: (1) a hierarchical adapter module that adaptively fuses complementary features from Hiera and DINO-v2 pre-trained models, constructing more robust feature representations; (2) a shared subspace projection strategy that enables effective cross-domain knowledge transfer through bottleneck dimension constraints and skip connection mechanisms; (3) a MTL Decoder architecture supports simultaneous processing of multiple source domains, significantly enhancing model generalization capabilities; (4) an anomaly score inference method based on Sinkhorn-K-means clustering, combined with Gaussian filtering and adaptive threshold processing for precise pixel level. Valuated on the MVTec AD dataset, NexViTAD delivers state-of-the-art performance with an AUC of 97.5%, AP of 70.4%, and PRO of 95.2% in the target domains, surpassing other recent models, marking a transformative advance in cross-domain defect detection.         ",
    "url": "https://arxiv.org/abs/2507.07579",
    "authors": [
      "Tianwei Mu",
      "Feiyu Duan",
      "Bo Zhou",
      "Dan Xue",
      "Manhong Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07595",
    "title": "Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs",
    "abstract": "           Recent investigations on the effectiveness of Graph Neural Network (GNN)-based models for link prediction in Knowledge Graphs (KGs) show that vanilla aggregation does not significantly impact the model performance. In this paper, we introduce a novel method, named Context Pooling, to enhance GNN-based models' efficacy for link predictions in KGs. To our best of knowledge, Context Pooling is the first methodology that applies graph pooling in KGs. Additionally, Context Pooling is first-of-its-kind to enable the generation of query-specific graphs for inductive settings, where testing entities are unseen during training. Specifically, we devise two metrics, namely neighborhood precision and neighborhood recall, to assess the neighbors' logical relevance regarding the given queries, thereby enabling the subsequent comprehensive identification of only the logically relevant neighbors for link prediction. Our method is generic and assessed by being applied to two state-of-the-art (SOTA) models on three public transductive and inductive datasets, achieving SOTA performance in 42 out of 48 settings.         ",
    "url": "https://arxiv.org/abs/2507.07595",
    "authors": [
      "Zhixiang Su",
      "Di Wang",
      "Chunyan Miao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07619",
    "title": "Towards conservative inference in credal networks using belief functions: the case of credal chains",
    "abstract": "           This paper explores belief inference in credal networks using Dempster-Shafer theory. By building on previous work, we propose a novel framework for propagating uncertainty through a subclass of credal networks, namely chains. The proposed approach efficiently yields conservative intervals through belief and plausibility functions, combining computational speed with robust uncertainty representation. Key contributions include formalizing belief-based inference methods and comparing belief-based inference against classical sensitivity analysis. Numerical results highlight the advantages and limitations of applying belief inference within this framework, providing insights into its practical utility for chains and for credal networks in general.         ",
    "url": "https://arxiv.org/abs/2507.07619",
    "authors": [
      "Marco Sangalli",
      "Thomas Krak",
      "Cassio de Campos"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2507.07620",
    "title": "ViLU: Learning Vision-Language Uncertainties for Failure Prediction",
    "abstract": "           Reliable Uncertainty Quantification (UQ) and failure prediction remain open challenges for Vision-Language Models (VLMs). We introduce ViLU, a new Vision-Language Uncertainty quantification framework that contextualizes uncertainty estimates by leveraging all task-relevant textual representations. ViLU constructs an uncertainty-aware multi-modal representation by integrating the visual embedding, the predicted textual embedding, and an image-conditioned textual representation via cross-attention. Unlike traditional UQ methods based on loss prediction, ViLU trains an uncertainty predictor as a binary classifier to distinguish correct from incorrect predictions using a weighted binary cross-entropy loss, making it loss-agnostic. In particular, our proposed approach is well-suited for post-hoc settings, where only vision and text embeddings are available without direct access to the model itself. Extensive experiments on diverse datasets show the significant gains of our method compared to state-of-the-art failure prediction methods. We apply our method to standard classification datasets, such as ImageNet-1k, as well as large-scale image-caption datasets like CC12M and LAION-400M. Ablation studies highlight the critical role of our architecture and training in achieving effective uncertainty quantification. Our code is publicly available and can be found here: this https URL.         ",
    "url": "https://arxiv.org/abs/2507.07620",
    "authors": [
      "Marc Lafon",
      "Yannis Karmim",
      "Julio Silva-Rodriguez",
      "Paul Couairon",
      "Cl\u00e9ment Rambour",
      "Rapha\u00ebl Fournier-Sniehotta",
      "Ismail Ben Ayed",
      "Jose Dolz",
      "Nicolas Thome"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07621",
    "title": "Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation",
    "abstract": "           Unsupervised Graph Domain Adaptation (UGDA) leverages labeled source domain graphs to achieve effective performance in unlabeled target domains despite distribution shifts. However, existing methods often yield suboptimal results due to the entanglement of causal-spurious features and the failure of global alignment strategies. We propose SLOGAN (Sparse Causal Discovery with Generative Intervention), a novel approach that achieves stable graph representation transfer through sparse causal modeling and dynamic intervention mechanisms. Specifically, SLOGAN first constructs a sparse causal graph structure, leveraging mutual information bottleneck constraints to disentangle sparse, stable causal features while compressing domain-dependent spurious correlations through variational inference. To address residual spurious correlations, we innovatively design a generative intervention mechanism that breaks local spurious couplings through cross-domain feature recombination while maintaining causal feature semantic consistency via covariance constraints. Furthermore, to mitigate error accumulation in target domain pseudo-labels, we introduce a category-adaptive dynamic calibration strategy, ensuring stable discriminative learning. Extensive experiments on multiple real-world datasets demonstrate that SLOGAN significantly outperforms existing baselines.         ",
    "url": "https://arxiv.org/abs/2507.07621",
    "authors": [
      "Junyu Luo",
      "Yuhao Tang",
      "Yiwei Fu",
      "Xiao Luo",
      "Zhizhuo Kou",
      "Zhiping Xiao",
      "Wei Ju",
      "Wentao Zhang",
      "Ming Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07622",
    "title": "TransformEEG: Towards Improving Model Generalizability in Deep Learning-based EEG Parkinson's Disease Detection",
    "abstract": "           Electroencephalography (EEG) is establishing itself as an important, low-cost, noninvasive diagnostic tool for the early detection of Parkinson's Disease (PD). In this context, EEG-based Deep Learning (DL) models have shown promising results due to their ability to discover highly nonlinear patterns within the signal. However, current state-of-the-art DL models suffer from poor generalizability caused by high inter-subject variability. This high variability underscores the need for enhancing model generalizability by developing new architectures better tailored to EEG data. This paper introduces TransformEEG, a hybrid Convolutional-Transformer designed for Parkinson's disease detection using EEG data. Unlike transformer models based on the EEGNet structure, TransformEEG incorporates a depthwise convolutional tokenizer. This tokenizer is specialized in generating tokens composed by channel-specific features, which enables more effective feature mixing within the self-attention layers of the transformer encoder. To evaluate the proposed model, four public datasets comprising 290 subjects (140 PD patients, 150 healthy controls) were harmonized and aggregated. A 10-outer, 10-inner Nested-Leave-N-Subjects-Out (N-LNSO) cross-validation was performed to provide an unbiased comparison against seven other consolidated EEG deep learning models. TransformEEG achieved the highest balanced accuracy's median (78.45%) as well as the lowest interquartile range (6.37%) across all the N-LNSO partitions. When combined with data augmentation and threshold correction, median accuracy increased to 80.10%, with an interquartile range of 5.74%. In conclusion, TransformEEG produces more consistent and less skewed results. It demonstrates a substantial reduction in variability and more reliable PD detection using EEG data compared to the other investigated models.         ",
    "url": "https://arxiv.org/abs/2507.07622",
    "authors": [
      "Federico Del Pup",
      "Riccardo Brun",
      "Filippo Iotti",
      "Edoardo Paccagnella",
      "Mattia Pezzato",
      "Sabrina Bertozzo",
      "Andrea Zanola",
      "Louis Fabrice Tshimanga",
      "Henning M\u00fcller",
      "Manfredo Atzori"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07660",
    "title": "Scalable Signed Exponential Random Graph Models under Local Dependence",
    "abstract": "           Traditional network analysis focuses on binary edges, while real-world relationships are more nuanced, encompassing cooperation, neutrality, and conflict. The rise of negative edges in social media discussions spurred interest in analyzing signed interactions, especially in polarized debates. However, the vast data generated by digital networks presents challenges for traditional methods like Stochastic Block Models (SBM) and Exponential Family Random Graph Models (ERGM), particularly due to the homogeneity assumption and global dependence, which become increasingly unrealistic as network size grows. To address this, we propose a novel method that combines the strengths of SBM and ERGM while mitigating their weaknesses by incorporating local dependence based on non-overlapping blocks. Our approach involves a two-step process: first, decomposing the network into sub-networks using SBM approximation, and then estimating parameters using ERGM methods. We validate our method on large synthetic networks and apply it to a signed Wikipedia network of thousands of editors. Through the use of local dependence, we find patterns consistent with structural balance theory.         ",
    "url": "https://arxiv.org/abs/2507.07660",
    "authors": [
      "Marc Schalberger",
      "Cornelius Fritz"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation (stat.CO)"
    ]
  },
  {
    "id": "arXiv:2507.07675",
    "title": "Some Theoretical Results on Layerwise Effective Dimension Oscillations in Finite Width ReLU Networks",
    "abstract": "           We analyze the layerwise effective dimension (rank of the feature matrix) in fully-connected ReLU networks of finite width. Specifically, for a fixed batch of $m$ inputs and random Gaussian weights, we derive closed-form expressions for the expected rank of the \\$m\\times n\\$ hidden activation matrices. Our main result shows that $\\mathbb{E}[EDim(\\ell)]=m[1-(1-2/\\pi)^\\ell]+O(e^{-c m})$ so that the rank deficit decays geometrically with ratio $1-2 / \\pi \\approx 0.3634$. We also prove a sub-Gaussian concentration bound, and identify the \"revival\" depths at which the expected rank attains local maxima. In particular, these peaks occur at depths $\\ell_k^*\\approx(k+1/2)\\pi/\\log(1/\\rho)$ with height $\\approx (1-e^{-\\pi/2}) m \\approx 0.79m$. We further show that this oscillatory rank behavior is a finite-width phenomenon: under orthogonal weight initialization or strong negative-slope leaky-ReLU, the rank remains (nearly) full. These results provide a precise characterization of how random ReLU layers alternately collapse and partially revive the subspace of input variations, adding nuance to prior work on expressivity of deep networks.         ",
    "url": "https://arxiv.org/abs/2507.07675",
    "authors": [
      "Darshan Makwana"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07700",
    "title": "Rethinking the Privacy of Text Embeddings: A Reproducibility Study of \"Text Embeddings Reveal (Almost) As Much As Text\"",
    "abstract": "           Text embeddings are fundamental to many natural language processing (NLP) tasks, extensively applied in domains such as recommendation systems and information retrieval (IR). Traditionally, transmitting embeddings instead of raw text has been seen as privacy-preserving. However, recent methods such as Vec2Text challenge this assumption by demonstrating that controlled decoding can successfully reconstruct original texts from black-box embeddings. The unexpectedly strong results reported by Vec2Text motivated us to conduct further verification, particularly considering the typically non-intuitive and opaque structure of high-dimensional embedding spaces. In this work, we reproduce the Vec2Text framework and evaluate it from two perspectives: (1) validating the original claims, and (2) extending the study through targeted experiments. First, we successfully replicate the original key results in both in-domain and out-of-domain settings, with only minor discrepancies arising due to missing artifacts, such as model checkpoints and dataset splits. Furthermore, we extend the study by conducting a parameter sensitivity analysis, evaluating the feasibility of reconstructing sensitive inputs (e.g., passwords), and exploring embedding quantization as a lightweight privacy defense. Our results show that Vec2Text is effective under ideal conditions, capable of reconstructing even password-like sequences that lack clear semantics. However, we identify key limitations, including its sensitivity to input sequence length. We also find that Gaussian noise and quantization techniques can mitigate the privacy risks posed by Vec2Text, with quantization offering a simpler and more widely applicable solution. Our findings emphasize the need for caution in using text embeddings and highlight the importance of further research into robust defense mechanisms for NLP systems.         ",
    "url": "https://arxiv.org/abs/2507.07700",
    "authors": [
      "Dominykas Seputis",
      "Yongkang Li",
      "Karsten Langerak",
      "Serghei Mihailov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2507.07709",
    "title": "One Object, Multiple Lies: A Benchmark for Cross-task Adversarial Attack on Unified Vision-Language Models",
    "abstract": "           Unified vision-language models(VLMs) have recently shown remarkable progress, enabling a single model to flexibly address diverse tasks through different instructions within a shared computational architecture. This instruction-based control mechanism creates unique security challenges, as adversarial inputs must remain effective across multiple task instructions that may be unpredictably applied to process the same malicious content. In this paper, we introduce CrossVLAD, a new benchmark dataset carefully curated from MSCOCO with GPT-4-assisted annotations for systematically evaluating cross-task adversarial attacks on unified VLMs. CrossVLAD centers on the object-change objective-consistently manipulating a target object's classification across four downstream tasks-and proposes a novel success rate metric that measures simultaneous misclassification across all tasks, providing a rigorous evaluation of adversarial transferability. To tackle this challenge, we present CRAFT (Cross-task Region-based Attack Framework with Token-alignment), an efficient region-centric attack method. Extensive experiments on Florence-2 and other popular unified VLMs demonstrate that our method outperforms existing approaches in both overall cross-task attack performance and targeted object-change success rates, highlighting its effectiveness in adversarially influencing unified VLMs across diverse tasks.         ",
    "url": "https://arxiv.org/abs/2507.07709",
    "authors": [
      "Jiale Zhao",
      "Xinyang Jiang",
      "Junyao Gao",
      "Yuhao Xue",
      "Cairong Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07714",
    "title": "Adaptive Gaussian Mixture Models-based Anomaly Detection for under-constrained Cable-Driven Parallel Robots",
    "abstract": "           Cable-Driven Parallel Robots (CDPRs) are increasingly used for load manipulation tasks involving predefined toolpaths with intermediate stops. At each stop, where the platform maintains a fixed pose and the motors keep the cables under tension, the system must evaluate whether it is safe to proceed by detecting anomalies that could compromise performance (e.g., wind gusts or cable impacts). This paper investigates whether anomalies can be detected using only motor torque data, without additional sensors. It introduces an adaptive, unsupervised outlier detection algorithm based on Gaussian Mixture Models (GMMs) to identify anomalies from torque signals. The method starts with a brief calibration period, just a few seconds, during which a GMM is fit on known anomaly-free data. Real-time torque measurements are then evaluated using Mahalanobis distance from the GMM, with statistically derived thresholds triggering anomaly flags. Model parameters are periodically updated using the latest segments identified as anomaly-free to adapt to changing conditions. Validation includes 14 long-duration test sessions simulating varied wind intensities. The proposed method achieves a 100% true positive rate and 95.4% average true negative rate, with 1-second detection latency. Comparative evaluation against power threshold and non-adaptive GMM methods indicates higher robustness to drift and environmental variation.         ",
    "url": "https://arxiv.org/abs/2507.07714",
    "authors": [
      "Julio Garrido",
      "Javier Vales",
      "Diego Silva-Mu\u00f1iz",
      "Enrique Riveiro",
      "Pablo L\u00f3pez-Matencio",
      "Josu\u00e9 Rivera-Andrade"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07727",
    "title": "Beyond Connectivity: Higher-Order Network Framework for Capturing Memory-Driven Mobility Dynamics",
    "abstract": "           Understanding and predicting mobility dynamics in transportation networks is critical for infrastructure planning, resilience analysis, and traffic management. Traditional graph-based models typically assume memoryless movement, limiting their ability to capture sequential dependencies inherent in real-world mobility patterns. In this study, we introduce a novel higher-order network framework for modeling memory-dependent dynamics in transportation systems. By extending classical graph representations through higher-order Markov chains and de Bruijn graph structures, our framework encodes the spatial and temporal ordering of traversed paths, enabling the analysis of structurally and functionally critical components with improved fidelity. We generalize key network analytics, including betweenness centrality, PageRank, and next-step prediction, to this higher-order setting and validate our approach on the Sioux Falls transportation network using agent-based trajectory data generated with MATSim. Experimental results demonstrate that higher-order models outperform first-order baselines across multiple tasks, with the third-order model achieving an optimal balance between predictive accuracy and model complexity. These findings highlight the importance of incorporating memory effects into network-based transportation analysis and offer a scalable, data-driven methodology for capturing complex mobility behaviors in infrastructure systems.         ",
    "url": "https://arxiv.org/abs/2507.07727",
    "authors": [
      "Chen Zhang",
      "J\u00fcrgen Hackl"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2507.07734",
    "title": "EEvAct: Early Event-Based Action Recognition with High-Rate Two-Stream Spiking Neural Networks",
    "abstract": "           Recognizing human activities early is crucial for the safety and responsiveness of human-robot and human-machine interfaces. Due to their high temporal resolution and low latency, event-based vision sensors are a perfect match for this early recognition demand. However, most existing processing approaches accumulate events to low-rate frames or space-time voxels which limits the early prediction capabilities. In contrast, spiking neural networks (SNNs) can process the events at a high-rate for early predictions, but most works still fall short on final accuracy. In this work, we introduce a high-rate two-stream SNN which closes this gap by outperforming previous work by 2% in final accuracy on the large-scale THU EACT-50 dataset. We benchmark the SNNs within a novel early event-based recognition framework by reporting Top-1 and Top-5 recognition scores for growing observation time. Finally, we exemplify the impact of these methods on a real-world task of early action triggering for human motion capture in sports.         ",
    "url": "https://arxiv.org/abs/2507.07734",
    "authors": [
      "Michael Neumeier",
      "Jules Lecomte",
      "Nils Kazinski",
      "Soubarna Banik",
      "Bing Li",
      "Axel von Arnim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2507.07738",
    "title": "Efficient and Scalable Estimation of Distributional Treatment Effects with Multi-Task Neural Networks",
    "abstract": "           We propose a novel multi-task neural network approach for estimating distributional treatment effects (DTE) in randomized experiments. While DTE provides more granular insights into the experiment outcomes over conventional methods focusing on the Average Treatment Effect (ATE), estimating it with regression adjustment methods presents significant challenges. Specifically, precision in the distribution tails suffers due to data imbalance, and computational inefficiencies arise from the need to solve numerous regression problems, particularly in large-scale datasets commonly encountered in industry. To address these limitations, our method leverages multi-task neural networks to estimate conditional outcome distributions while incorporating monotonic shape constraints and multi-threshold label learning to enhance accuracy. To demonstrate the practical effectiveness of our proposed method, we apply our method to both simulated and real-world datasets, including a randomized field experiment aimed at reducing water consumption in the US and a large-scale A/B test from a leading streaming platform in Japan. The experimental results consistently demonstrate superior performance across various datasets, establishing our method as a robust and practical solution for modern causal inference applications requiring a detailed understanding of treatment effect heterogeneity.         ",
    "url": "https://arxiv.org/abs/2507.07738",
    "authors": [
      "Tomu Hirata",
      "Undral Byambadalai",
      "Tatsushi Oka",
      "Shota Yasui",
      "Shingo Uto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)"
    ]
  },
  {
    "id": "arXiv:2507.07768",
    "title": "TRIX- Trading Adversarial Fairness via Mixed Adversarial Training",
    "abstract": "           Adversarial Training (AT) is a widely adopted defense against adversarial examples. However, existing approaches typically apply a uniform training objective across all classes, overlooking disparities in class-wise vulnerability. This results in adversarial unfairness: classes with well distinguishable features (strong classes) tend to become more robust, while classes with overlapping or shared features(weak classes) remain disproportionately susceptible to adversarial attacks. We observe that strong classes do not require strong adversaries during training, as their non-robust features are quickly suppressed. In contrast, weak classes benefit from stronger adversaries to effectively reduce their vulnerabilities. Motivated by this, we introduce TRIX, a feature-aware adversarial training framework that adaptively assigns weaker targeted adversaries to strong classes, promoting feature diversity via uniformly sampled targets, and stronger untargeted adversaries to weak classes, enhancing their focused robustness. TRIX further incorporates per-class loss weighting and perturbation strength adjustments, building on prior work, to emphasize weak classes during the optimization. Comprehensive experiments on standard image classification benchmarks, including evaluations under strong attacks such as PGD and AutoAttack, demonstrate that TRIX significantly improves worst-case class accuracy on both clean and adversarial data, reducing inter-class robustness disparities, and preserves overall accuracy. Our results highlight TRIX as a practical step toward fair and effective adversarial defense.         ",
    "url": "https://arxiv.org/abs/2507.07768",
    "authors": [
      "Tejaswini Medi",
      "Steffen Jung",
      "Margret Keuper"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07773",
    "title": "Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors",
    "abstract": "           Image sensors are integral to a wide range of safety- and security-critical systems, including surveillance infrastructure, autonomous vehicles, and industrial automation. These systems rely on the integrity of visual data to make decisions. In this work, we investigate a novel class of electromagnetic signal injection attacks that target the analog domain of image sensors, allowing adversaries to manipulate raw visual inputs without triggering conventional digital integrity checks. We uncover a previously undocumented attack phenomenon on CMOS image sensors: rainbow-like color artifacts induced in images captured by image sensors through carefully tuned electromagnetic interference. We further evaluate the impact of these attacks on state-of-the-art object detection models, showing that the injected artifacts propagate through the image signal processing pipeline and lead to significant mispredictions. Our findings highlight a critical and underexplored vulnerability in the visual perception stack, highlighting the need for more robust defenses against physical-layer attacks in such systems.         ",
    "url": "https://arxiv.org/abs/2507.07773",
    "authors": [
      "Youqian Zhang",
      "Xinyu Ji",
      "Zhihao Wang",
      "Qinhong Jiang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07776",
    "title": "SCOOTER: A Human Evaluation Framework for Unrestricted Adversarial Examples",
    "abstract": "           Unrestricted adversarial attacks aim to fool computer vision models without being constrained by $\\ell_p$-norm bounds to remain imperceptible to humans, for example, by changing an object's color. This allows attackers to circumvent traditional, norm-bounded defense strategies such as adversarial training or certified defense strategies. However, due to their unrestricted nature, there are also no guarantees of norm-based imperceptibility, necessitating human evaluations to verify just how authentic these adversarial examples look. While some related work assesses this vital quality of adversarial attacks, none provide statistically significant insights. This issue necessitates a unified framework that supports and streamlines such an assessment for evaluating and comparing unrestricted attacks. To close this gap, we introduce SCOOTER - an open-source, statistically powered framework for evaluating unrestricted adversarial examples. Our contributions are: $(i)$ best-practice guidelines for crowd-study power, compensation, and Likert equivalence bounds to measure imperceptibility; $(ii)$ the first large-scale human vs. model comparison across 346 human participants showing that three color-space attacks and three diffusion-based attacks fail to produce imperceptible images. Furthermore, we found that GPT-4o can serve as a preliminary test for imperceptibility, but it only consistently detects adversarial examples for four out of six tested attacks; $(iii)$ open-source software tools, including a browser-based task template to collect annotations and analysis scripts in Python and R; $(iv)$ an ImageNet-derived benchmark dataset containing 3K real images, 7K adversarial examples, and over 34K human ratings. Our findings demonstrate that automated vision systems do not align with human perception, reinforcing the need for a ground-truth SCOOTER benchmark.         ",
    "url": "https://arxiv.org/abs/2507.07776",
    "authors": [
      "Dren Fazlija",
      "Monty-Maximilian Z\u00fchlke",
      "Johanna Schrader",
      "Arkadij Orlov",
      "Clara Stein",
      "Iyiola E. Olatunji",
      "Daniel Kudenko"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07781",
    "title": "SURPRISE3D: A Dataset for Spatial Understanding and Reasoning in Complex 3D Scenes",
    "abstract": "           The integration of language and 3D perception is critical for embodied AI and robotic systems to perceive, understand, and interact with the physical world. Spatial reasoning, a key capability for understanding spatial relationships between objects, remains underexplored in current 3D vision-language research. Existing datasets often mix semantic cues (e.g., object name) with spatial context, leading models to rely on superficial shortcuts rather than genuinely interpreting spatial relationships. To address this gap, we introduce S\\textsc{urprise}3D, a novel dataset designed to evaluate language-guided spatial reasoning segmentation in complex 3D scenes. S\\textsc{urprise}3D consists of more than 200k vision language pairs across 900+ detailed indoor scenes from ScanNet++ v2, including more than 2.8k unique object classes. The dataset contains 89k+ human-annotated spatial queries deliberately crafted without object name, thereby mitigating shortcut biases in spatial understanding. These queries comprehensively cover various spatial reasoning skills, such as relative position, narrative perspective, parametric perspective, and absolute distance reasoning. Initial benchmarks demonstrate significant challenges for current state-of-the-art expert 3D visual grounding methods and 3D-LLMs, underscoring the necessity of our dataset and the accompanying 3D Spatial Reasoning Segmentation (3D-SRS) benchmark suite. S\\textsc{urprise}3D and 3D-SRS aim to facilitate advancements in spatially aware AI, paving the way for effective embodied interaction and robotic planning. The code and datasets can be found in this https URL.         ",
    "url": "https://arxiv.org/abs/2507.07781",
    "authors": [
      "Jiaxin Huang",
      "Ziwen Li",
      "Hanlve Zhang",
      "Runnan Chen",
      "Xiao He",
      "Yandong Guo",
      "Wenping Wang",
      "Tongliang Liu",
      "Mingming Gong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.07792",
    "title": "Space-Filling Regularization for Robust and Interpretable Nonlinear State Space Models",
    "abstract": "           The state space dynamics representation is the most general approach for nonlinear systems and often chosen for system identification. During training, the state trajectory can deform significantly leading to poor data coverage of the state space. This can cause significant issues for space-oriented training algorithms which e.g. rely on grid structures, tree partitioning, or similar. Besides hindering training, significant state trajectory deformations also deteriorate interpretability and robustness properties. This paper proposes a new type of space-filling regularization that ensures a favorable data distribution in state space via introducing a data-distribution-based penalty. This method is demonstrated in local model network architectures where good interpretability is a major concern. The proposed approach integrates ideas from modeling and design of experiments for state space structures. This is why we present two regularization techniques for the data point distributions of the state trajectories for local affine state space models. Beyond that, we demonstrate the results on a widely known system identification benchmark.         ",
    "url": "https://arxiv.org/abs/2507.07792",
    "authors": [
      "Hermann Klein",
      "Max Heinz Herkersdorf",
      "Oliver Nelles"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.07795",
    "title": "Robust and Generalizable Heart Rate Estimation via Deep Learning for Remote Photoplethysmography in Complex Scenarios",
    "abstract": "           Non-contact remote photoplethysmography (rPPG) technology enables heart rate measurement from facial videos. However, existing network models still face challenges in accu racy, robustness, and generalization capability under complex scenarios. This paper proposes an end-to-end rPPG extraction network that employs 3D convolutional neural networks to reconstruct accurate rPPG signals from raw facial videos. We introduce a differential frame fusion module that integrates differential frames with original frames, enabling frame-level representations to capture blood volume pulse (BVP) variations. Additionally, we incorporate Temporal Shift Module (TSM) with self-attention mechanisms, which effectively enhance rPPG features with minimal computational overhead. Furthermore, we propose a novel dynamic hybrid loss function that provides stronger supervision for the network, effectively mitigating over fitting. Comprehensive experiments were conducted on not only the PURE and UBFC-rPPG datasets but also the challenging MMPD dataset under complex scenarios, involving both intra dataset and cross-dataset evaluations, which demonstrate the superior robustness and generalization capability of our network. Specifically, after training on PURE, our model achieved a mean absolute error (MAE) of 7.58 on the MMPD test set, outperforming the state-of-the-art models.         ",
    "url": "https://arxiv.org/abs/2507.07795",
    "authors": [
      "Kang Cen",
      "Chang-Hong Fu",
      "Hong Hong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07802",
    "title": "Synergistic Prompting for Robust Visual Recognition with Missing Modalities",
    "abstract": "           Large-scale multi-modal models have demonstrated remarkable performance across various visual recognition tasks by leveraging extensive paired multi-modal training data. However, in real-world applications, the presence of missing or incomplete modality inputs often leads to significant performance degradation. Recent research has focused on prompt-based strategies to tackle this issue; however, existing methods are hindered by two major limitations: (1) static prompts lack the flexibility to adapt to varying missing-data conditions, and (2) basic prompt-tuning methods struggle to ensure reliable performance when critical modalities are this http URL address these challenges, we propose a novel Synergistic Prompting (SyP) framework for robust visual recognition with missing modalities. The proposed SyP introduces two key innovations: (I) a Dynamic Adapter, which computes adaptive scaling factors to dynamically generate prompts, replacing static parameters for flexible multi-modal adaptation, and (II) a Synergistic Prompting Strategy, which combines static and dynamic prompts to balance information across modalities, ensuring robust reasoning even when key modalities are missing. The proposed SyP achieves significant performance improvements over existing approaches across three widely-used visual recognition datasets, demonstrating robustness under diverse missing rates and conditions. Extensive experiments and ablation studies validate its effectiveness in handling missing modalities, highlighting its superior adaptability and reliability.         ",
    "url": "https://arxiv.org/abs/2507.07802",
    "authors": [
      "Zhihui Zhang",
      "Luanyuan Dai",
      "Qika Lin",
      "Yunfeng Diao",
      "Guangyin Jin",
      "Yufei Guo",
      "Jing Zhang",
      "Xiaoshuai Hao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07838",
    "title": "3D-ADAM: A Dataset for 3D Anomaly Detection in Advanced Manufacturing",
    "abstract": "           Surface defects are one of the largest contributors to low yield in the manufacturing sector. Accurate and reliable detection of defects during the manufacturing process is therefore of great value across the sector. State-of-the-art approaches to automated defect detection yield impressive performance on current datasets, yet still fall short in real-world manufacturing settings and developing improved methods relies on large datasets representative of real-world scenarios. Unfortunately, high-quality, high-precision RGB+3D industrial anomaly detection datasets are scarce, and typically do not reflect real-world industrial deployment scenarios. To address this, we introduce 3D-ADAM, the first large-scale industry-relevant dataset for high-precision 3D Anomaly Detection. 3D-ADAM comprises 14,120 high-resolution scans across 217 unique parts, captured using 4 industrial depth imaging sensors. It includes 27,346 annotated defect instances from 12 categories, covering the breadth of industrial surface defects. 3D-ADAM uniquely captures an additional 8,110 annotations of machine element features, spanning the range of relevant mechanical design form factors. Unlike existing datasets, 3D-ADAM is captured in a real industrial environment with variations in part position and orientation, camera positioning, ambient lighting conditions, as well as partial occlusions. Our evaluation of SOTA models across various RGB+3D anomaly detection tasks demonstrates the significant challenge this dataset presents to current approaches. We further validated the industrial relevance and quality of the dataset through an expert labelling survey conducted by industry partners. By providing this challenging benchmark, 3D-ADAM aims to accelerate the development of robust 3D Anomaly Detection models capable of meeting the demands of modern manufacturing environments.         ",
    "url": "https://arxiv.org/abs/2507.07838",
    "authors": [
      "Paul McHard",
      "Florent P. Audonnet",
      "Oliver Summerell",
      "Sebastian Andraos",
      "Paul Henderson",
      "Gerardo Aragon-Camarasa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07839",
    "title": "MeD-3D: A Multimodal Deep Learning Framework for Precise Recurrence Prediction in Clear Cell Renal Cell Carcinoma (ccRCC)",
    "abstract": "           Accurate prediction of recurrence in clear cell renal cell carcinoma (ccRCC) remains a major clinical challenge due to the disease complex molecular, pathological, and clinical heterogeneity. Traditional prognostic models, which rely on single data modalities such as radiology, histopathology, or genomics, often fail to capture the full spectrum of disease complexity, resulting in suboptimal predictive accuracy. This study aims to overcome these limitations by proposing a deep learning (DL) framework that integrates multimodal data, including CT, MRI, histopathology whole slide images (WSI), clinical data, and genomic profiles, to improve the prediction of ccRCC recurrence and enhance clinical decision-making. The proposed framework utilizes a comprehensive dataset curated from multiple publicly available sources, including TCGA, TCIA, and CPTAC. To process the diverse modalities, domain-specific models are employed: CLAM, a ResNet50-based model, is used for histopathology WSIs, while MeD-3D, a pre-trained 3D-ResNet18 model, processes CT and MRI images. For structured clinical and genomic data, a multi-layer perceptron (MLP) is used. These models are designed to extract deep feature embeddings from each modality, which are then fused through an early and late integration architecture. This fusion strategy enables the model to combine complementary information from multiple sources. Additionally, the framework is designed to handle incomplete data, a common challenge in clinical settings, by enabling inference even when certain modalities are missing.         ",
    "url": "https://arxiv.org/abs/2507.07839",
    "authors": [
      "Hasaan Maqsood",
      "Saif Ur Rehman Khan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07845",
    "title": "Perceptual Distortions and Autonomous Representation Learning in a Minimal Robotic System",
    "abstract": "           Autonomous agents, particularly in the field of robotics, rely on sensory information to perceive and navigate their environment. However, these sensory inputs are often imperfect, leading to distortions in the agent's internal representation of the world. This paper investigates the nature of these perceptual distortions and how they influence autonomous representation learning using a minimal robotic system. We utilize a simulated two-wheeled robot equipped with distance sensors and a compass, operating within a simple square environment. Through analysis of the robot's sensor data during random exploration, we demonstrate how a distorted perceptual space emerges. Despite these distortions, we identify emergent structures within the perceptual space that correlate with the physical environment, revealing how the robot autonomously learns a structured representation for navigation without explicit spatial information. This work contributes to the understanding of embodied cognition, minimal agency, and the role of perception in self-generated navigation strategies in artificial life.         ",
    "url": "https://arxiv.org/abs/2507.07845",
    "authors": [
      "David Warutumo",
      "Ciira wa Maina"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.07850",
    "title": "Identifying the Smallest Adversarial Load Perturbations that Render DC-OPF Infeasible",
    "abstract": "           What is the globally smallest load perturbation that renders DC-OPF infeasible? Reliably identifying such \"adversarial attack\" perturbations has useful applications in a variety of emerging grid-related contexts, including machine learning performance verification, cybersecurity, and operational robustness of power systems dominated by stochastic renewable energy resources. In this paper, we formulate the inherently nonconvex adversarial attack problem by applying a parameterized version of Farkas' lemma to a perturbed set of DC-OPF equations. Since the resulting formulation is very hard to globally optimize, we also propose a parameterized generation control policy which, when applied to the primal DC-OPF problem, provides solvability guarantees. Together, these nonconvex problems provide guaranteed upper and lower bounds on adversarial attack size; by combining them into a single optimization problem, we can efficiently \"squeeze\" these bounds towards a common global solution. We apply these methods on a range of small- to medium-sized test cases from PGLib, benchmarking our results against the best adversarial attack lower bounds provided by Gurobi 12.0's spatial Branch and Bound solver.         ",
    "url": "https://arxiv.org/abs/2507.07850",
    "authors": [
      "Samuel Chevalier",
      "William A. Wheeler"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.07854",
    "title": "Credit Risk Analysis for SMEs Using Graph Neural Networks in Supply Chain",
    "abstract": "           Small and Medium-sized Enterprises (SMEs) are vital to the modern economy, yet their credit risk analysis often struggles with scarce data, especially for online lenders lacking direct credit records. This paper introduces a Graph Neural Network (GNN)-based framework, leveraging SME interactions from transaction and social data to map spatial dependencies and predict loan default risks. Tests on real-world datasets from Discover and Ant Credit (23.4M nodes for supply chain analysis, 8.6M for default prediction) show the GNN surpasses traditional and other GNN baselines, with AUCs of 0.995 and 0.701 for supply chain mining and default prediction, respectively. It also helps regulators model supply chain disruption impacts on banks, accurately forecasting loan defaults from material shortages, and offers Federal Reserve stress testers key data for CCAR risk buffers. This approach provides a scalable, effective tool for assessing SME credit risk.         ",
    "url": "https://arxiv.org/abs/2507.07854",
    "authors": [
      "Zizhou Zhang",
      "Qinyan Shen",
      "Zhuohuan Hu",
      "Qianying Liu",
      "Huijie Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07867",
    "title": "Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders",
    "abstract": "           Neural audio codecs and autoencoders have emerged as versatile models for audio compression, transmission, feature-extraction, and latent-space generation. However, a key limitation is that most are trained to maximize reconstruction fidelity, often neglecting the specific latent structure necessary for optimal performance in diverse downstream applications. We propose a simple, post-hoc framework to address this by modifying the bottleneck of a pre-trained autoencoder. Our method introduces a \"Re-Bottleneck\", an inner bottleneck trained exclusively through latent space losses to instill user-defined structure. We demonstrate the framework's effectiveness in three experiments. First, we enforce an ordering on latent channels without sacrificing reconstruction quality. Second, we align latents with semantic embeddings, analyzing the impact on downstream diffusion modeling. Third, we introduce equivariance, ensuring that a filtering operation on the input waveform directly corresponds to a specific transformation in the latent space. Ultimately, our Re-Bottleneck framework offers a flexible and efficient way to tailor representations of neural audio models, enabling them to seamlessly meet the varied demands of different applications with minimal additional training.         ",
    "url": "https://arxiv.org/abs/2507.07867",
    "authors": [
      "Dimitrios Bralios",
      "Jonah Casebeer",
      "Paris Smaragdis"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2507.07871",
    "title": "Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking",
    "abstract": "           Watermarking offers a promising solution for GenAI providers to establish the provenance of their generated content. A watermark is a hidden signal embedded in the generated content, whose presence can later be verified using a secret watermarking key. A threat to GenAI providers are \\emph{watermark stealing} attacks, where users forge a watermark into content that was \\emph{not} generated by the provider's models without access to the secret key, e.g., to falsely accuse the provider. Stealing attacks collect \\emph{harmless} watermarked samples from the provider's model and aim to maximize the expected success rate of generating \\emph{harmful} watermarked samples. Our work focuses on mitigating stealing attacks while treating the underlying watermark as a black-box. Our contributions are: (i) Proposing a multi-key extension to mitigate stealing attacks that can be applied post-hoc to any watermarking method across any modality. (ii) We provide theoretical guarantees and demonstrate empirically that our method makes forging substantially less effective across multiple datasets, and (iii) we formally define the threat of watermark forging as the task of generating harmful, watermarked content and model this threat via security games.         ",
    "url": "https://arxiv.org/abs/2507.07871",
    "authors": [
      "Toluwani Aremu",
      "Noor Hussein",
      "Munachiso Nwadike",
      "Samuele Poppi",
      "Jie Zhang",
      "Karthik Nandakumar",
      "Neil Gong",
      "Nils Lukas"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07872",
    "title": "Improving AEBS Validation Through Objective Intervention Classification Leveraging the Prediction Divergence Principle",
    "abstract": "           The safety validation of automatic emergency braking system (AEBS) requires accurately distinguishing between false positive (FP) and true positive (TP) system activations. While simulations allow straightforward differentiation by comparing scenarios with and without interventions, analyzing activations from open-loop resimulations - such as those from field operational testing (FOT) - is more complex. This complexity arises from scenario parameter uncertainty and the influence of driver interventions in the recorded data. Human labeling is frequently used to address these challenges, relying on subjective assessments of intervention necessity or situational criticality, potentially introducing biases and limitations. This work proposes a rule-based classification approach leveraging the Prediction Divergence Principle (PDP) to address those issues. Applied to a simplified AEBS, the proposed method reveals key strengths, limitations, and system requirements for effective implementation. The findings suggest that combining this approach with human labeling may enhance the transparency and consistency of classification, thereby improving the overall validation process. While the rule set for classification derived in this work adopts a conservative approach, the paper outlines future directions for refinement and broader applicability. Finally, this work highlights the potential of such methods to complement existing practices, paving the way for more reliable and reproducible AEBS validation frameworks.         ",
    "url": "https://arxiv.org/abs/2507.07872",
    "authors": [
      "Daniel Betschinske",
      "Steven Peters"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07885",
    "title": "UnIT: Scalable Unstructured Inference-Time Pruning for MAC-efficient Neural Inference on MCUs",
    "abstract": "           Existing pruning methods are typically applied during training or compile time and often rely on structured sparsity. While compatible with low-power microcontrollers (MCUs), structured pruning underutilizes the opportunity for fine-grained efficiency on devices without SIMD support or parallel compute. To address these limitations, we introduce UnIT (Unstructured Inference-Time pruning), a lightweight method that dynamically identifies and skips unnecessary multiply-accumulate (MAC) operations during inference, guided by input-specific activation patterns. Unlike structured pruning, UnIT embraces irregular sparsity and does not require retraining or hardware specialization. It transforms pruning decisions into lightweight comparisons, replacing multiplications with threshold checks and approximated divisions. UnIT further optimizes compute by reusing threshold computations across multiple connections and applying layer- and group-specific pruning sensitivity. We present three fast, hardware-friendly division approximations tailored to the capabilities of common embedded platforms. Demonstrated on the MSP430 microcontroller, UnIT achieves 11.02% to 82.03% MAC reduction, 27.30% to 84.19% faster inference, and 27.33% to 84.38% lower energy consumption compared to training-time pruned models, while maintaining accuracy with 0.48-7%. Under domain shift, UnIT matches or exceeds the accuracy of retrained models while requiring significantly fewer MACs. These results establish unstructured inference-time pruning as a viable and practical solution for efficient, retraining-free deployment of deep neural networks on MCUs.         ",
    "url": "https://arxiv.org/abs/2507.07885",
    "authors": [
      "Ashe Neth",
      "Sawinder kaur",
      "Mohammad Nur Hossain Khan",
      "Subrata Biswas",
      "Asif Salekin",
      "Bashima Islam"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07893",
    "title": "An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis",
    "abstract": "           The rapid development of artificial intelligence has positioned large language models as fundamental components of intelligent legal systems. However, these models face significant limitations in legal dispute analysis, including insufficient legal knowledge representation, limited concept understanding, and reasoning deficiencies. This research proposes an enhanced framework integrating prompt engineering with multidimensional knowledge graphs. The framework introduces a three-stage hierarchical prompt structure comprising task definition, knowledge background, and reasoning guidance, supplemented by legal-specific reasoning templates and dynamic optimization mechanisms. A three-layer knowledge graph architecture is constructed with legal classification ontology, representation, and instance layers. Four complementary methods enable precise legal concept retrieval: direct legal norm code matching, domain-specific semantic vector similarity, ontology-based path reasoning, and specialized lexical segmentation. These components integrate with web search technology to establish a knowledge-enhanced framework for legal decision-making. Experimental results demonstrate significant performance improvements in legal dispute analysis, enabling accurate legal application analysis for complex cases while exhibiting nuanced understanding of judicial decision-making logic, providing a novel technical approach for implementing intelligent legal assistance systems.         ",
    "url": "https://arxiv.org/abs/2507.07893",
    "authors": [
      "Mingda Zhang",
      "Na Zhao",
      "Jianglong Qing",
      "Qing xu",
      "Kaiwen Pan",
      "Ting luo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07898",
    "title": "Efficient Causal Discovery for Autoregressive Time Series",
    "abstract": "           In this study, we present a novel constraint-based algorithm for causal structure learning specifically designed for nonlinear autoregressive time series. Our algorithm significantly reduces computational complexity compared to existing methods, making it more efficient and scalable to larger problems. We rigorously evaluate its performance on synthetic datasets, demonstrating that our algorithm not only outperforms current techniques, but also excels in scenarios with limited data availability. These results highlight its potential for practical applications in fields requiring efficient and accurate causal inference from nonlinear time series data.         ",
    "url": "https://arxiv.org/abs/2507.07898",
    "authors": [
      "Mohammad Fesanghary",
      "Achintya Gopal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2507.07915",
    "title": "Improving the Price of Anarchy via Predictions in Parallel-Link Networks",
    "abstract": "           We study non-atomic congestion games on parallel-link networks with affine cost functions. We investigate the power of machine-learned predictions in the design of coordination mechanisms aimed at minimizing the impact of selfishness. Our main results demonstrate that enhancing coordination mechanisms with a simple advice on the input rate can optimize the social cost whenever the advice is accurate (consistency), while only incurring minimal losses even when the predictions are arbitrarily inaccurate (bounded robustness). Moreover, we provide a full characterization of the consistent mechanisms that holds for all monotone cost functions, and show that our suggested mechanism is optimal with respect to the robustness. We further explore the notion of smoothness within this context: we extend our mechanism to achieve error-tolerance, i.e. we provide an approximation guarantee that degrades smoothly as a function of the prediction error, up to a predetermined threshold, while achieving a bounded robustness.         ",
    "url": "https://arxiv.org/abs/2507.07915",
    "authors": [
      "George Christodoulou",
      "Vasilis Christoforidis",
      "Alkmini Sgouritsa",
      "Ioannis Vlachos"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2507.07920",
    "title": "ArteryX: Advancing Brain Artery Feature Extraction with Vessel-Fused Networks and a Robust Validation Framework",
    "abstract": "           Cerebrovascular pathology significantly contributes to cognitive decline and neurological disorders, underscoring the need for advanced tools to assess vascular integrity. Three-dimensional Time-of-Flight Magnetic Resonance Angiography (3D TOF MRA) is widely used to visualize cerebral vasculature, however, clinical evaluations generally focus on major arterial abnormalities, overlooking quantitative metrics critical for understanding subtle vascular changes. Existing methods for extracting structural, geometrical and morphological arterial features from MRA - whether manual or automated - face challenges including user-dependent variability, steep learning curves, and lack of standardized quantitative validations. We propose a novel semi-supervised artery evaluation framework, named ArteryX, a MATLAB-based toolbox that quantifies vascular features with high accuracy and efficiency, achieving processing times ~10-15 minutes per subject at 0.5 mm resolution with minimal user intervention. ArteryX employs a vessel-fused network based landmarking approach to reliably track and manage tracings, effectively addressing the issue of dangling/disconnected vessels. Validation on human subjects with cerebral small vessel disease demonstrated its improved sensitivity to subtle vascular changes and better performance than an existing semi-automated method. Importantly, the ArteryX toolbox enables quantitative feature validation by integrating an in-vivo like artery simulation framework utilizing vessel-fused graph nodes and predefined ground-truth features for specific artery types. Thus, the ArteryX framework holds promise for benchmarking feature extraction toolboxes and for seamless integration into clinical workflows, enabling early detection of cerebrovascular pathology and standardized comparisons across patient cohorts to advance understanding of vascular contributions to brain health.         ",
    "url": "https://arxiv.org/abs/2507.07920",
    "authors": [
      "Abrar Faiyaz",
      "Nhat Hoang",
      "Giovanni Schifitto",
      "Md Nasir Uddin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07939",
    "title": "SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment",
    "abstract": "           While Vision-Language Models (VLMs) have shown promising progress in general multimodal tasks, they often struggle in industrial anomaly detection and reasoning, particularly in delivering interpretable explanations and generalizing to unseen categories. This limitation stems from the inherently domain-specific nature of anomaly detection, which hinders the applicability of existing VLMs in industrial scenarios that require precise, structured, and context-aware analysis. To address these challenges, we propose SAGE, a VLM-based framework that enhances anomaly reasoning through Self-Guided Fact Enhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE integrates domain-specific knowledge into visual reasoning via fact extraction and fusion, while E-DPO aligns model outputs with expert preferences using entropy-aware optimization. Additionally, we introduce AD-PL, a preference-optimized dataset tailored for industrial anomaly reasoning, consisting of 28,415 question-answering instances with expert-ranked responses. To evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation (MLE), a quantitative framework analyzing model logic and consistency. SAGE demonstrates superior performance on industrial anomaly datasets under zero-shot and one-shot settings. The code, model and dataset are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.07939",
    "authors": [
      "Guoxin Zang",
      "Xue Li",
      "Donglin Di",
      "Lanshun Nie",
      "Dechen Zhan",
      "Yang Song",
      "Lei Fan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.07947",
    "title": "Low Resource Reconstruction Attacks Through Benign Prompts",
    "abstract": "           The recent advances in generative models such as diffusion models have raised several risks and concerns related to privacy, copyright infringements and data stewardship. To better understand and control the risks, various researchers have created techniques, experiments and attacks that reconstruct images, or part of images, from the training set. While these techniques already establish that data from the training set can be reconstructed, they often rely on high-resources, excess to the training set as well as well-engineered and designed prompts. In this work, we devise a new attack that requires low resources, assumes little to no access to the actual training set, and identifies, seemingly, benign prompts that lead to potentially-risky image reconstruction. This highlights the risk that images might even be reconstructed by an uninformed user and unintentionally. For example, we identified that, with regard to one existing model, the prompt ``blue Unisex T-Shirt'' can generate the face of a real-life human model. Our method builds on an intuition from previous works which leverages domain knowledge and identifies a fundamental vulnerability that stems from the use of scraped data from e-commerce platforms, where templated layouts and images are tied to pattern-like prompts.         ",
    "url": "https://arxiv.org/abs/2507.07947",
    "authors": [
      "Sol Yarkoni",
      "Roi Livni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07975",
    "title": "Finding sparse induced subgraphs on graphs of bounded induced matching treewidth",
    "abstract": "           The induced matching width of a tree decomposition of a graph $G$ is the cardinality of a largest induced matching $M$ of $G$, such that there exists a bag that intersects every edge in $M$. The induced matching treewidth of a graph $G$, denoted by $\\mathsf{tree-}\\mu(G)$, is the minimum induced matching width of a tree decomposition of $G$. The parameter $\\mathsf{tree-}\\mu$ was introduced by Yolov [SODA '18], who showed that, for example, Maximum-Weight Independent Set can be solved in polynomial-time on graphs of bounded $\\mathsf{tree-}\\mu$. Lima, Milani\u010d, Mur\u0161i\u010d, Okrasa, Rz\u0105\u017cewski, and \u0160torgel [ESA '24] conjectured that this algorithm can be generalized to a meta-problem called Maximum-Weight Induced Subgraph of Bounded Treewidth, where we are given a vertex-weighted graph $G$, an integer $w$, and a $\\mathsf{CMSO}_2$-sentence $\\Phi$, and are asked to find a maximum-weight set $X \\subseteq V(G)$ so that $G[X]$ has treewidth at most $w$ and satisfies $\\Phi$. They proved the conjecture for some special cases, such as for the problem Maximum-Weight Induced Forest. In this paper, we prove the general case of the conjecture. In particular, we show that Maximum-Weight Induced Subgraph of Bounded Treewidth is polynomial-time solvable when $\\mathsf{tree-}\\mu(G)$, $w$, and $|\\Phi|$ are bounded. The running time of our algorithm for $n$-vertex graphs $G$ with $\\mathsf{tree} - \\mu(G) \\le k$ is $f(k, w, |\\Phi|) \\cdot n^{O(k w^2)}$ for a computable function $f$.         ",
    "url": "https://arxiv.org/abs/2507.07975",
    "authors": [
      "Hans L. Bodlaender",
      "Fedor V. Fomin",
      "Tuukka Korhonen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2507.07982",
    "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling",
    "abstract": "           Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model's intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view-conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: this https URL.         ",
    "url": "https://arxiv.org/abs/2507.07982",
    "authors": [
      "Haoyu Wu",
      "Diankun Wu",
      "Tianyu He",
      "Junliang Guo",
      "Yang Ye",
      "Yueqi Duan",
      "Jiang Bian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07994",
    "title": "Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection",
    "abstract": "           Keypoint detection, integral to modern machine perception, faces challenges in few-shot learning, particularly when source data from the same distribution as the query is unavailable. This gap is addressed by leveraging sketches, a popular form of human expression, providing a source-free alternative. However, challenges arise in mastering cross-modal embeddings and handling user-specific sketch styles. Our proposed framework overcomes these hurdles with a prototypical setup, combined with a grid-based locator and prototypical domain adaptation. We also demonstrate success in few-shot convergence across novel keypoints and classes through extensive experiments.         ",
    "url": "https://arxiv.org/abs/2507.07994",
    "authors": [
      "Subhajit Maity",
      "Ayan Kumar Bhunia",
      "Subhadeep Koley",
      "Pinaki Nath Chowdhury",
      "Aneeshan Sain",
      "Yi-Zhe Song"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07007",
    "title": "Robust signal decompositions on the circle",
    "abstract": "           We consider the problem of decomposing a piecewise constant function on the circle into a sum of indicator functions of closed circular disks in the plane, whose number and location are not a priori known. This represents a situation where an agent moving on the circle is able to sense its proximity to some landmarks, and the goal is to estimate the number of these landmarks and their possible locations -- which can in turn enable control tasks such as motion planning and obstacle avoidance. Moreover, the exact values of the function at its discontinuities (which correspond to disk boundaries for the individual indicator functions) are not assumed to be known to the agent. We introduce suitable notions of robustness and degrees of freedom to single out those decompositions that are more desirable, or more likely, given this non-precise data collected by the agent. We provide a characterization of robust decompositions and give a procedure for generating all such decompositions. When the given function admits a robust decomposition, we compute the number of possible robust decompositions and derive bounds for the number of decompositions maximizing the degrees of freedom.         ",
    "url": "https://arxiv.org/abs/2507.07007",
    "authors": [
      "Aral Kose",
      "Daniel Liberzon"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.07126",
    "title": "DpDNet: An Dual-Prompt-Driven Network for Universal PET-CT Segmentation",
    "abstract": "           PET-CT lesion segmentation is challenging due to noise sensitivity, small and variable lesion morphology, and interference from physiological high-metabolic signals. Current mainstream approaches follow the practice of one network solving the segmentation of multiple cancer lesions by treating all cancers as a single task. However, this overlooks the unique characteristics of different cancer types. Considering the specificity and similarity of different cancers in terms of metastatic patterns, organ preferences, and FDG uptake intensity, we propose DpDNet, a Dual-Prompt-Driven network that incorporates specific prompts to capture cancer-specific features and common prompts to retain shared knowledge. Additionally, to mitigate information forgetting caused by the early introduction of prompts, prompt-aware heads are employed after the decoder to adaptively handle multiple segmentation tasks. Experiments on a PET-CT dataset with four cancer types show that DpDNet outperforms state-of-the-art models. Finally, based on the segmentation results, we calculated MTV, TLG, and SUVmax for breast cancer survival analysis. The results suggest that DpDNet has the potential to serve as a valuable tool for personalized risk stratification, supporting clinicians in optimizing treatment strategies and improving outcomes. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.07126",
    "authors": [
      "Xinglong Liang",
      "Jiaju Huang",
      "Luyi Han",
      "Tianyu Zhang",
      "Xin Wang",
      "Yuan Gao",
      "Chunyao Lu",
      "Lishan Cai",
      "Tao Tan",
      "Ritse Mann"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07150",
    "title": "Class conditional conformal prediction for multiple inputs by p-value aggregation",
    "abstract": "           Conformal prediction methods are statistical tools designed to quantify uncertainty and generate predictive sets with guaranteed coverage probabilities. This work introduces an innovative refinement to these methods for classification tasks, specifically tailored for scenarios where multiple observations (multi-inputs) of a single instance are available at prediction time. Our approach is particularly motivated by applications in citizen science, where multiple images of the same plant or animal are captured by individuals. Our method integrates the information from each observation into conformal prediction, enabling a reduction in the size of the predicted label set while preserving the required class-conditional coverage guarantee. The approach is based on the aggregation of conformal p-values computed from each observation of a multi-input. By exploiting the exact distribution of these p-values, we propose a general aggregation framework using an abstract scoring function, encompassing many classical statistical tools. Knowledge of this distribution also enables refined versions of standard strategies, such as majority voting. We evaluate our method on simulated and real data, with a particular focus on Pl@ntNet, a prominent citizen science platform that facilitates the collection and identification of plant species through user-submitted images.         ",
    "url": "https://arxiv.org/abs/2507.07150",
    "authors": [
      "Jean-Baptiste Fermanian",
      "Mohamed Hebiri",
      "Joseph Salmon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2507.07159",
    "title": "Large-scale portfolio optimization with variational neural annealing",
    "abstract": "           Portfolio optimization is a routine asset management operation conducted in financial institutions around the world. However, under real-world constraints such as turnover limits and transaction costs, its formulation becomes a mixed-integer nonlinear program that current mixed-integer optimizers often struggle to solve. We propose mapping this problem onto a classical Ising-like Hamiltonian and solving it with Variational Neural Annealing (VNA), via its classical formulation implemented using autoregressive neural networks. We demonstrate that VNA can identify near-optimal solutions for portfolios comprising more than 2,000 assets and yields performance comparable to that of state-of-the-art optimizers, such as Mosek, while exhibiting faster convergence on hard instances. Finally, we present a dynamical finite-size scaling analysis applied to the S&P 500, Russell 1000, and Russell 3000 indices, revealing universal behavior and polynomial annealing time scaling of the VNA algorithm on portfolio optimization problems.         ",
    "url": "https://arxiv.org/abs/2507.07159",
    "authors": [
      "Nishan Ranabhat",
      "Behnam Javanparast",
      "David Goerz",
      "Estelle Inack"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "Portfolio Management (q-fin.PM)"
    ]
  },
  {
    "id": "arXiv:2507.07263",
    "title": "Convergence and Robustness Bounds for Distributed Asynchronous Shortest-Path",
    "abstract": "           This work analyzes convergence times and robustness bounds for asynchronous distributed shortest-path computation. We focus on the Adaptive Bellman--Ford algorithm, a self-stabilizing method in which each agent updates its shortest-path estimate based only on the estimates of its neighbors and forgetting its previous estimate. In the asynchronous framework considered in this paper, agents are allowed to idle or encounter race conditions during their execution of the Adaptive Bellman--Ford algorithm. We build on Lyapunov-based results that develop finite-time convergence and robustness bounds for the synchronous shortest-path setting, in order to produce finite-time convergence and robustness bounds for the asynchronous setting. We also explore robustness against interval-bounded noise processes and establish convergence and robustness guarantees for asynchronous most-probable-path algorithms.         ",
    "url": "https://arxiv.org/abs/2507.07263",
    "authors": [
      "Jared Miller",
      "Mattia Bianchi",
      "Florian D\u00f6rfler"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.07293",
    "title": "Thermodynamic Prediction Enabled by Automatic Dataset Building and Machine Learning",
    "abstract": "           New discoveries in chemistry and materials science, with increasingly expanding volume of requisite knowledge and experimental workload, provide unique opportunities for machine learning (ML) to take critical roles in accelerating research efficiency. Here, we demonstrate (1) the use of large language models (LLMs) for automated literature reviews, and (2) the training of an ML model to predict chemical knowledge (thermodynamic parameters). Our LLM-based literature review tool (LMExt) successfully extracted chemical information and beyond into a machine-readable structure, including stability constants for metal cation-ligand interactions, thermodynamic properties, and other broader data types (medical research papers, and financial reports), effectively overcoming the challenges inherent in each domain. Using the autonomous acquisition of thermodynamic data, an ML model was trained using the CatBoost algorithm for accurately predicting thermodynamic parameters (e.g., enthalpy of formation) of minerals. This work highlights the transformative potential of integrated ML approaches to reshape chemistry and materials science research.         ",
    "url": "https://arxiv.org/abs/2507.07293",
    "authors": [
      "Juejing Liu",
      "Haydn Anderson",
      "Noah I. Waxman",
      "Vsevolod Kovalev",
      "Byron Fisher",
      "Elizabeth Li",
      "Xiaofeng Guo"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07337",
    "title": "An Equivalent Representation of Generalized Differentials",
    "abstract": "           We propose an equivalent formula for the higher-order derivatives used in the study of Generalized Almost Perfect Nonlinear functions over an arbitrary finite field of characteristic $p$. The result is obtained by counting the number of subsets of the prime field with a fixed cardinality for which the sum of their elements is constant. We then ask related questions regarding the diversity of higher-order derivatives.         ",
    "url": "https://arxiv.org/abs/2507.07337",
    "authors": [
      "Valentin Suder"
    ],
    "subjectives": [
      "Number Theory (math.NT)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2507.07339",
    "title": "Benchmarking Waitlist Mortality Prediction in Heart Transplantation Through Time-to-Event Modeling using New Longitudinal UNOS Dataset",
    "abstract": "           Decisions about managing patients on the heart transplant waitlist are currently made by committees of doctors who consider multiple factors, but the process remains largely ad-hoc. With the growing volume of longitudinal patient, donor, and organ data collected by the United Network for Organ Sharing (UNOS) since 2018, there is increasing interest in analytical approaches to support clinical decision-making at the time of organ availability. In this study, we benchmark machine learning models that leverage longitudinal waitlist history data for time-dependent, time-to-event modeling of waitlist mortality. We train on 23,807 patient records with 77 variables and evaluate both survival prediction and discrimination at a 1-year horizon. Our best model achieves a C-Index of 0.94 and AUROC of 0.89, significantly outperforming previous models. Key predictors align with known risk factors while also revealing novel associations. Our findings can support urgency assessment and policy refinement in heart transplant decision making.         ",
    "url": "https://arxiv.org/abs/2507.07339",
    "authors": [
      "Yingtao Luo",
      "Reza Skandari",
      "Carlos Martinez",
      "Arman Kilic",
      "Rema Padman"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07367",
    "title": "Platform for Representation and Integration of multimodal Molecular Embeddings",
    "abstract": "           Existing machine learning methods for molecular (e.g., gene) embeddings are restricted to specific tasks or data modalities, limiting their effectiveness within narrow domains. As a result, they fail to capture the full breadth of gene functions and interactions across diverse biological contexts. In this study, we have systematically evaluated knowledge representations of biomolecules across multiple dimensions representing a task-agnostic manner spanning three major data sources, including omics experimental data, literature-derived text data, and knowledge graph-based representations. To distinguish between meaningful biological signals from chance correlations, we devised an adjusted variant of Singular Vector Canonical Correlation Analysis (SVCCA) that quantifies signal redundancy and complementarity across different data modalities and sources. These analyses reveal that existing embeddings capture largely non-overlapping molecular signals, highlighting the value of embedding integration. Building on this insight, we propose Platform for Representation and Integration of multimodal Molecular Embeddings (PRISME), a machine learning based workflow using an autoencoder to integrate these heterogeneous embeddings into a unified multimodal representation. We validated this approach across various benchmark tasks, where PRISME demonstrated consistent performance, and outperformed individual embedding methods in missing value imputations. This new framework supports comprehensive modeling of biomolecules, advancing the development of robust, broadly applicable multimodal embeddings optimized for downstream biomedical machine learning applications.         ",
    "url": "https://arxiv.org/abs/2507.07367",
    "authors": [
      "Erika Yilin Zheng",
      "Yu Yan",
      "Baradwaj Simha Sankar",
      "Ethan Ji",
      "Steven Swee",
      "Irsyad Adam",
      "Ding Wang",
      "Alexander Russell Pelletier",
      "Alex Bui",
      "Wei Wang",
      "Peipei Ping"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07625",
    "title": "Concentration of measure for non-linear random matrices with applications to neural networks and non-commutative polynomials",
    "abstract": "           We prove concentration inequalities for several models of non-linear random matrices. As corollaries we obtain estimates for linear spectral statistics of the conjugate kernel of neural networks and non-commutative polynomials in (possibly dependent) random matrices.         ",
    "url": "https://arxiv.org/abs/2507.07625",
    "authors": [
      "Rados\u0142aw Adamczak"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.07631",
    "title": "Generic Speech Enhancement with Self-Supervised Representation Space Loss",
    "abstract": "           Single-channel speech enhancement is utilized in various tasks to mitigate the effect of interfering signals. Conventionally, to ensure the speech enhancement performs optimally, the speech enhancement has needed to be tuned for each task. Thus, generalizing speech enhancement models to unknown downstream tasks has been challenging. This study aims to construct a generic speech enhancement front-end that can improve the performance of back-ends to solve multiple downstream tasks. To this end, we propose a novel training criterion that minimizes the distance between the enhanced and the ground truth clean signal in the feature representation domain of self-supervised learning models. Since self-supervised learning feature representations effectively express high-level speech information useful for solving various downstream tasks, the proposal is expected to make speech enhancement models preserve such information. Experimental validation demonstrates that the proposal improves the performance of multiple speech tasks while maintaining the perceptual quality of the enhanced signal.         ",
    "url": "https://arxiv.org/abs/2507.07631",
    "authors": [
      "Hiroshi Sato",
      "Tsubasa Ochiai",
      "Marc Delcroix",
      "Takafumi Moriya",
      "Takanori Ashihara",
      "Ryo Masumura"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:1905.09226",
    "title": "Boundary Learning by Using Weighted Propagation in Convolution Network",
    "abstract": "           In material science, image segmentation is of great significance for quantitative analysis of microstructures. Here, we propose a novel Weighted Propagation Convolution Neural Network based on U-Net (WPU-Net) to detect boundary in poly-crystalline microscopic images. We introduce spatial consistency into network to eliminate the defects in raw microscopic image. And we customize adaptive boundary weight for each pixel in each grain, so that it leads the network to preserve grain's geometric and topological characteristics. Moreover, we provide our dataset with the goal of advancing the development of image processing in materials science. Experiments demonstrate that the proposed method achieves promising performance in both of objective and subjective assessment. In boundary detection task, it reduces the error rate by 7\\%, which outperforms state-of-the-art methods by a large margin.         ",
    "url": "https://arxiv.org/abs/1905.09226",
    "authors": [
      "Wei Liu",
      "Jiahao Chen",
      "Chuni Liu",
      "Xiaojuan Ban",
      "Boyuan Ma",
      "Hao Wang",
      "Weihua Xue",
      "Yu Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2303.01803",
    "title": "Uncertainty-Aware Gradient Stabilization for Small Object Detection",
    "abstract": "           Despite advances in generic object detection, there remains a performance gap in detecting small objects compared to normal-scale objects. We reveal that conventional object localization methods suffer from gradient instability in small objects due to sharper loss curvature, leading to a convergence challenge. To address the issue, we propose Uncertainty-Aware Gradient Stabilization (UGS), a framework that reformulates object localization as a classification task to stabilize gradients. UGS quantizes continuous labels into interval non-uniform discrete representations. Under a classification-based objective, the localization branch generates bounded and confidence-driven gradients, mitigating instability. Furthermore, UGS integrates an uncertainty minimization (UM) loss that reduces prediction variance and an uncertainty-guided refinement (UR) module that identifies and refines high-uncertainty regions via perturbations. Evaluated on four benchmarks, UGS consistently improves anchor-based, anchor-free, and leading small object detectors. Especially, UGS enhances DINO-5scale by 2.6 AP on VisDrone, surpassing previous state-of-the-art results.         ",
    "url": "https://arxiv.org/abs/2303.01803",
    "authors": [
      "Huixin Sun",
      "Yanjing Li",
      "Linlin Yang",
      "Xianbin Cao",
      "Baochang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2304.13431",
    "title": "Implicit Counterfactual Data Augmentation for Robust Learning",
    "abstract": "           Machine learning models are prone to capturing the spurious correlations between non-causal attributes and classes, with counterfactual data augmentation being a promising direction for breaking these spurious associations. However, generating counterfactual data explicitly poses a challenge, and incorporating augmented data into the training process decreases training efficiency. This study proposes an Implicit Counterfactual Data Augmentation (ICDA) method to remove spurious correlations and make stable predictions. Specifically, first, a novel sample-wise augmentation strategy is developed that generates semantically and counterfactually meaningful deep features with distinct augmentation strength for each sample. Second, we derive an easy-to-compute surrogate loss on the augmented feature set when the number of augmented samples becomes infinite. Third, two concrete schemes are proposed, including direct quantification and meta-learning, to derive the key parameters for the robust loss. In addition, ICDA is explained from a regularization perspective, revealing its capacity to improve intra-class compactness and augment margins at both class and sample levels. Extensive experiments have been conducted across various biased learning scenarios covering both image and text datasets, demonstrating that ICDA consistently enhances the generalization and robustness performance of popular networks.         ",
    "url": "https://arxiv.org/abs/2304.13431",
    "authors": [
      "Xiaoling Zhou",
      "Ou Wu",
      "Michael K. Ng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.13651",
    "title": "Adversarial Defenses via Vector Quantization",
    "abstract": "           Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...         ",
    "url": "https://arxiv.org/abs/2305.13651",
    "authors": [
      "Zhiyi Dong",
      "Yongyi Mao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.01364",
    "title": "Improving Cross-lingual Representation for Semantic Retrieval with Code-switching",
    "abstract": "           Semantic Retrieval (SR) has become an indispensable part of the FAQ system in the task-oriented question-answering (QA) dialogue scenario. The demands for a cross-lingual smart-customer-service system for an e-commerce platform or some particular business conditions have been increasing recently. Most previous studies exploit cross-lingual pre-trained models (PTMs) for multi-lingual knowledge retrieval directly, while some others also leverage the continual pre-training before fine-tuning PTMs on the downstream tasks. However, no matter which schema is used, the previous work ignores to inform PTMs of some features of the downstream task, i.e. train their PTMs without providing any signals related to SR. To this end, in this work, we propose an Alternative Cross-lingual PTM for SR via code-switching. We are the first to utilize the code-switching approach for cross-lingual SR. Besides, we introduce the novel code-switched continual pre-training instead of directly using the PTMs on the SR tasks. The experimental results show that our proposed approach consistently outperforms the previous SOTA methods on SR and semantic textual similarity (STS) tasks with three business corpora and four open datasets in 20+ languages.         ",
    "url": "https://arxiv.org/abs/2403.01364",
    "authors": [
      "Mieradilijiang Maimaiti",
      "Yuanhang Zheng",
      "Ji Zhang",
      "Yue Zhang",
      "Wenpei Luo",
      "Kaiyu Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2403.13268",
    "title": "Unifews: You Need Fewer Operations for Efficient Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNNs) have shown promising performance, but at the cost of resource-intensive operations on graph-scale matrices. To reduce computational overhead, previous studies attempt to sparsify the graph or network parameters, but with limited flexibility and precision boundaries. In this work, we propose Unifews, a joint sparsification technique to unify graph and weight matrix operations and enhance GNN learning efficiency. The Unifews design enables adaptive compression across GNN layers with progressively increased sparsity, and is applicable to a variety of architectures with on-the-fly simplification. Theoretically, we establish a novel framework to characterize sparsified GNN learning in view of the graph optimization process, showing that Unifews effectively approximates the learning objective with bounded error and reduced computational overhead. Extensive experiments demonstrate that Unifews achieves efficiency improvements with comparable or better accuracy, including 10-20x matrix operation reduction and up to 100x acceleration on graphs up to billion-edge scale.         ",
    "url": "https://arxiv.org/abs/2403.13268",
    "authors": [
      "Ningyi Liao",
      "Zihao Yu",
      "Ruixiao Zeng",
      "Siqiang Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2404.00699",
    "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models",
    "abstract": "           With the rise of Large Language Models (LLMs) in recent years, abundant new opportunities are emerging, but also new challenges, among which contamination is quickly becoming critical. Business applications and fundraising in Artificial Intelligence (AI) have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a major issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes real capability improvement in the field of NLP, yet, there remains a lack of methods on how to efficiently detect contamination. In this paper, we survey all recent work on contamination detection with LLMs, analyzing their methodologies and use cases to shed light on the appropriate usage of contamination detection methods. Our work calls the NLP research community's attention into systematically taking into account contamination bias in LLM evaluation.         ",
    "url": "https://arxiv.org/abs/2404.00699",
    "authors": [
      "Mathieu Ravaut",
      "Bosheng Ding",
      "Fangkai Jiao",
      "Hailin Chen",
      "Xingxuan Li",
      "Ruochen Zhao",
      "Chengwei Qin",
      "Caiming Xiong",
      "Shafiq Joty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2405.17556",
    "title": "Solving Probabilistic Verification Problems of Neural Networks using Branch and Bound",
    "abstract": "           Probabilistic verification problems of neural networks are concerned with formally analysing the output distribution of a neural network under a probability distribution of the inputs. Examples of probabilistic verification problems include verifying the demographic parity fairness notion or quantifying the safety of a neural network. We present a new algorithm for solving probabilistic verification problems of neural networks based on an algorithm for computing and iteratively refining lower and upper bounds on probabilities over the outputs of a neural network. By applying state-of-the-art bound propagation and branch and bound techniques from non-probabilistic neural network verification, our algorithm significantly outpaces existing probabilistic verification algorithms, reducing solving times for various benchmarks from the literature from tens of minutes to tens of seconds. Furthermore, our algorithm compares favourably even to dedicated algorithms for restricted probabilistic verification problems. We complement our empirical evaluation with a theoretical analysis, proving that our algorithm is sound and, under mildly restrictive conditions, also complete when using a suitable set of heuristics.         ",
    "url": "https://arxiv.org/abs/2405.17556",
    "authors": [
      "David Boetius",
      "Stefan Leue",
      "Tobias Sutter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.10427",
    "title": "Adaptive Randomized Smoothing: Certified Adversarial Robustness for Multi-Step Defences",
    "abstract": "           We propose Adaptive Randomized Smoothing (ARS) to certify the predictions of our test-time adaptive models against adversarial examples. ARS extends the analysis of randomized smoothing using $f$-Differential Privacy to certify the adaptive composition of multiple steps. For the first time, our theory covers the sound adaptive composition of general and high-dimensional functions of noisy inputs. We instantiate ARS on deep image classification to certify predictions against adversarial examples of bounded $L_{\\infty}$ norm. In the $L_{\\infty}$ threat model, ARS enables flexible adaptation through high-dimensional input-dependent masking. We design adaptivity benchmarks, based on CIFAR-10 and CelebA, and show that ARS improves standard test accuracy by $1$ to $15\\%$ points. On ImageNet, ARS improves certified test accuracy by up to $1.6\\%$ points over standard RS without adaptivity. Our code is available at this https URL .         ",
    "url": "https://arxiv.org/abs/2406.10427",
    "authors": [
      "Saiyue Lyu",
      "Shadab Shaikh",
      "Frederick Shpilevskiy",
      "Evan Shelhamer",
      "Mathias L\u00e9cuyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2406.14514",
    "title": "Solving a Stackelberg Game on Transportation Networks in a Dynamic Crime Scenario: A Mixed Approach on Multi-Layer Networks",
    "abstract": "           Interdicting a criminal with limited police resources is a challenging task as the criminal changes location over time. The size of the large transportation network further adds to the difficulty of this scenario. To tackle this issue, we consider the concept of a layered graph. At each time stamp, we create a copy of the entire transportation network to track the possible movements of both players, the attacker and the defenders. We consider a Stackelberg game in a dynamic crime scenario where the attacker changes location over time while the defenders attempt to interdict the attacker on his escape route. Given a set of defender strategies, the optimal attacker strategy is determined by applying Dijkstra's algorithm on the layered networks. Here, the attacker aims to minimize while the defenders aim to maximize the probability of interdiction. We develop an approximation algorithm on the layered networks to find near-optimal strategy for defenders. The efficacy of the developed approach is compared with the adopted MILP approach. We compare the results in terms of computational time and solution quality. The quality of the results demonstrates the need for the developed approach, as it effectively solves the complex problem within a short amount of time.         ",
    "url": "https://arxiv.org/abs/2406.14514",
    "authors": [
      "Sukanya Samanta",
      "Kei Kimura",
      "Makoto Yokoo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.17070",
    "title": "Curriculum Negative Mining For Temporal Networks",
    "abstract": "           Temporal networks are effective in capturing the evolving interactions of networks over time, such as social networks and e-commerce networks. In recent years, researchers have primarily concentrated on developing specific model architectures for Temporal Graph Neural Networks (TGNNs) in order to improve the representation quality of temporal nodes and edges. However, limited attention has been given to the quality of negative samples during the training of TGNNs. When compared with static networks, temporal networks present two specific challenges for negative sampling: positive sparsity and positive shift. Positive sparsity refers to the presence of a single positive sample amidst numerous negative samples at each timestamp, while positive shift relates to the variations in positive samples across different timestamps. To robustly address these challenges in training TGNNs, we introduce Curriculum Negative Mining (CurNM), a model-aware curriculum learning framework that adaptively adjusts the difficulty of negative samples. Within this framework, we first establish a dynamically updated negative pool that balances random, historical, and hard negatives to address the challenges posed by positive sparsity. Secondly, we implement a temporal-aware negative selection module that focuses on learning from the disentangled factors of recently active edges, thus accurately capturing shifting preferences. Finally, the selected negatives are combined with annealing random negatives to support stable training. Extensive experiments on 12 datasets and 3 TGNNs demonstrate that our method outperforms baseline methods by a significant margin. Additionally, thorough ablation studies and parameter sensitivity experiments verify the usefulness and robustness of our approach.         ",
    "url": "https://arxiv.org/abs/2407.17070",
    "authors": [
      "Ziyue Chen",
      "Tongya Zheng",
      "Mingli Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.17947",
    "title": "Supercritical Size-Width Tree-Like Resolution Trade-Offs for Graph Isomorphism",
    "abstract": "           We study the refutation complexity of graph isomorphism in the tree-like resolution calculus. Tor\u00e1n and W\u00f6rz (TOCL 2023) showed that there is a resolution refutation of narrow width $k$ for two graphs if and only if they can be distinguished in ($k+1$)-variable first-order logic (FO$^{k+1}$) and hence by a count-free variant of the $k$-dimensional Weisfeiler-Leman algorithm. While DAG-like narrow width $k$ resolution refutations have size at most $n^k$, tree-like refutations may be much larger. We show that there are graphs of order n, whose isomorphism can be refuted in narrow width $k$ but only in tree-like size $2^{\\Omega(n^{k/2})}$. This is a supercritical trade-off where bounding one parameter (the narrow width) causes the other parameter (the size) to grow above its worst case. The size lower bound is super-exponential in the formula size and improves a related supercritical width versus tree-like size trade-off by Razborov (JACM 2016). To prove our result, we develop a new variant of the $k$-pebble EF-game for FO$^k$ to reason about tree-like refutation size in a similar way as the Prover-Delayer games in proof complexity. We analyze this game on a modified variant of the compressed CFI graphs introduced by Grohe, Lichter, Neuen, and Schweitzer (FOCS 2023). Using a recent improved robust compressed CFI construction of Janett, Nordstr\u00f6m, and Pang (unpublished manuscript), we obtain a similar bound for width $k$ (instead of the stronger but less common narrow width) and make the result more robust.         ",
    "url": "https://arxiv.org/abs/2407.17947",
    "authors": [
      "Christoph Berkholz",
      "Moritz Lichter",
      "Harry Vinall-Smeeth"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2408.07517",
    "title": "Advancing Spatio-Temporal Processing in Spiking Neural Networks through Adaptation",
    "abstract": "           Implementations of spiking neural networks on neuromorphic hardware promise orders of magnitude less power consumption than their non-spiking counterparts. The standard neuron model for spike-based computation on such systems has long been the leaky integrate-and-fire (LIF) neuron. A computationally light augmentation of the LIF neuron model with an adaptation mechanism has recently been shown to exhibit superior performance on spatio-temporal processing tasks. The root of the superiority of these so-called adaptive LIF neurons however is not well understood. In this article, we thoroughly analyze the dynamical, computational, and learning properties of adaptive LIF neurons and networks thereof. Our investigation reveals significant challenges related to stability and parameterization when employing the conventional Euler-Forward discretization for this class of models. We report a rigorous theoretical and empirical demonstration that these challenges can be effectively addressed by adopting an alternative discretization approach - the Symplectic Euler method, allowing to improve over state-of-the-art performances on common event-based benchmark datasets. Our further analysis of the computational properties of networks of adaptive LIF neurons shows that they are particularly well suited to exploit the spatio-temporal structure of input sequences without any normalization techniques.         ",
    "url": "https://arxiv.org/abs/2408.07517",
    "authors": [
      "Maximilian Baronig",
      "Romain Ferrand",
      "Silvester Sabathiel",
      "Robert Legenstein"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2408.12246",
    "title": "RT-OVAD: Real-Time Open-Vocabulary Aerial Object Detection via Image-Text Collaboration",
    "abstract": "           Aerial object detection plays a crucial role in numerous applications. However, most existing methods focus on detecting predefined object categories, limiting their applicability in real-world open scenarios. In this paper, we extend aerial object detection to open scenarios through image-text collaboration and propose RT-OVAD, the first real-time open-vocabulary detector for aerial scenes. Specifically, we first introduce an image-to-text alignment loss to replace the conventional category regression loss, thereby eliminating category constraints. Next, we propose a lightweight image-text collaboration strategy comprising an image-text collaboration encoder and a text-guided decoder. The encoder simultaneously enhances visual features and refines textual embeddings, while the decoder guides object queries to focus on class-relevant image features. This design further improves detection accuracy without incurring significant computational overhead. Extensive experiments demonstrate that RT-OVAD consistently outperforms existing state-of-the-art methods across open-vocabulary, zero-shot, and traditional closed-set detection tasks. For instance, on the open-vocabulary aerial detection benchmarks DIOR, DOTA-v2.0, and LAE-80C, RT-OVAD achieves 87.7 AP$_{50}$, 53.8 mAP, and 23.7 mAP, respectively, surpassing the previous state-of-the-art (LAE-DINO) by 2.2, 7.0, and 3.5 points. In addition, RT-OVAD achieves an inference speed of 34 FPS on an RTX 4090 GPU, approximately three times faster than LAE-DINO (10 FPS), meeting the real-time detection requirements of diverse applications. The code will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2408.12246",
    "authors": [
      "Guoting Wei",
      "Xia Yuan",
      "Yu Liu",
      "Zhenhao Shang",
      "Xizhe Xue",
      "Peng Wang",
      "Kelu Yao",
      "Chunxia Zhao",
      "Haokui Zhang",
      "Rong Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.07414",
    "title": "Bayes-Nash Generative Privacy Against Membership Inference Attacks",
    "abstract": "           Membership inference attacks (MIAs) pose significant privacy risks by determining whether individual data is in a dataset. While differential privacy (DP) mitigates these risks, it has limitations including limited resolution in expressing privacy-utility tradeoffs and intractable sensitivity calculations for tight guarantees. We propose a game-theoretic framework modeling privacy protection as a Bayesian game between defender and attacker, where privacy loss corresponds to the attacker's membership inference ability. To address strategic complexity, we represent the defender's mixed strategy as a neural network generator mapping private datasets to public representations (e.g., noisy statistics) and the attacker's strategy as a discriminator making membership claims. This \\textit{general-sum Generative Adversarial Network} trains iteratively through alternating updates, yielding \\textit{Bayes-Nash Generative Privacy (BNGP)} strategies. BNGP avoids worst-case privacy proofs such as sensitivity calculations, supports correlated mechanism compositions, handles heterogeneous attacker preferences. Empirical studies on sensitive dataset summary statistics show our approach significantly outperforms state-of-the-art methods by generating stronger attacks and achieving better privacy-utility tradeoffs.         ",
    "url": "https://arxiv.org/abs/2410.07414",
    "authors": [
      "Tao Zhang",
      "Rajagopal Venkatesaramani",
      "Rajat K. De",
      "Bradley A. Malin",
      "Yevgeniy Vorobeychik"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2410.11719",
    "title": "Adaptive Graph Integration for Cross-Domain Recommendation via Heterogeneous Graph Coordinators",
    "abstract": "           In the digital era, users typically interact with diverse items across multiple domains (e.g., e-commerce, streaming platforms, and social networks), generating intricate heterogeneous interaction graphs. Leveraging multi-domain data can improve recommendation systems by enriching user insights and mitigating data sparsity in individual domains. However, integrating such multi-domain knowledge for cross-domain recommendation remains challenging due to inherent disparities in user behavior and item characteristics and the risk of negative transfer, where irrelevant or conflicting information from the source domains adversely impacts the target domain's performance. To tackle these challenges, we propose HAGO, a novel framework with \\textbf{H}eterogeneous \\textbf{A}daptive \\textbf{G}raph co\\textbf{O}rdinators, which dynamically integrates multi-domain graphs into a cohesive structure. HAGO adaptively adjusts the connections between coordinators and multi-domain graph nodes to enhance beneficial inter-domain interactions while alleviating negative transfer. Furthermore, we introduce a universal multi-domain graph pre-training strategy alongside HAGO to collaboratively learn high-quality node representations across domains. Being compatible with various graph-based models and pre-training techniques, HAGO demonstrates broad applicability and effectiveness. Extensive experiments show that our framework outperforms state-of-the-art methods in cross-domain recommendation scenarios, underscoring its potential for real-world applications. The source code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.11719",
    "authors": [
      "Hengyu Zhang",
      "Chunxu Shen",
      "Xiangguo Sun",
      "Jie Tan",
      "Yu Rong",
      "Chengzhi Piao",
      "Hong Cheng",
      "Lingling Yi"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2410.19969",
    "title": "A quantum graph FFT with applications to partial differential equations on networks",
    "abstract": "           The Fast Fourier Transform is extended to functions on finite graphs whose edges are identified with intervals of finite length. Spectral and pseudospectral methods are developed to solve a wide variety of time dependent partial differential equations on domains which are modeled as networks of one dimensional segments joined at nodes.         ",
    "url": "https://arxiv.org/abs/2410.19969",
    "authors": [
      "Robert Carlson"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2410.20160",
    "title": "Vibration-based damage detection of a trainer jet via multiple input tangential interpolation",
    "abstract": "           Control engineering is a highly developed field, which includes similarly advanced areas like system identification. In structural dynamics, system identification methods are employed for the extraction of modal parameters, such as natural frequencies and mode shapes, from any structure. In turn, these are the main building blocks of vibration-based damage detection. However, traditional comparisons of these parameters are often ambiguous in complex systems, complicating damage detection and assessment. The modified total modal assurance criterion (MTMAC), a metric well-known in the field of finite element model updating, is extended to address this challenge and is proposed as a metric for damage identification and severity assessment. To support the requirement for precise and robust modal identification of Structural Health Monitoring (SHM), the improved Loewner Framework (iLF), known for its reliability and computational performance, is pioneeringly employed within SHM. Since the MTMAC is proposed solely as a damage identification and severity assessment metric, the coordinate modal assurance criterion (COMAC), also a well-established tool, but for damage localisation using mode shapes, is used for completeness. The iLF SHM capabilities are validated through comparisons with traditional methods, including least-squares complex exponential (LSCE) and stochastic subspace identification with canonical variate analysis (SSI-CVA) on a numerical case study of a cantilever beam. Furthermore, the MTMAC is validated against the traditional vibration-based approach, which involves directly comparing natural frequencies and mode shapes. Finally, an experimental dataset from a BAE Systems Hawk T1A trainer jet ground vibration tests is used to demonstrate the iLF and MTMAC capabilities on a real-life, real-size SHM problem, showing their effectiveness in detecting and assessing damage.         ",
    "url": "https://arxiv.org/abs/2410.20160",
    "authors": [
      "Gabriele Dessena",
      "Marco Civera",
      "Andr\u00e9s Marcos",
      "Bernardino Chiaia",
      "Oscar E. Bonilla-Manrique"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.00461",
    "title": "A Multi-Granularity Supervised Contrastive Framework for Remaining Useful Life Prediction of Aero-engines",
    "abstract": "           Accurate remaining useful life (RUL) predictions are critical to the safe operation of aero-engines. Currently, the RUL prediction task is mainly a regression paradigm with only mean square error as the loss function and lacks research on feature space structure, the latter of which has shown excellent performance in a large number of studies. This paper develops a multi-granularity supervised contrastive (MGSC) framework from plain intuition that samples with the same RUL label should be aligned in the feature space, and address the problems of too large minibatch size and unbalanced samples in the implementation. The RUL prediction with MGSC is implemented on using the proposed multi-phase training strategy. This paper also demonstrates a simple and scalable basic network structure and validates the proposed MGSC strategy on the CMPASS dataset using a convolutional long short-term memory network as a baseline, which effectively improves the accuracy of RUL prediction.         ",
    "url": "https://arxiv.org/abs/2411.00461",
    "authors": [
      "Zixuan He",
      "Ziqian Kong",
      "Zhengyu Chen",
      "Yuling Zhan",
      "Zijun Que",
      "Zhengguo Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.01077",
    "title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection",
    "abstract": "           Jailbreaking techniques trick Large Language Models (LLMs) into producing restricted output, posing a potential threat. One line of defense is to use another LLM as a Judge to evaluate the harmfulness of generated text. However, we reveal that these Judge LLMs are vulnerable to token segmentation bias, an issue that arises when delimiters alter the tokenization process, splitting words into smaller sub-tokens. This alters the embeddings of the entire sequence, reducing detection accuracy and allowing harmful content to be misclassified as safe. In this paper, we introduce Emoji Attack, a novel strategy that amplifies existing jailbreak prompts by exploiting token segmentation bias. Our method leverages in-context learning to systematically insert emojis into text before it is evaluated by a Judge LLM, inducing embedding distortions that significantly lower the likelihood of detecting unsafe content. Unlike traditional delimiters, emojis also introduce semantic ambiguity, making them particularly effective in this attack. Through experiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack substantially reduces the unsafe prediction rate, bypassing existing safeguards.         ",
    "url": "https://arxiv.org/abs/2411.01077",
    "authors": [
      "Zhipeng Wei",
      "Yuqi Liu",
      "N. Benjamin Erichson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.07907",
    "title": "Diffusion of complex contagions is shaped by a trade-off between reach and reinforcement",
    "abstract": "           How does social network structure amplify or stifle behavior diffusion? Existing theory suggests that when social reinforcement makes the adoption of behavior more likely, it should spread more -- both farther and faster -- on clustered networks with redundant ties. Conversely, if adoption does not benefit from social reinforcement, it should spread more on random networks which avoid such redundancies. We develop a novel model of behavior diffusion with tunable probabilistic adoption and social reinforcement parameters to systematically evaluate the conditions under which clustered networks spread behavior better than random networks. Using simulations and analytical methods, we identify precise boundaries in the parameter space where one network type outperforms the other or they perform equally. We find that, in most cases, random networks spread behavior as far or farther than clustered networks, even when social reinforcement increases adoption. Although we find that probabilistic, socially reinforced behaviors can spread farther on clustered networks in some cases, this is not the dominant pattern. Clustered networks are even less advantageous when individuals remain influential for longer after adopting, have more neighbors, or need more neighbors before social reinforcement takes effect. Under such conditions, clustering tends to help only when adoption is nearly deterministic, which is not representative of socially reinforced behaviors more generally. Clustered networks outperform random networks by a 5% margin in only 22% of the parameter space under its most favorable conditions. This pattern reflects a fundamental tradeoff: random ties enhance reach, while clustered ties enhance social reinforcement.         ",
    "url": "https://arxiv.org/abs/2411.07907",
    "authors": [
      "Allison Wan",
      "Christoph Riedl",
      "David Lazer"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2412.12187",
    "title": "Random walk based snapshot clustering for detecting community dynamics in temporal networks",
    "abstract": "           The evolution of many dynamical systems that describe relationships or interactions between objects can be effectively modeled by temporal networks, which are typically represented as a sequence of static network snapshots. In this paper, we introduce a novel random walk-based approach that can identify clusters of time-snapshots in which network community structures are stable. This allows us to detect significant structural shifts over time, such as the splitting or merging of communities or their births and deaths. We also provide a low-dimensional representation of entire snapshots, placing those with similar community structure close to each other in the feature space. To validate our approach, we develop an agent-based algorithm that generates synthetic datasets with the desired characteristic properties, enabling thorough testing and benchmarking. We further demonstrate the effectiveness and broad applicability of our technique by testing it on various social dynamics models and real-world datasets and comparing its performance to several state-of-the-art algorithms. Our findings highlight the strength of our approach to correctly capture and analyze the dynamics of complex systems.         ",
    "url": "https://arxiv.org/abs/2412.12187",
    "authors": [
      "Filip Bla\u0161kovi\u0107",
      "Tim O. F. Conrad",
      "Stefan Klus",
      "Nata\u0161a Djurdjevac Conrad"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2502.00718",
    "title": "\"I am bad\": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models",
    "abstract": "           The rise of multimodal large language models has introduced innovative human-machine interaction paradigms but also significant challenges in machine learning safety. Audio-Language Models (ALMs) are especially relevant due to the intuitive nature of spoken communication, yet little is known about their failure modes. This paper explores audio jailbreaks targeting ALMs, focusing on their ability to bypass alignment mechanisms. We construct adversarial perturbations that generalize across prompts, tasks, and even base audio samples, demonstrating the first universal jailbreaks in the audio modality, and show that these remain effective in simulated real-world conditions. Beyond demonstrating attack feasibility, we analyze how ALMs interpret these audio adversarial examples and reveal them to encode imperceptible first-person toxic speech - suggesting that the most effective perturbations for eliciting toxic outputs specifically embed linguistic features within the audio signal. These results have important implications for understanding the interactions between different modalities in multimodal models, and offer actionable insights for enhancing defenses against adversarial audio attacks.         ",
    "url": "https://arxiv.org/abs/2502.00718",
    "authors": [
      "Isha Gupta",
      "David Khachaturov",
      "Robert Mullins"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2502.03023",
    "title": "Parametric Scaling Law of Tuning Bias in Conformal Prediction",
    "abstract": "           Conformal prediction is a popular framework of uncertainty quantification that constructs prediction sets with coverage guarantees. To uphold the exchangeability assumption, many conformal prediction methods necessitate an additional holdout set for parameter tuning. Yet, the impact of violating this principle on coverage remains underexplored, making it ambiguous in practical applications. In this work, we empirically find that the tuning bias - the coverage gap introduced by leveraging the same dataset for tuning and calibration, is negligible for simple parameter tuning in many conformal prediction methods. In particular, we observe the scaling law of the tuning bias: this bias increases with parameter space complexity and decreases with calibration set size. Formally, we establish a theoretical framework to quantify the tuning bias and provide rigorous proof for the scaling law of the tuning bias by deriving its upper bound. In the end, we discuss how to reduce the tuning bias, guided by the theories we developed.         ",
    "url": "https://arxiv.org/abs/2502.03023",
    "authors": [
      "Hao Zeng",
      "Kangdao Liu",
      "Bingyi Jing",
      "Hongxin Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2502.04057",
    "title": "Smart IoT Security: Lightweight Machine Learning Techniques for Multi-Class Attack Detection in IoT Networks",
    "abstract": "           The Internet of Things (IoT) is expanding at an accelerated pace, making it critical to have secure networks to mitigate a variety of cyber threats. This study addresses the limitation of multi-class attack detection of IoT devices and presents new machine learning-based lightweight ensemble methods that exploit its strong machine learning framework. We used a dataset entitled CICIoT 2023, which has a total of 34 different attack types categorized into 10 categories, and methodically assessed the performance of a substantial array of current machine learning techniques in our goal to identify the best-performing algorithmic choice for IoT application protection. In this work, we focus on ML classifier-based methods to address the biocharges presented by the difficult and heterogeneous properties of the attack vectors in IoT ecosystems. The best-performing method was the Decision Tree, achieving 99.56% accuracy and 99.62% F1, indicating this model is capable of detecting threats accurately and reliably. The Random Forest model also performed nearly as well, with an accuracy of 98.22% and an F1 score of 98.24%, indicating that ML methods excel in a scenario of high-dimensional data. These findings emphasize the promise of integrating ML classifiers into the protective defenses of IoT devices and provide motivations for pursuing subsequent studies towards scalable, keystroke-based attack detection frameworks. We think that our approach offers a new avenue for constructing complex machine learning algorithms for low-resource IoT devices that strike a balance between accuracy requirements and time efficiency. In summary, these contributions expand and enhance the knowledge of the current IoT security literature, establishing a solid baseline and framework for smart, adaptive security to be used in IoT environments.         ",
    "url": "https://arxiv.org/abs/2502.04057",
    "authors": [
      "Shahran Rahman Alve",
      "Muhammad Zawad Mahmud",
      "Samiha Islam",
      "Md. Asaduzzaman Chowdhury",
      "Jahirul Islam"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.13451",
    "title": "MapNav: A Novel Memory Representation via Annotated Semantic Maps for Vision-and-Language Navigation",
    "abstract": "           Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring agents to navigate diverse and unseen environments while following natural language instructions. Traditional approaches rely heavily on historical observations as spatio-temporal contexts for decision making, leading to significant storage and computational overhead. In this paper, we introduce MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map (ASM) to replace historical frames. Specifically, our approach constructs a top-down semantic map at the start of each episode and update it at each timestep, allowing for precise object mapping and structured navigation information. Then, we enhance this map with explicit textual labels for key regions, transforming abstract semantics into clear navigation cues and generate our ASM. MapNav agent using the constructed ASM as input, and use the powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, validating the effectiveness of our method. Moreover, we will release our ASM generation source code and dataset to ensure reproducibility, contributing valuable resources to the field. We believe that our proposed MapNav can be used as a new memory representation method in VLN, paving the way for future research in this field.         ",
    "url": "https://arxiv.org/abs/2502.13451",
    "authors": [
      "Lingfeng Zhang",
      "Xiaoshuai Hao",
      "Qinwen Xu",
      "Qiang Zhang",
      "Xinyao Zhang",
      "Pengwei Wang",
      "Jing Zhang",
      "Zhongyuan Wang",
      "Shanghang Zhang",
      "Renjing Xu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.18549",
    "title": "ARBoids: Adaptive Residual Reinforcement Learning With Boids Model for Cooperative Multi-USV Target Defense",
    "abstract": "           The target defense problem (TDP) for unmanned surface vehicles (USVs) concerns intercepting an adversarial USV before it breaches a designated target region, using one or more defending USVs. A particularly challenging scenario arises when the attacker exhibits superior maneuverability compared to the defenders, significantly complicating effective interception. To tackle this challenge, this letter introduces ARBoids, a novel adaptive residual reinforcement learning framework that integrates deep reinforcement learning (DRL) with the biologically inspired, force-based Boids model. Within this framework, the Boids model serves as a computationally efficient baseline policy for multi-agent coordination, while DRL learns a residual policy to adaptively refine and optimize the defenders' actions. The proposed approach is validated in a high-fidelity Gazebo simulation environment, demonstrating superior performance over traditional interception strategies, including pure force-based approaches and vanilla DRL policies. Furthermore, the learned policy exhibits strong adaptability to attackers with diverse maneuverability profiles, highlighting its robustness and generalization capability. The code of ARBoids will be released upon acceptance of this letter.         ",
    "url": "https://arxiv.org/abs/2502.18549",
    "authors": [
      "Jiyue Tao",
      "Tongsheng Shen",
      "Dexin Zhao",
      "Feitian Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.20954",
    "title": "Robust and Efficient Writer-Independent IMU-Based Handwriting Recognition",
    "abstract": "           Online handwriting recognition (HWR) using data from inertial measurement units (IMUs) remains challenging due to variations in writing styles and the limited availability of annotated datasets. Previous approaches often struggle with handwriting from unseen writers, making writer-independent (WI) recognition a crucial yet difficult problem. This paper presents an HWR model designed to improve WI HWR on IMU data, using a CNN encoder and a BiLSTM-based decoder. Our approach demonstrates strong robustness to unseen handwriting styles, outperforming existing methods on the WI splits of both the public OnHW dataset and our word-based dataset, achieving character error rates (CERs) of 7.37\\% and 9.44\\%, and word error rates (WERs) of 15.12\\% and 32.17\\%, respectively. Robustness evaluation shows that our model maintains superior accuracy across different age groups, and knowledge learned from one group generalizes better to another. Evaluation on our sentence-based dataset further demonstrates its potential in recognizing full sentences. Through comprehensive ablation studies, we show that our design choices lead to a strong balance between performance and efficiency. These findings support the development of more adaptable and scalable HWR systems for real-world applications.         ",
    "url": "https://arxiv.org/abs/2502.20954",
    "authors": [
      "Jindong Li",
      "Tim Hamann",
      "Jens Barth",
      "Peter K\u00e4mpf",
      "Dario Zanca",
      "Bj\u00f6rn Eskofier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.04720",
    "title": "FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video",
    "abstract": "           We study reconstructing and predicting 3D fluid appearance and velocity from a single video. Current methods require multi-view videos for fluid reconstruction. We present FluidNexus, a novel framework that bridges video generation and physics simulation to tackle this task. Our key insight is to synthesize multiple novel-view videos as references for reconstruction. FluidNexus consists of two key components: (1) a novel-view video synthesizer that combines frame-wise view synthesis with video diffusion refinement for generating realistic videos, and (2) a physics-integrated particle representation coupling differentiable simulation and rendering to simultaneously facilitate 3D fluid reconstruction and prediction. To evaluate our approach, we collect two new real-world fluid datasets featuring textured backgrounds and object interactions. Our method enables dynamic novel view synthesis, future prediction, and interaction simulation from a single fluid video. Project website: this https URL.         ",
    "url": "https://arxiv.org/abs/2503.04720",
    "authors": [
      "Yue Gao",
      "Hong-Xing Yu",
      "Bo Zhu",
      "Jiajun Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.11713",
    "title": "Revisiting the Predictability of Performative, Social Events",
    "abstract": "           Social predictions do not passively describe the future; they actively shape it. They inform actions and change individual expectations in ways that influence the likelihood of the predicted outcome. Given these dynamics, to what extent can social events be predicted? This question was discussed throughout the 20th century by authors like Merton, Morgenstern, Simon, and others who considered it a central issue in social science methodology. In this work, we provide a modern answer to this old problem. Using recent ideas from performative prediction and outcome indistinguishability, we establish that one can always efficiently predict social events accurately, regardless of how predictions influence data. While achievable, we also show that these predictions are often undesirable, highlighting the limitations of previous desiderata. We end with a discussion of various avenues forward.         ",
    "url": "https://arxiv.org/abs/2503.11713",
    "authors": [
      "Juan C. Perdomo"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Theoretical Economics (econ.TH)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.18438",
    "title": "ReconDreamer++: Harmonizing Generative and Reconstructive Models for Driving Scene Representation",
    "abstract": "           Combining reconstruction models with generative models has emerged as a promising paradigm for closed-loop simulation in autonomous driving. For example, ReconDreamer has demonstrated remarkable success in rendering large-scale maneuvers. However, a significant gap remains between the generated data and real-world sensor observations, particularly in terms of fidelity for structured elements, such as the ground surface. To address these challenges, we propose ReconDreamer++, an enhanced framework that significantly improves the overall rendering quality by mitigating the domain gap and refining the representation of the ground surface. Specifically, ReconDreamer++ introduces the Novel Trajectory Deformable Network (NTDNet), which leverages learnable spatial deformation mechanisms to bridge the domain gap between synthesized novel views and original sensor observations. Moreover, for structured elements such as the ground surface, we preserve geometric prior knowledge in 3D Gaussians, and the optimization process focuses on refining appearance attributes while preserving the underlying geometric structure. Experimental evaluations conducted on multiple datasets (Waymo, nuScenes, PandaSet, and EUVS) confirm the superior performance of ReconDreamer++. Specifically, on Waymo, ReconDreamer++ achieves performance comparable to Street Gaussians for the original trajectory while significantly outperforming ReconDreamer on novel trajectories. In particular, it achieves substantial improvements, including a 6.1% increase in NTA-IoU, a 23. 0% improvement in FID, and a remarkable 4.5% gain in the ground surface metric NTL-IoU, highlighting its effectiveness in accurately reconstructing structured elements such as the road surface.         ",
    "url": "https://arxiv.org/abs/2503.18438",
    "authors": [
      "Guosheng Zhao",
      "Xiaofeng Wang",
      "Chaojun Ni",
      "Zheng Zhu",
      "Wenkang Qin",
      "Guan Huang",
      "Xingang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.20286",
    "title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration via Tensorization",
    "abstract": "           Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.20286",
    "authors": [
      "Zhenyu Liang",
      "Hao Li",
      "Naiwei Yu",
      "Kebin Sun",
      "Ran Cheng"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2504.02670",
    "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
    "abstract": "           Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively while also minimizing bias and noise. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini. Moreover, harnessing a smaller model dramatically reduces operational costs by over 36x compared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and Deepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a scalable, affordable, versatile, and high-performing solution for AI assistants.         ",
    "url": "https://arxiv.org/abs/2504.02670",
    "authors": [
      "Maciej Besta",
      "Lorenzo Paleari",
      "Jia Hao Andrea Jiang",
      "Robert Gerstenberger",
      "You Wu",
      "J\u00f3n Gunnar Hannesson",
      "Patrick Iff",
      "Ales Kubicek",
      "Piotr Nyczyk",
      "Diana Khimey",
      "Nils Blach",
      "Haiqiang Zhang",
      "Tao Zhang",
      "Peiran Ma",
      "Grzegorz Kwa\u015bniewski",
      "Marcin Copik",
      "Hubert Niewiadomski",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05793",
    "title": "Negotiating Strict Latency Limits for Dynamic Real-Time Services in Vehicular Time-Sensitive Networks",
    "abstract": "           Future vehicles are expected to dynamically deploy in-vehicle applications within a Service-Oriented Architecture (SOA). Critical services operate under hard real-time constraints, which Time-Sensitive Networking (TSN) complements on the in-vehicle Ethernet layer. TSN ensures deterministic communication between critical services and its Credit-Based Shaper (CBS) supports dynamic resource reservations. However, the dynamic nature of service deployment challenges network resource configuration, since any new reservation may change the latency of already validated flows. In addition, standard methods of worst-case latency analysis for CBS have been found incorrect, and current TSN stream reservation procedures lack mechanisms to signal application layer Quality-of-Service (QoS) requirements or verify deadlines. In this paper, we propose a QoS negotiation scheme within the automotive SOA that interacts with the TSN network controller to reserve resources while ensuring latency bounds. We comparatively evaluate reservation schemes using worst-case analysis and simulations of a realistic In-Vehicle Network (IVN) for demonstrating their impact on QoS guarantees, resource utilization, and setup times. We find that only a reservation scheme utilizing per-queue delay budgets and network calculus provides valid configurations and guarantees acceptable latency bounds throughout the IVN. The proposed service negotiation mechanism efficiently establishes 450 vehicular network reservations in just 11 ms.         ",
    "url": "https://arxiv.org/abs/2504.05793",
    "authors": [
      "Timo Salomon",
      "Lisa Maile",
      "Philipp Meyer",
      "Franz Korf",
      "Thomas C. Schmidt"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2504.07793",
    "title": "Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations",
    "abstract": "           Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\\href{this https URL}{\\texttt{this https URL}}$.         ",
    "url": "https://arxiv.org/abs/2504.07793",
    "authors": [
      "Yifan Ding",
      "Arturas Aleksandraus",
      "Amirhossein Ahmadian",
      "Jonas Unger",
      "Fredrik Lindsten",
      "Gabriel Eilertsen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.15284",
    "title": "EditLord: Learning Code Transformation Rules for Code Editing",
    "abstract": "           Code editing is a foundational task in software development, where its effectiveness depends on whether it introduces desired code property changes without changing the original code's intended functionality. Existing approaches often formulate code editing as an implicit end-to-end task, omitting the fact that code-editing procedures inherently consist of discrete and explicit steps. Thus, they suffer from suboptimal performance and lack of robustness and generalization. We introduce EditLord, a code editing framework that makes the code transformation steps explicit. Our key insight is to employ a language model (LM) as an inductive learner to extract code editing rules from the training code pairs as concise meta-rule sets. Such rule sets will be manifested for each training sample to augment them for finetuning or assist in prompting- and iterative-based code editing. EditLord outperforms the state-of-the-art by an average of 22.7% in editing performance and 58.1% in robustness while achieving 20.2% higher functional correctness across critical software engineering and security applications, LM models, and editing modes.         ",
    "url": "https://arxiv.org/abs/2504.15284",
    "authors": [
      "Weichen Li",
      "Albert Jan",
      "Baishakhi Ray",
      "Junfeng Yang",
      "Chengzhi Mao",
      "Kexin Pei"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.19955",
    "title": "Robust Federated Personalised Mean Estimation for the Gaussian Mixture Model",
    "abstract": "           Federated learning with heterogeneous data and personalization has received significant recent attention. Separately, robustness to corrupted data in the context of federated learning has also been studied. In this paper we explore combining personalization for heterogeneous data with robustness, where a constant fraction of the clients are corrupted. Motivated by this broad problem, we formulate a simple instantiation which captures some of its difficulty. We focus on the specific problem of personalized mean estimation where the data is drawn from a Gaussian mixture model. We give an algorithm whose error depends almost linearly on the ratio of corrupted to uncorrupted samples, and show a lower bound with the same behavior, albeit with a gap of a constant factor.         ",
    "url": "https://arxiv.org/abs/2504.19955",
    "authors": [
      "Malhar A. Managoli",
      "Vinod M. Prabhakaran",
      "Suhas Diggavi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2504.20310",
    "title": "A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning",
    "abstract": "           In this paper, we initiate a cryptographically inspired theoretical study of detection versus mitigation of adversarial inputs produced by attackers on Machine Learning algorithms during inference time. We formally define defense by detection (DbD) and defense by mitigation (DbM). Our definitions come in the form of a 3-round protocol between two resource-bounded parties: a trainer/defender and an attacker. The attacker aims to produce inference-time inputs that fool the training algorithm. We define correctness, completeness, and soundness properties to capture successful defense at inference time while not degrading (too much) the performance of the algorithm on inputs from the training distribution. We first show that achieving DbD and achieving DbM are equivalent for ML classification tasks. Surprisingly, this is not the case for ML generative learning tasks, where there are many possible correct outputs for each input. We show a separation between DbD and DbM by exhibiting two generative learning tasks for which it is possible to defend by mitigation but it is provably impossible to defend by detection. The mitigation phase uses significantly less computational resources than the initial training algorithm. In the first learning task we consider sample complexity as the resource and in the second the time complexity. The first result holds under the assumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE), publicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of Knowledge (zk-SNARK), and Strongly Unforgeable Signatures exist. The second result assumes the existence of Non-Parallelizing Languages with Average-Case Hardness (NPL) and Incrementally-Verifiable Computation (IVC) and IB-FHE.         ",
    "url": "https://arxiv.org/abs/2504.20310",
    "authors": [
      "Greg Gluch",
      "Shafi Goldwasser"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.04931",
    "title": "Fair Uncertainty Quantification for Depression Prediction",
    "abstract": "           Trustworthy depression prediction based on deep learning, incorporating both predictive reliability and algorithmic fairness across diverse demographic groups, is crucial for clinical application. Recently, achieving reliable depression predictions through uncertainty quantification has attracted increasing attention. However, few studies have focused on the fairness of uncertainty quantification (UQ) in depression prediction. In this work, we investigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage (EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for depression prediction. FUQ pursues reliable and fair depression predictions through group-based analysis. Specifically, we first group all the participants by different sensitive attributes and leverage conformal prediction to quantify uncertainty within each demographic group, which provides a theoretically guaranteed and valid way to quantify uncertainty for depression prediction and facilitates the investigation of fairness across different demographic groups. Furthermore, we propose a fairness-aware optimization strategy that formulates fairness as a constrained optimization problem under EOC constraints. This enables the model to preserve predictive reliability while adapting to the heterogeneous uncertainty levels across demographic groups, thereby achieving optimal fairness. Through extensive evaluations on several visual and audio depression datasets, our approach demonstrates its effectiveness.         ",
    "url": "https://arxiv.org/abs/2505.04931",
    "authors": [
      "Yonghong Li",
      "Xiuzhuang Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.19598",
    "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study",
    "abstract": "           Large Audio-Language Models (LALMs) are increasingly deployed in real-world applications, yet their robustness against malicious audio injection attacks remains underexplored. This study systematically evaluates five leading LALMs across four attack scenarios: Audio Interference Attack, Instruction Following Attack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics like Defense Success Rate, Context Robustness Score, and Judgment Robustness Index, their vulnerabilities and resilience were quantitatively assessed. Experimental results reveal significant performance disparities among models; no single model consistently outperforms others across all attack types. The position of malicious content critically influences attack effectiveness, particularly when placed at the beginning of sequences. A negative correlation between instruction-following capability and robustness suggests models adhering strictly to instructions may be more susceptible, contrasting with greater resistance by safety-aligned models. Additionally, system prompts show mixed effectiveness, indicating the need for tailored strategies. This work introduces a benchmark framework and highlights the importance of integrating robustness into training pipelines. Findings emphasize developing multi-modal defenses and architectural designs that decouple capability from susceptibility for secure LALMs deployment.         ",
    "url": "https://arxiv.org/abs/2505.19598",
    "authors": [
      "Guanyu Hou",
      "Jiaming He",
      "Yinhang Zhou",
      "Ji Guo",
      "Yitong Qiao",
      "Rui Zhang",
      "Wenbo Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.00283",
    "title": "Direct-to-Cell: A First Look into Starlink's Direct Satellite-to-Device Radio Access Network through Crowdsourced Measurements",
    "abstract": "           Low Earth Orbit (LEO) satellite mega-constellations have recently emerged as a viable access solution for broadband services in underserved areas. In 2024, Direct Satellite-to-Device (DS2D) communications, which enable unmodified smartphones to connect directly to spaceborne base stations, entered large-scale beta testing, with Starlink globally leading deployments. This paper presents the first measurement study of commercial DS2D services. Using crowdsourced mobile network data collected in the U.S. between October 2024 and April 2025, our research derives evidence-based insights into the capabilities, limitations, and prospective evolution of DS2D technologies providing Supplemental Coverage from Space (SCS) services to expand existing mobile network connectivity. We observe a strong correlation between the number of satellites deployed and the expanding extension of observed measurements, concentrated in accessible but poorly covered areas by terrestrial networks, such as national parks and large low-density counties. The data reveal stable physical-layer value measurement throughout the observation period, with a lower median RSRP (24-dB difference) and a higher RSRQ (3 dB difference) compared to terrestrial networks, reflecting the SMS-only usage of the DS2D network during this period. Based on SINR measurements, we estimate the expected performance of the announced DS2D mobile data service to be around 4 Mbps per beam in outdoor conditions. We also discuss strategies to expand this capacity up to 12 Mbps in the future, depending on key regulatory decisions regarding satellite licenses, spectrum availability, and allowable radiated power levels.         ",
    "url": "https://arxiv.org/abs/2506.00283",
    "authors": [
      "Jorge Garcia-Cabeza",
      "Javier Albert-Smet",
      "Zoraida Frias",
      "Luis Mendo",
      "Santiago Andr\u00e9s Azcoitia",
      "Eduardo Yraola"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2506.00981",
    "title": "What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training",
    "abstract": "           How language-specific are speech representations learned by self-supervised models? Existing work has shown that a range of linguistic features can be successfully decoded from end-to-end models trained only on speech recordings. However, it's less clear to what extent pre-training on specific languages improves language-specific linguistic information. Here we test the encoding of Dutch phonetic and lexical information in internal representations of self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the representation of Dutch linguistic features as compared to pre-training on similar amounts of English or larger amounts of multilingual data. This language-specific advantage is well-detected by trained clustering or classification probes, and partially observable using zero-shot metrics. Furthermore, the language-specific benefit on linguistic feature encoding aligns with downstream performance on Automatic Speech Recognition.         ",
    "url": "https://arxiv.org/abs/2506.00981",
    "authors": [
      "Marianne de Heer Kloots",
      "Hosein Mohebbi",
      "Charlotte Pouw",
      "Gaofei Shen",
      "Willem Zuidema",
      "Martijn Bentum"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2506.01220",
    "title": "Vulnerability Management Chaining: An Integrated Framework for Efficient Cybersecurity Risk Prioritization",
    "abstract": "           As the number of Common Vulnerabilities and Exposures (CVE) continues to grow exponentially, security teams face increasingly difficult decisions about prioritization. Current approaches using Common Vulnerability Scoring System (CVSS) scores produce overwhelming volumes of high-priority vulnerabilities, while Exploit Prediction Scoring System (EPSS) and Known Exploited Vulnerabilities (KEV) catalog offer valuable but incomplete perspectives on actual exploitation risk. We present Vulnerability Management Chaining, a decision tree framework that systematically integrates these three approaches to achieve efficient vulnerability prioritization. Our framework employs a two-stage evaluation process: first applying threat-based filtering using KEV membership or EPSS threshold $\\geq$ 0.088), then applying vulnerability severity assessment using CVSS scores $\\geq$ 7.0) to enable informed deprioritization. Experimental validation using 28,377 real-world vulnerabilities and vendor-reported exploitation data demonstrates 18-fold efficiency improvements while maintaining 85.6\\% coverage. Organizations can reduce urgent remediation workload by approximately 95\\%. The integration identifies 48 additional exploited vulnerabilities that neither KEV nor EPSS captures individually. Our framework uses exclusively open-source data, enabling immediate adoption regardless of organizational resources.         ",
    "url": "https://arxiv.org/abs/2506.01220",
    "authors": [
      "Naoyuki Shimizu",
      "Masaki Hashimoto"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.08694",
    "title": "MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning",
    "abstract": "           Dense self-supervised learning has shown great promise for learning pixel- and patch-level representations, but extending it to videos remains challenging due to the complexity of motion dynamics. Existing approaches struggle as they rely on static augmentations that fail under object deformations, occlusions, and camera movement, leading to inconsistent feature learning over time. We propose a motion-guided self-supervised learning framework that clusters dense point tracks to learn spatiotemporally consistent representations. By leveraging an off-the-shelf point tracker, we extract long-range motion trajectories and optimize feature clustering through a momentum-encoder-based optimal transport mechanism. To ensure temporal coherence, we propagate cluster assignments along tracked points, enforcing feature consistency across views despite viewpoint changes. Integrating motion as an implicit supervisory signal, our method learns representations that generalize across frames, improving robustness in dynamic scenes and challenging occlusion scenarios. By initializing from strong image-pretrained models and leveraging video data for training, we improve state-of-the-art by 1% to 6% on six image and video datasets and four evaluation benchmarks. The implementation is publicly available at our GitHub repository: this https URL ",
    "url": "https://arxiv.org/abs/2506.08694",
    "authors": [
      "Mohammadreza Salehi",
      "Shashanka Venkataramanan",
      "Ioana Simion",
      "Efstratios Gavves",
      "Cees G. M. Snoek",
      "Yuki M Asano"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.15709",
    "title": "Studying and Improving Graph Neural Network-based Motif Estimation",
    "abstract": "           Graph Neural Networks (GNNs) are a predominant method for graph representation learning. However, beyond subgraph frequency estimation, their application to network motif significance-profile (SP) prediction remains under-explored, with no established benchmarks in the literature. We propose to address this problem, framing SP estimation as a task independent of subgraph frequency estimation. Our approach shifts from frequency counting to direct SP estimation and modulates the problem as multitarget regression. The reformulation is optimised for interpretability, stability and scalability on large graphs. We validate our method using a large synthetic dataset and further test it on real-world graphs. Our experiments reveal that 1-WL limited models struggle to make precise estimations of SPs. However, they can generalise to approximate the graph generation processes of networks by comparing their predicted SP with the ones originating from synthetic generators. This first study on GNN-based motif estimation also hints at how using direct SP estimation can help go past the theoretical limitations that motif estimation faces when performed through subgraph counting.         ",
    "url": "https://arxiv.org/abs/2506.15709",
    "authors": [
      "Pedro C. Vieira",
      "Miguel E. P. Silva",
      "Pedro Manuel Pinto Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.18939",
    "title": "Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction",
    "abstract": "           Training urban spatio-temporal foundation models that generalize well across diverse regions and cities is critical for deploying urban services in unseen or data-scarce regions. Recent studies have typically focused on fusing cross-domain spatio-temporal data to train unified Transformer-based models. However, these models suffer from quadratic computational complexity and high memory overhead, limiting their scalability and practical deployment. Inspired by the efficiency of Mamba, a state space model with linear time complexity, we explore its potential for efficient urban spatio-temporal prediction. However, directly applying Mamba as a spatio-temporal backbone leads to negative transfer and severe performance degradation. This is primarily due to spatio-temporal heterogeneity and the recursive mechanism of Mamba's hidden state updates, which limit cross-domain generalization. To overcome these challenges, we propose Damba-ST, a novel domain-adaptive Mamba-based model for efficient urban spatio-temporal prediction. Damba-ST retains Mamba's linear complexity advantage while significantly enhancing its adaptability to heterogeneous domains. Specifically, we introduce two core innovations: (1) a domain-adaptive state space model that partitions the latent representation space into a shared subspace for learning cross-domain commonalities and independent, domain-specific subspaces for capturing intra-domain discriminative features; (2) three distinct Domain Adapters, which serve as domain-aware proxies to bridge disparate domain distributions and facilitate the alignment of cross-domain commonalities. Extensive experiments demonstrate the generalization and efficiency of Damba-ST. It achieves state-of-the-art performance on prediction tasks and demonstrates strong zero-shot generalization, enabling seamless deployment in new urban environments without extensive retraining or fine-tuning.         ",
    "url": "https://arxiv.org/abs/2506.18939",
    "authors": [
      "Rui An",
      "Yifeng Zhang",
      "Ziran Liang",
      "Wenqi Fan",
      "Yuxuan Liang",
      "Xuequn Shang",
      "Qing Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.23446",
    "title": "User-Based Sequential Modeling with Transformer Encoders for Insider Threat Detection",
    "abstract": "           Insider threat detection presents unique challenges due to the authorized status of malicious actors and the subtlety of anomalous behaviors. Existing machine learning methods often treat user activity as isolated events, thereby failing to leverage sequential dependencies in user behavior. In this study, we propose a User-Based Sequencing (UBS) methodology, transforming the CERT insider threat dataset into structured temporal sequences suitable for deep sequential modeling. We deploy a Transformer Encoder architecture to model benign user activity and employ its reconstruction errors as anomaly scores. These scores are subsequently evaluated using three unsupervised outlier detection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and Isolation Forest (iForest). Across four rigorously designed test sets, including combinations of multiple CERT dataset releases, our UBS-Transformer pipeline consistently achieves state-of-the-art performance - notably 96.61% accuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low false negative (0.0057) and false positive (0.0571) rates. Comparative analyses demonstrate that our approach substantially outperforms tabular and conventional autoencoder baselines, underscoring the efficacy of sequential user modeling and advanced anomaly detection in the insider threat domain.         ",
    "url": "https://arxiv.org/abs/2506.23446",
    "authors": [
      "Mohamed Elbasheer",
      "Adewale Akinfaderin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.01003",
    "title": "Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes",
    "abstract": "           Recent studies have proposed interpreting the training process from an ergodic perspective. Building on this foundation, we present a unified framework for understanding and accelerating the training of deep neural networks via stochastic gradient descent (SGD). By analyzing the geometric landscape of the objective function we introduce a practical diagnostic, the running estimate of the largest Lyapunov exponent, which provably distinguishes genuine convergence toward stable minimizers from mere statistical stabilization near saddle points. We then propose a ghost category extension for standard classifiers that adds auxiliary ghost output nodes so the model gains extra descent directions that open a lateral corridor around narrow loss barriers and enable the optimizer to bypass poor basins during the early training phase. We show that this extension strictly reduces the approximation error and that after sufficient convergence the ghost dimensions collapse so that the extended model coincides with the original one and there exists a path in the enlarged parameter space along which the total loss does not increase. Taken together, these results provide a principled architecture level intervention that accelerates early stage trainability while preserving asymptotic behavior and simultaneously serves as an architecture-friendly regularizer.         ",
    "url": "https://arxiv.org/abs/2507.01003",
    "authors": [
      "Eun-Ji Park",
      "Sangwon Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.02398",
    "title": "Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection",
    "abstract": "           We introduce a deepfake video detection approach that exploits pixel-wise temporal inconsistencies, which traditional spatial frequency-based detectors often overlook. Traditional detectors represent temporal information merely by stacking spatial frequency spectra across frames, resulting in the failure to detect temporal artifacts in the pixel plane. Our approach performs a 1D Fourier transform on the time axis for each pixel, extracting features highly sensitive to temporal inconsistencies, especially in areas prone to unnatural movements. To precisely locate regions containing the temporal artifacts, we introduce an attention proposal module trained in an end-to-end manner. Additionally, our joint transformer module effectively integrates pixel-wise temporal frequency features with spatio-temporal context features, expanding the range of detectable forgery artifacts. Our framework represents a significant advancement in deepfake video detection, providing robust performance across diverse and challenging detection scenarios.         ",
    "url": "https://arxiv.org/abs/2507.02398",
    "authors": [
      "Taehoon Kim",
      "Jongwook Choi",
      "Yonghyun Jeong",
      "Haeun Noh",
      "Jaejun Yoo",
      "Seungryul Baek",
      "Jongwon Choi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.02409",
    "title": "S2FGL: Spatial Spectral Federated Graph Learning",
    "abstract": "           Federated Graph Learning (FGL) combines the privacy-preserving capabilities of federated learning (FL) with the strong graph modeling capability of Graph Neural Networks (GNNs). Current research addresses subgraph-FL only from the structural perspective, neglecting the propagation of graph signals on spatial and spectral domains of the structure. From a spatial perspective, subgraph-FL introduces edge disconnections between clients, leading to disruptions in label signals and a degradation in the class knowledge of the global GNN. From a spectral perspective, spectral heterogeneity causes inconsistencies in signal frequencies across subgraphs, which makes local GNNs overfit the local signal propagation schemes. As a result, spectral client drifts occur, undermining global generalizability. To tackle the challenges, we propose a global knowledge repository to mitigate label signal disruption and a frequency alignment to address spectral client drifts. The combination of spatial and spectral strategies forms our framework S2FGL. Extensive experiments on multiple datasets demonstrate the superiority of S2FGL. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.02409",
    "authors": [
      "Zihan Tan",
      "Suyuan Huang",
      "Guancheng Wan",
      "Wenke Huang",
      "He Li",
      "Mang Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.02901",
    "title": "Online Continual Learning via Spiking Neural Networks with Sleep Enhanced Latent Replay",
    "abstract": "           Edge computing scenarios necessitate the development of hardware-efficient online continual learning algorithms to be adaptive to dynamic environment. However, existing algorithms always suffer from high memory overhead and bias towards recently trained tasks. To tackle these issues, this paper proposes a novel online continual learning approach termed as SESLR, which incorporates a sleep enhanced latent replay scheme with spiking neural networks (SNNs). SESLR leverages SNNs' binary spike characteristics to store replay features in single bits, significantly reducing memory overhead. Furthermore, inspired by biological sleep-wake cycles, SESLR introduces a noise-enhanced sleep phase where the model exclusively trains on replay samples with controlled noise injection, effectively mitigating classification bias towards new classes. Extensive experiments on both conventional (MNIST, CIFAR10) and neuromorphic (NMNIST, CIFAR10-DVS) datasets demonstrate SESLR's effectiveness. On Split CIFAR10, SESLR achieves nearly 30% improvement in average accuracy with only one-third of the memory consumption compared to baseline methods. On Split CIFAR10-DVS, it improves accuracy by approximately 10% while reducing memory overhead by a factor of 32. These results validate SESLR as a promising solution for online continual learning in resource-constrained edge computing scenarios.         ",
    "url": "https://arxiv.org/abs/2507.02901",
    "authors": [
      "Erliang Lin",
      "Wenbin Luo",
      "Wei Jia",
      "Yu Chen",
      "Shaofu Yang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.02987",
    "title": "Leveraging the Structure of Medical Data for Improved Representation Learning",
    "abstract": "           Building generalizable medical AI systems requires pretraining strategies that are data-efficient and domain-aware. Unlike internet-scale corpora, clinical datasets such as MIMIC-CXR offer limited image counts and scarce annotations, but exhibit rich internal structure through multi-view imaging. We propose a self-supervised framework that leverages the inherent structure of medical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and lateral views) as natural positive pairs, learning to reconstruct each view from sparse patches while aligning their latent embeddings. Our method requires no textual supervision and produces informative representations. Evaluated on MIMIC-CXR, we show strong performance compared to supervised objectives and baselines being trained without leveraging structure. This work provides a lightweight, modality-agnostic blueprint for domain-specific pretraining where data is structured but scarce         ",
    "url": "https://arxiv.org/abs/2507.02987",
    "authors": [
      "Andrea Agostini",
      "Sonia Laguna",
      "Alain Ryser",
      "Samuel Ruiperez-Campillo",
      "Moritz Vandenhirtz",
      "Nicolas Deperrois",
      "Farhad Nooralahzadeh",
      "Michael Krauthammer",
      "Thomas M. Sutter",
      "Julia E. Vogt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.04750",
    "title": "MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry",
    "abstract": "           Particle Image Velocimetry (PIV) is fundamental to fluid dynamics, yet deep learning applications face significant hurdles. A critical gap exists: the lack of comprehensive evaluation of how diverse optical flow models perform specifically on PIV data, largely due to limitations in available datasets and the absence of a standardized benchmark. This prevents fair comparison and hinders progress. To address this, our primary contribution is a novel, large-scale synthetic PIV benchmark dataset generated from diverse CFD simulations (JHTDB and Blasius). It features unprecedented variety in particle densities, flow velocities, and continuous motion, enabling, for the first time, a standardized and rigorous evaluation of various optical flow and PIV algorithms. Complementing this, we propose Multi Cost Volume PIV (MCFormer), a new deep network architecture leveraging multi-frame temporal information and multiple cost volumes, specifically designed for PIV's sparse nature. Our comprehensive benchmark evaluation, the first of its kind, reveals significant performance variations among adapted optical flow models and demonstrates that MCFormer significantly outperforms existing methods, achieving the lowest overall normalized endpoint error (NEPE). This work provides both a foundational benchmark resource essential for future PIV research and a state-of-the-art method tailored for PIV challenges. We make our benchmark dataset and code publicly available to foster future research in this area.         ",
    "url": "https://arxiv.org/abs/2507.04750",
    "authors": [
      "Zicheng Lin",
      "Xiaoqiang Li",
      "Yichao Wang",
      "Chuang Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.05020",
    "title": "Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision",
    "abstract": "           Surgical AI often involves multiple tasks within a single procedure, like phase recognition or assessing the Critical View of Safety in laparoscopic cholecystectomy. Traditional models, built for one task at a time, lack flexibility, requiring a separate model for each. To address this, we introduce MML-SurgAdapt, a unified multi-task framework with Vision-Language Models (VLMs), specifically CLIP, to handle diverse surgical tasks through natural language supervision. A key challenge in multi-task learning is the presence of partial annotations when integrating different tasks. To overcome this, we employ Single Positive Multi-Label (SPML) learning, which traditionally reduces annotation burden by training models with only one positive label per instance. Our framework extends this approach to integrate data from multiple surgical tasks within a single procedure, enabling effective learning despite incomplete or noisy annotations. We demonstrate the effectiveness of our model on a combined dataset consisting of Cholec80, Endoscapes2023, and CholecT50, utilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt performs comparably to task-specific benchmarks, with the added advantage of handling noisy annotations. It also outperforms the existing SPML frameworks for the task. By reducing the required labels by 23%, our approach proposes a more scalable and efficient labeling process, significantly easing the annotation burden on clinicians. To our knowledge, this is the first application of SPML to integrate data from multiple surgical tasks, presenting a novel and generalizable solution for multi-task learning in surgical computer vision. Implementation is available at: this https URL ",
    "url": "https://arxiv.org/abs/2507.05020",
    "authors": [
      "Soham Walimbe",
      "Britty Baby",
      "Vinkle Srivastav",
      "Nicolas Padoy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.05110",
    "title": "Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution Shift",
    "abstract": "           Logical rule learning, a prominent category of knowledge graph (KG) reasoning methods, constitutes a critical research area aimed at learning explicit rules from observed facts to infer missing knowledge. However, like all KG reasoning methods, rule learning suffers from a critical weakness-its dependence on the I.I.D. assumption. This assumption can easily be violated due to selection bias during training or agnostic distribution shifts during testing (e.g., as in query shift scenarios), ultimately undermining model performance and reliability. To enable robust KG reasoning in wild environments, this study investigates logical rule learning in the presence of agnostic test-time distribution shifts. We formally define this challenge as out-of-distribution (OOD) KG reasoning-a previously underexplored problem, and propose the Stable Rule Learning (StableRule) framework as a solution. StableRule is an end-to-end framework that combines feature decorrelation with rule learning network, to enhance OOD generalization in KG reasoning. By leveraging feature decorrelation, StableRule mitigates the adverse effects of covariate shifts arising in OOD scenarios, improving the robustness of the rule learning network. Extensive experiments on seven benchmark KGs demonstrate the framework's superior effectiveness and stability across diverse heterogeneous environments, highlighting its practical significance for real-world applications.         ",
    "url": "https://arxiv.org/abs/2507.05110",
    "authors": [
      "Shixuan Liu",
      "Yue He",
      "Yunfei Wang",
      "Hao Zou",
      "Haoxiang Cheng",
      "Wenjing Yang",
      "Peng Cui",
      "Zhong Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.05289",
    "title": "Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models",
    "abstract": "           Code readability is one of the main aspects of code quality, influenced by various properties like identifier names, comments, code structure, and adherence to standards. However, measuring this attribute poses challenges in both industry and academia. While static analysis tools assess attributes such as code smells and comment percentage, code reviews introduce an element of subjectivity. This paper explores using Large Language Models (LLMs) to evaluate code quality attributes related to its readability in a standardized, reproducible, and consistent manner. We conducted a quasi-experiment study to measure the effects of code changes on Large Language Model (LLM)s interpretation regarding its readability quality attribute. Nine LLMs were tested, undergoing three interventions: removing comments, replacing identifier names with obscure names, and refactoring to remove code smells. Each intervention involved 10 batch analyses per LLM, collecting data on response variability. We compared the results with a known reference model and tool. The results showed that all LLMs were sensitive to the interventions, with agreement with the reference classifier being high for the original and refactored code scenarios. The LLMs demonstrated a strong semantic sensitivity that the reference model did not fully capture. A thematic analysis of the LLMs reasoning confirmed their evaluations directly reflected the nature of each intervention. The models also exhibited response variability, with 9.37% to 14.58% of executions showing a standard deviation greater than zero, indicating response oscillation, though this did not always compromise the statistical significance of the results. LLMs demonstrated potential for evaluating semantic quality aspects, such as coherence between identifier names, comments, and documentation with code purpose.         ",
    "url": "https://arxiv.org/abs/2507.05289",
    "authors": [
      "Igor Regis da Silva Simoes",
      "Elaine Venson"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2507.05411",
    "title": "AXLearn: Modular Large Model Training on Heterogeneous Infrastructure",
    "abstract": "           We design and implement AXLearn, a production deep learning system that facilitates scalable and high-performance training of large deep learning models. Compared to other state-of-the-art deep learning systems, AXLearn has a unique focus on modularity and support for heterogeneous hardware infrastructure. AXLearn's internal interfaces between software components follow strict encapsulation, allowing different components to be assembled to facilitate rapid model development and experimentation on heterogeneous compute infrastructure. We introduce a novel method of quantifying modularity via Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains constant complexity as we scale the components in the system, compared to linear or quadratic complexity in other systems. This allows integrating features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred of modules with just 10 lines of code, compared to hundreds as required in other systems. At the same time, AXLearn maintains equivalent performance compared to state-of-the-art training systems. Finally, we share our experience in the development and operation of AXLearn.         ",
    "url": "https://arxiv.org/abs/2507.05411",
    "authors": [
      "Mark Lee",
      "Tom Gunter",
      "Chang Lan",
      "John Peebles",
      "Hanzhi Zhou",
      "Kelvin Zou",
      "Sneha Bangalore",
      "Chung-Cheng Chiu",
      "Nan Du",
      "Xianzhi Du",
      "Philipp Dufter",
      "Ruixuan Hou",
      "Haoshuo Huang",
      "Dongseong Hwang",
      "Xiang Kong",
      "Jinhao Lei",
      "Tao Lei",
      "Meng Li",
      "Li Li",
      "Jiarui Lu",
      "Zhiyun Lu",
      "Yiping Ma",
      "David Qiu",
      "Vivek Rathod",
      "Senyu Tong",
      "Zhucheng Tu",
      "Jianyu Wang",
      "Yongqiang Wang",
      "Zirui Wang",
      "Floris Weers",
      "Sam Wiseman",
      "Guoli Yin",
      "Bowen Zhang",
      "Xiyou Zhou",
      "Danyang Zhuo",
      "Cheng Leong",
      "Ruoming Pang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.06156",
    "title": "Hedge Funds on a Swamp: Analyzing Patterns, Vulnerabilities, and Defense Measures in Blockchain Bridges [Experiment, Analysis & Benchmark]",
    "abstract": "           Blockchain bridges have become essential infrastructure for enabling interoperability across different blockchain networks, with more than $24B monthly bridge transaction volume. However, their growing adoption has been accompanied by a disproportionate rise in security breaches, making them the single largest source of financial loss in Web3. For cross-chain ecosystems to be robust and sustainable, it is essential to understand and address these vulnerabilities. In this study, we present a comprehensive systematization of blockchain bridge design and security. We define three bridge security priors, formalize the architectural structure of 13 prominent bridges, and identify 23 attack vectors grounded in real-world blockchain exploits. Using this foundation, we evaluate 43 representative attack scenarios and introduce a layered threat model that captures security failures across source chain, off-chain, and destination chain components. Our analysis at the static code and transaction network levels reveals recurring design flaws, particularly in access control, validator trust assumptions, and verification logic, and identifies key patterns in adversarial behavior based on transaction-level traces. To support future development, we propose a decision framework for bridge architecture design, along with defense mechanisms such as layered validation and circuit breakers. This work provides a data-driven foundation for evaluating bridge security and lays the groundwork for standardizing resilient cross-chain infrastructure.         ",
    "url": "https://arxiv.org/abs/2507.06156",
    "authors": [
      "Poupak Azad",
      "Jiahua Xu",
      "Yebo Feng",
      "Preston Strowbridge",
      "Cuneyt Akcora"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2507.06539",
    "title": "Large Language Model for Extracting Complex Contract Information in Industrial Scenes",
    "abstract": "           This paper proposes a high-quality dataset construction method for complex contract information extraction tasks in industrial scenarios and fine-tunes a large language model based on this dataset. Firstly, cluster analysis is performed on industrial contract texts, and GPT-4 and GPT-3.5 are used to extract key information from the original contract data, obtaining high-quality data annotations. Secondly, data augmentation is achieved by constructing new texts, and GPT-3.5 generates unstructured contract texts from randomly combined keywords, improving model robustness. Finally, the large language model is fine-tuned based on the high-quality dataset. Experimental results show that the model achieves excellent overall performance while ensuring high field recall and precision and considering parsing efficiency. LoRA, data balancing, and data augmentation effectively enhance model accuracy and robustness. The proposed method provides a novel and efficient solution for industrial contract information extraction tasks.         ",
    "url": "https://arxiv.org/abs/2507.06539",
    "authors": [
      "Yunyang Cao",
      "Yanjun Li",
      "Silong Dai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.06821",
    "title": "HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning",
    "abstract": "           Multi-modal emotion recognition has garnered increasing attention as it plays a significant role in human-computer interaction (HCI) in recent years. Since different discrete emotions may exist at the same time, compared with single-class emotion recognition, emotion distribution learning (EDL) that identifies a mixture of basic emotions has gradually emerged as a trend. However, existing EDL methods face challenges in mining the heterogeneity among multiple modalities. Besides, rich semantic correlations across arbitrary basic emotions are not fully exploited. In this paper, we propose a multi-modal emotion distribution learning framework, named HeLo, aimed at fully exploring the heterogeneity and complementary information in multi-modal emotional data and label correlation within mixed basic emotions. Specifically, we first adopt cross-attention to effectively fuse the physiological data. Then, an optimal transport (OT)-based heterogeneity mining module is devised to mine the interaction and heterogeneity between the physiological and behavioral representations. To facilitate label correlation learning, we introduce a learnable label embedding optimized by correlation matrix alignment. Finally, the learnable label embeddings and label correlation matrices are integrated with the multi-modal representations through a novel label correlation-driven cross-attention mechanism for accurate emotion distribution learning. Experimental results on two publicly available datasets demonstrate the superiority of our proposed method in emotion distribution learning.         ",
    "url": "https://arxiv.org/abs/2507.06821",
    "authors": [
      "Chuhang Zheng",
      "Chunwei Tian",
      "Jie Wen",
      "Daoqiang Zhang",
      "Qi Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2507.06850",
    "title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer Takeover",
    "abstract": "           The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables unprecedented capabilities in natural language processing and generation. However, these systems have introduced unprecedented security vulnerabilities that extend beyond traditional prompt injection attacks. This paper presents the first comprehensive evaluation of LLM agents as attack vectors capable of achieving complete computer takeover through the exploitation of trust boundaries within agentic AI systems where autonomous entities interact and influence each other. We demonstrate that adversaries can leverage three distinct attack surfaces - direct prompt injection, RAG backdoor attacks, and inter-agent trust exploitation - to coerce popular LLMs (including GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals an alarming vulnerability hierarchy: while 41.2% of models succumb to direct prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical 82.4% can be compromised through inter-agent trust exploitation. Notably, we discovered that LLMs which successfully resist direct malicious commands will execute identical payloads when requested by peer agents, revealing a fundamental flaw in current multi-agent security models. Our findings demonstrate that only 5.9% of tested models (1/17) proved resistant to all attack vectors, with the majority exhibiting context-dependent security behaviors that create exploitable blind spots. Our findings also highlight the need to increase awareness and research on the security risks of LLMs, showing a paradigm shift in cybersecurity threats, where AI tools themselves become sophisticated attack vectors.         ",
    "url": "https://arxiv.org/abs/2507.06850",
    "authors": [
      "Matteo Lupinacci",
      "Francesco Aurelio Pironti",
      "Francesco Blefari",
      "Francesco Romeo",
      "Luigi Arena",
      "Angelo Furfaro"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.06920",
    "title": "Rethinking Verification for LLM Code Generation: From Generation to Testing",
    "abstract": "           Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval and LiveCodeBench. However, a detailed examination reveals that these evaluation suites often comprise only a limited number of homogeneous test cases, resulting in subtle faults going undetected. This not only artificially inflates measured performance but also compromises accurate reward estimation in reinforcement learning frameworks utilizing verifiable rewards (RLVR). To address these critical shortcomings, we systematically investigate the test-case generation (TCG) task by proposing multi-dimensional metrics designed to rigorously quantify test-suite thoroughness. Furthermore, we introduce a human-LLM collaborative method (SAGA), leveraging human programming expertise with LLM reasoning capability, aimed at significantly enhancing both the coverage and the quality of generated test cases. In addition, we develop a TCGBench to facilitate the study of the TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc) of the code generation evaluation benchmark synthesized by SAGA is 10.78% higher than that of LiveCodeBench-v6. These results demonstrate the effectiveness of our proposed method. We hope this work contributes to building a scalable foundation for reliable LLM code evaluation, further advancing RLVR in code generation, and paving the way for automated adversarial test synthesis and adaptive benchmark integration.         ",
    "url": "https://arxiv.org/abs/2507.06920",
    "authors": [
      "Zihan Ma",
      "Taolin Zhang",
      "Maosong Cao",
      "Junnan Liu",
      "Wenwei Zhang",
      "Minnan Luo",
      "Songyang Zhang",
      "Kai Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.05798",
    "title": "Time Makes Space: Emergence of Place Fields in Networks Encoding Temporally Continuous Sensory Experiences",
    "abstract": "           The vertebrate hippocampus is believed to use recurrent connectivity in area CA3 to support episodic memory recall from partial cues. This brain area also contains place cells, whose location-selective firing fields implement maps supporting spatial memory. Here we show that place cells emerge in networks trained to remember temporally continuous sensory episodes. We model CA3 as a recurrent autoencoder that recalls and reconstructs sensory experiences from noisy and partially occluded observations by agents traversing simulated rooms. The agents move in realistic trajectories modeled from rodents and environments are modeled as high-dimensional sensory experience maps. Training our autoencoder to pattern-complete and reconstruct experiences with a constraint on total activity causes spatially localized firing fields, i.e., place cells, to emerge in the encoding layer. The emergent place fields reproduce key aspects of hippocampal phenomenology: a) remapping (maintenance of and reversion to distinct learned maps in different environments), implemented via repositioning of experience manifolds in the network's hidden layer, b) orthogonality of spatial representations in different arenas, c) robust place field emergence in differently shaped rooms, with single units showing multiple place fields in large or complex spaces, and d) slow representational drift of place fields. We argue that these results arise because continuous traversal of space makes sensory experience temporally continuous. We make testable predictions: a) rapidly changing sensory context will disrupt place fields, b) place fields will form even if recurrent connections are blocked, but reversion to previously learned representations upon remapping will be abolished, c) the dimension of temporally smooth experience sets the dimensionality of place fields, including during virtual navigation of abstract spaces.         ",
    "url": "https://arxiv.org/abs/2408.05798",
    "authors": [
      "Zhaoze Wang",
      "Ronald W. Di Tullio",
      "Spencer Rooke",
      "Vijay Balasubramanian"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.01361",
    "title": "Statistical physics analysis of graph neural networks: Approaching optimality in the contextual stochastic block model",
    "abstract": "           Graph neural networks (GNNs) are designed to process data associated with graphs. They are finding an increasing range of applications; however, as with other modern machine learning techniques, their theoretical understanding is limited. GNNs can encounter difficulties in gathering information from nodes that are far apart by iterated aggregation steps. This situation is partly caused by so-called oversmoothing; and overcoming it is one of the practically motivated challenges. We consider the situation where information is aggregated by multiple steps of convolution, leading to graph convolutional networks (GCNs). We analyze the generalization performance of a basic GCN, trained for node classification on data generated by the contextual stochastic block model. We predict its asymptotic performance by deriving the free energy of the problem, using the replica method, in the high-dimensional limit. Calling depth the number of convolutional steps, we show the importance of going to large depth to approach the Bayes-optimality. We detail how the architecture of the GCN has to scale with the depth to avoid oversmoothing. The resulting large depth limit can be close to the Bayes-optimality and leads to a continuous GCN. Technically, we tackle this continuous limit via an approach that resembles dynamical mean-field theory (DMFT) with constraints at the initial and final times. An expansion around large regularization allows us to solve the corresponding equations for the performance of the deep GCN. This promising tool may contribute to the analysis of further deep neural networks.         ",
    "url": "https://arxiv.org/abs/2503.01361",
    "authors": [
      "O. Duranthon",
      "L. Zdeborov\u00e1"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.04424",
    "title": "Determinant Estimation under Memory Constraints and Neural Scaling Laws",
    "abstract": "           Calculating or accurately estimating log-determinants of large positive definite matrices is of fundamental importance in many machine learning tasks. While its cubic computational complexity can already be prohibitive, in modern applications, even storing the matrices themselves can pose a memory bottleneck. To address this, we derive a novel hierarchical algorithm based on block-wise computation of the LDL decomposition for large-scale log-determinant calculation in memory-constrained settings. In extreme cases where matrices are highly ill-conditioned, accurately computing the full matrix itself may be infeasible. This is particularly relevant when considering kernel matrices at scale, including the empirical Neural Tangent Kernel (NTK) of neural networks trained on large datasets. Under the assumption of neural scaling laws in the test error, we show that the ratio of pseudo-determinants satisfies a power-law relationship, allowing us to derive corresponding scaling laws. This enables accurate estimation of NTK log-determinants from a tiny fraction of the full dataset; in our experiments, this results in a $\\sim$100,000$\\times$ speedup with improved accuracy over competing approximations. Using these techniques, we successfully estimate log-determinants for dense matrices of extreme sizes, which were previously deemed intractable and inaccessible due to their enormous scale and computational demands.         ",
    "url": "https://arxiv.org/abs/2503.04424",
    "authors": [
      "Siavash Ameli",
      "Chris van der Heide",
      "Liam Hodgkinson",
      "Fred Roosta",
      "Michael W. Mahoney"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2506.20573",
    "title": "LARP: Learner-Agnostic Robust Data Prefiltering",
    "abstract": "           The widespread availability of large public datasets is a key factor behind the recent successes of statistical inference and machine learning methods. However, these datasets often contain some low-quality or contaminated data, to which many learning procedures are sensitive. Therefore, the question of whether and how public datasets should be prefiltered to facilitate accurate downstream learning arises. On a technical level this requires the construction of principled data prefiltering methods which are learner-agnostic robust, in the sense of provably protecting a set of pre-specified downstream learners from corrupted data. In this work, we formalize the problem of Learner-Agnostic Robust data Prefiltering (LARP), which aims at finding prefiltering procedures that minimize a worst-case loss over a pre-specified set of learners. We first instantiate our framework in the context of scalar mean estimation with Huber estimators under the Huber data contamination model. We provide a hardness result on a specific problem instance and analyze several natural prefiltering procedures. Our theoretical results indicate that performing LARP on a heterogeneous set of learners leads to some loss in model performance compared to the alternative of prefiltering data for each learner/use-case individually. We explore the resulting utility loss and its dependence on the problem parameters via extensive experiments on real-world image and tabular data, observing statistically significant reduction in utility. Finally, we model the trade-off between the utility drop and the cost of repeated (learner-specific) prefiltering within a game-theoretic framework and showcase benefits of LARP for large datasets.         ",
    "url": "https://arxiv.org/abs/2506.20573",
    "authors": [
      "Kristian Minchev",
      "Dimitar Iliev Dimitrov",
      "Nikola Konstantinov"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.23664",
    "title": "Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation",
    "abstract": "           Medical image data is less accessible than in other domains due to privacy and regulatory constraints. In addition, labeling requires costly, time-intensive manual image annotation by clinical experts. To overcome these challenges, synthetic medical data generation offers a promising solution. Generative AI (GenAI), employing generative deep learning models, has proven effective at producing realistic synthetic images. This study proposes a novel mask-guided GenAI approach using diffusion models to generate synthetic fetal head ultrasound images paired with segmentation masks. These synthetic pairs augment real datasets for supervised fine-tuning of the Segment Anything Model (SAM). Our results show that the synthetic data captures real image features effectively, and this approach reaches state-of-the-art fetal head segmentation, especially when trained with a limited number of real image-mask pairs. In particular, the segmentation reaches Dice Scores of 94.66\\% and 94.38\\% using a handful of ultrasound images from the Spanish and African cohorts, respectively. Our code, models, and data are available on GitHub.         ",
    "url": "https://arxiv.org/abs/2506.23664",
    "authors": [
      "Fangyijie Wang",
      "Kevin Whelan",
      "F\u00e9lix Balado",
      "Kathleen M. Curran",
      "Gu\u00e9nol\u00e9 Silvestre"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.02644",
    "title": "Solving the Hubbard model with Neural Quantum States",
    "abstract": "           The rapid development of neural quantum states (NQS) has established it as a promising framework for studying quantum many-body systems. In this work, by leveraging the cutting-edge transformer-based architectures and developing highly efficient optimization algorithms, we achieve the state-of-the-art results for the doped two-dimensional (2D) Hubbard model, arguably the minimum model for high-Tc superconductivity. Interestingly, we find different attention heads in the NQS ansatz can directly encode correlations at different scales, making it capable of capturing long-range correlations and entanglements in strongly correlated systems. With these advances, we establish the half-filled stripe in the ground state of 2D Hubbard model with the next nearest neighboring hoppings, consistent with experimental observations in cuprates. Our work establishes NQS as a powerful tool for solving challenging many-fermions systems.         ",
    "url": "https://arxiv.org/abs/2507.02644",
    "authors": [
      "Yuntian Gu",
      "Wenrui Li",
      "Heng Lin",
      "Bo Zhan",
      "Ruichen Li",
      "Yifei Huang",
      "Di He",
      "Yantao Wu",
      "Tao Xiang",
      "Mingpu Qin",
      "Liwei Wang",
      "Dingshun Lv"
    ],
    "subjectives": [
      "Strongly Correlated Electrons (cond-mat.str-el)",
      "Artificial Intelligence (cs.AI)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2507.03421",
    "title": "Hybrid-View Attention Network for Clinically Significant Prostate Cancer Classification in Transrectal Ultrasound",
    "abstract": "           Prostate cancer (PCa) is a leading cause of cancer-related mortality in men, and accurate identification of clinically significant PCa (csPCa) is critical for timely intervention. Transrectal ultrasound (TRUS) is widely used for prostate biopsy; however, its low contrast and anisotropic spatial resolution pose diagnostic challenges. To address these limitations, we propose a novel hybrid-view attention (HVA) network for csPCa classification in 3D TRUS that leverages complementary information from transverse and sagittal views. Our approach integrates a CNN-transformer hybrid architecture, where convolutional layers extract fine-grained local features and transformer-based HVA models global dependencies. Specifically, the HVA comprises intra-view attention to refine features within a single view and cross-view attention to incorporate complementary information across views. Furthermore, a hybrid-view adaptive fusion module dynamically aggregates features along both channel and spatial dimensions, enhancing the overall representation. Experiments are conducted on an in-house dataset containing 590 subjects who underwent prostate biopsy. Comparative and ablation results prove the efficacy of our method. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.03421",
    "authors": [
      "Zetian Feng",
      "Juan Fu",
      "Xuebin Zou",
      "Hongsheng Ye",
      "Hong Wu",
      "Jianhua Zhou",
      "Yi Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  }
]