[
  {
    "id": "arXiv:2507.11547",
    "title": "Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming",
    "abstract": "           In recent years, various artificial intelligence-based surrogate models have been proposed to provide rapid manufacturability predictions of material forming processes. However, traditional AI-based surrogate models, typically built with scalar or image-based neural networks, are limited in their ability to capture complex 3D spatial relationships and to operate in a permutation-invariant manner. To overcome these issues, emerging graph-based surrogate models are developed using graph neural networks. This study developed a new graph neural network surrogate model named Recurrent U Net-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate predictions of sheet material deformation fields across multiple forming timesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model temporal dynamics and a U-Net inspired graph-based downsample/upsample mechanism to handle spatial long-range dependencies. A novel 'node-to-surface' contact representation method was proposed, offering significant improvements in computational efficiency for large-scale contact interactions. The RUGNN model was validated using a cold forming case study and a more complex hot forming case study using aluminium alloys. Results demonstrate that the RUGNN model provides accurate deformation predictions closely matching ground truth FE simulations and outperforming several baseline GNN architectures. Model tuning was also performed to identify suitable hyperparameters, training strategies, and input feature representations. These results demonstrate that RUGNN is a reliable approach to support sheet material forming design by enabling accurate manufacturability predictions.         ",
    "url": "https://arxiv.org/abs/2507.11547",
    "authors": [
      "Yingxue Zhao",
      "Qianyi Chen",
      "Haoran Li",
      "Haosu Zhou",
      "Hamid Reza Attar",
      "Tobias Pfaff",
      "Tailin Wu",
      "Nan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.11549",
    "title": "An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search",
    "abstract": "           Deformable Attention Transformers (DAT) have shown remarkable performance in computer vision tasks by adaptively focusing on informative image regions. However, their data-dependent sampling mechanism introduces irregular memory access patterns, posing significant challenges for efficient hardware deployment. Existing acceleration methods either incur high hardware overhead or compromise model accuracy. To address these issues, this paper proposes a hardware-friendly optimization framework for DAT. First, a neural architecture search (NAS)-based method with a new slicing strategy is proposed to automatically divide the input feature into uniform patches during the inference process, avoiding memory conflicts without modifying model architecture. The method explores the optimal slice configuration by jointly optimizing hardware cost and inference accuracy. Secondly, an FPGA-based verification system is designed to test the performance of this framework on edge-side hardware. Algorithm experiments on the ImageNet-1K dataset demonstrate that our hardware-friendly framework can maintain have only 0.2% accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA show the proposed method reduces DRAM access times to 18% compared with existing DAT acceleration methods.         ",
    "url": "https://arxiv.org/abs/2507.11549",
    "authors": [
      "Wendong Mao",
      "Mingfan Zhao",
      "Jianfeng Guan",
      "Qiwei Dong",
      "Zhongfeng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.11550",
    "title": "Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction",
    "abstract": "           Spatio-temporal traffic prediction plays a key role in intelligent transportation systems by enabling accurate prediction in complex urban areas. Although not only accuracy but also efficiency for scalability is important, some previous methods struggle to capture heterogeneity such as varying traffic patterns across regions and time periods. Moreover, Graph Neural Networks (GNNs), which are the mainstream of traffic prediction, not only require predefined adjacency matrix, but also limit scalability to large-scale data containing many nodes due to their inherent complexity. To overcome these limitations, we propose Deformable Dynamic Convolution Network (DDCN) for accurate yet efficient traffic prediction. Traditional Convolutional Neural Networks (CNNs) are limited in modeling non-Euclidean spatial structures and spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically applying deformable filters based on offset. Specifically, DDCN decomposes transformer-style CNN to encoder-decoder structure, and applies proposed approaches to the spatial and spatio-temporal attention blocks of the encoder to emphasize important features. The decoder, composed of feed-forward module, complements the output of the encoder. This novel structure make DDCN can perform accurate yet efficient traffic prediction. In comprehensive experiments on four real-world datasets, DDCN achieves competitive performance, emphasizing the potential and effectiveness of CNN-based approaches for spatio-temporal traffic prediction.         ",
    "url": "https://arxiv.org/abs/2507.11550",
    "authors": [
      "Hyeonseok Jin",
      "Geonmin Kim",
      "Kyungbaek Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.11559",
    "title": "RSD-15K: A Large-Scale User-Level Annotated Dataset for Suicide Risk Detection on Social Media",
    "abstract": "           In recent years, cognitive and mental health (CMH) disorders have increasingly become an important challenge for global public health, especially the suicide problem caused by multiple factors such as social competition, economic pressure and interpersonal relationships among young and middle-aged people. Social media, as an important platform for individuals to express emotions and seek help, provides the possibility for early detection and intervention of suicide risk. This paper introduces a large-scale dataset containing 15,000 user-level posts. Compared with existing datasets, this dataset retains complete user posting time sequence information, supports modeling the dynamic evolution of suicide risk, and we have also conducted comprehensive and rigorous annotations on these datasets. In the benchmark experiment, we systematically evaluated the performance of traditional machine learning methods, deep learning models, and fine-tuned large language models. The experimental results show that our dataset can effectively support the automatic assessment task of suicide risk. Considering the sensitivity of mental health data, we also discussed the privacy protection and ethical use of the dataset. In addition, we also explored the potential applications of the dataset in mental health testing, clinical psychiatric auxiliary treatment, etc., and provided directional suggestions for future research work.         ",
    "url": "https://arxiv.org/abs/2507.11559",
    "authors": [
      "Shouwen Zheng",
      "Yingzhi Tao",
      "Taiqi Zhou"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2507.11566",
    "title": "Emergent Heterogeneous Swarm Control Through Hebbian Learning",
    "abstract": "           In this paper, we introduce Hebbian learning as a novel method for swarm robotics, enabling the automatic emergence of heterogeneity. Hebbian learning presents a biologically inspired form of neural adaptation that solely relies on local information. By doing so, we resolve several major challenges for learning heterogeneous control: 1) Hebbian learning removes the complexity of attributing emergent phenomena to single agents through local learning rules, thus circumventing the micro-macro problem; 2) uniform Hebbian learning rules across all swarm members limit the number of parameters needed, mitigating the curse of dimensionality with scaling swarm sizes; and 3) evolving Hebbian learning rules based on swarm-level behaviour minimises the need for extensive prior knowledge typically required for optimising heterogeneous swarms. This work demonstrates that with Hebbian learning heterogeneity naturally emerges, resulting in swarm-level behavioural switching and in significantly improved swarm capabilities. It also demonstrates how the evolution of Hebbian learning rules can be a valid alternative to Multi Agent Reinforcement Learning in standard benchmarking tasks.         ",
    "url": "https://arxiv.org/abs/2507.11566",
    "authors": [
      "Fuda van Diggelen",
      "Tugay Alperen Karag\u00fczel",
      "Andres Garcia Rincon",
      "A.E. Eiben",
      "Dario Floreano",
      "Eliseo Ferrante"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.11570",
    "title": "SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery",
    "abstract": "           Objective: To develop and evaluate machine learning (ML) models for predicting length of stay (LOS) in elective spine surgery, with a focus on the benefits of temporal modeling and model interpretability. Materials and Methods: We compared traditional ML models (e.g., linear regression, random forest, support vector machine (SVM), and XGBoost) with our developed model, SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an attention, using structured perioperative electronic health records (EHR) data. Performance was evaluated using the coefficient of determination (R2), and key predictors were identified using explainable AI. Results: SurgeryLSTM achieved the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85) and baseline models. The attention mechanism improved interpretability by dynamically identifying influential temporal segments within preoperative clinical sequences, allowing clinicians to trace which events or features most contributed to each LOS prediction. Key predictors of LOS included bone disorder, chronic kidney disease, and lumbar fusion identified as the most impactful predictors of LOS. Discussion: Temporal modeling with attention mechanisms significantly improves LOS prediction by capturing the sequential nature of patient data. Unlike static models, SurgeryLSTM provides both higher accuracy and greater interpretability, which are critical for clinical adoption. These results highlight the potential of integrating attention-based temporal models into hospital planning workflows. Conclusion: SurgeryLSTM presents an effective and interpretable AI solution for LOS prediction in elective spine surgery. Our findings support the integration of temporal, explainable ML approaches into clinical decision support systems to enhance discharge readiness and individualized patient care.         ",
    "url": "https://arxiv.org/abs/2507.11570",
    "authors": [
      "Ha Na Cho",
      "Sairam Sutari",
      "Alexander Lopez",
      "Hansen Bow",
      "Kai Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2507.11574",
    "title": "Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators",
    "abstract": "           Robust uncertainty quantification (UQ) remains a critical barrier to the safe deployment of deep learning in real-time virtual sensing, particularly in high-stakes domains where sparse, noisy, or non-collocated sensor data are the norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework that transforms neural operator-based virtual sensing with calibrated, distribution-free prediction intervals. By unifying Monte Carlo dropout with split conformal prediction in a single DeepONet architecture, CMCO achieves spatially resolved uncertainty estimates without retraining, ensembling, or custom loss design. Our method addresses a longstanding challenge: how to endow operator learning with efficient and reliable UQ across heterogeneous domains. Through rigorous evaluation on three distinct applications: turbulent flow, elastoplastic deformation, and global cosmic radiation dose estimation-CMCO consistently attains near-nominal empirical coverage, even in settings with strong spatial gradients and proxy-based sensing. This breakthrough offers a general-purpose, plug-and-play UQ solution for neural operators, unlocking real-time, trustworthy inference in digital twins, sensor fusion, and safety-critical monitoring. By bridging theory and deployment with minimal computational overhead, CMCO establishes a new foundation for scalable, generalizable, and uncertainty-aware scientific machine learning.         ",
    "url": "https://arxiv.org/abs/2507.11574",
    "authors": [
      "Kazuma Kobayashi",
      "Shailesh Garg",
      "Farid Ahmed",
      "Souvik Chakraborty",
      "Syed Bahauddin Alam"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2507.11589",
    "title": "Einstein Fields: A Neural Perspective To Computational General Relativity",
    "abstract": "           We introduce Einstein Fields, a neural representation that is designed to compress computationally intensive four-dimensional numerical relativity simulations into compact implicit neural network weights. By modeling the \\emph{metric}, which is the core tensor field of general relativity, Einstein Fields enable the derivation of physical quantities via automatic differentiation. However, unlike conventional neural fields (e.g., signed distance, occupancy, or radiance fields), Einstein Fields are \\emph{Neural Tensor Fields} with the key difference that when encoding the spacetime geometry of general relativity into neural field representations, dynamics emerge naturally as a byproduct. Einstein Fields show remarkable potential, including continuum modeling of 4D spacetime, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use. We address these challenges across several canonical test beds of general relativity and release an open source JAX-based library, paving the way for more scalable and expressive approaches to numerical relativity. Code is made available at this https URL ",
    "url": "https://arxiv.org/abs/2507.11589",
    "authors": [
      "Sandeep Suresh Cranganore",
      "Andrei Bodnar",
      "Arturs Berzins",
      "Johannes Brandstetter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "General Relativity and Quantum Cosmology (gr-qc)"
    ]
  },
  {
    "id": "arXiv:2507.11638",
    "title": "Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders",
    "abstract": "           Effective treatment for rectal cancer relies on accurate lymph node metastasis (LNM) staging. However, radiological criteria based on lymph node (LN) size, shape and texture morphology have limited diagnostic accuracy. In this work, we investigate applying a Variational Autoencoder (VAE) as a feature encoder model to replace the large pre-trained Convolutional Neural Network (CNN) used in existing approaches. The motivation for using a VAE is that the generative model aims to reconstruct the images, so it directly encodes visual features and meaningful patterns across the data. This leads to a disentangled and structured latent space which can be more interpretable than a CNN. Models are deployed on an in-house MRI dataset with 168 patients who did not undergo neo-adjuvant treatment. The post-operative pathological N stage was used as the ground truth to evaluate model predictions. Our proposed model 'VAE-MLP' achieved state-of-the-art performance on the MRI dataset, with cross-validated metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85 +/- 0.05. Code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2507.11638",
    "authors": [
      "Benjamin Keel",
      "Aaron Quyn",
      "David Jayne",
      "Maryam Mohsin",
      "Samuel D. Relton"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.11640",
    "title": "Quantifying data needs in surrogate modeling for flow fields in 2D stirred tanks with physics-informed neural networks (PINNs)",
    "abstract": "           Stirred tanks are vital in chemical and biotechnological processes, particularly as bioreactors. Although computational fluid dynamics (CFD) is widely used to model the flow in stirred tanks, its high computational cost$-$especially in multi-query scenarios for process design and optimization$-$drives the need for efficient data-driven surrogate models. However, acquiring sufficiently large datasets can be costly. Physics-informed neural networks (PINNs) offer a promising solution to reduce data requirements while maintaining accuracy by embedding underlying physics into neural network (NN) training. This study quantifies the data requirements of vanilla PINNs for developing surrogate models of a flow field in a 2D stirred tank. We compare these requirements with classical supervised neural networks and boundary-informed neural networks (BINNs). Our findings demonstrate that surrogate models can achieve prediction errors around 3% across Reynolds numbers from 50 to 5000 using as few as six datapoints. Moreover, employing an approximation of the velocity profile in place of real data labels leads to prediction errors of around 2.5%. These results indicate that even with limited or approximate datasets, PINNs can be effectively trained to deliver high accuracy comparable to high-fidelity data.         ",
    "url": "https://arxiv.org/abs/2507.11640",
    "authors": [
      "Veronika Tr\u00e1vn\u00edkov\u00e1",
      "Eric von Lieres",
      "Marek Behr"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2507.11645",
    "title": "Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation",
    "abstract": "           Grokking refers to delayed generalization in which the increase in test accuracy of a neural network occurs appreciably after the improvement in training accuracy This paper introduces several practical metrics including variance under dropout, robustness, embedding similarity, and sparsity measures, that can forecast grokking behavior. Specifically, the resilience of neural networks to noise during inference is estimated from a Dropout Robustness Curve (DRC) obtained from the variation of the accuracy with the dropout rate as the model transitions from memorization to generalization. The variance of the test accuracy under stochastic dropout across training checkpoints further exhibits a local maximum during the grokking. Additionally, the percentage of inactive neurons decreases during generalization, while the embeddings tend to a bimodal distribution independent of initialization that correlates with the observed cosine similarity patterns and dataset symmetries. These metrics additionally provide valuable insight into the origin and behaviour of grokking.         ",
    "url": "https://arxiv.org/abs/2507.11645",
    "authors": [
      "Ahmed Salah",
      "David Yevick"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.11660",
    "title": "STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics",
    "abstract": "           The advent of single-cell technology has significantly improved our understanding of cellular states and subpopulations in various tissues under normal and diseased conditions by employing data-driven approaches such as clustering and trajectory inference. However, these methods consider cells as independent data points of population distributions. With spatial transcriptomics, we can represent cellular organization, along with dynamic cell-cell interactions that lead to changes in cell state. Still, key computational advances are necessary to enable the data-driven learning of such complex interactive cellular dynamics. While agent-based modeling (ABM) provides a powerful framework, traditional approaches rely on handcrafted rules derived from domain knowledge rather than data-driven approaches. To address this, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED) integrating ABM with deep learning to model intercellular communication, and its effect on the intracellular gene regulatory network. Using graph ODE networks (GDEs) with shared weights per cell type, our approach represents genes as vertices and interactions as directed edges, dynamically learning their strengths through a designed attention mechanism. Trained to match continuous trajectories of simulated as well as inferred trajectories from spatial transcriptomics data, the model captures both intercellular and intracellular interactions, enabling a more adaptive and accurate representation of cellular dynamics.         ",
    "url": "https://arxiv.org/abs/2507.11660",
    "authors": [
      "Joao F. Rocha",
      "Ke Xu",
      "Xingzhi Sun",
      "Ananya Krishna",
      "Dhananjay Bhaskar",
      "Blanche Mongeon",
      "Morgan Craig",
      "Mark Gerstein",
      "Smita Krishnaswamy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2507.11687",
    "title": "MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization",
    "abstract": "           Large Language Models, though successful in code generation, struggle with code quality analysis because they are limited by static training data and can't easily adapt to evolving best practices. We introduce MetaLint, a new instruction-following framework that formulates code quality analysis as the task of detecting and fixing problematic semantic code fragments or code idioms based on high-level specifications. Unlike conventional approaches that train models on static, rule-based data, MetaLint employs instruction tuning on synthetic linter-generated data to support easy-to-hard generalization, enabling models to adapt to novel or complex code patterns without retraining. To evaluate this, we construct a benchmark of challenging idioms inspired by real-world coding standards such as Python Enhancement Proposals (PEPs) and assess whether MetaLint-trained models reason adaptively or simply memorize. Our results show that MetaLint improves generalization to unseen PEP idioms, achieving a 70.37% F-score on idiom detection with the highest recall (70.43%) among all evaluated models. It also achieves 26.73% on localization, competitive for its 4B parameter size and comparable to larger state-of-the-art models like o3-mini, highlighting its potential for future-proof code quality analysis.         ",
    "url": "https://arxiv.org/abs/2507.11687",
    "authors": [
      "Atharva Naik",
      "Lawanya Baghel",
      "Dhakshin Govindarajan",
      "Darsh Agrawal",
      "Daniel Fried",
      "Carolyn Rose"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.11690",
    "title": "The Impact of Coreset Selection on Spurious Correlations and Group Robustness",
    "abstract": "           Coreset selection methods have shown promise in reducing the training data size while maintaining model performance for data-efficient machine learning. However, as many datasets suffer from biases that cause models to learn spurious correlations instead of causal features, it is important to understand whether and how dataset reduction methods may perpetuate, amplify, or mitigate these biases. In this work, we conduct the first comprehensive analysis of the implications of data selection on the spurious bias levels of the selected coresets and the robustness of downstream models trained on them. We use an extensive experimental setting spanning ten different spurious correlations benchmarks, five score metrics to characterize sample importance/ difficulty, and five data selection policies across a broad range of coreset sizes. Thereby, we unravel a series of nontrivial nuances in interactions between sample difficulty and bias alignment, as well as dataset bias and resultant model robustness. For example, we find that selecting coresets using embedding-based sample characterization scores runs a comparatively lower risk of inadvertently exacerbating bias than selecting using characterizations based on learning dynamics. Most importantly, our analysis reveals that although some coreset selection methods could achieve lower bias levels by prioritizing difficult samples, they do not reliably guarantee downstream robustness.         ",
    "url": "https://arxiv.org/abs/2507.11690",
    "authors": [
      "Amaya Dharmasiri",
      "William Yang",
      "Polina Kirichenko",
      "Lydia Liu",
      "Olga Russakovsky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.11706",
    "title": "Reinforcement Learning from Adversarial Preferences in Tabular MDPs",
    "abstract": "           We introduce a new framework of episodic tabular Markov decision processes (MDPs) with adversarial preferences, which we refer to as preference-based MDPs (PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the numerical value of the loss is directly observed, in PbMDPs the learner instead observes preferences between two candidate arms, which represent the choices being compared. In this work, we focus specifically on the setting where the reward functions are determined by Borda scores. We begin by establishing a regret lower bound for PbMDPs with Borda scores. As a preliminary step, we present a simple instance to prove a lower bound of $\\Omega(\\sqrt{HSAT})$ for episodic MDPs with adversarial losses, where $H$ is the number of steps per episode, $S$ is the number of states, $A$ is the number of actions, and $T$ is the number of episodes. Leveraging this construction, we then derive a regret lower bound of $\\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda scores, where $K$ is the number of arms. Next, we develop algorithms that achieve a regret bound of order $T^{2/3}$. We first propose a global optimization approach based on online linear optimization over the set of all occupancy measures, achieving a regret bound of $\\tilde{O}((H^2 S^2 K)^{1/3} T^{2/3} )$ under known transitions. However, this approach suffers from suboptimal dependence on the potentially large number of states $S$ and computational inefficiency. To address this, we propose a policy optimization algorithm whose regret is roughly bounded by $\\tilde{O}( (H^6 S K^5)^{1/3} T^{2/3} )$ under known transitions, and further extend the result to the unknown-transition setting.         ",
    "url": "https://arxiv.org/abs/2507.11706",
    "authors": [
      "Taira Tsuchiya",
      "Shinji Ito",
      "Haipeng Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2507.11721",
    "title": "Evasion Under Blockchain Sanctions",
    "abstract": "           Sanctioning blockchain addresses has become a common regulatory response to malicious activities. However, enforcement on permissionless blockchains remains challenging due to complex transaction flows and sophisticated fund-obfuscation techniques. Using cryptocurrency mixing tool Tornado Cash as a case study, we quantitatively assess the effectiveness of U.S. Office of Foreign Assets Control (OFAC) sanctions over a 957-day period, covering 6.79 million Ethereum blocks and 1.07 billion transactions. Our analysis reveals that while OFAC sanctions reduced overall Tornado Cash deposit volume by 71.03% to approximately 2 billion USD, attackers still relied on Tornado Cash in 78.33% of Ethereum-related security incidents, underscoring persistent evasion strategies. We identify three structural limitations in current sanction enforcement practices: (i) the susceptibility of binary sanction classifications to dusting attacks; (ii) fragmented censorship by blockchain producers; and (iii) the complexity of obfuscation services exploited by users. To address these gaps, we introduce a more practical algorithm for scoring and tracking, grounded in quantitative impurity. On average, our algorithm processes Ethereum blocks within 0.07 $\\pm$ 0.03 seconds and achieves 97.61% precision and 74.08% recall when evaluated on the Bybit exploit. Our findings contribute to ongoing discussions around regulatory effectiveness in Decentralized Finance by providing empirical evidence, clarifying enforcement challenges, and informing future compliance strategies in response to sanctions and blockchain-based security risks.         ",
    "url": "https://arxiv.org/abs/2507.11721",
    "authors": [
      "Endong Liu",
      "Mark Ryan",
      "Liyi Zhou",
      "Pascal Berrang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2507.11731",
    "title": "Picat Through the Lens of Advent of Code",
    "abstract": "           Picat is a logic-based, multi-paradigm programming language that integrates features from logic, functional, constraint, and imperative programming paradigms. This paper presents solutions to several problems from the 2024 Advent of Code (AoC). While AoC problems are not designed for any specific programming language, certain problem types, such as reverse engineering and path-finding, are particularly well-suited to Picat due to its built-in constraint solving, pattern matching, backtracking, and dynamic programming with tabling. This paper demonstrates that Picat's features, especially its SAT-based constraint solving and tabling, enable concise, declarative, and highly efficient implementations of problems that would require significantly more effort in imperative languages.         ",
    "url": "https://arxiv.org/abs/2507.11731",
    "authors": [
      "Neng-Fa Zhou",
      "Cristian Grozea",
      "H\u00e5kan Kjellerstrand",
      "Ois\u00edn Mac Fheara\u00ed"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2507.11732",
    "title": "Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning",
    "abstract": "           Graph neural networks (GNNs) have emerged as a powerful framework for a wide range of node-level graph learning tasks. However, their performance is often constrained by reliance on random or minimally informed initial feature representations, which can lead to slow convergence and suboptimal solutions. In this paper, we leverage a statistically grounded method, one-hot graph encoder embedding (GEE), to generate high-quality initial node features that enhance the end-to-end training of GNNs. We refer to this integrated framework as the GEE-powered GNN (GG), and demonstrate its effectiveness through extensive simulations and real-world experiments across both unsupervised and supervised settings. In node clustering, GG consistently achieves state-of-the-art performance, ranking first across all evaluated real-world datasets, while exhibiting faster convergence compared to the standard GNN. For node classification, we further propose an enhanced variant, GG-C, which concatenates the outputs of GG and GEE and outperforms competing baselines. These results confirm the importance of principled, structure-aware feature initialization in realizing the full potential of GNNs.         ",
    "url": "https://arxiv.org/abs/2507.11732",
    "authors": [
      "Shiyu Chen",
      "Cencheng Shen",
      "Youngser Park",
      "Carey E. Priebe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2507.11739",
    "title": "Sparse Identification of Nonlinear Dynamics with Conformal Prediction",
    "abstract": "           The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for discovering nonlinear dynamical system models from data. Quantifying uncertainty in SINDy models is essential for assessing their reliability, particularly in safety-critical applications. While various uncertainty quantification methods exist for SINDy, including Bayesian and ensemble approaches, this work explores the integration of Conformal Prediction, a framework that can provide valid prediction intervals with coverage guarantees based on minimal assumptions like data exchangeability. We introduce three applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1) quantifying uncertainty in time series prediction, (2) model selection based on library feature importance, and (3) quantifying the uncertainty of identified model coefficients using feature conformal prediction. We demonstrate the three applications on stochastic predator-prey dynamics and several chaotic dynamical systems. We show that conformal prediction methods integrated with E-SINDy can reliably achieve desired target coverage for time series forecasting, effectively quantify feature importance, and produce more robust uncertainty intervals for model coefficients, even under non-Gaussian noise, compared to standard E-SINDy coefficient estimates.         ",
    "url": "https://arxiv.org/abs/2507.11739",
    "authors": [
      "Urban Fasel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2507.11751",
    "title": "Survey of Genetic and Differential Evolutionary Algorithm Approaches to Search Documents Based On Semantic Similarity",
    "abstract": "           Identifying similar documents within extensive volumes of data poses a significant challenge. To tackle this issue, researchers have developed a variety of effective distributed computing techniques. With the advancement of computing power and the rise of big data, deep neural networks and evolutionary computing algorithms such as genetic algorithms and differential evolution algorithms have achieved greater success. This survey will explore the most recent advancements in the search for documents based on their semantic text similarity, focusing on genetic and differential evolutionary computing algorithms.         ",
    "url": "https://arxiv.org/abs/2507.11751",
    "authors": [
      "Chandrashekar Muniyappa",
      "Eunjin Kim"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.11757",
    "title": "A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction",
    "abstract": "           Accurately predicting drug-target interactions (DTIs) is pivotal for advancing drug discovery and target validation techniques. While machine learning approaches including those that are based on Graph Neural Networks (GNN) have achieved notable success in DTI prediction, many of them have difficulties in effectively integrating the diverse features of drugs, targets and their interactions. To address this limitation, we introduce a novel framework to take advantage of the power of both transductive learning and inductive learning so that features at molecular level and drug-target interaction network level can be exploited. Within this framework is a GNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and target molecular structures as meta-nodes in a drug-target interaction graph, enabling a detailed exploration of their intricate relationships. To evaluate the proposed model, we have compiled a special benchmark comprising drug SMILES, protein sequences, and their interaction data, which is interesting in its own right. Our experimental results demonstrate that the GiG model significantly outperforms existing approaches across all evaluation metrics, highlighting the benefits of integrating different learning paradigms and interaction data.         ",
    "url": "https://arxiv.org/abs/2507.11757",
    "authors": [
      "Yuehua Song",
      "Yong Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2507.11764",
    "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles",
    "abstract": "           This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).         ",
    "url": "https://arxiv.org/abs/2507.11764",
    "authors": [
      "Matteo Fasulo",
      "Luca Babboni",
      "Luca Tedeschini"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2507.11770",
    "title": "Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies",
    "abstract": "           In robotics, the effective integration of environmental data into actionable knowledge remains a significant challenge due to the variety and incompatibility of data formats commonly used in scene descriptions, such as MJCF, URDF, and SDF. This paper presents a novel approach that addresses these challenges by developing a unified scene graph model that standardizes these varied formats into the Universal Scene Description (USD) format. This standardization facilitates the integration of these scene graphs with robot ontologies through semantic reporting, enabling the translation of complex environmental data into actionable knowledge essential for cognitive robotic control. We evaluated our approach by converting procedural 3D environments into USD format, which is then annotated semantically and translated into a knowledge graph to effectively answer competency questions, demonstrating its utility for real-time robotic decision-making. Additionally, we developed a web-based visualization tool to support the semantic mapping process, providing users with an intuitive interface to manage the 3D environment.         ",
    "url": "https://arxiv.org/abs/2507.11770",
    "authors": [
      "Giang Nguyen",
      "Mihai Pomarlan",
      "Sascha Jongebloed",
      "Nils Leusmann",
      "Minh Nhat Vu",
      "Michael Beetz"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.11772",
    "title": "How To Mitigate And Defend Against DDoS Attacks In IoT Devices",
    "abstract": "           Distributed Denial of Service (DDoS) attacks have become increasingly prevalent and dangerous in the context of Internet of Things (IoT) networks, primarily due to the low-security configurations of many connected devices. This paper analyzes the nature and impact of DDoS attacks such as those launched by the Mirai botnet, and proposes layered mitigation strategies tailored to IoT environments. Key solutions explored include IPv6 Unique Local Addresses (ULA), edge computing, software-defined networking (SDN), honeypot deception, and machine learning-based intrusion detection systems. The paper aims to help engineers and researchers understand and implement practical countermeasures to protect IoT infrastructures.         ",
    "url": "https://arxiv.org/abs/2507.11772",
    "authors": [
      "Ifiyemi Leigha",
      "Basak Comlekcioglu",
      "Maria Pilar Bezanilla"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2507.11776",
    "title": "Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network",
    "abstract": "           The Dutch railway network is one of the busiest in the world, with delays being a prominent concern for the principal passenger railway operator NS. This research addresses a gap in delay prediction studies within the Dutch railway network by employing an XGBoost Classifier with a focus on topological features. Current research predominantly emphasizes short-term predictions and neglects the broader network-wide patterns essential for mitigating ripple effects. This research implements and improves an existing methodology, originally designed to forecast the evolution of the fast-changing US air network, to predict delays in the Dutch Railways. By integrating Node Centrality Measures and comparing multiple classifiers like RandomForest, DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is to predict delayed trajectories. However, the results reveal limited performance, especially in non-simultaneous testing scenarios, suggesting the necessity for more context-specific adaptations. Regardless, this research contributes to the understanding of transportation network evaluation and proposes future directions for developing more robust predictive models for delays.         ",
    "url": "https://arxiv.org/abs/2507.11776",
    "authors": [
      "Merel Kampere",
      "Ali Mohammed Mansoor Alsahag"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.11777",
    "title": "Towards Scalable AASIST: Refining Graph Attention for Speech Deepfake Detection",
    "abstract": "           Advances in voice conversion and text-to-speech synthesis have made automatic speaker verification (ASV) systems more susceptible to spoofing attacks. This work explores modest refinements to the AASIST anti-spoofing architecture. It incorporates a frozen Wav2Vec 2.0 encoder to retain self-supervised speech representations in limited-data settings, substitutes the original graph attention block with a standardized multi-head attention module using heterogeneous query projections, and replaces heuristic frame-segment fusion with a trainable, context-aware integration layer. When evaluated on the ASVspoof 5 corpus, the proposed system reaches a 7.6\\% equal error rate (EER), improving on a re-implemented AASIST baseline under the same training conditions. Ablation experiments suggest that each architectural change contributes to the overall performance, indicating that targeted adjustments to established models may help strengthen speech deepfake detection in practical scenarios. The code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.11777",
    "authors": [
      "Ivan Viakhirev",
      "Daniil Sirota",
      "Aleksandr Smirnov",
      "Kirill Borodin"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2507.11808",
    "title": "New allocation rule based on graph structures and their application to economic phenomena",
    "abstract": "           This study introduces the \\emph{edge-based Shapley value}, a novel allocation rule within cooperative game theory, specifically tailored for networked systems, where value is generated through interactions represented by edges. Traditional allocation rules, such as the Shapley and Myerson values, evaluate player contributions based on node-level characteristics, or connected components. However, these approaches often fail to adequately capture the functional role of edges, which are crucial in systems such as supply chains and digital platforms, where interactions, rather than individual agents, are the primary drivers of value. Our edge-based Shapley value shifts the characteristic function from node sets to edge sets, thereby enabling a more granular and context-sensitive evaluation of the contributions. We establish its theoretical foundations, demonstrate its relationship to classical allocation rules, and show that it retains key properties such as fairness and symmetry. To illustrate its applicability, we present two use cases: content platform networks and supply chain logistics (SCL). In both cases, our method produces intuitive and structurally consistent allocations, particularly in scenarios with overlapping routes, exclusive contracts or cost-sensitive paths. This framework offers a new perspective on value attribution in cooperative settings with complex interaction structures and provides practical tools for analyzing real-world economic and logistical networks.         ",
    "url": "https://arxiv.org/abs/2507.11808",
    "authors": [
      "Taiki Yamada",
      "Taisuke Matsubae",
      "Tomoya Akamatsu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2507.11812",
    "title": "A Multimodal Data Fusion Generative Adversarial Network for Real Time Underwater Sound Speed Field Construction",
    "abstract": "           Sound speed profiles (SSPs) are essential parameters underwater that affects the propagation mode of underwater signals and has a critical impact on the energy efficiency of underwater acoustic communication and accuracy of underwater acoustic positioning. Traditionally, SSPs can be obtained by matching field processing (MFP), compressive sensing (CS), and deep learning (DL) methods. However, existing methods mainly rely on on-site underwater sonar observation data, which put forward strict requirements on the deployment of sonar observation systems. To achieve high-precision estimation of sound velocity distribution in a given sea area without on-site underwater data measurement, we propose a multi-modal data-fusion generative adversarial network model with residual attention block (MDF-RAGAN) for SSP construction. To improve the model's ability for capturing global spatial feature correlations, we embedded the attention mechanisms, and use residual modules for deeply capturing small disturbances in the deep ocean sound velocity distribution caused by changes of SST. Experimental results on real open dataset show that the proposed model outperforms other state-of-the-art methods, which achieves an accuracy with an error of less than 0.3m/s. Specifically, MDF-RAGAN not only outperforms convolutional neural network (CNN) and spatial interpolation (SITP) by nearly a factor of two, but also achieves about 65.8\\% root mean square error (RMSE) reduction compared to mean profile, which fully reflects the enhancement of overall profile matching by multi-source fusion and cross-modal attention.         ",
    "url": "https://arxiv.org/abs/2507.11812",
    "authors": [
      "Wei Huang",
      "Yuqiang Huang",
      "Yanan Wu",
      "Tianhe Xu",
      "Junting Wang",
      "Hao Zhang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2507.11836",
    "title": "HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction",
    "abstract": "           Dynamic link prediction in continuous-time dynamic graphs is a fundamental task for modeling evolving complex systems. Existing node-centric and event-centric methods focus on individual interactions or atomic states, failing to capture the structural cohesion of composite hyper-events, groups of causally related events. To address this, we propose HyperEvent, a framework reframing dynamic link prediction as hyper-event recognition. Central to HyperEvent is the dynamic construction of an association sequence using event correlation vectors. These vectors quantify pairwise dependencies between the query event and relevant historical events, thereby characterizing the structural cohesion of a potential hyper-event. The framework predicts the occurrence of the query event by evaluating whether it collectively forms a valid hyper-event with these historical events. Notably, HyperEvent outperforms state-of-the-art methods on 4 out of 5 datasets in the official leaderboard. For scalability, we further introduce an efficient parallel training algorithm that segments large event streams to enable concurrent training. Experiments validate HyperEvent's superior accuracy and efficiency on large-scale graphs. Among which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank over state-of-the-art baseline on the large-scale Flight dataset while utilizing only 10.17% of the training time.         ",
    "url": "https://arxiv.org/abs/2507.11836",
    "authors": [
      "Jian Gao",
      "Jianshe Wu",
      "JingYi Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.11845",
    "title": "ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification",
    "abstract": "           Open-set few-shot image classification aims to train models using a small amount of labeled data, enabling them to achieve good generalization when confronted with unknown environments. Existing methods mainly use visual information from a single image to learn class representations to distinguish known from unknown categories. However, these methods often overlook the benefits of integrating rich contextual information. To address this issue, this paper proposes a prototypical augmentation and alignment method, termed ProtoConNet, which incorporates background information from different samples to enhance the diversity of the feature space, breaking the spurious associations between context and image subjects in few-shot scenarios. Specifically, it consists of three main modules: the clustering-based data selection (CDS) module mines diverse data patterns while preserving core features; the contextual-enhanced semantic refinement (CSR) module builds a context dictionary to integrate into image representations, which boosts the model's robustness in various scenarios; and the prototypical alignment (PA) module reduces the gap between image representations and class prototypes, amplifying feature distances for known and unknown classes. Experimental results from two datasets verified that ProtoConNet enhances the effectiveness of representation learning in few-shot scenarios and identifies open-set samples, making it superior to existing methods.         ",
    "url": "https://arxiv.org/abs/2507.11845",
    "authors": [
      "Kexuan Shi",
      "Zhuang Qi",
      "Jingjing Zhu",
      "Lei Meng",
      "Yaochen Zhang",
      "Haibei Huang",
      "Xiangxu Meng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.11851",
    "title": "Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential",
    "abstract": "           Autoregressive language models are constrained by their inherently sequential nature, generating one token at a time. This paradigm limits inference speed and parallelism, especially during later stages of generation when the direction and semantics of text are relatively certain. In this work, we propose a novel framework that leverages the inherent knowledge of vanilla autoregressive language models about future tokens, combining techniques to realize this potential and enable simultaneous prediction of multiple subsequent tokens. Our approach introduces several key innovations: (1) a masked-input formulation where multiple future tokens are jointly predicted from a common prefix; (2) a gated LoRA formulation that preserves the original LLM's functionality, while equipping it for multi-token prediction; (3) a lightweight, learnable sampler module that generates coherent sequences from the predicted future tokens; (4) a set of auxiliary training losses, including a consistency loss, to enhance the coherence and accuracy of jointly generated tokens; and (5) a speculative generation strategy that expands tokens quadratically in the future while maintaining high fidelity. Our method achieves significant speedups through supervised fine-tuning on pretrained models. For example, it generates code and math nearly 5x faster, and improves general chat and knowledge tasks by almost 2.5x. These gains come without any loss in quality.         ",
    "url": "https://arxiv.org/abs/2507.11851",
    "authors": [
      "Mohammad Samragh",
      "Arnav Kundu",
      "David Harrison",
      "Kumari Nishu",
      "Devang Naik",
      "Minsik Cho",
      "Mehrdad Farajtabar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.11870",
    "title": "MNO : A Multi-modal Neural Operator for Parametric Nonlinear BVPs",
    "abstract": "           We introduce a novel Multimodal Neural Operator (MNO) architecture designed to learn solution operators for multi-parameter nonlinear boundary value problems (BVPs). Traditional neural operators primarily map either the PDE coefficients or source terms independently to the solution, limiting their flexibility and applicability. In contrast, our proposed MNO architecture generalizes these approaches by mapping multiple parameters including PDE coefficients, source terms, and boundary conditions to the solution space in a unified manner. Our MNO is motivated by the hierarchical nested bases of the Fast Multipole Method (FMM) and is constructed systematically through three key components: a parameter efficient Generalized FMM (GFMM) block, a Unimodal Neural Operator (UNO) built upon GFMM blocks for single parameter mappings, and most importantly, a multimodal fusion mechanism extending these components to learn the joint map. We demonstrate the multimodal generalization capacity of our approach on both linear and nonlinear BVPs. Our experiments show that the network effectively handles simultaneous variations in PDE coefficients and source or boundary terms.         ",
    "url": "https://arxiv.org/abs/2507.11870",
    "authors": [
      "Vamshi C. Madala",
      "Nithin Govindarajan",
      "Shivkumar Chandrasekaran"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2507.11898",
    "title": "Extremal Testing for Network Software using LLMs",
    "abstract": "           Physicists often manually consider extreme cases when testing a theory. In this paper, we show how to automate extremal testing of network software using LLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS name length limits); then ask the LLM to generate tests that violate the constraints. We demonstrate how easy this process is by generating extremal tests for HTTP, BGP and DNS implementations, each of which uncovered new bugs. We show how this methodology extends to centralized network software such as shortest path algorithms, and how LLMs can generate filtering code to reject extremal input. We propose using agentic AI to further automate extremal testing. LLM-generated extremal testing goes beyond an old technique in software testing called Boundary Value Analysis.         ",
    "url": "https://arxiv.org/abs/2507.11898",
    "authors": [
      "Rathin Singha",
      "Harry Qian",
      "Srinath Saikrishnan",
      "Tracy Zhao",
      "Ryan Beckett",
      "Siva Kesava Reddy Kakarla",
      "George Varghese"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2507.11908",
    "title": "Unveiling Usability Challenges in Web Privacy Controls",
    "abstract": "           With the increasing concerns around privacy and the enforcement of data privacy laws, many websites now provide users with privacy controls. However, locating these controls can be challenging, as they are frequently hidden within multiple settings and layers. Moreover, the lack of standardization means these controls can vary widely across services. The technical or confusing terminology used to describe these controls further complicates users' ability to understand and use them effectively. This paper presents a large-scale empirical analysis investigating usability challenges of web privacy controls across 18,628 websites. While aiming for a multi-scenario view, our automated data collection faced significant hurdles, particularly in simulating sign-up and authenticated user visits, leading to more focused insights on guest visit scenarios and challenges in automated capture of dynamic user interactions. Our heuristic evaluation of three different user visit scenarios identifies significant website usability issues. Our results show that privacy policies are most common across all visit scenarios, with nudges and notices being prevalent in sign-up situations. We recommend designing privacy controls that: enhance awareness through pop-up nudges and notices; offer a table of contents as navigational aids and customized settings links in policies for more informed choice; and ensure accessibility via direct links to privacy settings from nudges.         ",
    "url": "https://arxiv.org/abs/2507.11908",
    "authors": [
      "Rahat Masood",
      "Sunday Oyinlola Ogundoyin",
      "Muhammad Ikram",
      "Alex Ye"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2507.11940",
    "title": "IANN-MPPI: Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral Approach for Autonomous Driving",
    "abstract": "           Motion planning for autonomous vehicles (AVs) in dense traffic is challenging, often leading to overly conservative behavior and unmet planning objectives. This challenge stems from the AVs' limited ability to anticipate and respond to the interactive behavior of surrounding agents. Traditional decoupled prediction and planning pipelines rely on non-interactive predictions that overlook the fact that agents often adapt their behavior in response to the AV's actions. To address this, we propose Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral (IANN-MPPI) control, which enables interactive trajectory planning by predicting how surrounding agents may react to each control sequence sampled by MPPI. To improve performance in structured lane environments, we introduce a spline-based prior for the MPPI sampling distribution, enabling efficient lane-changing behavior. We evaluate IANN-MPPI in a dense traffic merging scenario, demonstrating its ability to perform efficient merging maneuvers. Our project website is available at this https URL ",
    "url": "https://arxiv.org/abs/2507.11940",
    "authors": [
      "Kanghyun Ryu",
      "Minjun Sung",
      "Piyush Gupta",
      "Jovin D'sa",
      "Faizan M. Tariq",
      "David Isele",
      "Sangjae Bae"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.11954",
    "title": "The benefits of query-based KGQA systems for complex and temporal questions in LLM era",
    "abstract": "           Large language models excel in question-answering (QA) yet still struggle with multi-hop reasoning and temporal questions. Query-based knowledge graph QA (KGQA) offers a modular alternative by generating executable queries instead of direct answers. We explore multi-stage query-based framework for WikiData QA, proposing multi-stage approach that enhances performance on challenging multi-hop and temporal benchmarks. Through generalization and rejection studies, we evaluate robustness across multi-hop and temporal QA datasets. Additionally, we introduce a novel entity linking and predicate matching method using CoT reasoning. Our results demonstrate the potential of query-based multi-stage KGQA framework for improving multi-hop and temporal QA with small language models. Code and data: this https URL ",
    "url": "https://arxiv.org/abs/2507.11954",
    "authors": [
      "Artem Alekseev",
      "Mikhail Chaichuk",
      "Miron Butko",
      "Alexander Panchenko",
      "Elena Tutubalina",
      "Oleg Somov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.11968",
    "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation",
    "abstract": "           Multimodal Large Language Models (MLLMs) are increasingly used for content moderation, yet their robustness in short-form video contexts remains underexplored. Current safety evaluations often rely on unimodal attacks, failing to address combined attack vulnerabilities. In this paper, we introduce a comprehensive framework for evaluating the tri-modal safety of MLLMs. First, we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising diverse short-form videos with human-guided synthetic adversarial attacks. Second, we propose ChimeraBreak, a novel tri-modal attack strategy that simultaneously challenges visual, auditory, and semantic reasoning pathways. Extensive experiments on state-of-the-art MLLMs reveal significant vulnerabilities with high Attack Success Rates (ASR). Our findings uncover distinct failure modes, showing model biases toward misclassifying benign or policy-violating content. We assess results using LLM-as-a-judge, demonstrating attack reasoning efficacy. Our dataset and findings provide crucial insights for developing more robust and safe MLLMs.         ",
    "url": "https://arxiv.org/abs/2507.11968",
    "authors": [
      "Sahid Hossain Mustakim",
      "S M Jishanul Islam",
      "Ummay Maria Muna",
      "Montasir Chowdhury",
      "Mohammed Jawwadul Islam",
      "Sadia Ahmmed",
      "Tashfia Sikder",
      "Syed Tasdid Azam Dhrubo",
      "Swakkhar Shatabda"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.11971",
    "title": "HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing",
    "abstract": "           Current 3D representations like meshes, voxels, point clouds, and NeRF-based neural implicit fields exhibit significant limitations: they are often task-specific, lacking universal applicability across reconstruction, generation, editing, and driving. While meshes offer high precision, their dense vertex data complicates editing; NeRFs deliver excellent rendering but suffer from structural ambiguity, hindering animation and manipulation; all representations inherently struggle with the trade-off between data complexity and fidelity. To overcome these issues, we introduce a novel 3D Hierarchical Proxy Node representation. Its core innovation lies in representing an object's shape and texture via a sparse set of hierarchically organized (tree-structured) proxy nodes distributed on its surface and interior. Each node stores local shape and texture information (implicitly encoded by a small MLP) within its neighborhood. Querying any 3D coordinate's properties involves efficient neural interpolation and lightweight decoding from relevant nearby and parent nodes. This framework yields a highly compact representation where nodes align with local semantics, enabling direct drag-and-edit manipulation, and offers scalable quality-complexity control. Extensive experiments across 3D reconstruction and editing demonstrate our method's expressive efficiency, high-fidelity rendering quality, and superior editability.         ",
    "url": "https://arxiv.org/abs/2507.11971",
    "authors": [
      "Tielong Wang",
      "Yuxuan Xiong",
      "Jinfan Liu",
      "Zhifan Zhang",
      "Ye Chen",
      "Yue Shi",
      "Bingbing Ni"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.11972",
    "title": "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker",
    "abstract": "           Reading comprehension is a fundamental skill in human cognitive development. With the advancement of Large Language Models (LLMs), there is a growing need to compare how humans and LLMs understand language across different contexts and apply this understanding to functional tasks such as inference, emotion interpretation, and information retrieval. Our previous work used LLMs and human biomarkers to study the reading comprehension process. The results showed that the biomarkers corresponding to words with high and low relevance to the inference target, as labeled by the LLMs, exhibited distinct patterns, particularly when validated using eye-tracking data. However, focusing solely on individual words limited the depth of understanding, which made the conclusions somewhat simplistic despite their potential significance. This study used an LLM-based AI agent to group words from a reading passage into nodes and edges, forming a graph-based text representation based on semantic meaning and question-oriented prompts. We then compare the distribution of eye fixations on important nodes and edges. Our findings indicate that LLMs exhibit high consistency in language understanding at the level of graph topological structure. These results build on our previous findings and offer insights into effective human-AI co-learning strategies.         ",
    "url": "https://arxiv.org/abs/2507.11972",
    "authors": [
      "Yuhong Zhang",
      "Jialu Li",
      "Shilai Yang",
      "Yuchen Xu",
      "Gert Cauwenberghs",
      "Tzyy-Ping Jung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2507.11975",
    "title": "Online Training and Pruning of Deep Reinforcement Learning Networks",
    "abstract": "           Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms has been shown to enhance performance when feature extraction networks are used but the gained performance comes at the significant expense of increased computational and memory complexity. Neural network pruning methods have successfully addressed this challenge in supervised learning. However, their application to RL is underexplored. We propose an approach to integrate simultaneous training and pruning within advanced RL methods, in particular to RL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our networks (XiNet) are trained to solve stochastic optimization problems over the RL networks' weights and the parameters of variational Bernoulli distributions for 0/1 Random Variables $\\xi$ scaling each unit in the networks. The stochastic problem formulation induces regularization terms that promote convergence of the variational parameters to 0 when a unit contributes little to the performance. In this case, the corresponding structure is rendered permanently inactive and pruned from its network. We propose a cost-aware, sparsity-promoting regularization scheme, tailored to the DenseNet architecture of OFENets expressing the parameter complexity of involved networks in terms of the parameters of the RVs in these networks. Then, when matching this cost with the regularization terms, the many hyperparameters associated with them are automatically selected, effectively combining the RL objectives and network compression. We evaluate our method on continuous control benchmarks (MuJoCo) and the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned considerably with minimal loss in performance. Furthermore, our results confirm that pruning large networks during training produces more efficient and higher performing RL agents rather than training smaller networks from scratch.         ",
    "url": "https://arxiv.org/abs/2507.11975",
    "authors": [
      "Valentin Frank Ingmar Guenter",
      "Athanasios Sideris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.11987",
    "title": "Formal Verification of Neural Certificates Done Dynamically",
    "abstract": "           Neural certificates have emerged as a powerful tool in cyber-physical systems control, providing witnesses of correctness. These certificates, such as barrier functions, often learned alongside control policies, once verified, serve as mathematical proofs of system safety. However, traditional formal verification of their defining conditions typically faces scalability challenges due to exhaustive state-space exploration. To address this challenge, we propose a lightweight runtime monitoring framework that integrates real-time verification and does not require access to the underlying control policy. Our monitor observes the system during deployment and performs on-the-fly verification of the certificate over a lookahead region to ensure safety within a finite prediction horizon. We instantiate this framework for ReLU-based control barrier functions and demonstrate its practical effectiveness in a case study. Our approach enables timely detection of safety violations and incorrect certificates with minimal overhead, providing an effective but lightweight alternative to the static verification of the certificates.         ",
    "url": "https://arxiv.org/abs/2507.11987",
    "authors": [
      "Thomas A. Henzinger",
      "Konstantin Kueffner",
      "Emily Yu"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.11991",
    "title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers",
    "abstract": "           High-risk traffic zones such as intersections are a major cause of collisions. This study leverages deep generative models to enhance the safety of autonomous vehicles in an intersection context. We train a 1000-step denoising diffusion probabilistic model to generate collision-causing sensor noise sequences for an autonomous vehicle navigating a four-way intersection based on the current relative position and velocity of an intruder. Using the generative adversarial architecture, the 1000-step model is distilled into a single-step denoising diffusion model which demonstrates fast inference speed while maintaining similar sampling quality. We demonstrate one possible application of the single-step model in building a robust planner for the autonomous vehicle. The planner uses the single-step model to efficiently sample potential failure cases based on the currently measured traffic state to inform its decision-making. Through simulation experiments, the robust planner demonstrates significantly lower failure rate and delay rate compared with the baseline Intelligent Driver Model controller.         ",
    "url": "https://arxiv.org/abs/2507.11991",
    "authors": [
      "Juanran Wang",
      "Marc R. Schlichting",
      "Mykel J. Kochenderfer"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.11997",
    "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection",
    "abstract": "           Graph fraud detection has garnered significant attention as Graph Neural Networks (GNNs) have proven effective in modeling complex relationships within multimodal data. However, existing graph fraud detection methods typically use preprocessed node embeddings and predefined graph structures to reveal fraudsters, which ignore the rich semantic cues contained in raw textual information. Although Large Language Models (LLMs) exhibit powerful capabilities in processing textual information, it remains a significant challenge to perform multimodal fusion of processed textual embeddings with graph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM \\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In MLED, we utilize LLMs to extract external knowledge from textual information to enhance graph fraud detection methods. To integrate LLMs with graph structure information and enhance the ability to distinguish fraudsters, we design a multi-level LLM enhanced framework including type-level enhancer and relation-level enhancer. One is to enhance the difference between the fraudsters and the benign entities, the other is to enhance the importance of the fraudsters in different relations. The experiments on four real-world datasets show that MLED achieves state-of-the-art performance in graph fraud detection as a generalized framework that can be applied to existing methods.         ",
    "url": "https://arxiv.org/abs/2507.11997",
    "authors": [
      "Tairan Huang",
      "Yili Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.11999",
    "title": "Envisage: Towards Expressive Visual Graph Querying",
    "abstract": "           Graph querying is the process of retrieving information from graph data using specialized languages (e.g., Cypher), often requiring programming expertise. Visual Graph Querying (VGQ) streamlines this process by enabling users to construct and execute queries via an interactive interface without resorting to complex coding. However, current VGQ tools only allow users to construct simple and specific query graphs, limiting users' ability to interactively express their query intent, especially for underspecified query intent. To address these limitations, we propose Envisage, an interactive visual graph querying system to enhance the expressiveness of VGQ in complex query scenarios by supporting intuitive graph structure construction and flexible parameterized rule specification. Specifically, Envisage comprises four stages: Query Expression allows users to interactively construct graph queries through intuitive operations; Query Verification enables the validation of constructed queries via rule verification and query instantiation; Progressive Query Execution can progressively execute queries to ensure meaningful querying results; and Result Analysis facilitates result exploration and interpretation. To evaluate Envisage, we conducted two case studies and in-depth user interviews with 14 graph analysts. The results demonstrate its effectiveness and usability in constructing, verifying, and executing complex graph queries.         ",
    "url": "https://arxiv.org/abs/2507.11999",
    "authors": [
      "Xiaolin Wen",
      "Qishuang Fu",
      "Shuangyue Han",
      "Yichen Guo",
      "Joseph K. Liu",
      "Yong Wang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2507.12004",
    "title": "Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis",
    "abstract": "           This thesis addresses challenges related to data and parameter efficiency in neural language models, with a focus on representation analysis and the introduction of new optimization techniques. The first part examines the properties and dynamics of language representations within neural models, emphasizing their significance in enhancing robustness and generalization. It proposes innovative approaches based on representation smoothness, including regularization strategies that utilize Jacobian and Hessian matrices to stabilize training and mitigate sensitivity to input perturbations. The second part focuses on methods to significantly enhance data and parameter efficiency by integrating active learning strategies with parameter-efficient fine-tuning, guided by insights from representation smoothness analysis. It presents smoothness-informed early-stopping techniques designed to eliminate the need for labeled validation sets and proposes innovative combinations of active learning and parameter-efficient fine-tuning to reduce labeling efforts and computational resources. Extensive experimental evaluations across various NLP tasks demonstrate that these combined approaches substantially outperform traditional methods in terms of performance, stability, and efficiency. The third part explores weak supervision techniques enhanced by in-context learning to effectively utilize unlabeled data, further reducing dependence on extensive labeling. It shows that using in-context learning as a mechanism for weak supervision enables models to better generalize from limited labeled data by leveraging unlabeled examples more effectively during training. Comprehensive empirical evaluations confirm significant gains in model accuracy, adaptability, and robustness, especially in low-resource settings and dynamic data environments.         ",
    "url": "https://arxiv.org/abs/2507.12004",
    "authors": [
      "Josip Juki\u0107"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.12006",
    "title": "Frequency-Dynamic Attention Modulation for Dense Prediction",
    "abstract": "           Vision Transformers (ViTs) have significantly advanced computer vision, demonstrating strong performance across various tasks. However, the attention mechanism in ViTs makes each layer function as a low-pass filter, and the stacked-layer architecture in existing transformers suffers from frequency vanishing. This leads to the loss of critical details and textures. We propose a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly modulates the overall frequency response of ViTs and consists of two techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling (FreqScale). Since circuit theory uses low-pass filters as fundamental elements, we introduce AttInv, a method that generates complementary high-pass filtering by inverting the low-pass filter in the attention matrix, and dynamically combining the two. We further design FreqScale to weight different frequency components for fine-grained adjustments to the target response function. Through feature similarity analysis and effective rank evaluation, we demonstrate that our approach avoids representation collapse, leading to consistent performance improvements across various models, including SegFormer, DeiT, and MaskDINO. These improvements are evident in tasks such as semantic segmentation, object detection, and instance segmentation. Additionally, we apply our method to remote sensing detection, achieving state-of-the-art results in single-scale settings. The code is available at \\href{this https URL}{this https URL}.         ",
    "url": "https://arxiv.org/abs/2507.12006",
    "authors": [
      "Linwei Chen",
      "Lin Gu",
      "Ying Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.12009",
    "title": "Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli",
    "abstract": "           We propose an end-to-end deep neural encoder-decoder model to encode and decode brain activity in response to naturalistic stimuli using functional magnetic resonance imaging (fMRI) data. Leveraging temporally correlated input from consecutive film frames, we employ temporal convolutional layers in our architecture, which effectively allows to bridge the temporal resolution gap between natural movie stimuli and fMRI acquisitions. Our model predicts activity of voxels in and around the visual cortex and performs reconstruction of corresponding visual inputs from neural activity. Finally, we investigate brain regions contributing to visual decoding through saliency maps. We find that the most contributing regions are the middle occipital area, the fusiform area, and the calcarine, respectively employed in shape perception, complex recognition (in particular face perception), and basic visual features such as edges and contrasts. These functions being strongly solicited are in line with the decoder's capability to reconstruct edges, faces, and contrasts. All in all, this suggests the possibility to probe our understanding of visual processing in films using as a proxy the behaviour of deep learning models such as the one proposed in this paper.         ",
    "url": "https://arxiv.org/abs/2507.12009",
    "authors": [
      "Florian David",
      "Michael Chan",
      "Elenor Morgenroth",
      "Patrik Vuilleumier",
      "Dimitri Van De Ville"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2507.12015",
    "title": "EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis",
    "abstract": "           In recent years, emotional Text-to-Speech (TTS) synthesis and emphasis-controllable speech synthesis have advanced significantly. However, their interaction remains underexplored. We propose Emphasis Meets Emotion TTS (EME-TTS), a novel framework designed to address two key research questions: (1) how to effectively utilize emphasis to enhance the expressiveness of emotional speech, and (2) how to maintain the perceptual clarity and stability of target emphasis across different emotions. EME-TTS employs weakly supervised learning with emphasis pseudo-labels and variance-based emphasis features. Additionally, the proposed Emphasis Perception Enhancement (EPE) block enhances the interaction between emotional signals and emphasis positions. Experimental results show that EME-TTS, when combined with large language models for emphasis position prediction, enables more natural emotional speech synthesis while preserving stable and distinguishable target emphasis across emotions. Synthesized samples are available on-line.         ",
    "url": "https://arxiv.org/abs/2507.12015",
    "authors": [
      "Haoxun Li",
      "Leyuan Qu",
      "Jiaxi Hu",
      "Taihao Li"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2507.12017",
    "title": "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection",
    "abstract": "           Unsupervised domain adaptive object detection (UDAOD) from the visible domain to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB domain as a unified domain and neglect the multiple subdomains within it, such as daytime, nighttime, and foggy scenes. We argue that decoupling the domain-invariant (DI) and domain-specific (DS) features across these multiple subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper proposes a new SS-DC framework based on a decoupling-coupling strategy. In terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID) module in the aspect of spectral decomposition. Due to the style and content information being highly embedded in different frequency bands, this module can decouple DI and DS components more accurately and interpretably. A novel filter bank-based spectral processing paradigm and a self-distillation-driven decoupling loss are proposed to improve the spectral domain decoupling. In terms of coupling, a new spatial-spectral coupling method is proposed, which realizes joint coupling through spatial and spectral DI feature pyramids. Meanwhile, this paper introduces DS from decoupling to reduce the domain bias. Extensive experiments demonstrate that our method can significantly improve the baseline performance and outperform existing UDAOD methods on multiple RGB-IR datasets, including a new experimental protocol proposed in this paper based on the FLIR-ADAS dataset.         ",
    "url": "https://arxiv.org/abs/2507.12017",
    "authors": [
      "Xiwei Zhang",
      "Chunjin Yang",
      "Yiming Xiao",
      "Runtong Zhang",
      "Fanman Meng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.12027",
    "title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation",
    "abstract": "           We propose SGLoc, a novel localization system that directly regresses camera poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic information. Our method utilizes the semantic relationship between 2D image and 3D scene representation to estimate the 6DoF pose without prior pose information. In this system, we introduce a multi-level pose regression strategy that progressively estimates and refines the pose of query image from the global 3DGS map, without requiring initial pose priors. Moreover, we introduce a semantic-based global retrieval algorithm that establishes correspondences between 2D (image) and 3D (3DGS map). By matching the extracted scene semantic descriptors of 2D query image and 3DGS semantic representation, we align the image with the local region of the global 3DGS map, thereby obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by iteratively optimizing the difference between the query image and the rendered image from 3DGS. Our SGLoc demonstrates superior performance over baselines on 12scenes and 7scenes datasets, showing excellent capabilities in global localization without initial pose prior. Code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.12027",
    "authors": [
      "Beining Xu",
      "Siting Zhu",
      "Hesheng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.12031",
    "title": "Towards Ultra-Reliable 6G in-X Subnetworks: Dynamic Link Adaptation by Deep Reinforcement Learning",
    "abstract": "           6G networks are composed of subnetworks expected to meet ultra-reliable low-latency communication (URLLC) requirements for mission-critical applications such as industrial control and automation. An often-ignored aspect in URLLC is consecutive packet outages, which can destabilize control loops and compromise safety in in-factory environments. Hence, the current work proposes a link adaptation framework to support extreme reliability requirements using the soft actor-critic (SAC)-based deep reinforcement learning (DRL) algorithm that jointly optimizes energy efficiency (EE) and reliability under dynamic channel and interference conditions. Unlike prior work focusing on average reliability, our method explicitly targets reducing burst/consecutive outages through adaptive control of transmit power and blocklength based solely on the observed signal-to-interference-plus-noise ratio (SINR). The joint optimization problem is formulated under finite blocklength and quality of service constraints, balancing reliability and EE. Simulation results show that the proposed method significantly outperforms the baseline algorithms, reducing outage bursts while consuming only 18\\% of the transmission cost required by a full/maximum resource allocation policy in the evaluated scenario. The framework also supports flexible trade-off tuning between EE and reliability by adjusting reward weights, making it adaptable to diverse industrial requirements.         ",
    "url": "https://arxiv.org/abs/2507.12031",
    "authors": [
      "Fateme Salehi",
      "Aamir Mahmood",
      "Sarder Fakhrul Abedin",
      "Kyi Thar",
      "Mikael Gidlund"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.12042",
    "title": "Stereo Sound Event Localization and Detection with Onscreen/offscreen Classification",
    "abstract": "           This paper presents the objective, dataset, baseline, and metrics of Task 3 of the DCASE2025 Challenge on sound event localization and detection (SELD). In previous editions, the challenge used four-channel audio formats of first-order Ambisonics (FOA) and microphone array. In contrast, this year's challenge investigates SELD with stereo audio data (termed stereo SELD). This change shifts the focus from more specialized 360\u00b0 audio and audiovisual scene analysis to more commonplace audio and media scenarios with limited field-of-view (FOV). Due to inherent angular ambiguities in stereo audio data, the task focuses on direction-of-arrival (DOA) estimation in the azimuth plane (left-right axis) along with distance estimation. The challenge remains divided into two tracks: audio-only and audiovisual, with the audiovisual track introducing a new sub-task of onscreen/offscreen event classification necessitated by the limited FOV. This challenge introduces the DCASE2025 Task3 Stereo SELD Dataset, whose stereo audio and perspective video clips are sampled and converted from the STARSS23 recordings. The baseline system is designed to process stereo audio and corresponding video frames as inputs. In addition to the typical SELD event classification and localization, it integrates onscreen/offscreen classification for the audiovisual track. The evaluation metrics have been modified to introduce an onscreen/offscreen accuracy metric, which assesses the models' ability to identify which sound sources are onscreen. In the experimental evaluation, the baseline system performs reasonably well with the stereo audio data.         ",
    "url": "https://arxiv.org/abs/2507.12042",
    "authors": [
      "Kazuki Shimada",
      "Archontis Politis",
      "Iran R. Roman",
      "Parthasaarathy Sudarsanam",
      "David Diaz-Guerra",
      "Ruchi Pandey",
      "Kengo Uchida",
      "Yuichiro Koyama",
      "Naoya Takahashi",
      "Takashi Shibuya",
      "Shusuke Takahashi",
      "Tuomas Virtanen",
      "Yuki Mitsufuji"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2507.12047",
    "title": "Pathfinding in Self-Deleting Graphs",
    "abstract": "           In this paper, we study the problem of pathfinding on traversal-dependent graphs, i.e., graphs whose edges change depending on the previously visited vertices. In particular, we study \\emph{self-deleting graphs}, introduced by Carmesin et al. (Sarah Carmesin, David Woller, David Parker, Miroslav Kulich, and Masoumeh Mansouri. The Hamiltonian cycle and travelling salesperson problems with traversal-dependent edge deletion. J. Comput. Sci.), which consist of a graph $G=(V, E)$ and a function $f\\colon V\\rightarrow 2^E$, where $f(v)$ is the set of edges that will be deleted after visiting the vertex $v$. In the \\textsc{(Shortest) Self-Deleting $s$-$t$-path} problem we are given a self-deleting graph and its vertices $s$ and $t$, and we are asked to find a (shortest) path from $s$ to $t$, such that it does not traverse an edge in $f(v)$ after visiting $v$ for any vertex $v$. We prove that \\textsc{Self-Deleting $s$-$t$-path} is NP-hard even if the given graph is outerplanar, bipartite, has maximum degree $3$, bandwidth $2$ and $|f(v)|\\leq 1$ for each vertex $v$. We show that \\textsc{Shortest Self-Deleting $s$-$t$-path} is W[1]-complete parameterized by the length of the sought path and that \\textsc{Self-Deleting $s$-$t$-path} is \\W{1}-complete parameterized by the vertex cover number, feedback vertex set number and treedepth. We also show that the problem becomes FPT when we parameterize by the maximum size of $f(v)$ and several structural parameters. Lastly, we show that the problem does not admit a polynomial kernel even for parameterization by the vertex cover number and the maximum size of $f(v)$ combined already on 2-outerplanar graphs.         ",
    "url": "https://arxiv.org/abs/2507.12047",
    "authors": [
      "Michal Dvo\u0159\u00e1k",
      "Du\u0161an Knop",
      "Michal Opler",
      "Jan Pokorn\u00fd",
      "Ond\u0159ej Such\u00fd",
      "Krisztina Szil\u00e1gyi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2507.12049",
    "title": "MoViAD: Modular Visual Anomaly Detection",
    "abstract": "           VAD is a critical field in machine learning focused on identifying deviations from normal patterns in images, often challenged by the scarcity of anomalous data and the need for unsupervised training. To accelerate research and deployment in this domain, we introduce MoViAD, a comprehensive and highly modular library designed to provide fast and easy access to state-of-the-art VAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array of scenarios, including continual, semi-supervised, few-shots, noisy, and many more. In addition, it addresses practical deployment challenges through dedicated Edge and IoT settings, offering optimized models and backbones, along with quantization and compression utilities for efficient on-device execution and distributed inference. MoViAD integrates a selection of backbones, robust evaluation VAD metrics (pixel-level and image-level) and useful profiling tools for efficiency analysis. The library is designed for fast, effortless deployment, enabling machine learning engineers to easily use it for their specific setup with custom models, datasets, and backbones. At the same time, it offers the flexibility and extensibility researchers need to develop and experiment with new methods.         ",
    "url": "https://arxiv.org/abs/2507.12049",
    "authors": [
      "Manuel Barusco",
      "Francesco Borsatti",
      "Arianna Stropeni",
      "Davide Dalle Pezze",
      "Gian Antonio Susto"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12062",
    "title": "MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning",
    "abstract": "           Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint specific moments and assess clip-wise relevance based on the text query. While DETR-based joint frameworks have made significant strides, there remains untapped potential in harnessing the intricate relationships between temporal motion and spatial semantics within video content. In this paper, we propose the Motion-Semantics DETR (MS-DETR), a framework that captures rich motion-semantics features through unified learning for MR/HD tasks. The encoder first explicitly models disentangled intra-modal correlations within motion and semantics dimensions, guided by the given text queries. Subsequently, the decoder utilizes the task-wise correlation across temporal motion and spatial semantics dimensions to enable precise query-guided localization for MR and refined highlight boundary delineation for HD. Furthermore, we observe the inherent sparsity dilemma within the motion and semantics dimensions of MR/HD datasets. To address this issue, we enrich the corpus from both dimensions by generation strategies and propose contrastive denoising learning to ensure the above components learn robustly and effectively. Extensive experiments on four MR/HD benchmarks demonstrate that our method outperforms existing state-of-the-art models by a margin. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.12062",
    "authors": [
      "Hongxu Ma",
      "Guanshuo Wang",
      "Fufu Yu",
      "Qiong Jia",
      "Shouhong Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12063",
    "title": "Contrastive Cascade Graph Learning for Classifying Real and Synthetic Information Diffusion Patterns",
    "abstract": "           A wide variety of information is disseminated through social media, and content that spreads at scale can have tangible effects on the real world. To curb the spread of harmful content and promote the dissemination of reliable information, research on cascade graph mining has attracted increasing attention. A promising approach in this area is Contrastive Cascade Graph Learning (CCGL). One important task in cascade graph mining is cascade classification, which involves categorizing cascade graphs based on their structural characteristics. Although CCGL is expected to be effective for this task, its performance has not yet been thoroughly evaluated. This study aims to investigate the effectiveness of CCGL for cascade classification. Our findings demonstrate the strong performance of CCGL in capturing platform- and model-specific structural patterns in cascade graphs, highlighting its potential for a range of downstream information diffusion analysis tasks.         ",
    "url": "https://arxiv.org/abs/2507.12063",
    "authors": [
      "Naoki Shibao",
      "Sho Tsugawa"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2507.12067",
    "title": "Robust Route Planning for Sidewalk Delivery Robots",
    "abstract": "           Sidewalk delivery robots are a promising solution for urban freight distribution, reducing congestion compared to trucks and providing a safer, higher-capacity alternative to drones. However, unreliable travel times on sidewalks due to pedestrian density, obstacles, and varying infrastructure conditions can significantly affect their efficiency. This study addresses the robust route planning problem for sidewalk robots, explicitly accounting for travel time uncertainty due to varying sidewalk conditions. Optimization is integrated with simulation to reproduce the effect of obstacles and pedestrian flows and generate realistic travel times. The study investigates three different approaches to derive uncertainty sets, including budgeted, ellipsoidal, and support vector clustering (SVC)-based methods, along with a distributionally robust method to solve the shortest path (SP) problem. A realistic case study reproducing pedestrian patterns in Stockholm's city center is used to evaluate the efficiency of robust routing across various robot designs and environmental conditions. The results show that, when compared to a conventional SP, robust routing significantly enhances operational reliability under variable sidewalk conditions. The Ellipsoidal and DRSP approaches outperform the other methods, yielding the most efficient paths in terms of average and worst-case delay. Sensitivity analyses reveal that robust approaches consistently outperform the conventional SP, particularly for sidewalk delivery robots that are wider, slower, and have more conservative navigation behaviors. These benefits are even more pronounced in adverse weather conditions and high pedestrian congestion scenarios.         ",
    "url": "https://arxiv.org/abs/2507.12067",
    "authors": [
      "Xing Tong",
      "Michele D. Simoni"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.12083",
    "title": "Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics",
    "abstract": "           Motion forecasting for on-road traffic agents presents both a significant challenge and a critical necessity for ensuring safety in autonomous driving systems. In contrast to most existing data-driven approaches that directly predict future trajectories, we rethink this task from a planning perspective, advocating a \"First Reasoning, Then Forecasting\" strategy that explicitly incorporates behavior intentions as spatial guidance for trajectory prediction. To achieve this, we introduce an interpretable, reward-driven intention reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL) scheme. Our method first encodes traffic agents and scene elements into a unified vectorized representation, then aggregates contextual features through a query-centric paradigm. This enables the derivation of a reward distribution, a compact yet informative representation of the target agent's behavior within the given scene context via IRL. Guided by this reward heuristic, we perform policy rollouts to reason about multiple plausible intentions, providing valuable priors for subsequent trajectory generation. Finally, we develop a hierarchical DETR-like decoder integrated with bidirectional selective state space models to produce accurate future trajectories along with their associated probabilities. Extensive experiments on the large-scale Argoverse and nuScenes motion forecasting datasets demonstrate that our approach significantly enhances trajectory prediction confidence, achieving highly competitive performance relative to state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2507.12083",
    "authors": [
      "Muleilan Pei",
      "Shaoshuai Shi",
      "Xuesong Chen",
      "Xu Liu",
      "Shaojie Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.12087",
    "title": "YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association",
    "abstract": "           Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned Aerial Vehicle (UAV) perspective is a highly challenging computer vision task. The difficulty stems from three main sources: the extreme scarcity of target appearance features, the complex motion entanglement caused by the combined dynamics of the camera and the targets themselves, and the frequent occlusions and identity ambiguity arising from dense flocking behavior. This paper details our championship-winning solution in the MVA 2025 \"Finding Birds\" Small Multi-Object Tracking Challenge (SMOT4SB), which adopts the tracking-by-detection paradigm with targeted innovations at both the detection and association levels. On the detection side, we propose a systematic training enhancement framework named \\textbf{SliceTrain}. This framework, through the synergy of 'deterministic full-coverage slicing' and 'slice-level stochastic augmentation, effectively addresses the problem of insufficient learning for small objects in high-resolution image training. On the tracking side, we designed a robust tracker that is completely independent of appearance information. By integrating a \\textbf{motion direction maintenance (EMA)} mechanism and an \\textbf{adaptive similarity metric} combining \\textbf{bounding box expansion and distance penalty} into the OC-SORT framework, our tracker can stably handle irregular motion and maintain target identities. Our method achieves state-of-the-art performance on the SMOT4SB public test set, reaching an SO-HOTA score of \\textbf{55.205}, which fully validates the effectiveness and advancement of our framework in solving complex real-world SMOT problems. The source code will be made available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.12087",
    "authors": [
      "Xiang Yu",
      "Xinyao Liu",
      "Guang Liang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12095",
    "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images",
    "abstract": "           Accurate 3D reconstruction of vehicles is vital for applications such as vehicle inspection, predictive maintenance, and urban planning. Existing methods like Neural Radiance Fields and Gaussian Splatting have shown impressive results but remain limited by their reliance on dense input views, which hinders real-world applicability. This paper addresses the challenge of reconstructing vehicles from sparse-view inputs, leveraging depth maps and a robust pose estimation architecture to synthesize novel views and augment training data. Specifically, we enhance Gaussian Splatting by integrating a selective photometric loss, applied only to high-confidence pixels, and replacing standard Structure-from-Motion pipelines with the DUSt3R architecture to improve camera pose estimation. Furthermore, we present a novel dataset featuring both synthetic and real-world public transportation vehicles, enabling extensive evaluation of our approach. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, showcasing the method's ability to achieve high-quality reconstructions even under constrained input conditions.         ",
    "url": "https://arxiv.org/abs/2507.12095",
    "authors": [
      "Davide Di Nucci",
      "Matteo Tomei",
      "Guido Borghi",
      "Luca Ciuffreda",
      "Roberto Vezzani",
      "Rita Cucchiara"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12098",
    "title": "A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy",
    "abstract": "           To mitigate privacy leakage and performance issues in personalized advertising, this paper proposes a framework that integrates federated learning and differential privacy. The system combines distributed feature extraction, dynamic privacy budget allocation, and robust model aggregation to balance model accuracy, communication overhead, and privacy protection. Multi-party secure computing and anomaly detection mechanisms further enhance system resilience against malicious attacks. Experimental results demonstrate that the framework achieves dual optimization of recommendation accuracy and system efficiency while ensuring privacy, providing both a practical solution and a theoretical foundation for applying privacy protection technologies in advertisement recommendation.         ",
    "url": "https://arxiv.org/abs/2507.12098",
    "authors": [
      "Xiang Li",
      "Yifan Lin",
      "Yuanzhe Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12107",
    "title": "Non-Adaptive Adversarial Face Generation",
    "abstract": "           Adversarial attacks on face recognition systems (FRSs) pose serious security and privacy threats, especially when these systems are used for identity verification. In this paper, we propose a novel method for generating adversarial faces-synthetic facial images that are visually distinct yet recognized as a target identity by the FRS. Unlike iterative optimization-based approaches (e.g., gradient descent or other iterative solvers), our method leverages the structural characteristics of the FRS feature space. We figure out that individuals sharing the same attribute (e.g., gender or race) form an attributed subsphere. By utilizing such subspheres, our method achieves both non-adaptiveness and a remarkably small number of queries. This eliminates the need for relying on transferability and open-source surrogate models, which have been a typical strategy when repeated adaptive queries to commercial FRSs are impossible. Despite requiring only a single non-adaptive query consisting of 100 face images, our method achieves a high success rate of over 93% against AWS's CompareFaces API at its default threshold. Furthermore, unlike many existing attacks that perturb a given image, our method can deliberately produce adversarial faces that impersonate the target identity while exhibiting high-level attributes chosen by the adversary.         ",
    "url": "https://arxiv.org/abs/2507.12107",
    "authors": [
      "Sunpill Kim",
      "Seunghun Paik",
      "Chanwoo Hwang",
      "Minsu Kim",
      "Jae Hong Seo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2507.12123",
    "title": "Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph",
    "abstract": "           We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects using 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor environment over a Hierarchical Scene Graph derived from sequences of RGB-D frames utilizing a set of open-vocabulary foundation models and sensor data processing. The hierarchical representation explicitly models spatial relations across floors, rooms, locations, and objects. To effectively address complex queries involving spatial reference to other objects, we integrate the hierarchical scene graph with a Large Language Model for multistep reasoning. This integration leverages inter-layer (e.g., room-to-object) and intra-layer (e.g., object-to-object) connections, enhancing spatial contextual understanding. We investigate the semantic and geometry accuracy of hierarchical representation on Habitat Matterport 3D Semantic multi-floor scenes. Our approach demonstrates efficient scene comprehension and robust object grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates strong potential for applications requiring spatial reasoning and understanding of indoor environments. Related materials can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.12123",
    "authors": [
      "Sergey Linok",
      "Gleb Naumov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12126",
    "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis",
    "abstract": "           Text data augmentation is a widely used strategy for mitigating data sparsity in natural language processing (NLP), particularly in low-resource settings where limited samples hinder effective semantic modeling. While augmentation can improve input diversity and downstream interpretability, existing techniques often lack mechanisms to ensure semantic preservation during large-scale or iterative generation, leading to redundancy and instability. This work introduces a principled evaluation framework for large language model (LLM) based text augmentation, comprising two components: (1) Scalability Analysis, which measures semantic consistency as augmentation volume increases, and (2) Iterative Augmentation with Summarization Refinement (IASR), which evaluates semantic drift across recursive paraphrasing cycles. Empirical evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the best balance of semantic fidelity, diversity, and generation efficiency. Applied to a real-world topic modeling task using BERTopic with GPT-enhanced few-shot labeling, the proposed approach results in a 400% increase in topic granularity and complete elimination of topic overlaps. These findings validated the utility of the proposed frameworks for structured evaluation of LLM-based augmentation in practical NLP pipelines.         ",
    "url": "https://arxiv.org/abs/2507.12126",
    "authors": [
      "Payal Bhattad",
      "Sai Manoj Pudukotai Dinakarrao",
      "Anju Gupta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12127",
    "title": "Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks",
    "abstract": "           Advancements in wireless and mobile technologies, including 5G advanced and the envisioned 6G, are driving exponential growth in wireless devices. However, this rapid expansion exacerbates spectrum scarcity, posing a critical challenge. Dynamic spectrum allocation (DSA)--which relies on sensing and dynamically sharing spectrum--has emerged as an essential solution to address this issue. While machine learning (ML) models hold significant potential for improving spectrum sensing, their adoption in centralized ML-based DSA systems is limited by privacy concerns, bandwidth constraints, and regulatory challenges. To overcome these limitations, distributed ML-based approaches such as Federated Learning (FL) offer promising alternatives. This work addresses two key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of labeled data for training FL models in practical spectrum sensing scenarios is tackled with a semi-supervised FL approach, combined with energy detection, enabling model training on unlabeled datasets. Second, we examine the security vulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our analysis highlights the shortcomings of existing majority-based defenses in countering such attacks. To address these vulnerabilities, we propose a novel defense mechanism inspired by vaccination, which effectively mitigates data poisoning attacks without relying on majority-based assumptions. Extensive experiments on both synthetic and real-world datasets validate our solutions, demonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets and maintain Byzantine robustness against both targeted and untargeted data poisoning attacks, even when a significant proportion of participants are malicious.         ",
    "url": "https://arxiv.org/abs/2507.12127",
    "authors": [
      "Ngoc Duy Pham",
      "Thusitha Dayaratne",
      "Viet Vo",
      "Shangqi Lai",
      "Sharif Abuadbba",
      "Hajime Suzuki",
      "Xingliang Yuan",
      "Carsten Rudolph"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12133",
    "title": "HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD",
    "abstract": "           Device recognition is vital for security in wireless communication systems, particularly for applications like access control. Radio Frequency Fingerprint Identification (RFFI) offers a non-cryptographic solution by exploiting hardware-induced signal distortions. This paper proposes HyDRA, a Hybrid Dual-mode RF Architecture that integrates an optimized Variational Mode Decomposition (VMD) with a novel architecture based on the fusion of Convolutional Neural Networks (CNNs), Transformers, and Mamba components, designed to support both closed-set and open-set classification tasks. The optimized VMD enhances preprocessing efficiency and classification accuracy by fixing center frequencies and using closed-form solutions. HyDRA employs the Transformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and the Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting to varying conditions. Evaluation on public datasets demonstrates state-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance in our proposed open-set classification method, effectively identifying unauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves millisecond-level inference speed with low power consumption, providing a practical solution for real-time wireless authentication in real-world environments.         ",
    "url": "https://arxiv.org/abs/2507.12133",
    "authors": [
      "Hanwen Liu",
      "Yuhe Huang",
      "Yifeng Gong",
      "Yanjie Zhai",
      "Jiaxuan Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2507.12137",
    "title": "AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving",
    "abstract": "           Modeling and rendering dynamic urban driving scenes is crucial for self-driving simulation. Current high-quality methods typically rely on costly manual object tracklet annotations, while self-supervised approaches fail to capture dynamic object motions accurately and decompose scenes properly, resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised framework for high-quality free-viewpoint rendering of driving scenes from a single log. At its core is a novel learnable motion model that integrates locality-aware B-spline curves with global-aware trigonometric functions, enabling flexible yet precise dynamic object modeling. Rather than requiring comprehensive semantic labeling, AD-GS automatically segments scenes into objects and background with the simplified pseudo 2D segmentation, representing objects using dynamic Gaussians and bidirectional temporal visibility masks. Further, our model incorporates visibility reasoning and physically rigid regularization to enhance robustness. Extensive evaluations demonstrate that our annotation-free model significantly outperforms current state-of-the-art annotation-free methods and is competitive with annotation-dependent approaches.         ",
    "url": "https://arxiv.org/abs/2507.12137",
    "authors": [
      "Jiawei Xu",
      "Kai Deng",
      "Zexin Fan",
      "Shenlong Wang",
      "Jin Xie",
      "Jian Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12138",
    "title": "Neural Human Pose Prior",
    "abstract": "           We introduce a principled, data-driven approach for modeling a neural prior over human body poses using normalizing flows. Unlike heuristic or low-expressivity alternatives, our method leverages RealNVP to learn a flexible density over poses represented in the 6D rotation format. We address the challenge of modeling distributions on the manifold of valid 6D rotations by inverting the Gram-Schmidt process during training, enabling stable learning while preserving downstream compatibility with rotation-based frameworks. Our architecture and training pipeline are framework-agnostic and easily reproducible. We demonstrate the effectiveness of the learned prior through both qualitative and quantitative evaluations, and we analyze its impact via ablation studies. This work provides a sound probabilistic foundation for integrating pose priors into human motion capture and reconstruction pipelines.         ",
    "url": "https://arxiv.org/abs/2507.12138",
    "authors": [
      "Michal Heker",
      "Sefy Kararlitsky",
      "David Tolpin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12140",
    "title": "A Hybrid High-Order method for the power-law Brinkman problem robust in all regimes",
    "abstract": "           In this work we propose and analyze a new Hybrid High-Order method for the Brinkman problem for fluids with power-law viscosity. The proposed method supports general meshes and arbitrary approximation orders and is robust in all regimes, from pure (power-law) Stokes to pure Darcy. Robustness is reflected by error estimates that distinguish the contributions from Stokes- and Darcy-dominated elements as identified by an appropriate dimensionless number, and that additionally account for pre-asymptotic orders of convergence. Theoretical results are illustrated by a complete panel of numerical experiments.         ",
    "url": "https://arxiv.org/abs/2507.12140",
    "authors": [
      "Daniel Casta\u00f1\u00f3n Quiroz",
      "Daniele A. Di Pietro",
      "J\u00e9r\u00f4me Droniou",
      "Marwa Salah"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2507.12155",
    "title": "Lowering Error Floors for Hard Decision Decoding of OFEC Code",
    "abstract": "           Stall patterns are known to cause an error floor in hard decision decoding of the OFEC code. We propose a novel stall pattern removal algorithm that lowers the error floor of state-of-the-art algorithms by an order of magnitude         ",
    "url": "https://arxiv.org/abs/2507.12155",
    "authors": [
      "Jasper Lagendijk",
      "Yunus Can G\u00fcltekin",
      "Alexios Balatsoukas-Stimming",
      "Gabriele Liga",
      "Alex Alvarado"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2507.12157",
    "title": "Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation",
    "abstract": "           Fine-grained image recognition (FGIR) aims to distinguish visually similar sub-categories within a broader class, such as identifying bird species. While most existing FGIR methods rely on backbones pretrained on large-scale datasets like ImageNet, this dependence limits adaptability to resource-constrained environments and hinders the development of task-specific architectures tailored to the unique challenges of FGIR. In this work, we challenge the conventional reliance on pretrained models by demonstrating that high-performance FGIR systems can be trained entirely from scratch. We introduce a novel training framework, TGDA, that integrates data-aware augmentation with weak supervision via a fine-grained-aware teacher model, implemented through knowledge distillation. This framework unlocks the design of task-specific and hardware-aware architectures, including LRNets for low-resolution FGIR and ViTFS, a family of Vision Transformers optimized for efficient inference. Extensive experiments across three FGIR benchmarks over diverse settings involving low-resolution and high-resolution inputs show that our method consistently matches or surpasses state-of-the-art pretrained counterparts. In particular, in the low-resolution setting, LRNets trained with TGDA improve accuracy by up to 23\\% over prior methods while requiring up to 20.6x less parameters, lower FLOPs, and significantly less training data. Similarly, ViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k while using 15.3x fewer trainable parameters and requiring orders of magnitudes less data. These results highlight TGDA's potential as an adaptable alternative to pretraining, paving the way for more efficient fine-grained vision systems.         ",
    "url": "https://arxiv.org/abs/2507.12157",
    "authors": [
      "Edwin Arkel Rios",
      "Fernando Mikael",
      "Oswin Gosal",
      "Femiloye Oyerinde",
      "Hao-Chun Liang",
      "Bo-Cheng Lai",
      "Min-Chun Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12175",
    "title": "RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection",
    "abstract": "           This study introduces RUMAA, a transformer-based framework for music performance analysis that unifies score-to-performance alignment, score-informed transcription, and mistake detection in a near end-to-end manner. Unlike prior methods addressing these tasks separately, RUMAA integrates them using pre-trained score and audio encoders and a novel tri-stream decoder capturing task interdependencies through proxy tasks. It aligns human-readable MusicXML scores with repeat symbols to full-length performance audio, overcoming traditional MIDI-based methods that rely on manually unfolded score-MIDI data with pre-specified repeat structures. RUMAA matches state-of-the-art alignment methods on non-repeated scores and outperforms them on scores with repeats in a public piano music dataset, while also delivering promising transcription and mistake detection results.         ",
    "url": "https://arxiv.org/abs/2507.12175",
    "authors": [
      "Sungkyun Chang",
      "Simon Dixon",
      "Emmanouil Benetos"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2507.12185",
    "title": "Exploiting Jailbreaking Vulnerabilities in Generative AI to Bypass Ethical Safeguards for Facilitating Phishing Attacks",
    "abstract": "           The advent of advanced Generative AI (GenAI) models such as DeepSeek and ChatGPT has significantly reshaped the cybersecurity landscape, introducing both promising opportunities and critical risks. This study investigates how GenAI powered chatbot services can be exploited via jailbreaking techniques to bypass ethical safeguards, enabling the generation of phishing content, recommendation of hacking tools, and orchestration of phishing campaigns. In ethically controlled experiments, we used ChatGPT 4o Mini selected for its accessibility and status as the latest publicly available model at the time of experimentation, as a representative GenAI system. Our findings reveal that the model could successfully guide novice users in executing phishing attacks across various vectors, including web, email, SMS (smishing), and voice (vishing). Unlike automated phishing campaigns that typically follow detectable patterns, these human-guided, AI assisted attacks are capable of evading traditional anti phishing mechanisms, thereby posing a growing security threat. We focused on DeepSeek and ChatGPT due to their widespread adoption and technical relevance in 2025. The study further examines common jailbreaking techniques and the specific vulnerabilities exploited in these models. Finally, we evaluate a range of mitigation strategies such as user education, advanced authentication mechanisms, and regulatory policy measures and discuss emerging trends in GenAI facilitated phishing, outlining future research directions to strengthen cybersecurity defenses in the age of artificial intelligence.         ",
    "url": "https://arxiv.org/abs/2507.12185",
    "authors": [
      "Rina Mishra",
      "Gaurav Varshney"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2507.12201",
    "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models",
    "abstract": "           Diffusion models have achieved state-of-the-art performance in generative modeling, yet their sampling procedures remain vulnerable to hallucinations, often stemming from inaccuracies in score approximation. In this work, we reinterpret diffusion sampling through the lens of optimization and introduce RODS (Robust Optimization-inspired Diffusion Sampler), a novel method that detects and corrects high-risk sampling steps using geometric cues from the loss landscape. RODS enforces smoother sampling trajectories and adaptively adjusts perturbations, reducing hallucinations without retraining and at minimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands demonstrate that RODS improves both sampling fidelity and robustness, detecting over 70% of hallucinated samples and correcting more than 25%, all while avoiding the introduction of new artifacts.         ",
    "url": "https://arxiv.org/abs/2507.12201",
    "authors": [
      "Yiqi Tian",
      "Pengfei Jin",
      "Mingze Yuan",
      "Na Li",
      "Bo Zeng",
      "Quanzheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2507.12232",
    "title": "MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM",
    "abstract": "           Recent studies have utilized visual large language models (VLMs) to answer not only \"Is this face a forgery?\" but also \"Why is the face a forgery?\" These studies introduced forgery-related attributes, such as forgery location and type, to construct deepfake VQA datasets and train VLMs, achieving high accuracy while providing human-understandable explanatory text descriptions. However, these methods still have limitations. For example, they do not fully leverage face quality-related attributes, which are often abnormal in forged faces, and they lack effective training strategies for forgery-aware VLMs. In this paper, we extend the VQA dataset to create DD-VQA+, which features a richer set of attributes and a more diverse range of samples. Furthermore, we introduce a novel forgery detection framework, MGFFD-VLM, which integrates an Attribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual Large Language Models (VLMs). Additionally, our framework incorporates Multi-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By transforming classification and forgery segmentation results into prompts, our method not only improves forgery classification but also enhances interpretability. To further boost detection performance, we design multiple forgery-related auxiliary losses. Experimental results demonstrate that our approach surpasses existing methods in both text-based forgery judgment and analysis, achieving superior accuracy.         ",
    "url": "https://arxiv.org/abs/2507.12232",
    "authors": [
      "Tao Chen",
      "Jingyi Zhang",
      "Decheng Liu",
      "Chunlei Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12233",
    "title": "Universal Fourier Neural Operators for Micromechanics",
    "abstract": "           \\noindent Solving cell problems in homogenization is hard, and available deep-learning frameworks fail to match the speed and generality of traditional computational frameworks. More to the point, it is generally unclear what to expect of machine-learning approaches, let alone single out which approaches are promising. In the work at hand, we advocate Fourier Neural Operators (FNOs) for micromechanics, empowering them by insights from computational micromechanics methods based on the fast Fourier transform (FFT). We construct an FNO surrogate mimicking the basic scheme foundational for FFT-based methods and show that the resulting operator predicts solutions to cell problems with \\emph{arbitrary} stiffness distribution only subject to a material-contrast constraint up to a desired accuracy. In particular, there are no restrictions on the material symmetry like isotropy, on the number of phases and on the geometry of the interfaces between materials. Also, the provided fidelity is sharp and uniform, providing explicit guarantees leveraging our physical empowerment of FNOs. To show the desired universal approximation property, we construct an FNO explicitly that requires no training to begin with. Still, the obtained neural operator complies with the same memory requirements as the basic scheme and comes with runtimes proportional to classical FFT solvers. In particular, large-scale problems with more than 100 million voxels are readily handled. The goal of this work is to underline the potential of FNOs for solving micromechanical problems, linking FFT-based methods to FNOs. This connection is expected to provide a fruitful exchange between both worlds.         ",
    "url": "https://arxiv.org/abs/2507.12233",
    "authors": [
      "Binh Huy Nguyen",
      "Matti Schneider"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12244",
    "title": "Which graph motif parameters count?",
    "abstract": "           For a fixed graph H, the function #IndSub(H,*) maps graphs G to the count of induced H-copies in G; this function obviously \"counts something\" in that it has a combinatorial interpretation. Linear combinations of such functions are called graph motif parameters and have recently received significant attention in counting complexity after a seminal paper by Curticapean, Dell and Marx (STOC'17). We show that, among linear combinations of functions #IndSub(H,*) involving only graphs H without isolated vertices, precisely those with positive integer coefficients maintain a combinatorial interpretation. It is important to note that graph motif parameters can be nonnegative for all inputs G, even when some coefficients are negative. Formally, we show that evaluating any graph motif parameter with a negative coefficient is impossible in an oracle variant of #P, where an implicit graph is accessed by oracle queries. Our proof follows the classification of the relativizing closure properties of #P by Hertrampf, Vollmer, and Wagner (SCT'95) and the framework developed by Ikenmeyer and Pak (STOC'22), but our application of the required Ramsey theorem turns out to be more subtle, as graphs do not have the required Ramsey property. Our techniques generalize from graphs to relational structures, including colored graphs. Vastly generalizing this, we introduce motif parameters over categories that count occurrences of sub-objects in the category. We then prove a general dichotomy theorem that characterizes which such parameters have a combinatorial interpretation. Using known results in Ramsey theory for categories, we obtain a dichotomy for motif parameters of finite vector spaces as well as parameter sets.         ",
    "url": "https://arxiv.org/abs/2507.12244",
    "authors": [
      "Markus Bl\u00e4ser",
      "Radu Curticapean",
      "Julian D\u00f6rfler",
      "Christian Ikenmeyer"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2507.12257",
    "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws",
    "abstract": "           Exploring causal relationships in stochastic time series is a challenging yet crucial task with a vast range of applications, including finance, economics, neuroscience, and climate science. Many algorithms for Causal Discovery (CD) have been proposed, but they often exhibit a high sensitivity to noise, resulting in misleading causal inferences when applied to real data. In this paper, we observe that the frequency spectra of typical real-world time series follow a power-law distribution, notably due to an inherent self-organizing behavior. Leveraging this insight, we build a robust CD method based on the extraction of power -law spectral features that amplify genuine causal signals. Our method consistently outperforms state-of-the-art alternatives on both synthetic benchmarks and real-world datasets with known causal structures, demonstrating its robustness and practical relevance.         ",
    "url": "https://arxiv.org/abs/2507.12257",
    "authors": [
      "Matteo Tusoni",
      "Giuseppe Masi",
      "Andrea Coletta",
      "Aldo Glielmo",
      "Viviana Arrigoni",
      "Novella Bartolini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Machine Learning (stat.ML)",
      "Other Statistics (stat.OT)"
    ]
  },
  {
    "id": "arXiv:2507.12259",
    "title": "Neural Co-state Regulator: A Data-Driven Paradigm for Real-time Optimal Control with Input Constraints",
    "abstract": "           We propose a novel unsupervised learning framework for solving nonlinear optimal control problems (OCPs) with input constraints in real-time. In this framework, a neural network (NN) learns to predict the optimal co-state trajectory that minimizes the control Hamiltonian for a given system, at any system's state, based on the Pontryagin's Minimum Principle (PMP). Specifically, the NN is trained to find the norm-optimal co-state solution that simultaneously satisfies the nonlinear system dynamics and minimizes a quadratic regulation cost. The control input is then extracted from the predicted optimal co-state trajectory by solving a quadratic program (QP) to satisfy input constraints and optimality conditions. We coin the term neural co-state regulator (NCR) to describe the combination of the co-state NN and control input QP solver. To demonstrate the effectiveness of the NCR, we compare its feedback control performance with that of an expert nonlinear model predictive control (MPC) solver on a unicycle model. Because the NCR's training does not rely on expert nonlinear control solvers which are often suboptimal, the NCR is able to produce solutions that outperform the nonlinear MPC solver in terms of convergence error and input trajectory smoothness even for system conditions that are outside its original training domain. At the same time, the NCR offers two orders of magnitude less computational time than the nonlinear MPC.         ",
    "url": "https://arxiv.org/abs/2507.12259",
    "authors": [
      "Lihan Lian",
      "Yuxin Tong",
      "Uduak Inyang-Udoh"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.12262",
    "title": "A Framework for Nonstationary Gaussian Processes with Neural Network Parameters",
    "abstract": "           Gaussian processes have become a popular tool for nonparametric regression because of their flexibility and uncertainty quantification. However, they often use stationary kernels, which limit the expressiveness of the model and may be unsuitable for many datasets. We propose a framework that uses nonstationary kernels whose parameters vary across the feature space, modeling these parameters as the output of a neural network that takes the features as input. The neural network and Gaussian process are trained jointly using the chain rule to calculate derivatives. Our method clearly describes the behavior of the nonstationary parameters and is compatible with approximation methods for scaling to large datasets. It is flexible and easily adapts to different nonstationary kernels without needing to redesign the optimization procedure. Our methods are implemented with the GPyTorch library and can be readily modified. We test a nonstationary variance and noise variant of our method on several machine learning datasets and find that it achieves better accuracy and log-score than both a stationary model and a hierarchical model approximated with variational inference. Similar results are observed for a model with only nonstationary variance. We also demonstrate our approach's ability to recover the nonstationary parameters of a spatial dataset.         ",
    "url": "https://arxiv.org/abs/2507.12262",
    "authors": [
      "Zachary James",
      "Joseph Guinness"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2507.12265",
    "title": "FastReChain: Highly Responsive and Low-Overhead Centralized Route Scheduling in Clos Datacenter Networks",
    "abstract": "           Ever since Clos topologies were used in datacenter networks (DCNs), a practical centralized scheduling algorithm that supports dynamic scheduling has been absent. The introduction of optical switches in DCNs as a future-proof solution exacerbates this problem due to several properties of optical switches, such as the fact that they are generally bufferless and therefore rely on centralized scheduling, and that they have long switching times and therefore require the number of rearrangements to be minimized. In this paper, we propose a centralized scheduling algorithm that achieves theoretical maximum throughput even in one-rate bidirectional Clos networks, while producing schemes with near-minimal numbers of rearrangements. It is the only algorithm that directly supports bidirectional Clos networks and has a time efficiency high enough to support dynamic scheduling to date. For static minimal rewiring, its running time ranges from a fraction to a few hundredths of other algorithms, and the number of rearrangements has also been steadily improved, allowing for more frequent adjustments and less impact on ongoing communications. In addition, the algorithm is very flexible and can support various functional requirements in real-world environments. We achieve this result through the replacement chain concept and bitset optimization.         ",
    "url": "https://arxiv.org/abs/2507.12265",
    "authors": [
      "Zihan Zhu",
      "Dongchao Wu",
      "Zhanbang Zhang",
      "Jian Yang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2507.12269",
    "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants",
    "abstract": "           Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of extremely low birth weight infants. Defined by oxygen dependence at 36 weeks postmenstrual age, it causes lifelong respiratory complications. However, preventive interventions carry severe risks, including neurodevelopmental impairment, ventilator-induced lung injury, and systemic complications. Therefore, early BPD prognosis and prediction of BPD outcome is crucial to avoid unnecessary toxicity in low risk infants. Admission radiographs of extremely preterm infants are routinely acquired within 24h of life and could serve as a non-invasive prognostic tool. In this work, we developed and investigated a deep learning approach using chest X-rays from 163 extremely low-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within 24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult chest radiographs, employing progressive layer freezing with discriminative learning rates to prevent overfitting and evaluated a CutMix augmentation and linear probing. For moderate/severe BPD outcome prediction, our best performing model with progressive freezing, linear probing and CutMix achieved an AUROC of 0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67 $\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet initialization (p = 0.031) which confirms domain-specific pretraining to be important for BPD outcome prediction. Routine IRDS grades showed limited prognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned markers. Our approach demonstrates that domain-specific pretraining enables accurate BPD prediction from routine day-1 radiographs. Through progressive freezing and linear probing, the method remains computationally feasible for site-level implementation and future federated learning deployments.         ",
    "url": "https://arxiv.org/abs/2507.12269",
    "authors": [
      "Sybelle Goedicke-Fritz",
      "Michelle Bous",
      "Annika Engel",
      "Matthias Flotho",
      "Pascal Hirsch",
      "Hannah Wittig",
      "Dino Milanovic",
      "Dominik Mohr",
      "Mathias Kaspar",
      "Sogand Nemat",
      "Dorothea Kerner",
      "Arno B\u00fccker",
      "Andreas Keller",
      "Sascha Meyer",
      "Michael Zemlin",
      "Philipp Flotho"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12283",
    "title": "FADE: Adversarial Concept Erasure in Flow Models",
    "abstract": "           Diffusion models have demonstrated remarkable image generation capabilities, but also pose risks in privacy and fairness by memorizing sensitive concepts or perpetuating biases. We propose a novel \\textbf{concept erasure} method for text-to-image diffusion models, designed to remove specified concepts (e.g., a private individual or a harmful stereotype) from the model's generative repertoire. Our method, termed \\textbf{FADE} (Fair Adversarial Diffusion Erasure), combines a trajectory-aware fine-tuning strategy with an adversarial objective to ensure the concept is reliably removed while preserving overall model fidelity. Theoretically, we prove a formal guarantee that our approach minimizes the mutual information between the erased concept and the model's outputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable Diffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity, explicit content, and style erasure tasks from MACE). FADE achieves state-of-the-art concept removal performance, surpassing recent baselines like ESD, UCE, MACE, and ANT in terms of removal efficacy and image quality. Notably, FADE improves the harmonic mean of concept removal and fidelity by 5--10\\% over the best prior method. We also conduct an ablation study to validate each component of FADE, confirming that our adversarial and trajectory-preserving objectives each contribute to its superior performance. Our work sets a new standard for safe and fair generative modeling by unlearning specified concepts without retraining from scratch.         ",
    "url": "https://arxiv.org/abs/2507.12283",
    "authors": [
      "Zixuan Fu",
      "Yan Ren",
      "Finn Carter",
      "Chenyue Wang",
      "Ze Niu",
      "Dacheng Yu",
      "Emily Davis",
      "Bo Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12284",
    "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks",
    "abstract": "           Advancements in LLMs have enhanced task automation in software engineering; however, current evaluations primarily focus on natural language tasks, overlooking code quality. Most benchmarks prioritize high-level reasoning over executable code and real-world performance, leaving gaps in understanding true capabilities and risks associated with these models in production. To address this issue, we propose MERA Code, a new addition to the MERA benchmark family, specifically focused on evaluating code for the latest code generation LLMs in Russian. This benchmark includes 11 evaluation tasks that span 8 programming languages. Our proposed evaluation methodology features a taxonomy that outlines the practical coding skills necessary for models to complete these tasks. The benchmark comprises an open-source codebase for users to conduct MERA assessments, a scoring system compatible with various programming environments, and a platform featuring a leaderboard and submission system. We evaluate open LLMs and frontier API models, analyzing their limitations in terms of practical coding tasks in non-English languages. We are publicly releasing MERA to guide future research, anticipate groundbreaking features in model development, and standardize evaluation procedures.         ",
    "url": "https://arxiv.org/abs/2507.12284",
    "authors": [
      "Artem Chervyakov",
      "Alexander Kharitonov",
      "Pavel Zadorozhny",
      "Adamenko Pavel",
      "Rodion Levichev",
      "Dmitrii Vorobev",
      "Dmitrii Salikhov",
      "Aidar Valeev",
      "Alena Pestova",
      "Maria Dziuba",
      "Ilseyar Alimova",
      "Artem Zavgorodnev",
      "Aleksandr Medvedev",
      "Stanislav Moiseev",
      "Elena Bruches",
      "Daniil Grebenkin",
      "Roman Derunets",
      "Vikulov Vladimir",
      "Anton Emelyanov",
      "Dmitrii Babaev",
      "Vladimir V. Ivanov",
      "Valentin Malykh",
      "Alena Fenogenova"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.12295",
    "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding",
    "abstract": "           Text anomaly detection is a critical task in natural language processing (NLP), with applications spanning fraud detection, misinformation identification, spam detection and content moderation, etc. Despite significant advances in large language models (LLMs) and anomaly detection algorithms, the absence of standardized and comprehensive benchmarks for evaluating the existing anomaly detection methods on text data limits rigorous comparison and development of innovative approaches. This work performs a comprehensive empirical study and introduces a benchmark for text anomaly detection, leveraging embeddings from diverse pre-trained language models across a wide array of text datasets. Our work systematically evaluates the effectiveness of embedding-based text anomaly detection by incorporating (1) early language models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI (small, ada, large)); (3) multi-domain text datasets (news, social media, scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC). Our experiments reveal a critical empirical insight: embedding quality significantly governs anomaly detection efficacy, and deep learning-based approaches demonstrate no performance advantage over conventional shallow algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived this http URL addition, we observe strongly low-rank characteristics in cross-model performance matrices, which enables an efficient strategy for rapid model evaluation (or embedding evaluation) and selection in practical applications. Furthermore, by open-sourcing our benchmark toolkit that includes all embeddings from different models and code at this https URL, this work provides a foundation for future research in robust and scalable text anomaly detection systems.         ",
    "url": "https://arxiv.org/abs/2507.12295",
    "authors": [
      "Feng Xiao",
      "Jicong Fan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12308",
    "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization",
    "abstract": "           Large Language Models (LLMs) have become widely used across diverse NLP tasks and domains, demonstrating their adaptability and effectiveness. In the realm of Electronic Design Automation (EDA), LLMs show promise for tasks like Register-Transfer Level (RTL) code generation and summarization. However, despite the proliferation of LLMs for general code-related tasks, there's a dearth of research focused on evaluating and refining these models for hardware description languages (HDLs), notably VHDL. In this study, we evaluate the performance of existing code LLMs for VHDL code generation and summarization using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter, an in-house dataset, aims to gauge LLMs' understanding of functionally equivalent code. Our findings reveal consistent underperformance of these models across different metrics, underscoring a significant gap in their suitability for this domain. To address this challenge, we propose Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of LLMs for VHDL code generation and summarization tasks. CoDes involves generating a series of intermediate descriptive steps based on: (i) the problem statement for code generation, and (ii) the VHDL code for summarization. These steps are then integrated with the original input prompt (problem statement or code) and provided as input to the LLMs to generate the final output. Our experiments demonstrate that the CoDes approach significantly surpasses the standard prompting strategy across various metrics on both datasets. This method not only improves the quality of VHDL code generation and summarization but also serves as a framework for future research aimed at enhancing code LLMs for VHDL.         ",
    "url": "https://arxiv.org/abs/2507.12308",
    "authors": [
      "Prashanth Vijayaraghavan",
      "Apoorva Nitsure",
      "Charles Mackin",
      "Luyao Shi",
      "Stefano Ambrogio",
      "Arvind Haran",
      "Viresh Paruthi",
      "Ali Elzein",
      "Dan Coops",
      "David Beymer",
      "Tyler Baldwin",
      "Ehsan Degan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2507.12314",
    "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack",
    "abstract": "           While reinforcement learning-trained Large Reasoning Models (LRMs, e.g., Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large Language Models (LLMs) domain, their susceptibility to security threats remains a critical vulnerability. This weakness is particularly evident in Chain-of-Thought (CoT) generation processes, where adversarial methods like backdoor prompt attacks can systematically subvert the model's core reasoning mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this vulnerability through exploiting prompt controllability, simultaneously degrading both CoT safety and task performance with low-cost interventions. To address this compounded security-performance vulnerability, we propose Thought Purity (TP): a defense paradigm that systematically strengthens resistance to malicious content while preserving operational efficacy. Our solution achieves this through three synergistic components: (1) a safety-optimized data processing pipeline (2) reinforcement learning-enhanced rule constraints (3) adaptive monitoring metrics. Our approach establishes the first comprehensive defense mechanism against CoTA vulnerabilities in reinforcement learning-aligned reasoning systems, significantly advancing the security-functionality equilibrium for next-generation AI architectures.         ",
    "url": "https://arxiv.org/abs/2507.12314",
    "authors": [
      "Zihao Xue",
      "Zhen Bi",
      "Long Ma",
      "Zhenlin Hu",
      "Yan Wang",
      "Zhenfang Liu",
      "Qing Sheng",
      "Jie Xiao",
      "Jungang Lou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2507.12318",
    "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models",
    "abstract": "           We argue that diffusion models' success in modeling complex distributions is, for the most part, coming from their input conditioning. This paper investigates the representation used to condition diffusion models from the perspective that ideal representations should improve sample fidelity, be easy to generate, and be compositional to allow out-of-training samples generation. We introduce Discrete Latent Code (DLC), an image representation derived from Simplicial Embeddings trained with a self-supervised learning objective. DLCs are sequences of discrete tokens, as opposed to the standard continuous image embeddings. They are easy to generate and their compositionality enables sampling of novel images beyond the training distribution. Diffusion models trained with DLCs have improved generation fidelity, establishing a new state-of-the-art for unconditional image generation on ImageNet. Additionally, we show that composing DLCs allows the image generator to produce out-of-distribution samples that coherently combine the semantics of images in diverse ways. Finally, we showcase how DLCs can enable text-to-image generation by leveraging large-scale pretrained language models. We efficiently finetune a text diffusion language model to generate DLCs that produce novel samples outside of the image generator training distribution.         ",
    "url": "https://arxiv.org/abs/2507.12318",
    "authors": [
      "Samuel Lavoie",
      "Michael Noukhovitch",
      "Aaron Courville"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12329",
    "title": "Neural Polar Decoders for Deletion Channels",
    "abstract": "           This paper introduces a neural polar decoder (NPD) for deletion channels with a constant deletion rate. Existing polar decoders for deletion channels exhibit high computational complexity of $O(N^4)$, where $N$ is the block length. This limits the application of polar codes for deletion channels to short-to-moderate block lengths. In this work, we demonstrate that employing NPDs for deletion channels can reduce the computational complexity. First, we extend the architecture of the NPD to support deletion channels. Specifically, the NPD architecture consists of four neural networks (NNs), each replicating fundamental successive cancellation (SC) decoder operations. To support deletion channels, we change the architecture of only one. The computational complexity of the NPD is $O(AN\\log N)$, where the parameter $A$ represents a computational budget determined by the user and is independent of the channel. We evaluate the new extended NPD for deletion channels with deletion rates $\\delta\\in\\{0.01, 0.1\\}$ and we verify the NPD with the ground truth given by the trellis decoder by Tal et al. We further show that due to the reduced complexity of the NPD, we are able to incorporate list decoding and further improve performance. We believe that the extended NPD presented here could have applications in future technologies like DNA storage.         ",
    "url": "https://arxiv.org/abs/2507.12329",
    "authors": [
      "Ziv Aharoni",
      "Henry D. Pfister"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12339",
    "title": "Symbolic Control: Unveiling Free Robustness Margins",
    "abstract": "           This paper addresses the challenge of ensuring robustness in the presence of system perturbations for symbolic control techniques. Given a discrete-time control system that is related to its symbolic model by an alternating simulation relation. In this paper, we focus on computing the maximum robustness margin under which the symbolic model remains valid for a perturbed-version of the discrete-time control system. We first show that symbolic models are inherently equipped with a certain free robustness margins. We then provide constructive procedures to compute uniform and non-uniform (state and input dependent) robustness margins. We also show that the tightness of the robustness margin depends on the tightness of the reachability technique used to compute the symbolic model. We then explain how the computed robustness margin can be used for the sake of controller synthesis. Finally, we present two illustrative examples to demonstrate the effectiveness of our approach.         ",
    "url": "https://arxiv.org/abs/2507.12339",
    "authors": [
      "Youssef Ait Si",
      "Antoine Girard",
      "Adnane Saoud"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.12344",
    "title": "Improving Lightweight Weed Detection via Knowledge Distillation",
    "abstract": "           Weed detection is a critical component of precision agriculture, facilitating targeted herbicide application and reducing environmental impact. However, deploying accurate object detection models on resource-limited platforms remains challenging, particularly when differentiating visually similar weed species commonly encountered in plant phenotyping applications. In this work, we investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative Distillation (MGD) to enhance the performance of lightweight models for real-time smart spraying systems. Utilizing YOLO11x as the teacher model and YOLO11n as both reference and student, both CWD and MGD effectively transfer knowledge from the teacher to the student model. Our experiments, conducted on a real-world dataset comprising sugar beet crops and four weed types (Cirsium, Convolvulus, Fallopia, and Echinochloa), consistently show increased AP50 across all classes. The distilled CWD student model achieves a notable improvement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without increasing model complexity. Additionally, we validate real-time deployment feasibility by evaluating the student YOLO11n model on Jetson Orin Nano and Raspberry Pi 5 embedded devices, performing five independent runs to evaluate performance stability across random seeds. These findings confirm CWD and MGD as an effective, efficient, and practical approach for improving deep learning-based weed detection accuracy in precision agriculture and plant phenotyping scenarios.         ",
    "url": "https://arxiv.org/abs/2507.12344",
    "authors": [
      "Ahmet O\u011fuz Salt\u0131k",
      "Max Voigt",
      "Sourav Modak",
      "Mike Beckworth",
      "Anthony Stein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12359",
    "title": "Cluster Contrast for Unsupervised Visual Representation Learning",
    "abstract": "           We introduce Cluster Contrast (CueCo), a novel approach to unsupervised visual representation learning that effectively combines the strengths of contrastive learning and clustering methods. Inspired by recent advancements, CueCo is designed to simultaneously scatter and align feature representations within the feature space. This method utilizes two neural networks, a query and a key, where the key network is updated through a slow-moving average of the query outputs. CueCo employs a contrastive loss to push dissimilar features apart, enhancing inter-class separation, and a clustering objective to pull together features of the same cluster, promoting intra-class compactness. Our method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18 backbone. By integrating contrastive learning with clustering, CueCo sets a new direction for advancing unsupervised visual representation learning.         ",
    "url": "https://arxiv.org/abs/2507.12359",
    "authors": [
      "Nikolaos Giakoumoglou",
      "Tania Stathaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.12366",
    "title": "FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization",
    "abstract": "           Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical analysis and reasoning. Hyperdimensional Computing (HDC), a promising brain-inspired computational model, is integral to neuro-symbolic AI. Various HDC models have been proposed to represent class-instance and class-class relations, but when representing the more complex class-subclass relation, where multiple objects associate different levels of classes and subclasses, they face challenges for factorization, a crucial task for neuro-symbolic AI systems. In this article, we propose FactorHD, a novel HDC model capable of representing and factorizing the complex class-subclass relation efficiently. FactorHD features a symbolic encoding method that embeds an extra memorization clause, preserving more information for multiple objects. In addition, it employs an efficient factorization algorithm that selectively eliminates redundant classes by identifying the memorization clause of the target class. Such model significantly enhances computing efficiency and accuracy in representing and factorizing multiple objects with class-subclass relation, overcoming limitations of existing HDC models such as \"superposition catastrophe\" and \"the problem of 2\". Evaluations show that FactorHD achieves approximately 5667x speedup at a representation size of 10^9 compared to existing HDC models. When integrated with the ResNet-18 neural network, FactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.         ",
    "url": "https://arxiv.org/abs/2507.12366",
    "authors": [
      "Yifei Zhou",
      "Xuchu Huang",
      "Chenyu Ni",
      "Min Zhou",
      "Zheyu Yan",
      "Xunzhao Yin",
      "Cheng Zhuo"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12367",
    "title": "GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities",
    "abstract": "           The rapid evolution of software libraries poses a considerable hurdle for code generation, necessitating continuous adaptation to frequent version updates while preserving backward compatibility. While existing code evolution benchmarks provide valuable insights, they typically lack execution-based evaluation for generating code compliant with specific library versions. To address this, we introduce GitChameleon, a novel, meticulously curated dataset comprising 328 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. GitChameleon rigorously evaluates the capacity of contemporary large language models (LLMs), LLM-powered agents, code assistants, and RAG systems to perform version-conditioned code generation that demonstrates functional accuracy through execution. Our extensive evaluations indicate that state-of-the-art systems encounter significant challenges with this task; enterprise models achieving baseline success rates in the 48-51\\% range, underscoring the intricacy of the problem. By offering an execution-based benchmark emphasizing the dynamic nature of code libraries, GitChameleon enables a clearer understanding of this challenge and helps guide the development of more adaptable and dependable AI code generation methods. We make the dataset and evaluation code publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.12367",
    "authors": [
      "Diganta Misra",
      "Nizar Islah",
      "Victor May",
      "Brice Rauby",
      "Zihan Wang",
      "Justine Gehring",
      "Antonio Orvieto",
      "Muawiz Chaudhary",
      "Eilif B. Muller",
      "Irina Rish",
      "Samira Ebrahimi Kahou",
      "Massimo Caccia"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2507.12370",
    "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate",
    "abstract": "           Large Language Models (LLMs) have demonstrated significant capabilities in understanding and generating human language, contributing to more natural interactions with complex systems. However, they face challenges such as ambiguity in user requests processed by LLMs. To address these challenges, this paper introduces and evaluates a multi-agent debate framework designed to enhance detection and resolution capabilities beyond single models. The framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and Mistral-7B variants) and a dataset with diverse ambiguities. The debate framework markedly enhanced the performance of Llama3-8B and Mistral-7B variants over their individual baselines, with Mistral-7B-led debates achieving a notable 76.7% success rate and proving particularly effective for complex ambiguities and efficient consensus. While acknowledging varying model responses to collaborative strategies, these findings underscore the debate framework's value as a targeted method for augmenting LLM capabilities. This work offers important insights for developing more robust and adaptive language understanding systems by showing how structured debates can lead to improved clarity in interactive systems.         ",
    "url": "https://arxiv.org/abs/2507.12370",
    "authors": [
      "Ana Davila",
      "Jacinto Colan",
      "Yasuhisa Hasegawa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2507.12372",
    "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics",
    "abstract": "           Large language models (LLMs) have traditionally relied on static training data, limiting their knowledge to fixed snapshots. Recent advancements, however, have equipped LLMs with web browsing capabilities, enabling real time information retrieval and multi step reasoning over live web content. While prior studies have demonstrated LLMs ability to access and analyze websites, their capacity to directly retrieve and analyze social media data remains unexplored. Here, we evaluate whether web browsing LLMs can infer demographic attributes of social media users given only their usernames. Using a synthetic dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international participants, we show that these models can access social media content and predict user demographics with reasonable accuracy. Analysis of the synthetic dataset further reveals how LLMs parse and interpret social media profiles, which may introduce gender and political biases against accounts with minimal activity. While this capability holds promise for computational social science in the post API era, it also raises risks of misuse particularly in information operations and targeted advertising underscoring the need for safeguards. We recommend that LLM providers restrict this capability in public facing applications, while preserving controlled access for verified research purposes.         ",
    "url": "https://arxiv.org/abs/2507.12372",
    "authors": [
      "Meysam Alizadeh",
      "Fabrizio Gilardi",
      "Zeynab Samei",
      "Mohsen Mosleh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.12378",
    "title": "Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker",
    "abstract": "           Traditional information extraction systems face challenges with text only language models as it does not consider infographics (visual elements of information) such as tables, charts, images etc. often used to convey complex information to readers. Multimodal LLM (MLLM) face challenges of finding needle in the haystack problem i.e., either longer context length or substantial number of documents as search space. Late interaction mechanism over visual language models has shown state of the art performance in retrieval-based vision augmented Q&A tasks. There are yet few challenges using it for RAG based multi-modal Q&A. Firstly, many popular and widely adopted vector databases do not support native multi-vector retrieval. Secondly, late interaction requires computation which inflates space footprint and can hinder enterprise adoption. Lastly, the current state of late interaction mechanism does not leverage the approximate neighbor search indexing methods for large speed ups in retrieval process. This paper explores a pragmatic approach to make vision retrieval process scalable and efficient without compromising on performance quality. We propose multi-step custom implementation utilizing widely adopted hybrid search (metadata & embedding) and state of the art late interaction re-ranker to retrieve best matching pages. Finally, MLLM are prompted as reader to generate answers from contextualized best matching pages. Through experiments, we observe that the proposed design is scalable (significant speed up) and stable (without degrading performance quality), hence can be used as production systems at enterprises.         ",
    "url": "https://arxiv.org/abs/2507.12378",
    "authors": [
      "Rachna Saxena",
      "Abhijeet Kumar",
      "Suresh Shanmugam"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.12396",
    "title": "OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments",
    "abstract": "           Realistic human surveillance datasets are crucial for training and evaluating computer vision models under real-world conditions, facilitating the development of robust algorithms for human and human-interacting object detection in complex environments. These datasets need to offer diverse and challenging data to enable a comprehensive assessment of model performance and the creation of more reliable surveillance systems for public safety. To this end, we present two visual object detection benchmarks named OD-VIRAT Large and OD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance imagery. The video sequences in both benchmarks cover 10 different scenes of human surveillance recorded from significant height and distance. The proposed benchmarks offer rich annotations of bounding boxes and categories, where OD-VIRAT Large has 8.7 million annotated instances in 599,996 images and OD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also focuses on benchmarking state-of-the-art object detection architectures, including RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object detection-specific variant of VIRAT dataset. To the best of our knowledge, it is the first work to examine the performance of these recently published state-of-the-art object detection architectures on realistic surveillance imagery under challenging conditions such as complex backgrounds, occluded objects, and small-scale objects. The proposed benchmarking and experimental settings will help in providing insights concerning the performance of selected object detection models and set the base for developing more efficient and robust object detection architectures.         ",
    "url": "https://arxiv.org/abs/2507.12396",
    "authors": [
      "Hayat Ullah",
      "Abbas Khan",
      "Arslan Munir",
      "Hari Kalva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12400",
    "title": "Modeling Feasible Locomotion of Nanobots for Cancer Detection and Treatment",
    "abstract": "           Deploying motile nanosized particles, also known as ``nanobots'', in the human body promises to improve selectivity in drug delivery and reduce side effects. We consider a swarm of nanobots locating a single cancerous region and treating it by releasing an onboard payload of drugs at the site. At nanoscale, the computation, communication, sensing, and locomotion capabilities of individual agents are extremely limited, noisy, and/or nonexistent. We present a general model to formally describe the individual and collective behavior of agents in a colloidal environment, such as the bloodstream, for cancer detection and treatment by nanobots. This includes a feasible and precise model of agent locomotion, inspired by actual nanoparticles that, in the presence of an external chemical gradient, move towards areas of higher concentration by means of self-propulsion. We present two variants of our general model: The first assumes an endogenous chemical gradient that is fixed over time and centered at the targeted cancer site; the second is a more speculative and dynamic variant in which agents themselves create and amplify a chemical gradient centered at the cancer site. In both settings, agents can sense the gradient and ascend it noisily, locating the cancer site more quickly than via simple Brownian motion. For the first variant of the model, we present simulation results to show the behavior of agents under our locomotion model, as well as {analytical results} to bound the time it takes for the agents to reach the cancer site. For the second variant, simulation results highlight the collective benefit in having agents issue their own chemical signal. While arguably more speculative in its agent capability assumptions, this variant shows a significant improvement in runtime performance over the first variant, resulting from its chemical signal amplification mechanism.         ",
    "url": "https://arxiv.org/abs/2507.12400",
    "authors": [
      "Noble Harasha",
      "Cristina Gava",
      "Nancy Lynch",
      "Claudia Contini",
      "Frederik Mallmann-Trenn"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2507.12415",
    "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?",
    "abstract": "           Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field.         ",
    "url": "https://arxiv.org/abs/2507.12415",
    "authors": [
      "Xinyi He",
      "Qian Liu",
      "Mingzhe Du",
      "Lin Yan",
      "Zhijie Fan",
      "Yiming Huang",
      "Zejian Yuan",
      "Zejun Ma"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2507.12426",
    "title": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition",
    "abstract": "           The landscape of video recognition has evolved significantly, shifting from traditional Convolutional Neural Networks (CNNs) to Transformer-based architectures for improved accuracy. While 3D CNNs have been effective at capturing spatiotemporal dynamics, recent Transformer models leverage self-attention to model long-range spatial and temporal dependencies. Despite achieving state-of-the-art performance on major benchmarks, Transformers remain computationally expensive, particularly with dense video data. To address this, we propose a lightweight Video Focal Modulation Network, DVFL-Net, which distills spatiotemporal knowledge from a large pre-trained teacher into a compact nano student model, enabling efficient on-device deployment. DVFL-Net utilizes knowledge distillation and spatial-temporal feature modulation to significantly reduce computation while preserving high recognition performance. We employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal focal modulation to effectively transfer both local and global context from the Video-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate DVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it against recent state-of-the-art methods in Human Action Recognition (HAR). Additionally, we conduct a detailed ablation study analyzing the impact of forward KL divergence. The results confirm the superiority of DVFL-Net in achieving an optimal balance between performance and efficiency, demonstrating lower memory usage, reduced GFLOPs, and strong accuracy, making it a practical solution for real-time HAR applications.         ",
    "url": "https://arxiv.org/abs/2507.12426",
    "authors": [
      "Hayat Ullah",
      "Muhammad Ali Shafique",
      "Abbas Khan",
      "Arslan Munir"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12433",
    "title": "Traffic-Aware Pedestrian Intention Prediction",
    "abstract": "           Accurate pedestrian intention estimation is crucial for the safe navigation of autonomous vehicles (AVs) and hence attracts a lot of research attention. However, current models often fail to adequately consider dynamic traffic signals and contextual scene information, which are critical for real-world applications. This paper presents a Traffic-Aware Spatio-Temporal Graph Convolutional Network (TA-STGCN) that integrates traffic signs and their states (Red, Yellow, Green) into pedestrian intention prediction. Our approach introduces the integration of dynamic traffic signal states and bounding box size as key features, allowing the model to capture both spatial and temporal dependencies in complex urban environments. The model surpasses existing methods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy compared to the baseline model on the PIE dataset, demonstrating its effectiveness in improving pedestrian intention prediction.         ",
    "url": "https://arxiv.org/abs/2507.12433",
    "authors": [
      "Fahimeh Orvati Nia",
      "Hai Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.12435",
    "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks",
    "abstract": "           Modern deep neural networks are powerful predictive tools yet often lack valid inference for causal parameters, such as treatment effects or entire survival curves. While frameworks like Double Machine Learning (DML) and Targeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits, existing neural implementations either rely on \"targeted losses\" that do not guarantee solving the efficient influence function equation or computationally expensive post-hoc \"fluctuations\" for multi-parameter settings. We propose Targeted Deep Architectures (TDA), a new framework that embeds TMLE directly into the network's parameter space with no restrictions on the backbone architecture. Specifically, TDA partitions model parameters - freezing all but a small \"targeting\" subset - and iteratively updates them along a targeting gradient, derived from projecting the influence functions onto the span of the gradients of the loss with respect to weights. This procedure yields plug-in estimates that remove first-order bias and produce asymptotically valid confidence intervals. Crucially, TDA easily extends to multi-dimensional causal estimands (e.g., entire survival curves) by merging separate targeting gradients into a single universal targeting update. Theoretically, TDA inherits classical TMLE properties, including double robustness and semiparametric efficiency. Empirically, on the benchmark IHDP dataset (average treatment effects) and simulated survival data with informative censoring, TDA reduces bias and improves coverage relative to both standard neural-network estimators and prior post-hoc approaches. In doing so, TDA establishes a direct, scalable pathway toward rigorous causal inference within modern deep architectures for complex multi-parameter targets.         ",
    "url": "https://arxiv.org/abs/2507.12435",
    "authors": [
      "Yi Li",
      "David Mccoy",
      "Nolan Gunter",
      "Kaitlyn Lee",
      "Alejandro Schuler",
      "Mark van der Laan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2304.12063",
    "title": "Risk in Stochastic and Robust Model Predictive Path-Following Control for Vehicular Motion Planning",
    "abstract": "           In automated driving, risk describes potential harm to passengers of an autonomous vehicle (AV) and other road users. Recent studies suggest that human-like driving behavior emerges from embedding risk in AV motion planning algorithms. Additionally, providing evidence that risk is minimized during the AV operation is essential to vehicle safety certification. However, there has yet to be a consensus on how to define and operationalize risk in motion planning or how to bound or minimize it during operation. In this paper, we define a stochastic risk measure and introduce it as a constraint into both robust and stochastic nonlinear model predictive path-following controllers (RMPC and SMPC respectively). We compare the vehicle's behavior arising from employing SMPC and RMPC with respect to safety and path-following performance. Further, the implementation of an automated driving example is provided, showcasing the effects of different risk tolerances and uncertainty growths in predictions of other road users for both cases. We find that the RMPC is significantly more conservative than the SMPC, while also displaying greater following errors towards references. Further, the RMPCs behavior cannot be considered as human-like. Moreover, unlike SMPC, the RMPC cannot account for different risk tolerances. The RMPC generates undesired driving behavior for even moderate uncertainties, which are handled better by the SMPC.         ",
    "url": "https://arxiv.org/abs/2304.12063",
    "authors": [
      "Leon Tolksdorf",
      "Arturo Tejada",
      "Nathan van de Wouw",
      "Christian Birkner"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.11551",
    "title": "Landmark Detection for Medical Images using a General-purpose Segmentation Model",
    "abstract": "           Radiographic images are a cornerstone of medical diagnostics in orthopaedics, with anatomical landmark detection serving as a crucial intermediate step for information extraction. General-purpose foundational segmentation models, such as SAM (Segment Anything Model), do not support landmark segmentation out of the box and require prompts to function. However, in medical imaging, the prompts for landmarks are highly specific. Since SAM has not been trained to recognize such landmarks, it cannot generate accurate landmark segmentations for diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has been trained to identify larger anatomical structures, such as organs and their parts, and lacks the fine-grained precision required for orthopaedic pelvic landmarks. To address this limitation, we propose leveraging another general-purpose, non-foundational model: YOLO. YOLO excels in object detection and can provide bounding boxes that serve as input prompts for SAM. While YOLO is efficient at detection, it is significantly outperformed by SAM in segmenting complex structures. In combination, these two models form a reliable pipeline capable of segmenting not only a small pilot set of eight anatomical landmarks but also an expanded set of 72 landmarks and 16 regions with complex outlines, such as the femoral cortical bone and the pelvic inlet. By using YOLO-generated bounding boxes to guide SAM, we trained the hybrid model to accurately segment orthopaedic pelvic radiographs. Our results show that the proposed combination of YOLO and SAM yields excellent performance in detecting anatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.         ",
    "url": "https://arxiv.org/abs/2507.11551",
    "authors": [
      "Ekaterina Stansfield",
      "Jennifer A. Mitterer",
      "Abdulrahman Altahhan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.11686",
    "title": "Multiset Metric Dimension of Binomial Random Graphs",
    "abstract": "           For a graph $G = (V,E)$ and a subset $R \\subseteq V$, we say that $R$ is \\textit{multiset resolving} for $G$ if for every pair of vertices $v,w$, the \\textit{multisets} $\\{d(v,r): r \\in R\\}$ and $\\{d(w,r):r \\in R\\}$ are distinct, where $d(x,y)$ is the graph distance between vertices $x$ and $y$. The \\textit{multiset metric dimension} of $G$ is the size of a smallest set $R \\subseteq V$ that is multiset resolving (or $\\infty$ if no such set exists). This graph parameter was introduced by Simanjuntak, Siagian, and Vitr\u00edk in 2017~\\cite{simanjuntak2017multiset}, and has since been studied for a variety of graph families. We prove bounds which hold with high probability for the multiset metric dimension of the binomial random graph $G(n,p)$ in the regime $d = (n-1)p = \\Theta(n^{x})$ for fixed $x \\in (0,1)$.         ",
    "url": "https://arxiv.org/abs/2507.11686",
    "authors": [
      "Austin Eide",
      "Pawel Pralat"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2507.11799",
    "title": "Fragment size density estimator for shrinkage-induced fracture based on a physics-informed neural network",
    "abstract": "           This paper presents a neural network (NN)-based solver for an integro-differential equation that models shrinkage-induced fragmentation. The proposed method directly maps input parameters to the corresponding probability density function without numerically solving the governing equation, thereby significantly reducing computational costs. Specifically, it enables efficient evaluation of the density function in Monte Carlo simulations while maintaining accuracy comparable to or even exceeding that of conventional finite difference schemes. Validatation on synthetic data demonstrates both the method's computational efficiency and predictive reliability. This study establishes a foundation for the data-driven inverse analysis of fragmentation and suggests the potential for extending the framework beyond pre-specified model structures.         ",
    "url": "https://arxiv.org/abs/2507.11799",
    "authors": [
      "Shin-ichi Ito"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.11842",
    "title": "CosmoFlow: Scale-Aware Representation Learning for Cosmology with Flow Matching",
    "abstract": "           Generative machine learning models have been demonstrated to be able to learn low dimensional representations of data that preserve information required for downstream tasks. In this work, we demonstrate that flow matching based generative models can learn compact, semantically rich latent representations of field level cold dark matter (CDM) simulation data without supervision. Our model, CosmoFlow, learns representations 32x smaller than the raw field data, usable for field level reconstruction, synthetic data generation, and parameter inference. Our model also learns interpretable representations, in which different latent channels correspond to features at different cosmological scales.         ",
    "url": "https://arxiv.org/abs/2507.11842",
    "authors": [
      "Sidharth Kannan",
      "Tian Qiu",
      "Carolina Cuesta-Lazaro",
      "Haewon Jeong"
    ],
    "subjectives": [
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12005",
    "title": "Kernelization for list $H$-coloring for graphs with small vertex cover",
    "abstract": "           For a fixed graph $H$, in the List $H$-Coloring problem, we are given a graph $G$ along with list $L(v) \\subseteq V(H)$ for every $v \\in V(G)$, and we have to determine if there exists a list homomorphism $\\varphi$ from $(G,L)$ to $H$, i.e., an edge preserving mapping $\\varphi: V(G)\\to V(H)$ that satisfies $\\varphi(v)\\in L(v)$ for every $v\\in V(G)$. Note that if $H$ is the complete graph on $q$ vertices, the problem is equivalent to List $q$-Coloring. We investigate the kernelization properties of List $H$-Coloring parameterized by the vertex cover number of $G$: given an instance $(G,L)$ and a vertex cover of $G$ of size $k$, can we reduce $(G,L)$ to an equivalent instance $(G',L')$ of List $H$-Coloring where the size of $G'$ is bounded by a low-degree polynomial $p(k)$ in $k$? This question has been investigated previously by Jansen and Pieterse [Algorithmica 2019], who provided an upper bound, which turns out to be optimal if $H$ is a complete graph, i.e., for List $q$-Coloring. This result was one of the first applications of the method of kernelization via bounded-degree polynomials. We define two new integral graph invariants, $c^*(H)$ and $d^*(H)$, with $d^*(H) \\leq c^*(H) \\leq d^*(H)+1$, and show that for every graph $H$, List $H$-Coloring -- has a kernel with $\\mathcal{O}(k^{c^*(H)})$ vertices, -- admits no kernel of size $\\mathcal{O}(k^{d^*(H)-\\varepsilon})$ for any $\\varepsilon > 0$, unless the polynomial hierarchy collapses. -- Furthermore, if $c^*(H) > d^*(H)$, then there is a kernel with $\\mathcal{O}(k^{c^*(H)-\\varepsilon})$ vertices where $\\varepsilon \\geq 2^{1-c^*(H)}$. Additionally, we show that for some classes of graphs, including powers of cycles and graphs $H$ where $\\Delta(H) \\leq c^*(H)$ (which in particular includes cliques), the bound $d^*(H)$ is tight, using the polynomial method. We conjecture that this holds in general.         ",
    "url": "https://arxiv.org/abs/2507.12005",
    "authors": [
      "Marta Piecyk",
      "Astrid Pieterse",
      "Pawe\u0142 Rz\u0105\u017cewski",
      "Magnus Wahlstr\u00f6m"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2507.12132",
    "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi",
    "abstract": "           Wi-Fi Channel State Information (CSI) has gained increasing interest for remote sensing applications. Recent studies show that Doppler velocity projections extracted from CSI can enable human activity recognition (HAR) that is robust to environmental changes and generalizes to new users. However, despite these advances, generalizability still remains insufficient for practical deployment. Inspired by neural radiance fields (NeRF), which learn a volumetric representation of a 3D scene from 2D images, this work proposes a novel approach to reconstruct an informative 3D latent motion representation from one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The resulting latent representation is then used to construct a uniform Doppler radiance field (DoRF) of the motion, providing a comprehensive view of the performed activity and improving the robustness to environmental variability. The results show that the proposed approach noticeably enhances the generalization accuracy of Wi-Fi-based HAR, highlighting the strong potential of DoRFs for practical sensing applications.         ",
    "url": "https://arxiv.org/abs/2507.12132",
    "authors": [
      "Navid Hasanzadeh",
      "Shahrokh Valaee"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.12301",
    "title": "Leveraging Bi-Directional Channel Reciprocity for Robust Ultra-Low-Rate Implicit CSI Feedback with Deep Learning",
    "abstract": "           Deep learning-based implicit channel state information (CSI) feedback has been introduced to enhance spectral efficiency in massive MIMO systems. Existing methods often show performance degradation in ultra-low-rate scenarios and inadaptability across diverse environments. In this paper, we propose Dual-ImRUNet, an efficient uplink-assisted deep implicit CSI feedback framework incorporating two novel plug-in preprocessing modules to achieve ultra-low feedback rates while maintaining high environmental robustness. First, a novel bi-directional correlation enhancement module is proposed to strengthen the correlation between uplink and downlink CSI eigenvector matrices. This module projects highly correlated uplink and downlink channel matrices into their respective eigenspaces, effectively reducing redundancy for ultra-low-rate feedback. Second, an innovative input format alignment module is designed to maintain consistent data distributions at both encoder and decoder sides without extra transmission overhead, thereby enhancing robustness against environmental variations. Finally, we develop an efficient transformer-based implicit CSI feedback network to exploit angular-delay domain sparsity and bi-directional correlation for ultra-low-rate CSI compression. Simulation results demonstrate successful reduction of the feedback overhead by 85% compared with the state-of-the-art method and robustness against unseen environments.         ",
    "url": "https://arxiv.org/abs/2507.12301",
    "authors": [
      "Zhenyu Liu",
      "Yi Ma",
      "Rahim Tafazolli",
      "Zhi Ding"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2507.12404",
    "title": "Neural Network-Guided Symbolic Regression for Interpretable Descriptor Discovery in Perovskite Catalysts",
    "abstract": "           Understanding and predicting the activity of oxide perovskite catalysts for the oxygen evolution reaction (OER) requires descriptors that are both accurate and physically interpretable. While symbolic regression (SR) offers a path to discover such formulas, its performance degrades with high-dimensional inputs and small datasets. We present a two-phase framework that combines neural networks (NN), feature importance analysis, and symbolic regression (SR) to discover interpretable descriptors for OER activity in oxide perovskites. In Phase I, using a small dataset and seven structural features, we reproduce and improve the known {\\mu}/t descriptor by engineering composite features and applying symbolic regression, achieving training and validation MAEs of 22.8 and 20.8 meV, respectively. In Phase II, we expand to 164 features, reduce dimensionality, and identify LUMO energy as a key electronic descriptor. A final formula using {\\mu}/t, {\\mu}/RA, and LUMO energy achieves improved accuracy (training and validation MAEs of 22.1 and 20.6 meV) with strong physical interpretability. Our results demonstrate that NN-guided symbolic regression enables accurate, interpretable, and physically meaningful descriptor discovery in data-scarce regimes, indicating interpretability need not sacrifice accuracy for materials informatics.         ",
    "url": "https://arxiv.org/abs/2507.12404",
    "authors": [
      "Yeming Xian",
      "Xiaoming Wang",
      "Yanfa Yan"
    ],
    "subjectives": [
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2507.12427",
    "title": "Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation",
    "abstract": "           We propose UTS, a unit-based tissue segmentation framework for histopathology that classifies each fixed-size 32 * 32 tile, rather than each pixel, as the segmentation unit. This approach reduces annotation effort and improves computational efficiency without compromising accuracy. To implement this approach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits the multi-level feature representation to capture both fine-grained morphology and global tissue context. Trained to segment breast tissue into three categories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports clinically relevant tasks such as tumor-stroma quantification and surgical margin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it outperforms U-Net variants and transformer-based baselines. Code and Dataset will be available at GitHub.         ",
    "url": "https://arxiv.org/abs/2507.12427",
    "authors": [
      "Ashkan Shakarami",
      "Azade Farshad",
      "Yousef Yeganeh",
      "Lorenzo Nicole",
      "Peter Schuffler",
      "Stefano Ghidoni",
      "Nassir Navab"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2309.10527",
    "title": "SPOT: Scalable 3D Pre-training via Occupancy Prediction for Learning Transferable 3D Representations",
    "abstract": "           Annotating 3D LiDAR point clouds for perception tasks is fundamental for many applications e.g., autonomous driving, yet it still remains notoriously labor-intensive. Pretraining-finetuning approach can alleviate the labeling burden by fine-tuning a pre-trained backbone across various downstream datasets as well as tasks. In this paper, we propose SPOT, namely Scalable Pre-training via Occupancy prediction for learning Transferable 3D representations under such a label-efficient fine-tuning paradigm. SPOT achieves effectiveness on various public datasets with different downstream tasks, showcasing its general representation power, cross-domain robustness and data scalability which are three key factors for real-world application. Specifically, we both theoretically and empirically show, for the first time, that general representations learning can be achieved through the task of occupancy prediction. Then, to address the domain gap caused by different LiDAR sensors and annotation methods, we develop a beam re-sampling technique for point cloud augmentation combined with class-balancing strategy. Furthermore, scalable pre-training is observed, that is, the downstream performance across all the experiments gets better with more pre-training data. Additionally, such pre-training strategy also remains compatible with unlabeled data. The hope is that our findings will facilitate the understanding of LiDAR points and pave the way for future advancements in LiDAR pre-training.         ",
    "url": "https://arxiv.org/abs/2309.10527",
    "authors": [
      "Xiangchao Yan",
      "Runjian Chen",
      "Bo Zhang",
      "Hancheng Ye",
      "Renqiu Xia",
      "Jiakang Yuan",
      "Hongbin Zhou",
      "Xinyu Cai",
      "Botian Shi",
      "Wenqi Shao",
      "Ping Luo",
      "Yu Qiao",
      "Tao Chen",
      "Junchi Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.12216",
    "title": "Sharing is CAIRing: Characterizing Principles and Assessing Properties of Universal Privacy Evaluation for Synthetic Tabular Data",
    "abstract": "           Data sharing is a necessity for innovative progress in many domains, especially in healthcare. However, the ability to share data is hindered by regulations protecting the privacy of natural persons. Synthetic tabular data provide a promising solution to address data sharing difficulties but does not inherently guarantee privacy. Still, there is a lack of agreement on appropriate methods for assessing the privacy-preserving capabilities of synthetic data, making it difficult to compare results across studies. To the best of our knowledge, this is the first work to identify properties that constitute good universal privacy evaluation metrics for synthetic tabular data. The goal of universally applicable metrics is to enable comparability across studies and to allow non-technical stakeholders to understand how privacy is protected. We identify four principles for the assessment of metrics: Comparability, Applicability, Interpretability, and Representativeness (CAIR). To quantify and rank the degree to which evaluation metrics conform to the CAIR principles, we design a rubric using a scale of 1-4. Each of the four properties is scored on four parameters, yielding 16 total dimensions. We study the applicability and usefulness of the CAIR principles and rubric by assessing a selection of metrics popular in other studies. The results provide granular insights into the strengths and weaknesses of existing metrics that not only rank the metrics but highlight areas of potential improvements. We expect that the CAIR principles will foster agreement among researchers and organizations on which universal privacy evaluation metrics are appropriate for synthetic tabular data.         ",
    "url": "https://arxiv.org/abs/2312.12216",
    "authors": [
      "Tobias Hyrup",
      "Anton Danholt Lautrup",
      "Arthur Zimek",
      "Peter Schneider-Kamp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2402.11662",
    "title": "TDE-3: An improved prior for optical flow computation in spiking neural networks",
    "abstract": "           Motion detection is a primary task required for robotic systems to perceive and navigate in their environment. Proposed in the literature bioinspired neuromorphic Time-Difference Encoder (TDE-2) combines event-based sensors and processors with spiking neural networks to provide real-time and energy-efficient motion detection through extracting temporal correlations between two points in space. However, on the algorithmic level, this design leads to loss of direction-selectivity of individual TDEs in textured environments. Here we propose an augmented 3-point TDE (TDE-3) with additional inhibitory input that makes TDE-3 direction-selectivity robust in textured environments. We developed a procedure to train the new TDE-3 using backpropagation through time and surrogate gradients to linearly map input velocities into an output spike count or an Inter-Spike Interval (ISI). Our work is the first instance of training a spiking neuron to have a specific ISI. Using synthetic data we compared training and inference with spike count and ISI with respect to changes in stimuli dynamic range, spatial frequency, and level of noise. ISI turns out to be more robust towards variation in spatial frequency, whereas the spike count is a more reliable training signal in the presence of noise. We performed the first in-depth quantitative investigation of optical flow coding with TDE and compared TDE-2 vs TDE-3 in terms of energy-efficiency and coding precision. Results show that on the network level both detectors show similar precision (20 degree angular error, 88% correlation with ground truth). Yet, due to the more robust direction-selectivity of individual TDEs, TDE-3 based network spike less and hence is more energy-efficient. Reported precision is on par with model-based methods but the spike-based processing of the TDEs provides allows more energy-efficient inference with neuromorphic hardware.         ",
    "url": "https://arxiv.org/abs/2402.11662",
    "authors": [
      "Matthew Yedutenko",
      "Federico Paredes-Valles",
      "Lyes Khacef",
      "Guido C.H.E. De Croon"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2404.06050",
    "title": "Incremental Joint Learning of Depth, Pose and Implicit Scene Representation on Monocular Camera in Large-scale Scenes",
    "abstract": "           Dense scene reconstruction for photo-realistic view synthesis has various applications, such as VR/AR, autonomous vehicles. However, most existing methods have difficulties in large-scale scenes due to three core challenges: \\textit{(a) inaccurate depth input.} Accurate depth input is impossible to get in real-world large-scale scenes. \\textit{(b) inaccurate pose estimation.} Most existing approaches rely on accurate pre-estimated camera poses. \\textit{(c) insufficient scene representation capability.} A single global radiance field lacks the capacity to effectively scale to large-scale scenes. To this end, we propose an incremental joint learning framework, which can achieve accurate depth, pose estimation, and large-scale scene reconstruction. A vision transformer-based network is adopted as the backbone to enhance performance in scale information estimation. For pose estimation, a feature-metric bundle adjustment (FBA) method is designed for accurate and robust camera tracking in large-scale scenes. In terms of implicit scene representation, we propose an incremental scene representation method to construct the entire large-scale scene as multiple local radiance fields to enhance the scalability of 3D scene representation. Extended experiments have been conducted to demonstrate the effectiveness and accuracy of our method in depth estimation, pose estimation, and large-scale scene reconstruction.         ",
    "url": "https://arxiv.org/abs/2404.06050",
    "authors": [
      "Tianchen Deng",
      "Nailin Wang",
      "Chongdi Wang",
      "Shenghai Yuan",
      "Jingchuan Wang",
      "Hesheng Wang",
      "Danwei Wang",
      "Weidong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2406.08379",
    "title": "Gazing Into Missteps: Leveraging Eye-Gaze for Unsupervised Mistake Detection in Egocentric Videos of Skilled Human Activities",
    "abstract": "           We address the challenge of unsupervised mistake detection in egocentric video of skilled human activities through the analysis of gaze signals. While traditional methods rely on manually labeled mistakes, our approach does not require mistake annotations, hence overcoming the need of domain-specific labeled data. Based on the observation that eye movements closely follow object manipulation activities, we assess to what extent eye-gaze signals can support mistake detection, proposing to identify deviations in attention patterns measured through a gaze tracker with respect to those estimated by a gaze prediction model. Since predicting gaze in video is characterized by high uncertainty, we propose a novel gaze completion task, where eye fixations are predicted from visual observations and partial gaze trajectories, and contribute a novel gaze completion approach which explicitly models correlations between gaze information and local visual tokens. Inconsistencies between predicted and observed gaze trajectories act as an indicator to identify mistakes. Experiments highlight the effectiveness of the proposed approach in different settings, with relative gains up to +14%, +11%, and +5% in EPIC-Tent, HoloAssist and IndustReal respectively, remarkably matching results of supervised approaches without seeing any labels. We further show that gaze-based analysis is particularly useful in the presence of skilled actions, low action execution confidence, and actions requiring hand-eye coordination and object manipulation skills. Our method is ranked first on the HoloAssist Mistake Detection challenge.         ",
    "url": "https://arxiv.org/abs/2406.08379",
    "authors": [
      "Michele Mazzamuto",
      "Antonino Furnari",
      "Yoichi Sato",
      "Giovanni Maria Farinella"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.08788",
    "title": "Towards Understanding Link Predictor Generalizability Under Distribution Shifts",
    "abstract": "           State-of-the-art link prediction (LP) models demonstrate impressive benchmark results. However, popular benchmark datasets often assume that training, validation, and testing samples are representative of the overall dataset distribution. In real-world situations, this assumption is often incorrect; uncontrolled factors lead new dataset samples to come from a different distribution than training samples. Additionally, the majority of recent work with graph dataset shift focuses on node- and graph-level tasks, largely ignoring link-level tasks. To bridge this gap, we introduce a novel splitting strategy, known as LPShift, which utilizes structural properties to induce a controlled distribution shift. We verify LPShift's effect through empirical evaluation of SOTA LP models on 16 LPShift variants of original dataset splits, with results indicating drastic changes to model performance. Additional experiments demonstrate graph structure has a strong influence on the success of current generalization methods. Source Code Available Here: this https URL ",
    "url": "https://arxiv.org/abs/2406.08788",
    "authors": [
      "Jay Revolinsky",
      "Harry Shomer",
      "Jiliang Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.14335",
    "title": "Linearly-Interpretable Concept Embedding Models for Text Analysis",
    "abstract": "           Despite their success, Large-Language Models (LLMs) still face criticism due to their lack of interpretability. Traditional post-hoc interpretation methods, based on attention and gradient-based analysis, offer limited insights as they only approximate the model's decision-making processes and have been proved to be unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been lately proposed in the textual field to provide interpretable predictions based on human-understandable concepts. However, CBMs still exhibit several limitations due to their architectural constraints limiting their expressivity, to the absence of task-interpretability when employing non-linear task predictors and for requiring extensive annotations that are impractical for real-world text data. In this paper, we address these challenges by proposing a novel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the current accuracy-interpretability trade-off. LICEMs classification accuracy is better than existing interpretable models and matches black-box ones. We show that the explanations provided by our models are more interveneable and causally consistent with respect to existing solutions. Finally, we show that LICEMs can be trained without requiring any concept supervision, as concepts can be automatically predicted when using an LLM backbone.         ",
    "url": "https://arxiv.org/abs/2406.14335",
    "authors": [
      "Francesco De Santis",
      "Philippe Bich",
      "Gabriele Ciravegna",
      "Pietro Barbiero",
      "Danilo Giordano",
      "Tania Cerquitelli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.00765",
    "title": "Structured and Balanced Multi-Component and Multi-Layer Neural Networks",
    "abstract": "           In this work, we propose a balanced multi-component and multi-layer neural network (MMNN) structure to accurately and efficiently approximate functions with complex features, in terms of both degrees of freedom and computational cost. The main idea is inspired by a multi-component approach, in which each component can be effectively approximated by a single-layer network, combined with a multi-layer decomposition strategy to capture the complexity of the target function. Although MMNNs can be viewed as a simple modification of fully connected neural networks (FCNNs) or multi-layer perceptrons (MLPs) by introducing balanced multi-component structures, they achieve a significant reduction in training parameters, a much more efficient training process, and improved accuracy compared to FCNNs or MLPs. Extensive numerical experiments demonstrate the effectiveness of MMNNs in approximating highly oscillatory functions and their ability to automatically adapt to localized features.         ",
    "url": "https://arxiv.org/abs/2407.00765",
    "authors": [
      "Shijun Zhang",
      "Hongkai Zhao",
      "Yimin Zhong",
      "Haomin Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2409.15615",
    "title": "KISS-Matcher: Fast and Robust Point Cloud Registration Revisited",
    "abstract": "           While global point cloud registration systems have advanced significantly in all aspects, many studies have focused on specific components, such as feature extraction, graph-theoretic pruning, or pose solvers. In this paper, we take a holistic view on the registration problem and develop an open-source and versatile C++ library for point cloud registration, called KISS-Matcher. KISS-Matcher combines a novel feature detector, Faster-PFH, that improves over the classical fast point feature histogram (FPFH). Moreover, it adopts a $k$-core-based graph-theoretic pruning to reduce the time complexity of rejecting outlier correspondences. Finally, it combines these modules in a complete, user-friendly, and ready-to-use pipeline. As verified by extensive experiments, KISS-Matcher has superior scalability and broad applicability, achieving a substantial speed-up compared to state-of-the-art outlier-robust registration pipelines while preserving accuracy. Our code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.15615",
    "authors": [
      "Hyungtae Lim",
      "Daebeom Kim",
      "Gunhee Shin",
      "Jingnan Shi",
      "Ignacio Vizzo",
      "Hyun Myung",
      "Jaesik Park",
      "Luca Carlone"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2410.00603",
    "title": "An Empirical Study of Large Language Models for Type and Call Graph Analysis in Python and JavaScript",
    "abstract": "           Large Language Models (LLMs) are increasingly being explored for their potential in software engineering, particularly in static analysis tasks. In this study, we investigate the potential of current LLMs to enhance call-graph analysis and type inference for Python and JavaScript programs. We empirically evaluated 24 LLMs, including OpenAI's GPT series and open-source models like LLaMA and Mistral, using existing and newly developed benchmarks. Specifically, we enhanced TypeEvalPy, a micro-benchmarking framework for type inference in Python, with auto-generation capabilities, expanding its scope from 860 to 77,268 type annotations for Python. Additionally, we introduced SWARM-CG and SWARM-JS, comprehensive benchmarking suites for evaluating call-graph construction tools across multiple programming languages. Our findings reveal a contrasting performance of LLMs in static analysis tasks. For call-graph generation, traditional static analysis tools such as PyCG for Python and Jelly for JavaScript consistently outperform LLMs. While advanced models like mistral-large-it-2407-123b and gpt-4o show promise, they still struggle with completeness and soundness in call-graph analysis across both languages. In contrast, LLMs demonstrate a clear advantage in type inference for Python, surpassing traditional tools like HeaderGen and hybrid approaches such as HiTyper. These results suggest that, while LLMs hold promise in type inference, their limitations in call-graph analysis highlight the need for further research. Our study provides a foundation for integrating LLMs into static analysis workflows, offering insights into their strengths and current limitations.         ",
    "url": "https://arxiv.org/abs/2410.00603",
    "authors": [
      "Ashwin Prasad Shivarpatna Venkatesh",
      "Rose Sunil",
      "Samkutty Sabu",
      "Amir M. Mir",
      "Sofia Reis",
      "Eric Bodden"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2410.03103",
    "title": "Planning-Aware Code Infilling via Horizon-Length Prediction",
    "abstract": "           Fill-in-the-Middle (FIM), or infilling, has become integral to code language models, enabling generation of missing code given both left and right contexts. However, the current FIM training paradigm which performs next-token prediction (NTP) over reordered sequence often leads to models struggling to generate content that aligns well with the surrounding context. We hypothesize that NTP alone is insufficient for models to learn effective planning conditioned on the distant right context, a critical factor for successful code infilling. To overcome this, we propose Horizon-Length Prediction (HLP), a novel training objective that teaches models to predict the number of remaining middle tokens at each step. HLP advances FIM with lookahead planning, enabling models to inherently learn infilling boundaries for arbitrary left and right contexts without relying on dataset-specific post-processing. Our evaluation across different model families and sizes shows that HLP significantly improves FIM performance by up to 24% relatively on diverse benchmarks, across file-level and repository-level. Furthermore, the enhanced planning capability gained through HLP boosts model performance on code reasoning. Importantly, HLP incurs negligible training overhead and no additional inference cost, ensuring its practicality for real-world scenarios.         ",
    "url": "https://arxiv.org/abs/2410.03103",
    "authors": [
      "Yifeng Ding",
      "Hantian Ding",
      "Shiqi Wang",
      "Qing Sun",
      "Varun Kumar",
      "Zijian Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2410.09474",
    "title": "Distilling Invariant Representations with Dual Augmentation",
    "abstract": "           Knowledge distillation (KD) has been widely used to transfer knowledge from large, accurate models (teachers) to smaller, efficient ones (students). Recent methods have explored enforcing consistency by incorporating causal interpretations to distill invariant representations. In this work, we extend this line of research by introducing a dual augmentation strategy to promote invariant feature learning in both teacher and student models. Our approach leverages different augmentations applied to both models during distillation, pushing the student to capture robust, transferable features. This dual augmentation strategy complements invariant causal distillation by ensuring that the learned representations remain stable across a wider range of data variations and transformations. Extensive experiments on CIFAR-100 demonstrate the effectiveness of this approach, achieving competitive results in same-architecture KD.         ",
    "url": "https://arxiv.org/abs/2410.09474",
    "authors": [
      "Nikolaos Giakoumoglou",
      "Tania Stathaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.00265",
    "title": "Quantifying calibration error in modern neural networks through evidence based theory",
    "abstract": "           Trustworthiness in neural networks is crucial for their deployment in critical applications, where reliability, confidence, and uncertainty play pivotal roles in decision-making. Traditional performance metrics such as accuracy and precision fail to capture these aspects, particularly in cases where models exhibit overconfidence. To address these limitations, this paper introduces a novel framework for quantifying the trustworthiness of neural networks by incorporating subjective logic into the evaluation of Expected Calibration Error (ECE). This method provides a comprehensive measure of trust, disbelief, and uncertainty by clustering predicted probabilities and fusing opinions using appropriate fusion operators. We demonstrate the effectiveness of this approach through experiments on MNIST and CIFAR-10 datasets, where post-calibration results indicate improved trustworthiness. The proposed framework offers a more interpretable and nuanced assessment of AI models, with potential applications in sensitive domains such as healthcare and autonomous systems.         ",
    "url": "https://arxiv.org/abs/2411.00265",
    "authors": [
      "Koffi Ismael Ouattara"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Logic (math.LO)"
    ]
  },
  {
    "id": "arXiv:2411.02179",
    "title": "CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality",
    "abstract": "           High-quality environment lighting is essential for creating immersive mobile augmented reality (AR) experiences. However, achieving visually coherent estimation for mobile AR is challenging due to several key limitations in AR device sensing capabilities, including low camera FoV and limited pixel dynamic ranges. Recent advancements in generative AI, which can generate high-quality images from different types of prompts, including texts and images, present a potential solution for high-quality lighting estimation. Still, to effectively use generative image diffusion models, we must address two key limitations of content quality and slow inference. In this work, we design and implement a generative lighting estimation system called CleAR that can produce high-quality, diverse environment maps in the format of 360\u00b0 HDR images. Specifically, we design a two-step generation pipeline guided by AR environment context data to ensure the output aligns with the physical environment's visual context and color appearance. To improve the estimation robustness under different lighting conditions, we design a real-time refinement component to adjust lighting estimation results on AR devices. Through a combination of quantitative and qualitative evaluations, we show that CleAR outperforms state-of-the-art lighting estimation methods on both estimation accuracy, latency, and robustness, and is rated by 31 participants as producing better renderings for most virtual objects. For example, CleAR achieves 51% to 56% accuracy improvement on virtual object renderings across objects of three distinctive types of materials and reflective properties. CleAR produces lighting estimates of comparable or better quality in just 3.2 seconds -- over 110X faster than state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2411.02179",
    "authors": [
      "Yiqin Zhao",
      "Mallesham Dasari",
      "Tian Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2411.02572",
    "title": "ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy",
    "abstract": "           Large-scale cell microscopy screens are used in drug discovery and molecular biology research to study the effects of millions of chemical and genetic perturbations on cells. To use these images in downstream analysis, we need models that can map each image into a feature space that represents diverse biological phenotypes consistently, in the sense that perturbations with similar biological effects have similar representations. In this work, we present the largest foundation model for cell microscopy data to date, a new 1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image crops. Compared to a previous published ViT-L/8 MAE, our new model achieves a 60% improvement in linear separability of genetic perturbations and obtains the best overall performance on whole-genome biological relationship recall and replicate consistency benchmarks. Beyond scaling, we developed two key methods that improve performance: (1) training on a curated and diverse dataset; and, (2) using biologically motivated linear probing tasks to search across each transformer block for the best candidate representation of whole-genome screens. We find that many self-supervised vision transformers, pretrained on either natural or microscopy images, yield significantly more biologically meaningful representations of microscopy images in their intermediate blocks than in their typically used final blocks. More broadly, our approach and results provide insights toward a general strategy for successfully building foundation models for large-scale biological data.         ",
    "url": "https://arxiv.org/abs/2411.02572",
    "authors": [
      "Kian Kenyon-Dean",
      "Zitong Jerry Wang",
      "John Urbanik",
      "Konstantin Donhauser",
      "Jason Hartford",
      "Saber Saberian",
      "Nil Sahin",
      "Ihab Bendidi",
      "Safiye Celik",
      "Marta Fay",
      "Juan Sebastian Rodriguez Vera",
      "Imran S Haque",
      "Oren Kraus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.04202",
    "title": "Observability and Generalized Sensor Placement for Nonlinear Quality Models in Drinking Water Networks",
    "abstract": "           This paper studies the problem of optimal placement of water quality (WQ) sensors in water distribution networks (WDNs), with a focus on chlorine transport, decay, and reaction models. Such models are traditionally used as suitable proxies for WQ. The literature on this topic is inveterate, but has a key limitation: it utilizes simplified single-species decay and reaction models that do not capture WQ transients for nonlinear, multi-species interactions. This results in sensor placements (SP) that do not account for nonlinear WQ dynamics. Furthermore, as WQ simulations are parameterized by hydraulic profiles and demand patterns, the placement of sensors are often hydraulics-dependent. This study produces a greedy algorithm that addresses the two aforementioned limitations. The algorithm is grounded in nonlinear dynamic systems and observability theory, and yields SPs that are submodular and robust to hydraulic changes. Case studies on benchmark water networks are provided. The key findings provide practical recommendations for WDN operators.         ",
    "url": "https://arxiv.org/abs/2411.04202",
    "authors": [
      "Mohamad H. Kazma",
      "Salma M. Elsherif",
      "Ahmad F. Taha"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.05813",
    "title": "AI for Explosive Ordnance Detection in Clearance Operations: The State of Research",
    "abstract": "           The detection and clearance of explosive ordnance (EO) continues to be a predominantly manual and high-risk process that can benefit from advances in technology to improve its efficiency and effectiveness. Research on artificial intelligence (AI) for EO detection in clearance operations has grown significantly in recent years. However, this research spans a wide range of fields, making it difficult to gain a comprehensive understanding of current trends and developments. Therefore, this article provides a literature review of academic research on AI for EO detection in clearance operations. It finds that research can be grouped into two main streams: AI for EO object detection and AI for EO risk prediction, with the latter being much less studied than the former. From the literature review, we develop three opportunities for future research. These include a call for renewed efforts in the use of AI for EO risk prediction, the combination of different AI systems and data sources, and novel approaches to improve EO risk prediction performance, such as pattern-based predictions. Finally, we provide a perspective on the future of AI for EO detection in clearance operations. We emphasize the role of traditional machine learning (ML) for this task, the need to dynamically incorporate expert knowledge into the models, and the importance of effectively integrating AI systems with real-world operations.         ",
    "url": "https://arxiv.org/abs/2411.05813",
    "authors": [
      "Bj\u00f6rn Kischelewski",
      "Gregory Cathcart",
      "David Wahl",
      "Benjamin Guedj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.09127",
    "title": "Complexity-Aware Training of Deep Neural Networks for Optimal Structure Discovery",
    "abstract": "           We propose a novel algorithm for combined unit and layer pruning of deep neural networks that functions during training and without requiring a pre-trained network to apply. Our algorithm optimally trades-off learning accuracy and pruning levels while balancing layer vs. unit pruning and computational vs. parameter complexity using only three user-defined parameters, which are easy to interpret and tune. We formulate a stochastic optimization problem over the network weights and the parameters of variational Bernoulli distributions for binary Random Variables taking values either 0 or 1 and scaling the units and layers of the network. Optimal network structures are found as the solution to this optimization problem. Pruning occurs when a variational parameter converges to 0 rendering the corresponding structure permanently inactive, thus saving computations both during training and prediction. A key contribution of our approach is to define a cost function that combines the objectives of prediction accuracy and network pruning in a computational/parameter complexity-aware manner and the automatic selection of the many regularization parameters. We show that the proposed algorithm converges to solutions of the optimization problem corresponding to deterministic networks. We analyze the ODE system that underlies our stochastic optimization algorithm and establish domains of attraction for the dynamics of the network parameters. These theoretical results lead to practical pruning conditions avoiding the premature pruning of units and layers during training. We evaluate our method on the CIFAR-10/100 and ImageNet datasets using ResNet architectures and demonstrate that it gives improved results with respect to pruning ratios and test accuracy over layer-only or unit-only pruning and favorably competes with combined unit and layer pruning algorithms requiring pre-trained networks.         ",
    "url": "https://arxiv.org/abs/2411.09127",
    "authors": [
      "Valentin Frank Ingmar Guenter",
      "Athanasios Sideris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.13140",
    "title": "Robust Convergency Indicator using MIMO-PI Controller in the presence of disturbances",
    "abstract": "           The PID controller remains the most widely adopted control architecture, with groundbreaking success across extensive implications. However, optimal parameter tuning for PID controller remains a critical challenge. Existing theories predominantly focus on linear time-invariant systems and Single-Input Single-Output (SISO) scenarios, leaving a research gap in addressing complex PID control problems for Multi-Input Multi-Output (MIMO) nonlinear systems with disturbances. This study enhances controller robustness by leveraging insights into the velocity form of nonlinear systems. It establishes a quantitative metric to evaluate the robustness of MIMO-PI controller, clarifies key theories on how robustness influences exponential error stabilization. Guided by these theories, an optimal robust MIMO-PI controller is developed without oversimplifying assumptions. Experimental results demonstrate that the controller achieves effective exponential stabilization and exhibits exceptional robustness under the guidance of the proposed robust indicator. Notably, the robust convergence indicator can also effectively assess comprehensive performance.         ",
    "url": "https://arxiv.org/abs/2411.13140",
    "authors": [
      "Zimao Sheng",
      "Hongan Yang",
      "Jiakang Wang",
      "Tong Zhang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2501.01038",
    "title": "Energy-Efficient and Intelligent ISAC in V2X Networks with Spiking Neural Networks-Driven DRL",
    "abstract": "           Integrated sensing and communication (ISAC) is emerging as a key enabler for vehicle-to-everything (V2X) systems. However, designing efficient beamforming schemes for ISAC signals to achieve accurate sensing and enhance communication performance in the dynamic and uncertain environments of V2X networks presents significant challenges. While artificial intelligence technologies offer promising solutions, the energy-intensive nature of neural networks imposes substantial burdens on communication infrastructures. To address these challenges, this work proposes an energy-efficient and intelligent ISAC system for V2X networks. Specifically, we first leverage a Markov Decision Process framework to model the dynamic and uncertain nature of V2X networks. This framework allows the roadside unit to develop beamforming schemes relying solely on its current sensing information, eliminating the need for numerous pilot signals and extensive CSI acquisition. We then introduce an advanced deep reinforcement learning (DRL) algorithm, enabling the joint optimization of beamforming and power allocation to guarantee both communication rate and sensing accuracy in dynamic and uncertain V2X scenario. To alleviate the energy demands of neural networks, we integrate spiking neural networks (SNNs) into the DRL algorithm. The event-driven, sparse spike-based processing of SNNs significantly improves energy efficiency while maintaining strong performance. Extensive simulation results validate the effectiveness of the proposed scheme with lower energy consumption, superior communication performance, and improved sensing accuracy.         ",
    "url": "https://arxiv.org/abs/2501.01038",
    "authors": [
      "Chen Shang",
      "Jiadong Yu",
      "Dinh Thai Hoang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2501.03629",
    "title": "CFFormer: Cross CNN-Transformer Channel Attention and Spatial Feature Fusion for Improved Segmentation of Heterogeneous Medical Images",
    "abstract": "           Medical image segmentation plays an important role in computer-aided diagnosis. Existing methods mainly utilize spatial attention to highlight the region of interest. However, due to limitations of medical imaging devices, medical images exhibit significant heterogeneity, posing challenges for segmentation. Ultrasound images, for instance, often suffer from speckle noise, low resolution, and poor contrast between target tissues and background, which may lead to inaccurate boundary delineation. To address these challenges caused by heterogeneous image quality, we propose a hybrid CNN-Transformer model,called CFFormer, which leverages effective channel feature extraction to enhance the model' s ability to accurately identify tissue regions by capturing rich contextual information. The proposed architecture contains two key components: the Cross Feature Channel Attention (CFCA) module and the X-Spatial Feature Fusion (XFF) module. The model incorporates dual encoders, with the CNN encoder focusing on capturing local features and the Transformer encoder modeling global features. The CFCA module filters and facilitates interactions between the channel features from the two encoders, while the XFF module effectively reduces the significant semantic information differences in spatial features, enabling a smooth and cohesive spatial feature fusion. We evaluate our model across eight datasets covering five modalities to test its generalization capability. Experimental results demonstrate that our model outperforms current state-of-the-art methods and maintains accurate tissue region segmentation across heterogeneous medical image datasets. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2501.03629",
    "authors": [
      "Jiaxuan Li",
      "Qing Xu",
      "Xiangjian He",
      "Ziyu Liu",
      "Daokun Zhang",
      "Ruili Wang",
      "Rong Qu",
      "Guoping Qiu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.13916",
    "title": "PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy",
    "abstract": "           We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL), a communication-efficient Vertical Federated Learning algorithm with Differential Privacy guarantees. PBM-VFL combines Secure Multi-Party Computation with the recently introduced Poisson Binomial Mechanism to protect parties' private datasets during model training. We define the novel concept of feature privacy and analyze end-to-end feature and sample privacy of our algorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We also provide the first theoretical characterization of the relationship between privacy budget, convergence error, and communication cost in differentially-private VFL. Finally, we empirically show that our model performs well with high levels of privacy.         ",
    "url": "https://arxiv.org/abs/2501.13916",
    "authors": [
      "Linh Tran",
      "Timothy Castiglia",
      "Stacy Patterson",
      "Ana Milanova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.15151",
    "title": "SpikeDet: Better Firing Patterns for Accurate and Energy-Efficient Object Detection with Spiking Neuron Networks",
    "abstract": "           Spiking Neural Networks (SNNs) are the third generation of neural networks. They have gained widespread attention in object detection due to their low power consumption and biological interpretability. However, existing SNN-based object detection methods suffer from local firing saturation, where neurons in information-concentrated regions fire continuously throughout all time steps. This abnormal neuron firing pattern reduces the feature discrimination capability and detection accuracy, while also increasing the firing rates that prevent SNNs from achieving their potential energy efficiency. To address this problem, we propose SpikeDet, a novel spiking object detector that optimizes firing patterns for accurate and energy-efficient detection. Specifically, we design a spiking backbone network, MDSNet, which effectively adjusts the membrane synaptic input distribution at each layer, achieving better neuron firing patterns during spiking feature extraction. Additionally, to better utilize and preserve these high-quality backbone features, we introduce the Spiking Multi-direction Fusion Module (SMFM), which realizes multi-direction fusion of spiking features, enhancing the multi-scale detection capability of the model. Experimental results demonstrate that SpikeDet achieves superior performance. On the COCO 2017 dataset, it achieves 51.4% AP, outperforming previous SNN-based methods by 2.5% AP while requiring only half the power consumption. On object detection sub-tasks, including the GEN1 event-based dataset and the URPC 2019 underwater dataset, SpikeDet also achieves the best performance. Notably, on GEN1, our method achieves 47.6% AP, outperforming previous SNN-based methods by 7.2% AP with better energy efficiency.         ",
    "url": "https://arxiv.org/abs/2501.15151",
    "authors": [
      "Yimeng Fan",
      "Changsong Liu",
      "Mingyang Li",
      "Dongze Liu",
      "Yanyan Liu",
      "Wei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.05668",
    "title": "The late-stage training dynamics of (stochastic) subgradient descent on homogeneous neural networks",
    "abstract": "           We analyze the implicit bias of constant step stochastic subgradient descent (SGD). We consider the setting of binary classification with homogeneous neural networks - a large class of deep neural networks with ReLU-type activation functions such as MLPs and CNNs without biases. We interpret the dynamics of normalized SGD iterates as an Euler-like discretization of a conservative field flow that is naturally associated to the normalized classification margin. Owing to this interpretation, we show that normalized SGD iterates converge to the set of critical points of the normalized margin at late-stage training (i.e., assuming that the data is correctly classified with positive normalized margin). Up to our knowledge, this is the first extension of the analysis of Lyu and Li (2020) on the discrete dynamics of gradient descent to the nonsmooth and stochastic setting. Our main result applies to binary classification with exponential or logistic losses. We additionally discuss extensions to more general settings.         ",
    "url": "https://arxiv.org/abs/2502.05668",
    "authors": [
      "Sholom Schechtman",
      "Nicolas Schreuder"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.05843",
    "title": "From Objects to Events: Unlocking Complex Visual Understanding in Object Detectors via LLM-guided Symbolic Reasoning",
    "abstract": "           Current object detectors excel at entity localization and classification, yet exhibit inherent limitations in event recognition capabilities. This deficiency arises from their architecture's emphasis on discrete object identification rather than modeling the compositional reasoning, inter-object correlations, and contextual semantics essential for comprehensive event understanding. To address this challenge, we present a novel framework that expands the capability of standard object detectors beyond mere object recognition to complex event understanding through LLM-guided symbolic reasoning. Our key innovation lies in bridging the semantic gap between object detection and event understanding without requiring expensive task-specific training. The proposed plug-and-play framework interfaces with any open-vocabulary detector while extending their inherent capabilities across architectures. At its core, our approach combines (i) a symbolic regression mechanism exploring relationship patterns among detected entities and (ii) a LLM-guided strategically guiding the search toward meaningful expressions. These discovered symbolic rules transform low-level visual perception into interpretable event understanding, providing a transparent reasoning path from objects to events with strong transferability across this http URL compared our training-free framework against specialized event recognition systems across diverse application domains. Experiments demonstrate that our framework enhances multiple object detector architectures to recognize complex events such as illegal fishing activities (75% AUROC, +8.36% improvement), construction safety violations (+15.77%), and abnormal crowd behaviors (+23.16%). Code is available at \\href{this https URL}{here}.         ",
    "url": "https://arxiv.org/abs/2502.05843",
    "authors": [
      "Yuhui Zeng",
      "Haoxiang Wu",
      "Wenjie Nie",
      "Xiawu Zheng",
      "Guangyao Chen",
      "Yunhang Shen",
      "Jun Peng",
      "Yonghong Tian",
      "Rongrong Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.09724",
    "title": "Navigating the Social Welfare Frontier: Portfolios for Multi-objective Reinforcement Learning",
    "abstract": "           In many real-world applications of reinforcement learning (RL), deployed policies have varied impacts on different stakeholders, creating challenges in reaching consensus on how to effectively aggregate their preferences. Generalized $p$-means form a widely used class of social welfare functions for this purpose, with broad applications in fair resource allocation, AI alignment, and decision-making. This class includes well-known welfare functions such as Egalitarian, Nash, and Utilitarian welfare. However, selecting the appropriate social welfare function is challenging for decision-makers, as the structure and outcomes of optimal policies can be highly sensitive to the choice of $p$. To address this challenge, we study the concept of an $\\alpha$-approximate portfolio in RL, a set of policies that are approximately optimal across the family of generalized $p$-means for all $p \\in [-\\infty, 1]$. We propose algorithms to compute such portfolios and provide theoretical guarantees on the trade-offs among approximation factor, portfolio size, and computational efficiency. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of our approach in summarizing the policy space induced by varying $p$ values, empowering decision-makers to navigate this landscape more effectively.         ",
    "url": "https://arxiv.org/abs/2502.09724",
    "authors": [
      "Cheol Woo Kim",
      "Jai Moondra",
      "Shresth Verma",
      "Madeleine Pollack",
      "Lingkai Kong",
      "Milind Tambe",
      "Swati Gupta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.16075",
    "title": "Implicit Bias of Gradient Descent for Non-Homogeneous Deep Networks",
    "abstract": "           We establish the asymptotic implicit bias of gradient descent (GD) for generic non-homogeneous deep networks under exponential loss. Specifically, we characterize three key properties of GD iterates starting from a sufficiently small empirical risk, where the threshold is determined by a measure of the network's non-homogeneity. First, we show that a normalized margin induced by the GD iterates increases nearly monotonically. Second, we prove that while the norm of the GD iterates diverges to infinity, the iterates themselves converge in direction. Finally, we establish that this directional limit satisfies the Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem. Prior works on implicit bias have focused exclusively on homogeneous networks; in contrast, our results apply to a broad class of non-homogeneous networks satisfying a mild near-homogeneity condition. In particular, our results apply to networks with residual connections and non-homogeneous activation functions, thereby resolving an open problem posed by Ji and Telgarsky (2020).         ",
    "url": "https://arxiv.org/abs/2502.16075",
    "authors": [
      "Yuhang Cai",
      "Kangjie Zhou",
      "Jingfeng Wu",
      "Song Mei",
      "Michael Lindsey",
      "Peter L. Bartlett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.24144",
    "title": "Graph Local Complementation is Inherently Sequential",
    "abstract": "           Local complementation of a graph $G$ on vertex $v$ is an operation that results in a new graph $G*v$, where the neighborhood of $v$ is complemented. This operation has been widely studied in graph theory and quantum computing. This article introduces the Local Complementation Problem, a decision problem that captures the complexity of applying a sequence of local complementations. Given a graph $G$, a sequence of vertices $s$, and a pair of vertices $u,v$, the problem asks whether the edge $(u,v)$ is present in the graph obtained after applying local complementations according to $s$. The main contribution of this work is proving that this problem is $\\mathsf{P}$-complete, implying that computing a sequence of local complementation is unlikely to be efficiently parallelizable. The proof is based on a reduction from the Circuit Value Problem, a well-known $\\mathsf{P}$-complete problem, by simulating circuits through local complementations. Additionally, the complexity of this problem is analyzed under different restrictions. In particular, it is shown that for complete and star graphs, the problem belongs to $\\mathsf{LOGSPACE}$. Finally, it is conjectured that the problem remains $\\mathsf{P}$-complete for the class of circle graphs.         ",
    "url": "https://arxiv.org/abs/2503.24144",
    "authors": [
      "Pablo Concha-Vega"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2504.03865",
    "title": "Towards an Optimal Bound for the Interleaving Distance on Mapper Graphs",
    "abstract": "           Mapper graphs are a widely used tool in topological data analysis and visualization. They can be viewed as discrete approximations of Reeb graphs, offering insight into the shape and connectivity of complex data. Given a high-dimensional point cloud $\\mathbb{X}$ equipped with a function $f: \\mathbb{X} \\to \\mathbb{R}$, a mapper graph provides a summary of the topological structure of $\\mathbb{X}$ induced by $f$, where each node represents a local neighborhood, and edges connect nodes whose corresponding neighborhoods overlap. Our focus is the interleaving distance for mapper graphs, arising from a discretization of the version for Reeb graphs, which is NP-hard to compute. This distance quantifies the similarity between two mapper graphs by measuring the extent to which they must be ``stretched\" to become comparable. Recent work introduced a loss function that provides an upper bound on the interleaving distance for mapper graphs, which evaluates how far a given assignment is from being a true interleaving. Finding the loss is computationally tractable, offering a practical way to estimate the distance. In this paper, we employ a categorical formulation of mapper graphs and develop the first framework for computing the associated loss function. Since the quality of the bound depends on the chosen assignment, we optimize this loss function by formulating the problem of finding the best assignment as an integer linear programming problem. To evaluate the effectiveness of our optimization, we apply it to small mapper graphs where the interleaving distance is known, demonstrating that the optimized upper bound successfully matches the interleaving distance in these cases. Additionally, we conduct an experiment on the MPEG-7 dataset, computing the pairwise optimal loss on a collection of mapper graphs derived from images and leveraging the distance bound for image classification.         ",
    "url": "https://arxiv.org/abs/2504.03865",
    "authors": [
      "Erin Wolf Chambers",
      "Ishika Ghosh",
      "Elizabeth Munch",
      "Sarah Percival",
      "Bei Wang"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2505.14635",
    "title": "Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning",
    "abstract": "           We present the first theoretical framework that connects predictive coding (PC), a biologically inspired local learning rule, with the minimum description length (MDL) principle in deep networks. We prove that layerwise PC performs block-coordinate descent on the MDL two-part code objective, thereby jointly minimizing empirical risk and model complexity. Using Hoeffding's inequality and a prefix-code prior, we derive a novel generalization bound of the form $R(\\theta) \\le \\hat{R}(\\theta) + \\frac{L(\\theta)}{N}$, capturing the tradeoff between fit and compression. We further prove that each PC sweep monotonically decreases the empirical two-part codelength, yielding tighter high-probability risk bounds than unconstrained gradient descent. Finally, we show that repeated PC updates converge to a block-coordinate stationary point, providing an approximate MDL-optimal solution. To our knowledge, this is the first result offering formal generalization and convergence guarantees for PC-trained deep models, positioning PC as a theoretically grounded and biologically plausible alternative to backpropagation.         ",
    "url": "https://arxiv.org/abs/2505.14635",
    "authors": [
      "Benjamin Prada",
      "Shion Matsumoto",
      "Abdul Malik Zekri",
      "Ankur Mali"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.00713",
    "title": "AKReF: An argumentative knowledge representation framework for structured argumentation",
    "abstract": "           This paper presents a framework to convert argumentative texts into argument knowledge graphs (AKG). The proposed argumentative knowledge representation framework (AKReF) extends the theoretical foundation and enables the AKG to provide a graphical view of the argumentative structure that is easier to understand. Starting with basic annotations of argumentative components (ACs) and argumentative relations (ARs), we enrich the information by constructing a knowledge base (KB) graph with metadata attributes for nodes. Next, we apply modus ponens on premises and inference rules from the KB to form arguments. From these arguments, we create an AKG. The nodes and edges of the AKG have attributes capturing key argumentative features such as the type of premise (e.g., axiom, ordinary premise, assumption), the type of inference rule (e.g., strict, defeasible), preference order over defeasible rules, markers (e.g., \"therefore\", \"however\"), and the type of attack (e.g., undercut, rebuttal, undermining). We identify inference rules by locating a specific set of markers, called inference markers (IM). This, in turn, makes it possible to identify undercut attacks previously undetectable in existing datasets. AKG prepares the ground for reasoning tasks, including checking the coherence of arguments and identifying opportunities for revision. For this, it is essential to find indirect relations, many of which are implicit. Our proposed AKG format, with annotated inference rules and modus ponens, helps reasoning models learn the implicit, indirect relations that require inference over arguments and their interconnections. We use an essay from the AAEC dataset to illustrate the framework. We further show its application in complex analyses such as extracting a conflict-free set and a maximal set of admissible arguments.         ",
    "url": "https://arxiv.org/abs/2506.00713",
    "authors": [
      "Debarati Bhattacharjee",
      "Ashish Anand"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.05566",
    "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation",
    "abstract": "           Recent advances in large language models (LLMs) have enabled near-human performance on software coding benchmarks, but their effectiveness in RTL code generation remains limited due to the scarcity of high-quality training data. While prior efforts have fine-tuned LLMs for RTL tasks, they do not fundamentally overcome the data bottleneck and lack support for test-time scaling due to their non-reasoning nature. In this work, we introduce ScaleRTL, the first reasoning LLM for RTL coding that scales up both high-quality reasoning data and test-time compute. Specifically, we curate a diverse set of long chain-of-thought reasoning traces averaging 56K tokens each, resulting in a dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a general-purpose reasoning model on this corpus yields ScaleRTL that is capable of deep RTL reasoning. Subsequently, we further enhance the performance of ScaleRTL through a novel test-time scaling strategy that extends the reasoning process via iteratively reflecting on and self-correcting previous reasoning steps. Experimental results show that ScaleRTL achieves state-of-the-art performance on VerilogEval and RTLLM, outperforming 18 competitive baselines by up to 18.4% on VerilogEval and 12.7% on RTLLM.         ",
    "url": "https://arxiv.org/abs/2506.05566",
    "authors": [
      "Chenhui Deng",
      "Yun-Da Tsai",
      "Guan-Ting Liu",
      "Zhongzhi Yu",
      "Haoxing Ren"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.06852",
    "title": "Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation",
    "abstract": "           Semantic segmentation of satellite imagery is crucial for Earth observation applications, but remains constrained by limited labelled training data. While self-supervised pretraining methods like Masked Autoencoders (MAE) have shown promise, they focus on reconstruction rather than localisation-a fundamental aspect of segmentation tasks. We propose adapting LOCA (Location-aware), a position prediction self-supervised learning method, for multimodal satellite imagery semantic segmentation. Our approach addresses the unique challenges of satellite data by extending SatMAE's channel grouping from multispectral to multimodal data, enabling effective handling of multiple modalities, and introducing same-group attention masking to encourage cross-modal interaction during pretraining. The method uses relative patch position prediction, encouraging spatial reasoning for localisation rather than reconstruction. We evaluate our approach on the Sen1Floods11 flood mapping dataset, where it significantly outperforms existing reconstruction-based self-supervised learning methods for satellite imagery. Our results demonstrate that position prediction tasks, when properly adapted for multimodal satellite imagery, learn representations more effective for satellite image semantic segmentation than reconstruction-based approaches.         ",
    "url": "https://arxiv.org/abs/2506.06852",
    "authors": [
      "John Waithaka",
      "Moise Busogi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.22196",
    "title": "Scott's Representation Theorem and the Univalent Karoubi Envelope",
    "abstract": "           Lambek and Scott constructed a correspondence between simply-typed lambda calculi and Cartesian closed categories. Scott's Representation Theorem is a cousin to this result for untyped lambda calculi. It states that every untyped lambda calculus arises from a reflexive object in some category. We present a formalization of Scott's Representation Theorem in univalent foundations, in the (Rocq-)UniMath library. Specifically, we implement two proofs of that theorem, one by Scott and one by Hyland. We also explain the role of the Karoubi envelope -- a categorical construction -- in the proofs and the impact the chosen foundation has on this construction. Finally, we report on some automation we have implemented for the reduction of $\\lambda$-terms.         ",
    "url": "https://arxiv.org/abs/2506.22196",
    "authors": [
      "Arnoud van der Leer",
      "Kobe Wullaert",
      "Benedikt Ahrens"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)"
    ]
  },
  {
    "id": "arXiv:2506.23603",
    "title": "SoK: Semantic Privacy in Large Language Models",
    "abstract": "           As Large Language Models (LLMs) are increasingly deployed in sensitive domains, traditional data privacy measures prove inadequate for protecting information that is implicit, contextual, or inferable - what we define as semantic privacy. This Systematization of Knowledge (SoK) introduces a lifecycle-centric framework to analyze how semantic privacy risks emerge across input processing, pretraining, fine-tuning, and alignment stages of LLMs. We categorize key attack vectors and assess how current defenses, such as differential privacy, embedding encryption, edge computing, and unlearning, address these threats. Our analysis reveals critical gaps in semantic-level protection, especially against contextual inference and latent representation leakage. We conclude by outlining open challenges, including quantifying semantic leakage, protecting multimodal inputs, balancing de-identification with generation quality, and ensuring transparency in privacy enforcement. This work aims to inform future research on designing robust, semantically aware privacy-preserving techniques for LLMs.         ",
    "url": "https://arxiv.org/abs/2506.23603",
    "authors": [
      "Baihe Ma",
      "Yanna Jiang",
      "Xu Wang",
      "Guangsheng Yu",
      "Qin Wang",
      "Caijun Sun",
      "Chen Li",
      "Xuelei Qi",
      "Ying He",
      "Wei Ni",
      "Ren Ping Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.03560",
    "title": "Simplifying Graph Kernels for Efficient",
    "abstract": "           While kernel methods and Graph Neural Networks offer complementary strengths, integrating the two has posed challenges in efficiency and scalability. The Graph Neural Tangent Kernel provides a theoretical bridge by interpreting GNNs through the lens of neural tangent kernels. However, its reliance on deep, stacked layers introduces repeated computations that hinder performance. In this work, we introduce a new perspective by designing the simplified graph kernel, which replaces deep layer stacking with a streamlined $K$-step message aggregation process. This formulation avoids iterative layer-wise propagation altogether, leading to a more concise and computationally efficient framework without sacrificing the expressive power needed for graph tasks. Beyond this simplification, we propose another Simplified Graph Kernel, which draws from Gaussian Process theory to model infinite-width GNNs. Rather than simulating network depth, this kernel analytically computes kernel values based on the statistical behavior of nonlinear activations in the infinite limit. This eliminates the need for explicit architecture simulation, further reducing complexity. Our experiments on standard graph and node classification benchmarks show that our methods achieve competitive accuracy while reducing runtime. This makes them practical alternatives for learning on graphs at scale. Full implementation and reproducibility materials are provided at: this https URL.         ",
    "url": "https://arxiv.org/abs/2507.03560",
    "authors": [
      "Lin Wang",
      "Shijie Wang",
      "Sirui Huang",
      "Qing Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.03564",
    "title": "2.5D Object Detection for Intelligent Roadside Infrastructure",
    "abstract": "           On-board sensors of autonomous vehicles can be obstructed, occluded, or limited by restricted fields of view, complicating downstream driving decisions. Intelligent roadside infrastructure perception systems, installed at elevated vantage points, can provide wide, unobstructed intersection coverage, supplying a complementary information stream to autonomous vehicles via vehicle-to-everything (V2X) communication. However, conventional 3D object-detection algorithms struggle to generalize under the domain shift introduced by top-down perspectives and steep camera angles. We introduce a 2.5D object detection framework, tailored specifically for infrastructure roadside-mounted cameras. Unlike conventional 2D or 3D object detection, we employ a prediction approach to detect ground planes of vehicles as parallelograms in the image frame. The parallelogram preserves the planar position, size, and orientation of objects while omitting their height, which is unnecessary for most downstream applications. For training, a mix of real-world and synthetically generated scenes is leveraged. We evaluate generalizability on a held-out camera viewpoint and in adverse-weather scenarios absent from the training set. Our results show high detection accuracy, strong cross-viewpoint generalization, and robustness to diverse lighting and weather conditions. Model weights and inference code are provided at: this https URL ",
    "url": "https://arxiv.org/abs/2507.03564",
    "authors": [
      "Nikolai Polley",
      "Yacin Boualili",
      "Ferdinand M\u00fctsch",
      "Maximilian Zipfl",
      "Tobias Fleck",
      "J. Marius Z\u00f6llner"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.03976",
    "title": "Robust Low-light Scene Restoration via Illumination Transition",
    "abstract": "           Synthesizing normal-light novel views from low-light multiview images is an important yet challenging task, given the low visibility and high ISO noise present in the input images. Existing low-light enhancement methods often struggle to effectively preprocess such low-light inputs, as they fail to consider correlations among multiple views. Although other state-of-the-art methods have introduced illumination-related components offering alternative solutions to the problem, they often result in drawbacks such as color distortions and artifacts, and they provide limited denoising effectiveness. In this paper, we propose a novel Robust Low-light Scene Restoration framework (RoSe), which enables effective synthesis of novel views in normal lighting conditions from low-light multiview image inputs, by formulating the task as an illuminance transition estimation problem in 3D space, conceptualizing it as a specialized rendering task. This multiview-consistent illuminance transition field establishes a robust connection between low-light and normal-light conditions. By further exploiting the inherent low-rank property of illumination to constrain the transition representation, we achieve more effective denoising without complex 2D techniques or explicit noise modeling. To implement RoSe, we design a concise dual-branch architecture and introduce a low-rank denoising module. Experiments demonstrate that RoSe significantly outperforms state-of-the-art models in both rendering quality and multiview consistency on standard benchmarks. The codes and data are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.03976",
    "authors": [
      "Ze Li",
      "Feng Zhang",
      "Xiatian Zhu",
      "Meng Zhang",
      "Yanghong Zhou",
      "P. Y. Mok"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.08492",
    "title": "Dual Dimensions Geometric Representation Learning Based Document Dewarping",
    "abstract": "           Document image dewarping remains a challenging task in the deep learning era. While existing methods have improved by leveraging text line awareness, they typically focus only on a single horizontal dimension. In this paper, we propose a fine-grained deformation perception model that focuses on Dual Dimensions of document horizontal-vertical-lines to improve document Dewarping called D2Dewarp. It can perceive distortion trends in different directions across document details. To combine the horizontal and vertical granularity features, an effective fusion module based on X and Y coordinate is designed to facilitate interaction and constraint between the two dimensions for feature complementarity. Due to the lack of annotated line features in current public dewarping datasets, we also propose an automatic fine-grained annotation method using public document texture images and an automatic rendering engine to build a new large-scale distortion training dataset. The code and dataset will be publicly released. On public Chinese and English benchmarks, both quantitative and qualitative results show that our method achieves better rectification results compared with the state-of-the-art methods. The dataset will be publicly available at this https URL ",
    "url": "https://arxiv.org/abs/2507.08492",
    "authors": [
      "Heng Li",
      "Qingcai Chen",
      "Xiangping Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.08677",
    "title": "Qualitative Assessment of Low Power Wide Area Network Protocols and their Security Aspect",
    "abstract": "           There are currently many communication options in the Internet of Things, even in particular areas such as constrained and battery-powered devices, such as Low Power Wide Area Networks. Understanding the differences and characteristics of each option is a challenge, even for professionals and researchers in the field. To meet this need, this work analyses the qualitative characteristics of Low Power Wide Area Network protocols and the challenges and opportunities of using constrained devices for sparse networks based on long-life batteries. For this study, a bibliographic survey of the literature was carried out as an analysis of three protocols (LoRaWAN, NB-IoT, and Sigfox), and a detailing of the first one. As a result, there is a discussion about the chosen network protocol and its use in IoT solutions with sparse sensors.         ",
    "url": "https://arxiv.org/abs/2507.08677",
    "authors": [
      "Wesley dos Reis Bezerra",
      "Lais Machado Bezerra",
      "Carlos Becker Westphall"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2507.10578",
    "title": "When and Where do Data Poisons Attack Textual Inversion?",
    "abstract": "           Poisoning attacks pose significant challenges to the robustness of diffusion models (DMs). In this paper, we systematically analyze when and where poisoning attacks textual inversion (TI), a widely used personalization technique for DMs. We first introduce Semantic Sensitivity Maps, a novel method for visualizing the influence of poisoning on text embeddings. Second, we identify and experimentally verify that DMs exhibit non-uniform learning behavior across timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias and inject adversarial signals predominantly at lower timesteps. Lastly, we observe that adversarial signals distract learning away from relevant concept regions within training data, corrupting the TI process. Based on these insights, we propose Safe-Zone Training (SZT), a novel defense mechanism comprised of 3 key components: (1) JPEG compression to weaken high-frequency poison signals, (2) restriction to high timesteps during TI training to avoid adversarial signals at lower timesteps, and (3) loss masking to constrain learning to relevant regions. Extensive experiments across multiple poisoning methods demonstrate that SZT greatly enhances the robustness of TI against all poisoning attacks, improving generative quality beyond prior published defenses. Code: this http URL Data: this http URL ",
    "url": "https://arxiv.org/abs/2507.10578",
    "authors": [
      "Jeremy Styborski",
      "Mingzhi Lyu",
      "Jiayou Lu",
      "Nupur Kapur",
      "Adams Kong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.10678",
    "title": "A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks",
    "abstract": "           A major challenge in the use of neural networks both for modeling human cognitive function and for artificial intelligence is the design of systems with the capacity to efficiently learn functions that support radical generalization. At the roots of this is the capacity to discover and implement symmetry functions. In this paper, we investigate a paradigmatic example of radical generalization through the use of symmetry: base addition. We present a group theoretic analysis of base addition, a fundamental and defining characteristic of which is the carry function -- the transfer of the remainder, when a sum exceeds the base modulus, to the next significant place. Our analysis exposes a range of alternative carry functions for a given base, and we introduce quantitative measures to characterize these. We then exploit differences in carry functions to probe the inductive biases of neural networks in symmetry learning, by training neural networks to carry out base addition using different carries, and comparing efficacy and rate of learning as a function of their structure. We find that even simple neural networks can achieve radical generalization with the right input format and carry function, and that learnability is closely correlated with carry function structure. We then discuss the relevance this has for cognitive science and machine learning.         ",
    "url": "https://arxiv.org/abs/2507.10678",
    "authors": [
      "Cutter Dawes",
      "Simon Segert",
      "Kamesh Krishnamurthy",
      "Jonathan D. Cohen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2507.10786",
    "title": "\"Is it always watching? Is it always listening?\" Exploring Contextual Privacy and Security Concerns Toward Domestic Social Robots",
    "abstract": "           Equipped with artificial intelligence (AI) and advanced sensing capabilities, social robots are gaining interest among consumers in the United States. These robots seem like a natural evolution of traditional smart home devices. However, their extensive data collection capabilities, anthropomorphic features, and capacity to interact with their environment make social robots a more significant security and privacy threat. Increased risks include data linkage, unauthorized data sharing, and the physical safety of users and their homes. It is critical to investigate U.S. users' security and privacy needs and concerns to guide the design of social robots while these devices are still in the early stages of commercialization in the U.S. market. Through 19 semi-structured interviews, we identified significant security and privacy concerns, highlighting the need for transparency, usability, and robust privacy controls to support adoption. For educational applications, participants worried most about misinformation, and in medical use cases, they worried about the reliability of these devices. Participants were also concerned with the data inference that social robots could enable. We found that participants expect tangible privacy controls, indicators of data collection, and context-appropriate functionality.         ",
    "url": "https://arxiv.org/abs/2507.10786",
    "authors": [
      "Henry Bell",
      "Jabari Kwesi",
      "Hiba Laabadli",
      "Pardis Emami-Naeini"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Emerging Technologies (cs.ET)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2507.11049",
    "title": "Journalism-Guided Agentic In-Context Learning for News Stance Detection",
    "abstract": "           As online news consumption grows, personalized recommendation systems have become integral to digital journalism. However, these systems risk reinforcing filter bubbles and political polarization by failing to incorporate diverse perspectives. Stance detection -- identifying a text's position on a target -- can help mitigate this by enabling viewpoint-aware recommendations and data-driven analyses of media bias. Yet, existing stance detection research remains largely limited to short texts and high-resource languages. To address these gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for article-level stance detection, comprising 2,000 news articles with article-level and 19,650 segment-level stance annotations across 47 societal issues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided \\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that employs a language model agent to predict the stances of key structural segments (e.g., leads, quotes), which are then aggregated to infer the overall article stance. Experiments show that \\textsc{JoA-ICL} outperforms existing stance detection methods, highlighting the benefits of segment-level agency in capturing the overall position of long-form news articles. Two case studies further demonstrate its broader utility in promoting viewpoint diversity in news recommendations and uncovering patterns of media bias.         ",
    "url": "https://arxiv.org/abs/2507.11049",
    "authors": [
      "Dahyun Lee",
      "Jonghyeon Choi",
      "Jiyoung Han",
      "Kunwoo Park"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.11439",
    "title": "Data Augmentation in Time Series Forecasting through Inverted Framework",
    "abstract": "           Currently, iTransformer is one of the most popular and effective models for multivariate time series (MTS) forecasting. Thanks to its inverted framework, iTransformer effectively captures multivariate correlation. However, the inverted framework still has some limitations. It diminishes temporal interdependency information, and introduces noise in cases of nonsignificant variable correlation. To address these limitations, we introduce a novel data augmentation method on inverted framework, called DAIF. Unlike previous data augmentation methods, DAIF stands out as the first real-time augmentation specifically designed for the inverted framework in MTS forecasting. We first define the structure of the inverted sequence-to-sequence framework, then propose two different DAIF strategies, Frequency Filtering and Cross-variation Patching to address the existing challenges of the inverted framework. Experiments across multiple datasets and inverted models have demonstrated the effectiveness of our DAIF.         ",
    "url": "https://arxiv.org/abs/2507.11439",
    "authors": [
      "Hongming Tan",
      "Ting Chen",
      "Ruochong Jin",
      "Wai Kin Chan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:1809.08467",
    "title": "On Bivariegated Graphs and Line Graphs",
    "abstract": "           This note is on the structures of line graphs and 2-variegated graphs. We have given here solutions of some graph equations involving line graphs and 2-variegated graphs. In addition, a characterization of potentially 2-variegated line graphic degree sequences is given.         ",
    "url": "https://arxiv.org/abs/1809.08467",
    "authors": [
      "Ranjan N. Naik"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2309.10509",
    "title": "Polynomial-time Solver of Tridiagonal QUBO, QUDO and Tensor QUDO problems with Tensor Networks",
    "abstract": "           We present a quantum-inspired tensor network algorithm for solving tridiagonal Quadratic Unconstrained Binary Optimization (QUBO) problems and quadratic unconstrained discrete optimization (QUDO) problems. We also solve the more general Tensor quadratic unconstrained discrete optimization (T-QUDO) problems with one-neighbor interactions in a lineal chain. This method provides an exact and explicit equation for these problems. Our algorithms are based on the simulation of a state that undergoes imaginary time evolution and a Half partial trace. In addition, we address the degenerate case and evaluate the polynomial complexity of the algorithm, also providing a parallelized version. We implemented and tested them with other well-known classical algorithms and observed an improvement in the quality of the results. The performance of the proposed algorithms is compared with the Google OR-TOOLS and dimod solvers, improving their results.         ",
    "url": "https://arxiv.org/abs/2309.10509",
    "authors": [
      "Alejandro Mata Ali",
      "I\u00f1igo Perez Delgado",
      "Marina Ristol Roura",
      "Aitor Moreno Fdez. de Leceta"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2401.15801",
    "title": "On the Statistical Properties of Generative Adversarial Models for Low Intrinsic Data Dimension",
    "abstract": "           Despite the remarkable empirical successes of Generative Adversarial Networks (GANs), the theoretical guarantees for their statistical accuracy remain rather pessimistic. In particular, the data distributions on which GANs are applied, such as natural images, are often hypothesized to have an intrinsic low-dimensional structure in a typically high-dimensional feature space, but this is often not reflected in the derived rates in the state-of-the-art analyses. In this paper, we attempt to bridge the gap between the theory and practice of GANs and their bidirectional variant, Bi-directional GANs (BiGANs), by deriving statistical guarantees on the estimated densities in terms of the intrinsic dimension of the data and the latent space. We analytically show that if one has access to $n$ samples from the unknown target distribution and the network architectures are properly chosen, the expected Wasserstein-1 distance of the estimates from the target scales as $O\\left( n^{-1/d_\\mu } \\right)$ for GANs and $\\tilde{O}\\left( n^{-1/(d_\\mu+\\ell)} \\right)$ for BiGANs, where $d_\\mu$ and $\\ell$ are the upper Wasserstein-1 dimension of the data-distribution and latent-space dimension, respectively. The theoretical analyses not only suggest that these methods successfully avoid the curse of dimensionality, in the sense that the exponent of $n$ in the error rates does not depend on the data dimension but also serve to bridge the gap between the theoretical analyses of GANs and the known sharp rates from optimal transport literature. Additionally, we demonstrate that GANs can effectively achieve the minimax optimal rate even for non-smooth underlying distributions, with the use of interpolating generator networks.         ",
    "url": "https://arxiv.org/abs/2401.15801",
    "authors": [
      "Saptarshi Chakraborty",
      "Peter L. Bartlett"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2410.19704",
    "title": "Multi-view biomedical foundation models for molecule-target and property prediction",
    "abstract": "           Quality molecular representations are key to foundation model development in bio-medical research. Previous efforts have typically focused on a single representation or molecular view, which may have strengths or weaknesses on a given task. We develop Multi-view Molecular Embedding with Late Fusion (MMELON), an approach that integrates graph, image and text views in a foundation model setting and may be readily extended to additional representations. Single-view foundation models are each pre-trained on a dataset of up to 200M molecules. The multi-view model performs robustly, matching the performance of the highest-ranked single-view. It is validated on over 120 tasks, including molecular solubility, ADME properties, and activity against G Protein-Coupled receptors (GPCRs). We identify 33 GPCRs that are related to Alzheimer's disease and employ the multi-view model to select strong binders from a compound screen. Predictions are validated through structure-based modeling and identification of key binding motifs.         ",
    "url": "https://arxiv.org/abs/2410.19704",
    "authors": [
      "Parthasarathy Suryanarayanan",
      "Yunguang Qiu",
      "Shreyans Sethi",
      "Diwakar Mahajan",
      "Hongyang Li",
      "Yuxin Yang",
      "Elif Eyigoz",
      "Aldo Guzman Saenz",
      "Daniel E. Platt",
      "Timothy H. Rumbell",
      "Kenney Ng",
      "Sanjoy Dey",
      "Myson Burch",
      "Bum Chul Kwon",
      "Pablo Meyer",
      "Feixiong Cheng",
      "Jianying Hu",
      "Joseph A. Morrone"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.16425",
    "title": "Patherea: Cell Detection and Classification for the 2020s",
    "abstract": "           We present Patherea, a unified framework for point-based cell detection and classification that enables the development and fair evaluation of state-of-the-art methods. To support this, we introduce a large-scale dataset that replicates the clinical workflow for Ki-67 proliferation index estimation. Our method directly predicts cell locations and classes without relying on intermediate representations. It incorporates a hybrid Hungarian matching strategy for accurate point assignment and supports flexible backbones and training regimes, including recent pathology foundation models. Patherea achieves state-of-the-art performance on public datasets - Lizard, BRCA-M2C, and BCData - while highlighting performance saturation on these benchmarks. In contrast, our newly proposed Patherea dataset presents a significantly more challenging benchmark. Additionally, we identify and correct common errors in current evaluation protocols and provide an updated benchmarking utility for standardized assessment. The Patherea dataset and code are publicly available to facilitate further research and fair comparisons.         ",
    "url": "https://arxiv.org/abs/2412.16425",
    "authors": [
      "Dejan \u0160tepec",
      "Maja Jer\u0161e",
      "Sne\u017eana \u0110oki\u0107",
      "Jera Jeruc",
      "Nina Zidar",
      "Danijel Sko\u010daj"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.05590",
    "title": "Negative Ties Highlight Hidden Extremes in Social Media Polarization",
    "abstract": "           Human interactions in the online world comprise a combination of positive and negative exchanges. These diverse interactions can be captured using signed network representations, where edges take positive or negative weights to indicate the sentiment of the interaction between individuals. Signed networks offer valuable insights into online political polarization by capturing antagonistic interactions and ideological divides on social media platforms. This study analyzes polarization on Meneame, a Spanish social media platform that facilitates engagement with news stories through comments and voting. Using a dual-method approach, Signed Hamiltonian Eigenvector Embedding for Proximity (SHEEP) for signed networks and Correspondence Analysis (CA) for unsigned networks, we investigate how including negative ties enhances the understanding of structural polarization levels across different conversation topics on the platform. While the unsigned Meneame network effectively delineates ideological communities, only by incorporating negative ties can we identify ideologically extreme users who engage in antagonistic behaviors: without them, the most extreme users remain indistinguishable from their less confrontational ideological peers.         ",
    "url": "https://arxiv.org/abs/2501.05590",
    "authors": [
      "Elena Candellone",
      "Shazia'Ayn Babul",
      "\u00d6zg\u00fcr Togay",
      "Alexandre Bovet",
      "Javier Garcia-Bernardo"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2502.05676",
    "title": "Generalized Venn and Venn-Abers Calibration with Applications in Conformal Prediction",
    "abstract": "           Ensuring model calibration is critical for reliable prediction, yet popular distribution-free methods such as histogram binning and isotonic regression offer only asymptotic guarantees. We introduce a unified framework for Venn and Venn-Abers calibration that extends Vovk's approach beyond binary classification to a broad class of prediction problems defined by generic loss functions. Our method transforms any perfectly in-sample calibrated predictor into a set-valued predictor that, in finite samples, outputs at least one marginally calibrated point prediction. These set predictions shrink asymptotically and converge to a single conditionally calibrated prediction, capturing epistemic uncertainty. We further propose Venn multicalibration, a new approach for achieving finite-sample calibration across subpopulations. For quantile loss, our framework recovers group-conditional and multicalibrated conformal prediction as special cases and yields novel prediction intervals with quantile-conditional coverage.         ",
    "url": "https://arxiv.org/abs/2502.05676",
    "authors": [
      "Lars van der Laan",
      "Ahmed Alaa"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2502.15186",
    "title": "LUMINA-Net: Low-light Upgrade through Multi-stage Illumination and Noise Adaptation Network for Image Enhancement",
    "abstract": "           Low-light image enhancement (LLIE) is a crucial task in computer vision aimed at enhancing the visual fidelity of images captured under low-illumination conditions. Conventional methods frequently struggle with noise, overexposure, and color distortion, leading to significant image quality degradation. To address these challenges, we propose LUMINA-Net, an unsupervised deep learning framework that learns adaptive priors from low-light image pairs by integrating multi-stage illumination and reflectance modules. To assist the Retinex decomposition, inappropriate features in the raw image can be removed using a simple self-supervised mechanism. First, the illumination module intelligently adjusts brightness and contrast while preserving intricate textural details. Second, the reflectance module incorporates a noise reduction mechanism that leverages spatial attention and channel-wise feature refinement to mitigate noise contamination. Through extensive experiments on LOL and SICE datasets, evaluated using PSNR, SSIM, and LPIPS metrics, LUMINA-Net surpasses state-of-the-art methods, demonstrating its efficacy in low-light image enhancement.         ",
    "url": "https://arxiv.org/abs/2502.15186",
    "authors": [
      "Namrah Siddiqua",
      "Kim Suneung",
      "Seong-Whan Lee"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.01881",
    "title": "A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs",
    "abstract": "           Low-dose computed tomography (LDCT) imaging employed in lung cancer screening (LCS) programs is increasing in uptake worldwide. LCS programs herald a generational opportunity to simultaneously detect cancer and non-cancer-related early-stage lung disease. Yet these efforts are hampered by a shortage of radiologists to interpret scans at scale. Here, we present TANGERINE, a computationally frugal, open-source vision foundation model for volumetric LDCT analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can be fine-tuned off the shelf for a wide range of disease-specific tasks with limited computational resources and training data. Relative to models trained from scratch, TANGERINE demonstrates fast convergence during fine-tuning, thereby requiring significantly fewer GPU hours, and displays strong label efficiency, achieving comparable or superior performance with a fraction of fine-tuning data. Pretrained using self-supervised learning on over 98,000 thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public datasets, TANGERINE achieves state-of-the-art performance across 14 disease classification tasks, including lung cancer and multiple respiratory diseases, while generalising robustly across diverse clinical centres. By extending a masked autoencoder framework to 3D imaging, TANGERINE offers a scalable solution for LDCT analysis, departing from recent closed, resource-intensive models by combining architectural simplicity, public availability, and modest computational requirements. Its accessible, open-source lightweight design lays the foundation for rapid integration into next-generation medical imaging tools that could transform LCS initiatives, allowing them to pivot from a singular focus on lung cancer detection to comprehensive respiratory disease management in high-risk populations.         ",
    "url": "https://arxiv.org/abs/2507.01881",
    "authors": [
      "Niccol\u00f2 McConnell",
      "Pardeep Vasudev",
      "Daisuke Yamada",
      "Daryl Cheng",
      "Mehran Azimbagirad",
      "John McCabe",
      "Shahab Aslani",
      "Ahmed H. Shahin",
      "Yukun Zhou",
      "SUMMIT Consortium",
      "Andre Altmann",
      "Yipeng Hu",
      "Paul Taylor",
      "Sam M. Janes",
      "Daniel C. Alexander",
      "Joseph Jacob"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.06273",
    "title": "Magneto-radiative modelling and artificial neural network optimization of biofluid flow in a stenosed arterial domain",
    "abstract": "           The increasing complexity of cardiovascular diseases and limitations in traditional healing methods mandate the invention of new drug delivery systems that assure targeted, effective, and regulated treatments, contributing directly to UN SDGs 3 and 9, thereby encouraging the utilization of sustainable medical technologies in healthcare. This study investigates the flow of a Casson-Maxwell nanofluid through a stenosed arterial domain. The quantities, such as skin friction and heat transfer rate, are analysed in detail. The Casson-Maxwell fluid shows a lower velocity profile than the Casson fluids, which indicates the improved residence time for efficient drug delivery. The heat transfer rate shows an increase with higher volume fractions of copper and aluminium oxide nanoparticles and a decrease with higher volume fractions of silver nanoparticles. The skin friction coefficient decreases by 219% with a unit increase in the Maxwell parameter, whereas it increases by 66.1% with a unit rise in the Casson parameter. This work supports SDGs 4 and 17 by fostering interdisciplinary learning and collaboration in fluid dynamics and healthcare innovation. Additionally, the rate of heat flow was forecasted (with an overall R-value of 0.99457) using the Levenberg-Marquardt backpropagation training scheme under the influence of magneto-radiative, linear heat source and Casson-Maxwell parameters along with the tri-metallic nanoparticle volume fractions. It is also observed that the drag coefficient is most sensitive to the changes in the Maxwell parameter.         ",
    "url": "https://arxiv.org/abs/2507.06273",
    "authors": [
      "S P Shivakumar",
      "Gunisetty Ramasekhar",
      "P Nimmy",
      "Sujesh Areekara",
      "L Thanuja",
      "T V Smitha",
      "S Devanathan",
      "Ganesh R Naik",
      "K V Nagaraja"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)",
      "Biological Physics (physics.bio-ph)"
    ]
  },
  {
    "id": "arXiv:2507.11381",
    "title": "From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies",
    "abstract": "           We propose a framework for building patient-specific treatment recommendation models, building on the large recent literature on learning patient-level causal models and inspired by the target trial paradigm of Hernan and Robins. We focus on safety and validity, including the crucial issue of causal identification when using observational data. We do not provide a specific model, but rather a way to integrate existing methods and know-how into a practical pipeline. We further provide a real world use-case of treatment optimization for patients with heart failure who develop acute kidney injury during hospitalization. The results suggest our pipeline can improve patient outcomes over the current treatment regime.         ",
    "url": "https://arxiv.org/abs/2507.11381",
    "authors": [
      "Rom Gutman",
      "Shimon Sheiba",
      "Omer Noy Klein",
      "Naama Dekel Bird",
      "Amit Gruber",
      "Doron Aronson",
      "Oren Caspi",
      "Uri Shalit"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  }
]