[
  {
    "id": "arXiv:2510.25781",
    "title": "A Practitioner's Guide to Kolmogorov-Arnold Networks",
    "abstract": "           Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising alternative to traditional Multilayer Perceptrons (MLPs), inspired by the Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed activation functions on nodes, KANs employ learnable univariate basis functions on edges, offering enhanced expressivity and interpretability. This review provides a systematic and comprehensive overview of the rapidly expanding KAN landscape, moving beyond simple performance comparisons to offer a structured synthesis of theoretical foundations, architectural variants, and practical implementation strategies. By collecting and categorizing a vast array of open-source implementations, we map the vibrant ecosystem supporting KAN development. We begin by bridging the conceptual gap between KANs and MLPs, establishing their formal equivalence and highlighting the superior parameter efficiency of the KAN formulation. A central theme of our review is the critical role of the basis function; we survey a wide array of choices, including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions, Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in terms of smoothness, locality, and computational cost. We then categorize recent advancements into a clear roadmap, covering techniques for improving accuracy, efficiency, and regularization. Key topics include physics-informed loss design, adaptive sampling, domain decomposition, hybrid architectures, and specialized methods for handling discontinuities. Finally, we provide a practical \"Choose-Your-KAN\" guide to help practitioners select appropriate architectures, and we conclude by identifying current research gaps. The associated GitHub repository this https URL complements this paper and serves as a structured reference for ongoing KAN research.         ",
    "url": "https://arxiv.org/abs/2510.25781",
    "authors": [
      "Amir Noorizadegan",
      "Sifan Wang",
      "Leevan Ling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2510.25797",
    "title": "Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks",
    "abstract": "           This study examines the effectiveness of spatio-temporal modeling and the integration of spatial attention mechanisms in deep learning models for underwater object detection. Specifically, in the first phase, the performance of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is developed, through the addition of a Convolutional Block Attention Module (CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the research highlights how temporal modeling improves detection accuracy in dynamic marine environments, particularly under conditions of sudden movements, partial occlusions, and gradual motion. The testing results showed that YOLOv5 achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively, highlighting their superior accuracy and generalization in detecting complex objects. The findings demonstrate that T-YOLOv5 significantly enhances detection reliability compared to the standard model, while T-YOLOv5 with CBAM further improves performance in challenging scenarios, although there is a loss of accuracy when it comes to simpler scenarios.         ",
    "url": "https://arxiv.org/abs/2510.25797",
    "authors": [
      "Sai Likhith Karri",
      "Ansh Saxena"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2510.25800",
    "title": "FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series Tasks",
    "abstract": "           The inherent autocorrelation of time series data presents an ongoing challenge to multivariate time series prediction. Recently, a widely adopted approach has been the incorporation of frequency domain information to assist in long-term prediction tasks. Many researchers have independently observed the spectral bias phenomenon in neural networks, where models tend to fit low-frequency signals before high-frequency ones. However, these observations have often been attributed to the specific architectures designed by the researchers, rather than recognizing the phenomenon as a universal characteristic across models. To unify the understanding of the spectral bias phenomenon in long-term time series prediction, we conducted extensive empirical experiments to measure spectral bias in existing mainstream models. Our findings reveal that virtually all models exhibit this phenomenon. To mitigate the impact of spectral bias, we propose the FreLE (Frequency Loss Enhancement) algorithm, which enhances model generalization through both explicit and implicit frequency regularization. This is a plug-and-play model loss function unit. A large number of experiments have proven the superior performance of FreLE. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.25800",
    "authors": [
      "Jialong Sun",
      "Xinpeng Ling",
      "Jiaxuan Zou",
      "Jiawen Kang",
      "Kejia Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.25802",
    "title": "Attention Augmented GNN RNN-Attention Models for Advanced Cybersecurity Intrusion Detection",
    "abstract": "           In this paper, we propose a novel hybrid deep learning architecture that synergistically combines Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), and multi-head attention mechanisms to significantly enhance cy- bersecurity intrusion detection capabilities. By leveraging the comprehensive UNSW-NB15 dataset containing diverse network traffic patterns, our approach effectively captures both spatial dependencies through graph structural relationships and tem- poral dynamics through sequential analysis of network events. The integrated attention mechanism provides dual benefits of improved model interpretability and enhanced feature selection, enabling cybersecurity analysts to focus computational resources on high-impact security events - a critical requirement in modern real-time intrusion detection systems. Our extensive experimental evaluation demonstrates that the proposed hybrid model achieves superior performance compared to traditional machine learning approaches and standalone deep learning models across multiple evaluation metrics, including accuracy, precision, recall, and F1-score. The model achieves particularly strong performance in detecting sophisticated attack patterns such as Advanced Persistent Threats (APTs), Distributed Denial of Service (DDoS) attacks, and zero-day exploits, making it a promising solution for next-generation cybersecurity applications in complex network environments.         ",
    "url": "https://arxiv.org/abs/2510.25802",
    "authors": [
      "Jayant Biradar",
      "Smit Shah",
      "Tanmay Naik"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.25809",
    "title": "Flex-GAD : Flexible Graph Anomaly Detection",
    "abstract": "           Detecting anomalous nodes in attributed networks, where each node is associated with both structural connections and descriptive attributes, is essential for identifying fraud, misinformation, and suspicious behavior in domains such as social networks, academic citation graphs, and e-commerce platforms. We propose Flex-GAD, a novel unsupervised framework for graph anomaly detection at the node level. Flex-GAD integrates two encoders to capture complementary aspects of graph data. The framework incorporates a novel community-based GCN encoder to model intra-community and inter-community information into node embeddings, thereby ensuring structural consistency, along with a standard attribute encoder. These diverse representations are fused using a self-attention-based representation fusion module, which enables adaptive weighting and effective integration of the encoded information. This fusion mechanism allows automatic emphasis of the most relevant node representation across different encoders. We evaluate Flex-GAD on seven real-world attributed graphs with varying sizes, node degrees, and attribute homogeneity. Flex-GAD achieves an average AUC improvement of 7.98% over the previously best-performing method, GAD-NR, demonstrating its effectiveness and flexibility across diverse graph structures. Moreover, it significantly reduces training time, running 102x faster per epoch than Anomaly DAE and 3x faster per epoch than GAD-NR on average across seven benchmark datasets.         ",
    "url": "https://arxiv.org/abs/2510.25809",
    "authors": [
      "Apu Chakraborty",
      "Anshul Kumar",
      "Gagan Raj Gupta"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.25810",
    "title": "Adversarial Pre-Padding: Generating Evasive Network Traffic Against Transformer-Based Classifiers",
    "abstract": "           To date, traffic obfuscation techniques have been widely adopted to protect network data privacy and security by obscuring the true patterns of traffic. Nevertheless, as the pre-trained models emerge, especially transformer-based classifiers, existing traffic obfuscation methods become increasingly vulnerable, as witnessed by current studies reporting the traffic classification accuracy up to 99\\% or higher. To counter such high-performance transformer-based classification models, we in this paper propose a novel and effective \\underline{adv}ersarial \\underline{traffic}-generating approach (AdvTraffic\\footnote{The code and data are available at: http://xxx}). Our approach has two key innovations: (i) a pre-padding strategy is proposed to modify packets, which effectively overcomes the limitations of existing research against transformer-based models for network traffic classification; and (ii) a reinforcement learning model is employed to optimize network traffic perturbations, aiming to maximize adversarial effectiveness against transformer-based classification models. To the best of our knowledge, this is the first attempt to apply adversarial perturbation techniques to defend against transformer-based traffic classifiers. Furthermore, our method can be easily deployed into practical network environments. Finally, multi-faceted experiments are conducted across several real-world datasets, and the experimental results demonstrate that our proposed method can effectively undermine transformer-based classifiers, significantly reducing classification accuracy from 99\\% to as low as 25.68\\%.         ",
    "url": "https://arxiv.org/abs/2510.25810",
    "authors": [
      "Quanliang Jing",
      "Xinxin Fan",
      "Yanyan Liu",
      "Jingping Bi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2510.25892",
    "title": "Topology-Aware Active Learning on Graphs",
    "abstract": "           We propose a graph-topological approach to active learning that directly targets the core challenge of exploration versus exploitation under scarce label budgets. To guide exploration, we introduce a coreset construction algorithm based on Balanced Forman Curvature (BFC), which selects representative initial labels that reflect the graph's cluster structure. This method includes a data-driven stopping criterion that signals when the graph has been sufficiently explored. We further use BFC to dynamically trigger the shift from exploration to exploitation within active learning routines, replacing hand-tuned heuristics. To improve exploitation, we introduce a localized graph rewiring strategy that efficiently incorporates multiscale information around labeled nodes, enhancing label propagation while preserving sparsity. Experiments on benchmark classification tasks show that our methods consistently outperform existing graph-based semi-supervised baselines at low label rates.         ",
    "url": "https://arxiv.org/abs/2510.25892",
    "authors": [
      "Harris Hardiman-Mostow",
      "Jack Mauro",
      "Adrien Weihs",
      "Andrea L. Bertozzi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.25917",
    "title": "Coherence-Aware Distributed Learning under Heterogeneous Downlink Impairments",
    "abstract": "           The performance of federated learning (FL) over wireless networks critically depends on accurate and timely channel state information (CSI) across distributed devices. This requirement is tightly linked to how rapidly the channel gains vary, i.e., the coherence intervals. In practice, edge devices often exhibit unequal coherence times due to differences in mobility and scattering environments, leading to unequal demands for pilot signaling and channel estimation resources. Conventional FL schemes that overlook this coherence disparity can suffer from severe communication inefficiencies and training overhead. This paper proposes a coherence-aware, communication-efficient framework for joint channel training and model updating in practical wireless FL systems operating under heterogeneous fading dynamics. Focusing on downlink impairments, we introduce a resource-reuse strategy based on product superposition, enabling the parameter server to efficiently schedule both static and dynamic devices by embedding global model updates for static devices within pilot transmissions intended for mobile devices. We theoretically analyze the convergence behavior of the proposed scheme and quantify its gains in expected communication efficiency and training accuracy. Experiments demonstrate the effectiveness of the proposed framework under mobility-induced dynamics and offer useful insights for the practical deployment of FL over wireless channels.         ",
    "url": "https://arxiv.org/abs/2510.25917",
    "authors": [
      "Mehdi Karbalayghareh",
      "David J. Love",
      "Christopher G. Brinton"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2510.25924",
    "title": "Transferring Causal Effects using Proxies",
    "abstract": "           We consider the problem of estimating a causal effect in a multi-domain setting. The causal effect of interest is confounded by an unobserved confounder and can change between the different domains. We assume that we have access to a proxy of the hidden confounder and that all variables are discrete or categorical. We propose methodology to estimate the causal effect in the target domain, where we assume to observe only the proxy variable. Under these conditions, we prove identifiability (even when treatment and response variables are continuous). We introduce two estimation techniques, prove consistency, and derive confidence intervals. The theoretical results are supported by simulation studies and a real-world example studying the causal effect of website rankings on consumer choices.         ",
    "url": "https://arxiv.org/abs/2510.25924",
    "authors": [
      "Manuel Iglesias-Alonso",
      "Felix Schur",
      "Julius von K\u00fcgelgen",
      "Jonas Peters"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2510.25932",
    "title": "FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X",
    "abstract": "           Social platforms distribute information at unprecedented speed, which in turn accelerates the spread of misinformation and threatens public discourse. We present FakeZero, a fully client-side, cross-platform browser extension that flags unreliable posts on Facebook and X (formerly Twitter) while the user scrolls. All computation, DOM scraping, tokenisation, Transformer inference, and UI rendering run locally through the Chromium messaging API, so no personal data leaves the this http URL employs a three-stage training curriculum: baseline fine-tuning and domain-adaptive training enhanced with focal loss, adversarial augmentation, and post-training quantisation. Evaluated on a dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1% macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to 14.7 MB and lowering latency to approximately 40 ms, showing that high-quality fake-news detection is feasible under tight resource budgets with only modest performance this http URL providing inline credibility cues, the extension can serve as a valuable tool for policymakers seeking to curb the spread of misinformation across social networks. With user consent, FakeZero also opens the door for researchers to collect large-scale datasets of fake news in the wild, enabling deeper analysis and the development of more robust detection techniques.         ",
    "url": "https://arxiv.org/abs/2510.25932",
    "authors": [
      "Soufiane Essahli",
      "Oussama Sarsar",
      "Imane Fouad",
      "Anas Motii",
      "Ahmed Bentajer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2510.25934",
    "title": "Robust GNN Watermarking via Implicit Perception of Topological Invariants",
    "abstract": "           Graph Neural Networks (GNNs) are valuable intellectual property, yet many watermarks rely on backdoor triggers that break under common model edits and create ownership ambiguity. We present InvGNN-WM, which ties ownership to a model's implicit perception of a graph invariant, enabling trigger-free, black-box verification with negligible task impact. A lightweight head predicts normalized algebraic connectivity on an owner-private carrier set; a sign-sensitive decoder outputs bits, and a calibrated threshold controls the false-positive rate. Across diverse node and graph classification datasets and backbones, InvGNN-WM matches clean accuracy while yielding higher watermark accuracy than trigger- and compression-based baselines. It remains strong under unstructured pruning, fine-tuning, and post-training quantization; plain knowledge distillation (KD) weakens the mark, while KD with a watermark loss (KD+WM) restores it. We provide guarantees for imperceptibility and robustness, and we prove that exact removal is NP-complete.         ",
    "url": "https://arxiv.org/abs/2510.25934",
    "authors": [
      "Jipeng Li",
      "Yannning Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2510.25935",
    "title": "A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows",
    "abstract": "           CodeSight is an end-to-end system designed to anticipate deadline compliance in software development workflows. It captures development and deployment data directly from GitHub, transforming it into process mining logs for detailed analysis. From these logs, the system generates metrics and dashboards that provide actionable insights into PR activity patterns and workflow efficiency. Building on this structured representation, CodeSight employs an LSTM model that predicts remaining PR resolution times based on sequential activity traces and static features, enabling early identification of potential deadline breaches. In tests, the system demonstrates high precision and F1 scores in predicting deadline compliance, illustrating the value of integrating process mining with machine learning for proactive software project management.         ",
    "url": "https://arxiv.org/abs/2510.25935",
    "authors": [
      "Ant\u00eda Dorado",
      "Iv\u00e1n Folgueira",
      "Sof\u00eda Mart\u00edn",
      "Gonzalo Mart\u00edn",
      "\u00c1lvaro Porto",
      "Alejandro Ramos",
      "John Wallace"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.25954",
    "title": "Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi",
    "abstract": "           The reliability of routine health data in low and middle-income countries (LMICs) is often constrained by reporting delays and incomplete coverage, necessitating the exploration of novel data sources and analytics. Geospatial Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse spatial, temporal, and behavioral data into mathematical embeddings that can be efficiently used for downstream prediction tasks. This study evaluated the predictive performance of three GeoFM embedding sources - Google Population Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite imagery), and mobile phone call detail records (CDR) - for modeling 15 routine health programmatic outputs in Malawi, and compared their utility to traditional geospatial interpolation methods. We used XGBoost models on data from 552 health catchment areas (January 2021-May 2023), assessing performance with R2, and using an 80/20 training and test data split with 5-fold cross-validation used in training. While predictive performance was mixed, the embedding-based approaches improved upon baseline geostatistical methods in 13 of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three embedding sources produced the most robust predictions, achieving average 5-fold cross validated R2 values for indicators like population density (0.63), new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64, 0.68, and 0.55, respectively. Prediction was poor for prediction targets with low primary data availability, such as TB and malnutrition cases. These results demonstrate that GeoFM embeddings imbue a modest predictive improvement for select health and demographic outcomes in an LMIC context. We conclude that the integration of multiple GeoFM sources is an efficient and valuable tool for supplementing and strengthening constrained routine health information systems.         ",
    "url": "https://arxiv.org/abs/2510.25954",
    "authors": [
      "Lynn Metz",
      "Rachel Haggard",
      "Michael Moszczynski",
      "Samer Asbah",
      "Chris Mwase",
      "Patricia Khomani",
      "Tyler Smith",
      "Hannah Cooper",
      "Annie Mwale",
      "Arbaaz Muslim",
      "Gautam Prasad",
      "Mimi Sun",
      "Tomer Shekel",
      "Joydeep Paul",
      "Anna Carter",
      "Shravya Shetty",
      "Dylan Green"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.25962",
    "title": "On the Dataless Training of Neural Networks",
    "abstract": "           This paper surveys studies on the use of neural networks for optimization in the training-data-free setting. Specifically, we examine the dataless application of neural network architectures in optimization by re-parameterizing problems using fully connected (or MLP), convolutional, graph, and quadratic neural networks. Although MLPs have been used to solve linear programs a few decades ago, this approach has recently gained increasing attention due to its promising results across diverse applications, including those based on combinatorial optimization, inverse problems, and partial differential equations. The motivation for this setting stems from two key (possibly over-lapping) factors: (i) data-driven learning approaches are still underdeveloped and have yet to demonstrate strong results, as seen in combinatorial optimization, and (ii) the availability of training data is inherently limited, such as in medical image reconstruction and other scientific applications. In this paper, we define the dataless setting and categorize it into two variants based on how a problem instance -- defined by a single datum -- is encoded onto the neural network: (i) architecture-agnostic methods and (ii) architecture-specific methods. Additionally, we discuss similarities and clarify distinctions between the dataless neural network (dNN) settings and related concepts such as zero-shot learning, one-shot learning, lifting in optimization, and over-parameterization.         ",
    "url": "https://arxiv.org/abs/2510.25962",
    "authors": [
      "Alvaro Velasquez",
      "Susmit Jha",
      "Ismail R. Alkhouri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.25975",
    "title": "SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation",
    "abstract": "           Large Language Models (LLMs) often struggle with complex mathematical reasoning, where prose-based generation leads to unverified and arithmetically unsound solutions. Current prompting strategies like Chain of Thought still operate within this unreliable medium, lacking a mechanism for deterministic verification. To address these limitations, we introduce SymCode, a neurosymbolic framework that reframes mathematical problem-solving as a task of verifiable code generation using the SymPy library. We evaluate SymCode on challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating significant accuracy improvements of up to 13.6 percentage points over baselines. Our analysis shows that SymCode is not only more token-efficient but also fundamentally shifts model failures from opaque logical fallacies towards transparent, programmatic errors. By grounding LLM reasoning in a deterministic symbolic engine, SymCode represents a key step towards more accurate and trustworthy AI in formal domains.         ",
    "url": "https://arxiv.org/abs/2510.25975",
    "authors": [
      "Sina Bagheri Nezhad",
      "Yao Li",
      "Ameeta Agrawal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2510.25978",
    "title": "On the Go with AR: Attention to Virtual and Physical Targets while Varying Augmentation Density",
    "abstract": "           Augmented reality is projected to be a primary mode of information consumption on the go, seamlessly integrating virtual content into the physical world. However, the potential perceptual demands of viewing virtual annotations while navigating a physical environment could impact user efficacy and safety, and the implications of these demands are not well understood. Here, we investigate the impact of virtual path guidance and augmentation density (visual clutter) on search performance and memory. Participants walked along a predefined path, searching for physical or virtual items. They experienced two levels of augmentation density, and either walked freely or with enforced speed and path guidance. Augmentation density impacted behavior and reduced awareness of uncommon objects in the environment. Analysis of search task performance and post-experiment item recall revealed differing attention to physical and virtual objects. On the basis of these findings we outline considerations for AR apps designed for use on the go.         ",
    "url": "https://arxiv.org/abs/2510.25978",
    "authors": [
      "You-Jin Kim",
      "Radha Kumaran",
      "Jingjing Luo",
      "Tom Bullock",
      "Barry Giesbrecht",
      "Tobias H\u00f6llerer"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2510.26003",
    "title": "Message Recovery Attack in NTRU via Knapsack",
    "abstract": "           In the present paper, we introduce a message-recovery attack based on the Modular Knapsack Problem, applicable to all variants of the NTRU-HPS cryptosystem. Assuming that a fraction $\\epsilon$ of the coefficients of the message ${\\bf{m}}\\in\\{-1,0,1\\}^N$ and of the nonce vector ${\\bf r}\\in\\{-1,0,1\\}^N$ are known in advance at random positions, we reduce message decryption to finding a short vector in a lattice that encodes an instance of a modular knapsack system. This allows us to address a key question: how much information about ${\\bf m}$, or about the pair $({\\bf m},{\\bf r})$, is required before recovery becomes feasible? A FLATTER reduction successfully recovers the message, in practice when $\\epsilon\\approx 0.45$. Our implementation finds ${\\bf m}$ within a few minutes on a commodity desktop.         ",
    "url": "https://arxiv.org/abs/2510.26003",
    "authors": [
      "Eirini Poimenidou",
      "K. A. Draziotis"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2510.26004",
    "title": "DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection System",
    "abstract": "           Rapid and reliable incident detection is critical for reducing crash-related fatalities, injuries, and congestion. However, conventional methods, such as closed-circuit television, dashcam footage, and sensor-based detection, separate detection from verification, suffer from limited flexibility, and require dense infrastructure or high penetration rates, restricting adaptability and scalability to shifting incident hotspots. To overcome these challenges, we developed DARTS, a drone-based, AI-powered real-time traffic incident detection system. DARTS integrates drones' high mobility and aerial perspective for adaptive surveillance, thermal imaging for better low-visibility performance and privacy protection, and a lightweight deep learning framework for real-time vehicle trajectory extraction and incident detection. The system achieved 99% detection accuracy on a self-collected dataset and supports simultaneous online visual verification, severity assessment, and incident-induced congestion propagation monitoring via a web-based interface. In a field test on Interstate 75 in Florida, DARTS detected and verified a rear-end collision 12 minutes earlier than the local transportation management center and monitored incident-induced congestion propagation, suggesting potential to support faster emergency response and enable proactive traffic control to reduce congestion and secondary crash risk. Crucially, DARTS's flexible deployment architecture reduces dependence on frequent physical patrols, indicating potential scalability and cost-effectiveness for use in remote areas and resource-constrained settings. This study presents a promising step toward a more flexible and integrated real-time traffic incident detection system, with significant implications for the operational efficiency and responsiveness of modern transportation management.         ",
    "url": "https://arxiv.org/abs/2510.26004",
    "authors": [
      "Bai Li",
      "Achilleas Kourtellis",
      "Rong Cao",
      "Joseph Post",
      "Brian Porter",
      "Yu Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.26017",
    "title": "Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning",
    "abstract": "           Climate change and sea-level rise (SLR) pose escalating threats to coastal cities, intensifying the need for efficient and accurate methods to predict potential flood hazards. Traditional physics-based hydrodynamic simulators, although precise, are computationally expensive and impractical for city-scale coastal planning applications. Deep Learning (DL) techniques offer promising alternatives, however, they are often constrained by challenges such as data scarcity and high-dimensional output requirements. Leveraging a recently proposed vision-based, low-resource DL framework, we develop a novel, lightweight Convolutional Neural Network (CNN)-based model designed to predict coastal flooding under variable SLR projections and shoreline adaptation scenarios. Furthermore, we demonstrate the ability of the model to generalize across diverse geographical contexts by utilizing datasets from two distinct regions: Abu Dhabi and San Francisco. Our findings demonstrate that the proposed model significantly outperforms state-of-the-art methods, reducing the mean absolute error (MAE) in predicted flood depth maps on average by nearly 20%. These results highlight the potential of our approach to serve as a scalable and practical tool for coastal flood management, empowering decision-makers to develop effective mitigation strategies in response to the growing impacts of climate change. Project Page: this https URL ",
    "url": "https://arxiv.org/abs/2510.26017",
    "authors": [
      "Bilal Hassan",
      "Areg Karapetyan",
      "Aaron Chung Hin Chow",
      "Samer Madanat"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26033",
    "title": "Engineering Social Optimality via Utility Shaping in Non-Cooperative Games under Incomplete Information and Imperfect Monitoring",
    "abstract": "           In this paper, we study decentralized decision-making where agents optimize private objectives under incomplete information and imperfect public monitoring, in a non-cooperative setting. By shaping utilities-embedding shadow prices or Karush-Kuhn-Tucker(KKT)-aligned penalties-we make the stage game an exact-potential game whose unique equilibrium equals the (possibly constrained) social optimum. We characterize the Bayesian equilibrium as a stochastic variational inequality; strong monotonicity follows from a single-inflection compressed/stretched-exponential response combined with convex pricing. We give tracking bounds for damped-gradient and best-response-with-hysteresis updates under a noisy public index, and corresponding steady-state error. The framework accommodates discrete and continuous action sets and composes with slower discrete assignment. Deployable rules include: embed prices/penalties; publish a single public index; tune steps, damping, and dual rates for contraction. Computational experiments cover (i) a multi-tier supply chain and (ii) a non-cooperative agentic-AI compute market of bidding bots. Relative to price-only baselines, utility shaping attains near-centralized welfare, eliminates steady-state constraint/capacity violations when feasible, and accelerates convergence; with quantization, discrete equilibria track continuous ones within the mesh. The blueprint is portable to demand response, cloud/edge scheduling, and transportation pricing and biosecurity/agriculture. Overall, utility shaping plus a public index implements the constrained social optimum with stable equilibria under noise and drift-an operations-research-friendly alternative to heavy messaging or full mechanism design.         ",
    "url": "https://arxiv.org/abs/2510.26033",
    "authors": [
      "David Smith",
      "Jie Dong",
      "Yizhou Yang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2510.26055",
    "title": "NP-Hardness of Approximating Nash Social Welfare with Supermodular Valuations",
    "abstract": "           We study the problem of allocating a set of indivisible items to agents with supermodular utilities to maximize the Nash social welfare. We show that the problem is NP-hard for any approximation factor.         ",
    "url": "https://arxiv.org/abs/2510.26055",
    "authors": [
      "Alon Bebchuk"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2510.26067",
    "title": "Morphology-Aware Graph Reinforcement Learning for Tensegrity Robot Locomotion",
    "abstract": "           Tensegrity robots combine rigid rods and elastic cables, offering high resilience and deployability but posing major challenges for locomotion control due to their underactuated and highly coupled dynamics. This paper introduces a morphology-aware reinforcement learning framework that integrates a graph neural network (GNN) into the Soft Actor-Critic (SAC) algorithm. By representing the robot's physical topology as a graph, the proposed GNN-based policy captures coupling among components, enabling faster and more stable learning than conventional multilayer perceptron (MLP) policies. The method is validated on a physical 3-bar tensegrity robot across three locomotion primitives, including straight-line tracking and bidirectional turning. It shows superior sample efficiency, robustness to noise and stiffness variations, and improved trajectory accuracy. Notably, the learned policies transfer directly from simulation to hardware without fine-tuning, achieving stable real-world locomotion. These results demonstrate the advantages of incorporating structural priors into reinforcement learning for tensegrity robot control.         ",
    "url": "https://arxiv.org/abs/2510.26067",
    "authors": [
      "Chi Zhang",
      "Mingrui Li",
      "Wenzhe Tong",
      "Xiaonan Huang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2510.26071",
    "title": "Symmetry-Driven Asynchronous Forwarding for Reliable Distributed Coordination in Toroidal Networks",
    "abstract": "           The proliferation of large-scale distributed systems, such as satellite constellations and high-performance computing clusters, demands robust communication primitives that maintain coordination under unreliable links. The torus topology, with its inherent rotational and reflection symmetries, is a prevalent architecture in these domains. However, conventional routing schemes suffer from substantial packet loss during control-plane synchronization after link failures. This paper introduces a symmetry-driven asynchronous forwarding mechanism that leverages the torus's geometric properties to achieve reliable packet delivery without control-plane coordination. We model packet flow using a topological potential gradient and demonstrate that symmetry-breaking failures naturally induce a reverse flow, which we harness for fault circumvention. We propose two local forwarding strategies, Reverse Flow with Counter-facing Priority (RF-CF) and Lateral-facing Priority (RF-LF), that guarantee reachability to the destination via forward-flow phase transition points, without protocol modifications or additional in-packet overhead. Through percolation analysis and packet-level simulations on a 16 x 16 torus, we show that our mechanism reduces packet loss by up to 17.5% under a 1% link failure rate, with the RF-LF strategy contributing to 28% of successfully delivered packets. This work establishes a foundational link between topological symmetry and communication resilience, providing a lightweight, protocol-agnostic substrate for enhancing distributed systems.         ",
    "url": "https://arxiv.org/abs/2510.26071",
    "authors": [
      "Shenshen Luan",
      "Yumo Tian",
      "Xinyu Zhang",
      "Qingwen Zhang",
      "Tianheng Wang",
      "Yan Yang",
      "Shuguo Xie"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2510.26075",
    "title": "FGGM: Formal Grey-box Gradient Method for Attacking DRL-based MU-MIMO Scheduler",
    "abstract": "           In 5G mobile communication systems, MU-MIMO has been applied to enhance spectral efficiency and support high data rates. To maximize spectral efficiency while providing fairness among users, the base station (BS) needs to selects a subset of users for data transmission. Given that this problem is NP-hard, DRL-based methods have been proposed to infer the near-optimal solutions in real-time, yet this approach has an intrinsic security problem. This paper investigates how a group of adversarial users can exploit unsanitized raw CSIs to launch a throughput degradation attack. Most existing studies only focused on systems in which adversarial users can obtain the exact values of victims' CSIs, but this is impractical in the case of uplink transmission in LTE/5G mobile systems. We note that the DRL policy contains an observation normalizer which has the mean and variance of the observation to improve training convergence. Adversarial users can then estimate the upper and lower bounds of the local observations including the CSIs of victims based solely on that observation normalizer. We develop an attacking scheme FGGM by leveraging polytope abstract domains, a technique used to bound the outputs of a neural network given the input ranges. Our goal is to find one set of intentionally manipulated CSIs which can achieve the attacking goals for the whole range of local observations of victims. Experimental results demonstrate that FGGM can determine a set of adversarial CSI vector controlled by adversarial users, then reuse those CSIs throughout the simulation to reduce the network throughput of a victim up to 70\\% without knowing the exact value of victims' local observations. This study serves as a case study and can be applied to many other DRL-based problems, such as a knapsack-oriented resource allocation problems.         ",
    "url": "https://arxiv.org/abs/2510.26075",
    "authors": [
      "Thanh Le",
      "Hai Duong",
      "Yusheng Ji",
      "ThanhVu Nguyen",
      "John C.S. Lui"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2510.26092",
    "title": "Signed Graph Unlearning",
    "abstract": "           The proliferation of signed networks in contemporary social media platforms necessitates robust privacy-preserving mechanisms. Graph unlearning, which aims to eliminate the influence of specific data points from trained models without full retraining, becomes particularly critical in these scenarios where user interactions are sensitive and dynamic. Existing graph unlearning methodologies are exclusively designed for unsigned networks and fail to account for the unique structural properties of signed graphs. Their naive application to signed networks neglects edge sign information, leading to structural imbalance across subgraphs and consequently degrading both model performance and unlearning efficiency. This paper proposes SGU (Signed Graph Unlearning), a graph unlearning framework specifically for signed networks. SGU incorporates a new graph unlearning partition paradigm and a novel signed network partition algorithm that preserve edge sign information during partitioning and ensure structural balance across partitions. Compared with baselines, SGU achieves state-of-the-art results in both model performance and unlearning efficiency.         ",
    "url": "https://arxiv.org/abs/2510.26092",
    "authors": [
      "Zhifei Luo",
      "Lin Li",
      "Xiaohui Tao",
      "Kaize Shi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2510.26102",
    "title": "PEEL: A Poisoning-Exposing Encoding Theoretical Framework for Local Differential Privacy",
    "abstract": "           Local Differential Privacy (LDP) is a widely adopted privacy-protection model in the Internet of Things (IoT) due to its lightweight, decentralized, and scalable nature. However, it is vulnerable to poisoning attacks, and existing defenses either incur prohibitive resource overheads or rely on domain-specific prior knowledge, limiting their practical deployment. To address these limitations, we propose PEEL, a Poisoning-Exposing Encoding theoretical framework for LDP, which departs from resource- or prior-dependent countermeasures and instead leverages the inherent structural consistency of LDP-perturbed data. As a non-intrusive post-processing module, PEEL amplifies stealthy poisoning effects by re-encoding LDP-perturbed data via sparsification, normalization, and low-rank projection, thereby revealing both output and rule poisoning attacks through structural inconsistencies in the reconstructed space. Theoretical analysis proves that PEEL, integrated with LDP, retains unbiasedness and statistical accuracy, while being robust to expose both output and rule poisoning attacks. Moreover, evaluation results show that LDP-integrated PEEL not only outperforms four state-of-the-art defenses in terms of poisoning exposure accuracy but also significantly reduces client-side computational costs, making it highly suitable for large-scale IoT deployments.         ",
    "url": "https://arxiv.org/abs/2510.26102",
    "authors": [
      "Lisha Shuai",
      "Jiuling Dong",
      "Nan Zhang",
      "Shaofeng Tan",
      "Haokun Zhang",
      "Zilong Song",
      "Gaoya Dong",
      "Xiaolong Yang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2510.26130",
    "title": "Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation",
    "abstract": "           Large language models (LLMs) have advanced code generation at the function level, yet their ability to produce correct class-level implementations in authentic software projects remains poorly understood. This work introduces a novel benchmark derived from open-source repositories, comprising real-world classes divided into seen and unseen partitions to evaluate generalization under practical conditions. The evaluation examines multiple LLMs under varied input specifications, retrieval-augmented configurations, and documentation completeness levels. Results reveal a stark performance disparity: LLMs achieve 84% to 89% correctness on established synthetic benchmarks but only 25% to 34% on real-world class tasks, with negligible differences between familiar and novel codebases. Comprehensive docstrings yield modest gains of 1% to 3% in functional accuracy, though statistical significance is rare. Retrieval-augmented generation proves most effective with partial documentation, improving correctness by 4% to 7% by supplying concrete implementation patterns absent from specifications. Error profiling identifies AttributeError, TypeError, and AssertionError as dominant failure modes (84% of cases), with synthetic tests overemphasizing assertion issues and real-world scenarios highlighting type and attribute mismatches. Retrieval augmentation reduces logical flaws but can introduce dependency conflicts. The benchmark and analysis expose critical limitations in current LLM capabilities for class-level engineering, offering actionable insights for enhancing context modelling, documentation strategies, and retrieval integration in production code assistance tools.         ",
    "url": "https://arxiv.org/abs/2510.26130",
    "authors": [
      "Musfiqur Rahman",
      "SayedHassan Khatoonabadi",
      "Emad Shihab"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26135",
    "title": "Green Wireless Network Scaling for Joint Deployment: Multi-BSs or Multi-RISs?",
    "abstract": "           The imminent emergence of sixth-generation (6G) networks faces critical challenges from spatially heterogeneous traffic and escalating energy consumption, necessitating sustainable scaling strategies for network infrastructure such as base stations (BSs) and reconfigurable intelligent surfaces (RISs). This paper establishes fundamental scaling laws for the Integrated Relative Energy Efficiency (IREE) metric under joint multi-BS and multi-RIS deployment in traffic-mismatched scenarios. Specifically, we propose an Alternating Directional Dual-Radial Basis Function (ADD-RBF) framework that models the channels of BSs and RISs as two type of spatially decoupled RBF neurons to maximize IREE through alternative optimization, with proven universal approximation capability and convergence guarantees. Theoretical analysis reveals a scaling dichotomy: BS proliferation drives logarithmic capacity growth $\\mathcal{O}(\\log N^{BS})$ but only polynomial mismatch reduction $\\mathcal{O}(1/\\sqrt{N^{BS}})$, whereas RIS deployment achieves exponential mismatch mitigation $\\mathcal{O}(\\delta_{\\text{err}}^{-(N^R+1)})$ despite its sub-logarithmic capacity gains. Simulation results validate that RISs excel in capturing spatial traffic correlations and alleviating hotspots, making them particularly effective when mismatch dominates, while BSs are preferable under capacity shortages. These findings offer practical guidelines for green 6G network design.         ",
    "url": "https://arxiv.org/abs/2510.26135",
    "authors": [
      "Tao Yu",
      "Simin Wang",
      "Shunqing Zhang",
      "Mingyao Cui",
      "Kaibin Huang",
      "Wen Chen",
      "QingQing Wu",
      "Jihong Li",
      "Kaixuan Huang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.26146",
    "title": "maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition",
    "abstract": "           WiFi Channel State Information (CSI)-based human activity recognition (HAR) provides a privacy-preserving, device-free sensing solution for smart environments. However, its deployment on edge devices is severely constrained by domain shift, where recognition performance deteriorates under varying environmental and hardware conditions. This study presents maxVSTAR (maximally adaptive Vision-guided Sensing Technology for Activity Recognition), a closed-loop, vision-guided model adaptation framework that autonomously mitigates domain shift for edge-deployed CSI sensing systems. The proposed system integrates a cross-modal teacher-student architecture, where a high-accuracy YOLO-based vision model serves as a dynamic supervisory signal, delivering real-time activity labels for the CSI data stream. These labels enable autonomous, online fine-tuning of a lightweight CSI-based HAR model, termed Sensing Technology for Activity Recognition (STAR), directly at the edge. This closed-loop retraining mechanism allows STAR to continuously adapt to environmental changes without manual intervention. Extensive experiments demonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated hardware, the baseline STAR model's recognition accuracy declined from 93.52% to 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored the accuracy to 81.51%. These results confirm the system's capacity for dynamic, self-supervised model adaptation in privacy-conscious IoT environments, establishing a scalable and practical paradigm for long-term autonomous HAR using CSI sensing at the network edge.         ",
    "url": "https://arxiv.org/abs/2510.26146",
    "authors": [
      "Kexing Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26151",
    "title": "MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction",
    "abstract": "           Large annotated datasets are essential for training robust Computer-Aided Diagnosis (CAD) models for breast cancer detection or risk prediction. However, acquiring such datasets with fine-detailed annotation is both costly and time-consuming. Vision-Language Models (VLMs), such as CLIP, which are pre-trained on large image-text pairs, offer a promising solution by enhancing robustness and data efficiency in medical imaging tasks. This paper introduces a novel Multi-View Mammography and Language Model for breast cancer classification and risk prediction, trained on a dataset of paired mammogram images and synthetic radiology reports. Our MV-MLM leverages multi-view supervision to learn rich representations from extensive radiology data by employing cross-modal self-supervision across image-text pairs. This includes multiple views and the corresponding pseudo-radiology reports. We propose a novel joint visual-textual learning strategy to enhance generalization and accuracy performance over different data types and tasks to distinguish breast tissues or cancer characteristics(calcification, mass) and utilize these patterns to understand mammography images and predict cancer risk. We evaluated our method on both private and publicly available datasets, demonstrating that the proposed model achieves state-of-the-art performance in three classification tasks: (1) malignancy classification, (2) subtype classification, and (3) image-based cancer risk prediction. Furthermore, the model exhibits strong data efficiency, outperforming existing fully supervised or VLM baselines while trained on synthetic text reports and without the need for actual radiology reports.         ",
    "url": "https://arxiv.org/abs/2510.26151",
    "authors": [
      "Shunjie-Fabian Zheng",
      "Hyeonjun Lee",
      "Thijs Kooi",
      "Ali Diba"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26159",
    "title": "Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series",
    "abstract": "           In this study, we investigate the effectiveness of advanced feature engineering and hybrid model architectures for anomaly detection in a multivariate industrial time series, focusing on a steam turbine system. We evaluate the impact of change point-derived statistical features, clustering-based substructure representations, and hybrid learning strategies on detection performance. Despite their theoretical appeal, these complex approaches consistently underperformed compared to a simple Random Forest + XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of 0.976, F1-score of 0.41, and 100% early detection within the defined time window. Our findings highlight that, in scenarios with highly imbalanced and temporally uncertain data, model simplicity combined with optimized segmentation can outperform more sophisticated architectures, offering greater robustness, interpretability, and operational utility.         ",
    "url": "https://arxiv.org/abs/2510.26159",
    "authors": [
      "Emilio Mastriani",
      "Alessandro Costa",
      "Federico Incardona",
      "Kevin Munari",
      "Sebastiano Spinello"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26161",
    "title": "A two-dimensional fractional-order element-free Galerkin method for nonlocal elasticity and complex domain problems",
    "abstract": "           This study presents a meshfree two-dimensional fractional-order Element-Free Galerkin (2D f-EFG) method as a viable alternative to conventional mesh-based FEM for a numerical solution of (spatial) fractional-order differential equations (FDEs). The previously developed one-dimensional f-EFG solver offers a limited demonstration of the true efficacy of EFG formulations for FDEs, as it is restricted to simple 1D line geometries. In contrast, the 2D f-EFG solver proposed and developed here effectively demonstrates the potential of meshfree approaches for solving FDEs. The proposed solver can handle complex and irregular 2D domains that are challenging for mesh-based methods. As an example, the developed framework is employed to investigate nonlocal elasticity governed by fractional-order constitutive relations in a square and circular plate. Furthermore, the proposed approach mitigates key drawbacks of FEM, including high computational cost, mesh generation, and reduced accuracy in irregular domains. The 2D f-EFG employs 2D Moving Least Squares (MLS) approximants, which are particularly effective in approximating fractional derivatives from nodal values. The 2D f-EFG solver is employed here for the numerical solution of fractional-order linear and nonlinear partial differential equations corresponding to the nonlocal elastic response of a plate. The solver developed here is validated with the benchmark results available in the literature. While the example chosen here focuses on nonlocal elasticity, the numerical method can be extended for diverse applications of fractional-order derivatives in multiscale modeling, multiphysics coupling, anomalous diffusion, and complex material behavior.         ",
    "url": "https://arxiv.org/abs/2510.26161",
    "authors": [
      "Shubham Desai",
      "Malapeta Hemasundara Rao",
      "Sai Sidhardh"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2510.26172",
    "title": "Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis",
    "abstract": "           Social media platforms generate massive volumes of heterogeneous data, capturing user behaviors, textual content, temporal dynamics, and network structures. Analyzing such data is crucial for understanding phenomena such as opinion dynamics, community formation, and information diffusion. However, discovering insights from this complex landscape is exploratory, conceptually challenging, and requires expertise in social media mining and visualization. Existing automated approaches, though increasingly leveraging large language models (LLMs), remain largely confined to structured tabular data and cannot adequately address the heterogeneity of social media analysis. We present SIA (Social Insight Agents), an LLM agent system that links heterogeneous multi-modal data -- including raw inputs (e.g., text, network, and behavioral data), intermediate outputs, mined analytical results, and visualization artifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy that connects insight types with suitable mining and visualization techniques, SIA enables agents to plan and execute coherent analysis strategies. To ensure multi-modal integration, it incorporates a data coordinator that unifies tabular, textual, and network data into a consistent flow. Its interactive interface provides a transparent workflow where users can trace, validate, and refine the agent's reasoning, supporting both adaptability and trustworthiness. Through expert-centered case studies and quantitative evaluation, we show that SIA effectively discovers diverse and meaningful insights from social media while supporting human-agent collaboration in complex analytical tasks.         ",
    "url": "https://arxiv.org/abs/2510.26172",
    "authors": [
      "Shifu Chen",
      "Dazhen Deng",
      "Zhihong Xu",
      "Sijia Xu",
      "Tai-Quan Peng",
      "Yingcai Wu"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2510.26196",
    "title": "Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction",
    "abstract": "           3D human pose estimation from sketches has broad applications in computer animation and film production. Unlike traditional human pose estimation, this task presents unique challenges due to the abstract and disproportionate nature of sketches. Previous sketch-to-pose methods, constrained by the lack of large-scale sketch-3D pose annotations, primarily relied on optimization with heuristic rules-an approach that is both time-consuming and limited in generalizability. To address these challenges, we propose a novel approach leveraging a \"learn from synthesis\" strategy. First, a diffusion model is trained to synthesize sketch images from 2D poses projected from 3D human poses, mimicking disproportionate human structures in sketches. This process enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k accurate sketch-3D pose annotation pairs across various sketch styles. Building on this synthetic dataset, we introduce an end-to-end data-driven framework for estimating human poses and shapes from diverse sketch styles. Our framework combines existing 2D pose detectors and generative diffusion priors for sketch feature extraction with a feed-forward neural network for efficient 2D pose estimation. Multiple heuristic loss functions are incorporated to guarantee geometric coherence between the derived 3D poses and the detected 2D poses while preserving accurate self-contacts. Qualitative, quantitative, and subjective evaluations collectively show that our model substantially surpasses previous ones in both estimation accuracy and speed for sketch-to-pose tasks.         ",
    "url": "https://arxiv.org/abs/2510.26196",
    "authors": [
      "Li Wang",
      "Yiyu Zhuang",
      "Yanwen Wang",
      "Xun Cao",
      "Chuan Guo",
      "Xinxin Zuo",
      "Hao Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.26203",
    "title": "Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management",
    "abstract": "           The sustainability of supply chain plays a key role in achieving optimal performance in controlling the supply chain. The management of risks that occur in a supply chain is a fundamental problem for the purpose of developing the sustainability of the network and elevating the performance efficiency of the supply chain. The correct classification of products is another essential element in a sustainable supply chain. Acknowledging recent breakthroughs in the context of deep networks, several architectural options have been deployed to analyze supply chain datasets. A novel geometric deep network is used to propose an ensemble deep network. The proposed Chebyshev ensemble geometric network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This network is proposed to leverage the information dependencies in supply chain to derive invisible states of samples in the database. The functionality of the proposed deep network is assessed on the two different databases. The SupplyGraph Dataset and DataCo are considered in this research. The prediction of delivery status of DataCo supply chain is done for risk administration. The product classification and edge classification are performed using the SupplyGraph database to enhance the sustainability of the supply network. An average accuracy of 98.95% is obtained for the ensemble network for risk management. The average accuracy of 100% and 98.07% are obtained for sustainable supply chain in terms of 5 product group classification and 4 product relation classification, respectively. The average accuracy of 92.37% is attained for 25 company relation classification. The results confirm an average improvement and efficiency of the proposed method compared to the state-of-the-art approaches.         ",
    "url": "https://arxiv.org/abs/2510.26203",
    "authors": [
      "Mehdi Khaleghi",
      "Nastaran Khaleghi",
      "Sobhan Sheykhivand",
      "Sebelan Danishvar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.26289",
    "title": "Contribution-Guided Asymmetric Learning for Robust Multimodal Fusion under Imbalance and Noise",
    "abstract": "           Multimodal learning faces two major challenges: modality imbalance and data noise, which significantly affect the robustness and generalization ability of models. Existing methods achieve modality balance by suppressing dominant modalities, but they neglect the inherent differences in the information value between modalities, potentially leading to convergence to suboptimal solutions. This paper proposes an innovative modality compression paradigm, Contribution-Guided Asymmetric Learning (CAL), which aims to enhance the contribution of high-contribution modalities while compressing weak modalities to increase their contribution, allowing both to improve the performance of multimodal information fusion. CAL is based on a modality contribution metric W^m combining the information quantity I(m) and confidence D(m), and it designs an asymmetric gradient acceleration mechanism and a contribution-aware Asymmetric Information Bottleneck (AIB) compression mechanism. The former accelerates the gradient update of modalities, while the latter dynamically compresses the noise of low-contribution modalities. On five benchmark datasets, including emotion recognition, scene recognition, and event localization tasks, CAL has shown outstanding performance in imbalanced fusion tasks and noise robustness tests. On CREMA-D, KS, and AVE, CAL achieves 79.30%, 74.82%, and 74.21% accuracy, significantly outperforming the existing state-of-the-art model ARL. In high-noise robustness tests, CAL also achieved leading performance under various attack strategies on the MVSA-Single and NYUD2 datasets. These results validate the significant advantages of CAL in modality imbalance and noise interference. CAL, as a flexible and efficient framework, is easy to transfer to other tasks and has broad adaptability and potential application prospects.         ",
    "url": "https://arxiv.org/abs/2510.26289",
    "authors": [
      "Zijing Xu",
      "Yunfeng Kou",
      "Kunming Wu",
      "Hong Liu"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2510.26299",
    "title": "Modeling strategies for speech enhancement in the latent space of a neural audio codec",
    "abstract": "           Neural audio codecs (NACs) provide compact latent speech representations in the form of sequences of continuous vectors or discrete tokens. In this work, we investigate how these two types of speech representations compare when used as training targets for supervised speech enhancement. We consider both autoregressive and non-autoregressive speech enhancement models based on the Conformer architecture, as well as a simple baseline where the NAC encoder is simply fine-tuned for speech enhancement. Our experiments reveal three key findings: predicting continuous latent representations consistently outperforms discrete token prediction; autoregressive models achieve higher quality but at the expense of intelligibility and efficiency, making non-autoregressive models more attractive in practice; and encoder fine-tuning yields the strongest enhancement metrics overall, though at the cost of degraded codec reconstruction. The code and audio samples are available online.         ",
    "url": "https://arxiv.org/abs/2510.26299",
    "authors": [
      "Sofiene Kammoun",
      "Xavier Alameda-Pineda",
      "Simon Leglaive"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2510.26301",
    "title": "Offline Clustering of Preference Learning with Active-data Augmentation",
    "abstract": "           Preference learning from pairwise feedback is a widely adopted framework in applications such as reinforcement learning with human feedback and recommendations. In many practical settings, however, user interactions are limited or costly, making offline preference learning necessary. Moreover, real-world preference learning often involves users with different preferences. For example, annotators from different backgrounds may rank the same responses differently. This setting presents two central challenges: (1) identifying similarity across users to effectively aggregate data, especially under scenarios where offline data is imbalanced across dimensions, and (2) handling the imbalanced offline data where some preference dimensions are underrepresented. To address these challenges, we study the Offline Clustering of Preference Learning problem, where the learner has access to fixed datasets from multiple users with potentially different preferences and aims to maximize utility for a test user. To tackle the first challenge, we first propose Off-C$^2$PL for the pure offline setting, where the learner relies solely on offline data. Our theoretical analysis provides a suboptimality bound that explicitly captures the tradeoff between sample noise and bias. To address the second challenge of inbalanced data, we extend our framework to the setting with active-data augmentation where the learner is allowed to select a limited number of additional active-data for the test user based on the cluster structure learned by Off-C$^2$PL. In this setting, our second algorithm, A$^2$-Off-C$^2$PL, actively selects samples that target the least-informative dimensions of the test user's preference. We prove that these actively collected samples contribute more effectively than offline ones. Finally, we validate our theoretical results through simulations on synthetic and real-world datasets.         ",
    "url": "https://arxiv.org/abs/2510.26301",
    "authors": [
      "Jingyuan Liu",
      "Fatemeh Ghaffari",
      "Xuchuang Wang",
      "Mohammad Hajiesmaili",
      "Carlee Joe-Wong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26302",
    "title": "Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens",
    "abstract": "           Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal generalization by aligning images and texts in a shared embedding space, yet it persistently fails at compositional reasoning over objects, attributes, and relations often behaving like a bag-of-words matcher. Prior causal accounts typically model text as a single vector, obscuring token-level structure and leaving core phenomena-such as prompt sensitivity and failures on hard negatives unexplained. We address this gap with a token-aware causal representation learning (CRL) framework grounded in a sequential, language-token SCM. Our theory extends block identifiability to tokenized text, proving that CLIP's contrastive objective can recover the modal-invariant latent variable under both sentence-level and token-level SCMs. Crucially, token granularity yields the first principled explanation of CLIP's compositional brittleness: composition nonidentifiability. We show the existence of pseudo-optimal text encoders that achieve perfect modal-invariant alignment yet are provably insensitive to SWAP, REPLACE, and ADD operations over atomic concepts, thereby failing to distinguish correct captions from hard negatives despite optimizing the same training objective as true-optimal encoders. The analysis further links language-side nonidentifiability to visual-side failures via the modality gap and shows how iterated composition operators compound hardness, motivating improved negative mining strategies.         ",
    "url": "https://arxiv.org/abs/2510.26302",
    "authors": [
      "Ziliang Chen",
      "Tianang Xiao",
      "Jusheng Zhang",
      "Yongsen Zheng",
      "Xipeng Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26307",
    "title": "A Survey of Heterogeneous Graph Neural Networks for Cybersecurity Anomaly Detection",
    "abstract": "           Anomaly detection is a critical task in cybersecurity, where identifying insider threats, access violations, and coordinated attacks is essential for ensuring system resilience. Graph-based approaches have become increasingly important for modeling entity interactions, yet most rely on homogeneous and static structures, which limits their ability to capture the heterogeneity and temporal evolution of real-world environments. Heterogeneous Graph Neural Networks (HGNNs) have emerged as a promising paradigm for anomaly detection by incorporating type-aware transformations and relation-sensitive aggregation, enabling more expressive modeling of complex cyber data. However, current research on HGNN-based anomaly detection remains fragmented, with diverse modeling strategies, limited comparative evaluation, and an absence of standardized benchmarks. To address this gap, we provide a comprehensive survey of HGNN-based anomaly detection methods in cybersecurity. We introduce a taxonomy that classifies approaches by anomaly type and graph dynamics, analyze representative models, and map them to key cybersecurity applications. We also review commonly used benchmark datasets and evaluation metrics, highlighting their strengths and limitations. Finally, we identify key open challenges related to modeling, data, and deployment, and outline promising directions for future research. This survey aims to establish a structured foundation for advancing HGNN-based anomaly detection toward scalable, interpretable, and practically deployable solutions.         ",
    "url": "https://arxiv.org/abs/2510.26307",
    "authors": [
      "Laura Jiang",
      "Reza Ryan",
      "Qian Li",
      "Nasim Ferdosian"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26309",
    "title": "GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance",
    "abstract": "           Compliance at web scale poses practical challenges: each request may require a regulatory assessment. Regulatory texts (e.g., the General Data Protection Regulation, GDPR) are cross-referential and normative, while runtime contexts are expressed in unstructured natural language. This setting motivates us to align semantic information in unstructured text with the structured, normative elements of regulations. To this end, we introduce GraphCompliance, a framework that represents regulatory texts as a Policy Graph and runtime contexts as a Context Graph, and aligns them. In this formulation, the policy graph encodes normative structure and cross-references, whereas the context graph formalizes events as subject-action-object (SAO) and entity-relation triples. This alignment anchors the reasoning of a judge large language model (LLM) in structured information and helps reduce the burden of regulatory interpretation and event parsing, enabling a focus on the core reasoning step. In experiments on 300 GDPR-derived real-world scenarios spanning five evaluation tasks, GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than LLM-only and RAG baselines, with fewer under- and over-predictions, resulting in higher recall and lower false positive rates. Ablation studies indicate contributions from each graph component, suggesting that structured representations and a judge LLM are complementary for normative reasoning.         ",
    "url": "https://arxiv.org/abs/2510.26309",
    "authors": [
      "Jiseong Chung",
      "Ronny Ko",
      "Wonchul Yoo",
      "Makoto Onizuka",
      "Sungmok Kim",
      "Tae-Wan Kim",
      "Won-Yong Shin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2510.26342",
    "title": "Linear Causal Discovery with Interventional Constraints",
    "abstract": "           Incorporating causal knowledge and mechanisms is essential for refining causal models and improving downstream tasks such as designing new treatments. In this paper, we introduce a novel concept in causal discovery, termed interventional constraints, which differs fundamentally from interventional data. While interventional data require direct perturbations of variables, interventional constraints encode high-level causal knowledge in the form of inequality constraints on causal effects. For instance, in the Sachs dataset (Sachs et al.\\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3 exerts a positive causal effect on Akt. Existing causal discovery methods allow enforcing structural constraints (for example, requiring a causal path from PIP3 to Akt), but they may still produce incorrect causal conclusions such as learning that \"PIP3 inhibits Akt\". Interventional constraints bridge this gap by explicitly constraining the total causal effect between variable pairs, ensuring learned models respect known causal influences. To formalize interventional constraints, we propose a metric to quantify total causal effects for linear causal models and formulate the problem as a constrained optimization task, solved using a two-stage constrained optimization method. We evaluate our approach on real-world datasets and demonstrate that integrating interventional constraints not only improves model accuracy and ensures consistency with established findings, making models more explainable, but also facilitates the discovery of new causal relationships that would otherwise be costly to identify.         ",
    "url": "https://arxiv.org/abs/2510.26342",
    "authors": [
      "Zhigao Guo",
      "Feng Dong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26344",
    "title": "From Embedding to Control: Representations for Stochastic Multi-Object Systems",
    "abstract": "           This paper studies how to achieve accurate modeling and effective control in stochastic nonlinear dynamics with multiple interacting objects. However, non-uniform interactions and random topologies make this task challenging. We address these challenges by proposing \\textit{Graph Controllable Embeddings} (GCE), a general framework to learn stochastic multi-object dynamics for linear control. Specifically, GCE is built on Hilbert space embeddings, allowing direct embedding of probability distributions of controlled stochastic dynamics into a reproducing kernel Hilbert space (RKHS), which enables linear operations in its RKHS while retaining nonlinear expressiveness. We provide theoretical guarantees on the existence, convergence, and applicability of GCE. Notably, a mean field approximation technique is adopted to efficiently capture inter-object dependencies and achieve provably low sample complexity. By integrating graph neural networks, we construct data-dependent kernel features that are capable of adapting to dynamic interaction patterns and generalizing to even unseen topologies with only limited training instances. GCE scales seamlessly to multi-object systems of varying sizes and topologies. Leveraging the linearity of Hilbert spaces, GCE also supports simple yet effective control algorithms for synthesizing optimal sequences. Experiments on physical systems, robotics, and power grids validate GCE and demonstrate consistent performance improvement over various competitive embedding methods in both in-distribution and few-shot tests         ",
    "url": "https://arxiv.org/abs/2510.26344",
    "authors": [
      "Xiaoyuan Cheng",
      "Yiming Yang",
      "Wei Jiang",
      "Chenyang Yuan",
      "Zhuo Sun",
      "Yukun Hu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.26347",
    "title": "Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle",
    "abstract": "           Reinforcement learning (RL) algorithms are designed to optimize problem-solving by learning actions that maximize rewards, a task that becomes particularly challenging in random and nonstationary environments. Even advanced RL algorithms are often limited in their ability to solve problems in these conditions. In applications such as searching for underwater pollution clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate reward-sparse environments, where actions frequently result in a zero reward. This paper aims to address these challenges by revisiting and modifying classical RL approaches to efficiently operate in sparse, randomized, and nonstationary environments. We systematically study a large number of modifications, including hierarchical algorithm changes, multigoal learning, and the integration of a location memory as an external output filter to prevent state revisits. Our results demonstrate that a modified Monte Carlo-based approach significantly outperforms traditional Q-learning and two exhaustive search patterns, illustrating its potential in adapting RL to complex environments. These findings suggest that reinforcement learning approaches can be effectively adapted for use in random, nonstationary, and reward-sparse environments.         ",
    "url": "https://arxiv.org/abs/2510.26347",
    "authors": [
      "Sebastian Zieglmeier",
      "Niklas Erdmann",
      "Narada D. Warakagoda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26412",
    "title": "LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation",
    "abstract": "           Recently text-to-video generation has made impressive progress in producing short, high-quality clips, but evaluating long-form outputs remains a major challenge especially when processing complex prompts. Existing benchmarks mostly rely on simplified prompts and focus on low-level metrics, overlooking fine-grained alignment with prompts and abstract dimensions such as narrative coherence and thematic expression. To address these gaps, we propose LoCoT2V-Bench, a benchmark specifically designed for long video generation (LVG) under complex input conditions. Based on various real-world videos, LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating elements like scene transitions and event dynamics. Moreover, it constructs a multi-dimensional evaluation framework that includes our newly proposed metrics such as event-level alignment, fine-grained temporal consistency, content clarity, and the Human Expectation Realization Degree (HERD) that focuses on more abstract attributes like narrative flow, emotional response, and character development. Using this framework, we conduct a comprehensive evaluation of nine representative LVG models, finding that while current methods perform well on basic visual and temporal aspects, they struggle with inter-event consistency, fine-grained alignment, and high-level thematic adherence, etc. Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for evaluating long-form complex text-to-video generation and highlights critical directions for future method improvement.         ",
    "url": "https://arxiv.org/abs/2510.26412",
    "authors": [
      "Xiangqing Zheng",
      "Chengyue Wu",
      "Kehai Chen",
      "Min Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26420",
    "title": "SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification",
    "abstract": "           The rapid advancement of deep neural networks (DNNs) heavily relies on large-scale, high-quality datasets. However, unauthorized commercial use of these datasets severely violates the intellectual property rights of dataset owners. Existing backdoor-based dataset ownership verification methods suffer from inherent limitations: poison-label watermarks are easily detectable due to label inconsistencies, while clean-label watermarks face high technical complexity and failure on high-resolution images. Moreover, both approaches employ static watermark patterns that are vulnerable to detection and removal. To address these issues, this paper proposes a sample-specific clean-label backdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked sample generator, this method generates unique watermarks for each sample, fundamentally overcoming the vulnerability of static watermark patterns. The core innovation lies in designing a composite loss function with three components: target sample loss ensures watermark effectiveness, non-target sample loss guarantees trigger reliability, and perceptual similarity loss maintains visual imperceptibility. During ownership verification, black-box testing is employed to check whether suspicious models exhibit predefined backdoor behaviors. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method and its robustness against potential watermark removal attacks.         ",
    "url": "https://arxiv.org/abs/2510.26420",
    "authors": [
      "Yingjia Wang",
      "Ting Qiao",
      "Xing Liu",
      "Chongzuo Li",
      "Sixing Wu",
      "Jianbin Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26444",
    "title": "Personalized Treatment Outcome Prediction from Scarce Data via Dual-Channel Knowledge Distillation and Adaptive Fusion",
    "abstract": "           Personalized treatment outcome prediction based on trial data for small-sample and rare patient groups is critical in precision medicine. However, the costly trial data limit the prediction performance. To address this issue, we propose a cross-fidelity knowledge distillation and adaptive fusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation data to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN incorporates a dual-channel knowledge distillation module to extract complementary knowledge from the low-fidelity model, along with an attention-guided fusion module to dynamically integrate multi-source information. Experiments on treatment outcome prediction for the chronic obstructive pulmonary disease demonstrates significant improvements of CFKD-AFN over state-of-the-art methods in prediction accuracy, ranging from 6.67\\% to 74.55\\%, and strong robustness to varying high-fidelity dataset sizes. Furthermore, we extend CFKD-AFN to an interpretable variant, enabling the exploration of latent medical semantics to support clinical decision-making.         ",
    "url": "https://arxiv.org/abs/2510.26444",
    "authors": [
      "Wenjie Chen",
      "Li Zhuang",
      "Ziying Luo",
      "Yu Liu",
      "Jiahao Wu",
      "Shengcai Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26451",
    "title": "Robust Graph Condensation via Classification Complexity Mitigation",
    "abstract": "           Graph condensation (GC) has gained significant attention for its ability to synthesize smaller yet informative graphs. However, existing studies often overlook the robustness of GC in scenarios where the original graph is corrupted. In such cases, we observe that the performance of GC deteriorates significantly, while existing robust graph learning technologies offer only limited effectiveness. Through both empirical investigation and theoretical analysis, we reveal that GC is inherently an intrinsic-dimension-reducing process, synthesizing a condensed graph with lower classification complexity. Although this property is critical for effective GC performance, it remains highly vulnerable to adversarial perturbations. To tackle this vulnerability and improve GC robustness, we adopt the geometry perspective of graph data manifold and propose a novel Manifold-constrained Robust Graph Condensation framework named MRGC. Specifically, we introduce three graph data manifold learning modules that guide the condensed graph to lie within a smooth, low-dimensional manifold with minimal class ambiguity, thereby preserving the classification complexity reduction capability of GC and ensuring robust performance under universal adversarial attacks. Extensive experiments demonstrate the robustness of \\ModelName\\ across diverse attack scenarios.         ",
    "url": "https://arxiv.org/abs/2510.26451",
    "authors": [
      "Jiayi Luo",
      "Qingyun Sun",
      "Beining Yang",
      "Haonan Yuan",
      "Xingcheng Fu",
      "Yanbiao Ma",
      "Jianxin Li",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26457",
    "title": "SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning",
    "abstract": "           Identifying and addressing security issues during the early phase of the development lifecycle is critical for mitigating the long-term negative impacts on software systems. Code review serves as an effective practice that enables developers to check their teammates' code before integration into the codebase. To streamline the generation of review comments, various automated code review approaches have been proposed, where LLM-based methods have significantly advanced the capabilities of automated review generation. However, existing models primarily focus on general-purpose code review, their effectiveness in identifying and addressing security-related issues remains underexplored. Moreover, adapting existing code review approaches to target security issues faces substantial challenges, including data scarcity and inadequate evaluation metrics. To address these limitations, we propose SecureReviewer, a new approach designed for enhancing LLMs' ability to identify and resolve security-related issues during code review. Specifically, we first construct a dataset tailored for training and evaluating secure code review capabilities. Leveraging this dataset, we fine-tune LLMs to generate code review comments that can effectively identify security issues and provide fix suggestions with our proposed secure-aware fine-tuning strategy. To mitigate hallucination in LLMs and enhance the reliability of their outputs, we integrate the RAG technique, which grounds the generated comments in domain-specific security knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric designed to assess the effectiveness of review comments in addressing security issues. Experimental results demonstrate that SecureReviewer outperforms state-of-the-art baselines in both security issue detection accuracy and the overall quality and practical utility of generated review comments.         ",
    "url": "https://arxiv.org/abs/2510.26457",
    "authors": [
      "Fang Liu",
      "Simiao Liu",
      "Yinghao Zhu",
      "Xiaoli Lian",
      "Li Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2510.26464",
    "title": "Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection",
    "abstract": "           Few-shot anomaly detection (FSAD) methods identify anomalous regions with few known normal samples. Most existing methods rely on the generalization ability of pre-trained vision-language models (VLMs) to recognize potentially anomalous regions through feature similarity between text descriptions and images. However, due to the lack of detailed textual descriptions, these methods can only pre-define image-level descriptions to match each visual patch token to identify potential anomalous regions, which leads to the semantic misalignment between image descriptions and patch-level visual anomalies, achieving sub-optimal localization performance. To address the above issues, we propose the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and fine-grained textual descriptions for existing anomaly detection datasets with automatic construction pipeline. Based on the MFSC, we propose a novel framework named FineGrainedAD to improve anomaly localization performance, which consists of two components: Multi-Level Learnable Prompt (MLLP) and Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics into multi-level learnable prompts through automatic replacement and concatenation mechanism, while MLSA designs region aggregation strategy and multi-level alignment training to facilitate learnable prompts better align with corresponding visual regions. Experiments demonstrate that the proposed FineGrainedAD achieves superior overall performance in few-shot settings on MVTec-AD and VisA datasets.         ",
    "url": "https://arxiv.org/abs/2510.26464",
    "authors": [
      "Yuanting Fan",
      "Jun Liu",
      "Xiaochen Chen",
      "Bin-Bin Gao",
      "Jian Li",
      "Yong Liu",
      "Jinlong Peng",
      "Chengjie Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.26484",
    "title": "Bayesian Network Fusion of Large Language Models for Sentiment Analysis",
    "abstract": "           Large language models (LLMs) continue to advance, with an increasing number of domain-specific variants tailored for specialised tasks. However, these models often lack transparency and explainability, can be costly to fine-tune, require substantial prompt engineering, yield inconsistent results across domains, and impose significant adverse environmental impact due to their high computational demands. To address these challenges, we propose the Bayesian network LLM fusion (BNLF) framework, which integrates predictions from three LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic mechanism for sentiment analysis. BNLF performs late fusion by modelling the sentiment predictions from multiple LLMs as probabilistic nodes within a Bayesian network. Evaluated across three human-annotated financial corpora with distinct linguistic and contextual characteristics, BNLF demonstrates consistent gains of about six percent in accuracy over the baseline LLMs, underscoring its robustness to dataset variability and the effectiveness of probabilistic fusion for interpretable sentiment classification.         ",
    "url": "https://arxiv.org/abs/2510.26484",
    "authors": [
      "Rasoul Amirzadeh",
      "Dhananjay Thiruvady",
      "Fatemeh Shiri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26486",
    "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks",
    "abstract": "           Human smuggling networks are complex and constantly evolving, making them difficult to analyze comprehensively. Legal case documents offer rich factual and procedural insights into these networks but are often long, unstructured, and filled with ambiguous or shifting references, posing significant challenges for automated knowledge graph (KG) construction. Existing methods either overlook coreference resolution or fail to scale beyond short text spans, leading to fragmented graphs and inconsistent entity linking. We propose LINK-KG, a modular framework that integrates a three-stage, LLM-guided coreference resolution pipeline with downstream KG extraction. At the core of our approach is a type-specific Prompt Cache, which consistently tracks and resolves references across document chunks, enabling clean and disambiguated narratives for structured knowledge graph construction from both short and long legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods, resulting in cleaner and more coherent graph structures. These improvements establish LINK-KG as a strong foundation for analyzing complex criminal networks.         ",
    "url": "https://arxiv.org/abs/2510.26486",
    "authors": [
      "Dipak Meher",
      "Carlotta Domeniconi",
      "Guadalupe Correa-Cabrera"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26487",
    "title": "Quantum Gated Recurrent GAN with Gaussian Uncertainty for Network Anomaly Detection",
    "abstract": "           Anomaly detection in time-series data is a critical challenge with significant implications for network security. Recent quantum machine learning approaches, such as quantum kernel methods and variational quantum circuits, have shown promise in capturing complex data distributions for anomaly detection but remain constrained by limited qubit counts. We introduce in this work a novel Quantum Gated Recurrent Unit (QGRU)-based Generative Adversarial Network (GAN) employing Successive Data Injection (SuDaI) and a multi-metric gating strategy for robust network anomaly detection. Our model uniquely utilizes a quantum-enhanced generator that outputs parameters (mean and log-variance) of a Gaussian distribution via reparameterization, combined with a Wasserstein critic to stabilize adversarial training. Anomalies are identified through a novel gating mechanism that initially flags potential anomalies based on Gaussian uncertainty estimates and subsequently verifies them using a composite of critic scores and reconstruction errors. Evaluated on benchmark datasets, our method achieves a high time-series aware F1 score (TaF1) of 89.43% demonstrating superior capability in detecting anomalies accurately and promptly as compared to existing classical and quantum models. Furthermore, the trained QGRU-WGAN was deployed on real IBM Quantum hardware, where it retained high anomaly detection performance, confirming its robustness and practical feasibility on current noisy intermediate-scale quantum (NISQ) devices.         ",
    "url": "https://arxiv.org/abs/2510.26487",
    "authors": [
      "Wajdi Hammami",
      "Soumaya Cherkaoui",
      "Jean-Frederic Laprade",
      "Ola Ahmad",
      "Shengrui Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2510.26492",
    "title": "Wireless Sensor Networks as Parallel and Distributed Hardware Platform for Artificial Neural Networks",
    "abstract": "           We are proposing fully parallel and maximally distributed hardware realization of a generic neuro-computing system. More specifically, the proposal relates to the wireless sensor networks technology to serve as a massively parallel and fully distributed hardware platform to implement and realize artificial neural network (ANN) algorithms. A parallel and distributed (PDP) hardware realization of ANNs makes it possible to have real time computation of large-scale (and complex) problems in a highly robust framework. We will demonstrate how a network of hundreds of thousands of processing nodes (or motes of a wireless sensor network), which have on-board processing and wireless communication features, can be used to implement fully parallel and massively distributed computation of artificial neural network algorithms for solution of truly large-scale problems in real time. The realization of artificial neural network algorithms in a massively parallel and fully distributed hardware has been the goal of neural network computing researchers. This is because a parallel and distributed computation of artificial neural network algorithms could not have been achieved against all the advancements in silicon- or optics-based computing. Accordingly, artificial neural networks could not be applied to very large-scale problems for real time computation of solutions. This hindered the development of neural algorithms for affordable and practical solutions of challenging problems since often special-purpose computing approaches in hardware, software or hybrid (non-neural) had to be developed for and fine-tuned to specific problems that are very large-scale and highly complex. Successful implementation is likely to revolutionize computing as we know it by making it possible to solve very large scale scientific, engineering or technical problems in real time.         ",
    "url": "https://arxiv.org/abs/2510.26492",
    "authors": [
      "Gursel Serpen"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2510.26494",
    "title": "Simulating and Experimenting with Social Media Mobilization Using LLM Agents",
    "abstract": "           Online social networks have transformed the ways in which political mobilization messages are disseminated, raising new questions about how peer influence operates at scale. Building on the landmark 61-million-person Facebook experiment \\citep{bond201261}, we develop an agent-based simulation framework that integrates real U.S. Census demographic distributions, authentic Twitter network topology, and heterogeneous large language model (LLM) agents to examine the effect of mobilization messages on voter turnout. Each simulated agent is assigned demographic attributes, a personal political stance, and an LLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano}) reflecting its political sophistication. Agents interact over realistic social network structures, receiving personalized feeds and dynamically updating their engagement behaviors and voting intentions. Experimental conditions replicate the informational and social mobilization treatments of the original Facebook study. Across scenarios, the simulator reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. Our framework provides a controlled, reproducible environment for testing counterfactual designs and sensitivity analyses in political mobilization research, offering a bridge between high-validity field experiments and flexible computational modeling.\\footnote{Code and data available at this https URL}         ",
    "url": "https://arxiv.org/abs/2510.26494",
    "authors": [
      "Sadegh Shirani",
      "Mohsen Bayati"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26501",
    "title": "Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters",
    "abstract": "           Continuous electrocardiogram (ECG) monitoring via wearables offers significant potential for early cardiovascular disease (CVD) detection. However, deploying deep learning models for automated analysis in resource-constrained environments faces reliability challenges due to inevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen pathologies or noisecorrupted signals, often cause erroneous, high-confidence predictions by standard classifiers, compromising patient safety. Existing OOD detection methods either neglect computational constraints or address noise and unseen classes separately. This paper explores Unsupervised Anomaly Detection (UAD) as an independent, upstream filtering mechanism to improve robustness. We benchmark six UAD approaches, including Deep SVDD, reconstruction-based models, Masked Anomaly Detection, normalizing flows, and diffusion models, optimized via Neural Architecture Search (NAS) under strict resource constraints (at most 512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection of OOD CVD classes and signals unsuitable for analysis due to noise. Results show Deep SVDD consistently achieves the best trade-off between detection and efficiency. In a realistic deployment simulation, integrating the optimized Deep SVDD filter with a diagnostic classifier improved accuracy by up to 21 percentage points over a classifier-only baseline. This study demonstrates that optimized UAD filters can safeguard automated ECG analysis, enabling safer, more reliable continuous cardiovascular monitoring on wearables.         ",
    "url": "https://arxiv.org/abs/2510.26501",
    "authors": [
      "Mustafa Fuad Rifet Ibrahim",
      "Maurice Meijer",
      "Alexander Schlaefer",
      "Peer Stelldinger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26509",
    "title": "Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm",
    "abstract": "           The edge detection task is essential in image processing aiming to extract relevant information from an image. One recurring problem in this task is the weaknesses found in some detectors, such as the difficulty in detecting loose edges and the lack of context to extract relevant information from specific problems. To address these weaknesses and adapt the detector to the properties of an image, an adaptable detector described by two-dimensional cellular automaton and optimized by meta-heuristic combined with transfer learning techniques was developed. This study aims to analyze the impact of expanding the search space of the optimization phase and the robustness of the adaptability of the detector in identifying edges of a set of natural images and specialized subsets extracted from the same image set. The results obtained prove that expanding the search space of the optimization phase was not effective for the chosen image set. The study also analyzed the adaptability of the model through a series of experiments and validation techniques and found that, regardless of the validation, the model was able to adapt to the input and the transfer learning techniques applied to the model showed no significant improvements.         ",
    "url": "https://arxiv.org/abs/2510.26509",
    "authors": [
      "Vin\u00edcius Ferraria",
      "Eurico Ruivo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.26512",
    "title": "Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs",
    "abstract": "           Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.32% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.34% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts.         ",
    "url": "https://arxiv.org/abs/2510.26512",
    "authors": [
      "Dipak Meher",
      "Carlotta Domeniconi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26521",
    "title": "Hebrew Diacritics Restoration using Visual Representation",
    "abstract": "           Diacritics restoration in Hebrew is a fundamental task for ensuring accurate word pronunciation and disambiguating textual meaning. Despite the language's high degree of ambiguity when unvocalized, recent machine learning approaches have significantly advanced performance on this task. In this work, we present DIVRIT, a novel system for Hebrew diacritization that frames the task as a zero-shot classification problem. Our approach operates at the word level, selecting the most appropriate diacritization pattern for each undiacritized word from a dynamically generated candidate set, conditioned on the surrounding textual context. A key innovation of DIVRIT is its use of a Hebrew Visual Language Model, which processes undiacritized text as an image, allowing diacritic information to be embedded directly within the input's vector representation. Through a comprehensive evaluation across various configurations, we demonstrate that the system effectively performs diacritization without relying on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting where the correct diacritized form is guaranteed to be among the provided candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic architectural enhancements and optimized training methodologies yield significant improvements in the system's overall generalization capabilities. These findings highlight the promising potential of visual representations for accurate and automated Hebrew diacritization.         ",
    "url": "https://arxiv.org/abs/2510.26521",
    "authors": [
      "Yair Elboher",
      "Yuval Pinter"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2510.26523",
    "title": "Interdependent Privacy in Smart Homes: Hunting for Bystanders in Privacy Policies",
    "abstract": "           Smart home devices such as video doorbells and security cameras are becoming increasingly common in everyday life. While these devices offer convenience and safety, they also raise new privacy concerns: how these devices affect others, like neighbors, visitors, or people passing by. This issue is generally known as interdependent privacy, where one person's actions (or inaction) may impact the privacy of others, and, specifically, bystander privacy in the context of smart homes. Given lax data protection regulations in terms of shared physical spaces and amateur joint data controllers, we expect that the privacy policies of smart home products reflect the missing regulatory incentives. This paper presents a focused privacy policy analysis of 20 video doorbell and smart camera products, concentrating explicitly on the bystander aspect. We show that although some of the vendors acknowledge bystanders, they address it only to the extent of including disclaimers, shifting the ethical responsibility for collecting the data of non-users to the device owner. In addition, we identify and examine real-world cases related to bystander privacy, demonstrating how current deployments can impact non-users. Based on our findings, we analyze vendor privacy policies in light of existing legal frameworks and technical capabilities, and we provide practical recommendations for both policy language and system design to enhance transparency and empower both bystanders and device owners.         ",
    "url": "https://arxiv.org/abs/2510.26523",
    "authors": [
      "Shuaishuai Liu",
      "Gergely Acs",
      "Gergely Bicz\u00f3k"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2510.26536",
    "title": "RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration",
    "abstract": "           The proliferation of collaborative robots across diverse tasks and embodiments presents a central challenge: achieving lifelong adaptability, scalable coordination, and robust scheduling in multi-agent systems. Existing approaches, from vision-language-action (VLA) models to hierarchical frameworks, fall short due to their reliance on limited or dividual-agent memory. This fundamentally constrains their ability to learn over long horizons, scale to heterogeneous teams, or recover from failures, highlighting the need for a unified memory representation. To address these limitations, we introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable, and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene geometry, temporal event history, and embodiment profiles into a shared representation. This memory-centric design is integrated into a brain-cerebellum framework, where a high-level brain model performs global planning by retrieving and updating STEM, while low-level controllers execute actions locally. This closed loop between cognition, memory, and execution enables dynamic task allocation, fault-tolerant collaboration, and consistent state synchronization. We conduct extensive experiments spanning complex coordination tasks in restaurants, supermarkets, and households. Our results demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous embodiments, validating its effectiveness in enabling lifelong, scalable, and robust multi-robot collaboration. Project website: this https URL ",
    "url": "https://arxiv.org/abs/2510.26536",
    "authors": [
      "Huajie Tan",
      "Cheng Chi",
      "Xiansheng Chen",
      "Yuheng Ji",
      "Zhongxia Zhao",
      "Xiaoshuai Hao",
      "Yaoxu Lyu",
      "Mingyu Cao",
      "Junkai Zhao",
      "Huaihai Lyu",
      "Enshen Zhou",
      "Ning Chen",
      "Yankai Fu",
      "Cheng Peng",
      "Wei Guo",
      "Dong Liang",
      "Zhuo Chen",
      "Mengsi Lyu",
      "Chenrui He",
      "Yulong Ao",
      "Yonghua Lin",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Shanghang Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2510.26560",
    "title": "On Measuring Localization of Shortcuts in Deep Networks",
    "abstract": "           Shortcuts, spurious rules that perform well during training but fail to generalize, present a major challenge to the reliability of deep networks (Geirhos et al., 2020). However, the impact of shortcuts on feature representations remains understudied, obstructing the design of principled shortcut-mitigation methods. To overcome this limitation, we investigate the layer-wise localization of shortcuts in deep models. Our novel experiment design quantifies the layer-wise contribution to accuracy degradation caused by a shortcut-inducing skew by counterfactual training on clean and skewed datasets. We employ our design to study shortcuts on CIFAR-10, Waterbirds, and CelebA datasets across VGG, ResNet, DeiT, and ConvNeXt architectures. We find that shortcut learning is not localized in specific layers but distributed throughout the network. Different network parts play different roles in this process: shallow layers predominantly encode spurious features, while deeper layers predominantly forget core features that are predictive on clean data. We also analyze the differences in localization and describe its principal axes of variation. Finally, our analysis of layer-wise shortcut-mitigation strategies suggests the hardness of designing general methods, supporting dataset- and architecture-specific approaches instead.         ",
    "url": "https://arxiv.org/abs/2510.26560",
    "authors": [
      "Nikita Tsoy",
      "Nikola Konstantinov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2510.26578",
    "title": "Two-Timescale Optimization Framework for IAB-Enabled Heterogeneous UAV Networks",
    "abstract": "           In post-disaster scenarios, the rapid deployment of adequate communication infrastructure is essential to support disaster search, rescue, and recovery operations. To achieve this, uncrewed aerial vehicle (UAV) has emerged as a promising solution for emergency communication due to its low cost and deployment flexibility. However, conventional untethered UAV (U-UAV) is constrained by size, weight, and power (SWaP) limitations, making it incapable of maintaining the operation of a macro base station. To address this limitation, we propose a heterogeneous UAV-based framework that integrates tethered UAV (T-UAV) and U-UAVs, where U-UAVs are utilized to enhance the throughput of cell-edge ground user equipments (G-UEs) and guarantee seamless connectivity during G-UEs' mobility to safe zones. It is noted that the integrated access and backhaul (IAB) technique is adopted to support the wireless backhaul of U-UAVs. Accordingly, we formulate a two-timescale joint user scheduling and trajectory control optimization problem, aiming to maximize the downlink throughput under asymmetric traffic demands and G-UEs' mobility. To solve the formulated problem, we proposed a two-timescale multi-agent deep deterministic policy gradient (TTS-MADDPG) algorithm based on the centralized training and distributed execution paradigm. Numerical results show that the proposed algorithm outperforms other benchmarks, including the two-timescale multi-agent proximal policy optimization (TTS-MAPPO) algorithm and MADDPG scheduling method, with robust and higher throughput. Specifically, the proposed algorithm obtains up to 12.2\\% average throughput gain compared to the MADDPG scheduling method.         ",
    "url": "https://arxiv.org/abs/2510.26578",
    "authors": [
      "Jikang Deng",
      "Hui Zhou",
      "Mohamed-Slim Alouini"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.26609",
    "title": "CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing",
    "abstract": "           Accurate and timely crop yield prediction is crucial for global food security and modern agricultural management. Traditional methods often lack the scalability and granularity required for precision farming. This paper introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing), a deep learning model designed for high-resolution, intra-field canola yield prediction. CYPRESS leverages a pre-trained, large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for a continuous regression task, transforming multi-temporal satellite imagery into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from the Canadian Prairies, CYPRESS demonstrates superior performance over existing deep learning-based yield prediction models, highlighting the effectiveness of fine-tuning foundation models for specialized agricultural applications. By providing a continuous, high-resolution output, CYPRESS offers a more actionable tool for precision agriculture than conventional classification or county-level aggregation methods. This work validates a novel approach that bridges the gap between large-scale Earth observation and on-farm decision-making, offering a scalable solution for detailed agricultural monitoring.         ",
    "url": "https://arxiv.org/abs/2510.26609",
    "authors": [
      "Shayan Nejadshamsi",
      "Yuanyuan Zhang",
      "Shadi Zaki",
      "Brock Porth",
      "Lysa Porth",
      "Vahab Khoshdel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2510.26620",
    "title": "Toward Automated Security Risk Detection in Large Software Using Call Graph Analysis",
    "abstract": "           Threat modeling plays a critical role in the identification and mitigation of security risks; however, manual approaches are often labor intensive and prone to error. This paper investigates the automation of software threat modeling through the clustering of call graphs using density-based and community detection algorithms, followed by an analysis of the threats associated with the identified clusters. The proposed method was evaluated through a case study of the Splunk Forwarder Operator (SFO), wherein selected clustering metrics were applied to the software's call graph to assess pertinent code-density security weaknesses. The results demonstrate the viability of the approach and underscore its potential to facilitate systematic threat assessment. This work contributes to the advancement of scalable, semi-automated threat modeling frameworks tailored for modern cloud-native environments.         ",
    "url": "https://arxiv.org/abs/2510.26620",
    "authors": [
      "Nicholas Pecka",
      "Lotfi Ben Othmane",
      "Renee Bryce"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2510.26630",
    "title": "PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus",
    "abstract": "           To address the challenges in UAV object detection, such as complex backgrounds, severe occlusion, dense small objects, and varying lighting conditions,this paper proposes PT-DETR based on RT-DETR, a novel detection algorithm specifically designed for small objects in UAV imagery. In the backbone network, we introduce the Partially-Aware Detail Focus (PADF) Module to enhance feature extraction for small objects. Additionally,we design the Median-Frequency Feature Fusion (MFFF) module,which effectively improves the model's ability to capture small-object details and contextual information. Furthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box matching capability and increase its sensitivity to small-object features, thereby further enhancing detection accuracy and robustness. Compared with RT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the VisDrone2019 dataset with lower computational complexity and fewer parameters, demonstrating its robustness and feasibility for small-object detection tasks.         ",
    "url": "https://arxiv.org/abs/2510.26630",
    "authors": [
      "Bingcong Huo",
      "Zhiming Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.26643",
    "title": "MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection",
    "abstract": "           Anomaly detection is a fundamental task for time series analytics with important implications for the downstream performance of many applications. Despite increasing academic interest and the large number of methods proposed in the literature, recent benchmarks and evaluation studies demonstrated that no overall best anomaly detection methods exist when applied to very heterogeneous time series datasets. Therefore, the only scalable and viable solution to solve anomaly detection over very different time series collected from diverse domains is to propose a model selection method that will select, based on time series characteristics, the best anomaly detection methods to run. Existing AutoML solutions are, unfortunately, not directly applicable to time series anomaly detection, and no evaluation of time series-based approaches for model selection exists. Towards that direction, this paper studies the performance of time series classification methods used as model selection for anomaly detection. In total, we evaluate 234 model configurations derived from 16 base classifiers across more than 1980 time series, and we propose the first extensive experimental evaluation of time series classification as model selection for anomaly detection. Our results demonstrate that model selection methods outperform every single anomaly detection method while being in the same order of magnitude regarding execution time. This evaluation is the first step to demonstrate the accuracy and efficiency of time series classification algorithms for anomaly detection, and represents a strong baseline that can then be used to guide the model selection step in general AutoML pipelines. Preprint version of an article accepted at the VLDB Journal.         ",
    "url": "https://arxiv.org/abs/2510.26643",
    "authors": [
      "Emmanouil Sylligardos",
      "John Paparrizos",
      "Themis Palpanas",
      "Pierre Senellart",
      "Paul Boniol"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26676",
    "title": "Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study",
    "abstract": "           Software vulnerabilities often persist or re-emerge even after being fixed, revealing the complex interplay between code evolution and socio-technical factors. While source code metrics provide useful indicators of vulnerabilities, software engineering process metrics can uncover patterns that lead to their introduction. Yet few studies have explored whether process metrics can reveal risky development activities over time -- insights that are essential for anticipating and mitigating software vulnerabilities. This work highlights the critical role of process metrics along with code changes in understanding and mitigating vulnerability reintroduction. We move beyond file-level prediction and instead analyze security fixes at the commit level, focusing not only on whether a single fix introduces a vulnerability but also on the longer sequences of changes through which vulnerabilities evolve and re-emerge. Our approach emphasizes that reintroduction is rarely the result of one isolated action, but emerges from cumulative development activities and socio-technical conditions. To support this analysis, we conducted a case study on the ImageMagick project by correlating longitudinal process metrics such as bus factor, issue density, and issue spoilage with vulnerability reintroduction activities, encompassing 76 instances of reintroduced vulnerabilities. Our findings show that reintroductions often align with increased issue spoilage and fluctuating issue density, reflecting short-term inefficiencies in issue management and team responsiveness. These observations provide a foundation for broader studies that combine process and code metrics to predict risky fixes and strengthen software security.         ",
    "url": "https://arxiv.org/abs/2510.26676",
    "authors": [
      "Samiha Shimmi",
      "Nicholas M. Synovic",
      "Mona Rahimi",
      "George K. Thiruvathukal"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2510.26684",
    "title": "Process Integrated Computer Vision for Real-Time Failure Prediction in Steel Rolling Mill",
    "abstract": "           We present a long-term deployment study of a machine vision-based anomaly detection system for failure prediction in a steel rolling mill. The system integrates industrial cameras to monitor equipment operation, alignment, and hot bar motion in real time along the process line. Live video streams are processed on a centralized video server using deep learning models, enabling early prediction of equipment failures and process interruptions, thereby reducing unplanned breakdown costs. Server-based inference minimizes the computational load on industrial process control systems (PLCs), supporting scalable deployment across production lines with minimal additional resources. By jointly analyzing sensor data from data acquisition systems and visual inputs, the system identifies the location and probable root causes of failures, providing actionable insights for proactive maintenance. This integrated approach enhances operational reliability, productivity, and profitability in industrial manufacturing environments.         ",
    "url": "https://arxiv.org/abs/2510.26684",
    "authors": [
      "Vaibhav Kurrey",
      "Sivakalyan Pujari",
      "Gagan Raj Gupta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.26701",
    "title": "Graph approach for observability analysis in power system dynamic state estimation",
    "abstract": "           The proposed approach yields a numerical method that provably executes in linear time with respect to the number of nodes and edges in a graph. The graph, constructed from the power system model, requires only knowledge of the dependencies between state-to-state and output-to-state variables within a state-space framework. While graph-based observability analysis methods exist for power system static-state estimation, the approach presented here is the first for dynamic-state estimation (DSE). We examine decentralized and centralized DSE scenarios and compare our findings with a well-established, albeit non-scalable, observability analysis method in the literature. When compared to the latter in a centralized DSE setting, our method reduced computation time by 1440x.         ",
    "url": "https://arxiv.org/abs/2510.26701",
    "authors": [
      "Akhila Kandivalasa",
      "Marcos Netto"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.26704",
    "title": "How Regularization Terms Make Invertible Neural Networks Bayesian Point Estimators",
    "abstract": "           Can regularization terms in the training of invertible neural networks lead to known Bayesian point estimators in reconstruction? Invertible networks are attractive for inverse problems due to their inherent stability and interpretability. Recently, optimization strategies for invertible neural networks that approximate either a reconstruction map or the forward operator have been studied from a Bayesian perspective, but each has limitations. To address this, we introduce and analyze two regularization terms for the network training that, upon inversion of the network, recover properties of classical Bayesian point estimators: while the first can be connected to the posterior mean, the second resembles the MAP estimator. Our theoretical analysis characterizes how each loss shapes both the learned forward operator and its inverse reconstruction map. Numerical experiments support our findings and demonstrate how these loss-term regularizers introduce data-dependence in a stable and interpretable way.         ",
    "url": "https://arxiv.org/abs/2510.26704",
    "authors": [
      "Nick Heilenk\u00f6tter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2510.26708",
    "title": "Pareto-Optimal Sampling and Resource Allocation for Timely Communication in Shared-Spectrum Low-Altitude Networks",
    "abstract": "           Guaranteeing stringent data freshness for low-altitude unmanned aerial vehicles (UAVs) in shared spectrum forces a critical trade-off between two operational costs: the UAV's own energy consumption and the occupation of terrestrial channel resources. The core challenge is to satisfy the aerial data freshness while finding a Pareto-optimal balance between these costs. Leveraging predictive channel models and predictive UAV trajectories, we formulate a bi-objective Pareto optimization problem over a long-term planning horizon to jointly optimize the sampling timing for aerial traffic and the power and spectrum allocation for fair coexistence. However, the problem's non-convex, mixed-integer nature renders classical methods incapable of fully characterizing the complete Pareto frontier. Notably, we show monotonicity properties of the frontier, building on which we transform the bi-objective problem into several single-objective problems. We then propose a new graph-based algorithm and prove that it can find the complete set of Pareto optima with low complexity, linear in the horizon and near-quadratic in the resource block (RB) budget. Numerical comparisons show that our approach meets the stringent timeliness requirement and achieves a six-fold reduction in RB utilization or a 6 dB energy saving compared to benchmarks.         ",
    "url": "https://arxiv.org/abs/2510.26708",
    "authors": [
      "Bowen Li",
      "Jiping Luo",
      "Themistoklis Charalambous",
      "Nikolaos Pappas"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2510.26722",
    "title": "Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off",
    "abstract": "           Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \\emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.         ",
    "url": "https://arxiv.org/abs/2510.26722",
    "authors": [
      "Muhammad Faraz Ul Abrar",
      "Nicol\u00f2 Michelusi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.26769",
    "title": "SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models",
    "abstract": "           This work introduces SteerVLM, a lightweight steering module designed to guide Vision-Language Models (VLMs) towards outputs that better adhere to desired instructions. Our approach learns from the latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting the language modality with image context. This allows for fine-grained, inference-time control over complex output semantics without modifying model weights while preserving performance on off-target tasks. Our steering module requires learning parameters equal to 0.14% of the original VLM's size. Our steering module gains model control through dimension-wise activation modulation and adaptive steering across layers without requiring pre-extracted static vectors or manual tuning of intervention points. Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a multimodal dataset specifically created to facilitate the development and evaluation of VLM steering techniques. Our method outperforms existing intervention techniques on steering and hallucination mitigation benchmarks for VLMs and proposes a robust solution for multimodal model control through activation engineering.         ",
    "url": "https://arxiv.org/abs/2510.26769",
    "authors": [
      "Anushka Sivakumar",
      "Andrew Zhang",
      "Zaber Hakim",
      "Chris Thomas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.25774",
    "title": "Pulsar Detection with Deep Learning",
    "abstract": "           Pulsar surveys generate millions of candidates per run, overwhelming manual inspection. This thesis builds a deep learning pipeline for radio pulsar candidate selection that fuses array-derived features with image diagnostics. From approximately 500 GB of Giant Metrewave Radio Telescope (GMRT) data, raw voltages are converted to filterbanks (SIGPROC), then de-dispersed and folded across trial dispersion measures (PRESTO) to produce approximately 32,000 candidates. Each candidate yields four diagnostics--summed profile, time vs. phase, subbands vs. phase, and DM curve--represented as arrays and images. A baseline stacked model (ANNs for arrays + CNNs for images with logistic-regression fusion) reaches 68% accuracy. We then refine the CNN architecture and training (regularization, learning-rate scheduling, max-norm constraints) and mitigate class imbalance via targeted augmentation, including a GAN-based generator for the minority class. The enhanced CNN attains 87% accuracy; the final GAN+CNN system achieves 94% accuracy with balanced precision and recall on a held-out test set, while remaining lightweight enough for near--real-time triage. The results show that combining array and image channels improves separability over image-only approaches, and that modest generative augmentation substantially boosts minority (pulsar) recall. The methods are survey-agnostic and extensible to forthcoming high-throughput facilities.         ",
    "url": "https://arxiv.org/abs/2510.25774",
    "authors": [
      "Manideep Pendyala"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.25814",
    "title": "Optimizing Mirror-Image Peptide Sequence Design for Data Storage via Peptide Bond Cleavage Prediction",
    "abstract": "           Traditional non-biological storage media, such as hard drives, face limitations in both storage density and lifespan due to the rapid growth of data in the big data era. Mirror-image peptides composed of D-amino acids have emerged as a promising biological storage medium due to their high storage density, structural stability, and long lifespan. The sequencing of mirror-image peptides relies on \\textit{de-novo} technology. However, its accuracy is limited by the scarcity of tandem mass spectrometry datasets and the challenges that current algorithms encounter when processing these peptides directly. This study is the first to propose improving sequencing accuracy indirectly by optimizing the design of mirror-image peptide sequences. In this work, we introduce DBond, a deep neural network based model that integrates sequence features, precursor ion properties, and mass spectrometry environmental factors for the prediction of mirror-image peptide bond cleavage. In this process, sequences with a high peptide bond cleavage ratio, which are easy to sequence, are selected. The main contributions of this study are as follows. First, we constructed MiPD513, a tandem mass spectrometry dataset containing 513 mirror-image peptides. Second, we developed the peptide bond cleavage labeling algorithm (PBCLA), which generated approximately 12.5 million labeled data based on MiPD513. Third, we proposed a dual prediction strategy that combines multi-label and single-label classification. On an independent test set, the single-label classification strategy outperformed other methods in both single and multiple peptide bond cleavage prediction tasks, offering a strong foundation for sequence optimization.         ",
    "url": "https://arxiv.org/abs/2510.25814",
    "authors": [
      "Yilong Lu",
      "Si Chen",
      "Songyan Gao",
      "Han Liu",
      "Xin Dong",
      "Wenfeng Shen",
      "Guangtai Ding"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.25925",
    "title": "Equation Discovery, Parametric Simulation, and Optimization Using the Physics-Informed Neural Network (PINN) Method for the Heat Conduction Problem",
    "abstract": "           In this study, the capabilities of the Physics-Informed Neural Network (PINN) method are investigated for three major tasks: modeling, simulation, and optimization in the context of the heat conduction problem. In the modeling phase, the governing equation of heat transfer by conduction is reconstructed through equation discovery using fractional-order derivatives, enabling the identification of the fractional derivative order that best describes the physical behavior. In the simulation phase, the thermal conductivity is treated as a physical parameter, and a parametric simulation is performed to analyze its influence on the temperature field. In the optimization phase, the focus is placed on the inverse problem, where the goal is to infer unknown physical properties from observed data. The effectiveness of the PINN approach is evaluated across these three fundamental engineering problem types and compared against conventional numerical methods. The results demonstrate that although PINNs may not yet outperform traditional numerical solvers in terms of speed and accuracy for forward problems, they offer a powerful and flexible framework for parametric simulation, optimization, and equation discovery, making them highly valuable for inverse and data-driven modeling applications.         ",
    "url": "https://arxiv.org/abs/2510.25925",
    "authors": [
      "Ehsan Ghaderi",
      "Mohamad Ali Bijarchi",
      "Siamak Kazemzadeh Hannani",
      "Ali Nouri Boroujerdi"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Mathematical Physics (math-ph)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2510.26026",
    "title": "Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation",
    "abstract": "           Reliable uncertainty quantification is crucial for reinforcement learning (RL) in high-stakes settings. We propose a unified conformal prediction framework for infinite-horizon policy evaluation that constructs distribution-free prediction intervals {for returns} in both on-policy and off-policy settings. Our method integrates distributional RL with conformal calibration, addressing challenges such as unobserved returns, temporal dependencies, and distributional shifts. We propose a modular pseudo-return construction based on truncated rollouts and a time-aware calibration strategy using experience replay and weighted subsampling. These innovations mitigate model bias and restore approximate exchangeability, enabling uncertainty quantification even under policy shifts. Our theoretical analysis provides coverage guarantees that account for model misspecification and importance weight estimation. Empirical results, including experiments in synthetic and benchmark environments like Mountain Car, show that our method significantly improves coverage and reliability over standard distributional RL baselines.         ",
    "url": "https://arxiv.org/abs/2510.26026",
    "authors": [
      "Feichen Gan",
      "Youcun Lu",
      "Yingying Zhang",
      "Yukun Liu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26061",
    "title": "Data-driven Projection Generation for Efficiently Solving Heterogeneous Quadratic Programming Problems",
    "abstract": "           We propose a data-driven framework for efficiently solving quadratic programming (QP) problems by reducing the number of variables in high-dimensional QPs using instance-specific projection. A graph neural network-based model is designed to generate projections tailored to each QP instance, enabling us to produce high-quality solutions even for previously unseen problems. The model is trained on heterogeneous QPs to minimize the expected objective value evaluated on the projected solutions. This is formulated as a bilevel optimization problem; the inner optimization solves the QP under a given projection using a QP solver, while the outer optimization updates the model parameters. We develop an efficient algorithm to solve this bilevel optimization problem, which computes parameter gradients without backpropagating through the solver. We provide a theoretical analysis of the generalization ability of solving QPs with projection matrices generated by neural networks. Experimental results demonstrate that our method produces high-quality feasible solutions with reduced computation time, outperforming existing methods.         ",
    "url": "https://arxiv.org/abs/2510.26061",
    "authors": [
      "Tomoharu Iwata",
      "Futoshi Futami"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2510.26097",
    "title": "Robust Super-Capacity SRS Channel Inpainting via Diffusion Models",
    "abstract": "           Accurate channel state information (CSI) is essential for reliable multiuser MIMO operation. In 5G NR, reciprocity-based beamforming via uplink Sounding Reference Signals (SRS) face resource and coverage constraints, motivating sparse non-uniform SRS allocation. Prior masked-autoencoder (MAE) approaches improve coverage but overfit to training masks and degrade under unseen distortions (e.g., additional masking, interference, clipping, non-Gaussian noise). We propose a diffusion-based channel inpainting framework that integrates system-model knowledge at inference via a likelihood-gradient term, enabling a single trained model to adapt across mismatched conditions. On standardized CDL channels, the score-based diffusion variant consistently outperforms a UNet score-model baseline and the one-step MAE under distribution shift, with improvements up to 14 dB NMSE in challenging settings (e.g., Laplace noise, user interference), while retaining competitive accuracy under matched conditions. These results demonstrate that diffusion-guided inpainting is a robust and generalizable approach for super-capacity SRS design in 5G NR systems.         ",
    "url": "https://arxiv.org/abs/2510.26097",
    "authors": [
      "Usman Akram",
      "Fan Zhang",
      "Yang Li",
      "Haris Vikalo"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26204",
    "title": "Sequential Change Detection Under A Markov Setup With Unknown Pre-Change and Post-Change Distributions",
    "abstract": "           In this work we extend the results developed in 2022 for a sequential change detection algorithm making use of Page's CUSUM statistic, the empirical distribution as an estimate of the pre-change distribution, and a universal code as a tool for estimating the post-change distribution, from the i.i.d. case to the Markov setup.         ",
    "url": "https://arxiv.org/abs/2510.26204",
    "authors": [
      "Ashish Bhoopesh Gulaguli",
      "Shashwat Singh",
      "Rakesh Kumar Bansal"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2510.26390",
    "title": "SPG-CDENet: Spatial Prior-Guided Cross Dual Encoder Network for Multi-Organ Segmentation",
    "abstract": "           Multi-organ segmentation is a critical task in computer-aided diagnosis. While recent deep learning methods have achieved remarkable success in image segmentation, huge variations in organ size and shape challenge their effectiveness in multi-organ segmentation. To address these challenges, we propose a Spatial Prior-Guided Cross Dual Encoder Network (SPG-CDENet), a novel two-stage segmentation paradigm designed to improve multi-organ segmentation accuracy. Our SPG-CDENet consists of two key components: a spatial prior network and a cross dual encoder network. The prior network generates coarse localization maps that delineate the approximate ROI, serving as spatial guidance for the dual encoder network. The cross dual encoder network comprises four essential components: a global encoder, a local encoder, a symmetric cross-attention module, and a flow-based decoder. The global encoder captures global semantic features from the entire image, while the local encoder focuses on features from the prior network. To enhance the interaction between the global and local encoders, a symmetric cross-attention module is proposed across all layers of the encoders to fuse and refine features. Furthermore, the flow-based decoder directly propagates high-level semantic features from the final encoder layer to all decoder layers, maximizing feature preservation and utilization. Extensive qualitative and quantitative experiments on two public datasets demonstrate the superior performance of SPG-CDENet compared to existing segmentation methods. Furthermore, ablation studies further validate the effectiveness of the proposed modules in improving segmentation accuracy.         ",
    "url": "https://arxiv.org/abs/2510.26390",
    "authors": [
      "Xizhi Tian",
      "Changjun Zhou",
      "Yulin. Yang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.26401",
    "title": "Multi-Output Robust and Conjugate Gaussian Processes",
    "abstract": "           Multi-output Gaussian process (MOGP) regression allows modelling dependencies among multiple correlated response variables. Similarly to standard Gaussian processes, MOGPs are sensitive to model misspecification and outliers, which can distort predictions within individual outputs. This situation can be further exacerbated by multiple anomalous response variables whose errors propagate due to correlations between outputs. To handle this situation, we extend and generalise the robust and conjugate Gaussian process (RCGP) framework introduced by Altamirano et al. (2024). This results in the multi-output RCGP (MO-RCGP): a provably robust MOGP that is conjugate, and jointly captures correlations across outputs. We thoroughly evaluate our approach through applications in finance and cancer research.         ",
    "url": "https://arxiv.org/abs/2510.26401",
    "authors": [
      "Joshua Rooijakkers",
      "Leiv R\u00f8nneberg",
      "Fran\u00e7ois-Xavier Briol",
      "Jeremias Knoblauch",
      "Matias Altamirano"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26565",
    "title": "Tackling the Challenges of Adding Pulse-level Support to a Heterogeneous HPCQC Software Stack: MQSS Pulse",
    "abstract": "           We study the problem of adding native pulse-level control to heterogeneous High Performance Computing-Quantum Computing (HPCQC) software stacks, using the Munich Quantum Software Stack (MQSS) as a case study. The goal is to expand the capabilities of HPCQC environments by offering the ability for low-level access and control, currently typically not foreseen for such hybrid systems. For this, we need to establish new interfaces that integrate such pulse-level control into the lower layers of the software stack, including the need for proper representation. Pulse-level quantum programs can be fully described with only three low-level abstractions: ports (input/output channels), frames (reference signals), and waveforms (pulse envelopes). We identify four key challenges to represent those pulse abstractions at: the user-interface level, at the compiler level (including the Intermediate Representation (IR)), and at the backend-interface level (including the appropriate exchange format). For each challenge, we propose concrete solutions in the context of MQSS. These include introducing a compiled (C/C++) pulse Application Programming Interface (API) to overcome Python runtime overhead, extending its LLVM support to include pulse-related instructions, using its C-based backend interface to query relevant hardware constraints, and designing a portable exchange format for pulse sequences. Our integrated approach provides an end-to-end path for pulse-aware compilation and runtime execution in HPCQC environments. This work lays out the architectural blueprint for extending HPCQC integration to support pulse-level quantum operations without disrupting state-of-the-art classical workflows.         ",
    "url": "https://arxiv.org/abs/2510.26565",
    "authors": [
      "Jorge Echavarria",
      "Muhammad Nufail Farooqi",
      "Amit Devra",
      "Santana Lujan",
      "L\u00e9o Van Damme",
      "Hossam Ahmed",
      "Mart\u00edn Letras",
      "Erc\u00fcment Kaya",
      "Adrian Vetter",
      "Max Werninghaus",
      "Martin Knudsen",
      "Felix Rohde",
      "Albert Frisch",
      "Eric Mansfield",
      "Rakhim Davletkaliyev",
      "Vladimir Kukushkin",
      "Noora F\u00e4rkkil\u00e4",
      "Janne M\u00e4ntyl\u00e4",
      "Nikolas Pomplun",
      "Andreas Sp\u00f6rl",
      "Lukas Burgholzer",
      "Yannick Stade",
      "Robert Wille",
      "Laura B. Schulz",
      "Martin Schulz"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2510.26597",
    "title": "Bijections Between Smirnov Words and Hamiltonian Cycles in Complete Multipartite Graphs",
    "abstract": "           We establish a bijective correspondence between Smirnov words with balanced letter multiplicities and Hamiltonian paths in complete $m$-partite graphs $K_{n,n,\\ldots,n}$. This bijection allows us to derive closed inclusion-exclusion formulas for the number of Hamiltonian cycles in such graphs. We further extend the enumeration to the generalized nonuniform case $K_{n_1,n_2,\\ldots,n_m}$. We also provide an asymptotic analysis based on Stirling's approximation, which yields compact factorial expressions and logarithmic expansions describing the growth of the number of Hamiltonian cycles in the considered graphs. Our approach unifies the combinatorial study of adjacency-constrained words and the enumeration of Hamiltonian cycles within a single analytical framework.         ",
    "url": "https://arxiv.org/abs/2510.26597",
    "authors": [
      "El-Mehdi Mehiri"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2510.26700",
    "title": "Assessment of the conditional exchangeability assumption in causal machine learning models: a simulation study",
    "abstract": "           Observational studies developing causal machine learning (ML) models for the prediction of individualized treatment effects (ITEs) seldom conduct empirical evaluations to assess the conditional exchangeability assumption. We aimed to evaluate the performance of these models under conditional exchangeability violations and the utility of negative control outcomes (NCOs) as a diagnostic. We conducted a simulation study to examine confounding bias in ITE estimates generated by causal forest and X-learner models under varying conditions, including the presence or absence of true heterogeneity. We simulated data to reflect real-world scenarios with differing levels of confounding, sample size, and NCO confounding structures. We then estimated and compared subgroup-level treatment effects on the primary outcome and NCOs across settings with and without unmeasured confounding. When conditional exchangeability was violated, causal forest and X-learner models failed to recover true treatment effect heterogeneity and, in some cases, falsely indicated heterogeneity when there was none. NCOs successfully identified subgroups affected by unmeasured confounding. Even when NCOs did not perfectly satisfy its ideal assumptions, it remained informative, flagging potential bias in subgroup level estimates, though not always pinpointing the subgroup with the largest confounding. Violations of conditional exchangeability substantially limit the validity of ITE estimates from causal ML models in routinely collected observational data. NCOs serve a useful empirical diagnostic tool for detecting subgroup-specific unmeasured confounding and should be incorporated into causal ML workflows to support the credibility of individualized inference.         ",
    "url": "https://arxiv.org/abs/2510.26700",
    "authors": [
      "Gerard T. Portela",
      "Jason B. Gibbons",
      "Sebastian Schneeweiss",
      "Rishi J. Desai"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.26703",
    "title": "ProstNFound+: A Prospective Study using Medical Foundation Models for Prostate Cancer Detection",
    "abstract": "           Purpose: Medical foundation models (FMs) offer a path to build high-performance diagnostic systems. However, their application to prostate cancer (PCa) detection from micro-ultrasound ({\\mu}US) remains untested in clinical settings. We present ProstNFound+, an adaptation of FMs for PCa detection from {\\mu}US, along with its first prospective validation. Methods: ProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt encoder that embeds PCa-specific clinical biomarkers. The model generates a cancer heatmap and a risk score for clinically significant PCa. Following training on multi-center retrospective data, the model is prospectively evaluated on data acquired five years later from a new clinical site. Model predictions are benchmarked against standard clinical scoring protocols (PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the prospective data, with no performance degradation compared to retrospective evaluation. It aligns closely with clinical scores and produces interpretable heatmaps consistent with biopsy-confirmed lesions. Conclusion: The results highlight its potential for clinical deployment, offering a scalable and interpretable alternative to expert-driven protocols.         ",
    "url": "https://arxiv.org/abs/2510.26703",
    "authors": [
      "Paul F. R. Wilson",
      "Mohamed Harmanani",
      "Minh Nguyen Nhat To",
      "Amoon Jamzad",
      "Tarek Elghareb",
      "Zhuoxin Guo",
      "Adam Kinnaird",
      "Brian Wodlinger",
      "Purang Abolmaesumi",
      "Parvin Mousavi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.26783",
    "title": "A Unified Theory for Causal Inference: Direct Debiased Machine Learning via Bregman-Riesz Regression",
    "abstract": "           This note introduces a unified theory for causal inference that integrates Riesz regression, covariate balancing, density-ratio estimation (DRE), targeted maximum likelihood estimation (TMLE), and the matching estimator in average treatment effect (ATE) estimation. In ATE estimation, the balancing weights and the regression functions of the outcome play important roles, where the balancing weights are referred to as the Riesz representer, bias-correction term, and clever covariates, depending on the context. Riesz regression, covariate balancing, DRE, and the matching estimator are methods for estimating the balancing weights, where Riesz regression is essentially equivalent to DRE in the ATE context, the matching estimator is a special case of DRE, and DRE is in a dual relationship with covariate balancing. TMLE is a method for constructing regression function estimators such that the leading bias term becomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density Ratio Estimation and Riesz Regression.         ",
    "url": "https://arxiv.org/abs/2510.26783",
    "authors": [
      "Masahiro Kato"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2205.13503",
    "title": "Multi-layer State Evolution Under Random Convolutional Design",
    "abstract": "           Signal recovery under generative neural network priors has emerged as a promising direction in statistical inference and computational imaging. Theoretical analysis of reconstruction algorithms under generative priors is, however, challenging. For generative priors with fully connected layers and Gaussian i.i.d. weights, this was achieved by the multi-layer approximate message (ML-AMP) algorithm via a rigorous state evolution. However, practical generative priors are typically convolutional, allowing for computational benefits and inductive biases, and so the Gaussian i.i.d. weight assumption is very limiting. In this paper, we overcome this limitation and establish the state evolution of ML-AMP for random convolutional layers. We prove in particular that random convolutional layers belong to the same universality class as Gaussian matrices. Our proof technique is of an independent interest as it establishes a mapping between convolutional matrices and spatially coupled sensing matrices used in coding theory.         ",
    "url": "https://arxiv.org/abs/2205.13503",
    "authors": [
      "Mara Daniels",
      "C\u00e9dric Gerbelot",
      "Florent Krzakala",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2208.08083",
    "title": "Two Heads are Better than One: Robust Learning Meets Multi-branch Models",
    "abstract": "           Deep neural networks (DNNs) are vulnerable to adversarial examples, in which DNNs are misled to false outputs due to inputs containing imperceptible perturbations. Adversarial training, a reliable and effective method of defense, may significantly reduce the vulnerability of neural networks and becomes the de facto standard for robust learning. While many recent works practice the data-centric philosophy, such as how to generate better adversarial examples or use generative models to produce additional training data, we look back to the models themselves and revisit the adversarial robustness from the perspective of deep feature distribution as an insightful complementarity. In this paper, we propose \\textit{Branch Orthogonality adveRsarial Training} (BORT) to obtain state-of-the-art performance with solely the original dataset for adversarial training. To practice our design idea of integrating multiple orthogonal solution spaces, we leverage a simple and straightforward multi-branch neural network that eclipses adversarial attacks with no increase in inference time. We heuristically propose a corresponding loss function, branch-orthogonal loss, to make each solution space of the multi-branch model orthogonal. We evaluate our approach on CIFAR-10, CIFAR-100 and SVHN against $\\ell_{\\infty}$ norm-bounded perturbations of size $\\epsilon = 8/255$, respectively. Exhaustive experiments are conducted to show that our method goes beyond all state-of-the-art methods without any tricks. Compared to all methods that do not use additional data for training, our models achieve 67.3\\% and 41.5\\% robust accuracy on CIFAR-10 and CIFAR-100 (improving upon the state-of-the-art by +7.23\\% and +9.07\\%). We also outperform methods using a training set with a far larger scale than ours.         ",
    "url": "https://arxiv.org/abs/2208.08083",
    "authors": [
      "Zongyuan Zhang",
      "Qingwen Bu",
      "Tianyang Duan",
      "Zheng Lin",
      "Yuhao Qing",
      "Zihan Fang",
      "Heming Cui",
      "Dong Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2311.07734",
    "title": "Quality-Aware Prototype Memory for Face Representation Learning",
    "abstract": "           Prototype Memory is a powerful model for face representation learning. It enables training face recognition models on datasets of any size by generating prototypes (classifier weights) on the fly and efficiently utilizing them. Prototype Memory demonstrated strong results in many face recognition benchmarks. However, the algorithm of prototype generation, used in it, is prone to the problems of imperfectly calculated prototypes in case of low-quality or poorly recognizable faces in the images, selected for the prototype creation. All images of the same person presented in the mini-batch are used with equal weights, and the resulting averaged prototype can be contaminated by imperfect embeddings of low-quality face images. This may lead to misleading training signals and degrade the performance of the trained models. In this paper, we propose a simple and effective way to improve Prototype Memory with quality-aware prototype generation. Quality-Aware Prototype Memory uses different weights for images of different quality in the process of prototype generation. With this improvement, prototypes receive more informative signals from high-quality images and are less affected by low-quality ones. We propose and compare several methods of quality estimation and usage, perform extensive experiments on the different face recognition benchmarks and demonstrate the advantages of the proposed model compared to the basic version of Prototype Memory.         ",
    "url": "https://arxiv.org/abs/2311.07734",
    "authors": [
      "Evgeny Smirnov",
      "Vasiliy Galyuk",
      "Evgeny Lukyanets"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2311.17434",
    "title": "GSE: Group-wise Sparse and Explainable Adversarial Attacks",
    "abstract": "           Sparse adversarial attacks fool deep neural networks (DNNs) through minimal pixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts have replaced this norm with a structural sparsity regularizer, such as the nuclear group norm, to craft group-wise sparse adversarial attacks. The resulting perturbations are thus explainable and hold significant practical relevance, shedding light on an even greater vulnerability of DNNs. However, crafting such attacks poses an optimization challenge, as it involves computing norms for groups of pixels within a non-convex objective. We address this by presenting a two-phase algorithm that generates group-wise sparse attacks within semantically meaningful areas of an image. Initially, we optimize a quasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored for non-convex programming. Subsequently, the algorithm transitions to a projected Nesterov's accelerated gradient descent with $2-$norm regularization applied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and ImageNet datasets demonstrate a remarkable increase in group-wise sparsity, e.g., $50.9\\%$ on CIFAR-10 and $38.4\\%$ on ImageNet (average case, targeted attack). This performance improvement is accompanied by significantly faster computation times, improved explainability, and a $100\\%$ attack success rate.         ",
    "url": "https://arxiv.org/abs/2311.17434",
    "authors": [
      "Shpresim Sadiku",
      "Moritz Wagner",
      "Sebastian Pokutta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2403.08788",
    "title": "VerifIoU - Robustness of Object Detection to Perturbations",
    "abstract": "           We introduce a novel Interval Bound Propagation (IBP) approach for the formal verification of object detection models, specifically targeting the Intersection over Union (IoU) metric. The approach has been implemented in an open source code, named IBP IoU, compatible with popular abstract interpretation based verification tools. The resulting verifier is evaluated on landing approach runway detection and handwritten digit recognition case studies. Comparisons against a baseline (Vanilla IBP IoU) highlight the superior performance of IBP IoU in ensuring accuracy and stability, contributing to more secure and robust machine learning applications.         ",
    "url": "https://arxiv.org/abs/2403.08788",
    "authors": [
      "No\u00e9mie Cohen",
      "M\u00e9lanie Ducoffe",
      "Ryma Boumazouza",
      "Christophe Gabreau",
      "Claire Pagetti",
      "Xavier Pucel",
      "Audrey Galametz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2406.05948",
    "title": "Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models",
    "abstract": "           Large Language Models (LLMs), especially those accessed via APIs, have demonstrated impressive capabilities across various domains. However, users without technical expertise often turn to (untrustworthy) third-party services, such as prompt engineering, to enhance their LLM experience, creating vulnerabilities to adversarial threats like backdoor attacks. Backdoor-compromised LLMs generate malicious outputs to users when inputs contain specific \"triggers\" set by attackers. Traditional defense strategies, originally designed for small-scale models, are impractical for API-accessible LLMs due to limited model access, high computational costs, and data requirements. To address these limitations, we propose Chain-of-Scrutiny (CoS) which leverages LLMs' unique reasoning abilities to mitigate backdoor attacks. It guides the LLM to generate reasoning steps for a given input and scrutinizes for consistency with the final output -- any inconsistencies indicating a potential attack. It is well-suited for the popular API-only LLM deployments, enabling detection at minimal cost and with little data. User-friendly and driven by natural language, it allows non-experts to perform the defense independently while maintaining transparency. We validate the effectiveness of CoS through extensive experiments on various tasks and LLMs, with results showing greater benefits for more powerful LLMs.         ",
    "url": "https://arxiv.org/abs/2406.05948",
    "authors": [
      "Xi Li",
      "Ruofan Mao",
      "Yusen Zhang",
      "Renze Lou",
      "Chen Wu",
      "Jiaqi Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.08525",
    "title": "A mathematical certification for positivity conditions in Neural Networks with applications to partial monotonicity and Trustworthy AI",
    "abstract": "           Artificial Neural Networks (ANNs) have become a powerful tool for modeling complex relationships in large-scale datasets. However, their black-box nature poses trustworthiness challenges. In certain situations, ensuring trust in predictions might require following specific partial monotonicity constraints. However, certifying if an already-trained ANN is partially monotonic is challenging. Therefore, ANNs are often disregarded in some critical applications, such as credit scoring, where partial monotonicity is required. To address this challenge, this paper presents a novel algorithm (LipVor) that certifies if a black-box model, such as an ANN, is positive based on a finite number of evaluations. Consequently, since partial monotonicity can be expressed as a positivity condition on partial derivatives, LipVor can certify whether an ANN is partially monotonic. To do so, for every positively evaluated point, the Lipschitzianity of the black-box model is used to construct a specific neighborhood where the function remains positive. Next, based on the Voronoi diagram of the evaluated points, a sufficient condition is stated to certify if the function is positive in the domain. Unlike prior methods, our approach certifies partial monotonicity without constrained architectures or piece-wise linear activations. Therefore, LipVor could open up the possibility of using unconstrained ANN in some critical fields. Moreover, some other properties of an ANN, such as convexity, can be posed as positivity conditions, and therefore, LipVor could also be applied.         ",
    "url": "https://arxiv.org/abs/2406.08525",
    "authors": [
      "Alejandro Polo-Molina",
      "David Alfaya",
      "Jose Portela"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.15863",
    "title": "EmoAttack: Emotion-to-Image Diffusion Models for Emotional Backdoor Generation",
    "abstract": "           Text-to-image diffusion models can generate realistic images based on textual inputs, enabling users to convey their opinions visually through language. Meanwhile, within language, emotion plays a crucial role in expressing personal opinions in our daily lives and the inclusion of maliciously negative content can lead users astray, exacerbating negative emotions. Recognizing the success of diffusion models and the significance of emotion, we investigate a previously overlooked risk associated with text-to-image diffusion models, that is, utilizing emotion in the input texts to introduce negative content and provoke unfavorable emotions in users. Specifically, we identify a new backdoor attack, i.e., emotion-aware backdoor attack (EmoAttack), which introduces malicious negative content triggered by emotional texts during image generation. We formulate such an attack as a diffusion personalization problem to avoid extensive model retraining and propose the EmoBooth. Unlike existing personalization methods, our approach fine-tunes a pre-trained diffusion model by establishing a mapping between a cluster of emotional words and a given reference image containing malicious negative content. To validate the effectiveness of our method, we built a dataset and conducted extensive analysis and discussion about its effectiveness. Given consumers' widespread use of diffusion models, uncovering this threat is critical for society.         ",
    "url": "https://arxiv.org/abs/2406.15863",
    "authors": [
      "Tianyu Wei",
      "Shanmin Pang",
      "Qi Guo",
      "Yizhuo Ma",
      "Xiaofeng Cao",
      "Qing Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.17489",
    "title": "AI's Social Forcefield: Reshaping Distributed Cognition in Human-AI Teams",
    "abstract": "           AI is not only a neutral tool in team settings; it actively reshapes the social and cognitive fabric of collaboration. We advance a unified framework of alignment in distributed cognition in human-AI teams -- a process through which linguistic, cognitive, and social coordination emerge as human and AI agents co-construct a shared representational space. Across two studies, we show that exposure to AI-generated language shapes not only how people speak, but also how they think, what they attend to, and how they relate to each other. Together, these findings reveal how AI participation reorganizes the distributed cognitive architecture of teams: AI systems function as implicit social forcefields. Our findings highlight the double-edged impact of AI: the same mechanisms that enable efficient collaboration can also erode epistemic diversity and undermine natural alignment processes. We argue for rethinking AI in teams as a socially influential actor and call for new design paradigms that foreground transparency, controllability, and group-level dynamics to foster responsible, productive human-AI collaboration.         ",
    "url": "https://arxiv.org/abs/2407.17489",
    "authors": [
      "Christoph Riedl",
      "Saiph Savage",
      "Josie Zvelebilova"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "General Economics (econ.GN)"
    ]
  },
  {
    "id": "arXiv:2407.18760",
    "title": "Maven-Hijack: Software Supply Chain Attack Exploiting Packaging Order",
    "abstract": "           Java projects frequently rely on package managers such as Maven to manage complex webs of external dependencies. While these tools streamline development, they also introduce subtle risks to the software supply chain. In this paper, we present Maven-Hijack, a novel attack that exploits the order in which Maven packages dependencies and the way the Java Virtual Machine resolves classes at runtime. By injecting a malicious class with the same fully qualified name as a legitimate one into a dependency that is packaged earlier, an attacker can silently override core application behavior without modifying the main codebase or library names. We demonstrate the real-world feasibility of this attack by compromising the Corona-Warn-App, a widely used open-source COVID-19 contact tracing system, and gaining control over its database connection logic. We evaluate three mitigation strategies, such as sealed JARs, Java Modules, and the Maven Enforcer plugin. Our results show that, while Java Modules offer strong protection, the Maven Enforcer plugin with duplicate class detection provides the most practical and effective defense for current Java projects. These findings highlight the urgent need for improved safeguards in Java's build and dependency management processes to prevent stealthy supply chain attacks.         ",
    "url": "https://arxiv.org/abs/2407.18760",
    "authors": [
      "Frank Reyes",
      "Federico Bono",
      "Aman Sharma",
      "Benoit Baudry",
      "Martin Monperrus"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2408.03534",
    "title": "Neural active manifolds: nonlinear dimensionality reduction for uncertainty quantification",
    "abstract": "           We present a new approach for nonlinear dimensionality reduction, specifically designed for computationally expensive mathematical models. We leverage autoencoders to discover a one-dimensional neural active manifold (NeurAM) capturing the model output variability, through the aid of a simultaneously learnt surrogate model with inputs on this manifold. Our method only relies on model evaluations and does not require the knowledge of gradients. The proposed dimensionality reduction framework can then be applied to assist outer loop many-query tasks in scientific computing, like sensitivity analysis and multifidelity uncertainty propagation. In particular, we prove, both theoretically under idealized conditions, and numerically in challenging test cases, how NeurAM can be used to obtain multifidelity sampling estimators with reduced variance by sampling the models on the discovered low-dimensional and shared manifold among models. Several numerical examples illustrate the main features of the proposed dimensionality reduction strategy and highlight its advantages with respect to existing approaches in the literature.         ",
    "url": "https://arxiv.org/abs/2408.03534",
    "authors": [
      "Andrea Zanoni",
      "Gianluca Geraci",
      "Matteo Salvador",
      "Alison L. Marsden",
      "Daniele E. Schiavazzi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2408.08493",
    "title": "Parallel Unlearning in Inherited Model Networks",
    "abstract": "           Unlearning is challenging in generic learning frameworks with the continuous growth and updates of models exhibiting complex inheritance relationships. This paper presents a novel unlearning framework that enables fully parallel unlearning among models exhibiting inheritance. We use a chronologically Directed Acyclic Graph (DAG) to capture various unlearning scenarios occurring in model inheritance networks. Central to our framework is the Fisher Inheritance Unlearning (FIUn) method, designed to enable efficient parallel unlearning within the DAG. FIUn utilizes the Fisher Information Matrix (FIM) to assess the significance of model parameters for unlearning tasks and adjusts them accordingly. To handle multiple unlearning requests simultaneously, we propose the Merging-FIM (MFIM) function, which consolidates FIMs from multiple upstream models into a unified matrix. This design supports all unlearning scenarios captured by the DAG, enabling one-shot removal of inherited knowledge while significantly reducing computational overhead. Experiments confirm the effectiveness of our unlearning framework. For single-class tasks, it achieves complete unlearning with 0% accuracy for unlearned labels while maintaining 94.53% accuracy for retained labels. For multi-class tasks, the accuracy is 1.07% for unlearned labels and 84.77% for retained labels. Our framework accelerates unlearning by 99% compared to alternative methods. Code is in this https URL.         ",
    "url": "https://arxiv.org/abs/2408.08493",
    "authors": [
      "Xiao Liu",
      "Mingyuan Li",
      "Guangsheng Yu",
      "Lixiang Li",
      "Haipeng Peng",
      "Ren Ping Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2409.06263",
    "title": "Speak & Spell: LLM-Driven Controllable Phonetic Error Augmentation for Robust Dialogue State Tracking",
    "abstract": "           Dialogue State Tracking (DST) is a key part of task-oriented dialogue systems, identifying important information in conversations. However, its accuracy drops significantly in spoken dialogue environments due to named entity errors from Automatic Speech Recognition (ASR) systems. We introduce a simple yet effective data augmentation method that targets those entities to improve the robustness of DST model. Our novel method can control the placement of errors using keyword-highlighted prompts while introducing phonetically similar errors. As a result, our method generated sufficient error patterns on keywords, leading to improved accuracy in noised and low-accuracy ASR environments.         ",
    "url": "https://arxiv.org/abs/2409.06263",
    "authors": [
      "Jihyun Lee",
      "Solee Im",
      "Wonjun Lee",
      "Gary Geunbae Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.03364",
    "title": "Unified Error Correction Code Transformer with Low Complexity",
    "abstract": "           Channel coding is vital for reliable sixth-generation (6G) data transmission, employing diverse error correction codes for various application scenarios. Traditional decoders require dedicated hardware for each code, leading to high hardware costs. Recently, artificial intelligence (AI)-driven approaches, such as the error correction code Transformer (ECCT) and its enhanced version, the foundation error correction code Transformer (FECCT), have been proposed to reduce the hardware cost by leveraging the Transformer to decode multiple codes. However, their excessively high computational complexity of $\\mathcal{O}(N^2)$ due to the self-attention mechanism in the Transformer limits scalability, where $N$ represents the sequence length. To reduce computational complexity, we propose a unified Transformer-based decoder that handles multiple linear block codes within a single framework. Specifically, a standardized unit is employed to align code length and code rate across different code types, while a redesigned low-rank unified attention module, with computational complexity of $\\mathcal{O}(N)$, is shared across various heads in the Transformer. Additionally, a sparse mask, derived from the parity-check matrix's sparsity, is introduced to enhance the decoder's ability to capture inherent constraints between information and parity-check bits, improving decoding accuracy and further reducing computational complexity by $86\\%$. Extensive experimental results demonstrate that the proposed unified Transformer-based decoder outperforms existing methods and provides a high-performance, low-complexity solution for next-generation wireless communication systems.         ",
    "url": "https://arxiv.org/abs/2410.03364",
    "authors": [
      "Yongli Yan",
      "Jieao Zhu",
      "Tianyue Zheng",
      "Zhuo Xu",
      "Chao Jiang",
      "Linglong Dai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.21004",
    "title": "A Continuous and Interpretable Morphometric for Robust Quantification of Dynamic Biological Shapes",
    "abstract": "           We introduce the Push-Forward Signed Distance Morphometric (PF-SDM) for shape quantification in biomedical imaging. The PF-SDM compactly encodes geometric and topological properties of closed shapes, including their skeleton and symmetries. This provides robust and interpretable features for shape comparison and machine learning. The PF-SDM is mathematically smooth, providing access to gradients and differential-geometric quantities. It also extends to temporal dynamics and allows fusing spatial intensity distributions, such as genetic markers, with shape dynamics. We present the PF-SDM theory, benchmark it on synthetic data, and apply it to predicting body-axis formation in mouse gastruloids, outperforming a CNN baseline in both accuracy and speed.         ",
    "url": "https://arxiv.org/abs/2410.21004",
    "authors": [
      "Roua Rouatbi",
      "Juan-Esteban Suarez Cardona",
      "Alba Villaronga-Luque",
      "Jesse V. Veenvliet",
      "Ivo F. Sbalzarini"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computational Geometry (cs.CG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2411.12052",
    "title": "HoGA: Higher-Order Graph Attention via Diversity-Aware k-Hop Sampling",
    "abstract": "           Graphs model latent variable relationships in many real-world systems, and Message Passing Neural Networks (MPNNs) are widely used to learn such structures for downstream tasks. While edge-based MPNNs effectively capture local interactions, their expressive power is theoretically bounded, limiting the discovery of higher-order relationships. We introduce the Higher-Order Graph Attention (HoGA) module, which constructs a k-order attention matrix by sampling subgraphs to maximize diversity among feature vectors. Unlike existing higher-order attention methods that greedily resample similar k-order relationships, HoGA targets diverse modalities in higher-order topology, reducing redundancy and expanding the range of captured substructures. Applied to two single-hop attention models, HoGA achieves at least a 5% accuracy gain on all benchmark node classification datasets and outperforms recent baselines on six of eight datasets. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.12052",
    "authors": [
      "Thomas Bailie",
      "Yun Sing Koh",
      "Karthik Mukkavilli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.17883",
    "title": "In Defence of Post-hoc Explainability",
    "abstract": "           This position paper defends post-hoc explainability methods as legitimate tools for scientific knowledge production in machine learning. Addressing criticism of these methods' reliability and epistemic status, we develop a philosophical framework grounded in mediated understanding and bounded factivity. We argue that scientific insights can emerge through structured interpretation of model behaviour without requiring complete mechanistic transparency, provided explanations acknowledge their approximative nature and undergo rigorous empirical validation. Through analysis of recent biomedical ML applications, we demonstrate how post-hoc methods, when properly integrated into scientific practice, generate novel hypotheses and advance phenomenal understanding.         ",
    "url": "https://arxiv.org/abs/2412.17883",
    "authors": [
      "Nick Oh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.04236",
    "title": "SHARE: Optimizing Secure Hub Allocation and Routing Efficiency in Payment Channel Networks",
    "abstract": "           Payment channel hub (PCH), by leveraging a powerful hub to reliably provide off-chain payment services, offers an effective enhancement to payment channel networks (PCNs). However, existing approaches typically rely on a single hub to relay transactions and provide relationship anonymity between participants. This design lacks flexibility under high-frequency transaction scenarios and fail to adequately balance the security of off-chain payments with PCH efficiency. Moreover, current PCNs often adopt source routing, where each transaction path is predetermined without considering the dynamic distribution of large-scale payment requests, leading to load imbalance and even transaction deadlocks. To address these issues, we propose SHARE, a multi-PCH distributed routing scheme based on trusted execution environments (TEE), designed to optimize secure hub allocation and routing efficiency in PCNs. For the multi-hub allocation problem, SHARE balances the management and synchronization costs among participants, and employs mixed-integer linear programming along with supermodular optimization techniques to transform the NP-hard problem into a solvable form, enabling optimal or approximate solutions across various PCN scales. At the routing layer, SHARE integrates global network state with local sender requests to design a TEE-assisted, privacy-preserving distributed routing protocol that dynamically adjusts multipath flow rates, achieving high-throughput and deadlock-free transaction forwarding. We formally prove the security of the SHARE protocol under the universally composable framework. Experimental results demonstrate that SHARE achieves a 43.6% improvement in transaction success ratio and an over 181.5% enhancement in system throughput compared to state-of-the-art PCN solutions, effectively realizing a secure extension of PCNs.         ",
    "url": "https://arxiv.org/abs/2501.04236",
    "authors": [
      "Lingxiao Yang",
      "Xuewen Dong",
      "Wei Wang",
      "Yong Yu",
      "Sheng Gao",
      "Qiang Qu",
      "Yulong Shen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2501.05783",
    "title": "UV-Attack: Physical-World Adversarial Attacks for Person Detection via Dynamic-NeRF-based UV Mapping",
    "abstract": "           In recent research, adversarial attacks on person detectors using patches or static 3D model-based texture modifications have struggled with low success rates due to the flexible nature of human movement. Modeling the 3D deformations caused by various actions has been a major challenge. Fortunately, advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer new possibilities. In this paper, we introduce UV-Attack, a groundbreaking approach that achieves high success rates even with extensive and unseen human actions. We address the challenge above by leveraging dynamic-NeRF-based UV mapping. UV-Attack can generate human images across diverse actions and viewpoints, and even create novel actions by sampling from the SMPL parameter space. While dynamic NeRF models are capable of modeling human bodies, modifying clothing textures is challenging because they are embedded in neural network parameters. To tackle this, UV-Attack generates UV maps instead of RGB images and modifies the texture stacks. This approach enables real-time texture edits and makes the attack more practical. We also propose a novel Expectation over Pose Transformation loss (EoPT) to improve the evasion success rate on unseen poses and views. Our experiments show that UV-Attack achieves a 92.7% attack success rate against the FastRCNN model across varied poses in dynamic video settings, significantly outperforming the state-of-the-art AdvCamou attack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the latest YOLOv8 detector in black-box settings. This work highlights the potential of dynamic NeRF-based UV mapping for creating more effective adversarial attacks on person detectors, addressing key challenges in modeling human movement and texture modification. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2501.05783",
    "authors": [
      "Yanjie Li",
      "Kaisheng Liang",
      "Bin Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.17021",
    "title": "Network Oblivious Transfer via Noisy Channels: Limits and Capacities",
    "abstract": "           In this paper, we aim to study the information-theoretical limits of oblivious transfer. This work also investigates the problem of oblivious transfer over a noisy multiple access channel involving two non-colluding senders and a single receiver. The channel model is characterized by correlations among the parties, with the parties assumed to be either honest-but-curious or, in the receiver's case, potentially malicious. At first, we study the information-theoretical limits of oblivious transfer between two parties and extend it to the multiple access channel model. We propose a multiparty protocol for honest-but-curious parties where the general multiple access channel is reduced to a certain correlation. In scenarios where the receiver is malicious, the protocol achieves an achievable rate region.         ",
    "url": "https://arxiv.org/abs/2501.17021",
    "authors": [
      "Hadi Aghaee",
      "Bahareh Akhbari",
      "Christian Deppe"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.01859",
    "title": "Fully discrete analysis of the Galerkin POD neural network approximation with application to 3D acoustic wave scattering",
    "abstract": "           In this work, we consider the approximation of parametric maps using the so-called Galerkin POD-NN method. This technique combines the computation of a reduced basis via proper orthogonal decomposition (POD) and artificial neural networks (NNs) for the construction of fast surrogates of said parametric maps. In contrast to the existing literature, which has studied the approximation properties of this kind of architecture on a continuous level, we provide a fully discrete error analysis of this approach. More precisely, our estimates also account for discretization errors during the construction of the NN architecture. We consider the number of reduced basis in the approximation of the solution manifold, truncation in the parameter space, and, most importantly, the number of samples in the computation of the reduced space, together with the effect of the use of NNs in the approximation of the reduced coefficients. Following this error analysis, we provide a-priori bounds on the required POD tolerance, the resulting POD ranks, and NN parameters to maintain the order of convergence of quasi Monte Carlo sampling techniques. We conclude this work by showcasing the applicability of this method through a practical industrial application: the sound-soft acoustic scattering problem by a parametrically defined scatterer in three physical dimensions.         ",
    "url": "https://arxiv.org/abs/2502.01859",
    "authors": [
      "J\u00fcrgen D\u00f6lz",
      "Fernando Henr\u00edquez"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2502.09969",
    "title": "Neural Networks for Learnable and Scalable Influence Estimation of Instruction Fine-Tuning Data",
    "abstract": "           Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: this https URL.         ",
    "url": "https://arxiv.org/abs/2502.09969",
    "authors": [
      "Ishika Agarwal",
      "Dilek Hakkani-T\u00fcr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.15475",
    "title": "Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning Solution for Protocols Compliance",
    "abstract": "           Neural network-based decoding methods show promise in enhancing error correction performance but face challenges with punctured codes. In particular, existing methods struggle to adapt to variable code rates or meet protocol compatibility requirements. This paper proposes a unified long short-term memory (LSTM)-based neural decoder for punctured convolutional and Turbo codes to address these challenges. The key component of the proposed LSTM-based neural decoder is puncturing-aware embedding, which integrates puncturing patterns directly into the neural network to enable seamless adaptation to different code rates. Moreover, a balanced bit error rate training strategy is designed to ensure the decoder's robustness across various code lengths, rates, and channels. In this way, the protocol compatibility requirement can be realized. Extensive simulations in both additive white Gaussian noise (AWGN) and Rayleigh fading channels demonstrate that the proposed neural decoder outperforms conventional decoding techniques, offering significant improvements in decoding accuracy and robustness.         ",
    "url": "https://arxiv.org/abs/2502.15475",
    "authors": [
      "Yongli Yan",
      "Linglong Dai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2502.20995",
    "title": "The RAG Paradox: A Black-Box Attack Exploiting Unintentional Vulnerabilities in Retrieval-Augmented Generation Systems",
    "abstract": "           With the growing adoption of retrieval-augmented generation (RAG) systems, various attack methods have been proposed to degrade their performance. However, most existing approaches rely on unrealistic assumptions in which external attackers have access to internal components such as the retriever. To address this issue, we introduce a realistic black-box attack based on the RAG paradox, a structural vulnerability arising from the system's effort to enhance trust by revealing both the retrieved documents and their sources to users. This transparency enables attackers to observe which sources are used and how information is phrased, allowing them to craft poisoned documents that are more likely to be retrieved and upload them to the identified sources. Moreover, as RAG systems directly provide retrieved content to users, these documents must not only be retrievable but also appear natural and credible to maintain user confidence in the search results. Unlike prior work that focuses solely on improving document retrievability, our attack method explicitly considers both retrievability and user trust in the retrieved content. Both offline and online experiments demonstrate that our method significantly degrades system performance without internal access, while generating natural-looking poisoned documents.         ",
    "url": "https://arxiv.org/abs/2502.20995",
    "authors": [
      "Chanwoo Choi",
      "Jinsoo Kim",
      "Sukmin Cho",
      "Soyeong Jeong",
      "Buru Chang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.00333",
    "title": "More of the Same: Persistent Representational Harms Under Increased Representation",
    "abstract": "           To recognize and mitigate the harms of generative AI systems, it is crucial to consider who is represented in the outputs of generative AI systems and how people are represented. A critical gap emerges when naively improving who is represented, as this does not imply bias mitigation efforts have been applied to address how people are represented. We critically examined this by investigating gender representation in occupation across state-of-the-art large language models. We first show evidence suggesting that over time there have been interventions to models altering the resulting gender distribution, and we find that women are more represented than men when models are prompted to generate biographies or personas. We then demonstrate that representational biases persist in how different genders are represented by examining statistically significant word differences across genders. This results in a proliferation of representational harms, stereotypes, and neoliberalism ideals that, despite existing interventions to increase female representation, reinforce existing systems of oppression.         ",
    "url": "https://arxiv.org/abs/2503.00333",
    "authors": [
      "Jennifer Mickel",
      "Maria De-Arteaga",
      "Leqi Liu",
      "Kevin Tian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.04852",
    "title": "CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data",
    "abstract": "           True intelligence hinges on the ability to uncover and leverage hidden causal relations. Despite significant progress in AI and computer vision (CV), there remains a lack of benchmarks for assessing models' abilities to infer latent causality from complex visual data. In this paper, we introduce \\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates structured data (tables) with corresponding visual representations (images) to evaluate causal reasoning. Designed within a systematic framework, Causal3D comprises 19 3D-scene datasets capturing diverse causal relations, views, and backgrounds, enabling evaluations across scenes of varying complexity. We assess multiple state-of-the-art methods, including classical causal discovery, causal representation learning, and large/vision-language models (LLMs/VLMs). Our experiments show that as causal structures grow more complex without prior knowledge, performance declines significantly, highlighting the challenges even advanced methods face in complex causal scenarios. Causal3D serves as a vital resource for advancing causal reasoning in CV and fostering trustworthy AI in critical domains.         ",
    "url": "https://arxiv.org/abs/2503.04852",
    "authors": [
      "Disheng Liu",
      "Yiran Qiao",
      "Wuche Liu",
      "Yiren Lu",
      "Yunlai Zhou",
      "Tuo Liang",
      "Yu Yin",
      "Jing Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.13160",
    "title": "Language-guided Open-world Video Anomaly Detection under Weak Supervision",
    "abstract": "           Video anomaly detection (VAD) aims to detect anomalies that deviate from what is expected. In open-world scenarios, the expected events may change as requirements change. For example, not wearing a mask may be considered abnormal during a flu outbreak but normal otherwise. However, existing methods assume that the definition of anomalies is invariable, and thus are not applicable to the open world. To address this, we propose a novel open-world VAD paradigm with variable definitions, allowing guided detection through user-provided natural language at inference time. This paradigm necessitates establishing a robust mapping from video and textual definition to anomaly scores. Therefore, we propose LaGoVAD (Language-guided Open-world Video Anomaly Detector), a model that dynamically adapts anomaly definitions under weak supervision with two regularization strategies: diversifying the relative durations of anomalies via dynamic video synthesis, and enhancing feature robustness through contrastive learning with negative mining. Training such adaptable models requires diverse anomaly definitions, but existing datasets typically provide labels without semantic descriptions. To bridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the largest and most diverse video anomaly dataset to date, featuring 35,279 annotated videos with multi-level category labels and descriptions that explicitly define anomalies. Zero-shot experiments on seven datasets demonstrate LaGoVAD's SOTA performance. Our dataset and code will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.13160",
    "authors": [
      "Zihao Liu",
      "Xiaoyu Wu",
      "Jianqin Wu",
      "Xuxu Wang",
      "Linlin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05747",
    "title": "SEA-LION: Southeast Asian Languages in One Network",
    "abstract": "           Recently, Large Language Models (LLMs) have dominated much of the artificial intelligence scene with their ability to process and generate natural languages. However, the majority of LLM research and development remains English-centric, leaving low-resource languages such as those in the Southeast Asian (SEA) region under-represented. To address this representation gap, we introduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge multilingual LLMs designed for SEA languages. The SEA-LION family of LLMs supports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese, Malay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages large-scale multilingual continued pre-training with a comprehensive post-training regime involving multiple stages of instruction fine-tuning, alignment, and model merging. Evaluation results on multilingual benchmarks indicate that our models achieve state-of-the-art performance across LLMs supporting SEA languages. We open-source the models to benefit the wider SEA community.         ",
    "url": "https://arxiv.org/abs/2504.05747",
    "authors": [
      "Raymond Ng",
      "Thanh Ngan Nguyen",
      "Yuli Huang",
      "Ngee Chia Tai",
      "Wai Yi Leong",
      "Wei Qi Leong",
      "Xianbin Yong",
      "Jian Gang Ngui",
      "Yosephine Susanto",
      "Nicholas Cheng",
      "Hamsawardhini Rengarajan",
      "Peerat Limkonchotiwat",
      "Adithya Venkatadri Hulagadri",
      "Kok Wai Teng",
      "Yeo Yeow Tong",
      "Bryan Siow",
      "Wei Yi Teo",
      "Wayne Lau",
      "Choon Meng Tan",
      "Brandon Ong",
      "Zhi Hao Ong",
      "Jann Railey Montalan",
      "Adwin Chan",
      "Sajeban Antonyrex",
      "Ren Lee",
      "Esther Choa",
      "David Ong Tat-Wee",
      "Bing Jie Darius Liu",
      "William Chandra Tjhi",
      "Erik Cambria",
      "Leslie Teo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.10456",
    "title": "Privacy-Preserving Distributed Link Predictions Among Peers in Online Classrooms Using Federated Learning",
    "abstract": "           Social interactions among classroom peers, represented as social learning networks (SLNs), play a crucial role in enhancing learning outcomes. While SLN analysis has recently garnered attention, most existing approaches rely on centralized training, where data is aggregated and processed on a local/cloud server with direct access to raw data. However, in real-world educational settings, such direct access across multiple classrooms is often restricted due to privacy concerns. Furthermore, training models on isolated classroom data prevents the identification of common interaction patterns that exist across multiple classrooms, thereby limiting model performance. To address these challenges, we propose one of the first frameworks that integrates Federated Learning (FL), a distributed and collaborative machine learning (ML) paradigm, with SLNs derived from students' interactions in multiple classrooms' online forums to predict future link formations (i.e., interactions) among students. By leveraging FL, our approach enables collaborative model training across multiple classrooms while preserving data privacy, as it eliminates the need for raw data centralization. Recognizing that each classroom may exhibit unique student interaction dynamics, we further employ model personalization techniques to adapt the FL model to individual classroom characteristics. Our results demonstrate the effectiveness of our approach in capturing both shared and classroom-specific representations of student interactions in SLNs. Additionally, we utilize explainable AI (XAI) techniques to interpret model predictions, identifying key factors that influence link formation across different classrooms. These insights unveil the drivers of social learning interactions within a privacy-preserving, collaborative, and distributed ML framework -- an aspect that has not been explored before.         ",
    "url": "https://arxiv.org/abs/2504.10456",
    "authors": [
      "Anurata Prabha Hridi",
      "Muntasir Hoq",
      "Zhikai Gao",
      "Collin Lynch",
      "Rajeev Sahay",
      "Seyyedali Hosseinalipour",
      "Bita Akram"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2504.14930",
    "title": "Kernel-learning parameter prediction and evaluation in algebraic multigrid method for several PDEs",
    "abstract": "           This paper explores the application of kernel learning methods for parameter prediction and evaluation in the Algebraic Multigrid Method (AMG), focusing on several Partial Differential Equation (PDE) problems. AMG is an efficient iterative solver for large-scale sparse linear systems, particularly those derived from elliptic and parabolic PDE discretizations. However, its performance heavily relies on numerous parameters, which are often set empirically and are highly sensitive to AMG's effectiveness. Traditional parameter optimization methods are either computationally expensive or lack theoretical support. To address this, we propose a Gaussian Process Regression (GPR)-based strategy to optimize AMG parameters and introduce evaluation metrics to assess their effectiveness. Trained on small-scale datasets, GPR predicts nearly optimal parameters, bypassing the time-consuming parameter sweeping process. We also use kernel learning techniques to build a kernel function library and determine the optimal kernel function through linear combination, enhancing prediction accuracy. In numerical experiments, we tested typical PDEs such as the constant-coefficient Poisson equation, variable-coefficient Poisson equation, diffusion equation, and Helmholtz equation. Results show that GPR-predicted parameters match grid search results in iteration counts while significantly reducing computational time. A comprehensive analysis using metrics like mean squared error, prediction interval coverage, and Bayesian information criterion confirms GPR's efficiency and reliability. These findings validate GPR's effectiveness in AMG parameter optimization and provide theoretical support for AMG's practical application.         ",
    "url": "https://arxiv.org/abs/2504.14930",
    "authors": [
      "Junyue Luo",
      "Xiaoqiang Yue",
      "Fangfang Zhang",
      "Juan Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2504.19419",
    "title": "Advancing Local Clustering on Graphs via Compressive Sensing: Semi-supervised and Unsupervised Methods",
    "abstract": "           Local clustering aims to identify specific substructures within a large graph without any additional structural information of the graph. These substructures are typically small compared to the overall graph, enabling the problem to be approached by finding a sparse solution to a linear system associated with the graph Laplacian. In this work, we first propose a method for identifying specific local clusters when very few labeled data are given, which we term semi-supervised local clustering. We then extend this approach to the unsupervised setting when no prior information on labels is available. The proposed methods involve randomly sampling the graph, applying diffusion through local cluster extraction, then examining the overlap among the results to find each cluster. We establish the co-membership conditions for any pair of nodes, and rigorously prove the correctness of our methods. Additionally, we conduct extensive experiments to demonstrate that the proposed methods achieve state of the art results in the low-label rates regime.         ",
    "url": "https://arxiv.org/abs/2504.19419",
    "authors": [
      "Zhaiming Shen",
      "Sung Ha Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.06203",
    "title": "A Robust and Non-Iterative Tensor Decomposition Method with Automatic Thresholding",
    "abstract": "           Recent advances in IoT and biometric sensing technologies have led to the generation of massive and high-dimensional tensor data, yet achieving accurate and efficient low-rank approximation remains a major challenge. Existing tensor decomposition methods typically require prior specification of the tensor rank and rely on iterative optimization, which often results in heavy computational costs and dependence on the analyst's expertise. In this study, we propose a novel low-rank approximation method for tensor data that requires neither prior rank specification nor iterative optimization. The proposed method performs statistical singular value hard thresholding on the mode-wise unfolded matrices to automatically extract only statistically significant components, thereby achieving noise reduction while preserving the intrinsic tensor structure. Theoretically, the optimal threshold for each mode is derived based on the asymptotic properties of the Mar\u010denko--Pastur distribution. Simulation experiments demonstrate that the proposed method outperforms conventional approaches such as Higher-Order Singular Value Decomposition, Higher-Order Orthogonal Iteration, and Tucker-L2E in terms of both estimation accuracy and computational efficiency. These results indicate that our method provides an effective and theoretically grounded framework for automatic, non-iterative, and analyst-independent tensor decomposition.         ",
    "url": "https://arxiv.org/abs/2505.06203",
    "authors": [
      "Hiroki Hasegawa",
      "Yukihiko Okada"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.11542",
    "title": "Cybersecurity threat detection based on a UEBA framework using Deep Autoencoders",
    "abstract": "           User and Entity Behaviour Analytics (UEBA) is a broad branch of data analytics that attempts to build a normal behavioural profile in order to detect anomalous events. Among the techniques used to detect anomalies, Deep Autoencoders constitute one of the most promising deep learning models on UEBA tasks, allowing explainable detection of security incidents that could lead to the leak of personal data, hijacking of systems, or access to sensitive business information. In this study, we introduce the first implementation of an explainable UEBA-based anomaly detection framework that leverages Deep Autoencoders in combination with Doc2Vec to process both numerical and textual features. Additionally, based on the theoretical foundations of neural networks, we offer a novel proof demonstrating the equivalence of two widely used definitions for fully-connected neural networks. The experimental results demonstrate the proposed framework capability to detect real and synthetic anomalies effectively generated from real attack data, showing that the models provide not only correct identification of anomalies but also explainable results that enable the reconstruction of the possible origin of the anomaly. Our findings suggest that the proposed UEBA framework can be seamlessly integrated into enterprise environments, complementing existing security systems for explainable threat detection.         ",
    "url": "https://arxiv.org/abs/2505.11542",
    "authors": [
      "Jose Fuentes",
      "Ines Ortega-Fernandez",
      "Nora M. Villanueva",
      "Marta Sestelo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.12191",
    "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum",
    "abstract": "           Self-Supervised Learning (SSL) has become a powerful solution to extract rich representations from unlabeled data. Yet, SSL research is mostly focused on clean, curated and high-quality datasets. As a result, applying SSL on noisy data remains a challenge, despite being crucial to applications such as astrophysics, medical imaging, geophysics or finance. In this work, we present a fully self-supervised framework that enables noise-robust representation learning without requiring a denoiser at inference or downstream fine-tuning. Our method first trains an SSL denoiser on noisy data, then uses it to construct a denoised-to-noisy data curriculum (i.e., training first on denoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2), combined with a teacher-guided regularization that anchors noisy embeddings to their denoised counterparts. This process encourages the model to internalize noise robustness. Notably, the denoiser can be discarded after pretraining, simplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise ($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by 4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from noise-aware pretraining. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.12191",
    "authors": [
      "Wenquan Lu",
      "Jiaqi Zhang",
      "Hugues Van Assel",
      "Randall Balestriero"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.13751",
    "title": "Multiple Proposer Transaction Fee Mechanism Design: Robust Incentives Against Censorship and Bribery",
    "abstract": "           Censorship resistance is one of the core value proposition of blockchains. A recurring design pattern aimed at providing censorship resistance is enabling multiple proposers to contribute inputs into block construction. Notably, Fork-Choice Enforced Inclusion Lists (FOCIL) is proposed to be included in Ethereum. However, the current proposal relies on altruistic behavior, without a Transaction Fee Mechanism (TFM). This study aims to address this gap by exploring how multiple proposers should be rewarded to incentivize censorship resistance. The main contribution of this work is the identification of TFMs that ensure censorship resistance under bribery attacks, while also satisfying the incentive compatibility properties of EIP-1559. We provide a concrete payment mechanism for FOCIL, along with generalizable contributions to the literature by analyzing 1) incentive compatibility of TFMs in the presence of a bribing adversary, 2) TFMs in protocols with multiple phases of transaction inclusion, and 3) TFMs of protocols in which parties are uncertain about the behavior and the possible bribe of others.         ",
    "url": "https://arxiv.org/abs/2505.13751",
    "authors": [
      "Aikaterini-Panagiota Stouka",
      "Julian Ma",
      "Thomas Thiery"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.13904",
    "title": "Learning to Insert for Constructive Neural Vehicle Routing Solver",
    "abstract": "           Neural Combinatorial Optimisation (NCO) is a promising learning-based approach for solving Vehicle Routing Problems (VRPs) without extensive manual design. While existing constructive NCO methods typically follow an appending-based paradigm that sequentially adds unvisited nodes to partial solutions, this rigid approach often leads to suboptimal results. To overcome this limitation, we explore the idea of insertion-based paradigm and propose Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel learning-based method for constructive NCO. Unlike traditional approaches, L2C-Insert builds solutions by strategically inserting unvisited nodes at any valid position in the current partial solution, which can significantly enhance the flexibility and solution quality. The proposed framework introduces three key components: a novel model architecture for precise insertion position prediction, an efficient training scheme for model optimization, and an advanced inference technique that fully exploits the insertion paradigm's flexibility. Extensive experiments on both synthetic and real-world instances of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior performance across various problem sizes.         ",
    "url": "https://arxiv.org/abs/2505.13904",
    "authors": [
      "Fu Luo",
      "Xi Lin",
      "Mengyuan Zhong",
      "Fei Liu",
      "Zhenkun Wang",
      "Jianyong Sun",
      "Qingfu Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2505.15095",
    "title": "Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English",
    "abstract": "           Sarcasm is a challenge to sentiment analysis because of the incongruity between stated and implied sentiment. The challenge is exacerbated when the implication may be relevant to a specific country or geographical region. Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that has been used for pragmatic reasoning. In this paper, we harness PMP for explainable sarcasm detection for Australian and Indian English, alongside a benchmark dataset for standard English. We manually add sarcasm explanations to an existing sarcasm-labeled dataset for Australian and Indian English called BESSTIE, and compare the performance for explainable sarcasm detection for them with FLUTE, a standard English dataset containing sarcasm explanations. Our approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA) achieves statistically significant performance improvement across all tasks and datasets when compared with four alternative prompting strategies. We also find that alternative techniques such as agentic prompting mitigate context-related failures by enabling external knowledge retrieval. The focused contribution of our work is utilising PMP in generating sarcasm explanations for varieties of English.         ",
    "url": "https://arxiv.org/abs/2505.15095",
    "authors": [
      "Ishmanbir Singh",
      "Dipankar Srirag",
      "Aditya Joshi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.15811",
    "title": "On the creation of narrow AI: hierarchy and nonlocality of neural network skills",
    "abstract": "           We study the problem of creating strong, yet narrow, AI systems. While recent AI progress has been driven by the training of large general-purpose foundation models, the creation of smaller models specialized for narrow domains could be valuable for both efficiency and safety. In this work, we explore two challenges involved in creating such systems, having to do with basic properties of how neural networks learn and structure their representations. The first challenge regards when it is possible to train narrow models from scratch. Through experiments on a synthetic task, we find that it is sometimes necessary to train networks on a wide distribution of data to learn certain narrow skills within that distribution. This effect arises when skills depend on each other hierarchically, and training on a broad distribution introduces a curriculum which substantially accelerates learning. The second challenge regards how to transfer particular skills from large general models into small specialized models. We find that model skills are often not perfectly localized to a particular set of prunable components. However, we find that methods based on pruning can still outperform distillation. We investigate the use of a regularization objective to align desired skills with prunable components while unlearning unnecessary skills.         ",
    "url": "https://arxiv.org/abs/2505.15811",
    "authors": [
      "Eric J. Michaud",
      "Asher Parker-Sartori",
      "Max Tegmark"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.18766",
    "title": "StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations",
    "abstract": "           Recently, text-to-image diffusion models have been widely used for style mimicry and personalized customization through methods such as DreamBooth and Textual Inversion. This has raised concerns about intellectual property protection and the generation of deceptive content. Recent studies, such as Glaze and Anti-DreamBooth, have proposed using adversarial noise to protect images from these attacks. However, recent purification-based methods, such as DiffPure and Noise Upscaling, have successfully attacked these latest defenses, showing the vulnerabilities of these methods. Moreover, present methods show limited transferability across models, making them less effective against unknown text-to-image models. To address these issues, we propose a novel anti-mimicry method, StyleGuard. We propose a novel style loss that optimizes the style-related features in the latent space to make it deviate from the original image, which improves model-agnostic transferability. Additionally, to enhance the perturbation's ability to bypass diffusion-based purification, we designed a novel upscale loss that involves ensemble purifiers and upscalers during training. Extensive experiments on the WikiArt and CelebA datasets demonstrate that StyleGuard outperforms existing methods in robustness against various transformations and purifications, effectively countering style mimicry in various models. Moreover, StyleGuard is effective on different style mimicry methods, including DreamBooth and Textual Inversion. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.18766",
    "authors": [
      "Yanjie Li",
      "Wenxuan Zhang",
      "Xinqi Lyu",
      "Yihao Liu",
      "Bin Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.24627",
    "title": "Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees",
    "abstract": "           Recent neural combinatorial optimization (NCO) methods have shown promising problem-solving ability without requiring domain-specific expertise. Most existing NCO methods use training and testing data with a fixed constraint value and lack research on the effect of constraint tightness on the performance of NCO methods. This paper takes the capacity-constrained vehicle routing problem (CVRP) as an example to empirically analyze the NCO performance under different tightness degrees of the capacity constraint. Our analysis reveals that existing NCO methods overfit the capacity constraint, and they can only perform satisfactorily on a small range of the constraint values but poorly on other values. To tackle this drawback of existing NCO methods, we develop an efficient training scheme that explicitly considers varying degrees of constraint tightness and proposes a multi-expert module to learn a generally adaptable solving strategy. Experimental results show that the proposed method can effectively overcome the overfitting issue, demonstrating superior performances on the CVRP and CVRP with time windows (CVRPTW) with various constraint tightness degrees.         ",
    "url": "https://arxiv.org/abs/2505.24627",
    "authors": [
      "Fu Luo",
      "Yaoxin Wu",
      "Zhi Zheng",
      "Zhenkun Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.02392",
    "title": "Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning",
    "abstract": "           Neural Combinatorial Optimization (NCO) has emerged as a promising learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by minimizing the need for extensive manual engineering. While existing NCO methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated considerable success on problems of similar scale, their performance significantly degrades when applied to large-scale scenarios. This degradation arises from the distributional shift between training and testing data, rendering policies learned on small instances ineffective for larger problems. To overcome this limitation, we introduce a novel learning framework driven by Large Language Models (LLMs). This framework learns a projection between the training and testing distributions, which is then deployed to enhance the scalability of the NCO model. Notably, unlike prevailing techniques that necessitate joint training with the neural network, our approach operates exclusively during the inference phase, obviating the need for model retraining. Extensive experiments demonstrate that our method enables a backbone model (trained on 100-node instances) to achieve superior performance on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) of up to 100K nodes from diverse distributions.         ",
    "url": "https://arxiv.org/abs/2506.02392",
    "authors": [
      "Yuanyao Chen",
      "Rongsheng Chen",
      "Fu Luo",
      "Zhenkun Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.02393",
    "title": "RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared Small Target Detection",
    "abstract": "           Infrared small target detection is a challenging task due to its unique characteristics (e.g., small, dim, shapeless and changeable). Recently published CNN-based methods have achieved promising performance with heavy feature extraction and fusion modules. To achieve efficient and effective detection, we propose a recurrent reusable-convolution attention network (RRCA-Net) for infrared small target detection. Specifically, RRCA-Net incorporates reusable-convolution block (RuCB) in a recurrent manner without introducing extra parameters. With the help of the repetitive iteration in RuCB, the high-level information of small targets in the deep layers can be well maintained and further refined. Then, a dual interactive attention aggregation module (DIAAM) is proposed to promote the mutual enhancement and fusion of refined information. In this way, RRCA-Net can both achieve high-level feature refinement and enhance the correlation of contextual information between adjacent layers. Moreover, to achieve steady convergence, we design a target characteristic inspired loss function (DpT-k loss) by integrating physical and mathematical constraints. Experimental results on three benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate that our RRCA-Net can achieve comparable performance to the state-of-the-art methods while maintaining a small number of parameters, and act as a plug and play module to introduce consistent performance improvement for several popular IRSTD methods.         ",
    "url": "https://arxiv.org/abs/2506.02393",
    "authors": [
      "Yongxian Liu",
      "Boyang Li",
      "Ting Liu",
      "Zaiping Lin",
      "Wei An"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.02935",
    "title": "MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver",
    "abstract": "           Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a promising approach to train a unified model capable of solving multiple Vehicle Routing Problem (VRP) variants. However, existing Reinforcement Learning (RL)-based multi-task methods can only train light decoder models on small-scale problems, exhibiting limited generalization ability when solving large-scale problems. To overcome this limitation, this work introduces a novel multi-task learning method driven by knowledge distillation (MTL-KD), which enables the efficient training of heavy decoder models with strong generalization ability. The proposed MTL-KD method transfers policy knowledge from multiple distinct RL-based single-task models to a single heavy decoder model, facilitating label-free training and effectively improving the model's generalization ability across diverse tasks. In addition, we introduce a flexible inference strategy termed Random Reordering Re-Construction (R3C), which is specifically adapted for diverse VRP tasks and further boosts the performance of the multi-task model. Experimental results on 6 seen and 10 unseen VRP variants with up to 1000 nodes indicate that our proposed method consistently achieves superior performance on both uniform and real-world benchmarks, demonstrating robust generalization abilities.         ",
    "url": "https://arxiv.org/abs/2506.02935",
    "authors": [
      "Yuepeng Zheng",
      "Fu Luo",
      "Zhenkun Wang",
      "Yaoxin Wu",
      "Yu Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.07001",
    "title": "Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text",
    "abstract": "           The increasing capabilities of Large Language Models (LLMs) have raised concerns about their misuse in AI-generated plagiarism and social engineering. While various AI-generated text detectors have been proposed to mitigate these risks, many remain vulnerable to simple evasion techniques such as paraphrasing. However, recent detectors have shown greater robustness against such basic attacks. In this work, we introduce Adversarial Paraphrasing, a training-free attack framework that universally humanizes any AI-generated text to evade detection more effectively. Our approach leverages an off-the-shelf instruction-following LLM to paraphrase AI-generated content under the guidance of an AI text detector, producing adversarial examples that are specifically optimized to bypass detection. Extensive experiments show that our attack is both broadly effective and highly transferable across several detection systems. For instance, compared to simple paraphrasing attack--which, ironically, increases the true positive at 1% false positive (T@1%F) by 8.57% on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on Fast-DetectGPT. Across a diverse set of detectors--including neural network-based, watermark-based, and zero-shot approaches--our attack achieves an average T@1%F reduction of 87.88% under the guidance of OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and attack success to find that our method can significantly reduce detection rates, with mostly a slight degradation in text quality. Our adversarial setup highlights the need for more robust and resilient detection strategies in the light of increasingly sophisticated evasion techniques.         ",
    "url": "https://arxiv.org/abs/2506.07001",
    "authors": [
      "Yize Cheng",
      "Vinu Sankar Sadasivan",
      "Mehrdad Saberi",
      "Shoumik Saha",
      "Soheil Feizi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.07500",
    "title": "Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks",
    "abstract": "           Modern neural networks demonstrate state-of-the-art performance on numerous existing benchmarks; however, their high computational requirements and energy consumption prompt researchers to seek more efficient solutions for real-world deployment. Logic gate networks (LGNs) learns a large network of logic gates for efficient image classification. However, learning a network that can solve a simple problem like CIFAR-10 can take days to weeks to train. Even then, almost half of the network remains unused, causing a discretization gap. This discretization gap hinders real-world deployment of LGNs, as the performance drop between training and inference negatively impacts accuracy. We inject Gumbel noise with a straight-through estimator during training to significantly speed up training, improve neuron utilization, and decrease the discretization gap. We theoretically show that this results from implicit Hessian regularization, which improves the convergence properties of LGNs. We train networks $4.5 \\times$ faster in wall-clock time, reduce the discretization gap by $98\\%$, and reduce the number of unused gates by $100\\%$.         ",
    "url": "https://arxiv.org/abs/2506.07500",
    "authors": [
      "Shakir Yousefi",
      "Andreas Plesner",
      "Till Aczel",
      "Roger Wattenhofer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2506.09937",
    "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
    "abstract": "           While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out of the box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while generalist VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, $\\pi_0$, and $\\pi_0$-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results and code can be found at the project webpage: this https URL ",
    "url": "https://arxiv.org/abs/2506.09937",
    "authors": [
      "Qiao Gu",
      "Yuanliang Ju",
      "Shengxiang Sun",
      "Igor Gilitschenski",
      "Haruki Nishimura",
      "Masha Itkina",
      "Florian Shkurti"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.19816",
    "title": "CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling",
    "abstract": "           Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong performance in robotic manipulation. However, these models remain constrained by the single-frame image paradigm and fail to fully leverage the temporal information offered by multi-frame histories, as directly feeding multiple frames into VLM backbones incurs substantial computational overhead and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame pretraining on large-scale embodied datasets with autoregressive prediction of action tokens, establishing an effective embodied vision-language foundation; (2) Multi-frame post-training, which adapts the prediction of the vision-language backbone from discrete tokens to learnable features, and aggregates historical information via feature chunking. CronusVLA effectively addresses the existing challenges of multi-frame modeling while enhancing performance and observational robustness. To evaluate the robustness under temporal and spatial disturbances, we introduce SimplerEnv-OR, a novel benchmark featuring 24 types of observational disturbances and 120 severity levels. Experiments across three embodiments in simulated and real-world environments demonstrate that CronusVLA achieves leading performance and superior robustness, with a 70.9% success rate on SimplerEnv, a 26.8% improvement over OpenVLA on LIBERO, and the highest robustness score on SimplerEnv-OR. These results highlight the potential of efficient multi-frame adaptation in VLA models for more powerful and robust real-world deployment.         ",
    "url": "https://arxiv.org/abs/2506.19816",
    "authors": [
      "Hao Li",
      "Shuai Yang",
      "Yilun Chen",
      "Xinyi Chen",
      "Xiaoda Yang",
      "Yang Tian",
      "Hanqing Wang",
      "Tai Wang",
      "Dahua Lin",
      "Feng Zhao",
      "Jiangmiao Pang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.21046",
    "title": "Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features",
    "abstract": "           The ability of deep neural networks (DNNs) come from extracting and interpreting features from the data provided. By exploiting intermediate features in DNNs instead of relying on hard labels, we craft adversarial perturbation that generalize more effectively, boosting black-box transferability. These features ubiquitously come from supervised learning in previous work. Inspired by the exceptional synergy between self-supervised learning and the Transformer architecture, this paper explores whether exploiting self-supervised Vision Transformer (ViT) representations can improve adversarial transferability. We present dSVA -- a generative dual self-supervised ViT features attack, that exploits both global structural features from contrastive learning (CL) and local textural features from masked image modeling (MIM), the self-supervised learning paradigm duo for ViTs. We design a novel generative training framework that incorporates a generator to create black-box adversarial examples, and strategies to train the generator by exploiting joint features and the attention mechanism of self-supervised ViTs. Our findings show that CL and MIM enable ViTs to attend to distinct feature tendencies, which, when exploited in tandem, boast great adversarial generalizability. By disrupting dual deep features distilled by self-supervised ViTs, we are rewarded with remarkable black-box transferability to models of various architectures that outperform state-of-the-arts. Code available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.21046",
    "authors": [
      "Shangbo Wu",
      "Yu-an Tan",
      "Ruinan Ma",
      "Wencong Ma",
      "Dehua Zhu",
      "Yuanzhang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.23292",
    "title": "DDL: A Large-Scale Datasets for Deepfake Detection and Localization in Diversified Real-World Scenarios",
    "abstract": "           Recent advances in AIGC have exacerbated the misuse of malicious deepfake content, making the development of reliable deepfake detection methods an essential means to address this challenge. Although existing deepfake detection models demonstrate outstanding performance in detection metrics, most methods only provide simple binary classification results, lacking interpretability. Recent studies have attempted to enhance the interpretability of classification results by providing spatial manipulation masks or temporal forgery segments. However, due to the limitations of forgery datasets, the practical effectiveness of these methods remains suboptimal. The primary reason lies in the fact that most existing deepfake datasets contain only binary labels, with limited variety in forgery scenarios, insufficient diversity in deepfake types, and relatively small data scales, making them inadequate for complex real-world this http URL address this predicament, we construct a novel large-scale deepfake detection and localization (\\textbf{DDL}) dataset containing over $\\textbf{1.4M+}$ forged samples and encompassing up to $\\textbf{80}$ distinct deepfake methods. The DDL design incorporates four key innovations: (1) \\textbf{Comprehensive Deepfake Methods} (covering 7 different generation architectures and a total of 80 methods), (2) \\textbf{Varied Manipulation Modes} (incorporating 7 classic and 3 novel forgery modes), (3) \\textbf{Diverse Forgery Scenarios and Modalities} (including 3 scenarios and 3 modalities), and (4) \\textbf{Fine-grained Forgery Annotations} (providing 1.18M+ precise spatial masks and 0.23M+ precise temporal segments).Through these improvements, our DDL not only provides a more challenging benchmark for complex real-world forgeries but also offers crucial support for building next-generation deepfake detection, localization, and interpretability methods.         ",
    "url": "https://arxiv.org/abs/2506.23292",
    "authors": [
      "Changtao Miao",
      "Yi Zhang",
      "Weize Gao",
      "Zhiya Tan",
      "Weiwei Feng",
      "Man Luo",
      "Jianshu Li",
      "Ajian Liu",
      "Yunfeng Diao",
      "Qi Chu",
      "Tao Gong",
      "Zhe Li",
      "Weibin Yao",
      "Joey Tianyi Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.00927",
    "title": "Understanding Generalization in Node and Link Prediction",
    "abstract": "           Using message-passing graph neural networks (MPNNs) for node and link prediction is crucial in various scientific and industrial domains, which has led to the development of diverse MPNN architectures. Besides working well in practical settings, their ability to generalize beyond the training set remains poorly understood. While some studies have explored MPNNs' generalization in graph-level prediction tasks, much less attention has been given to node- and link-level predictions. Existing works often rely on unrealistic i.i.d.\\@ assumptions, overlooking possible correlations between nodes or links, and assuming fixed aggregation and impractical loss functions while neglecting the influence of graph structure. In this work, we introduce a unified framework to analyze the generalization properties of MPNNs in inductive and transductive node and link prediction settings, incorporating diverse architectural parameters and loss functions and quantifying the influence of graph structure. Additionally, our proposed generalization framework can be applied beyond graphs to any classification task under the inductive or transductive setting. Our empirical study supports our theoretical insights, deepening our understanding of MPNNs' generalization capabilities in these tasks.         ",
    "url": "https://arxiv.org/abs/2507.00927",
    "authors": [
      "Antonis Vasileiou",
      "Timo Stoll",
      "Christopher Morris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.06078",
    "title": "ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models",
    "abstract": "           Despite the success of deep learning across various domains, it remains vulnerable to adversarial attacks. Although many existing adversarial attack methods achieve high success rates, they typically rely on $\\ell_{p}$-norm perturbation constraints, which do not align with human perceptual capabilities. Consequently, researchers have shifted their focus toward generating natural, unrestricted adversarial examples (UAEs). GAN-based approaches suffer from inherent limitations, such as poor image quality due to instability and mode collapse. Meanwhile, diffusion models have been employed for UAE generation, but they still rely on iterative PGD perturbation injection, without fully leveraging their central denoising capabilities. In this paper, we introduce a novel approach for generating UAEs based on diffusion models, named ScoreAdv. This method incorporates an interpretable adversarial guidance mechanism to gradually shift the sampling distribution towards the adversarial distribution, while using an interpretable saliency map to inject the visual information of a reference image into the generated samples. Notably, our method is capable of generating an unlimited number of natural adversarial examples and can attack not only classification models but also retrieval models. We conduct extensive experiments on ImageNet and CelebA datasets, validating the performance of ScoreAdv across ten target models in both black-box and white-box settings. Our results demonstrate that ScoreAdv achieves state-of-the-art attack success rates and image quality, while maintaining inference efficiency. Furthermore, the dynamic balance between denoising and adversarial perturbation enables ScoreAdv to remain robust even under defensive measures.         ",
    "url": "https://arxiv.org/abs/2507.06078",
    "authors": [
      "Chihan Huang",
      "Hao Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.08186",
    "title": "KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning",
    "abstract": "           Semantic segmentation of structural defects in civil infrastructure remains challenging due to variable defect appearances, harsh imaging conditions, and significant class imbalance. Current deep learning methods, despite their effectiveness, typically require millions of parameters, rendering them impractical for real-time inspection systems. We introduce KARMA (Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient semantic segmentation framework that models complex defect patterns through compositions of one-dimensional functions rather than conventional convolutions. KARMA features three technical innovations: (1) a parameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging low-rank factorization for KAN-based feature transformation; (2) an optimized feature pyramid structure with separable convolutions for multi-scale defect analysis; and (3) a static-dynamic prototype mechanism that enhances feature representation for imbalanced classes. Extensive experiments on benchmark infrastructure inspection datasets demonstrate that KARMA achieves competitive or superior mean IoU performance compared to state-of-the-art approaches, while using significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction). Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for real-time deployment, enabling practical automated infrastructure inspection systems without compromising accuracy. The source code can be accessed at the following URL: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.08186",
    "authors": [
      "Md Meftahul Ferdaus",
      "Mahdi Abdelguerfi",
      "Elias Ioup",
      "Steven Sloan",
      "Kendall N. Niles",
      "Ken Pathak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.02435",
    "title": "A Convolutional Hierarchical Deep-learning Neural Network (C-HiDeNN) Framework for Non-linear Finite Element Analysis",
    "abstract": "           We present a framework for the Convolutional Hierarchical Deep-learning Neural Network (C-HiDeNN) tailored for nonlinear finite element analysis. Building upon the structured foundation of HiDeNN, C-HiDeNN introduces a convolution operator to enhance numerical approximation. A distinctive feature of C-HiDeNN is its higher-order accurate approximation achieved through an expanded set of parameters, such as the polynomial order 'p,' dilation parameter 'a,' patch size 's,' and nodal position 'X'. These parameters function as the functional equivalents of weights and biases within each C-HiDeNN patch. In addition, C-HiDeNN can be selectively applied to regions requiring high resolution to adaptively improve local prediction accuracy. To demonstrate the effectiveness of this framework, we provide numerical examples in the context of nonlinear finite element analysis. The results show that our approach achieves significantly higher accuracy than conventional Finite Element Method (FEM) while substantially reducing computational costs.         ",
    "url": "https://arxiv.org/abs/2509.02435",
    "authors": [
      "Yingjian Liu",
      "Monish Yadav Pabbala",
      "Jiachen Guo",
      "Chanwook Park",
      "Gino Domel",
      "Wing Kam Liu",
      "Dong Qian"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ]
  },
  {
    "id": "arXiv:2509.04448",
    "title": "TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection",
    "abstract": "           Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. In this work, we observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. We hypothesize that joint training across distortion types facilitates knowledge sharing and enhances the model's ability to generalize. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, while also offering strong generalization and interpretability.         ",
    "url": "https://arxiv.org/abs/2509.04448",
    "authors": [
      "Zehong Yan",
      "Peng Qi",
      "Wynne Hsu",
      "Mong Li Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2509.13733",
    "title": "FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph",
    "abstract": "           Visual-Language Navigation (VLN) is a fundamental challenge in robotic systems, with broad applications for the deployment of embodied agents in real-world environments. Despite recent advances, existing approaches are limited in long-range spatial reasoning, often exhibiting low success rates and high inference latency, particularly in long-range navigation tasks. To address these limitations, we propose FSR-VLN, a vision-language navigation system that combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation supporting progressive retrieval, from coarse room-level localization to fine-grained goal view and object identification. Building on HMSG, FSR first performs fast matching to efficiently select candidate rooms, views, and objects, then applies VLM-driven refinement for final goal selection. We evaluated FSR-VLN across four comprehensive indoor datasets collected by humanoid robots, utilizing 87 instructions that encompass a diverse range of object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all datasets, measured by the retrieval success rate (RSR), while reducing the response time by 82% compared to VLM-based methods on tour videos by activating slow reasoning only when fast intuition fails. Furthermore, we integrate FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1 humanoid robot, enabling natural language interaction and real-time navigation.         ",
    "url": "https://arxiv.org/abs/2509.13733",
    "authors": [
      "Xiaolin Zhou",
      "Tingyang Xiao",
      "Liu Liu",
      "Yucheng Wang",
      "Maiyue Chen",
      "Xinrui Meng",
      "Xinjie Wang",
      "Wei Feng",
      "Wei Sui",
      "Zhizhong Su"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.15370",
    "title": "Adversarial generalization of unfolding (model-based) networks",
    "abstract": "           Unfolding networks are interpretable networks emerging from iterative algorithms, incorporate prior knowledge of data structure, and are designed to solve inverse problems like compressed sensing, which deals with recovering data from noisy, missing observations. Compressed sensing finds applications in critical domains, from medical imaging to cryptography, where adversarial robustness is crucial to prevent catastrophic failures. However, a solid theoretical understanding of the performance of unfolding networks in the presence of adversarial attacks is still in its infancy. In this paper, we study the adversarial generalization of unfolding networks when perturbed with $l_2$-norm constrained attacks, generated by the fast gradient sign method. Particularly, we choose a family of state-of-the-art overaparameterized unfolding networks and deploy a new framework to estimate their adversarial Rademacher complexity. Given this estimate, we provide adversarial generalization error bounds for the networks under study, which are tight with respect to the attack level. To our knowledge, this is the first theoretical analysis on the adversarial generalization of unfolding networks. We further present a series of experiments on real-world data, with results corroborating our derived theory, consistently for all data. Finally, we observe that the family's overparameterization can be exploited to promote adversarial robustness, shedding light on how to efficiently robustify neural networks.         ",
    "url": "https://arxiv.org/abs/2509.15370",
    "authors": [
      "Vicky Kouni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.16336",
    "title": "Neural Atlas Graphs for Dynamic Scene Decomposition and Editing",
    "abstract": "           Learning editable high-resolution scene representations for dynamic scenes is an open problem with applications across the domains from autonomous driving to creative editing - the most successful approaches today make a trade-off between editability and supporting scene complexity: neural atlases represent dynamic scenes as two deforming image layers, foreground and background, which are editable in 2D, but break down when multiple objects occlude and interact. In contrast, scene graph models make use of annotated data such as masks and bounding boxes from autonomous-driving datasets to capture complex 3D spatial relationships, but their implicit volumetric node representations are challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a hybrid high-resolution scene representation, where every graph node is a view-dependent neural atlas, facilitating both 2D appearance editing and 3D ordering and positioning of scene elements. Fit at test-time, NAGs achieve state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR increase compared to existing methods - and make environmental editing possible in high resolution and visual quality - creating counterfactual driving scenarios with new backgrounds and edited vehicle appearance. We find that the method also generalizes beyond driving scenes and compares favorably - by more than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS video dataset with a diverse set of human and animal-centric scenes. Project Page: this https URL ",
    "url": "https://arxiv.org/abs/2509.16336",
    "authors": [
      "Jan Philipp Schneider",
      "Pratik Singh Bisht",
      "Ilya Chugunov",
      "Andreas Kolb",
      "Michael Moeller",
      "Felix Heide"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.17302",
    "title": "TextCrafter: Optimization-Calibrated Noise for Defending Against Text Embedding Inversion",
    "abstract": "           Text embedding inversion attacks reconstruct original sentences from latent representations, posing severe privacy threats in collaborative inference and edge computing. We propose TextCrafter, an optimization-based adversarial perturbation mechanism that combines RL learned, geometry aware noise injection orthogonal to user embeddings with cluster priors and PII signal guidance to suppress inversion while preserving task utility. Unlike prior defenses either non learnable or agnostic to perturbation direction, TextCrafter provides a directional protective policy that balances privacy and utility. Under strong privacy setting, TextCrafter maintains 70 percentage classification accuracy on four datasets and consistently outperforms Gaussian/LDP baselines across lower privacy budgets, demonstrating a superior privacy utility trade off.         ",
    "url": "https://arxiv.org/abs/2509.17302",
    "authors": [
      "Duoxun Tang",
      "Xinhang Jiang",
      "Jiajun Niu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.17784",
    "title": "Revealing Multimodal Causality with Large Language Models",
    "abstract": "           Uncovering cause-and-effect mechanisms from data is fundamental to scientific progress. While large language models (LLMs) show promise for enhancing causal discovery (CD) from unstructured data, their application to the increasingly prevalent multimodal setting remains a critical challenge. Even with the advent of multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two primary limitations: (1) difficulty in exploring intra- and inter-modal interactions for comprehensive causal variable identification; and (2) insufficiency to handle structural ambiguities with purely observational data. To address these challenges, we propose MLLM-CD, a novel framework for multimodal causal discovery from unstructured data. It consists of three key components: (1) a novel contrastive factor discovery module to identify genuine multimodal factors based on the interactions explored from contrastive sample pairs; (2) a statistical causal structure discovery module to infer causal relationships among discovered factors; and (3) an iterative multimodal counterfactual reasoning module to refine the discovery outcomes iteratively by incorporating the world knowledge and reasoning capabilities of MLLMs. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed MLLM-CD in revealing genuine factors and causal relationships among them from multimodal unstructured data.         ",
    "url": "https://arxiv.org/abs/2509.17784",
    "authors": [
      "Jin Li",
      "Shoujin Wang",
      "Qi Zhang",
      "Feng Liu",
      "Tongliang Liu",
      "Longbing Cao",
      "Shui Yu",
      "Fang Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.23885",
    "title": "Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction",
    "abstract": "           Current models based on deep learning for low-dose CT denoising rely heavily on paired data and generalize poorly. Even the more concerned diffusion models need to learn the distribution of clean data for reconstruction, which is difficult to satisfy in medical clinical applications. At the same time, self-supervised-based methods face the challenge of significant degradation of generalizability of models pre-trained for the current dose to expand to other doses. To address these issues, this work proposes a novel method of TUnable-geneRalizatioN Diffusion (TurnDiff) powered by self-supervised contextual sub-data for low-dose CT reconstruction. Firstly, a contextual subdata self-enhancing similarity strategy is designed for denoising centered on the LDCT projection domain, which provides an initial prior for the subsequent progress. Subsequently, the initial prior is used to combine knowledge distillation with a deep combination of latent diffusion models for optimizing image details. The pre-trained model is used for inference reconstruction, and the pixel-level self-correcting fusion technique is proposed for fine-grained reconstruction of the image domain to enhance the image fidelity, using the initial prior and the LDCT image as a guide. In addition, the technique is flexibly applied to the generalization of upper and lower doses or even unseen doses. Dual-domain strategy cascade for self-supervised LDCT denoising, TurnDiff requires only LDCT projection domain data for training and testing. Comprehensive evaluation on both benchmark datasets and real-world data demonstrates that TurnDiff consistently outperforms state-of-the-art methods in both reconstruction and generalization.         ",
    "url": "https://arxiv.org/abs/2509.23885",
    "authors": [
      "Guoquan Wei",
      "Liu Shi",
      "Zekun Zhou",
      "Wenzhe Shan",
      "Qiegen Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.01988",
    "title": "PepCompass: Navigating peptide embedding spaces using Riemannian Geometry",
    "abstract": "           Antimicrobial peptide discovery is challenged by the astronomical size of peptide space and the relative scarcity of active peptides. Generative models provide continuous latent \"maps\" of peptide space, but conventionally ignore decoder-induced geometry and rely on flat Euclidean metrics, rendering exploration and optimization distorted and inefficient. Prior manifold-based remedies assume fixed intrinsic dimensionality, which critically fails in practice for peptide data. Here, we introduce PepCompass, a geometry-aware framework for peptide exploration and optimization. At its core, we define a Union of $\\kappa$-Stable Riemannian Manifolds $\\mathbb{M}^{\\kappa}$, a family of decoder-induced manifolds that captures local geometry while ensuring computational stability. We propose two local exploration methods: Second-Order Riemannian Brownian Efficient Sampling, which provides a convergent second-order approximation to Riemannian Brownian motion, and Mutation Enumeration in Tangent Space, which reinterprets tangent directions as discrete amino-acid substitutions. Combining these yields Local Enumeration Bayesian Optimization (LE-BO), an efficient algorithm for local activity optimization. Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which interpolates between prototype embeddings along property-enriched geodesics, biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro validation confirms the effectiveness of PepCompass: PoGS yields four novel seeds, and subsequent optimization with LE-BO discovers 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains. These results demonstrate that geometry-informed exploration provides a powerful new paradigm for antimicrobial peptide design.         ",
    "url": "https://arxiv.org/abs/2510.01988",
    "authors": [
      "Marcin Mo\u017cejko",
      "Adam Bielecki",
      "Jurand Pr\u0105dzy\u0144ski",
      "Marcin Traskowski",
      "Antoni Janowski",
      "Hyun-Su Lee",
      "Marcelo Der Torossian Torres",
      "Micha\u0142 Kmicikiewicz",
      "Paulina Szymczak",
      "Karol Jurasz",
      "Micha\u0142 Kucharczyk",
      "Cesar de la Fuente-Nunez",
      "Ewa Szczurek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.05014",
    "title": "Think Then Embed: Generative Context Improves Multimodal Embedding",
    "abstract": "           There is a growing interest in Universal Multimodal Embeddings (UME), where models are required to generate task-specific representations. While recent studies show that Multimodal Large Language Models (MLLMs) perform well on such tasks, they treat MLLMs solely as encoders, overlooking their generative capacity. However, such an encoding paradigm becomes less effective as instructions become more complex and require compositional reasoning. Inspired by the proven effectiveness of chain-of-thought reasoning, we propose a general Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an embedder. The reasoner MLLM first generates reasoning traces that explain complex queries, followed by an embedder that produces representations conditioned on both the original query and the intermediate reasoning. This explicit reasoning step enables more nuanced understanding of complex multimodal instructions. Our contributions are threefold. First, by leveraging a powerful MLLM reasoner, we achieve state-of-the-art performance on the MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune a smaller MLLM reasoner using high-quality embedding-centric reasoning traces, achieving the best performance among open-source models with a 7% absolute gain over recently proposed models. Third, we investigate strategies for integrating the reasoner and embedder into a unified model for improved efficiency without sacrificing performance.         ",
    "url": "https://arxiv.org/abs/2510.05014",
    "authors": [
      "Xuanming Cui",
      "Jianpeng Cheng",
      "Hong-you Chen",
      "Satya Narayan Shukla",
      "Abhijeet Awasthi",
      "Xichen Pan",
      "Chaitanya Ahuja",
      "Shlok Kumar Mishra",
      "Yonghuan Yang",
      "Jun Xiao",
      "Qi Guo",
      "Ser-Nam Lim",
      "Aashu Singh",
      "Xiangjun Fan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.11066",
    "title": "Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction",
    "abstract": "           Modern industrial recommendation systems improve recommendation performance by integrating multimodal representations from pre-trained models into ID-based Click-Through Rate (CTR) prediction frameworks. However, existing approaches typically adopt modality-centric modeling strategies that process ID-based and multimodal embeddings independently, failing to capture fine-grained interactions between content semantics and behavioral signals. In this paper, we propose Decoupled Multimodal Fusion (DMF), which introduces a modality-enriched modeling strategy to enable fine-grained interactions between ID-based collaborative representations and multimodal representations for user interest modeling. Specifically, we construct target-aware features to bridge the semantic gap across different embedding spaces and leverage them as side information to enhance the effectiveness of user interest modeling. Furthermore, we design an inference-optimized attention mechanism that decouples the computation of target-aware features and ID-based embeddings before the attention layer, thereby alleviating the computational bottleneck introduced by incorporating target-aware features. To achieve comprehensive multimodal integration, DMF combines user interest representations learned under the modality-centric and modality-enriched modeling strategies. Offline experiments on public and industrial datasets demonstrate the effectiveness of DMF. Moreover, DMF has been deployed on the product recommendation system of the international e-commerce platform Lazada, achieving relative improvements of 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.         ",
    "url": "https://arxiv.org/abs/2510.11066",
    "authors": [
      "Alin Fan",
      "Hanqing Li",
      "Sihan Lu",
      "Jingsong Yuan",
      "Jiandong Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2510.14431",
    "title": "Real-Time Neural Video Compression with Unified Intra and Inter Coding",
    "abstract": "           Neural video compression (NVC) technologies have advanced rapidly in recent years, yielding state-of-the-art schemes such as DCVC-RT that offer superior compression efficiency to H.266/VVC and real-time encoding/decoding capabilities. Nonetheless, existing NVC schemes have several limitations, including inefficiency in dealing with disocclusion and new content, interframe error propagation and accumulation, among others. To eliminate these limitations, we borrow the idea from classic video coding schemes, which allow intra coding within inter-coded frames. With the intra coding tool enabled, disocclusion and new content are properly handled, and interframe error propagation is naturally intercepted without the need for manual refresh mechanisms. We present an NVC framework with unified intra and inter coding, where every frame is processed by a single model that is trained to perform intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame compression design to exploit interframe redundancy not only forwardly but also backwardly. Experimental results show that our scheme outperforms DCVC-RT by an average of 12.1% BD-rate reduction, delivers more stable bitrate and quality per frame, and retains real-time encoding/decoding performances. Code and models will be released.         ",
    "url": "https://arxiv.org/abs/2510.14431",
    "authors": [
      "Hui Xiang",
      "Yifan Bian",
      "Li Li",
      "Jingran Wu",
      "Xianguo Zhang",
      "Dong Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.14889",
    "title": "Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media",
    "abstract": "           On social media, many individuals experiencing suicidal ideation (SI) do not disclose their distress explicitly. Instead, signs may surface indirectly through everyday posts or peer interactions. Detecting such implicit signals early is critical but remains challenging. We frame early and implicit SI as a forward-looking prediction task and develop a computational framework that models a user's information environment, consisting of both their longitudinal posting histories as well as the discourse of their socially proximal peers. We adopted a composite network centrality measure to identify top neighbors of a user, and temporally aligned the user's and neighbors' interactions -- integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves early and implicit SI detection by 15% over individual-only baselines. These findings highlight that peer interactions offer valuable predictive signals and carry broader implications for designing early detection systems that capture indirect as well as masked expressions of risk in online environments.         ",
    "url": "https://arxiv.org/abs/2510.14889",
    "authors": [
      "Soorya Ram Shimgekar",
      "Ruining Zhao",
      "Agam Goyal",
      "Violeta J. Rodriguez",
      "Paul A. Bloom",
      "Hari Sundaram",
      "Koustuv Saha"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2510.16386",
    "title": "Surrogate-Assisted Evolutionary Optimization Based on Interpretable Convolution Network",
    "abstract": "           When performing evolutionary optimization for computationally expensive objective, surrogate-assisted evolutionary algorithm(SAEA) is an effective approach. However, due to the limited availability of data in these scenarios, it can be challenging to create a highly accurate surrogate model, leading to reduced optimization effectiveness. To address this issue, we propose an Interpretable Convolution Network(ICN) for offline surrogate-assited evolutionary optimization. ICN retains the non-linear expression ability of traditional neural networks, while possessing the advantages of clear physical structure and the ability to incorporate prior knowledge during network parameter design and training process. We compare ICN-SAEA with tri-training method(TT-DDEA) and model-ensemble method(DDEA-SA) in several benchmark problems. Experimental results show that ICN-SAEA is better in searching optimal solution than compared algorithms.         ",
    "url": "https://arxiv.org/abs/2510.16386",
    "authors": [
      "Lihong Xu",
      "Wenxiang Jiang"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2510.16556",
    "title": "Fit for Purpose? Deepfake Detection in the Real World",
    "abstract": "           The rapid proliferation of AI-generated content, driven by advances in generative adversarial networks, diffusion models, and multimodal large language models, has made the creation and dissemination of synthetic media effortless, heightening the risks of misinformation, particularly political deepfakes that distort truth and undermine trust in political institutions. In turn, governments, research institutions, and industry have strongly promoted deepfake detection initiatives as solutions. Yet, most existing models are trained and validated on synthetic, laboratory-controlled datasets, limiting their generalizability to the kinds of real-world political deepfakes circulating on social platforms that affect the public. In this work, we introduce the first systematic benchmark based on the Political Deepfakes Incident Database, a curated collection of real-world political deepfakes shared on social media since 2018. Our study includes a systematic evaluation of state-of-the-art deepfake detectors across academia, government, and industry. We find that the detectors from academia and government perform relatively poorly. While paid detection tools achieve relatively higher performance than free-access models, all evaluated detectors struggle to generalize effectively to authentic political deepfakes, and are vulnerable to simple manipulations, especially in the video domain. Results urge the need for politically contextualized deepfake detection frameworks to better safeguard the public in real-world settings.         ",
    "url": "https://arxiv.org/abs/2510.16556",
    "authors": [
      "Guangyu Lin",
      "Li Lin",
      "Christina P. Walker",
      "Daniel S. Schiff",
      "Shu Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.21038",
    "title": "Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset",
    "abstract": "           Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from large, public benchmarks. However, current benchmarks target relatively simple, foundational tasks like Speech Detection and Phoneme Classification, while application-ready results on tasks like Brain-to-Text remain elusive. We propose Keyword Spotting (KWS) as a practically applicable, privacy-aware intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we provide standardized train/validation/test splits for reproducible benchmarking, and adopt an evaluation protocol tailored to extreme class imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as a robust evaluation metric, complemented by false alarms per hour (FA/h) at fixed recall to capture user-facing trade-offs. To simplify deployment and further experimentation within the research community, we are releasing an updated version of the pnpl library with word-level dataloaders and Colab-ready tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet baseline with focal loss and top-k pooling that is trainable on a single consumer-class GPU. The reference model achieves approximately 13x the permutation baseline AUPRC on held-out sessions, demonstrating the viability of the task. Exploratory analyses reveal: (i) predictable within-subject scaling - performance improves log-linearly with more training hours - and (ii) the existence of word-level factors (frequency and duration) that systematically modulate detectability.         ",
    "url": "https://arxiv.org/abs/2510.21038",
    "authors": [
      "Gereon Elvers",
      "Gilad Landau",
      "Oiwi Parker Jones"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.21513",
    "title": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair",
    "abstract": "           Today's pursuit of a single Large Language Model (LMM) for all software engineering tasks is resource-intensive and overlooks the potential benefits of complementarity, where different models contribute unique strengths. However, the degree to which coding LLMs complement each other and the best strategy for maximizing an ensemble's potential are unclear, leaving practitioners without a clear path to move beyond single-model systems. To address this gap, we empirically compare ten individual LLMs from five families, and three ensembles of these LLMs across three software engineering benchmarks covering code generation and program repair. We assess the complementarity between models and the performance gap between the best individual model and the ensembles. Next, we evaluate various selection heuristics to identify correct solutions from an ensemble's candidate pool. We find that the theoretical upperbound for an ensemble's performance can be 83% above the best single model. Our results show that consensus-based strategies for selecting solutions fall into a \"popularity trap,\" amplifying common but incorrect outputs. In contrast, a diversity-based strategy realizes up to 95% of this theoretical potential, and proves effective even in small two-model ensembles, enabling a cost-efficient way to enhance performance by leveraging multiple LLMs.         ",
    "url": "https://arxiv.org/abs/2510.21513",
    "authors": [
      "Fernando Vallecillos-Ruiz",
      "Max Hort",
      "Leon Moonen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.22094",
    "title": "Hierarchical Graph Networks for Accurate Weather Forecasting via Lightweight Training",
    "abstract": "           Climate events arise from intricate, multivariate dynamics governed by global-scale drivers, profoundly impacting food, energy, and infrastructure. Yet, accurate weather prediction remains elusive due to physical processes unfolding across diverse spatio-temporal scales, which fixed-resolution methods cannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscale representation, but nonlinear downward mappings often erase global trends, weakening the integration of physics into forecasts. We introduce HiFlowCast and its ensemble variant HiAntFlow, HGNNs that embed physics within a multiscale prediction framework. Two innovations underpin their design: a Latent-Memory-Retention mechanism that preserves global trends during downward traversal, and a Latent-to-Physics branch that integrates PDE solution fields across diverse scales. Our Flow models cut errors by over 5% at 13-day lead times and by 5-8% under 1st and 99th quantile extremes, improving reliability for rare events. Leveraging pretrained model weights, they converge within a single epoch, reducing training cost and their carbon footprint. Such efficiency is vital as the growing scale of machine learning challenges sustainability and limits research accessibility. Code and model weights are in the supplementary materials.         ",
    "url": "https://arxiv.org/abs/2510.22094",
    "authors": [
      "Thomas Bailie",
      "S. Karthik Mukkavilli",
      "Varvara Vetrova",
      "Yun Sing Koh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2510.22400",
    "title": "ProGQL: A Provenance Graph Query System for Cyber Attack Investigation",
    "abstract": "           Provenance analysis (PA) has recently emerged as an important solution for cyber attack investigation. PA leverages system monitoring to monitor system activities as a series of system audit events and organizes these events as a provenance graph to show the dependencies among system activities, which can reveal steps of cyber attacks. Despite their potential, existing PA techniques face two critical challenges: (1) they are inflexible and non-extensible, making it difficult to incorporate analyst expertise, and (2) they are memory inefficient, often requiring>100GB of RAM to hold entire event streams, which fundamentally limits scalability and deployment in real-world environments. To address these limitations, we propose the ProGQL framework, which provides a domain-specific graph search language with a well-engineered query engine, allowing PA over system audit events and expert knowledge to be jointly expressed as a graph search query and thereby facilitating the investigation of complex cyberattacks. In particular, to support dependency searches from a starting edge required in PA, ProGQL introduces new language constructs for constrained graph traversal, edge weight computation, value propagation along weighted edges, and graph merging to integrate multiple searches. Moreover, the ProGQL query engine is optimized for efficient incremental graph search across heterogeneous database backends, eliminating the need for full in-memory materialization and reducing memory overhead. Our evaluations on real attacks demonstrate the effectiveness of the ProGQL language in expressing a diverse set of complex attacks compared with the state-of-the-art graph query language Cypher, and the comparison with the SOTA PA technique DEPIMPACT further demonstrates the significant improvement of the scalability brought by our ProGQL framework's design.         ",
    "url": "https://arxiv.org/abs/2510.22400",
    "authors": [
      "Fei Shao",
      "Jia Zou",
      "Zhichao Cao",
      "Xusheng Xiao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2510.23117",
    "title": "Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction",
    "abstract": "           Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs. The complete data and code are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.23117",
    "authors": [
      "Omer Jauhar Khan",
      "Sudais Khan",
      "Hafeez Anwar",
      "Shahzeb Khan",
      "Shams Ul Arifeen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.24043",
    "title": "Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection",
    "abstract": "           This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection framework that overcomes the coexisting limitations of conventional projection-based methods: their reliance on a fixed statistical metric and their assumption of a single data structure. Our framework uniquely synthesizes three key concepts: (1) a generalized loss-based outlyingness measure (PLO) that replaces the fixed metric with flexible, adaptive loss functions like our proposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear data structures; and (3) a subsequent local clustering stage to handle multi-modal distributions. Comprehensive 5-fold cross-validation experiments on 10 benchmark datasets, with automated hyperparameter optimization, demonstrate that Two-Stage LKPLO achieves state-of-the-art performance. It significantly outperforms strong baselines on datasets with challenging structures where existing methods fail, most notably on multi-cluster data (Optdigits) and complex, high-dimensional data (Arrhythmia). Furthermore, an ablation study empirically confirms that the synergistic combination of both the kernelization and localization stages is indispensable for its superior performance. This work contributes a powerful new tool for a significant class of outlier detection problems and underscores the importance of hybrid, multi-stage architectures.         ",
    "url": "https://arxiv.org/abs/2510.24043",
    "authors": [
      "Akira Tamamori"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2510.25366",
    "title": "A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks",
    "abstract": "           The key task of machine learning is to minimize the loss function that measures the model fit to the training data. The numerical methods to do this efficiently depend on the properties of the loss function. The most decisive among these properties is the convexity or non-convexity of the loss function. The fact that the loss function can have, and frequently has, non-convex regions has led to a widespread commitment to non-convex methods such as Adam. However, a local minimum implies that, in some environment around it, the function is convex. In this environment, second-order minimizing methods such as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We propose a novel framework grounded in the hypothesis that loss functions in real-world tasks swap from initial non-convexity to convexity towards the optimum. This is a property we leverage to design an innovative two-phase optimization algorithm. The presented algorithm detects the swap point by observing the gradient norm dependence on the loss. In these regions, non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing experiments confirm the hypothesis that this simple convexity structure is frequent enough to be practically exploited to substantially improve convergence and accuracy.         ",
    "url": "https://arxiv.org/abs/2510.25366",
    "authors": [
      "Tomas Hrycej",
      "Bernhard Bermeitinger",
      "Massimo Pavone",
      "G\u00f6tz-Henrik Wiegand",
      "Siegfried Handschuh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2510.25406",
    "title": "Dissect-and-Restore: AI-based Code Verification with Transient Refactoring",
    "abstract": "           Formal verification is increasingly recognized as a critical foundation for building reliable software systems. However, the need for specialized expertise to write precise specifications, navigate complex proof obligations, and learn annotations often makes verification an order of magnitude more expensive than implementation. While modern AI systems can recognize patterns in mathematical proofs and interpret natural language, effectively integrating them into the formal verification process remains an open challenge. We present Prometheus, a novel AI-assisted system that facilitates automated code verification with current AI capabilities in conjunction with modular software engineering principles (e.g., modular refactoring). Our approach begins by decomposing complex program logic, such as nested loops, into smaller, verifiable components. Once verified, these components are recomposed to construct a proof of the original program. This decomposition-recomposition workflow is non-trivial. Prometheus addresses this by guiding the proof search through structured decomposition of complex lemmas into smaller, verifiable sub-lemmas. When automated tools are insufficient, users can provide lightweight natural language guidance to steer the proof process effectively. Our evaluation demonstrates that transiently applying modular restructuring to the code substantially improves the AI's effectiveness in verifying individual components. This approach successfully verifies 86% of tasks in our curated dataset, compared to 68% for the baseline. Gains are more pronounced with increasing specification complexity, improving from 30% to 69%, and when integrating proof outlines for complex programs, from 25% to 87%.         ",
    "url": "https://arxiv.org/abs/2510.25406",
    "authors": [
      "Changjie Wang",
      "Mariano Scazzariello",
      "Anoud Alshnakat",
      "Roberto Guanciale",
      "Dejan Kosti\u0107",
      "Marco Chiesa"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2510.25622",
    "title": "MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation",
    "abstract": "           Industrial recommender systems rely on unique Item Identifiers (ItemIDs). However, this method struggles with scalability and generalization in large, dynamic datasets that have sparse long-tail data. Content-based Semantic IDs (SIDs) address this by sharing knowledge through content quantization. However, by ignoring dynamic behavioral properties, purely content-based SIDs have limited expressive power. Existing methods attempt to incorporate behavioral information but overlook a critical distinction: unlike relatively uniform content features, user-item interactions are highly skewed and diverse, creating a vast information gap in quality and quantity between popular and long-tail items. This oversight leads to two critical limitations: (1) Noise Corruption: Indiscriminate behavior-content alignment allows collaborative noise from long-tail items to corrupt their content representations, leading to the loss of critical multimodal information. (2)Signal Obscurity: The equal-weighting scheme for SIDs fails to reflect the varying importance of different behavioral signals, making it difficult for downstream tasks to distinguish important SIDs from uninformative ones. To tackle these issues, we propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align, Denoise, and Amplify multimodal information from content and behavior modalities for semantic IDs learning. The semantic IDs generated by this framework named ADA-SID. It introduces two innovations: an adaptive behavior-content alignment that is aware of information richness to shield representations from noise, and a dynamic behavioral router to amplify critical signals by applying different weights to SIDs. Extensive experiments on public and large-scale industrial datasets demonstrate ADA-SID's significant superiority in both generative and discriminative recommendation tasks.         ",
    "url": "https://arxiv.org/abs/2510.25622",
    "authors": [
      "Yi Xu",
      "Moyu Zhang",
      "Chaofan Fan",
      "Jinxin Hu",
      "Xiaochen Li",
      "Yu Zhang",
      "Xiaoyi Zeng",
      "Jing Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2510.25697",
    "title": "Fourier Neural Operators for Two-Phase, 2D Mold-Filling Problems Related to Metal Casting",
    "abstract": "           We formulate mold filling in metal casting as a 2D neural operator learning problem that maps geometry and boundary data on an unstructured mesh to time resolved flow quantities, replacing expensive transient CFD. In the proposed method, a graph based encoder aggregates local neighborhood information on the input mesh and encodes geometry and boundary data, a Fourier spectral core operates on a regular latent grid to capture global interactions across the domain, and a graph based decoder projects the latent fields to a target mesh. The model is trained to jointly predict velocity components, pressure, and liquid volume fraction over a fixed rollout horizon and generalizes across different ingate locations and process settings. On held out geometries and inlet conditions, it reproduces large scale advection and the fluid-air interface evolution with localized errors near steep gradients. The mean relative L2 error is about 5% across all fields, and inference is two to three orders of magnitude faster than conventional CFD, enabling design in the loop exploration. Ablation studies show monotonic accuracy degradation under stronger spatial subsampling of input vertices and a smoother decline under temporal subsampling. Halving the training set yields only a small increase in error. These results establish neural operators as accurate and data efficient surrogates for 2D mold filling and enable rapid optimization of gating systems in casting workflows.         ",
    "url": "https://arxiv.org/abs/2510.25697",
    "authors": [
      "Edgard Moreira Minete",
      "Mathis Immertreu",
      "Fabian Teichmann",
      "Sebastian M\u00fcller"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2407.11873",
    "title": "Infinite-dimensional Mahalanobis Distance with Applications to Kernelized Novelty Detection",
    "abstract": "           The Mahalanobis distance is a classical tool used to measure the covariance-adjusted distance between points in $\\bbR^d$. In this work, we extend the concept of Mahalanobis distance to separable Banach spaces by reinterpreting it as a Cameron-Martin norm associated with a probability measure. This approach leads to a basis-free, data-driven notion of anomaly distance through the so-called variance norm, which can naturally be estimated using empirical measures of a sample. Our framework generalizes the classical $\\bbR^d$, functional $(L^2[0,1])^d$, and kernelized settings; importantly, it incorporates non-injective covariance operators. We prove that the variance norm is invariant under invertible bounded linear transformations of the data, extending previous results which are limited to unitary operators. In the Hilbert space setting, we connect the variance norm to the RKHS of the covariance operator, and establish consistency and convergence results for estimation using empirical measures with Tikhonov regularization. Using the variance norm, we introduce the notion of a kernelized nearest-neighbour Mahalanobis distance, and study some of its finite-sample concentration properties. In an empirical study on 12 real-world data sets, we demonstrate that the kernelized nearest-neighbour Mahalanobis distance outperforms the traditional kernelized Mahalanobis distance for multivariate time series novelty detection, using state-of-the-art time series kernels such as the signature, global alignment, and Volterra reservoir kernels.         ",
    "url": "https://arxiv.org/abs/2407.11873",
    "authors": [
      "Nikita Zozoulenko",
      "Thomas Cass",
      "Lukas Gonon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2505.17789",
    "title": "Optimal Online Change Detection via Random Fourier Features",
    "abstract": "           This article studies the problem of online non-parametric change point detection in multivariate data streams. We approach the problem through the lens of kernel-based two-sample testing and introduce a sequential testing procedure based on random Fourier features, running with logarithmic time complexity per observation and with overall logarithmic space complexity. The algorithm has two advantages compared to the state of the art. First, our approach is genuinely online, and no access to training data known to be from the pre-change distribution is necessary. Second, the algorithm does not require the user to specify a window parameter over which local tests are to be calculated. We prove strong theoretical guarantees on the algorithm's performance, including information-theoretic bounds demonstrating that the detection delay is optimal in the minimax sense. Numerical studies on real and synthetic data show that our algorithm is competitive with respect to the state of the art.         ",
    "url": "https://arxiv.org/abs/2505.17789",
    "authors": [
      "Florian Kalinke",
      "Shakeel Gavioli-Akilagun"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.03237",
    "title": "UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection",
    "abstract": "           The detection of ligand binding sites for proteins is a fundamental step in Structure-Based Drug Design. Despite notable advances in recent years, existing methods, datasets, and evaluation metrics are confronted with several key challenges: (1) current datasets and methods are centered on individual protein-ligand complexes and neglect that diverse binding sites may exist across multiple complexes of the same protein, introducing significant statistical bias; (2) ligand binding site detection is typically modeled as a discontinuous workflow, employing binary segmentation and subsequent clustering algorithms; (3) traditional evaluation metrics do not adequately reflect the actual performance of different binding site prediction methods. To address these issues, we first introduce UniSite-DS, the first UniProt (Unique Protein)-centric ligand binding site dataset, which contains 4.81 times more multi-site data and 2.08 times more overall data compared to the previously most widely used datasets. We then propose UniSite, the first end-to-end ligand binding site detection framework supervised by set prediction loss with bijective matching. In addition, we introduce Average Precision based on Intersection over Union (IoU) as a more accurate evaluation metric for ligand binding site prediction. Extensive experiments on UniSite-DS and several representative benchmark datasets demonstrate that IoU-based Average Precision provides a more accurate reflection of prediction quality, and that UniSite outperforms current state-of-the-art methods in ligand binding site detection. The dataset and codes will be made publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.03237",
    "authors": [
      "Jigang Fan",
      "Quanlin Wu",
      "Shengjie Luo",
      "Liwei Wang"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2506.05567",
    "title": "Partially-Supervised Neural Network Model For Quadratic Multiparametric Programming",
    "abstract": "           Neural Networks (NN) with ReLU activation functions are used to model multiparametric quadratic optimization problems (mp-QP) in diverse engineering applications. Researchers have suggested leveraging the piecewise affine property of deep NN models to solve mp-QP with linear constraints, which also exhibit piecewise affine behaviour. However, traditional deep NN applications to mp-QP fall short of providing optimal and feasible predictions, even when trained on large datasets. This study proposes a partially-supervised NN (PSNN) architecture that directly represents the mathematical structure of the global solution function. In contrast to generic NN training approaches, the proposed PSNN method derives a large proportion of model weights directly from the mathematical properties of the optimization problem, producing more accurate solutions despite significantly smaller training data sets. Many energy management problems are formulated as QP, so we apply the proposed approach to energy systems (specifically DC optimal power flow) to demonstrate proof of concept. Model performance in terms of solution accuracy and speed of predictions was compared against a commercial solver and a generic Deep NN model based on classical training. Results show KKT sufficient conditions for PSNN consistently outperform generic NN architectures with classical training using far less data, including when tested on extreme, out-of-training distribution test data. Given its speed advantages over traditional solvers, the PSNN model can quickly produce optimal and feasible solutions within a second for millions of input parameters sampled from a distribution of stochastic demands and renewable generator dispatches, which can be used for simulations and long term planning.         ",
    "url": "https://arxiv.org/abs/2506.05567",
    "authors": [
      "Fuat Can Beylunioglu",
      "Mehrdad Pirnia",
      "P. Robert Duimering"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.08896",
    "title": "Predictive Causal Inference via Spatio-Temporal Modeling and Penalized Empirical Likelihood",
    "abstract": "           This study introduces an integrated framework for predictive causal inference designed to overcome limitations inherent in conventional single model approaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial health state estimation with a Multi Task and Multi Graph Convolutional Network (MTGCN) for capturing temporal outcome trajectories. The framework asymmetrically treats temporal and spatial information regarding them as endogenous variables in the outcome regression, and exogenous variables in the propensity score model, thereby expanding the standard doubly robust treatment effect estimation to jointly enhance bias correction and predictive accuracy. To demonstrate its utility, we focus on clinical domains such as cancer, dementia, and Parkinson disease, where treatment effects are challenging to observe directly. Simulation studies are conducted to emulate latent disease dynamics and evaluate the model performance under varying conditions. Overall, the proposed framework advances predictive causal inference by structurally adapting to spatiotemporal complexities common in biomedical data.         ",
    "url": "https://arxiv.org/abs/2507.08896",
    "authors": [
      "Byunghee Lee",
      "Hye Yeon Sin",
      "Joonsung Kang"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2509.14213",
    "title": "PoPStat-COVID19: Leveraging Population Pyramids to Quantify Demographic Vulnerability to COVID-19",
    "abstract": "           Understanding how population age structure shapes COVID-19 burden is crucial for pandemic preparedness, yet common summary measures such as median age ignore key distributional features like skewness, bimodality, and the proportional weight of high-risk cohorts. We extend the PoPStat framework, originally devised to link entire population pyramids with cause-specific mortality by applying it to COVID-19. Using 2019 United Nations World Population Prospects age-sex distributions together with cumulative cases and deaths per million recorded up to 5 May 2023 by Our World in Data, we calculate PoPDivergence (the Kullback-Leibler divergence from an optimised reference pyramid) for 180+ countries and derive PoPStat-COVID19 as the Pearson correlation between that divergence and log-transformed incidence or mortality. Optimisation selects Malta's old-skewed pyramid as the reference, yielding strong negative correlations for cases (r=-0.86, p<0.001, R^2=0.74) and deaths (r=-0.82, p<0.001, R^2=0.67). Sensitivity tests across twenty additional, similarly old-skewed references confirm that these associations are robust to reference choice. Benchmarking against eight standard indicators like gross domestic product per capita, Gini index, Human Development Index, life expectancy at birth, median age, population density, Socio-demographic Index, and Universal Health Coverage Index shows that PoPStat-COVID19 surpasses GDP per capita, median age, population density, and several other traditional measures, and outperforms every comparator for fatality burden. PoPStat-COVID19 therefore provides a concise, distribution-aware scalar for quantifying demographic vulnerability to COVID-19.         ",
    "url": "https://arxiv.org/abs/2509.14213",
    "authors": [
      "Buddhi Wijenayake",
      "Athulya Ratnayake",
      "Lelumi Edirisinghe",
      "Uditha Wijeratne",
      "Tharaka Fonseka",
      "Roshan Godaliyadda",
      "Samath Dharmaratne",
      "Parakrama Ekanayake",
      "Vijitha Herath",
      "Insoha Alwis",
      "Supun Manathunga"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2509.20410",
    "title": "Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex Speech Interaction",
    "abstract": "           Spoken dialogue models have significantly advanced intelligent human-computer interaction, yet they lack a plug-and-play full-duplex prediction module for semantic endpoint detection, hindering seamless audio interactions. In this paper, we introduce Phoenix-VAD, an LLM-based model that enables streaming semantic endpoint detection. Specifically, Phoenix-VAD leverages the semantic comprehension capability of the LLM and a sliding window training strategy to achieve reliable semantic endpoint detection while supporting streaming inference. Experiments on both semantically complete and incomplete speech scenarios indicate that Phoenix-VAD achieves excellent and competitive performance. Furthermore, this design enables the full-duplex prediction module to be optimized independently of the dialogue model, providing more reliable and flexible support for next-generation human-computer interaction.         ",
    "url": "https://arxiv.org/abs/2509.20410",
    "authors": [
      "Weijie Wu",
      "Wenhao Guan",
      "Kaidi Wang",
      "Peijie Chen",
      "Zhuanling Zha",
      "Junbo Li",
      "Jun Fang",
      "Lin Li",
      "Qingyang Hong"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2510.02781",
    "title": "GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular Degeneration Risk Factor Detection and Prediction",
    "abstract": "           Age Related Macular Degeneration(AMD) has been one of the most leading causes of permanent vision impairment in ophthalmology. Though treatments, such as anti VEGF drugs or photodynamic therapies, were developed to slow down the degenerative process of AMD, there is still no specific cure to reverse vision loss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD or AMD itself within the patient retina in early stages is a crucial task to reduce the possibility of vision impairment. Apart from traditional approaches, deep learning based methods, especially attention mechanism based CNNs and GradCAM based XAI analysis on OCT scans, exhibited successful performance in distinguishing AMD retina from normal retinas, making it possible to use AI driven models to aid medical diagnosis and analysis by ophthalmologists regarding AMD. However, though having significant success, previous works mostly focused on prediction performance itself, not pathologies or underlying causal mechanisms of AMD, which can prohibit intervention analysis on specific factors or even lead to less reliable decisions. Thus, this paper introduces a novel causal AMD analysis model: GCVAMD, which incorporates a modified CausalVAE approach that can extract latent causal factors from only raw OCT images. By considering causality in AMD detection, GCVAMD enables causal inference such as treatment simulation or intervention analysis regarding major risk factors: drusen and neovascularization, while returning informative latent causal features that can enhance downstream tasks. Results show that through GCVAMD, drusen status and neovascularization status can be identified with AMD causal mechanisms in GCVAMD latent spaces, which can in turn be used for various tasks from AMD detection(classification) to intervention analysis.         ",
    "url": "https://arxiv.org/abs/2510.02781",
    "authors": [
      "Daeyoung Kim"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.10728",
    "title": "Rough Path Signatures: Learning Neural RDEs for Portfolio Optimization",
    "abstract": "           We tackle high-dimensional, path-dependent valuation and control and introduce a deep BSDE/2BSDE solver that couples truncated log-signatures with a neural rough differential equation (RDE) backbone. The architecture aligns stochastic analysis with sequence-to-path learning: a CVaR-tilted terminal objective targets left-tail risk, while an optional second-order (2BSDE) head supplies curvature estimates for risk-sensitive control. Under matched compute and parameter budgets, the method improves accuracy, tail fidelity, and training stability across Asian and barrier option pricing and portfolio control: at d=200 it achieves CVaR(0.99)=9.80% versus 12.00-13.10% for strong baselines, attains the lowest HJB residual (0.011), and yields the lowest RMSEs for Z and Gamma. Ablations over truncation depth, local windows, and tilt parameters confirm complementary gains from the sequence-to-path representation and the 2BSDE head. Taken together, the results highlight a bidirectional dialogue between stochastic analysis and modern deep learning: stochastic tools inform representations and objectives, while sequence-to-path models expand the class of solvable financial models at scale.         ",
    "url": "https://arxiv.org/abs/2510.10728",
    "authors": [
      "Ali Atiah Alzahrani"
    ],
    "subjectives": [
      "Mathematical Finance (q-fin.MF)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.22720",
    "title": "Larger holes as narrower degree distributions in complex networks",
    "abstract": "           Although the analysis of loops is not so much because of the complications, it has already been found that heuristically enhancing loops decreases the variance of degree distributions for improving the robustness of connectivity. While many real scale-free networks are known to contain shorter loops such as triangles, it remains to investigate the distributions of longer loops in more wide class of networks. We find a relation between narrower degree distributions and longer loops in investigating the lengths of the shortest loops in various networks with continuously changing degree distributions, including three typical types of realistic scale-free networks, classical Erd\u00f6s-R\u00e9nyi random graphs, and regular networks. In particular, we show that narrower degree distributions contain longer shortest loops, as a universal property in a wide class of random networks. We suggest that the robustness of connectivity is enhanced by constructing long loops of O(log N).         ",
    "url": "https://arxiv.org/abs/2510.22720",
    "authors": [
      "Kiri Kawato",
      "Yukio Hayashi"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Discrete Mathematics (cs.DM)",
      "Social and Information Networks (cs.SI)"
    ]
  }
]