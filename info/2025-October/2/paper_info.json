[
  {
    "id": "arXiv:2510.00030",
    "title": "Temporal-Aware Iterative Speech Model for Dementia Detection",
    "abstract": "           Deep learning systems often struggle with processing long sequences, where computational complexity can become a bottleneck. Current methods for automated dementia detection using speech frequently rely on static, time-agnostic features or aggregated linguistic content, lacking the flexibility to model the subtle, progressive deterioration inherent in speech production. These approaches often miss the dynamic temporal patterns that are critical early indicators of cognitive decline. In this paper, we introduce TAI-Speech, a Temporal Aware Iterative framework that dynamically models spontaneous speech for dementia detection. The flexibility of our method is demonstrated through two key innovations: 1) Optical Flow-inspired Iterative Refinement: By treating spectrograms as sequential frames, this component uses a convolutional GRU to capture the fine-grained, frame-to-frame evolution of acoustic features. 2) Cross-Attention Based Prosodic Alignment: This component dynamically aligns spectral features with prosodic patterns, such as pitch and pauses, to create a richer representation of speech production deficits linked to functional decline (IADL). TAI-Speech adaptively models the temporal evolution of each utterance, enhancing the detection of cognitive markers. Experimental results on the DementiaBank dataset show that TAI-Speech achieves a strong AUC of 0.839 and 80.6\\% accuracy, outperforming text-based baselines without relying on ASR. Our work provides a more flexible and robust solution for automated cognitive assessment, operating directly on the dynamics of raw audio.         ",
    "url": "https://arxiv.org/abs/2510.00030",
    "authors": [
      "Chukwuemeka Ugwu",
      "Oluwafemi Oyeleke"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2510.00031",
    "title": "VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs",
    "abstract": "           We propose VibeCodeHPC, an automatic tuning system for HPC programs based on multi-agent LLMs for code generation. VibeCodeHPC tunes programs through multi-agent role allocation and iterative prompt refinement. We describe the system configuration with four roles: Project Manager (PM), System Engineer (SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent deployment and activity monitoring functions to facilitate effective multi-agent collaboration. In our case study, we convert and optimize CPU-based matrix-matrix multiplication code written in C to GPU code using CUDA. The multi-agent configuration of VibeCodeHPC achieved higher-quality code generation per unit time compared to a solo-agent configuration. Additionally, the dynamic agent deployment and activity monitoring capabilities facilitated more effective identification of requirement violations and other issues.         ",
    "url": "https://arxiv.org/abs/2510.00031",
    "authors": [
      "Shun-ichiro Hayashi",
      "Koki Morita",
      "Daichi Mukunoki",
      "Tetsuya Hoshino",
      "Takahiro Katagiri"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2510.00037",
    "title": "On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations",
    "abstract": "           In Vision-Language-Action (VLA) models, robustness to real-world perturbations is critical for deployment. Existing methods target simple visual disturbances, overlooking the broader multi-modal perturbations that arise in actions, instructions, environments, and observations. Here, we first evaluate the robustness of mainstream VLAs under 17 perturbations across four modalities. We find (1) actions as the most fragile modality, (2) Existing visual-robust VLA do not gain robustness in other modality, and (3) pi0 demonstrates superior robustness with a diffusion-based action head. To build multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA inputs and outputs. For output robustness, we perform offline robust optimization against worst-case action noise that maximizes mismatch in flow matching objective. This can be seen as adversarial training, label smoothing, and outlier penalization. For input robustness, we enforce consistent actions across input variations that preserve task semantics. To account for multiple perturbations, we formulate robustness as a multi-armed bandit problem and apply an upper confidence bound algorithm to automatically identify the most harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations. Our RobustVLA is particularly effective on real-world FR5 robot with limited demonstrations, showing absolute gains by 65.6% under perturbations of four modalities.         ",
    "url": "https://arxiv.org/abs/2510.00037",
    "authors": [
      "Jianing Guo",
      "Zhenhong Wu",
      "Chang Tu",
      "Yiyao Ma",
      "Xiangqi Kong",
      "Zhiqian Liu",
      "Jiaming Ji",
      "Shuning Zhang",
      "Yuanpei Chen",
      "Kai Chen",
      "Xianglong Liu",
      "Qi Dou",
      "Yaodong Yang",
      "Huijie Zhao",
      "Weifeng Lv",
      "Simin Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00039",
    "title": "AutoPK: Leveraging LLMs and a Hybrid Similarity Metric for Advanced Retrieval of Pharmacokinetic Data from Complex Tables and Documents",
    "abstract": "           Pharmacokinetics (PK) plays a critical role in drug development and regulatory decision-making for human and veterinary medicine, directly affecting public health through drug safety and efficacy assessments. However, PK data are often embedded in complex, heterogeneous tables with variable structures and inconsistent terminologies, posing significant challenges for automated PK data retrieval and standardization. AutoPK, a novel two-stage framework for accurate and scalable extraction of PK data from complex scientific tables. In the first stage, AutoPK identifies and extracts PK parameter variants using large language models (LLMs), a hybrid similarity metric, and LLM-based validation. The second stage filters relevant rows, converts the table into a key-value text format, and uses an LLM to reconstruct a standardized table. Evaluated on a real-world dataset of 605 PK tables, including captions and footnotes, AutoPK shows significant improvements in precision and recall over direct LLM baselines. For instance, AutoPK with LLaMA 3.1-70B achieved an F1-score of 0.92 on half-life and 0.91 on clearance parameters, outperforming direct use of LLaMA 3.1-70B by margins of 0.10 and 0.21, respectively. Smaller models such as Gemma 3-27B and Phi 3-12B with AutoPK achieved 2-7 fold F1 gains over their direct use, with Gemma's hallucination rates reduced from 60-95% down to 8-14%. Notably, AutoPK enabled open-source models like Gemma 3-27B to outperform commercial systems such as GPT-4o Mini on several PK parameters. AutoPK enables scalable and high-confidence PK data extraction, making it well-suited for critical applications in veterinary pharmacology, drug safety monitoring, and public health decision-making, while addressing heterogeneous table structures and terminology and demonstrating generalizability across key PK parameters. Code and data: this https URL ",
    "url": "https://arxiv.org/abs/2510.00039",
    "authors": [
      "Hossein Sholehrasa",
      "Amirhossein Ghanaatian",
      "Doina Caragea",
      "Lisa A. Tell",
      "Jim E. Riviere",
      "Majid Jaberi-Douraki"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2510.00059",
    "title": "FSDENet: A Frequency and Spatial Domains based Detail Enhancement Network for Remote Sensing Semantic Segmentation",
    "abstract": "           To fully leverage spatial information for remote sensing image segmentation and address semantic edge ambiguities caused by grayscale variations (e.g., shadows and low-contrast regions), we propose the Frequency and Spatial Domains based Detail Enhancement Network (FSDENet). Our framework employs spatial processing methods to extract rich multi-scale spatial features and fine-grained semantic details. By effectively integrating global and frequency-domain information through the Fast Fourier Transform (FFT) in global mappings, the model's capability to discern global representations under grayscale variations is significantly strengthened. Additionally, we utilize Haar wavelet transform to decompose features into high- and low-frequency components, leveraging their distinct sensitivity to edge information to refine boundary segmentation. The model achieves dual-domain synergy by integrating spatial granularity with frequency-domain edge sensitivity, substantially improving segmentation accuracy in boundary regions and grayscale transition zones. Comprehensive experimental results demonstrate that FSDENet achieves state-of-the-art (SOTA) performance on four widely adopted datasets: LoveDA, Vaihingen, Potsdam, and iSAID.         ",
    "url": "https://arxiv.org/abs/2510.00059",
    "authors": [
      "Jiahao Fu",
      "Yinfeng Yu",
      "Liejun Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00065",
    "title": "Federated Learning Meets LLMs: Feature Extraction From Heterogeneous Clients",
    "abstract": "           Federated learning (FL) enables collaborative model training without sharing raw data, making it attractive for privacy-sensitive domains such as healthcare, finance, and IoT. A major obstacle, however, is the heterogeneity of tabular data across clients, where divergent schemas and incompatible feature spaces prevent straightforward aggregation. To address this challenge, we propose FedLLM-Align, a federated framework that leverages pre-trained large language models (LLMs) as universal feature extractors. Tabular records are serialized into text, and embeddings from models such as DistilBERT, ALBERT, RoBERTa, and ClinicalBERT provide semantically aligned representations that support lightweight local classifiers under the standard FedAvg protocol. This approach removes the need for manual schema harmonization while preserving privacy, since raw data remain strictly local. We evaluate FedLLM-Align on coronary heart disease prediction using partitioned Framingham datasets with simulated schema divergence. Across all client settings and LLM backbones, our method consistently outperforms state-of-the-art baselines, achieving up to +0.25 improvement in F1-score and a 65% reduction in communication cost. Stress testing under extreme schema divergence further demonstrates graceful degradation, unlike traditional methods that collapse entirely. These results establish FedLLM-Align as a robust, privacy-preserving, and communication-efficient solution for federated learning in heterogeneous environments.         ",
    "url": "https://arxiv.org/abs/2510.00065",
    "authors": [
      "Abdelrhman Gaber",
      "Hassan Abd-Eltawab",
      "Youssif Abuzied",
      "Muhammad ElMahdy",
      "Tamer ElBatt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00080",
    "title": "SoREX: Towards Self-Explainable Social Recommendation with Relevant Ego-Path Extraction",
    "abstract": "           Social recommendation has been proven effective in addressing data sparsity in user-item interaction modeling by leveraging social networks. The recent integration of Graph Neural Networks (GNNs) has further enhanced prediction accuracy in contemporary social recommendation algorithms. However, many GNN-based approaches in social recommendation lack the ability to furnish meaningful explanations for their predictions. In this study, we confront this challenge by introducing SoREX, a self-explanatory GNN-based social recommendation framework. SoREX adopts a two-tower framework enhanced by friend recommendation, independently modeling social relations and user-item interactions, while jointly optimizing an auxiliary task to reinforce social signals. To offer explanations, we propose a novel ego-path extraction approach. This method involves transforming the ego-net of a target user into a collection of multi-hop ego-paths, from which we extract factor-specific and candidate-aware ego-path subsets as explanations. This process facilitates the summarization of detailed comparative explanations among different candidate items through intricate substructure analysis. Furthermore, we conduct explanation re-aggregation to explicitly correlate explanations with downstream predictions, imbuing our framework with inherent self-explainability. Comprehensive experiments conducted on four widely adopted benchmark datasets validate the effectiveness of SoREX in predictive accuracy. Additionally, qualitative and quantitative analyses confirm the efficacy of the extracted explanations in SoREX. Our code and data are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00080",
    "authors": [
      "Hanze Guo",
      "Yijun Ma",
      "Xiao Zhou"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00083",
    "title": "Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks",
    "abstract": "           Deep neural networks have been widely adopted in many vision and robotics applications with visual inputs. It is essential to verify its robustness against semantic transformation perturbations, such as brightness and contrast. However, current certified training and robustness certification methods face the challenge of over-parameterization, which hinders the tightness and scalability due to the over-complicated neural networks. To this end, we first analyze stability and variance of layers and neurons against input perturbation, showing that certifiable robustness can be indicated by a fundamental Unbiased and Smooth Neuron metric (USN). Based on USN, we introduce a novel neural network pruning method that removes neurons with low USN and retains those with high USN, thereby preserving model expressiveness without over-parameterization. To further enhance this pruning process, we propose a new Wasserstein distance loss to ensure that pruned neurons are more concentrated across layers. We validate our approach through extensive experiments on the challenging robust keypoint detection task, which involves realistic brightness and contrast perturbations, demonstrating that our method achieves superior robustness certification performance and efficiency compared to baselines.         ",
    "url": "https://arxiv.org/abs/2510.00083",
    "authors": [
      "Hanjiang Hu",
      "Bowei Li",
      "Ziwei Wang",
      "Tianhao Wei",
      "Casidhe Hutchison",
      "Eric Sample",
      "Changliu Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00088",
    "title": "Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction",
    "abstract": "           Large language models (LLMs) have been extensively used for legal judgment prediction tasks based on case reports and crime history. However, with a surge in the availability of large vision language models (VLMs), legal judgment prediction systems can now be made to leverage the images of the criminals in addition to the textual case reports/crime history. Applications built in this way could lead to inadvertent consequences and be used with malicious intent. In this work, we run an audit to investigate the efficiency of standalone VLMs in the bail decision prediction task. We observe that the performance is poor across multiple intersectional groups and models \\textit{wrongly deny bail to deserving individuals with very high confidence}. We design different intervention algorithms by first including legal precedents through a RAG pipeline and then fine-tuning the VLMs using innovative schemes. We demonstrate that these interventions substantially improve the performance of bail prediction. Our work paves the way for the design of smarter interventions on VLMs in the future, before they can be deployed for real-world legal judgment prediction.         ",
    "url": "https://arxiv.org/abs/2510.00088",
    "authors": [
      "Sagnik Basu",
      "Shubham Prakash",
      "Ashish Maruti Barge",
      "Siddharth D Jaiswal",
      "Abhisek Dash",
      "Saptarshi Ghosh",
      "Animesh Mukherjee"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2510.00133",
    "title": "Large Language Models Inference Engines based on Spiking Neural Networks",
    "abstract": "           Foundational models based on the transformer architecture are currently the state-of-the-art in general language modeling, as well as in scientific areas such as material science and climate. However, training and deploying these models is computationally challenging as the time and space complexity has a quadratic relation to the input sequence length. Several efforts exploring efficient computational paradigms and model architectures to address these limitations have been made. In this work, we explore spiking neural networks (SNNs) to design transformer models. A challenge in training large-scale SNNs, using existing surrogate learning methods is inefficient and time-consuming. On the other hand, techniques to convert existing transformer-based models to their SNN equivalent are not scalable, as achieving optimal performance comes at the cost of a large number of spike time-steps, i.e. increased latency. To address this, we propose NeurTransformer, a methodology for designing transformer-based SNN for inference using a supervised fine-tuning approach with existing conversion methods. The proposed methodology works by: (1) replacing the self-attention mechanism with a spike-based self-attention (SSA), (2) converting the feed-forward block of the trained transformer model to its equivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate learning algorithms. We benchmark the proposed methodology and demonstrate its accuracy and scalability using three variants of the GPT-2 model of increasing model size. We observe that the converted GPT-2 small models demonstrate a 5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we demonstrate the energy efficiency of the SSA block compared to the ASA block and show between 64.71% and 85.28% reductions in estimated energy consumption when implementing the self-attention mechanism on a digital hardware.         ",
    "url": "https://arxiv.org/abs/2510.00133",
    "authors": [
      "Adarsha Balaji",
      "Sandeep Madireddy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00137",
    "title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval",
    "abstract": "           Dual-encoder retrievers depend on the principle that relevant documents should score higher than irrelevant ones for a given query. Yet the dominant Noise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss, optimizes a softened ranking surrogate that we rigorously prove is fundamentally oblivious to score separation quality and unrelated to AUC. This mismatch leads to poor calibration and suboptimal performance in downstream tasks like retrieval-augmented generation (RAG). To address this fundamental limitation, we introduce the MW loss, a new training objective that maximizes the Mann-Whitney U statistic, which is mathematically equivalent to the Area under the ROC Curve (AUC). MW loss encourages each positive-negative pair to be correctly ranked by minimizing binary cross entropy over score differences. We provide theoretical guarantees that MW loss directly upper-bounds the AoC, better aligning optimization with retrieval goals. We further promote ROC curves and AUC as natural threshold free diagnostics for evaluating retriever calibration and ranking quality. Empirically, retrievers trained with MW loss consistently outperform contrastive counterparts in AUC and standard retrieval metrics. Our experiments show that MW loss is an empirically superior alternative to Contrastive Loss, yielding better-calibrated and more discriminative retrievers for high-stakes applications like RAG.         ",
    "url": "https://arxiv.org/abs/2510.00137",
    "authors": [
      "Nima Sheikholeslami",
      "Erfan Hosseini",
      "Patrice Bechard",
      "Srivatsava Daruru",
      "Sai Rajeswar"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2510.00148",
    "title": "Improved Hyperspectral Anomaly Detection via Unsupervised Subspace Modeling in the Signed Cumulative Distribution Transform Domain",
    "abstract": "           Hyperspectral anomaly detection (HAD), a crucial approach for many civilian and military applications, seeks to identify pixels with spectral signatures that are anomalous relative to a preponderance of background signatures. Significant effort has been made to improve HAD techniques, but challenges arise due to complex real-world environments and, by definition, limited prior knowledge of potential signatures of interest. This paper introduces a novel HAD method by proposing a transport-based mathematical model to describe the pixels comprising a given hyperspectral image. In this approach, hyperspectral pixels are viewed as observations of a template pattern undergoing unknown deformations that enables their representation in the signed cumulative distribution transform (SCDT) domain. An unsupervised subspace modeling technique is then used to construct a model of abundant background signals in this domain, whereupon anomalous signals are detected as deviations from the learned model. Comprehensive evaluations across five distinct datasets illustrate the superiority of our approach compared to state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2510.00148",
    "authors": [
      "Abu Hasnat Mohammad Rubaiyat",
      "Jordan Vincent",
      "Colin Olson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2510.00188",
    "title": "A Novel Robust Control Method Combining DNN-Based NMPC Approximation and PI Control: Application to Exoskeleton Squat Movements",
    "abstract": "           Nonlinear Model Predictive Control (NMPC) is a precise controller, but its heavy computational load often prevents application in robotic systems. Some studies have attempted to approximate NMPC using deep neural networks (NMPC-DNN). However, in the presence of unexpected disturbances or when operating conditions differ from training data, this approach lacks robustness, leading to large tracking errors. To address this issue, for the first time, the NMPC-DNN output is combined with a PI controller (Hybrid NMPC-DNN-PI). The proposed controller is validated by applying it to an exoskeleton robot during squat movement, which has a complex dynamic model and has received limited attention regarding robust nonlinear control design. A human-robot dynamic model with three active joints (ankle, knee, hip) is developed, and more than 5.3 million training samples are used to train the DNN. The results show that, under unseen conditions for the DNN, the tracking error in Hybrid NMPC-DNN-PI is significantly lower compared to NMPC-DNN. Moreover, human joint torques are greatly reduced with the use of the exoskeleton, with RMS values for the studied case reduced by 30.9%, 41.8%, and 29.7% at the ankle, knee, and hip, respectively. In addition, the computational cost of Hybrid NMPC-DNN-PI is 99.93% lower than that of NMPC.         ",
    "url": "https://arxiv.org/abs/2510.00188",
    "authors": [
      "Alireza Aliyari",
      "Gholamreza Vossoughi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.00192",
    "title": "PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning",
    "abstract": "           Low-rank adaptation (LoRA) has become a widely used paradigm for parameter-efficient fine-tuning of large language models, yet its representational capacity often lags behind full fine-tuning. Within the context of LoRA, a key open question is how to obtain expressive low-rank adapters from over-parameterized spaces. We propose \\textit{PrunedLoRA}, a new framework that leverages structured pruning to obtain highly representative low-rank adapters from an over-parameterized initialization. Unlike prior approaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes less important components during fine-tuning and prevents their reactivation, enabling flexible and adaptive rank allocation. For structured pruning, by minimizing the pruning error for overall loss, we provide fine-grained pruning and recovery updates in a gradient-based pruning strategy with grounded interpretation. We provide the first theoretical analysis of the robustness of structured pruning and provably show that under the impact of weight perturbation, gradient-based pruning is more robust than activation-based pruning with respect to overall loss. Empirically, PrunedLoRA consistently outperforms LoRA and its variants across supervised fine-tuning tasks in mathematical reasoning, code generation, and natural language understanding, and it also demonstrates advantages over existing structured pruning methods across diverse sparsity levels.         ",
    "url": "https://arxiv.org/abs/2510.00192",
    "authors": [
      "Xin Yu",
      "Cong Xie",
      "Ziyu Zhao",
      "Tiantian Fan",
      "Lingzhou Xue",
      "Zhi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00208",
    "title": "Robust Attitude Control of Nonlinear Multi-Rotor Dynamics with LFT Models and $\\mathcal{H}_\\infty$ Performance",
    "abstract": "           Attitude stabilization of unmanned aerial vehicles in uncertain environments presents significant challenges due to nonlinear dynamics, parameter variations, and sensor limitations. This paper presents a comparative study of $\\mathcal{H}_\\infty$ and classical PID controllers for multi-rotor attitude regulation in the presence of wind disturbances and gyroscope noise. The flight dynamics are modeled using a linear parameter-varying (LPV) framework, where nonlinearities and parameter variations are systematically represented as structured uncertainties within a linear fractional transformation formulation. A robust controller based on $\\mathcal{H}_\\infty$ formulation is designed using only gyroscope measurements to ensure guaranteed performance bounds. Nonlinear simulation results demonstrate the effectiveness of the robust controllers compared to classical PID control, showing significant improvement in attitude regulation under severe wind disturbances.         ",
    "url": "https://arxiv.org/abs/2510.00208",
    "authors": [
      "Tanay Kumar",
      "Raktim Bhattacharya"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2510.00233",
    "title": "Differentiable Autoencoding Neural Operator for Interpretable and Integrable Latent Space Modeling",
    "abstract": "           Scientific machine learning has enabled the extraction of physical insights from high-dimensional spatiotemporal flow data using linear and nonlinear dimensionality reduction techniques. Despite these advances, achieving interpretability within the latent space remains a challenge. To address this, we propose the DIfferentiable Autoencoding Neural Operator (DIANO), a deterministic autoencoding neural operator framework that constructs physically interpretable latent spaces for both dimensional and geometric reduction, with the provision to enforce differential governing equations directly within the latent space. Built upon neural operators, DIANO compresses high-dimensional input functions into a low-dimensional latent space via spatial coarsening through an encoding neural operator and subsequently reconstructs the original inputs using a decoding neural operator through spatial refinement. We assess DIANO's latent space interpretability and performance in dimensionality reduction against baseline models, including the Convolutional Neural Operator and standard autoencoders. Furthermore, a fully differentiable partial differential equation (PDE) solver is developed and integrated within the latent space, enabling the temporal advancement of both high- and low-fidelity PDEs, thereby embedding physical priors into the latent dynamics. We further investigate various PDE formulations, including the 2D unsteady advection-diffusion and the 3D Pressure-Poisson equation, to examine their influence on shaping the latent flow representations. Benchmark problems considered include flow past a 2D cylinder, flow through a 2D symmetric stenosed artery, and a 3D patient-specific coronary artery. These case studies demonstrate DIANO's capability to solve PDEs within a latent space that facilitates both dimensional and geometrical reduction while allowing latent interpretability.         ",
    "url": "https://arxiv.org/abs/2510.00233",
    "authors": [
      "Siva Viknesh",
      "Amirhossein Arzani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2510.00239",
    "title": "Cooperation in Bilateral Generalized Network Creation",
    "abstract": "           Studying the impact of cooperation in strategic settings is one of the cornerstones of algorithmic game theory. Intuitively, allowing more cooperation yields equilibria that are more beneficial for the society of agents. However, for many games it is still an open question how much cooperation is actually needed to ensure socially good equilibria. We contribute to this research endeavor by analyzing the benefits of cooperation in a network formation game that models the creation of communication networks via the interaction of selfish agents. In our game, agents that correspond to nodes of a network can buy incident edges of a given weighted host graph to increase their centrality in the formed network. The cost of an edge is proportional to its length, and both endpoints must agree and pay for an edge to be created. This setting is known for having a high price of anarchy. To uncover the impact of cooperation, we investigate the price of anarchy of our network formation game with respect to multiple solution concepts that allow for varying amounts of cooperation. On the negative side, we show that on host graphs with arbitrary edge weights even the strongest form of cooperation cannot improve the price of anarchy. In contrast to this, as our main result, we show that cooperation has a significant positive impact if the given host graph has metric edge weights. For this, we prove asymptotically tight bounds on the price of anarchy via a novel proof technique that might be of independent interest and can be applied in other models with metric weights.         ",
    "url": "https://arxiv.org/abs/2510.00239",
    "authors": [
      "Hans Gawendowicz",
      "Pascal Lenzner",
      "Lukas Weyand"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2510.00268",
    "title": "Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction",
    "abstract": "           Large Language Models (LLMs) have shown extraordinary success across various text generation tasks; however, their potential for simple yet essential text classification remains underexplored, as LLM pre-training tends to emphasize generation over classification. While LLMs with instruction tuning can transform classification into a generation task, they often struggle to categorize nuanced texts. One such example is text revision, which involves nuanced edits between pairs of texts. Although simply fine-tuning LLMs for revision classification seems plausible, it requires a large amount of revision annotations, which are exceptionally expensive and scarce in the community. To address this issue, we introduce a plug-and-play layer-wise parameter-efficient fine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of important LLM layers that are dynamically selected based on their gradient norm distribution, while freezing those of redundant layers. Extensive experiments suggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse text revisions, while achieving fast convergence, low GPU memory consumption, and effectiveness on small revision corpora.         ",
    "url": "https://arxiv.org/abs/2510.00268",
    "authors": [
      "Zhexiong Liu",
      "Diane Litman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00279",
    "title": "SLogic: Subgraph-Informed Logical Rule Learning for Knowledge Graph Completion",
    "abstract": "           Logical rule-based methods offer an interpretable approach to knowledge graph completion by capturing compositional relationships in the form of human-readable inference rules. However, current approaches typically treat logical rules as universal, assigning each rule a fixed confidence score that ignores query-specific context. This is a significant limitation, as a rule's importance can vary depending on the query. To address this, we introduce SLogic (Subgraph-Informed Logical Rule learning), a novel framework that assigns query-dependent scores to logical rules. The core of SLogic is a scoring function that utilizes the subgraph centered on a query's head entity, allowing the significance of each rule to be assessed dynamically. Extensive experiments on benchmark datasets show that by leveraging local subgraph context, SLogic consistently outperforms state-of-the-art baselines, including both embedding-based and rule-based methods.         ",
    "url": "https://arxiv.org/abs/2510.00279",
    "authors": [
      "Trung Hoang Le",
      "Tran Cao Son",
      "Huiping Cao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00296",
    "title": "Beyond Token Probes: Hallucination Detection via Activation Tensors with ACT-ViT",
    "abstract": "           Detecting hallucinations in Large Language Model-generated text is crucial for their safe deployment. While probing classifiers show promise, they operate on isolated layer-token pairs and are LLM-specific, limiting their effectiveness and hindering cross-LLM applications. In this paper, we introduce a novel approach to address these shortcomings. We build on the natural sequential structure of activation data in both axes (layers $\\times$ tokens) and advocate treating full activation tensors akin to images. We design ACT-ViT, a Vision Transformer-inspired model that can be effectively and efficiently applied to activation tensors and supports training on data from multiple LLMs simultaneously. Through comprehensive experiments encompassing diverse LLMs and datasets, we demonstrate that ACT-ViT consistently outperforms traditional probing techniques while remaining extremely efficient for deployment. In particular, we show that our architecture benefits substantially from multi-LLM training, achieves strong zero-shot performance on unseen datasets, and can be transferred effectively to new LLMs through fine-tuning. Full code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00296",
    "authors": [
      "Guy Bar-Shalom",
      "Fabrizio Frasca",
      "Yaniv Galron",
      "Yftah Ziser",
      "Haggai Maron"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00303",
    "title": "Looking Beyond the Known: Towards a Data Discovery Guided Open-World Object Detection",
    "abstract": "           Open-World Object Detection (OWOD) enriches traditional object detectors by enabling continual discovery and integration of unknown objects via human guidance. However, existing OWOD approaches frequently suffer from semantic confusion between known and unknown classes, alongside catastrophic forgetting, leading to diminished unknown recall and degraded known-class accuracy. To overcome these challenges, we propose Combinatorial Open-World Detection (CROWD), a unified framework reformulating unknown object discovery and adaptation as an interwoven combinatorial (set-based) data-discovery (CROWD-Discover) and representation learning (CROWD-Learn) task. CROWD-Discover strategically mines unknown instances by maximizing Submodular Conditional Gain (SCG) functions, selecting representative examples distinctly dissimilar from known objects. Subsequently, CROWD-Learn employs novel combinatorial objectives that jointly disentangle known and unknown representations while maintaining discriminative coherence among known classes, thus mitigating confusion and forgetting. Extensive evaluations on OWOD benchmarks illustrate that CROWD achieves improvements of 2.83% and 2.05% in known-class accuracy on M-OWODB and S-OWODB, respectively, and nearly 2.4x unknown recall compared to leading baselines.         ",
    "url": "https://arxiv.org/abs/2510.00303",
    "authors": [
      "Anay Majee",
      "Amitesh Gangrade",
      "Rishabh Iyer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00310",
    "title": "Robust Federated Inference",
    "abstract": "           Federated inference, in the form of one-shot federated learning, edge ensembles, or federated ensembles, has emerged as an attractive solution to combine predictions from multiple models. This paradigm enables each model to remain local and proprietary while a central server queries them and aggregates predictions. Yet, the robustness of federated inference has been largely neglected, leaving them vulnerable to even simple attacks. To address this critical gap, we formalize the problem of robust federated inference and provide the first robustness analysis of this class of methods. Our analysis of averaging-based aggregators shows that the error of the aggregator is small either when the dissimilarity between honest responses is small or the margin between the two most probable classes is large. Moving beyond linear averaging, we show that problem of robust federated inference with non-linear aggregators can be cast as an adversarial machine learning problem. We then introduce an advanced technique using the DeepSet aggregation model, proposing a novel composition of adversarial training and test-time robust aggregation to robustify non-linear aggregators. Our composition yields significant improvements, surpassing existing robust aggregation methods by 4.7 - 22.2% in accuracy points across diverse benchmarks.         ",
    "url": "https://arxiv.org/abs/2510.00310",
    "authors": [
      "Akash Dhasade",
      "Sadegh Farhadkhani",
      "Rachid Guerraoui",
      "Nirupam Gupta",
      "Maxime Jacovella",
      "Anne-Marie Kermarrec",
      "Rafael Pinot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2510.00317",
    "title": "MAVUL: Multi-Agent Vulnerability Detection via Contextual Reasoning and Interactive Refinement",
    "abstract": "           The widespread adoption of open-source software (OSS) necessitates the mitigation of vulnerability risks. Most vulnerability detection (VD) methods are limited by inadequate contextual understanding, restrictive single-round interactions, and coarse-grained evaluations, resulting in undesired model performance and biased evaluation results. To address these challenges, we propose MAVUL, a novel multi-agent VD system that integrates contextual reasoning and interactive refinement. Specifically, a vulnerability analyst agent is designed to flexibly leverage tool-using capabilities and contextual reasoning to achieve cross-procedural code understanding and effectively mine vulnerability patterns. Through iterative feedback and refined decision-making within cross-role agent interactions, the system achieves reliable reasoning and vulnerability prediction. Furthermore, MAVUL introduces multi-dimensional ground truth information for fine-grained evaluation, thereby enhancing evaluation accuracy and reliability. Extensive experiments conducted on a pairwise vulnerability dataset demonstrate MAVUL's superior performance. Our findings indicate that MAVUL significantly outperforms existing multi-agent systems with over 62% higher pairwise accuracy and single-agent systems with over 600% higher average performance. The system's effectiveness is markedly improved with increased communication rounds between the vulnerability analyst agent and the security architect agent, underscoring the importance of contextual reasoning in tracing vulnerability flows and the crucial feedback role. Additionally, the integrated evaluation agent serves as a critical, unbiased judge, ensuring a more accurate and reliable estimation of the system's real-world applicability by preventing misleading binary comparisons.         ",
    "url": "https://arxiv.org/abs/2510.00317",
    "authors": [
      "Youpeng Li",
      "Kartik Joshi",
      "Xinda Wang",
      "Eric Wong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2510.00324",
    "title": "Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?",
    "abstract": "           Code search is an important information retrieval application. Benefits of better code search include faster new developer on-boarding, reduced software maintenance, and ease of understanding for large repositories. Despite improvements in search algorithms and search benchmarks, the domain of code search has lagged behind. One reason is the high cost of human annotation for code queries and answers. While humans may annotate search results in general text QA systems, code annotations require specialized knowledge of a programming language (PL), as well as domain specific software engineering knowledge. In this work we study the use of Large Language Models (LLMs) to retrieve code at the level of functions and to generate annotations for code search results. We compare the impact of the retriever representation (sparse vs. semantic), programming language, and LLM by comparing human annotations across several popular languages (C, Java, Javascript, Go, and Python). We focus on repositories that implement common data structures likely to be implemented in any PLs. For the same human annotations, we compare several LLM-as-a-Judge models to evaluate programming language and other affinities between LLMs. We find that the chosen retriever and PL exhibit affinities that can be leveraged to improve alignment of human and AI relevance determinations, with significant performance implications. We also find differences in representation (sparse vs. semantic) across PLs that impact alignment of human and AI relevance determinations. We propose using transpilers to bootstrap scalable code search benchmark datasets in other PLs and in a case study demonstrate that human-AI relevance agreement rates largely match the (worst case) human-human agreement under study. The application code used in this work is available at \\href{this https URL}{this github repo}.         ",
    "url": "https://arxiv.org/abs/2510.00324",
    "authors": [
      "Lucas Roberts",
      "Denisa Roberts"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00332",
    "title": "When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets",
    "abstract": "           We present CAIA, a benchmark exposing a critical blind spot in AI evaluation: the inability of state-of-the-art models to operate in adversarial, high-stakes environments where misinformation is weaponized and errors are irreversible. While existing benchmarks measure task completion in controlled settings, real-world deployment demands resilience against active deception. Using crypto markets as a testbed where $30 billion was lost to exploits in 2024, we evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish truth from manipulation, navigate fragmented information landscapes, and make irreversible financial decisions under adversarial pressure. Our results reveal a fundamental capability gap: without tools, even frontier models achieve only 28% accuracy on tasks junior analysts routinely handle. Tool augmentation improves performance but plateaus at 67.4% versus 80% human baseline, despite unlimited access to professional resources. Most critically, we uncover a systematic tool selection catastrophe: models preferentially choose unreliable web search over authoritative data, falling for SEO-optimized misinformation and social media manipulation. This behavior persists even when correct answers are directly accessible through specialized tools, suggesting foundational limitations rather than knowledge gaps. We also find that Pass@k metrics mask dangerous trial-and-error behavior for autonomous deployment. The implications extend beyond crypto to any domain with active adversaries, e.g. cybersecurity, content moderation, etc. We release CAIA with contamination controls and continuous updates, establishing adversarial robustness as a necessary condition for trustworthy AI autonomy. The benchmark reveals that current models, despite impressive reasoning scores, remain fundamentally unprepared for environments where intelligence must survive active opposition.         ",
    "url": "https://arxiv.org/abs/2510.00332",
    "authors": [
      "Zeshi Dai",
      "Zimo Peng",
      "Zerui Cheng",
      "Ryan Yihe Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2510.00350",
    "title": "Security and Privacy Analysis of Tile's Location Tracking Protocol",
    "abstract": "           We conduct the first comprehensive security analysis of Tile, the second most popular crowd-sourced location-tracking service behind Apple's AirTags. We identify several exploitable vulnerabilities and design flaws, disproving many of the platform's claimed security and privacy guarantees: Tile's servers can persistently learn the location of all users and tags, unprivileged adversaries can track users through Bluetooth advertisements emitted by Tile's devices, and Tile's anti-theft mode is easily subverted. Despite its wide deployment -- millions of users, devices, and purpose-built hardware tags -- Tile provides no formal description of its protocol or threat model. Worse, Tile intentionally weakens its antistalking features to support an antitheft use-case and relies on a novel \"accountability\" mechanism to punish those abusing the system to stalk victims. We examine Tile's accountability mechanism, a unique feature of independent interest; no other provider attempts to guarantee accountability. While an ideal accountability mechanism may disincentivize abuse in crowd-sourced location tracking protocols, we show that Tile's implementation is subvertible and introduces new exploitable vulnerabilities. We conclude with a discussion on the need for new, formal definitions of accountability in this setting.         ",
    "url": "https://arxiv.org/abs/2510.00350",
    "authors": [
      "Akshaya Kumar",
      "Anna Raymaker",
      "Michael Specter"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2510.00358",
    "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts",
    "abstract": "           Soft snake robots offer remarkable flexibility and adaptability in complex environments, yet their control remains challenging due to highly nonlinear dynamics. Existing model-based and bio-inspired controllers rely on simplified assumptions that limit performance. Deep reinforcement learning (DRL) has recently emerged as a promising alternative, but online training is often impractical because of costly and potentially damaging real-world interactions. Offline RL provides a safer option by leveraging pre-collected datasets, but it suffers from distribution shift, which degrades generalization to unseen scenarios. To overcome this challenge, we propose DiSA-IQL (Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that incorporates robustness modulation by penalizing unreliable state-action pairs to mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks across two settings: in-distribution and out-of-distribution evaluation. Simulation results show that DiSA-IQL consistently outperforms baseline models, including Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla IQL, achieving higher success rates, smoother trajectories, and improved robustness. The codes are open-sourced to support reproducibility and to facilitate further research in offline RL for soft robot control.         ",
    "url": "https://arxiv.org/abs/2510.00358",
    "authors": [
      "Linjin He",
      "Xinda Qi",
      "Dong Chen",
      "Zhaojian Li",
      "Xiaobo Tan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00374",
    "title": "GDLNN: Marriage of Programming Language and Neural Networks for Accurate and Easy-to-Explain Graph Classification",
    "abstract": "           We present GDLNN, a new graph machine learning architecture, for graph classification tasks. GDLNN combines a domain-specific programming language, called GDL, with neural networks. The main strength of GDLNN lies in its GDL layer, which generates expressive and interpretable graph representations. Since the graph representation is interpretable, existing model explanation techniques can be directly applied to explain GDLNN's predictions. Our evaluation shows that the GDL-based representation achieves high accuracy on most graph classification benchmark datasets, outperforming dominant graph learning methods such as GNNs. Applying an existing model explanation technique also yields high-quality explanations of GDLNN's predictions. Furthermore, the cost of GDLNN is low when the explanation cost is included.         ",
    "url": "https://arxiv.org/abs/2510.00374",
    "authors": [
      "Minseok Jeon",
      "Seunghyun Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2510.00376",
    "title": "Discrete Wavelet Transform as a Facilitator for Expressive Latent Space Representation in Variational Autoencoders in Satellite Imagery",
    "abstract": "           Latent Diffusion Models (LDM), a subclass of diffusion models, mitigate the computational complexity of pixel-space diffusion by operating within a compressed latent space constructed by Variational Autoencoders (VAEs), demonstrating significant advantages in Remote Sensing (RS) applications. Though numerous studies enhancing LDMs have been conducted, investigations explicitly targeting improvements within the intrinsic latent space remain scarce. This paper proposes an innovative perspective, utilizing the Discrete Wavelet Transform (DWT) to enhance the VAE's latent space representation, designed for satellite imagery. The proposed method, ExpDWT-VAE, introduces dual branches: one processes spatial domain input through convolutional operations, while the other extracts and processes frequency-domain features via 2D Haar wavelet decomposition, convolutional operation, and inverse DWT reconstruction. These branches merge to create an integrated spatial-frequency representation, further refined through convolutional and diagonal Gaussian mapping into a robust latent representation. We utilize a new satellite imagery dataset housed by the TerraFly mapping system to validate our method. Experimental results across several performance metrics highlight the efficacy of the proposed method at enhancing latent space representation.         ",
    "url": "https://arxiv.org/abs/2510.00376",
    "authors": [
      "Arpan Mahara",
      "Md Rezaul Karim Khan",
      "Naphtali Rishe",
      "Wenjia Wang",
      "Seyed Masoud Sadjadi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00379",
    "title": "Composer: A Search Framework for Hybrid Neural Architecture Design",
    "abstract": "           Hybrid model architectures that combine computational primitives (e.g., Attention, MLP) in different ratios have shown promising performance beyond Transformers. Some studies have shown that different interleavings of primitives can affect model quality as well. However, prior works explore the hybrid model architecture design space manually. Due to the large design space and training costs, discovering hybrid models that combine key computational primitives for pre-training is challenging. In this work, we take a principled approach in designing a modular hybrid model architecture search framework -- Composer. Composer explores model architectures at a small scale and extrapolates the top-performing model architectures to a larger scale using our proposed scaling strategies. Using Composer, we discover new hybrid LLM architectures that outperform Llama 3.2. Compared to Llama 3.2 and previous state-of-the-art baselines, the new model architectures consistently reduce validation loss at parameter scales of 350M-3B and improve evaluation accuracy on the downstream tasks by up to 2.8-8.3% (1.1-3.1% on average) while improving both training and inference efficiency.         ",
    "url": "https://arxiv.org/abs/2510.00379",
    "authors": [
      "Bilge Acun",
      "Prasoon Sinha",
      "Newsha Ardalani",
      "Sangmin Bae",
      "Alicia Golden",
      "Chien-Yu Lin",
      "Meghana Madhyastha",
      "Fei Sun",
      "Neeraja J. Yadwadkar",
      "Carole-Jean Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00382",
    "title": "Efficient Probabilistic Tensor Networks",
    "abstract": "           Tensor networks (TNs) enable compact representations of large tensors through shared parameters. Their use in probabilistic modeling is particularly appealing, as probabilistic tensor networks (PTNs) allow for tractable computation of marginals. However, existing approaches for learning parameters of PTNs are either computationally demanding and not fully compatible with automatic differentiation frameworks, or numerically unstable. In this work, we propose a conceptually simple approach for learning PTNs efficiently, that is numerically stable. We show our method provides significant improvements in time and space complexity, achieving 10x reduction in latency for generative modeling on the MNIST dataset. Furthermore, our approach enables learning of distributions with 10x more variables than previous approaches when applied to a variety of density estimation benchmarks. Our code is publicly available at this http URL.         ",
    "url": "https://arxiv.org/abs/2510.00382",
    "authors": [
      "Marawan Gamal Abdel Hameed",
      "Guillaume Rabusseau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00394",
    "title": "Graph2Region: Efficient Graph Similarity Learning with Structure and Scale Restoration",
    "abstract": "           Graph similarity is critical in graph-related tasks such as graph retrieval, where metrics like maximum common subgraph (MCS) and graph edit distance (GED) are commonly used. However, exact computations of these metrics are known to be NP-Hard. Recent neural network-based approaches approximate the similarity score in embedding spaces to alleviate the computational burden, but they either involve expensive pairwise node comparisons or fail to effectively utilize structural and scale information of graphs. To tackle these issues, we propose a novel geometric-based graph embedding method called Graph2Region (G2R). G2R represents nodes as closed regions and recovers their adjacency patterns within graphs in the embedding space. By incorporating the node features and adjacency patterns of graphs, G2R summarizes graph regions, i.e., graph embeddings, where the shape captures the underlying graph structures and the volume reflects the graph size. Consequently, the overlap between graph regions can serve as an approximation of MCS, signifying similar node regions and adjacency patterns. We further analyze the relationship between MCS and GED and propose using disjoint parts as a proxy for GED similarity. This analysis enables concurrent computation of MCS and GED, incorporating local and global structural information. Experimental evaluation highlights G2R's competitive performance in graph similarity computation. It achieves up to a 60.0\\% relative accuracy improvement over state-of-the-art methods in MCS similarity learning, while maintaining efficiency in both training and inference. Moreover, G2R showcases remarkable capability in predicting both MCS and GED similarities simultaneously, providing a holistic assessment of graph similarity. Code available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00394",
    "authors": [
      "Zhouyang Liu",
      "Yixin Chen",
      "Ning Liu",
      "Jiezhong He",
      "Dongsheng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2510.00401",
    "title": "Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting",
    "abstract": "           Long-horizon motion forecasting for multiple autonomous robots is challenging due to non-linear agent interactions, compounding prediction errors, and continuous-time evolution of dynamics. Learned dynamics of such a system can be useful in various applications such as travel time prediction, prediction-guided planning and generative simulation. In this work, we aim to develop an efficient trajectory forecasting model conditioned on multi-agent goals. Motivated by the recent success of physics-guided deep learning for partially known dynamical systems, we develop a model based on neural Controlled Differential Equations (CDEs) for long-horizon motion forecasting. Unlike discrete-time methods such as RNNs and transformers, neural CDEs operate in continuous time, allowing us to combine physics-informed constraints and biases to jointly model multi-robot dynamics. Our approach, named PINCoDE (Physics-Informed Neural Controlled Differential Equations), learns differential equation parameters that can be used to predict the trajectories of a multi-agent system starting from an initial condition. PINCoDE is conditioned on future goals and enforces physics constraints for robot motion over extended periods of time. We adopt a strategy that scales our model from 10 robots to 100 robots without the need for additional model parameters, while producing predictions with an average ADE below 0.5 m for a 1-minute horizon. Furthermore, progressive training with curriculum learning for our PINCoDE model results in a 2.7X reduction of forecasted pose error over 4 minute horizons compared to analytical models.         ",
    "url": "https://arxiv.org/abs/2510.00401",
    "authors": [
      "Shounak Sural",
      "Charles Kekeh",
      "Wenliang Liu",
      "Federico Pecora",
      "Mouhacine Benosman"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2510.00402",
    "title": "Hierarchy-Aware Neural Subgraph Matching with Enhanced Similarity Measure",
    "abstract": "           Subgraph matching is challenging as it necessitates time-consuming combinatorial searches. Recent Graph Neural Network (GNN)-based approaches address this issue by employing GNN encoders to extract graph information and hinge distance measures to ensure containment constraints in the embedding space. These methods significantly shorten the response time, making them promising solutions for subgraph retrieval. However, they suffer from scale differences between graph pairs during encoding, as they focus on feature counts but overlook the relative positions of features within node-rooted subtrees, leading to disturbed containment constraints and false predictions. Additionally, their hinge distance measures lack discriminative power for matched graph pairs, hindering ranking applications. We propose NC-Iso, a novel GNN architecture for neural subgraph matching. NC-Iso preserves the relative positions of features by building the hierarchical dependencies between adjacent echelons within node-rooted subtrees, ensuring matched graph pairs maintain consistent hierarchies while complying with containment constraints in feature counts. To enhance the ranking ability for matched pairs, we introduce a novel similarity dominance ratio-enhanced measure, which quantifies the dominance of similarity over dissimilarity between graph pairs. Empirical results on nine datasets validate the effectiveness, generalization ability, scalability, and transferability of NC-Iso while maintaining time efficiency, offering a more discriminative neural subgraph matching solution for subgraph retrieval. Code available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00402",
    "authors": [
      "Zhouyang Liu",
      "Ning Liu",
      "Yixin Chen",
      "Jiezhong He",
      "Menghan Jia",
      "Dongsheng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00405",
    "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations",
    "abstract": "           Reliable trajectory prediction from an ego-centric perspective is crucial for robotic navigation in human-centric environments. However, existing methods typically assume idealized observation histories, failing to account for the perceptual artifacts inherent in first-person vision, such as occlusions, ID switches, and tracking drift. This discrepancy between training assumptions and deployment reality severely limits model robustness. To bridge this gap, we introduce EgoTraj-Bench, the first real-world benchmark that grounds noisy, first-person visual histories in clean, bird's-eye-view future trajectories, enabling robust learning under realistic perceptual constraints. Building on this benchmark, we propose BiFlow, a dual-stream flow matching model that concurrently denoises historical observations and forecasts future motion by leveraging a shared latent representation. To better model agent intent, BiFlow incorporates our EgoAnchor mechanism, which conditions the prediction decoder on distilled historical features via feature modulation. Extensive experiments show that BiFlow achieves state-of-the-art performance, reducing minADE and minFDE by 10-15% on average and demonstrating superior robustness. We anticipate that our benchmark and model will provide a critical foundation for developing trajectory forecasting systems truly resilient to the challenges of real-world, ego-centric perception.         ",
    "url": "https://arxiv.org/abs/2510.00405",
    "authors": [
      "Jiayi Liu",
      "Jiaming Zhou",
      "Ke Ye",
      "Kun-Yu Lin",
      "Allan Wang",
      "Junwei Liang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2510.00411",
    "title": "David and Goliath in Medical Vision: Convolutional Networks vs Biomedical Vision Language Models",
    "abstract": "           The accurate interpretation of chest radiographs using automated methods is a critical task in medical imaging. This paper presents a comparative analysis between a supervised lightweight Convolutional Neural Network (CNN) and a state-of-the-art, zero-shot medical Vision-Language Model (VLM), BiomedCLIP, across two distinct diagnostic tasks: pneumonia detection on the PneumoniaMNIST benchmark and tuberculosis detection on the Shenzhen TB dataset. Our experiments show that supervised CNNs serve as highly competitive baselines in both cases. While the default zero-shot performance of the VLM is lower, we demonstrate that its potential can be unlocked via a simple yet crucial remedy: decision threshold calibration. By optimizing the classification threshold on a validation set, the performance of BiomedCLIP is significantly boosted across both datasets. For pneumonia detection, calibration enables the zero-shot VLM to achieve a superior F1-score of 0.8841, surpassing the supervised CNN's 0.8803. For tuberculosis detection, calibration dramatically improves the F1-score from 0.4812 to 0.7684, bringing it close to the supervised baseline's 0.7834. This work highlights a key insight: proper calibration is essential for leveraging the full diagnostic power of zero-shot VLMs, enabling them to match or even outperform efficient, task-specific supervised models.         ",
    "url": "https://arxiv.org/abs/2510.00411",
    "authors": [
      "Ran Tong",
      "Jiaqi Liu",
      "Su Liu",
      "Jiexi Xu",
      "Lanruo Wang",
      "Tong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00425",
    "title": "Conflict-Based Search as a Protocol: A Multi-Agent Motion Planning Protocol for Heterogeneous Agents, Solvers, and Independent Tasks",
    "abstract": "           Imagine the future construction site, hospital, office, or even sophisticated household with dozens of robots bought from different manufacturers. How can we enable these different systems to effectively move in a shared environment, given that each robot may have its own independent motion planning system? This work shows how we can get efficient collision-free movements between algorithmically heterogeneous agents by using Conflict-Based Search (Sharon et al. 2015) as a protocol. At its core, the CBS Protocol requires one specific single-agent motion planning API; finding a collision-free path that satisfies certain space-time constraints. Given such an API, CBS uses a central planner to find collision-free paths - independent of how the API is implemented. We show how this protocol enables multi-agent motion planning for a heterogeneous team of agents completing independent tasks with a variety of single-agent planners including: Heuristic Search (e.g., A*), Sampling Based Search (e.g., RRT), Optimization (e.g., Direct Collocation), Diffusion, and Reinforcement Learning.         ",
    "url": "https://arxiv.org/abs/2510.00425",
    "authors": [
      "Rishi Veerapaneni",
      "Alvin Tang",
      "Haodong He",
      "Sophia Zhao",
      "Viraj Shah",
      "Yidai Cen",
      "Ziteng Ji",
      "Gabriel Olin",
      "Jon Arrizabalaga",
      "Yorai Shaoul",
      "Jiaoyang Li",
      "Maxim Likhachev"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2510.00434",
    "title": "On-the-Fly Data Augmentation via Gradient-Guided and Sample-Aware Influence Estimation",
    "abstract": "           Data augmentation has been widely employed to improve the generalization of deep neural networks. Most existing methods apply fixed or random transformations. However, we find that sample difficulty evolves along with the model's generalization capabilities in dynamic training environments. As a result, applying uniform or stochastic augmentations, without accounting for such dynamics, can lead to a mismatch between augmented data and the model's evolving training needs, ultimately degrading training effectiveness. To address this, we introduce SADA, a Sample-Aware Dynamic Augmentation that performs on-the-fly adjustment of augmentation strengths based on each sample's evolving influence on model optimization. Specifically, we estimate each sample's influence by projecting its gradient onto the accumulated model update direction and computing the temporal variance within a local training window. Samples with low variance, indicating stable and consistent influence, are augmented more strongly to emphasize diversity, while unstable samples receive milder transformations to preserve semantic fidelity and stabilize learning. Our method is lightweight, which does not require auxiliary models or policy tuning. It can be seamlessly integrated into existing training pipelines as a plug-and-play module. Experiments across various benchmark datasets and model architectures show consistent improvements of SADA, including +7.3\\% on fine-grained tasks and +4.3\\% on long-tailed datasets, highlighting the method's effectiveness and practicality.         ",
    "url": "https://arxiv.org/abs/2510.00434",
    "authors": [
      "Suorong Yang",
      "Jie Zong",
      "Lihang Wang",
      "Ziheng Qin",
      "Hai Gan",
      "Pengfei Zhou",
      "Kai Wang",
      "Yang You",
      "Furao Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00441",
    "title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation",
    "abstract": "           Visual navigation is a fundamental problem in embodied AI, yet practical deployments demand long-horizon planning capabilities to address multi-objective tasks. A major bottleneck is data scarcity: policies learned from limited data often overfit and fail to generalize OOD. Existing neural network-based agents typically increase architectural complexity that paradoxically become counterproductive in the small-sample regime. This paper introduce NeuRO, a integrated learning-to-optimize framework that tightly couples perception networks with downstream task-level robust optimization. Specifically, NeuRO addresses core difficulties in this integration: (i) it transforms noisy visual predictions under data scarcity into convex uncertainty sets using Partially Input Convex Neural Networks (PICNNs) with conformal calibration, which directly parameterize the optimization constraints; and (ii) it reformulates planning under partial observability as a robust optimization problem, enabling uncertainty-aware policies that transfer across environments. Extensive experiments on both unordered and sequential multi-object navigation tasks demonstrate that NeuRO establishes SoTA performance, particularly in generalization to unseen environments. Our work thus presents a significant advancement for developing robust, generalizable autonomous agents.         ",
    "url": "https://arxiv.org/abs/2510.00441",
    "authors": [
      "Yiyuan Pan",
      "Yunzhe Xu",
      "Zhe Liu",
      "Hesheng Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2510.00442",
    "title": "Randomized Matrix Sketching for Neural Network Training and Gradient Monitoring",
    "abstract": "           Neural network training relies on gradient computation through backpropagation, yet memory requirements for storing layer activations present significant scalability challenges. We present the first adaptation of control-theoretic matrix sketching to neural network layer activations, enabling memory-efficient gradient reconstruction in backpropagation. This work builds on recent matrix sketching frameworks for dynamic optimization problems, where similar state trajectory storage challenges motivate sketching techniques. Our approach sketches layer activations using three complementary sketch matrices maintained through exponential moving averages (EMA) with adaptive rank adjustment, automatically balancing memory efficiency against approximation quality. Empirical evaluation on MNIST, CIFAR-10, and physics-informed neural networks demonstrates a controllable accuracy-memory tradeoff. We demonstrate a gradient monitoring application on MNIST showing how sketched activations enable real-time gradient norm tracking with minimal memory overhead. These results establish that sketched activation storage provides a viable path toward memory-efficient neural network training and analysis.         ",
    "url": "https://arxiv.org/abs/2510.00442",
    "authors": [
      "Harbir Antil",
      "Deepanshu Verma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00446",
    "title": "LongCodeZip: Compress Long Context for Code Language Models",
    "abstract": "           Code generation under long contexts is becoming increasingly critical as Large Language Models (LLMs) are required to reason over extensive information in the codebase. While recent advances enable code LLMs to process long inputs, high API costs and generation latency remain substantial bottlenecks. Existing context pruning techniques, such as LLMLingua, achieve promising results for general text but overlook code-specific structures and dependencies, leading to suboptimal performance in programming tasks. In this paper, we propose LongCodeZip, a novel plug-and-play code compression framework designed specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1) coarse-grained compression, which identifies and ranks function-level chunks using conditional perplexity with respect to the instruction, retaining only the most relevant functions; and (2) fine-grained compression, which segments retained functions into blocks based on perplexity and selects an optimal subset under an adaptive token budget to maximize relevance. Evaluations across multiple tasks, including code completion, summarization, and question answering, show that LongCodeZip consistently outperforms baseline methods, achieving up to a 5.6x compression ratio without degrading task performance. By effectively reducing context size while preserving essential information, LongCodeZip enables LLMs to better scale to real-world, large-scale code scenarios, advancing the efficiency and capability of code intelligence applications.         ",
    "url": "https://arxiv.org/abs/2510.00446",
    "authors": [
      "Yuling Shi",
      "Yichun Qian",
      "Hongyu Zhang",
      "Beijun Shen",
      "Xiaodong Gu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2510.00449",
    "title": "Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews",
    "abstract": "           Personalizing the outputs of large language models (LLMs) to align with individual user preferences is an active research area. However, previous studies have mainly focused on classification or ranking tasks and have not considered Likert-scale rating prediction, a regression task that requires both language and mathematical reasoning to be solved effectively. This task has significant industrial applications, but the utilization of LLMs remains underexplored, particularly regarding the capabilities of off-the-shelf LLMs. This study investigates the performance of off-the-shelf LLMs on rating prediction, providing different in-context information. Through comprehensive experiments with eight models across three datasets, we demonstrate that user-written reviews significantly improve the rating prediction performance of LLMs. This result is comparable to traditional methods like matrix factorization, highlighting the potential of LLMs as a promising solution for the cold-start problem. We also find that the reviews for concrete items are more effective than general preference descriptions that are not based on any specific item. Furthermore, we discover that prompting LLMs to first generate a hypothetical review enhances the rating prediction performance. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00449",
    "authors": [
      "Koki Ryu",
      "Hitomi Yanaka"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2510.00454",
    "title": "Measuring and Controlling the Spectral Bias for Self-Supervised Image Denoising",
    "abstract": "           Current self-supervised denoising methods for paired noisy images typically involve mapping one noisy image through the network to the other noisy image. However, after measuring the spectral bias of such methods using our proposed Image Pair Frequency-Band Similarity, it suffers from two practical limitations. Firstly, the high-frequency structural details in images are not preserved well enough. Secondly, during the process of fitting high frequencies, the network learns high-frequency noise from the mapped noisy images. To address these challenges, we introduce a Spectral Controlling network (SCNet) to optimize self-supervised denoising of paired noisy images. First, we propose a selection strategy to choose frequency band components for noisy images, to accelerate the convergence speed of training. Next, we present a parameter optimization method that restricts the learning ability of convolutional kernels to high-frequency noise using the Lipschitz constant, without changing the network structure. Finally, we introduce the Spectral Separation and low-rank Reconstruction module (SSR module), which separates noise and high-frequency details through frequency domain separation and low-rank space reconstruction, to retain the high-frequency structural details of images. Experiments performed on synthetic and real-world datasets verify the effectiveness of SCNet.         ",
    "url": "https://arxiv.org/abs/2510.00454",
    "authors": [
      "Wang Zhang",
      "Huaqiu Li",
      "Xiaowan Hu",
      "Tao Jiang",
      "Zikang Chen",
      "Haoqian Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00457",
    "title": "UrbanGraph: Physics-Informed Spatio-Temporal Dynamic Heterogeneous Graphs for Urban Microclimate Prediction",
    "abstract": "           With rapid urbanization, predicting urban microclimates has become critical, as it affects building energy demand and public health risks. However, existing generative and homogeneous graph approaches fall short in capturing physical consistency, spatial dependencies, and temporal variability. To address this, we introduce UrbanGraph, a physics-informed framework integrating heterogeneous and dynamic spatio-temporal graphs. It encodes key physical processes -- vegetation evapotranspiration, shading, and convective diffusion -- while modeling complex spatial dependencies among diverse urban entities and their temporal evolution. We evaluate UrbanGraph on UMC4/12, a physics-based simulation dataset covering diverse urban configurations and climates. Results show that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0% over all baselines, with heterogeneous and dynamic graphs contributing 3.5% and 7.1% gains. Our dataset provides the first high-resolution benchmark for spatio-temporal microclimate modeling, and our method extends to broader urban heterogeneous dynamic computing tasks.         ",
    "url": "https://arxiv.org/abs/2510.00457",
    "authors": [
      "Weilin Xin",
      "Chenyu Huang",
      "Peilin Li",
      "Jing Zhong",
      "Jiawei Yao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2510.00460",
    "title": "Robust Spatiotemporally Contiguous Anomaly Detection Using Tensor Decomposition",
    "abstract": "           Anomaly detection in spatiotemporal data is a challenging problem encountered in a variety of applications, including video surveillance, medical imaging data, and urban traffic monitoring. Existing anomaly detection methods focus mainly on point anomalies and cannot deal with temporal and spatial dependencies that arise in spatio-temporal data. Tensor-based anomaly detection methods have been proposed to address this problem. Although existing methods can capture dependencies across different modes, they are primarily supervised and do not account for the specific structure of anomalies. Moreover, these methods focus mainly on extracting anomalous features without providing any statistical confidence. In this paper, we introduce an unsupervised tensor-based anomaly detection method that simultaneously considers the sparse and spatiotemporally smooth nature of anomalies. The anomaly detection problem is formulated as a regularized robust low-rank + sparse tensor decomposition where the total variation of the tensor with respect to the underlying spatial and temporal graphs quantifies the spatiotemporal smoothness of the anomalies. Once the anomalous features are extracted, we introduce a statistical anomaly scoring framework that accounts for local spatio-temporal dependencies. The proposed framework is evaluated on both synthetic and real data.         ",
    "url": "https://arxiv.org/abs/2510.00460",
    "authors": [
      "Rachita Mondal",
      "Mert Indibi",
      "Tapabrata Maiti",
      "Selin Aviyente"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2510.00466",
    "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation",
    "abstract": "           Offline reinforcement learning (RL) has emerged as a promising framework for addressing robot social navigation challenges. However, inherent uncertainties in pedestrian behavior and limited environmental interaction during training often lead to suboptimal exploration and distributional shifts between offline training and online deployment. To overcome these limitations, this paper proposes a novel offline-to-online fine-tuning RL algorithm for robot social navigation by integrating Return-to-Go (RTG) prediction into a causal Transformer architecture. Our algorithm features a spatiotem-poral fusion model designed to precisely estimate RTG values in real-time by jointly encoding temporal pedestrian motion patterns and spatial crowd dynamics. This RTG prediction framework mitigates distribution shift by aligning offline policy training with online environmental interactions. Furthermore, a hybrid offline-online experience sampling mechanism is built to stabilize policy updates during fine-tuning, ensuring balanced integration of pre-trained knowledge and real-time adaptation. Extensive experiments in simulated social navigation environments demonstrate that our method achieves a higher success rate and lower collision rate compared to state-of-the-art baselines. These results underscore the efficacy of our algorithm in enhancing navigation policy robustness and adaptability. This work paves the way for more reliable and adaptive robotic navigation systems in real-world applications.         ",
    "url": "https://arxiv.org/abs/2510.00466",
    "authors": [
      "Run Su",
      "Hao Fu",
      "Shuai Zhou",
      "Yingao Fu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00476",
    "title": "Analyzing Latent Concepts in Code Language Models",
    "abstract": "           Interpreting the internal behavior of large language models trained on code remains a critical challenge, particularly for applications demanding trust, transparency, and semantic robustness. We propose Code Concept Analysis (CoCoA): a global post-hoc interpretability framework that uncovers emergent lexical, syntactic, and semantic structures in a code language model's representation space by clustering contextualized token embeddings into human-interpretable concept groups. We propose a hybrid annotation pipeline that combines static analysis tool-based syntactic alignment with prompt-engineered large language models (LLMs), enabling scalable labeling of latent concepts across abstraction levels. We analyse the distribution of concepts across layers and across three finetuning tasks. Emergent concept clusters can help identify unexpected latent interactions and be used to identify trends and biases within the model's learned representations. We further integrate LCA with local attribution methods to produce concept-grounded explanations, improving the coherence and interpretability of token-level saliency. Empirical evaluations across multiple models and tasks show that LCA discovers concepts that remain stable under semantic-preserving perturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve predictably with fine-tuning. In a user study, concept-augmented explanations disambiguate token roles. In a user study on the programming-language classification task, concept-augmented explanations disambiguated token roles and improved human-centric explainability by 37 percentage points compared with token-level attributions using Integrated Gradients.         ",
    "url": "https://arxiv.org/abs/2510.00476",
    "authors": [
      "Arushi Sharma",
      "Vedant Pungliya",
      "Christopher J. Quinn",
      "Ali Jannesari"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00495",
    "title": "Normal-Abnormal Guided Generalist Anomaly Detection",
    "abstract": "           Generalist Anomaly Detection (GAD) aims to train a unified model on an original domain that can detect anomalies in new target domains. Previous GAD methods primarily use only normal samples as references, overlooking the valuable information contained in anomalous samples that are often available in real-world scenarios. To address this limitation, we propose a more practical approach: normal-abnormal-guided generalist anomaly detection, which leverages both normal and anomalous samples as references to guide anomaly detection across diverse domains. We introduce the Normal-Abnormal Generalist Learning (NAGL) framework, consisting of two key components: Residual Mining (RM) and Anomaly Feature Learning (AFL). RM extracts abnormal patterns from normal-abnormal reference residuals to establish transferable anomaly representations, while AFL adaptively learns anomaly features in query images through residual mapping to identify instance-aware anomalies. Our approach effectively utilizes both normal and anomalous references for more accurate and efficient cross-domain anomaly detection. Extensive experiments across multiple benchmarks demonstrate that our method significantly outperforms existing GAD approaches. This work represents the first to adopt a mixture of normal and abnormal samples as references in generalist anomaly detection. The code and datasets are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00495",
    "authors": [
      "Yuexin Wang",
      "Xiaolei Wang",
      "Yizheng Gong",
      "Jimin Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00501",
    "title": "CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling",
    "abstract": "           Code Large Language Models (CodeLLMs) are increasingly used in code generation tasks across a wide range of applications. However, their performance is often inconsistent across different programming languages (PLs), with low-resource PLs suffering the most due to limited training data. In this paper, we present CodeChemist, a novel and efficient framework for test-time scaling that enables functional knowledge transfer from high-resource to low-resource PLs using generated test cases. CodeChemist first generates and executes code in high-resource PLs to create test cases that encapsulate functional knowledge. It then uses multi-temperature hedged sampling to generate code snippets in the low-resource PL and selects the best one based on the pass rate of the test cases. Our extensive experiments show that CodeChemist outperforms existing test-time scaling approaches, boosting the performance of code generation for low-resource PLs without requiring any model retraining.         ",
    "url": "https://arxiv.org/abs/2510.00501",
    "authors": [
      "Kaixin Wang",
      "Tianlin Li",
      "Xiaoyu Zhang",
      "Aishan Liu",
      "Xianglong Liu",
      "Ziqi Liu",
      "Zhiqiang Zhang",
      "Jun Zhou",
      "and Bin Shi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2510.00507",
    "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs",
    "abstract": "           As multimodal LLM-driven agents continue to advance in autonomy and generalization, evaluation based on static datasets can no longer adequately assess their true capabilities in dynamic environments and diverse tasks. Existing LLM-based synthetic data methods are largely designed for LLM training and evaluation, and thus cannot be directly applied to agent tasks that require tool use and interactive capabilities. While recent studies have explored automatic agent task generation with LLMs, most efforts remain limited to text or image analysis, without systematically modeling multi-step interactions in web environments. To address these challenges, we propose Graph2Eval, a knowledge graph-based framework that automatically generates both multimodal document comprehension tasks and web interaction tasks, enabling comprehensive evaluation of agents' reasoning, collaboration, and interactive capabilities. In our approach, knowledge graphs constructed from multi-source external data serve as the task space, where we translate semantic relations into structured multimodal tasks using subgraph sampling, task templates, and meta-paths. A multi-stage filtering pipeline based on node reachability, LLM scoring, and similarity analysis is applied to guarantee the quality and executability of the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures reasoning, collaboration, and interaction capabilities. We instantiate the framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning document comprehension and web interaction scenarios. Experiments show that Graph2Eval efficiently generates tasks that differentiate agent and model performance, revealing gaps in reasoning, collaboration, and web interaction across different settings and offering a new perspective for agent evaluation.         ",
    "url": "https://arxiv.org/abs/2510.00507",
    "authors": [
      "Yurun Chen",
      "Xavier Hu",
      "Yuhan Liu",
      "Ziqi Wang",
      "Zeyi Liao",
      "Lin Chen",
      "Feng Wei",
      "Yuxi Qian",
      "Bo Zheng",
      "Keting Yin",
      "Shengyu Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00517",
    "title": "Understanding Sensitivity of Differential Attention through the Lens of Adversarial Robustness",
    "abstract": "           Differential Attention (DA) has been proposed as a refinement to standard attention, suppressing redundant or noisy context through a subtractive structure and thereby reducing contextual hallucination. While this design sharpens task-relevant focus, we show that it also introduces a structural fragility under adversarial perturbations. Our theoretical analysis identifies negative gradient alignment-a configuration encouraged by DA's subtraction-as the key driver of sensitivity amplification, leading to increased gradient norms and elevated local Lipschitz constants. We empirically validate this Fragile Principle through systematic experiments on ViT/DiffViT and evaluations of pretrained CLIP/DiffCLIP, spanning five datasets in total. These results demonstrate higher attack success rates, frequent gradient opposition, and stronger local sensitivity compared to standard attention. Furthermore, depth-dependent experiments reveal a robustness crossover: stacking DA layers attenuates small perturbations via depth-dependent noise cancellation, though this protection fades under larger attack budgets. Overall, our findings uncover a fundamental trade-off: DA improves discriminative focus on clean inputs but increases adversarial vulnerability, underscoring the need to jointly design for selectivity and robustness in future attention mechanisms.         ",
    "url": "https://arxiv.org/abs/2510.00517",
    "authors": [
      "Tsubasa Takahashi",
      "Shojiro Yamabe",
      "Futa Waseda",
      "Kento Sasaki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2510.00522",
    "title": "ARIONet: An Advanced Self-supervised Contrastive Representation Network for Birdsong Classification and Future Frame Prediction",
    "abstract": "           Automated birdsong classification is essential for advancing ecological monitoring and biodiversity studies. Despite recent progress, existing methods often depend heavily on labeled data, use limited feature representations, and overlook temporal dynamics essential for accurate species identification. In this work, we propose a self-supervised contrastive network, ARIONet (Acoustic Representation for Interframe Objective Network), that jointly optimizes contrastive classification and future frame prediction using augmented audio representations. The model simultaneously integrates multiple complementary audio features within a transformer-based encoder model. Our framework is designed with two key objectives: (1) to learn discriminative species-specific representations for contrastive learning through maximizing similarity between augmented views of the same audio segment while pushing apart different samples, and (2) to model temporal dynamics by predicting future audio frames, both without requiring large-scale annotations. We validate our framework on four diverse birdsong datasets, including the British Birdsong Dataset, Bird Song Dataset, and two extended Xeno-Canto subsets (A-M and N-Z). Our method consistently outperforms existing baselines and achieves classification accuracies of 98.41%, 93.07%, 91.89%, and 91.58%, and F1-scores of 97.84%, 94.10%, 91.29%, and 90.94%, respectively. Furthermore, it demonstrates low mean absolute errors and high cosine similarity, up to 95%, in future frame prediction tasks. Extensive experiments further confirm the effectiveness of our self-supervised learning strategy in capturing complex acoustic patterns and temporal dependencies, as well as its potential for real-world applicability in ecological conservation and monitoring.         ",
    "url": "https://arxiv.org/abs/2510.00522",
    "authors": [
      "Md. Abdur Rahman",
      "Selvarajah Thuseethan",
      "Kheng Cher Yeo",
      "Reem E. Mohamed",
      "Sami Azam"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2510.00524",
    "title": "Two stage GNSS outlier detection for factor graph optimization based GNSS-RTK/INS/odometer fusion",
    "abstract": "           Reliable GNSS positioning in complex environments remains a critical challenge due to non-line-of-sight (NLOS) propagation, multipath effects, and frequent signal blockages. These effects can easily introduce large outliers into the raw pseudo-range measurements, which significantly degrade the performance of global navigation satellite system (GNSS) real-time kinematic (RTK) positioning and limit the effectiveness of tightly coupled GNSS-based integrated navigation system. To address this issue, we propose a two-stage outlier detection method and apply the method in a tightly coupled GNSS-RTK, inertial navigation system (INS), and odometer integration based on factor graph optimization (FGO). In the first stage, Doppler measurements are employed to detect pseudo-range outliers in a GNSS-only manner, since Doppler is less sensitive to multipath and NLOS effects compared with pseudo-range, making it a more stable reference for detecting sudden inconsistencies. In the second stage, pre-integrated inertial measurement units (IMU) and odometer constraints are used to generate predicted double-difference pseudo-range measurements, which enable a more refined identification and rejection of remaining outliers. By combining these two complementary stages, the system achieves improved robustness against both gross pseudo-range errors and degraded satellite measuring quality. The experimental results demonstrate that the two-stage detection framework significantly reduces the impact of pseudo-range outliers, and leads to improved positioning accuracy and consistency compared with representative baseline approaches. In the deep urban canyon test, the outlier mitigation method has limits the RMSE of GNSS-RTK/INS/odometer fusion from 0.52 m to 0.30 m, with 42.3% improvement.         ",
    "url": "https://arxiv.org/abs/2510.00524",
    "authors": [
      "Baoshan Song",
      "Penggao Yan",
      "Xiao Xia",
      "Yihan Zhong",
      "Weisong Wen",
      "Li-Ta Hsu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2510.00529",
    "title": "Memory-Augmented Log Analysis with Phi-4-mini: Enhancing Threat Detection in Structured Security Logs",
    "abstract": "           Structured security logs are critical for detecting advanced persistent threats (APTs). Large language models (LLMs) struggle in this domain due to limited context and domain mismatch. We propose \\textbf{DM-RAG}, a dual-memory retrieval-augmented generation framework for structured log analysis. It integrates a short-term memory buffer for recent summaries and a long-term FAISS-indexed memory for historical patterns. An instruction-tuned Phi-4-mini processes the combined context and outputs structured predictions. Bayesian fusion promotes reliable persistence into memory. On the UNSW-NB15 dataset, DM-RAG achieves 53.64% accuracy and 98.70% recall, surpassing fine-tuned and RAG baselines in recall. The architecture is lightweight, interpretable, and scalable, enabling real-time threat monitoring without extra corpora or heavy tuning.         ",
    "url": "https://arxiv.org/abs/2510.00529",
    "authors": [
      "Anbi Guo",
      "Mahfuza Farooque"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2510.00537",
    "title": "Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?",
    "abstract": "           As large language models (LLMs) scale, the question is not only how large they become, but how much of their capacity is effectively utilized. Existing scaling laws relate model size to loss, yet overlook how components exploit their latent space. We study feed-forward networks (FFNs) and recast width selection as a spectral utilization problem. Using a lightweight diagnostic suite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral Concentration, and the composite Spectral Utilization Index (SUI) -- we quantify how many latent directions are meaningfully activated across LLaMA, GPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling law: soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly and with high variance. This asymmetry suggests that widening FFNs mostly adds low-energy tail directions, while dominant-mode subspaces saturate early. Moreover, at larger widths, variance further collapses into a narrow subspace, leaving much of the latent space under-utilized. These results recast FFN width selection as a principled trade-off between tail capacity and dominant-mode capacity, offering concrete guidance for inference-efficient LLM design.         ",
    "url": "https://arxiv.org/abs/2510.00537",
    "authors": [
      "Nandan Kumar Jha",
      "Brandon Reagen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2510.00547",
    "title": "Forestpest-YOLO: A High-Performance Detection Framework for Small Forestry Pests",
    "abstract": "           Detecting agricultural pests in complex forestry environments using remote sensing imagery is fundamental for ecological preservation, yet it is severely hampered by practical challenges. Targets are often minuscule, heavily occluded, and visually similar to the cluttered background, causing conventional object detection models to falter due to the loss of fine-grained features and an inability to handle extreme data imbalance. To overcome these obstacles, this paper introduces Forestpest-YOLO, a detection framework meticulously optimized for the nuances of forestry remote sensing. Building upon the YOLOv8 architecture, our framework introduces a synergistic trio of innovations. We first integrate a lossless downsampling module, SPD-Conv, to ensure that critical high-resolution details of small targets are preserved throughout the network. This is complemented by a novel cross-stage feature fusion block, CSPOK, which dynamically enhances multi-scale feature representation while suppressing background noise. Finally, we employ VarifocalLoss to refine the training objective, compelling the model to focus on high-quality and hard-to-classify samples. Extensive experiments on our challenging, self-constructed ForestPest dataset demonstrate that Forestpest-YOLO achieves state-of-the-art performance, showing marked improvements in detecting small, occluded pests and significantly outperforming established baseline models.         ",
    "url": "https://arxiv.org/abs/2510.00547",
    "authors": [
      "Aoduo Li",
      "Peikai Lin",
      "Jiancheng Li",
      "Zhen Zhang",
      "Shiting Wu",
      "Zexiao Liang",
      "Zhifa Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00561",
    "title": "Assessing Foundation Models for Mold Colony Detection with Limited Training Data",
    "abstract": "           The process of quantifying mold colonies on Petri dish samples is of critical importance for the assessment of indoor air quality, as high colony counts can indicate potential health risks and deficiencies in ventilation systems. Conventionally the automation of such a labor-intensive process, as well as other tasks in microbiology, relies on the manual annotation of large datasets and the subsequent extensive training of models like YoloV9. To demonstrate that exhaustive annotation is not a prerequisite anymore when tackling a new vision task, we compile a representative dataset of 5000 Petri dish images annotated with bounding boxes, simulating both a traditional data collection approach as well as few-shot and low-shot scenarios with well curated subsets with instance level masks. We benchmark three vision foundation models against traditional baselines on task specific metrics, reflecting realistic real-world requirements. Notably, MaskDINO attains near-parity with an extensively trained YoloV9 model while finetuned only on 150 images, retaining competitive performance with as few as 25 images, still being reliable on $\\approx$ 70% of the samples. Our results show that data-efficient foundation models can match traditional approaches with only a fraction of the required data, enabling earlier development and faster iterative improvement of automated microbiological systems with a superior upper-bound performance than traditional models would achieve.         ",
    "url": "https://arxiv.org/abs/2510.00561",
    "authors": [
      "Henrik Pichler",
      "Janis Keuper",
      "Matthew Copping"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00565",
    "title": "Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability",
    "abstract": "           Diffusion language models (DLMs) generate tokens in parallel through iterative denoising, which can reduce latency and enable bidirectional conditioning. However, the safety risks posed by jailbreak attacks that exploit this inference mechanism are not well understood. In this paper, we reveal that DLMs have a critical vulnerability stemming from their iterative denoising process and propose a countermeasure. Specifically, our investigation shows that if an affirmative token for a harmful query appears at an intermediate step, subsequent denoising can be steered toward a harmful response even in aligned models. As a result, simply injecting such affirmative tokens can readily bypass the safety guardrails. Furthermore, we demonstrate that the vulnerability allows existing optimization-based jailbreak attacks to succeed on DLMs. Building on this analysis, we propose a novel safety alignment method tailored to DLMs that trains models to generate safe responses from contaminated intermediate states that contain affirmative tokens. Our experiments indicate that the proposed method significantly mitigates the vulnerability with minimal impact on task performance. Furthermore, our method improves robustness against conventional jailbreak attacks. Our work underscores the need for DLM-specific safety research.         ",
    "url": "https://arxiv.org/abs/2510.00565",
    "authors": [
      "Shojiro Yamabe",
      "Jun Sakuma"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00572",
    "title": "IntrusionX: A Hybrid Convolutional-LSTM Deep Learning Framework with Squirrel Search Optimization for Network Intrusion Detection",
    "abstract": "           Intrusion Detection Systems (IDS) face persistent challenges due to evolving cyberattacks, high-dimensional traffic data, and severe class imbalance in benchmark datasets such as NSL-KDD. To address these issues, we propose IntrusionX, a hybrid deep learning framework that integrates Convolutional Neural Networks (CNNs) for local feature extraction and Long Short-Term Memory (LSTM) networks for temporal modeling. The architecture is further optimized using the Squirrel Search Algorithm (SSA), enabling effective hyperparameter tuning while maintaining computational efficiency. Our pipeline incorporates rigorous preprocessing, stratified data splitting, and dynamic class weighting to enhance the detection of rare classes. Experimental evaluation on NSL-KDD demonstrates that IntrusionX achieves 98% accuracy in binary classification and 87% in 5-class classification, with significant improvements in minority class recall (U2R: 71%, R2L: 93%). The novelty of IntrusionX lies in its reproducible, imbalance-aware design with metaheuristic optimization.         ",
    "url": "https://arxiv.org/abs/2510.00572",
    "authors": [
      "Ahsan Farabi",
      "Muhaiminul Rashid Shad",
      "Israt Khandaker"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00582",
    "title": "SAGE-LD: Towards Scalable and Generalizable End-to-End Language Diarization via Simulated Data Augmentation",
    "abstract": "           In this paper, we present a neural spoken language diarization model that supports an unconstrained span of languages within a single framework. Our approach integrates a learnable query-based architecture grounded in multilingual awareness, with large-scale pretraining on simulated code-switching data. By jointly leveraging these two components, our method overcomes the limitations of conventional approaches in data scarcity and architecture optimization, and generalizes effectively to real-world multilingual settings across diverse environments. Experimental results demonstrate that our approach achieves state-of-the-art performance on several language diarization benchmarks, with a relative performance improvement of 23% to 52% over previous methods. We believe that this work not only advances research in language diarization but also establishes a foundational framework for code-switching speech technologies.         ",
    "url": "https://arxiv.org/abs/2510.00582",
    "authors": [
      "Sangmin Lee",
      "Woongjib Choi",
      "Jihyun Kim",
      "Hong-Goo Kang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2510.00586",
    "title": "Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors",
    "abstract": "           Existing data poisoning attacks on retrieval-augmented generation (RAG) systems scale poorly because they require costly optimization of poisoned documents for each target phrase. We introduce Eyes-on-Me, a modular attack that decomposes an adversarial document into reusable Attention Attractors and Focus Regions. Attractors are optimized to direct attention to the Focus Region. Attackers can then insert semantic baits for the retriever or malicious instructions for the generator, adapting to new targets at near zero cost. This is achieved by steering a small subset of attention heads that we empirically identify as strongly correlated with attack success. Across 18 end-to-end RAG settings (3 datasets $\\times$ 2 retrievers $\\times$ 3 generators), Eyes-on-Me raises average attack success rates from 21.9 to 57.8 (+35.9 points, 2.6$\\times$ over prior work). A single optimized attractor transfers to unseen black box retrievers and generators without retraining. Our findings establish a scalable paradigm for RAG data poisoning and show that modular, reusable components pose a practical threat to modern AI systems. They also reveal a strong link between attention concentration and model outputs, informing interpretability research.         ",
    "url": "https://arxiv.org/abs/2510.00586",
    "authors": [
      "Yen-Shan Chen",
      "Sian-Yao Huang",
      "Cheng-Lin Yang",
      "Yun-Nung Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2510.00588",
    "title": "Dynamic Low Power Traffic Pattern for Energy Constrained Wireless Sensor Networks",
    "abstract": "           Wireless Sensor Networks (WSNs) are extensively utilized in critical applications, including remote monitoring, target tracking, healthcare systems, industrial automation, and smart control in both residential and industrial settings. One of the primary challenges in these systems is maintaining energy efficiency, given that most sensor nodes rely on limited battery resources. To tackle this problem, this study introduces an energy-saving strategy designed for tree-structured networks with dynamic traffic patterns. The approach focuses on lowering power usage by decreasing the length and occurrence of idle listening state where nodes remain active unnecessarily while waiting for data transmissions that may never occur. By reducing this form of energy waste, the proposed approach is designed to extend the operational lifetime and enhance the throughput of the wireless sensor network. Simulation results obtained using the OMNeT++ simulator with the MiXiM framework demonstrate that the solution significantly reduces energy consumption, increases data throughput, and improves overall network efficiency and longevity.         ",
    "url": "https://arxiv.org/abs/2510.00588",
    "authors": [
      "Almamoon Alauthman"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2510.00599",
    "title": "Designing Ambiguity Sets for Distributionally Robust Optimization Using Structural Causal Optimal Transport",
    "abstract": "           Distributionally robust optimization tackles out-of-sample issues like overfitting and distribution shifts by adopting an adversarial approach over a range of possible data distributions, known as the ambiguity set. To balance conservatism and accuracy, these sets must include realistic probability distributions by leveraging information from the nominal distribution. Assuming that nominal distributions arise from a structural causal model with a directed acyclic graph $\\mathcal{G}$ and structural equations, previous methods such as adapted and $\\mathcal{G}$-causal optimal transport have only utilized causal graph information in designing ambiguity sets. In this work, we propose incorporating structural equations, which include causal graph information, to enhance ambiguity sets, resulting in more realistic distributions. We introduce structural causal optimal transport and its associated ambiguity set, demonstrating their advantages and connections to previous methods. A key benefit of our approach is a relaxed version, where a regularization term replaces the complex causal constraints, enabling an efficient algorithm via difference-of-convex programming to solve structural causal optimal transport. We also show that when structural information is absent and must be estimated, our approach remains effective and provides finite sample guarantees. Lastly, we address the radius of ambiguity sets, illustrating how our method overcomes the curse of dimensionality in optimal transport problems, achieving faster shrinkage with dimension-free order.         ",
    "url": "https://arxiv.org/abs/2510.00599",
    "authors": [
      "Ahmad-Reza Ehyaei",
      "Golnoosh Farnadi",
      "Samira Samadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00604",
    "title": "Disentangling Foreground and Background for vision-Language Navigation via Online Augmentation",
    "abstract": "           Following language instructions, vision-language navigation (VLN) agents are tasked with navigating unseen environments. While augmenting multifaceted visual representations has propelled advancements in VLN, the significance of foreground and background in visual observations remains underexplored. Intuitively, foreground regions provide semantic cues, whereas the background encompasses spatial connectivity information. Inspired on this insight, we propose a Consensus-driven Online Feature Augmentation strategy (COFA) with alternative foreground and background features to facilitate the navigable generalization. Specifically, we first leverage semantically-enhanced landmark identification to disentangle foreground and background as candidate augmented features. Subsequently, a consensus-driven online augmentation strategy encourages the agent to consolidate two-stage voting results on feature preferences according to diverse instructions and navigational locations. Experiments on REVERIE and R2R demonstrate that our online foreground-background augmentation boosts the generalization of baseline and attains state-of-the-art performance.         ",
    "url": "https://arxiv.org/abs/2510.00604",
    "authors": [
      "Yunbo Xu",
      "Xuesong Zhang",
      "Jia Li",
      "Zhenzhen Hu",
      "Richang Hong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00618",
    "title": "Robust Context-Aware Object Recognition",
    "abstract": "           In visual recognition, both the object of interest (referred to as foreground, FG, for simplicity) and its surrounding context (background, BG) play an important role. However, standard supervised learning often leads to unintended over-reliance on the BG, known as shortcut learning of spurious correlations, limiting model robustness in real-world deployment settings. In the literature, the problem is mainly addressed by suppressing the BG, sacrificing context information for improved generalization. We propose RCOR -- Robust Context-Aware Object Recognition -- the first approach that jointly achieves robustness and context-awareness without compromising either. RCOR treats localization as an integral part of recognition to decouple object-centric and context-aware modelling, followed by a robust, non-parametric fusion. It improves the performance of both supervised models and VLM on datasets with both in-domain and out-of-domain BG, even without fine-tuning. The results confirm that localization before recognition is now possible even in complex scenes as in ImageNet-1k.         ",
    "url": "https://arxiv.org/abs/2510.00618",
    "authors": [
      "Klara Janouskova",
      "Cristian Gavrus",
      "Jiri Matas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00627",
    "title": "Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and Lightweight Trajectory Prediction",
    "abstract": "           Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and Intelligent Transportation Systems (ITS), supporting efficient motion planning and real-time traffic safety management. Diffusion models have recently demonstrated strong performance in probabilistic trajectory prediction, but their large model size and slow sampling process hinder real-world deployment. This paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel method for real-time and lightweight trajectory prediction. Built upon Collaborative Progressive Distillation (CPD), CDDM progressively transfers knowledge from a high-capacity teacher diffusion model to a lightweight student model, jointly reducing both the number of sampling steps and the model size across distillation iterations. A dual-signal regularized distillation loss is further introduced to incorporate guidance from both the teacher and ground-truth data, mitigating potential overfitting and ensuring robust performance. Extensive experiments on the ETH-UCY pedestrian benchmark and the nuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art prediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the baseline model's ADE and FDE performance on pedestrian trajectories, while requiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x compression, 31x acceleration, and 9 ms latency. Qualitative results further show that CDDM generates diverse and accurate trajectories under dynamic agent behaviors and complex social interactions. By bridging high-performing generative models with practical deployment constraints, CDDM enables resource-efficient probabilistic prediction for AVs and ITS. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00627",
    "authors": [
      "Bingzhang Wang",
      "Kehua Chen",
      "Yinhai Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00634",
    "title": "LAKAN: Landmark-assisted Adaptive Kolmogorov-Arnold Network for Face Forgery Detection",
    "abstract": "           The rapid development of deepfake generation techniques necessitates robust face forgery detection algorithms. While methods based on Convolutional Neural Networks (CNNs) and Transformers are effective, there is still room for improvement in modeling the highly complex and non-linear nature of forgery artifacts. To address this issue, we propose a novel detection method based on the Kolmogorov-Arnold Network (KAN). By replacing fixed activation functions with learnable splines, our KAN-based approach is better suited to this challenge. Furthermore, to guide the network's focus towards critical facial areas, we introduce a Landmark-assisted Adaptive Kolmogorov-Arnold Network (LAKAN) module. This module uses facial landmarks as a structural prior to dynamically generate the internal parameters of the KAN, creating an instance-specific signal that steers a general-purpose image encoder towards the most informative facial regions with artifacts. This core innovation creates a powerful combination between geometric priors and the network's learning process. Extensive experiments on multiple public datasets show that our proposed method achieves superior performance.         ",
    "url": "https://arxiv.org/abs/2510.00634",
    "authors": [
      "Jiayao Jiang",
      "Siran Peng",
      "Bin Liu",
      "Qi Chu",
      "Nenghai Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00635",
    "title": "Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack",
    "abstract": "           Recent advances in text-to-image (T2I) diffusion models have enabled impressive generative capabilities, but they also raise significant safety concerns due to the potential to produce harmful or undesirable content. While concept erasure has been explored as a mitigation strategy, most existing approaches and corresponding attack evaluations are tailored to Stable Diffusion (SD) and exhibit limited effectiveness when transferred to next-generation rectified flow transformers such as Flux. In this work, we present ReFlux, the first concept attack method specifically designed to assess the robustness of concept erasure in the latest rectified flow-based T2I framework. Our approach is motivated by the observation that existing concept erasure techniques, when applied to Flux, fundamentally rely on a phenomenon known as attention localization. Building on this insight, we propose a simple yet effective attack strategy that specifically targets this property. At its core, a reverse-attention optimization strategy is introduced to effectively reactivate suppressed signals while stabilizing attention. This is further reinforced by a velocity-guided dynamic that enhances the robustness of concept reactivation by steering the flow matching process, and a consistency-preserving objective that maintains the global layout and preserves unrelated content. Extensive experiments consistently demonstrate the effectiveness and efficiency of the proposed attack method, establishing a reliable benchmark for evaluating the robustness of concept erasure strategies in rectified flow transformers.         ",
    "url": "https://arxiv.org/abs/2510.00635",
    "authors": [
      "Nanxiang Jiang",
      "Zhaoxin Fan",
      "Enhan Kang",
      "Daiheng Gao",
      "Yun Zhou",
      "Yanxia Chang",
      "Zheng Zhu",
      "Yeying Jin",
      "Wenjun Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00650",
    "title": "Threats to the sustainability of Community Notes on X",
    "abstract": "           Community Notes are emerging as an important option for content moderation. The Community Notes system pioneered by Twitter, now known as X, uses a bridging algorithm to identify user-generated context with upvotes across political divides, supposedly spinning consensual gold from partisan straw. It is important to understand the nature of the community behind Community Notes, especially as the feature has now been imitated by several billion-user platforms. We look for signs of stability and disruption in the X Community Notes community and interrogate the motivations other than partisan animus (Allen, Martel, and Rand 2022) which may be driving users to contribute. We conduct a novel analysis of the impact of having a note published, which requires being considered \"helpful\" by the bridging algorithm, utilising a regression discontinuity design. This allows stronger causal inference than conventional methods used with observational data. Our analysis shows the positive effect on future note authoring of having a note published. This highlights the risk of the current system, where the proportion of notes considered \"helpful\" (and therefore shown to users on X) is low, 10%, and declining. This analysis has implications for the future of Community Notes on X and the extension of this approach to other platforms.         ",
    "url": "https://arxiv.org/abs/2510.00650",
    "authors": [
      "Zahra Arjmandi-Lari",
      "Alexios Mantzarlis",
      "Tom Stafford"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2510.00651",
    "title": "FIN: Fast Inference Network for Map Segmentation",
    "abstract": "           Multi-sensor fusion in autonomous vehicles is becoming more common to offer a more robust alternative for several perception tasks. This need arises from the unique contribution of each sensor in collecting data: camera-radar fusion offers a cost-effective solution by combining rich semantic information from cameras with accurate distance measurements from radar, without incurring excessive financial costs or overwhelming data processing requirements. Map segmentation is a critical task for enabling effective vehicle behaviour in its environment, yet it continues to face significant challenges in achieving high accuracy and meeting real-time performance requirements. Therefore, this work presents a novel and efficient map segmentation architecture, using cameras and radars, in the \\acrfull{bev} space. Our model introduces a real-time map segmentation architecture considering aspects such as high accuracy, per-class balancing, and inference time. To accomplish this, we use an advanced loss set together with a new lightweight head to improve the perception results. Our results show that, with these modifications, our approach achieves results comparable to large models, reaching 53.5 mIoU, while also setting a new benchmark for inference time, improving it by 260\\% over the strongest baseline models.         ",
    "url": "https://arxiv.org/abs/2510.00651",
    "authors": [
      "Ruan Bispo",
      "Tim Brophy",
      "Reenu Mohandas",
      "Anthony Scanlan",
      "Ciar\u00e1n Eising"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00652",
    "title": "OTTER: Open-Tagging via Text-Image Representation for Multi-modal Understanding",
    "abstract": "           We introduce OTTER, a unified open-set multi-label tagging framework that harmonizes the stability of a curated, predefined category set with the adaptability of user-driven open tags. OTTER is built upon a large-scale, hierarchically organized multi-modal dataset, collected from diverse online repositories and annotated through a hybrid pipeline combining automated vision-language labeling with human refinement. By leveraging a multi-head attention architecture, OTTER jointly aligns visual and textual representations with both fixed and open-set label embeddings, enabling dynamic and semantically consistent tagging. OTTER consistently outperforms competitive baselines on two benchmark datasets: it achieves an overall F1 score of 0.81 on Otter and 0.75 on Favorite, surpassing the next-best results by margins of 0.10 and 0.02, respectively. OTTER attains near-perfect performance on open-set labels, with F1 of 0.99 on Otter and 0.97 on Favorite, while maintaining competitive accuracy on predefined labels. These results demonstrate OTTER's effectiveness in bridging closed-set consistency with open-vocabulary flexibility for multi-modal tagging applications.         ",
    "url": "https://arxiv.org/abs/2510.00652",
    "authors": [
      "Jieer Ouyang",
      "Xiaoneng Xiang",
      "Zheng Wang",
      "Yangkai Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00654",
    "title": "Weakly Supervised Cloud Detection Combining Spectral Features and Multi-Scale Deep Network",
    "abstract": "           Clouds significantly affect the quality of optical satellite images, which seriously limits their precise application. Recently, deep learning has been widely applied to cloud detection and has achieved satisfactory results. However, the lack of distinctive features in thin clouds and the low quality of training samples limit the cloud detection accuracy of deep learning methods, leaving space for further improvements. In this paper, we propose a weakly supervised cloud detection method that combines spectral features and multi-scale scene-level deep network (SpecMCD) to obtain highly accurate pixel-level cloud masks. The method first utilizes a progressive training framework with a multi-scale scene-level dataset to train the multi-scale scene-level cloud detection network. Pixel-level cloud probability maps are then obtained by combining the multi-scale probability maps and cloud thickness map based on the characteristics of clouds in dense cloud coverage and large cloud-area coverage images. Finally, adaptive thresholds are generated based on the differentiated regions of the scene-level cloud masks at different scales and combined with distance-weighted optimization to obtain binary cloud masks. Two datasets, WDCD and GF1MS-WHU, comprising a total of 60 Gaofen-1 multispectral (GF1-MS) images, were used to verify the effectiveness of the proposed method. Compared to the other weakly supervised cloud detection methods such as WDCD and WSFNet, the F1-score of the proposed SpecMCD method shows an improvement of over 7.82%, highlighting the superiority and potential of the SpecMCD method for cloud detection under different cloud coverage conditions.         ",
    "url": "https://arxiv.org/abs/2510.00654",
    "authors": [
      "Shaocong Zhu",
      "Zhiwei Li",
      "Xinghua Li",
      "Huanfeng Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00664",
    "title": "Batch-CAM: Introduction to better reasoning in convolutional deep learning models",
    "abstract": "           Understanding the inner workings of deep learning models is crucial for advancing artificial intelligence, particularly in high-stakes fields such as healthcare, where accurate explanations are as vital as precision. This paper introduces Batch-CAM, a novel training paradigm that fuses a batch implementation of the Grad-CAM algorithm with a prototypical reconstruction loss. This combination guides the model to focus on salient image features, thereby enhancing its performance across classification tasks. Our results demonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and image reconstruction quality while reducing training and inference times. By ensuring models learn from evidence-relevant information,this approach makes a relevant contribution to building more transparent, explainable, and trustworthy AI systems.         ",
    "url": "https://arxiv.org/abs/2510.00664",
    "authors": [
      "Giacomo Ignesti",
      "Davide Moroni",
      "Massimo Martinelli"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00678",
    "title": "Net-Zero 6G from Earth to Orbit: Sustainable Design of Integrated Terrestrial and Non-Terrestrial Networks",
    "abstract": "           The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks (NTN) plays a crucial role in bridging the digital divide and enabling Sixth Generation (6G) and beyond to achieve truly ubiquitous connectivity. However, combining TN and NTN introduces significant energy challenges due to the diverse characteristics and operational environments of these systems. In this paper, we present for the first time a comprehensive overview of the design challenges associated with achieving Net-Zero energy targets in integrated TN and NTN systems. We outline a set of key enabling technologies that can support the energy demands of such networks while aligning with Net-Zero objectives. To enhance the Energy Efficiency (EE) of integrated TN and NTN systems, we provide a use case analysis that leverages Artificial Intelligence (AI) to deliver adaptable solutions across diverse deployment scenarios. Finally, we highlight promising research directions that can guide the sustainable evolution of integrated TN and NTN.         ",
    "url": "https://arxiv.org/abs/2510.00678",
    "authors": [
      "Muhammad Ali Jamshed",
      "Malik Muhammad Saad",
      "Muhammad Ahmed Mohsin",
      "Dongkyun Kim",
      "Octavia A. Dobre",
      "Halim Yanikomeroglu",
      "Lina Mohjazi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2510.00680",
    "title": "TShape: Rescuing Machine Learning Models from Complex Shapelet Anomalies",
    "abstract": "           Time series anomaly detection (TSAD) is critical for maintaining the reliability of modern IT infrastructures, where complex anomalies frequently arise in highly dynamic environments. In this paper, we present TShape, a novel framework designed to address the challenges in industrial time series anomaly detection. Existing methods often struggle to detect shapelet anomalies that manifest as complex shape deviations, which appear obvious to human experts but prove challenging for machine learning algorithms. TShape introduces a patch-wise dual attention mechanism with multi-scale convolution to model intricate sub-sequence variations by balancing local, fine-grained shape features with global contextual dependencies. Our extensive evaluation on five diverse benchmarks demonstrates that TShape outperforms existing state-of-the-art models, achieving an average 10\\% F1 score improvement in anomaly detection. Additionally, ablation studies and attention visualizations confirm the essential contributions of each component, highlighting the robustness and adaptability of TShape to complex shapelet shapes in time series data.         ",
    "url": "https://arxiv.org/abs/2510.00680",
    "authors": [
      "Hang Cui",
      "Jingjing Li",
      "Haotian Si",
      "Quan Zhou",
      "Changhua Pei",
      "Gaogang Xie",
      "Dan Pei"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2510.00681",
    "title": "Adaptive Event Stream Slicing for Open-Vocabulary Event-Based Object Detection via Vision-Language Knowledge Distillation",
    "abstract": "           Event cameras offer advantages in object detection tasks due to high-speed response, low latency, and robustness to motion blur. However, event cameras lack texture and color information, making open-vocabulary detection particularly challenging. Current event-based detection methods are typically trained on predefined categories, limiting their ability to generalize to novel objects, where encountering previously unseen objects is common. Vision-language models (VLMs) have enabled open-vocabulary object detection in RGB images. However, the modality gap between images and event streams makes it ineffective to directly transfer CLIP to event data, as CLIP was not designed for event streams. To bridge this gap, we propose an event-image knowledge distillation framework that leverages CLIP's semantic understanding to achieve open-vocabulary object detection on event data. Instead of training CLIP directly on event streams, we use image frames as inputs to a teacher model, guiding the event-based student model to learn CLIP's rich visual representations. Through spatial attention-based distillation, the student network learns meaningful visual features directly from raw event inputs while inheriting CLIP's broad visual knowledge. Furthermore, to prevent information loss due to event data segmentation, we design a hybrid spiking neural network (SNN) and convolutional neural network (CNN) framework. Unlike fixed-group event segmentation methods, which often discard crucial temporal information, our SNN adaptively determines the optimal event segmentation moments, ensuring that key temporal features are extracted. The extracted event features are then processed by CNNs for object detection.         ",
    "url": "https://arxiv.org/abs/2510.00681",
    "authors": [
      "Jinchang Zhang",
      "Zijun Li",
      "Jiakai Lin",
      "Guoyu Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00690",
    "title": "ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning",
    "abstract": "           Aligning large-scale vision-language models (VLMs) for complex reasoning via reinforcement learning is often hampered by the limitations of existing policy optimization algorithms, such as static training schedules and the rigid, uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work, we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework that addresses these challenges through a dual-component adaptive learning strategy. First, ACPO employs a dynamic curriculum that orchestrates a principled transition from a stable, near on-policy exploration phase to an efficient, off-policy exploitation phase by progressively increasing sample reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism that replaces the fixed clipping hyperparameter with dynamic, sample-wise bounds modulated by the normalized advantage of each token. This allows for more granular and robust policy updates, enabling larger gradients for high-potential samples while safeguarding against destructive ones. We conduct extensive experiments on a suite of challenging multimodal reasoning benchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate that ACPO consistently outperforms strong baselines such as DAPO and PAPO, achieving state-of-the-art performance, accelerated convergence, and superior training stability.         ",
    "url": "https://arxiv.org/abs/2510.00690",
    "authors": [
      "Yunhao Wang",
      "Ziting Li",
      "Shuai Chen",
      "Tao Liu",
      "Chao Song",
      "Junjie Jiang",
      "Jian Zhu",
      "Peng Gao",
      "Bin Qin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00701",
    "title": "Graph Integrated Multimodal Concept Bottleneck Model",
    "abstract": "           With growing demand for interpretability in deep learning, especially in high stakes domains, Concept Bottleneck Models (CBMs) address this by inserting human understandable concepts into the prediction pipeline, but they are generally single modal and ignore structured concept relationships. To overcome these limitations, we present MoE-SGT, a reasoning driven framework that augments CBMs with a structure injecting Graph Transformer and a Mixture of Experts (MoE) module. We construct answer-concept and answer-question graphs for multimodal inputs to explicitly model the structured relationships among concepts. Subsequently, we integrate Graph Transformer to capture multi level dependencies, addressing the limitations of traditional Concept Bottleneck Models in modeling concept interactions. However, it still encounters bottlenecks in adapting to complex concept patterns. Therefore, we replace the feed forward layers with a Mixture of Experts (MoE) module, enabling the model to have greater capacity in learning diverse concept relationships while dynamically allocating reasoning tasks to different sub experts, thereby significantly enhancing the model's adaptability to complex concept reasoning. MoE-SGT achieves higher accuracy than other concept bottleneck networks on multiple datasets by modeling structured relationships among concepts and utilizing a dynamic expert selection mechanism.         ",
    "url": "https://arxiv.org/abs/2510.00701",
    "authors": [
      "Jiakai Lin",
      "Jinchang Zhang",
      "Guoyu Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00705",
    "title": "Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs",
    "abstract": "           Multimodal Large Language Models (MLLMs) often struggle with fine-grained perception, such as identifying small objects in high-resolution images or finding key moments in long videos. Existing works typically rely on complicated, task-specific fine-tuning, which limits their generalizability and increases model complexity. In this work, we propose an effective, training-free framework that uses an MLLM's intrinsic uncertainty as a proactive guidance signal. Our core insight is that a model's output entropy decreases when presented with relevant visual information. We introduce a unified mechanism that scores candidate visual inputs by response uncertainty, enabling the model to autonomously focus on the most salient data. We apply this simple principle to three complex visual tasks: Visual Search, Long Video Understanding, and Temporal Grounding, allowing off-the-shelf MLLMs to achieve performance competitive with specialized, fine-tuned methods. Our work validates that harnessing intrinsic uncertainty is a powerful, general strategy for enhancing fine-grained multimodal performance.         ",
    "url": "https://arxiv.org/abs/2510.00705",
    "authors": [
      "Sanghwan Kim",
      "Rui Xiao",
      "Stephan Alaniz",
      "Yongqin Xian",
      "Zeynep Akata"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00733",
    "title": "Neural Diffusion Processes for Physically Interpretable Survival Prediction",
    "abstract": "           We introduce DeepFHT, a survival-analysis framework that couples deep neural networks with first hitting time (FHT) distributions from stochastic process theory. Time to event is represented as the first passage of a latent diffusion process to an absorbing boundary. A neural network maps input variables to physically meaningful parameters including initial condition, drift, and diffusion, within a chosen FHT process such as Brownian motion, both with drift and driftless. This yields closed-form survival and hazard functions and captures time-varying risk without assuming proportional-hazards. We compare DeepFHT with Cox regression and other existing parametric survival models, using synthetic and real-world datasets. The method achieves predictive accuracy on par with state-of-the-art approaches, while maintaining a physics-based interpretable parameterization that elucidates the relation between input features and risk. This combination of stochastic process theory and deep learning provides a principled avenue for modeling survival phenomena in complex systems.         ",
    "url": "https://arxiv.org/abs/2510.00733",
    "authors": [
      "Alessio Cristofoletto",
      "Cesare Rollo",
      "Giovanni Birolo",
      "Piero Fariselli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2510.00741",
    "title": "Discovering Communities in Continuous-Time Temporal Networks by Optimizing L-Modularity",
    "abstract": "           Community detection is a fundamental problem in network analysis, with many applications in various fields. Extending community detection to the temporal setting with exact temporal accuracy, as required by real-world dynamic data, necessitates methods specifically adapted to the temporal nature of interactions. We introduce LAGO, a novel method for uncovering dynamic communities by greedy optimization of Longitudinal Modularity, a specific adaptation of Modularity for continuous-time networks. Unlike prior approaches that rely on time discretization or assume rigid community evolution, LAGO captures the precise moments when nodes enter and exit communities. We evaluate LAGO on synthetic benchmarks and real-world datasets, demonstrating its ability to efficiently uncover temporally and topologically coherent communities.         ",
    "url": "https://arxiv.org/abs/2510.00741",
    "authors": [
      "Victor Brabant",
      "Angela Bonifati",
      "R\u00e9my Cazabet"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00757",
    "title": "LEAP: Local ECT-Based Learnable Positional Encodings for Graphs",
    "abstract": "           Graph neural networks (GNNs) largely rely on the message-passing paradigm, where nodes iteratively aggregate information from their neighbors. Yet, standard message passing neural networks (MPNNs) face well-documented theoretical and practical limitations. Graph positional encoding (PE) has emerged as a promising direction to address these limitations. The Euler Characteristic Transform (ECT) is an efficiently computable geometric-topological invariant that characterizes shapes and graphs. In this work, we combine the differentiable approximation of the ECT (DECT) and its local variant ($\\ell$-ECT) to propose LEAP, a new end-to-end trainable local structural PE for graphs. We evaluate our approach on multiple real-world datasets as well as on a synthetic task designed to test its ability to extract topological features. Our results underline the potential of LEAP-based encodings as a powerful component for graph representation learning pipelines.         ",
    "url": "https://arxiv.org/abs/2510.00757",
    "authors": [
      "Juan Amboage",
      "Ernst R\u00f6ell",
      "Patrick Schnider",
      "Bastian Rieck"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00758",
    "title": "Decentralized and Self-adaptive Core Maintenance on Temporal Graphs",
    "abstract": "           Key graph-based problems play a central role in understanding network topology and uncovering patterns of similarity in homogeneous and temporal data. Such patterns can be revealed by analyzing communities formed by nodes, which in turn can be effectively modeled through temporal $k$-cores. This paper introduces a novel decentralized and incremental algorithm for computing the core decomposition of temporal networks. Decentralized solutions leverage the ability of network nodes to communicate and coordinate locally, addressing complex problems in a scalable, adaptive, and timely manner. By leveraging previously computed coreness values, our approach significantly reduces the activation of nodes and the volume of message exchanges when the network changes over time. This enables scalability with only a minimal trade-off in precision. Experimental evaluations on large real-world networks under varying levels of dynamism demonstrate the efficiency of our solution compared to a state-of-the-art approach, particularly in terms of active nodes, communication overhead, and convergence speed.         ",
    "url": "https://arxiv.org/abs/2510.00758",
    "authors": [
      "Davide Rucci",
      "Emanuele Carlini",
      "Patrizio Dazzi",
      "Hanna Kavalionak",
      "Matteo Mordacchini"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2510.00761",
    "title": "Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning",
    "abstract": "           Large language model (LLM) unlearning aims to surgically remove the influence of undesired data or knowledge from an existing model while preserving its utility on unrelated tasks. This paradigm has shown promise in addressing privacy and safety concerns. However, recent findings reveal that unlearning effects are often fragile: post-unlearning manipulations such as weight quantization or fine-tuning can quickly neutralize the intended forgetting. Prior efforts to improve robustness primarily reformulate unlearning objectives by explicitly assuming the role of vulnerability sources. In this work, we take a different perspective by investigating the role of the optimizer, independent of unlearning objectives and formulations, in shaping unlearning robustness. We show that the 'grade' of the optimizer, defined by the level of information it exploits, ranging from zeroth-order (gradient-free) to first-order (gradient-based) to second-order (Hessian-based), is tightly linked to the resilience of unlearning. Surprisingly, we find that downgrading the optimizer, such as using zeroth-order methods or compressed-gradient variants (e.g., gradient sign-based optimizers), often leads to stronger robustness. While these optimizers produce noisier and less precise updates, they encourage convergence to harder-to-disturb basins in the loss landscape, thereby resisting post-training perturbations. By connecting zeroth-order methods with randomized smoothing, we further highlight their natural advantage for robust unlearning. Motivated by these insights, we propose a hybrid optimizer that combines first-order and zeroth-order updates, preserving unlearning efficacy while enhancing robustness. Extensive experiments on the MUSE and WMDP benchmarks, across multiple LLM unlearning algorithms, validate that our approach achieves more resilient forgetting without sacrificing unlearning quality.         ",
    "url": "https://arxiv.org/abs/2510.00761",
    "authors": [
      "Yicheng Lang",
      "Yihua Zhang",
      "Chongyu Fan",
      "Changsheng Wang",
      "Jinghan Jia",
      "Sijia Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00769",
    "title": "ZQBA: Zero Query Black-box Adversarial Attack",
    "abstract": "           Current black-box adversarial attacks either require multiple queries or diffusion models to produce adversarial samples that can impair the target model performance. However, these methods require training a surrogate loss or diffusion models to produce adversarial samples, which limits their applicability in real-world settings. Thus, we propose a Zero Query Black-box Adversarial (ZQBA) attack that exploits the representations of Deep Neural Networks (DNNs) to fool other networks. Instead of requiring thousands of queries to produce deceiving adversarial samples, we use the feature maps obtained from a DNN and add them to clean images to impair the classification of a target model. The results suggest that ZQBA can transfer the adversarial samples to different models and across various datasets, namely CIFAR and Tiny ImageNet. The experiments also show that ZQBA is more effective than state-of-the-art black-box attacks with a single query, while maintaining the imperceptibility of perturbations, evaluated both quantitatively (SSIM) and qualitatively, emphasizing the vulnerabilities of employing DNNs in real-world contexts. All the source code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00769",
    "authors": [
      "Joana C. Costa",
      "Tiago Roxo",
      "Hugo Proen\u00e7a",
      "Pedro R. M. In\u00e1cio"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00778",
    "title": "DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models",
    "abstract": "           Diffusion models have shown to be strong representation learners, showcasing state-of-the-art performance across multiple domains. Aside from accelerated sampling, DDIM also enables the inversion of real images back to their latent codes. A direct inheriting application of this inversion operation is real image editing, where the inversion yields latent trajectories to be utilized during the synthesis of the edited image. Unfortunately, this practical tool has enabled malicious users to freely synthesize misinformative or deepfake contents with greater ease, which promotes the spread of unethical and abusive, as well as privacy-, and copyright-infringing contents. While defensive algorithms such as AdvDM and Photoguard have been shown to disrupt the diffusion process on these images, the misalignment between their objectives and the iterative denoising trajectory at test time results in weak disruptive this http URL this work, we present the DDIM Inversion Attack (DIA) that attacks the integrated DDIM trajectory path. Our results support the effective disruption, surpassing previous defensive methods across various editing methods. We believe that our frameworks and results can provide practical defense methods against the malicious use of AI for both the industry and the research community. Our code is available here: this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00778",
    "authors": [
      "Seunghoo Hong",
      "Geonho Son",
      "Juhun Lee",
      "Simon S. Woo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00794",
    "title": "Complex System Exploration with Interactive Human Guidance",
    "abstract": "           The diversity of patterns that emerge from complex systems motivates their use for scientific or artistic purposes. When exploring these systems, the challenges faced are the size of the parameter space and the strongly non-linear mapping between parameters and emerging patterns. In addition, artists and scientists who explore complex systems do so with an expectation of particular patterns. Taking these expectations into account adds a new set of challenges, which the exploration process must address. We provide design choices and their implementation to address these challenges; enabling the maximization of the diversity of patterns discovered in the user's region of interest -- which we call the constrained diversity -- in a sample-efficient manner. The region of interest is expressed in the form of explicit constraints. These constraints are formulated by the user in a system-agnostic way, and their addition enables interactive system exploration leading to constrained diversity, while maintaining global diversity.         ",
    "url": "https://arxiv.org/abs/2510.00794",
    "authors": [
      "Bastien Morel",
      "Cl\u00e9ment Moulin-Frier",
      "Pascal Barla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00796",
    "title": "MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically Equivalent Prompts",
    "abstract": "           Recent advances in text-to-image (T2I) models, especially diffusion-based architectures, have significantly improved the visual quality of generated images. However, these models continue to struggle with a critical limitation: maintaining semantic consistency when input prompts undergo minor linguistic variations. Despite being logically equivalent, such prompt pairs often yield misaligned or semantically inconsistent images, exposing a lack of robustness in reasoning and generalisation. To address this, we propose MetaLogic, a novel evaluation framework that detects T2I misalignment without relying on ground truth images. MetaLogic leverages metamorphic testing, generating image pairs from prompts that differ grammatically but are semantically identical. By directly comparing these image pairs, the framework identifies inconsistencies that signal failures in preserving the intended meaning, effectively diagnosing robustness issues in the model's logic understanding. Unlike existing evaluation methods that compare a generated image to a single prompt, MetaLogic evaluates semantic equivalence between paired images, offering a scalable, ground-truth-free approach to identifying alignment failures. It categorises these alignment errors (e.g., entity omission, duplication, positional misalignment) and surfaces counterexamples that can be used for model debugging and refinement. We evaluate MetaLogic across multiple state-of-the-art T2I models and reveal consistent robustness failures across a range of logical constructs. We find that even the SOTA text-to-image models like this http URL and DALLE-3 demonstrate a 59 percent and 71 percent misalignment rate, respectively. Our results show that MetaLogic is not only efficient and scalable, but also effective in uncovering fine-grained logical inconsistencies that are overlooked by existing evaluation metrics.         ",
    "url": "https://arxiv.org/abs/2510.00796",
    "authors": [
      "Yifan Shen",
      "Yangyang Shu",
      "Hye-young Paik",
      "Yulei Sui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00802",
    "title": "Guiding Evolutionary Molecular Design: Adding Reinforcement Learning for Mutation Selection",
    "abstract": "           The efficient exploration of chemical space remains a central challenge, as many generative models still produce unstable or non-synthesizable compounds. To address these limitations, we present EvoMol-RL, a significant extension of the EvoMol evolutionary algorithm that integrates reinforcement learning to guide molecular mutations based on local structural context. By leveraging Extended Connectivity Fingerprints (ECFPs), EvoMol-RL learns context-aware mutation policies that prioritize chemically plausible transformations. This approach significantly improves the generation of valid and realistic molecules, reducing the frequency of structural artifacts and enhancing optimization performance. The results demonstrate that EvoMol-RL consistently outperforms its baseline in molecular pre-filtering realism. These results emphasize the effectiveness of combining reinforcement learning with molecular fingerprints to generate chemically relevant molecular structures.         ",
    "url": "https://arxiv.org/abs/2510.00802",
    "authors": [
      "Gaelle Milon-Harnois",
      "Chaimaa Touhami",
      "Nicolas Gutowski",
      "Benoit Da Mota",
      "Thomas Cauchy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00820",
    "title": "NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image Super-Resolution",
    "abstract": "           Most recent real-world image super-resolution (Real-ISR) methods employ pre-trained text-to-image (T2I) diffusion models to synthesize the high-quality image either from random Gaussian noise, which yields realistic results but is slow due to iterative denoising, or directly from the input low-quality image, which is efficient but at the price of lower output quality. These approaches train ControlNet or LoRA modules while keeping the pre-trained model fixed, which often introduces over-enhanced artifacts and hallucinations, suffering from the robustness to inputs of varying degradations. Recent visual autoregressive (AR) models, such as pre-trained Infinity, can provide strong T2I generation capabilities while offering superior efficiency by using the bitwise next-scale prediction strategy. Building upon next-scale prediction, we introduce a robust Real-ISR framework, namely Next-Scale Autoregressive Modeling (NSARM). Specifically, we train NSARM in two stages: a transformation network is first trained to map the input low-quality image to preliminary scales, followed by an end-to-end full-model fine-tuning. Such a comprehensive fine-tuning enhances the robustness of NSARM in Real-ISR tasks without compromising its generative capability. Extensive quantitative and qualitative evaluations demonstrate that as a pure AR model, NSARM achieves superior visual results over existing Real-ISR methods while maintaining a fast inference speed. Most importantly, it demonstrates much higher robustness to the quality of input images, showing stronger generalization performance. Project page: this https URL ",
    "url": "https://arxiv.org/abs/2510.00820",
    "authors": [
      "Xiangtao Kong",
      "Rongyuan Wu",
      "Shuaizheng Liu",
      "Lingchen Sun",
      "Lei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00836",
    "title": "Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based Models and Synthetic Oversampling Techniques",
    "abstract": "           This study aims to detect pump and dump (P&D) manipulation in cryptocurrency markets, where the scarcity of such events causes severe class imbalance and hinders accurate detection. To address this issue, the Synthetic Minority Oversampling Technique (SMOTE) was applied, and advanced ensemble learning models were evaluated to distinguish manipulative trading behavior from normal market activity. The experimental results show that applying SMOTE greatly enhanced the ability of all models to detect P&D events by increasing recall and improving the overall balance between precision and recall. In particular, XGBoost and LightGBM achieved high recall rates (94.87% and 93.59%, respectively) with strong F1-scores and demonstrated fast computational performance, making them suitable for near real time surveillance. These findings indicate that integrating data balancing techniques with ensemble methods significantly improves the early detection of manipulative activities, contributing to a fairer, more transparent, and more stable cryptocurrency market.         ",
    "url": "https://arxiv.org/abs/2510.00836",
    "authors": [
      "Jieun Yu",
      "Minjung Park",
      "Sangmi Chai"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Risk Management (q-fin.RM)"
    ]
  },
  {
    "id": "arXiv:2510.00872",
    "title": "A Visual Diagnostics Framework for District Heating Data: Enhancing Data Quality for AI-Driven Heat Consumption Prediction",
    "abstract": "           High-quality data is a prerequisite for training reliable Artificial Intelligence (AI) models in the energy domain. In district heating networks, sensor and metering data often suffer from noise, missing values, and temporal inconsistencies, which can significantly degrade model performance. This paper presents a systematic approach for evaluating and improving data quality using visual diagnostics, implemented through an interactive web-based dashboard. The dashboard employs Python-based visualization techniques, including time series plots, heatmaps, box plots, histograms, correlation matrices, and anomaly-sensitive KPIs such as skewness and anomaly detection based on the modified z-scores. These tools al-low human experts to inspect and interpret data anomalies, enabling a human-in-the-loop strategy for data quality assessment. The methodology is demonstrated on a real-world dataset from a Danish district heating provider, covering over four years of hourly data from nearly 7000 meters. The findings show how visual analytics can uncover systemic data issues and, in the future, guide data cleaning strategies that enhance the accuracy, stability, and generalizability of Long Short-Term Memory and Gated Recurrent Unit models for heat demand forecasting. The study contributes to a scalable, generalizable framework for visual data inspection and underlines the critical role of data quality in AI-driven energy management systems.         ",
    "url": "https://arxiv.org/abs/2510.00872",
    "authors": [
      "Kristoffer Christensen",
      "Bo N\u00f8rregaard J\u00f8rgensen",
      "Zheng Grace Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2510.00882",
    "title": "AI-CNet3D: An Anatomically-Informed Cross-Attention Network with Multi-Task Consistency Fine-tuning for 3D Glaucoma Classification",
    "abstract": "           Glaucoma is a progressive eye disease that leads to optic nerve damage, causing irreversible vision loss if left untreated. Optical coherence tomography (OCT) has become a crucial tool for glaucoma diagnosis, offering high-resolution 3D scans of the retina and optic nerve. However, the conventional practice of condensing information from 3D OCT volumes into 2D reports often results in the loss of key structural details. To address this, we propose a novel hybrid deep learning model that integrates cross-attention mechanisms into a 3D convolutional neural network (CNN), enabling the extraction of critical features from the superior and inferior hemiretinas, as well as from the optic nerve head (ONH) and macula, within OCT volumes. We introduce Channel Attention REpresentations (CAREs) to visualize cross-attention outputs and leverage them for consistency-based multi-task fine-tuning, aligning them with Gradient-Weighted Class Activation Maps (Grad-CAMs) from the CNN's final convolutional layer to enhance performance, interpretability, and anatomical coherence. We have named this model AI-CNet3D (AI-`See'-Net3D) to reflect its design as an Anatomically-Informed Cross-attention Network operating on 3D data. By dividing the volume along two axes and applying cross-attention, our model enhances glaucoma classification by capturing asymmetries between the hemiretinal regions while integrating information from the optic nerve head and macula. We validate our approach on two large datasets, showing that it outperforms state-of-the-art attention and convolutional models across all key metrics. Finally, our model is computationally efficient, reducing the parameter count by one-hundred--fold compared to other attention mechanisms while maintaining high diagnostic performance and comparable GFLOPS.         ",
    "url": "https://arxiv.org/abs/2510.00882",
    "authors": [
      "Roshan Kenia",
      "Anfei Li",
      "Rishabh Srivastava",
      "Kaveri A. Thakoor"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00884",
    "title": "COMMET: orders-of-magnitude speed-up in finite element method via batch-vectorized neural constitutive updates",
    "abstract": "           Constitutive evaluations often dominate the computational cost of finite element (FE) simulations whenever material models are complex. Neural constitutive models (NCMs) offer a highly expressive and flexible framework for modeling complex material behavior in solid mechanics. However, their practical adoption in large-scale FE simulations remains limited due to significant computational costs, especially in repeatedly evaluating stress and stiffness. NCMs thus represent an extreme case: their large computational graphs make stress and stiffness evaluations prohibitively expensive, restricting their use to small-scale problems. In this work, we introduce COMMET, an open-source FE framework whose architecture has been redesigned from the ground up to accelerate high-cost constitutive updates. Our framework features a novel assembly algorithm that supports batched and vectorized constitutive evaluations, compute-graph-optimized derivatives that replace automatic differentiation, and distributed-memory parallelism via MPI. These advances dramatically reduce runtime, with speed-ups exceeding three orders of magnitude relative to traditional non-vectorized automatic differentiation-based implementations. While we demonstrate these gains primarily for NCMs, the same principles apply broadly wherever for-loop based assembly or constitutive updates limit performance, establishing a new standard for large-scale, high-fidelity simulations in computational mechanics.         ",
    "url": "https://arxiv.org/abs/2510.00884",
    "authors": [
      "Benjamin Alheit",
      "Mathias Peirlinck",
      "Siddhant Kumar"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00890",
    "title": "Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration",
    "abstract": "           The rapid adoption of large language models (LLMs) in scientific writing raises serious concerns regarding authorship integrity and the reliability of scholarly publications. Existing detection approaches mainly rely on document-level classification or surface-level statistical cues; however, they neglect fine-grained span localization, exhibit weak calibration, and often fail to generalize across disciplines and generators. To address these limitations, we present Sci-SpanDet, a structure-aware framework for detecting AI-generated scholarly texts. The proposed method combines section-conditioned stylistic modeling with multi-level contrastive learning to capture nuanced human-AI differences while mitigating topic dependence, thereby enhancing cross-domain robustness. In addition, it integrates BIO-CRF sequence labeling with pointer-based boundary decoding and confidence calibration to enable precise span-level detection and reliable probability estimates. Extensive experiments on a newly constructed cross-disciplinary dataset of 100,000 annotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek, LLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with F1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows strong resilience under adversarial rewriting and maintains balanced accuracy across IMRaD sections and diverse disciplines, substantially surpassing existing baselines. To ensure reproducibility and to foster further research on AI-generated text detection in scholarly documents, the curated dataset and source code will be publicly released upon publication.         ",
    "url": "https://arxiv.org/abs/2510.00890",
    "authors": [
      "Zhen Yin",
      "Shenghua Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00894",
    "title": "FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs",
    "abstract": "           Multimodal Knowledge Graphs (MMKGs) incorporate various modalities, including text and images, to enhance entity and relation representations. Notably, different modalities for the same entity often present complementary and diverse information. However, existing MMKG methods primarily align modalities into a shared space, which tends to overlook the distinct contributions of specific modalities, limiting their performance particularly in low-resource settings. To address this challenge, we propose FusionAdapter for the learning of few-shot relationships (FSRL) in MMKG. FusionAdapter introduces (1) an adapter module that enables efficient adaptation of each modality to unseen relations and (2) a fusion strategy that integrates multimodal entity representations while preserving diverse modality-specific characteristics. By effectively adapting and fusing information from diverse modalities, FusionAdapter improves generalization to novel relations with minimal supervision. Extensive experiments on two benchmark MMKG datasets demonstrate that FusionAdapter achieves superior performance over state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2510.00894",
    "authors": [
      "Ran Liu",
      "Yuan Fang",
      "Xiaoli Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00909",
    "title": "\"We are not Future-ready\": Understanding AI Privacy Risks and Existing Mitigation Strategies from the Perspective of AI Developers in Europe",
    "abstract": "           The proliferation of AI has sparked privacy concerns related to training data, model interfaces, downstream applications, and more. We interviewed 25 AI developers based in Europe to understand which privacy threats they believe pose the greatest risk to users, developers, and businesses and what protective strategies, if any, would help to mitigate them. We find that there is little consensus among AI developers on the relative ranking of privacy risks. These differences stem from salient reasoning patterns that often relate to human rather than purely technical factors. Furthermore, while AI developers are aware of proposed mitigation strategies for addressing these risks, they reported minimal real-world adoption. Our findings highlight both gaps and opportunities for empowering AI developers to better address privacy risks in AI.         ",
    "url": "https://arxiv.org/abs/2510.00909",
    "authors": [
      "Alexandra Klymenko",
      "Stephen Meisenbacher",
      "Patrick Gage Kelley",
      "Sai Teja Peddinti",
      "Kurt Thomas",
      "Florian Matthes"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00922",
    "title": "On Discovering Algorithms for Adversarial Imitation Learning",
    "abstract": "           Adversarial Imitation Learning (AIL) methods, while effective in settings with limited expert demonstrations, are often considered unstable. These approaches typically decompose into two components: Density Ratio (DR) estimation $\\frac{\\rho_E}{\\rho_{\\pi}}$, where a discriminator estimates the relative occupancy of state-action pairs under the policy versus the expert; and Reward Assignment (RA), where this ratio is transformed into a reward signal used to train the policy. While significant research has focused on improving density estimation, the role of reward assignment in influencing training dynamics and final policy performance has been largely overlooked. RA functions in AIL are typically derived from divergence minimization objectives, relying heavily on human design and ingenuity. In this work, we take a different approach: we investigate the discovery of data-driven RA functions, i.e, based directly on the performance of the resulting imitation policy. To this end, we leverage an LLM-guided evolutionary framework that efficiently explores the space of RA functions, yielding \\emph{Discovered Adversarial Imitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably, DAIL generalises across unseen environments and policy optimization algorithms, outperforming the current state-of-the-art of \\emph{human-designed} baselines. Finally, we analyse why DAIL leads to more stable training, offering novel insights into the role of RA functions in the stability of AIL. Code is publicly available: this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00922",
    "authors": [
      "Shashank Reddy Chirra",
      "Jayden Teoh",
      "Praveen Paruchuri",
      "Pradeep Varakantham"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00929",
    "title": "Equivariant Splitting: Self-supervised learning from incomplete data",
    "abstract": "           Self-supervised learning for inverse problems allows to train a reconstruction network from noise and/or incomplete data alone. These methods have the potential of enabling learning-based solutions when obtaining ground-truth references for training is expensive or even impossible. In this paper, we propose a new self-supervised learning strategy devised for the challenging setting where measurements are observed via a single incomplete observation model. We introduce a new definition of equivariance in the context of reconstruction networks, and show that the combination of self-supervised splitting losses and equivariant reconstruction networks results in unbiased estimates of the supervised loss. Through a series of experiments on image inpainting, accelerated magnetic resonance imaging, and compressive sensing, we demonstrate that the proposed loss achieves state-of-the-art performance in settings with highly rank-deficient forward models.         ",
    "url": "https://arxiv.org/abs/2510.00929",
    "authors": [
      "Victor Sechaud",
      "J\u00e9r\u00e9my Scanvic",
      "Quentin Barth\u00e9lemy",
      "Patrice Abry",
      "Juli\u00e1n Tachella"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00933",
    "title": "Product-oriented Product-Process-Resource Asset Network and its Representation in AutomationML for Asset Administration Shell",
    "abstract": "           Current products, especially in the automotive sector, pose complex technical systems having a multi-disciplinary mechatronic nature. Industrial standards supporting system engineering and production typically (i) address the production phase only, but do not cover the complete product life cycle, and (ii) focus on production processes and resources rather than the products themselves. The presented approach is motivated by incorporating impacts of end-of-life phase of the product life cycle into the engineering phase. This paper proposes a modelling approach coming up from the Product-Process-Resource (PPR) modeling paradigm. It combines requirements on (i) respecting the product structure as a basis for the model, and (ii) it incorporates repairing, remanufacturing, or upcycling within cyber-physical production systems. The proposed model called PoPAN should accompany the product during the entire life cycle as a digital shadow encapsulated within the Asset Administration Shell of a product. To facilitate the adoption of the proposed paradigm, the paper also proposes serialization of the model in the AutomationML data format. The model is demonstrated on a use-case for disassembling electric vehicle batteries to support their remanufacturing for stationary battery applications.         ",
    "url": "https://arxiv.org/abs/2510.00933",
    "authors": [
      "Sara Strakosova",
      "Petr Novak",
      "Petr Kadera"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.00946",
    "title": "ChatGPT in Introductory Programming: Counterbalanced Evaluation of Code Quality, Conceptual Learning, and Student Perceptions",
    "abstract": "           Background: Large language models (LLMs) such as ChatGPT are increasingly used in introductory programming courses to provide real-time code generation, debugging, and explanations. While these tools can boost productivity and code quality, concerns remain about over-reliance and potential impacts on conceptual learning. Objective: To investigate how ChatGPT access affects code quality, conceptual understanding, task completion times, and student perceptions in a CS1 course. Methods: We conducted a counterbalanced, quasi-experimental study in which students alternated between ChatGPT and non-ChatGPT conditions across two programming assignments in C (functions and structures). We evaluated their code submissions using multidimensional rubrics, conceptual post-surveys, and task completion time. Results: Students who had access to ChatGPT produced significantly higher rubric scores for code quality and completed tasks in less time compared to those without access. However, gains in conceptual understanding were mixed, lower for the functions topic but higher for the structures topic. Students reported positive experiences with ChatGPT, citing its value for debugging and practice, while expressing concerns about accuracy and long-term skill development. Conclusions: ChatGPT can enhance code quality and efficiency for novice programmers, but may not uniformly improve conceptual understanding. Structured integration and complementary instructional strategies are recommended to foster independent problem-solving skills.         ",
    "url": "https://arxiv.org/abs/2510.00946",
    "authors": [
      "Shiza Andleeb",
      "Brandon Kantorski",
      "Jeffrey C. Carver"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2510.00956",
    "title": "Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning",
    "abstract": "           Machine Learning (ML)-based network models provide fast and accurate predictions for complex network behaviors but require substantial training data. Collecting such data from real networks is often costly and limited, especially for critical scenarios like failures. As a result, researchers commonly rely on simulated data, which reduces accuracy when models are deployed in real environments. We propose a hybrid approach leveraging transfer learning to combine simulated and real-world data. Using RouteNet-Fermi, we show that fine-tuning a pre-trained model with a small real dataset significantly improves performance. Our experiments with OMNeT++ and a custom testbed reduce the Mean Absolute Percentage Error (MAPE) in packet delay prediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, and with 50 scenarios, by 48%.         ",
    "url": "https://arxiv.org/abs/2510.00956",
    "authors": [
      "Carlos G\u00fcemes-Palau",
      "Miquel Ferriol-Galm\u00e9s",
      "Jordi Paillisse-Vilanova",
      "Albert L\u00f3pez-Bresc\u00f3",
      "Pere Barlet-Ros",
      "Albert Cabellos-Aparicio"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00958",
    "title": "Test-Time Search in Neural Graph Coarsening Procedures for the Capacitated Vehicle Routing Problem",
    "abstract": "           The identification of valid inequalities, such as the rounded capacity inequalities (RCIs), is a key component of cutting plane methods for the Capacitated Vehicle Routing Problem (CVRP). While a deep learning-based separation method can learn to find high-quality cuts, our analysis reveals that the model produces fewer cuts than expected because it is insufficiently sensitive to generate a diverse set of generated subsets. This paper proposes an alternative: enhancing the performance of a trained model at inference time through a new test-time search with stochasticity. First, we introduce stochastic edge selection into the graph coarsening procedure, replacing the previously proposed greedy approach. Second, we propose the Graph Coarsening History-based Partitioning (GraphCHiP) algorithm, which leverages coarsening history to identify not only RCIs but also, for the first time, the Framed capacity inequalities (FCIs). Experiments on randomly generated CVRP instances demonstrate the effectiveness of our approach in reducing the dual gap compared to the existing neural separation method. Additionally, our method discovers effective FCIs on a specific instance, despite the challenging nature of identifying such cuts.         ",
    "url": "https://arxiv.org/abs/2510.00958",
    "authors": [
      "Yoonju Sim",
      "Hyeonah Kim",
      "Changhyun Kwon"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2510.00964",
    "title": "Social Photo-Elicitation: The Use of Communal Production of Meaning to Hear a Vulnerable Population",
    "abstract": "           We report on an initial ethnographic exploration of the situation of sex-trafficking survivors in Nepal. In the course of studying trafficking survivors in a protected-living situation created by a non-governmental organization in Nepal, we adapted photo-elicitation to hear the voices of the survivors by making the technique more communal. Bringing sociality to the forefront of the method reduced the pressure on survivors to assert voices as individuals, allowing them to speak. We make three contributions to research. First, we propose a communal form of photo-elicitation as a method to elicit values in sensitive settings. Second, we present the complex circumstances of the survivors as they undergo rehabilitation and move towards life with a ``new normal''. Third, our work adds to HCI and CSCW literature on understanding specific concerns of trafficking survivors and aims to inform designs that can support reintegration of survivors in society. The values that the survivors hold and their notion of future opportunities suggest possession of limited but important social capital in some domains that could be leveraged to aid reintegration.         ",
    "url": "https://arxiv.org/abs/2510.00964",
    "authors": [
      "Aakash Gautam",
      "Chandani Shrestha",
      "Deborah Tatar",
      "Steve Harrison"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2510.00967",
    "title": "QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL",
    "abstract": "           Designing and optimizing task-specific quantum circuits are crucial to leverage the advantage of quantum computing. Recent large language model (LLM)-based quantum circuit generation has emerged as a promising automatic solution. However, the fundamental challenges remain unaddressed: (i) parameterized quantum gates require precise numerical values for optimal performance, which also depend on multiple aspects, including the number of quantum gates, their parameters, and the layout/depth of the circuits. (ii) LLMs often generate low-quality or incorrect quantum circuits due to the lack of quantum domain-specific knowledge. We propose QUASAR, an agentic reinforcement learning (RL) framework for quantum circuits generation and optimization based on tool-augmented LLMs. To align the LLM with quantum-specific knowledge and improve the generated quantum circuits, QUASAR designs (i) a quantum circuit verification approach with external quantum simulators and (ii) a sophisticated hierarchical reward mechanism in RL training. Extensive evaluation shows improvements in both syntax and semantic performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10, outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several supervised-fine-tuning (SFT)-only and RL-only baselines.         ",
    "url": "https://arxiv.org/abs/2510.00967",
    "authors": [
      "Cong Yu",
      "Valter Uotila",
      "Shilong Deng",
      "Qingyuan Wu",
      "Tuo Shi",
      "Songlin Jiang",
      "Lei You",
      "Bo Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2510.00981",
    "title": "FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates",
    "abstract": "           Neural audio codecs are foundational to speech language models. It is expected to have a low frame rate and decoupled semantic and acoustic information. A lower frame rate codec can reduce the computational cost of speech language models by shortening the sequence length. Recent studies have developed 12.5Hz low-frame-rate audio codecs, but even lower frame rate codecs remain underexplored. We find that a major challenge for very low frame rate tokens is missing semantic information. This paper introduces FlexiCodec to address this limitation. FlexiCodec improves semantic preservation with a dynamic frame rate approach and introduces a novel architecture featuring an ASR feature-assisted dual stream encoding and Transformer bottlenecks. With dynamic frame rates, it uses less frames at information-sparse regions through adaptively merging semantically similar frames. A dynamic frame rate also allows FlexiCodec to support inference-time controllable frame rates between 3Hz and 12.5Hz. Experiments on 6.25Hz, 8.3Hz and 12.5Hz average frame rates confirm that FlexiCodec excels over baseline systems in semantic information preservation and delivers a high audio reconstruction quality. We also validate the effectiveness of FlexiCodec in language model-based TTS. Demos are available at: this https URL ",
    "url": "https://arxiv.org/abs/2510.00981",
    "authors": [
      "Jiaqi Li",
      "Yao Qian",
      "Yuxuan Hu",
      "Leying Zhang",
      "Xiaofei Wang",
      "Heng Lu",
      "Manthan Thakker",
      "Jinyu Li",
      "Shang Zhao",
      "Zhizheng Wu"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2510.00992",
    "title": "Optimal Pricing of Electric Vehicle Charging on Coupled Power-Transportation Network based on Generalized Sensitivity Analysis",
    "abstract": "           In the last decade, charging service providers are emerging along with the prevalence of electric vehicles. These providers need to strategically optimize their charging prices to improve the profits considering operation conditions of the coupled power-transportation network. However, the optimal pricing problem generally involves the user equilibrium model, which leads to a mathematical program with equilibrium constraints. As a result, the pricing problem is non-convex and computationally intractable especially for large-scale network. To address this challenge, we propose a generalized sensitivity analysis approach for optimal pricing of electric vehicle charging on coupled power-transportation network. Specifically, we adopt a sensitivity analysis to capture the best response of charging demand to charging price in the gradient form. Consequently, charging service providers can make pricing decisions based on the gradient information instead of the conventional KKT conditions of the user equilibrium model. We then propose a tailored gradient descent algorithm to solve the whole pricing problem. The mathematical proof of validity is given and the time complexity of the proposed algorithm is theoretically polynomial. Numerical experiments on different scales of networks verify the computational efficiency of the proposed algorithm, indicating its potential in evaluating the impact of the optimal pricing on the operational performance of large-scale coupled power-transportation network.         ",
    "url": "https://arxiv.org/abs/2510.00992",
    "authors": [
      "Lyuzhu Pan",
      "Hongcai Zhang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.01002",
    "title": "Semantics-Aligned, Curriculum-Driven, and Reasoning-Enhanced Vulnerability Repair Framework",
    "abstract": "           Current learning-based Automated Vulnerability Repair (AVR) approaches, while promising, often fail to generalize effectively in real-world scenarios. Our diagnostic analysis reveals three fundamental weaknesses in state-of-the-art AVR approaches: (1) limited cross-repository generalization, with performance drops on unseen codebases; (2) inability to capture long-range dependencies, causing a performance degradation on complex, multi-hunk repairs; and (3) over-reliance on superficial lexical patterns, leading to significant performance drops on vulnerabilities with minor syntactic variations like variable renaming. To address these limitations, we propose SeCuRepair, a semantics-aligned, curriculum-driven, and reasoning-enhanced framework for vulnerability repair. At its core, SeCuRepair adopts a reason-then-edit paradigm, requiring the model to articulate why and how a vulnerability should be fixed before generating the patch. This explicit reasoning enforces a genuine understanding of repair logic rather than superficial memorization of lexical patterns. SeCuRepair also moves beyond traditional supervised fine-tuning and employs semantics-aware reinforcement learning, rewarding patches for their syntactic and semantic alignment with the oracle patch rather than mere token overlap. Complementing this, a difficulty-aware curriculum progressively trains the model, starting with simple fixes and advancing to complex, multi-hunk coordinated edits. We evaluate SeCuRepair on strict, repository-level splits of BigVul and newly crafted PrimeVul_AVR datasets. SeCuRepair significantly outperforms all baselines, surpassing the best-performing baselines by 34.52% on BigVul and 31.52% on PrimeVul\\textsubscript{AVR} in terms of CodeBLEU, respectively. Comprehensive ablation studies further confirm that each component of our framework contributes to its final performance.         ",
    "url": "https://arxiv.org/abs/2510.01002",
    "authors": [
      "Chengran Yang",
      "Ting Zhang",
      "Jinfeng Jiang",
      "Xin Zhou",
      "Haoye Tian",
      "Jieke Shi",
      "Junkai Chen",
      "Yikun Li",
      "Eng Lieh Ouh",
      "Lwin Khin Shar",
      "David Lo"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2510.01003",
    "title": "Improving Code Localization with Repository Memory",
    "abstract": "           Code localization is a fundamental challenge in repository-level software engineering tasks such as bug fixing. While existing methods equip language agents with comprehensive tools/interfaces to fetch information from the repository, they overlook the critical aspect of memory, where each instance is typically handled from scratch assuming no prior repository knowledge. In contrast, human developers naturally build long-term repository memory, such as the functionality of key modules and associations between various bug types and their likely fix locations. In this work, we augment language agents with such memory by leveraging a repository's commit history - a rich yet underutilized resource that chronicles the codebase's evolution. We introduce tools that allow the agent to retrieve from a non-parametric memory encompassing recent historical commits and linked issues, as well as functionality summaries of actively evolving parts of the codebase identified via commit patterns. We demonstrate that augmenting such a memory can significantly improve LocAgent, a state-of-the-art localization framework, on both SWE-bench-verified and the more recent SWE-bench-live benchmarks. Our research contributes towards developing agents that can accumulate and leverage past experience for long-horizon tasks, more closely emulating the expertise of human developers.         ",
    "url": "https://arxiv.org/abs/2510.01003",
    "authors": [
      "Boshi Wang",
      "Weijian Xu",
      "Yunsheng Li",
      "Mei Gao",
      "Yujia Xie",
      "Huan Sun",
      "Dongdong Chen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2510.01012",
    "title": "Random Feature Spiking Neural Networks",
    "abstract": "           Spiking Neural Networks (SNNs) as Machine Learning (ML) models have recently received a lot of attention as a potentially more energy-efficient alternative to conventional Artificial Neural Networks. The non-differentiability and sparsity of the spiking mechanism can make these models very difficult to train with algorithms based on propagating gradients through the spiking non-linearity. We address this problem by adapting the paradigm of Random Feature Methods (RFMs) from Artificial Neural Networks (ANNs) to Spike Response Model (SRM) SNNs. This approach allows training of SNNs without approximation of the spike function gradient. Concretely, we propose a novel data-driven, fast, high-performance, and interpretable algorithm for end-to-end training of SNNs inspired by the SWIM algorithm for RFM-ANNs, which we coin S-SWIM. We provide a thorough theoretical discussion and supplementary numerical experiments showing that S-SWIM can reach high accuracies on time series forecasting as a standalone strategy and serve as an effective initialisation strategy before gradient-based training. Additional ablation studies show that our proposed method performs better than random sampling of network weights.         ",
    "url": "https://arxiv.org/abs/2510.01012",
    "authors": [
      "Maximilian Gollwitzer",
      "Felix Dietrich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2510.01014",
    "title": "Towards Adversarial Training under Hyperspectral Images",
    "abstract": "           Recent studies have revealed that hyperspectral classification models based on deep learning are highly vulnerable to adversarial attacks, which pose significant security risks. Although several approaches have attempted to enhance adversarial robustness by modifying network architectures, these methods often rely on customized designs that limit scalability and fail to defend effectively against strong attacks. To address these challenges, we introduce adversarial training to the hyperspectral domain, which is widely regarded as one of the most effective defenses against adversarial attacks. Through extensive empirical analyses, we demonstrate that while adversarial training does enhance robustness across various models and datasets, hyperspectral data introduces unique challenges not seen in RGB images. Specifically, we find that adversarial noise and the non-smooth nature of adversarial examples can distort or eliminate important spectral semantic information. To mitigate this issue, we employ data augmentation techniques and propose a novel hyperspectral adversarial training method, termed AT-RA. By increasing the diversity of spectral information and ensuring spatial smoothness, AT-RA preserves and corrects spectral semantics in hyperspectral images. Experimental results show that AT-RA improves adversarial robustness by 21.34% against AutoAttack and 18.78% against PGD-50 while boosting benign accuracy by 2.68%.         ",
    "url": "https://arxiv.org/abs/2510.01014",
    "authors": [
      "Weihua Zhang",
      "Chengze Jiang",
      "Jie Gui",
      "Lu Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.01022",
    "title": "Equivariant Geometric Scattering Networks via Vector Diffusion Wavelets",
    "abstract": "           We introduce a novel version of the geometric scattering transform for geometric graphs containing scalar and vector node features. This new scattering transform has desirable symmetries with respect to rigid-body roto-translations (i.e., $SE(3)$-equivariance) and may be incorporated into a geometric GNN framework. We empirically show that our equivariant scattering-based GNN achieves comparable performance to other equivariant message-passing-based GNNs at a fraction of the parameter count.         ",
    "url": "https://arxiv.org/abs/2510.01022",
    "authors": [
      "David R. Johnson",
      "Rishabh Anand",
      "Smita Krishnaswamy",
      "Michael Perlmutter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2510.01038",
    "title": "Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI",
    "abstract": "           Black-box explainability methods are popular tools for explaining the decisions of image classifiers. A major drawback of these tools is their reliance on mutants obtained by occluding parts of the input, leading to out-of-distribution images. This raises doubts about the quality of the explanations. Moreover, choosing an appropriate occlusion value often requires domain knowledge. In this paper we introduce a novel forward-pass paradigm Activation-Deactivation (AD), which removes the effects of occluded input features from the model's decision-making by switching off the parts of the model that correspond to the occlusions. We introduce ConvAD, a drop-in mechanism that can be easily added to any trained Convolutional Neural Network (CNN), and which implements the AD paradigm. This leads to more robust explanations without any additional training or fine-tuning. We prove that the ConvAD mechanism does not change the decision-making process of the network. We provide experimental evaluation across several datasets and model architectures. We compare the quality of AD-explanations with explanations achieved using a set of masking values, using the proxies of robustness, size, and confidence drop-off. We observe a consistent improvement in robustness of AD explanations (up to 62.5%) compared to explanations obtained with occlusions, demonstrating that ConvAD extracts more robust explanations without the need for domain knowledge.         ",
    "url": "https://arxiv.org/abs/2510.01038",
    "authors": [
      "Akchunya Chanchal",
      "David A. Kelly",
      "Hana Chockler"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.01049",
    "title": "KeySG: Hierarchical Keyframe-Based 3D Scene Graphs",
    "abstract": "           In recent years, 3D scene graphs have emerged as a powerful world representation, offering both geometric accuracy and semantic richness. Combining 3D scene graphs with large language models enables robots to reason, plan, and navigate in complex human-centered environments. However, current approaches for constructing 3D scene graphs are semantically limited to a predefined set of relationships, and their serialization in large environments can easily exceed an LLM's context window. We introduce KeySG, a framework that represents 3D scenes as a hierarchical graph consisting of floors, rooms, objects, and functional elements, where nodes are augmented with multi-modal information extracted from keyframes selected to optimize geometric and visual coverage. The keyframes allow us to efficiently leverage VLM to extract scene information, alleviating the need to explicitly model relationship edges between objects, enabling more general, task-agnostic reasoning and planning. Our approach can process complex and ambiguous queries while mitigating the scalability issues associated with large scene graphs by utilizing a hierarchical retrieval-augmented generation (RAG) pipeline to extract relevant context from the graph. Evaluated across four distinct benchmarks -- including 3D object segmentation and complex query retrieval -- KeySG outperforms prior approaches on most metrics, demonstrating its superior semantic richness and efficiency.         ",
    "url": "https://arxiv.org/abs/2510.01049",
    "authors": [
      "Abdelrhman Werby",
      "Dennis Rotondi",
      "Fabio Scaparro",
      "Kai O. Arras"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2510.01077",
    "title": "CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code",
    "abstract": "           Large Language Models (LLMs) are widely used in software development tasks nowadays. Unlike reusing code taken from the Web, for LLMs' generated code, developers are concerned about its lack of trustworthiness and possible copyright or licensing violations, due to the lack of code provenance information. This paper proposes CodeGenLink, a GitHub CoPilot extension for Visual Studio Code aimed at (i) suggesting links containing code very similar to automatically generated code, and (ii) whenever possible, indicating the license of the likely origin of the code. CodeGenLink retrieves candidate links by combining LLMs with their web search features and then performs similarity analysis between the generated and retrieved code. Preliminary results show that CodeGenLink effectively filters unrelated links via similarity analysis and provides licensing information when available. Tool URL: this https URL Tool Video: this https URL ",
    "url": "https://arxiv.org/abs/2510.01077",
    "authors": [
      "Daniele Bifolco",
      "Guido Annicchiarico",
      "Pierluigi Barbiero",
      "Massimiliano Di Penta",
      "Fiorella Zampetti"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.01105",
    "title": "Geometric Properties of Neural Multivariate Regression",
    "abstract": "           Neural multivariate regression underpins a wide range of domains such as control, robotics, and finance, yet the geometry of its learned representations remains poorly characterized. While neural collapse has been shown to benefit generalization in classification, we find that analogous collapse in regression consistently degrades performance. To explain this contrast, we analyze models through the lens of intrinsic dimension. Across control tasks and synthetic datasets, we estimate the intrinsic dimension of last-layer features (ID_H) and compare it with that of the regression targets (ID_Y). Collapsed models exhibit ID_H < ID_Y, leading to over-compression and poor generalization, whereas non-collapsed models typically maintain ID_H > ID_Y. For the non-collapsed models, performance with respect to ID_H depends on the data quantity and noise levels. From these observations, we identify two regimes (over-compressed and under-compressed) that determine when expanding or reducing feature dimensionality improves performance. Our results provide new geometric insights into neural regression and suggest practical strategies for enhancing generalization.         ",
    "url": "https://arxiv.org/abs/2510.01105",
    "authors": [
      "George Andriopoulos",
      "Zixuan Dong",
      "Bimarsha Adhikari",
      "Keith Ross"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.01107",
    "title": "Perfect Fractional Matchings in Bipartite Graphs Via Proportional Allocations",
    "abstract": "           Given a bipartite graph that has a perfect matching, a prefect proportional allocation is an assignment of positive weights to the nodes of the right partition so that every left node is fractionally assigned to its neighbors in proportion to their weights, and these assignments define a fractional perfect matching. We prove that a bipartite graph has a perfect proportional allocation if and only if it is matching covered, by using a classical result on matrix scaling. We also present an extension of this result to provide simple proportional allocations in non-matching-covered bipartite graphs.         ",
    "url": "https://arxiv.org/abs/2510.01107",
    "authors": [
      "Daniel Hathcock",
      "R. Ravi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2510.01111",
    "title": "Augmenting LLMs for General Time Series Understanding and Prediction",
    "abstract": "           Time series data is fundamental to decision-making in many crucial domains including healthcare, finance, and environmental science. However, analyzing this data often requires incorporating unstructured contextual information, answering domain-specific questions, and generating natural language explanations -- capabilities that traditional time series models lack due to their inability to process text. While Large Language Models (LLMs) excel at contextual reasoning and knowledge integration, they struggle with numerical time series due to inefficient text-based representations and limited exposure to temporal data during pretraining. We address this gap by augmenting an LLM with specialized time series perception through a patch-based encoder-decoder architecture. We train this Time Series-augmented LLM (TsLLM) on a large corpus of over 2 million interleaved time series and text examples spanning diverse analysis tasks: forecasting with contextual information, time series question-answering, pattern explanation, classification with natural language outputs, and report generation. This training enables TsLLM to leverage both its language understanding and newly acquired temporal reasoning capabilities. While not designed to surpass specialized models on traditional benchmarks, TsLLM demonstrates strong performance on tasks requiring the integration of time series analysis with natural language -- capabilities that existing approaches cannot provide. Our work establishes a new paradigm for time series analysis that bridges numerical computation and natural language understanding, democratizing access to sophisticated temporal reasoning through natural language interaction.         ",
    "url": "https://arxiv.org/abs/2510.01111",
    "authors": [
      "Felix Parker",
      "Nimeesha Chan",
      "Chi Zhang",
      "Kimia Ghobadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.01113",
    "title": "Privacy Preserved Federated Learning with Attention-Based Aggregation for Biometric Recognition",
    "abstract": "           Because biometric data is sensitive, centralized training poses a privacy risk, even though biometric recognition is essential for contemporary applications. Federated learning (FL), which permits decentralized training, provides a privacy-preserving substitute. Conventional FL, however, has trouble with interpretability and heterogeneous data (non-IID). In order to handle non-IID biometric data, this framework adds an attention mechanism at the central server that weights local model updates according to their significance. Differential privacy and secure update protocols safeguard data while preserving accuracy. The A3-FL framework is evaluated in this study using FVC2004 fingerprint data, with each client's features extracted using a Siamese Convolutional Neural Network (Siamese-CNN). By dynamically modifying client contributions, the attention mechanism increases the accuracy of the global this http URL accuracy, convergence speed, and robustness of the A3-FL framework are superior to those of standard FL (FedAvg) and static baselines, according to experimental evaluations using fingerprint data (FVC2004). The accuracy of the attention-based approach was 0.8413, while FedAvg, Local-only, and Centralized approaches were 0.8164, 0.7664, and 0.7997, respectively. Accuracy stayed high at 0.8330 even with differential privacy. A scalable and privacy-sensitive biometric system for secure and effective recognition in dispersed environments is presented in this work.         ",
    "url": "https://arxiv.org/abs/2510.01113",
    "authors": [
      "Kassahun Azezew",
      "Minyechil Alehegn",
      "Tsega Asresa",
      "Bitew Mekuria",
      "Tizazu Bayh",
      "Ayenew Kassie",
      "Amsalu Tesema",
      "Animut Embiyale"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.01115",
    "title": "Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis",
    "abstract": "           Large Language Models (LLMs) struggle with the complex, multi-modal, and network-native data underlying financial risk. Standard Retrieval-Augmented Generation (RAG) oversimplifies relationships, while specialist models are costly and static. We address this gap with an LLM-centric agent framework for supply chain risk analysis. Our core contribution is to exploit the inherent duality between networks and knowledge graphs (KG). We treat the supply chain network as a KG, allowing us to use structural network science principles for retrieval. A graph traverser, guided by network centrality scores, efficiently extracts the most economically salient risk paths. An agentic architecture orchestrates this graph retrieval alongside data from numerical factor tables and news streams. Crucially, it employs novel ``context shells'' -- descriptive templates that embed raw figures in natural language -- to make quantitative data fully intelligible to the LLM. This lightweight approach enables the model to generate concise, explainable, and context-rich risk narratives in real-time without costly fine-tuning or a dedicated graph database.         ",
    "url": "https://arxiv.org/abs/2510.01115",
    "authors": [
      "Evan Heus",
      "Rick Bookstaber",
      "Dhruv Sharma"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Theoretical Economics (econ.TH)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2510.01136",
    "title": "TabINR: An Implicit Neural Representation Framework for Tabular Data Imputation",
    "abstract": "           Tabular data builds the basis for a wide range of applications, yet real-world datasets are frequently incomplete due to collection errors, privacy restrictions, or sensor failures. As missing values degrade the performance or hinder the applicability of downstream models, and while simple imputing strategies tend to introduce bias or distort the underlying data distribution, we require imputers that provide high-quality imputations, are robust across dataset sizes and yield fast inference. We therefore introduce TabINR, an auto-decoder based Implicit Neural Representation (INR) framework that models tables as neural functions. Building on recent advances in generalizable INRs, we introduce learnable row and feature embeddings that effectively deal with the discrete structure of tabular data and can be inferred from partial observations, enabling instance adaptive imputations without modifying the trained model. We evaluate our framework across a diverse range of twelve real-world datasets and multiple missingness mechanisms, demonstrating consistently strong imputation accuracy, mostly matching or outperforming classical (KNN, MICE, MissForest) and deep learning based models (GAIN, ReMasker), with the clearest gains on high-dimensional datasets.         ",
    "url": "https://arxiv.org/abs/2510.01136",
    "authors": [
      "Vincent Ochs",
      "Florentin Bieder",
      "Sidaty el Hadramy",
      "Paul Friedrich",
      "Stephanie Taha-Mehlitz",
      "Anas Taha",
      "Philippe C. Cattin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.01144",
    "title": "Partial Resilient Leader-Follower Consensus in Time-Varying Graphs",
    "abstract": "           This work studies resilient leader-follower consensus with a bounded number of adversaries. Existing approaches typically require robustness conditions of the entire network to guarantee resilient consensus. However, the behavior of such systems when these conditions are not fully met remains unexplored. To address this gap, we introduce the notion of partial leader-follower consensus, in which a subset of non-adversarial followers successfully tracks the leader's reference state despite insufficient robustness. We propose a novel distributed algorithm - the Bootstrap Percolation and Mean Subsequence Reduced (BP-MSR) algorithm - and establish sufficient conditions for individual followers to achieve consensus via the BP-MSR algorithm in arbitrary time-varying graphs. We validate our findings through simulations, demonstrating that our method guarantees partial leader-follower consensus, even when standard resilient consensus algorithms fail.         ",
    "url": "https://arxiv.org/abs/2510.01144",
    "authors": [
      "Haejoon Lee",
      "Dimitra Panagou"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.01153",
    "title": "Neural Hamilton--Jacobi Characteristic Flows for Optimal Transport",
    "abstract": "           We present a novel framework for solving optimal transport (OT) problems based on the Hamilton--Jacobi (HJ) equation, whose viscosity solution uniquely characterizes the OT map. By leveraging the method of characteristics, we derive closed-form, bidirectional transport maps, thereby eliminating the need for numerical integration. The proposed method adopts a pure minimization framework: a single neural network is trained with a loss function derived from the method of characteristics of the HJ equation. This design guarantees convergence to the optimal map while eliminating adversarial training stages, thereby substantially reducing computational complexity. Furthermore, the framework naturally extends to a wide class of cost functions and supports class-conditional transport. Extensive experiments on diverse datasets demonstrate the accuracy, scalability, and efficiency of the proposed method, establishing it as a principled and versatile tool for OT applications with provable optimality.         ",
    "url": "https://arxiv.org/abs/2510.01153",
    "authors": [
      "Yesom Park",
      "Shu Liu",
      "Mo Zhou",
      "Stanley Osher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2510.01157",
    "title": "Backdoor Attacks Against Speech Language Models",
    "abstract": "           Large Language Models (LLMs) and their multimodal extensions are becoming increasingly popular. One common approach to enable multimodality is to cascade domain-specific encoders with an LLM, making the resulting model inherit vulnerabilities from all of its components. In this work, we present the first systematic study of audio backdoor attacks against speech language models. We demonstrate its effectiveness across four speech encoders and three datasets, covering four tasks: automatic speech recognition (ASR), speech emotion recognition, and gender and age prediction. The attack consistently achieves high success rates, ranging from 90.76% to 99.41%. To better understand how backdoors propagate, we conduct a component-wise analysis to identify the most vulnerable stages of the pipeline. Finally, we propose a fine-tuning-based defense that mitigates the threat of poisoned pretrained encoders.         ",
    "url": "https://arxiv.org/abs/2510.01157",
    "authors": [
      "Alexandrine Fortier",
      "Thomas Thebaud",
      "Jes\u00fas Villalba",
      "Najim Dehak",
      "Patrick Cardinal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2510.01163",
    "title": "How Does the Pretraining Distribution Shape In-Context Learning? Task Selection, Generalization, and Robustness",
    "abstract": "           The emergence of in-context learning (ICL) in large language models (LLMs) remains poorly understood despite its consistent effectiveness, enabling models to adapt to new tasks from only a handful of examples. To clarify and improve these capabilities, we characterize how the statistical properties of the pretraining distribution (e.g., tail behavior, coverage) shape ICL on numerical tasks. We develop a theoretical framework that unifies task selection and generalization, extending and sharpening earlier results, and show how distributional properties govern sample efficiency, task retrieval, and robustness. To this end, we generalize Bayesian posterior consistency and concentration results to heavy-tailed priors and dependent sequences, better reflecting the structure of LLM pretraining data. We then empirically study how ICL performance varies with the pretraining distribution on challenging tasks such as stochastic differential equations and stochastic processes with memory. Together, these findings suggest that controlling key statistical properties of the pretraining distribution is essential for building ICL-capable and reliable LLMs.         ",
    "url": "https://arxiv.org/abs/2510.01163",
    "authors": [
      "Wa\u00efss Azizian",
      "Ali Hasan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2510.01164",
    "title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare",
    "abstract": "           Large language models (LLMs) are increasingly entrusted with high-stakes decisions that affect human welfare. However, the principles and values that guide these models when distributing scarce societal resources remain largely unexamined. To address this, we introduce the Social Welfare Function (SWF) Benchmark, a dynamic simulation environment where an LLM acts as a sovereign allocator, distributing tasks to a heterogeneous community of recipients. The benchmark is designed to create a persistent trade-off between maximizing collective efficiency (measured by Return on Investment) and ensuring distributive fairness (measured by the Gini coefficient). We evaluate 20 state-of-the-art LLMs and present the first leaderboard for social welfare allocation. Our findings reveal three key insights: (i) A model's general conversational ability, as measured by popular leaderboards, is a poor predictor of its allocation skill. (ii) Most LLMs exhibit a strong default utilitarian orientation, prioritizing group productivity at the expense of severe inequality. (iii) Allocation strategies are highly vulnerable, easily perturbed by output-length constraints and social-influence framing. These results highlight the risks of deploying current LLMs as societal decision-makers and underscore the need for specialized benchmarks and targeted alignment for AI governance.         ",
    "url": "https://arxiv.org/abs/2510.01164",
    "authors": [
      "Zhengliang Shi",
      "Ruotian Ma",
      "Jen-tse Huang",
      "Xinbei Ma",
      "Xingyu Chen",
      "Mengru Wang",
      "Qu Yang",
      "Yue Wang",
      "Fanghua Ye",
      "Ziyang Chen",
      "Shanyi Wang",
      "Cixing Li",
      "Wenxuan Wang",
      "Zhaopeng Tu",
      "Xiaolong Li",
      "Zhaochun Ren",
      "Linus"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2510.01176",
    "title": "Audio Driven Real-Time Facial Animation for Social Telepresence",
    "abstract": "           We present an audio-driven real-time system for animating photorealistic 3D facial avatars with minimal latency, designed for social interactions in virtual reality for anyone. Central to our approach is an encoder model that transforms audio signals into latent facial expression sequences in real time, which are then decoded as photorealistic 3D facial avatars. Leveraging the generative capabilities of diffusion models, we capture the rich spectrum of facial expressions necessary for natural communication while achieving real-time performance (<15ms GPU time). Our novel architecture minimizes latency through two key innovations: an online transformer that eliminates dependency on future inputs and a distillation pipeline that accelerates iterative denoising into a single step. We further address critical design challenges in live scenarios for processing continuous audio signals frame-by-frame while maintaining consistent animation quality. The versatility of our framework extends to multimodal applications, including semantic modalities such as emotion conditions and multimodal sensors with head-mounted eye cameras on VR headsets. Experimental results demonstrate significant improvements in facial animation accuracy over existing offline state-of-the-art baselines, achieving 100 to 1000 times faster inference speed. We validate our approach through live VR demonstrations and across various scenarios such as multilingual speeches.         ",
    "url": "https://arxiv.org/abs/2510.01176",
    "authors": [
      "Jiye Lee",
      "Chenghui Li",
      "Linh Tran",
      "Shih-En Wei",
      "Jason Saragih",
      "Alexander Richard",
      "Hanbyul Joo",
      "Shaojie Bai"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2510.00035",
    "title": "Deep Learning-Based Pneumonia Detection from Chest X-ray Images: A CNN Approach with Performance Analysis and Clinical Implications",
    "abstract": "           Deep learning integration into medical imaging systems has transformed disease detection and diagnosis processes with a focus on pneumonia identification. The study introduces an intricate deep learning system using Convolutional Neural Networks for automated pneumonia detection from chest Xray images which boosts diagnostic precision and speed. The proposed CNN architecture integrates sophisticated methods including separable convolutions along with batch normalization and dropout regularization to enhance feature extraction while reducing overfitting. Through the application of data augmentation techniques and adaptive learning rate strategies the model underwent training on an extensive collection of chest Xray images to enhance its generalization capabilities. A convoluted array of evaluation metrics such as accuracy, precision, recall, and F1 score collectively verify the model exceptional performance by recording an accuracy rate of 91. This study tackles critical clinical implementation obstacles such as data privacy protection, model interpretability, and integration with current healthcare systems beyond just model performance. This approach introduces a critical advancement by integrating medical ontologies with semantic technology to improve diagnostic accuracy. The study enhances AI diagnostic reliability by integrating machine learning outputs with structured medical knowledge frameworks to boost interpretability. The findings demonstrate AI powered healthcare tools as a scalable efficient pneumonia detection solution. This study advances AI integration into clinical settings by developing more precise automated diagnostic methods that deliver consistent medical imaging results.         ",
    "url": "https://arxiv.org/abs/2510.00035",
    "authors": [
      "P K Dutta",
      "Anushri Chowdhury",
      "Anouska Bhattacharyya",
      "Shakya Chakraborty",
      "Sujatra Dey"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.00051",
    "title": "Latent Representation Learning from 3D Brain MRI for Interpretable Prediction in Multiple Sclerosis",
    "abstract": "           We present InfoVAE-Med3D, a latent-representation learning approach for 3D brain MRI that targets interpretable biomarkers of cognitive decline. Standard statistical models and shallow machine learning often lack power, while most deep learning methods behave as black boxes. Our method extends InfoVAE to explicitly maximize mutual information between images and latent variables, producing compact, structured embeddings that retain clinically meaningful content. We evaluate on two cohorts: a large healthy-control dataset (n=6527) with chronological age, and a clinical multiple sclerosis dataset from Charles University in Prague (n=904) with age and Symbol Digit Modalities Test (SDMT) scores. The learned latents support accurate brain-age and SDMT regression, preserve key medical attributes, and form intuitive clusters that aid interpretation. Across reconstruction and downstream prediction tasks, InfoVAE-Med3D consistently outperforms other VAE variants, indicating stronger information capture in the embedding space. By uniting predictive performance with interpretability, InfoVAE-Med3D offers a practical path toward MRI-based biomarkers and more transparent analysis of cognitive deterioration in neurological disease.         ",
    "url": "https://arxiv.org/abs/2510.00051",
    "authors": [
      "Trinh Ngoc Huynh",
      "Nguyen Duc Kien",
      "Nguyen Hai Anh",
      "Dinh Tran Hiep",
      "Manuela Vaneckova",
      "Tomas Uher",
      "Jeroen Van Schependom",
      "Stijn Denissen",
      "Tran Quoc Long",
      "Nguyen Linh Trung",
      "Guy Nagels"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2510.00053",
    "title": "DPsurv: Dual-Prototype Evidential Fusion for Uncertainty-Aware and Interpretable Whole-Slide Image Survival Prediction",
    "abstract": "           Pathology whole-slide images (WSIs) are widely used for cancer survival analysis because of their comprehensive histopathological information at both cellular and tissue levels, enabling quantitative, large-scale, and prognostically rich tumor feature analysis. However, most existing methods in WSI survival analysis struggle with limited interpretability and often overlook predictive uncertainty in heterogeneous slide images. In this paper, we propose DPsurv, a dual-prototype whole-slide image evidential fusion network that outputs uncertainty-aware survival intervals, while enabling interpretation of predictions through patch prototype assignment maps, component prototypes, and component-wise relative risk aggregation. Experiments on five publicly available datasets achieve the highest mean concordance index and the lowest mean integrated Brier score, validating the effectiveness and reliability of DPsurv. The interpretation of prediction results provides transparency at the feature, reasoning, and decision levels, thereby enhancing the trustworthiness and interpretability of DPsurv.         ",
    "url": "https://arxiv.org/abs/2510.00053",
    "authors": [
      "Yucheng Xing",
      "Ling Huang",
      "Jingying Ma",
      "Ruping Hong",
      "Jiangdong Qiu",
      "Pei Liu",
      "Kai He",
      "Huazhu Fu",
      "Mengling Feng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00087",
    "title": "Revealing the temporal dynamics of antibiotic anomalies in the infant gut microbiome with neural jump ODEs",
    "abstract": "           Detecting anomalies in irregularly sampled multi-variate time-series is challenging, especially in data-scarce settings. Here we introduce an anomaly detection framework for irregularly sampled time-series that leverages neural jump ordinary differential equations (NJODEs). The method infers conditional mean and variance trajectories in a fully path dependent way and computes anomaly scores. On synthetic data containing jump, drift, diffusion, and noise anomalies, the framework accurately identifies diverse deviations. Applied to infant gut microbiome trajectories, it delineates the magnitude and persistence of antibiotic-induced disruptions: revealing prolonged anomalies after second antibiotic courses, extended duration treatments, and exposures during the second year of life. We further demonstrate the predictive capabilities of the inferred anomaly scores in accurately predicting antibiotic events and outperforming diversity-based baselines. Our approach accommodates unevenly spaced longitudinal observations, adjusts for static and dynamic covariates, and provides a foundation for inferring microbial anomalies induced by perturbations, offering a translational opportunity to optimize intervention regimens by minimizing microbial disruptions.         ",
    "url": "https://arxiv.org/abs/2510.00087",
    "authors": [
      "Anja Adamov",
      "Markus Chardonnet",
      "Florian Krach",
      "Jakob Heiss",
      "Josef Teichmann",
      "Nicholas A. Bokulich"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2510.00203",
    "title": "A Review of Software for Designing and Operating Quantum Networks",
    "abstract": "           Quantum network protocol development is crucial to realizing a production-grade network that can support distributed sensing, secure communication, and utility-scale quantum computation. However, the transition from laboratory demonstration to deployable networks requires software implementations of architectures and protocols tailored to the unique constraints of quantum systems. This paper reviews the current state of software implementations for quantum networks, organized around the three-plane abstraction of infrastructure, logical, and control/service planes. We cover software for both designing quantum network protocols (e.g., SeQUeNCe, QuISP, and NetSquid) and operating them, with a focus on essential control/service plane functions such as entanglement, topology, and resource management, in a proposed taxonomy. Our review highlights a persistent gap between theoretical protocol proposals and their realization in simulators or testbeds, particularly in dynamic topology and network management. We conclude by outlining open challenges and proposing a roadmap for developing scalable software architectures to enable hybrid, large-scale quantum networks.         ",
    "url": "https://arxiv.org/abs/2510.00203",
    "authors": [
      "Robert J. Hayek",
      "Joaquin Chung",
      "Rajkumar Kettimuthu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.00218",
    "title": "Descriptor:: Extended-Length Audio Dataset for Synthetic Voice Detection and Speaker Recognition (ELAD-SVDSR)",
    "abstract": "           This paper introduces the Extended Length Audio Dataset for Synthetic Voice Detection and Speaker Recognition (ELAD SVDSR), a resource specifically designed to facilitate the creation of high quality deepfakes and support the development of detection systems trained against them. The dataset comprises 45 minute audio recordings from 36 participants, each reading various newspaper articles recorded under controlled conditions and captured via five microphones of differing quality. By focusing on extended duration audio, ELAD SVDSR captures a richer range of speech attributes such as pitch contours, intonation patterns, and nuanced delivery enabling models to generate more realistic and coherent synthetic voices. In turn, this approach allows for the creation of robust deepfakes that can serve as challenging examples in datasets used to train and evaluate synthetic voice detection methods. As part of this effort, 20 deepfake voices have already been created and added to the dataset to showcase its potential. Anonymized metadata accompanies the dataset on speaker demographics. ELAD SVDSR is expected to spur significant advancements in audio forensics, biometric security, and voice authentication systems.         ",
    "url": "https://arxiv.org/abs/2510.00218",
    "authors": [
      "Rahul Vijaykumar",
      "Ajan Ahmed",
      "John Parker",
      "Dinesh Pendyala",
      "Aidan Collins",
      "Stephanie Schuckers",
      "Masudul H. Imtiaz"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2510.00238",
    "title": "Room Impulse Response Synthesis via Differentiable Feedback Delay Networks for Efficient Spatial Audio Rendering",
    "abstract": "           We introduce a computationally efficient and tunable feedback delay network (FDN) architecture for real-time room impulse response (RIR) rendering that addresses the computational and latency challenges inherent in traditional convolution and Fourier transform based methods. Our approach directly optimizes FDN parameters to match target RIR acoustic and psychoacoustic metrics such as clarity and definition through novel differentiable programming-based optimization. Our method enables dynamic, real-time adjustments of room impulse responses that accommodates listener and source movement. When combined with previous work on representation of head-related impulse responses via infinite impulse responses, an efficient rendering of auditory objects is possible when the HRIR and RIR are known. Our method produces renderings with quality similar to convolution with long binaural room impulse response (BRIR) filters, but at a fraction of the computational cost.         ",
    "url": "https://arxiv.org/abs/2510.00238",
    "authors": [
      "Armin Gerami",
      "Ramani Duraiswami"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2510.00282",
    "title": "Electron neural closure for turbulent magnetosheath simulations: energy channels",
    "abstract": "           In this work, we introduce a non-local five-moment electron pressure tensor closure parametrized by a Fully Convolutional Neural Network (FCNN). Electron pressure plays an important role in generalized Ohm's law, competing with electron inertia. This model is used in the development of a surrogate model for a fully kinetic energy-conserving semi-implicit Particle-in-Cell simulation of decaying magnetosheath turbulence. We achieve this by training FCNN on a representative set of simulations with a smaller number of particles per cell and showing that our results generalise to a simulation with a large number of particles per cell. We evaluate the statistical properties of the learned equation of state, with a focus on pressure-strain interaction, which is crucial for understanding energy channels in turbulent plasmas. The resulting equation of state learned via FCNN significantly outperforms local closures, such as those learned by Multi-Layer Perceptron (MLP) or double adiabatic expressions. We report that the overall spatial distribution of pressure-strain and its conditional averages are reconstructed well. However, some small-scale features are missed, especially for the off-diagonal components of the pressure tensor. Nevertheless, the results are substantially improved with more training data, indicating favorable scaling and potential for improvement, which will be addressed in future work.         ",
    "url": "https://arxiv.org/abs/2510.00282",
    "authors": [
      "George Miloshevich",
      "Luka Vranckx",
      "Felipe Nathan de Oliveira Lopes",
      "Pietro Dazzi",
      "Giuseppe Arr\u00f2",
      "Giovanni Lapenta"
    ],
    "subjectives": [
      "Plasma Physics (physics.plasm-ph)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2510.00325",
    "title": "QSearchNet: A Quantum Walk Search Framework for Link Prediction",
    "abstract": "           Link prediction is one of the fundamental problems in graph theory, critical for understanding and forecasting the evolution of complex systems like social and biological networks. While classical heuristics capture certain aspects of graph topology, they often struggle to optimally integrate local and global structural information or adapt to complex dependencies. Quantum computing offers a powerful alternative by leveraging superposition for simultaneous multi-path exploration and interference-driven integration of both local and global graph features. In this work, we introduce QSearchNet, a quantum-inspired framework based on Discrete-Time Quantum Walk (DTQW) dynamics and Grover's amplitude amplification. QSearchNet simulates a topology-aware quantum evolution to propagate amplitudes across multiple nodes simultaneously. By aligning interference patterns through quantum reflection and oracle-like phase-flip operation, it adaptively prioritizes multi-hop dependencies and amplifies structurally relevant paths corresponding to potential connections. Experiments on diverse real-world networks demonstrate competitive performance, particularly with hard negative samples under realistic evaluation conditions.         ",
    "url": "https://arxiv.org/abs/2510.00325",
    "authors": [
      "Priyank Dubey"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2510.00334",
    "title": "Structural Refinement of Bayesian Networks for Efficient Model Parameterisation",
    "abstract": "           Many Bayesian network modelling applications suffer from the issue of data scarcity. Hence the use of expert judgement often becomes necessary to determine the parameters of the conditional probability tables (CPTs) throughout the network. There are usually a prohibitively large number of these parameters to determine, even when complementing any available data with expert judgements. To address this challenge, a number of CPT approximation methods have been developed that reduce the quantity and complexity of parameters needing to be determined to fully parameterise a Bayesian network. This paper provides a review of a variety of structural refinement methods that can be used in practice to efficiently approximate a CPT within a Bayesian network. We not only introduce and discuss the intrinsic properties and requirements of each method, but we evaluate each method through a worked example on a Bayesian network model of cardiovascular risk assessment. We conclude with practical guidance to help Bayesian network practitioners choose an alternative approach when direct parameterisation of a CPT is infeasible.         ",
    "url": "https://arxiv.org/abs/2510.00334",
    "authors": [
      "Kieran Drury",
      "Martine J. Barons",
      "Jim Q. Smith"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2510.00367",
    "title": "CINDES: Classification induced neural density estimator and simulator",
    "abstract": "           Neural network-based methods for (un)conditional density estimation have recently gained substantial attention, as various neural density estimators have outperformed classical approaches in real-data experiments. Despite these empirical successes, implementation can be challenging due to the need to ensure non-negativity and unit-mass constraints, and theoretical understanding remains limited. In particular, it is unclear whether such estimators can adaptively achieve faster convergence rates when the underlying density exhibits a low-dimensional structure. This paper addresses these gaps by proposing a structure-agnostic neural density estimator that is (i) straightforward to implement and (ii) provably adaptive, attaining faster rates when the true density admits a low-dimensional composition structure. Another key contribution of our work is to show that the proposed estimator integrates naturally into generative sampling pipelines, most notably score-based diffusion models, where it achieves provably faster convergence when the underlying density is structured. We validate its performance through extensive simulations and a real-data application.         ",
    "url": "https://arxiv.org/abs/2510.00367",
    "authors": [
      "Dehao Dai",
      "Jianqing Fan",
      "Yihong Gu",
      "Debarghya Mukherjee"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2510.00463",
    "title": "On the Adversarial Robustness of Learning-based Conformal Novelty Detection",
    "abstract": "           This paper studies the adversarial robustness of conformal novelty detection. In particular, we focus on AdaDetect, a powerful learning-based framework for novelty detection with finite-sample false discovery rate (FDR) control. While AdaDetect provides rigorous statistical guarantees under benign conditions, its behavior under adversarial perturbations remains unexplored. We first formulate an oracle attack setting that quantifies the worst-case degradation of FDR, deriving an upper bound that characterizes the statistical cost of attacks. This idealized formulation directly motivates a practical and effective attack scheme that only requires query access to AdaDetect's output labels. Coupling these formulations with two popular and complementary black-box adversarial algorithms, we systematically evaluate the vulnerability of AdaDetect on synthetic and real-world datasets. Our results show that adversarial perturbations can significantly increase the FDR while maintaining high detection power, exposing fundamental limitations of current error-controlled novelty detection methods and motivating the development of more robust alternatives.         ",
    "url": "https://arxiv.org/abs/2510.00463",
    "authors": [
      "Daofu Zhang",
      "Mehrdad Pournaderi",
      "Hanne M. Clifford",
      "Yu Xiang",
      "Pramod K. Varshney"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2510.00498",
    "title": "Emergence of robust looming selectivity via coordinated inhibitory neural computations",
    "abstract": "           In the locust's lobula giant movement detector neural pathways, four categories of inhibition, i.e., global inhibition, self-inhibition, lateral inhibition, and feed-forward inhibition, have been functionally explored in the context of looming perception. However, their combined influence on shaping selectivity to looming motion remains unclear. Driven by recent physiological advancements, this paper offers new insights into the roles of these inhibitory mechanisms at multiple levels and scales in simulations, refining the specific selectivity for responding only to objects approaching the eyes while remaining unresponsive to other forms of movement. Within a feed-forward, multi-layer neural network framework, global inhibition, lateral inhibition, self-inhibition, and feed-forward inhibition are integrated. Global inhibition acts as an immediate feedback mechanism, normalising light intensities delivered by ommatidia, particularly addressing low-contrast looming. Self-inhibition, modelled numerically for the first time, suppresses translational motion. Lateral inhibition is formed by delayed local excitation spreading across a larger area. Notably, self-inhibition and lateral inhibition are sequential in time and are combined through feed-forward inhibition, which indicates the angular size subtended by moving objects. Together, these inhibitory processes attenuate motion-induced excitation at multiple levels and scales. This research suggests that self-inhibition may act earlier than lateral inhibition to rapidly reduce excitation in situ, thereby suppressing translational motion, and global inhibition can modulate excitation on a finer scale, enhancing selectivity in higher contrast range.         ",
    "url": "https://arxiv.org/abs/2510.00498",
    "authors": [
      "Qinbing Fu",
      "Ziyan Qin"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2510.00512",
    "title": "Adaptive Data-Knowledge Alignment in Genetic Perturbation Prediction",
    "abstract": "           The transcriptional response to genetic perturbation reveals fundamental insights into complex cellular systems. While current approaches have made progress in predicting genetic perturbation responses, they provide limited biological understanding and cannot systematically refine existing knowledge. Overcoming these limitations requires an end-to-end integration of data-driven learning and existing knowledge. However, this integration is challenging due to inconsistencies between data and knowledge bases, such as noise, misannotation, and incompleteness. To address this challenge, we propose ALIGNED (Adaptive aLignment for Inconsistent Genetic kNowledgE and Data), a neuro-symbolic framework based on the Abductive Learning (ABL) paradigm. This end-to-end framework aligns neural and symbolic components and performs systematic knowledge refinement. We introduce a balanced consistency metric to evaluate the predictions' consistency against both data and knowledge. Our results show that ALIGNED outperforms state-of-the-art methods by achieving the highest balanced consistency, while also re-discovering biologically meaningful knowledge. Our work advances beyond existing methods to enable both the transparency and the evolution of mechanistic biological understanding.         ",
    "url": "https://arxiv.org/abs/2510.00512",
    "authors": [
      "Yuanfang Xiang",
      "Lun Ai"
    ],
    "subjectives": [
      "Molecular Networks (q-bio.MN)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.00545",
    "title": "Bayesian Neural Networks for Functional ANOVA model",
    "abstract": "           With the increasing demand for interpretability in machine learning, functional ANOVA decomposition has gained renewed attention as a principled tool for breaking down high-dimensional function into low-dimensional components that reveal the contributions of different variable groups. Recently, Tensor Product Neural Network (TPNN) has been developed and applied as basis functions in the functional ANOVA model, referred to as ANOVA-TPNN. A disadvantage of ANOVA-TPNN, however, is that the components to be estimated must be specified in advance, which makes it difficult to incorporate higher-order TPNNs into the functional ANOVA model due to computational and memory constraints. In this work, we propose Bayesian-TPNN, a Bayesian inference procedure for the functional ANOVA model with TPNN basis functions, enabling the detection of higher-order components with reduced computational cost compared to ANOVA-TPNN. We develop an efficient MCMC algorithm and demonstrate that Bayesian-TPNN performs well by analyzing multiple benchmark datasets. Theoretically, we prove that the posterior of Bayesian-TPNN is consistent.         ",
    "url": "https://arxiv.org/abs/2510.00545",
    "authors": [
      "Seokhun Park",
      "Choeun Kim",
      "Jihu Lee",
      "Yunseop Shin",
      "Insung Kong",
      "Yongdai Kim"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2510.01067",
    "title": "Networked Control and Mean Field Problems Under Diagonal Dominance: Decentralized and Social Optimality",
    "abstract": "           In this article, we employ an input-output approach to expand the study of cooperative multi-agent control and optimization problems characterized by mean-field interactions that admit decentralized and selfish solutions. The setting involves $n$ independent agents that interact solely through a shared cost function, which penalizes deviations of each agent from the group's average collective behavior. Building on our earlier results established for homogeneous agents, we extend the framework to nonidentical agents and show that, under a diagonal dominant interaction of the collective dynamics, with bounded local open-loop dynamics, the optimal controller for $H_\\infty$ and $H_2$ norm minimization remains decentralized and selfish in the limit as the number of agents $n$ grows to infinity.         ",
    "url": "https://arxiv.org/abs/2510.01067",
    "authors": [
      "Vivek Khatana",
      "Duo Wang",
      "Petros Voulgaris",
      "Nicola Elia",
      "Naira Hovakimyan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.01073",
    "title": "Vulnerability Analysis Evaluating Bilevel Optimal Power Flow Approaches for Multiple Load Cases",
    "abstract": "           This work presents two methodologies to enhance vulnerability assessment in power systems using bilevel attacker-defender network interdiction models. First, we introduce a systematic evaluation procedure for comparing different optimal power flow formulations in the lower-level problem. We demonstrate the procedure for a comparison of the widely used DC approximation and a linearized AC optimal power flow model. Second, we propose a novel scoring methodology to identify and prioritize critical attack vectors across diverse load and generation scenarios. Both methodologies go beyond traditional worst-case analysis. Case studies on a SimBench high-voltage test grid show that the DC approach fails to detect a significant portion of critical vulnerabilities. The scoring methodology further demonstrates the dependency of vulnerabilities on the considered load case and time step, highlighting the importance of assessing multiple scenarios and going beyond worst-case solutions. The proposed methodologies enhance power system vulnerability assessment and can support the effective development of robust defense strategies for future power systems.         ",
    "url": "https://arxiv.org/abs/2510.01073",
    "authors": [
      "Eric T\u00f6nges",
      "Martin Braun",
      "Philipp H\u00e4rtel"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2510.01112",
    "title": "The causal structure of galactic astrophysics",
    "abstract": "           Data-driven astrophysics currently relies on the detection and characterisation of correlations between objects' properties, which are then used to test physical theories that make predictions for them. This process fails to utilise information in the data that forms a crucial part of the theories' predictions, namely which variables are directly correlated (as opposed to accidentally correlated through others), the directions of these determinations, and the presence or absence of confounders that correlate variables in the dataset but are themselves absent from it. We propose to recover this information through causal discovery, a well-developed methodology for inferring the causal structure of datasets that is however almost entirely unknown to astrophysics. We develop a causal discovery algorithm suitable for astrophysical datasets and illustrate it on $\\sim$5$\\times10^5$ low-redshift galaxies from the Nasa Sloan Atlas, demonstrating its ability to distinguish physical mechanisms that are degenerate on the basis of correlations alone.         ",
    "url": "https://arxiv.org/abs/2510.01112",
    "authors": [
      "Harry Desmond",
      "Joseph Ramsey"
    ],
    "subjectives": [
      "Astrophysics of Galaxies (astro-ph.GA)",
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2106.04086",
    "title": "Complexity classification of counting graph homomorphisms modulo a prime number",
    "abstract": "           Counting graph homomorphisms and its generalizations such as the Counting Constraint Satisfaction Problem (CSP), its variations, and counting problems in general have been intensively studied since the pioneering work of Valiant. While the complexity of exact counting of graph homomorphisms (Dyer and Greenhill, 2000) and the counting CSP (Bulatov, 2013, and Dyer and Richerby, 2013) is well understood, counting modulo some natural number has attracted considerable interest as well. In their 2015 paper Faben and Jerrum suggested a conjecture stating that counting homomorphisms to a fixed graph H modulo a prime number is hard whenever it is hard to count exactly, unless H has automorphisms of certain kind. In this paper we confirm this conjecture. As a part of this investigation we develop techniques that widen the spectrum of reductions available for modular counting and apply to the general CSP rather than being limited to graph homomorphisms.         ",
    "url": "https://arxiv.org/abs/2106.04086",
    "authors": [
      "Andrei A.Bulatov",
      "Amirhossein Kazeminia"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2302.11354",
    "title": "Learning Dynamic Graph Embeddings with Neural Controlled Differential Equations",
    "abstract": "           This paper focuses on representation learning for dynamic graphs with temporal interactions. A fundamental issue is that both the graph structure and the nodes own their own dynamics, and their blending induces intractable complexity in the temporal evolution over graphs. Drawing inspiration from the recent progress of physical dynamic models in deep neural networks, we propose Graph Neural Controlled Differential Equations (GN-CDEs), a continuous-time framework that jointly models node embeddings and structural dynamics by incorporating a graph enhanced neural network vector field with a time-varying graph path as the control signal. Our framework exhibits several desirable characteristics, including the ability to express dynamics on evolving graphs without piecewise integration, the capability to calibrate trajectories with subsequent data, and robustness to missing observations. Empirical evaluation on a range of dynamic graph representation learning tasks demonstrates the effectiveness of our proposed approach in capturing the complex dynamics of dynamic graphs.         ",
    "url": "https://arxiv.org/abs/2302.11354",
    "authors": [
      "Tiexin Qin",
      "Benjamin Walker",
      "Terry Lyons",
      "Hong Yan",
      "Haoliang Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2309.17401",
    "title": "Adversarial Attacks to Latent Representations of Distributed Neural Networks in Split Computing",
    "abstract": "           Distributed deep neural networks (DNNs) have been shown to reduce the computational burden of mobile devices and decrease the end-to-end inference latency in edge computing scenarios. While distributed DNNs have been studied, to the best of our knowledge, the resilience of distributed DNNs to adversarial action remains an open problem. In this paper, we fill the existing research gap by rigorously analyzing the robustness of distributed DNNs against adversarial action. We cast this problem in the context of information theory and rigorously proved that (i) the compressed latent dimension improves the robustness but also affect task-oriented performance; and (ii) the deeper splitting point enhances the robustness but also increases the computational burden. These two trade-offs provide a novel perspective to design robust distributed DNN. To test our theoretical findings, we perform extensive experimental analysis by considering 6 different DNN architectures, 6 different approaches for distributed DNN and 10 different adversarial attacks using the ImageNet-1K dataset.         ",
    "url": "https://arxiv.org/abs/2309.17401",
    "authors": [
      "Milin Zhang",
      "Mohammad Abdi",
      "Jonathan Ashdown",
      "Francesco Restuccia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.03617",
    "title": "RouteKG: A knowledge graph-based framework for route prediction on road networks",
    "abstract": "           Short-term route prediction on road networks allows us to anticipate the future trajectories of road users, enabling various applications ranging from dynamic traffic control to personalized navigation. Despite recent advances in this area, existing methods focus primarily on learning sequential transition patterns, neglecting the inherent spatial relations in road networks that can affect human routing decisions. To fill this gap, this paper introduces RouteKG, a novel Knowledge Graph-based framework for route prediction. Specifically, we construct a Knowledge Graph on the road network to encode spatial relations, especially moving directions that are crucial for human navigation. Moreover, an n-ary tree-based algorithm is introduced to efficiently generate top-K routes in batch mode, enhancing computational efficiency. To further optimize prediction performance, a rank refinement module is incorporated to fine-tune candidate route rankings. The model performance is evaluated using two real-world vehicle trajectory datasets from two Chinese cities under various practical scenarios. The results demonstrate a significant improvement in accuracy over the baseline methods. We further validate the proposed method by utilizing the pre-trained model as a simulator for real-time traffic flow estimation at the link level. RouteKG has great potential to transform vehicle navigation, traffic management, and a variety of intelligent transportation tasks, playing a crucial role in advancing the core foundation of intelligent and connected urban systems. The source codes of RouteKG are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2310.03617",
    "authors": [
      "Yihong Tang",
      "Zhan Zhao",
      "Weipeng Deng",
      "Shuyu Lei",
      "Yuebing Liang",
      "Zhenliang Ma"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2312.01392",
    "title": "Neural Network Characterization and Entropy Regulated Data Balancing through Principal Component Analysis",
    "abstract": "           This paper examines in detail the geometric structure of principal component analysis (PCA) by considering in detail the distributions of both unrotated and rotated MNIST digits in the space defined by the lowest order PCA components. Since digits possessing salient geometric features are mapped to restricted regions far from the origin, they are predicted by neural networks with a greater accuracy than digits that are mapped to broad, diffuse and overlapping volumes of the low order PCA space. Motivated by these results, a new quantity, the local PCA entropy, obtained by dividing the spatial region spanned by the low order principal components into histogram bins and evaluating the entropy associated with the number of occurrences of each input class within a bin, is introduced. The metric locates the input data records that yield the largest confusion in prediction accuracy within reduced coordinate volumes that optimally discriminate among geometric features. As an example of the potential utility of the local PCA entropy, a simple data balancing procedure is realized by oversampling the data records in regions of large local entropy.         ",
    "url": "https://arxiv.org/abs/2312.01392",
    "authors": [
      "David Yevick",
      "Karolina Hutchison"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.19411",
    "title": "PaECTER: Patent-level Representation Learning using Citation-informed Transformers",
    "abstract": "           PaECTER is an open-source document-level encoder specific for patents. We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the patent specific pre-trained language model (BERT for Patents) and general-purpose text embedding models (e.g., E5, GTE, and BGE) on our patent citation prediction test dataset on different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and patent examiners.         ",
    "url": "https://arxiv.org/abs/2402.19411",
    "authors": [
      "Mainak Ghosh",
      "Michael E. Rose",
      "Sebastian Erhardt",
      "Erik Buunk",
      "Dietmar Harhoff"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.04215",
    "title": "NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions",
    "abstract": "           Classical planners are powerful systems, but modeling tasks in input formats such as PDDL is tedious and error-prone. In contrast, planning with Large Language Models (LLMs) allows for almost any input text, but offers no guarantees on plan quality or even soundness. In an attempt to merge the best of these two approaches, some work has begun to use LLMs to automate parts of the PDDL creation process. However, these methods still require various degrees of expert input or domain-specific adaptations. We present NL2Plan, the first fully automatic system for generating complete PDDL tasks from minimal natural language descriptions. NL2Plan uses an LLM to incrementally extract the necessary information from the short text input before creating a complete PDDL description of both the domain and the problem which is finally solved by a classical planner. We evaluate NL2Plan on seven planning domains, five of which are novel and thus not in the LLM training data, and find that NL2Plan outperforms directly generating the files with an LLM+validator combination. As such, NL2Plan is a powerful tool for assistive PDDL modeling and a step towards solving natural language planning task with interpretability and guarantees.         ",
    "url": "https://arxiv.org/abs/2405.04215",
    "authors": [
      "Elliot Gestrin",
      "Marco Kuhlmann",
      "Jendrik Seipp"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2405.20485",
    "title": "Phantom: General Backdoor Attacks on Retrieval Augmented Language Generation",
    "abstract": "           Retrieval Augmented Generation (RAG) expands the capabilities of modern large language models (LLMs), by anchoring, adapting, and personalizing their responses to the most relevant knowledge sources. It is particularly useful in chatbot applications, allowing developers to customize LLM output without expensive retraining. Despite their significant utility in various applications, RAG systems present new security risks. In this work, we propose a novel attack that allows an adversary to inject a single malicious document into a RAG system's knowledge base, and mount a backdoor poisoning attack. We design Phantom, a general two-stage optimization framework against RAG systems, that crafts a malicious poisoned document leading to an integrity violation in the model's output. First, the document is constructed to be retrieved only when a specific naturally occurring trigger sequence of tokens appears in the victim's queries. Second, the document is further optimized with crafted adversarial text that induces various adversarial objectives on the LLM output, including refusal to answer, reputation damage, privacy violations, and harmful this http URL demonstrate our attacks on multiple open-source LLM architectures, including Gemma, Vicuna, and Llama, and show that they transfer to closed-source models such as GPT-3.5 Turbo and GPT-4. Finally, we successfully demonstrate our attack on an end-to-end black-box production RAG system: NVIDIA's \"Chat with RTX''.         ",
    "url": "https://arxiv.org/abs/2405.20485",
    "authors": [
      "Harsh Chaudhari",
      "Giorgio Severi",
      "John Abascal",
      "Anshuman Suri",
      "Matthew Jagielski",
      "Christopher A. Choquette-Choo",
      "Milad Nasr",
      "Cristina Nita-Rotaru",
      "Alina Oprea"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.09262",
    "title": "Fully Heteroscedastic Count Regression with Deep Double Poisson Networks",
    "abstract": "           Neural networks capable of accurate, input-conditional uncertainty representation are essential for real-world AI systems. Deep ensembles of Gaussian networks have proven highly effective for continuous regression due to their ability to flexibly represent aleatoric uncertainty via unrestricted heteroscedastic variance, which in turn enables accurate epistemic uncertainty estimation. However, no analogous approach exists for count regression, despite many important applications. To address this gap, we propose the Deep Double Poisson Network (DDPN), a novel neural discrete count regression model that outputs the parameters of the Double Poisson distribution, enabling arbitrarily high or low predictive aleatoric uncertainty for count data and improving epistemic uncertainty estimation when ensembled. We formalize and prove that DDPN exhibits robust regression properties similar to heteroscedastic Gaussian models via learnable loss attenuation, and introduce a simple loss modification to control this behavior. Experiments on diverse datasets demonstrate that DDPN outperforms current baselines in accuracy, calibration, and out-of-distribution detection, establishing a new state-of-the-art in deep count regression.         ",
    "url": "https://arxiv.org/abs/2406.09262",
    "authors": [
      "Spencer Young",
      "Porter Jenkins",
      "Longchao Da",
      "Jeff Dotson",
      "Hua Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.17153",
    "title": "An Equilibrium Model for Schedule-Based Transit Networks with Hard Vehicle Capacities",
    "abstract": "           Modelling passenger assignments in public transport networks is a fundamental task for city planners, especially when deliberating network infrastructure decisions. A key aspect of a realistic model is to integrate passengers' selfish routing behaviour under limited vehicle capacities. We formulate a side-constrained user equilibrium model in a schedule-based transit network, where passengers are modelled via a continuum of non-atomic agents that travel from their origin to their destination. An agent's route may comprise several rides along given lines, each using vehicles with hard loading capacities. We give a characterization of (side-constrained) user equilibria via a quasi-variational inequality and prove their existence for fixed departure times by generalizing a well-known result of Bernstein and Smith (Transp. Sci., 1994). We further derive a polynomial time algorithm for single-commodity instances with fixed departure times. For the multi-commodity case with departure time choice, we show that deciding whether an equilibrium exists is NP-hard, and we devise an exponential-time algorithm that computes an equilibrium if it exists, and signals non-existence otherwise. Using our quasi-variational characterization, we formulate a heuristic for computing multi-commodity user equilibria in practice, which is tested on multiple real-world instances. In terms of social cost, the computed user-equilibria are quite efficient compared to a system optimum.         ",
    "url": "https://arxiv.org/abs/2406.17153",
    "authors": [
      "Tobias Harks",
      "Sven J\u00e4ger",
      "Michael Markl",
      "Philine Schiewe"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2408.06123",
    "title": "DPDETR: Decoupled Position Detection Transformer for Infrared-Visible Object Detection",
    "abstract": "           Infrared-visible object detection aims to achieve robust object detection by leveraging the complementary information of infrared and visible image pairs. However, the commonly existing modality misalignment problem presents two challenges: fusing misalignment complementary features is difficult, and current methods cannot reliably locate objects in both modalities under misalignment conditions. In this paper, we propose a Decoupled Position Detection Transformer (DPDETR) to address these issues. Specifically, we explicitly define the object category, visible modality position, and infrared modality position to enable the network to learn the intrinsic relationships and output reliably positions of objects in both modalities. To fuse misaligned object features reliably, we propose a Decoupled Position Multispectral Cross-attention module that adaptively samples and aggregates multispectral complementary features with the constraint of infrared and visible reference positions. Additionally, we design a query-decoupled Multispectral Decoder structure to address the the conflict in feature focus among the three kinds of object information in our task and propose a Decoupled Position Contrastive DeNoising Training strategy to enhance the DPDETR's ability to learn decoupled positions. Experiments on DroneVehicle and KAIST datasets demonstrate significant improvements compared to other state-of-the-art methods. The code will be released at this https URL ",
    "url": "https://arxiv.org/abs/2408.06123",
    "authors": [
      "Junjie Guo",
      "Chenqiang Gao",
      "Fangcen Liu",
      "Deyu Meng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2409.10836",
    "title": "SL$^{2}$A-INR: Single-Layer Learnable Activation for Implicit Neural Representation",
    "abstract": "           Implicit Neural Representation (INR), leveraging a neural network to transform coordinate input into corresponding attributes, has recently driven significant advances in several vision-related domains. However, the performance of INR is heavily influenced by the choice of the nonlinear activation function used in its multilayer perceptron (MLP) architecture. To date, multiple nonlinearities have been investigated, but current INRs still face limitations in capturing high-frequency components and diverse signal types. We show that these challenges can be alleviated by introducing a novel approach in INR architecture. Specifically, we propose SL$^{2}$A-INR, a hybrid network that combines a single-layer learnable activation function with an MLP that uses traditional ReLU activations. Our method performs superior across diverse tasks, including image representation, 3D shape reconstruction, and novel view synthesis. Through comprehensive experiments, SL$^{2}$A-INR sets new benchmarks in accuracy, quality, and robustness for INR. Our Code is publicly available on~\\href{this https URL}{\\textcolor{magenta}{GitHub}}.         ",
    "url": "https://arxiv.org/abs/2409.10836",
    "authors": [
      "Moein Heidari",
      "Reza Rezaeian",
      "Reza Azad",
      "Dorit Merhof",
      "Hamid Soltanian-Zadeh",
      "Ilker Hacihaliloglu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.15126",
    "title": "UTrace: Poisoning Forensics for Private Collaborative Learning",
    "abstract": "           Privacy-preserving machine learning (PPML) systems enable multiple data owners to collaboratively train models without revealing their raw, sensitive data by leveraging cryptographic protocols such as secure multi-party computation (MPC). While PPML offers strong privacy guarantees, it also introduces new attack surfaces: malicious data owners can inject poisoned data into the training process without being detected, thus undermining the integrity of the learned model. Although recent defenses, such as private input validation within MPC, can mitigate some specific poisoning strategies, they remain insufficient, particularly in preventing stealthy or distributed attacks. As the robustness of PPML remains an open challenge, strengthening trust in these systems increasingly necessitates post-hoc auditing mechanisms that instill accountability. In this paper we present UTrace, a framework for user-level traceback in PPML that attributes integrity failures to responsible data owners without compromising the privacy guarantees of MPC. UTrace encapsulates two mechanisms: a gradient similarity method that identifies suspicious update patterns linked to poisoning, and a user-level unlearning technique that quantifies each user's marginal influence on model behavior. Together, these methods allow UTrace to attribute model misbehavior to specific users with high precision. We implement UTrace within an MPC-compatible training and auditing pipeline and evaluate its effectiveness on four datasets spanning vision, text, and malware. Across ten canonical poisoning attacks, UTrace consistently achieves high detection accuracy with low false positive rates.         ",
    "url": "https://arxiv.org/abs/2409.15126",
    "authors": [
      "Evan Rose",
      "Hidde Lycklama",
      "Harsh Chaudhari",
      "Niklas Britz",
      "Anwar Hithnawi",
      "Alina Oprea"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.19168",
    "title": "Optimization-based Task and Motion Planning under Signal Temporal Logic Specifications using Logic Network Flow",
    "abstract": "           This paper proposes an optimization-based task and motion planning framework, named \"Logic Network Flow\", to integrate signal temporal logic (STL) specifications into efficient mixed-binary linear programmings. In this framework, temporal predicates are encoded as polyhedron constraints on each edge of the network flow, instead of as constraints between the nodes as in the traditional Logic Tree formulation. Synthesized with Dynamic Network Flows, Logic Network Flows render a tighter convex relaxation compared to Logic Trees derived from these STL specifications. Our formulation is evaluated on several multi-robot motion planning case studies. Empirical results demonstrate that our formulation outperforms Logic Tree formulation in terms of computation time for several planning problems. As the problem size scales up, our method still discovers better lower and upper bounds by exploring fewer number of nodes during the branch-and-bound process, although this comes at the cost of increased computational load for each node when exploring branches.         ",
    "url": "https://arxiv.org/abs/2409.19168",
    "authors": [
      "Xuan Lin",
      "Jiming Ren",
      "Samuel Coogan",
      "Ye Zhao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ]
  },
  {
    "id": "arXiv:2410.04525",
    "title": "Out-of-Distribution Detection with Relative Angles",
    "abstract": "           Deep learning systems deployed in real-world applications often encounter data that is different from their in-distribution (ID). A reliable model should ideally abstain from making decisions in this out-of-distribution (OOD) setting. Existing state-of-the-art methods primarily focus on feature distances, such as k-th nearest neighbors and distances to decision boundaries, either overlooking or ineffectively using in-distribution statistics. In this work, we propose a novel angle-based metric for OOD detection that is computed relative to the in-distribution structure. We demonstrate that the angles between feature representations and decision boundaries, viewed from the mean of in-distribution features, serve as an effective discriminative factor between ID and OOD data. We evaluate our method on nine ImageNet-pretrained models. Our approach achieves the lowest FPR in 5 out of 9 ImageNet models, obtains the best average FPR overall, and consistently ranking among the top 3 across all evaluated models. Furthermore, we highlight the benefits of contrastive representations by showing strong performance with ResNet SCL and CLIP architectures. Finally, we demonstrate that the scale-invariant nature of our score enables an ensemble strategy via simple score summation. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.04525",
    "authors": [
      "Berker Demirel",
      "Marco Fumero",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.05557",
    "title": "Source-Free Domain Adaptive Object Detection with Semantics Compensation",
    "abstract": "           Strong data augmentation is a fundamental component of state-of-the-art mean teacher-based Source-Free domain adaptive Object Detection (SFOD) methods, enabling consistency-based self-supervised optimization along weak augmentation. However, our theoretical analysis and empirical observations reveal a critical limitation: strong augmentation can inadvertently erase class-relevant components, leading to artificial inter-category confusion. To address this issue, we introduce Weak-to-strong Semantics Compensation (WSCo), a novel remedy that leverages weakly augmented images, which preserve full semantics, as anchors to enrich the feature space of their strongly augmented counterparts. Essentially, this compensates for the class-relevant semantics that may be lost during strong augmentation on the fly. Notably, WSCo can be implemented as a generic plug-in, easily integrable with any existing SFOD pipelines. Extensive experiments validate the negative impact of strong augmentation on detection performance, and the effectiveness of WSCo in enhancing the performance of previous detection models on standard benchmarks.         ",
    "url": "https://arxiv.org/abs/2410.05557",
    "authors": [
      "Song Tang",
      "Jiuzheng Yang",
      "Mao Ye",
      "Boyu Wang",
      "Yan Gan",
      "Xiatian Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.20075",
    "title": "Almost Sure Convergence of Networked Policy Gradient over Time-Varying Networks in Markov Potential Games",
    "abstract": "           We propose networked policy gradient play for solving Markov potential games with continuous and/or discrete state-action pairs. During the game, agents use parametrized and differentiable policies that depend on the current state and the policy parameters of other agents. During training, agents update their policy parameters following stochastic gradients. The gradient estimation involves two consecutive episodes, generating unbiased estimators of reward and policy score functions. In addition, it involves keeping estimates of others' parameters using consensus steps given local estimates received through a time-varying communication network. In Markov potential games, there exists a potential value function among agents with gradients corresponding to the gradients of local value functions. Using this structure, we prove almost sure convergence to a stationary point of the potential value function with rate $O(1/\\epsilon^2)$. Compared to previous works, our results do not require bounded policy gradients or initial agreement on the values of individual policy parameters. Numerical experiments on a dynamic multi-agent newsvendor problem verify the convergence of local beliefs and gradients. It further shows that networked policy gradient play converges as fast as independent policy gradient updates, while collecting higher rewards.         ",
    "url": "https://arxiv.org/abs/2410.20075",
    "authors": [
      "Sarper Aydin",
      "Ceyhun Eksin"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2501.02074",
    "title": "A Comprehensive Framework to Operationalize Social Stereotypes for Responsible AI Evaluations",
    "abstract": "           Societal stereotypes are at the center of a myriad of responsible AI interventions targeted at reducing the generation and propagation of potentially harmful outcomes. While these efforts are much needed, they tend to be fragmented and often address different parts of the issue without adopting a unified or holistic approach to social stereotypes and how they impact various parts of the machine learning pipeline. As a result, current interventions fail to capitalize on the underlying mechanisms that are common across different types of stereotypes, and to anchor on particular aspects that are relevant in certain cases. In this paper, we draw on social psychological research and build on NLP data and methods, to propose a unified framework to operationalize stereotypes in generative AI evaluations. Our framework identifies key components of stereotypes that are crucial in AI evaluation, including the target group, associated attribute, relationship characteristics, perceiving group, and context. We also provide considerations and recommendations for its responsible use.         ",
    "url": "https://arxiv.org/abs/2501.02074",
    "authors": [
      "Aida Davani",
      "Sunipa Dev",
      "H\u00e9ctor P\u00e9rez-Urbina",
      "Vinodkumar Prabhakaran"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2502.01859",
    "title": "Fully discrete analysis of the Galerkin POD neural network approximation with application to 3D acoustic wave scattering",
    "abstract": "           In this work, we consider the approximation of parametric maps using the so-called Galerkin POD-NN method. This technique combines the computation of a reduced basis via proper orthogonal decomposition (POD) and artificial neural networks (NNs) for the construction of fast surrogates of said parametric maps. In contrast to the existing literature, which has studied the approximation properties of this kind of architecture on a continuous level, we provide a fully discrete error analysis of this approach. More precisely, our estimates also account for discretization errors during the construction of the NN architecture. We consider the number of reduced basis in the approximation of the solution manifold, truncation in the parameter space, and, most importantly, the number of samples in the computation of the reduced space, together with the effect of the use of NNs in the approximation of the reduced coefficients. Following this error analysis, we provide a-priori bounds on the required POD tolerance, the resulting POD ranks, and NN parameters to maintain the order of convergence of quasi Monte Carlo sampling techniques. We conclude this work by showcasing the applicability of this method through a practical industrial application: the sound-soft acoustic scattering problem by a parametrically defined scatterer in three physical dimensions.         ",
    "url": "https://arxiv.org/abs/2502.01859",
    "authors": [
      "J\u00fcrgen D\u00f6lz",
      "Fernando Henr\u00edquez"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2502.06722",
    "title": "HetSwarm: Cooperative Navigation of Heterogeneous Swarm in Dynamic and Dense Environments through Impedance-based Guidance",
    "abstract": "           With the growing demand for efficient logistics and warehouse management, unmanned aerial vehicles (UAVs) are emerging as a valuable complement to automated guided vehicles (AGVs). UAVs enhance efficiency by navigating dense environments and operating at varying altitudes. However, their limited flight time, battery life, and payload capacity necessitate a supporting ground station. To address these challenges, we propose HetSwarm, a heterogeneous multi-robot system that combines a UAV and a mobile ground robot for collaborative navigation in cluttered and dynamic conditions. Our approach employs an artificial potential field (APF)-based path planner for the UAV, allowing it to dynamically adjust its trajectory in real time. The ground robot follows this path while maintaining connectivity through impedance links, ensuring stable coordination. Additionally, the ground robot establishes temporal impedance links with low-height ground obstacles to avoid local collisions, as these obstacles do not interfere with the UAV's flight. Experimental validation of HetSwarm in diverse environmental conditions demonstrated a 90% success rate across 30 test cases. The ground robot exhibited an average deviation of 45 cm near obstacles, confirming effective collision avoidance. Extensive simulations in the Gym PyBullet environment further validated the robustness of our system for real-world applications, demonstrating its potential for dynamic, real-time task execution in cluttered environments.         ",
    "url": "https://arxiv.org/abs/2502.06722",
    "authors": [
      "Malaika Zafar",
      "Roohan Ahmed Khan",
      "Aleksey Fedoseev",
      "Kumar Katyayan Jaiswal",
      "Dzmitry Tsetserukou"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.18238",
    "title": "Semantic Communications Services within Generalist Operated Networks",
    "abstract": "           This paper addresses the challenge of integrating semantic communication principles into operated networks, traditionally optimized based on network-centric metrics rather than application-specific needs. Operated networks strongly adhere to the principle of ``separation of concerns\", which emphasizes a clear distinction between network operation and application. Despite the initial perceived incompatibility between semantic communication and the principles of operated networks, this paper provides solutions to reconcile them. The foundations of these solutions include the adoption of non-arbitrary semantic representations as a standard encoding for communications, the establishment of a standard interface between the application and network, and a dedicated network control plane. These enable the application to describe the data typology and the nature of the task, and to agree upon a transmission scheme tailored to the supported task. Through three scenarios involving an application transmitting text representations, we illustrate the implementation of the proposal and demonstrate the potential of the approach.         ",
    "url": "https://arxiv.org/abs/2502.18238",
    "authors": [
      "Quentin Lampin",
      "Louis-Adrien Dufr\u00e8ne",
      "Guillaume Larue"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.13802",
    "title": "Multi-Harmonic Gridded 3D Deconvolution (MH3D) for Robust and Accurate Image Reconstruction in MPI for Single Axis Drive Field Scanners",
    "abstract": "           Objective: This work introduces a new magnetic particle imaging (MPI) reconstruction framework based on multi-harmonic 3D deconvolution (MH3D) of gridded portraits, offering a principled, model-driven approach to MPI imaging. Approach: MH3D defines a convolutional forward model using higher harmonic portraits, which are gridded images formed from filtered frequency-domain signal components. Each harmonic portrait is modeled as a convolution with a distinct PSF, closely approximated by derivatives of the Langevin function, and incorporates receive sensitivity and mesh downsampling for accurate modeling. We also introduce practical strategies for calibration, phase correction, and artifact reduction. Main Results: We validate the MH3D approach using analytic approximations, numerical simulations, and experimental phantom data. MH3D yields high-resolution 3D reconstructions on seconds-scale runtimes, improves image quality relative to common 3rd-harmonic-only reconstructions, and achieves image quality and resolution comparable to a generalized model-based method in simulations and phantom experiments. Significance: This work offers new theoretical insight into MPI signal structure, unveiling the methodological and theoretical underpinnings absent in earlier single-harmonic or heuristic methods, thereby supporting accurate and robust 3D imaging with excellent computational efficiency.         ",
    "url": "https://arxiv.org/abs/2503.13802",
    "authors": [
      "Toby Sanders",
      "Justin J. Konkle",
      "Erica E. Mason",
      "Patrick W. Goodwill"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2504.11415",
    "title": "Robustness and sex differences in skin cancer detection: logistic regression vs CNNs",
    "abstract": "           Deep learning has been reported to achieve high performances in the detection of skin cancer, yet many challenges regarding the reproducibility of results and biases remain. This study is a replication (different data, same analysis) of a previous study on Alzheimer's disease detection, which studied the robustness of logistic regression (LR) and convolutional neural networks (CNN) across patient sexes. We explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset with LR trained on handcrafted features reflecting dermatological guidelines (ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We evaluate these models in alignment with the replicated study: across multiple training datasets with varied sex composition to determine their robustness. Our results show that both the LR and the CNN were robust to the sex distribution, but the results also revealed that the CNN had a significantly higher accuracy (ACC) and area under the receiver operating characteristics (AUROC) for male patients compared to female patients. The data and relevant scripts to reproduce our results are publicly available (this https URL nikodice4/Skin-cancer-detection-sex-bias).         ",
    "url": "https://arxiv.org/abs/2504.11415",
    "authors": [
      "Nikolette Pedersen",
      "Regitze Sydendal",
      "Andreas Wulff",
      "Ralf Raumanns",
      "Eike Petersen",
      "Veronika Cheplygina"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.17017",
    "title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification",
    "abstract": "           Formally verifying properties of software code has been a highly desirable task, especially with the emergence of LLM-generated code. In the same vein, they provide an interesting avenue for the exploration of formal verification and mechanistic interpretability. Since the introduction of code-specific models, despite their successes in generating code in Lean4 and Isabelle, the task of generalized theorem proving still remains far from being fully solved and will be a benchmark for reasoning capability in LLMs. In this work, we introduce a framework that generates whole proofs in a formal language to be used within systems that utilize the power of built-in tactics and off-the-shelf automated theorem provers. Our framework includes 3 components: generating natural language statements of the code to be verified, an LLM that generates formal proofs for the given statement, and a module employing heuristics for building the final proof. To train the LLM, we employ a 2-stage fine-tuning process, where we first use SFT-based training to enable the model to generate syntactically correct Isabelle code and then RL-based training that encourages the model to generate proofs verified by a theorem prover. We validate our framework using the miniF2F-test benchmark and the Isabelle proof assistant and design a use case to verify the correctness of the AWS S3 bucket access policy code. We also curate a dataset based on the FVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks.         ",
    "url": "https://arxiv.org/abs/2504.17017",
    "authors": [
      "Balaji Rao",
      "William Eiers",
      "Carlo Lipizzi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2504.21747",
    "title": "Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data",
    "abstract": "           Conventional retrieval-augmented neural machine translation (RANMT) systems leverage bilingual corpora, e.g., translation memories (TMs). Yet, in many settings, monolingual corpora in the target language are often available. This work explores ways to take advantage of such resources by directly retrieving relevant target language segments, based on a source-side query. For this, we design improved cross-lingual retrieval systems, trained with both sentence level and word-level matching objectives. In our experiments with three RANMT architectures, we assess such cross-lingual objectives in a controlled setting, reaching performances that match those of standard TM-based models. We also showcase our method on a real-world settings, using much larger monolingual and observe strong improvements over both the baseline setting and general-purpose cross-lingual retrievers.         ",
    "url": "https://arxiv.org/abs/2504.21747",
    "authors": [
      "Maxime Bouthors",
      "Josep Crego",
      "Fran\u00e7ois Yvon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.12185",
    "title": "EVALOOOP: A Self-Consistency-Centered Framework for Assessing Large Language Model Robustness in Programming",
    "abstract": "           Evaluating the programming robustness of large language models (LLMs) is paramount for ensuring their reliability in AI-based software development. However, adversarial attacks exhibit fundamental limitations that compromise fair robustness assessment: they demonstrate contradictory evaluation outcomes where different attack strategies tend to favor different models, and more critically, they operate solely through external perturbations, failing to capture the intrinsic stability essential for autonomous coding agents where subsequent inputs are endogenously generated by the model itself. We introduce EVALOOOP, a novel assessment framework that evaluates robustness from a self-consistency perspective, leveraging the natural duality inherent in software engineering tasks (e.g., code generation and code summarization). EVALOOOP establishes a self-contained feedback loop where an LLM iteratively transforms between code and natural language until functional failure occurs, with robustness quantified by a novel Average Sustainable Loops (ASL) metric-the mean number of iterations maintaining functional correctness across benchmark tasks. This cyclical strategy intrinsically evaluates robustness without relying on external attack configurations, providing a unified metric that reveals how effectively LLMs preserve semantic integrity through sustained self-referential transformations. We evaluate 96 popular LLMs, ranging from 0.5B to 685B parameters, on EVALOOOP equipped with the MBPP Plus benchmark, and found that EVALOOOP typically induces a 2.65%-47.62% absolute drop in pass@1 accuracy within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, Qwen3-235B-A22B-Instruct-2507, despite inferior initial code generation compared to OpenAI's o-series models and DeepSeek-V3, demonstrated the superior robustness (ASL score).         ",
    "url": "https://arxiv.org/abs/2505.12185",
    "authors": [
      "Sen Fang",
      "Weiyuan Ding",
      "Bowen Xu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.13000",
    "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
    "abstract": "           Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: this https URL, code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2505.13000",
    "authors": [
      "Jiaqi Li",
      "Xiaolong Lin",
      "Zhekai Li",
      "Shixi Huang",
      "Yuancheng Wang",
      "Chaoren Wang",
      "Zhenpeng Zhan",
      "Zhizheng Wu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2505.18049",
    "title": "SpikeGen: Decoupled \"Rods and Cones\" Visual Representation Processing with Latent Generative Framework",
    "abstract": "           The process through which humans perceive and learn visual representations in dynamic environments is highly complex. From a structural perspective, the human eye decouples the functions of cone and rod cells: cones are primarily responsible for color perception, while rods are specialized in detecting motion, particularly variations in light intensity. These two distinct modalities of visual information are integrated and processed within the visual cortex, thereby enhancing the robustness of the human visual system. Inspired by this biological mechanism, modern hardware systems have evolved to include not only color-sensitive RGB cameras but also motion-sensitive Dynamic Visual Systems, such as spike cameras. Building upon these advancements, this study seeks to emulate the human visual system by integrating decomposed multi-modal visual inputs with modern latent-space generative frameworks. We named it SpikeGen. We evaluate its performance across various spike-RGB tasks, including conditional image and video deblurring, dense frame reconstruction from spike streams, and high-speed scene novel-view synthesis. Supported by extensive experiments, we demonstrate that leveraging the latent space manipulation capabilities of generative models enables an effective synergistic enhancement of different visual modalities, addressing spatial sparsity in spike inputs and temporal sparsity in RGB inputs.         ",
    "url": "https://arxiv.org/abs/2505.18049",
    "authors": [
      "Gaole Dai",
      "Menghang Dong",
      "Rongyu Zhang",
      "Ruichuan An",
      "Shanghang Zhang",
      "Tiejun Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.05744",
    "title": "Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties",
    "abstract": "           Recent large-scale reasoning models have achieved state-of-the-art performance on challenging mathematical benchmarks, yet the internal mechanisms underlying their success remain poorly understood. In this work, we introduce the notion of a reasoning graph, extracted by clustering hidden-state representations at each reasoning step, and systematically analyze three key graph-theoretic properties: cyclicity, diameter, and small-world index, across multiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled reasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly more recurrent cycles (about 5 per sample), substantially larger graph diameters, and pronounced small-world characteristics (about 6x) compared to their base counterparts. Notably, these structural advantages grow with task difficulty and model capacity, with cycle detection peaking at the 14B scale and exploration diameter maximized in the 32B variant, correlating positively with accuracy. Furthermore, we show that supervised fine-tuning on an improved dataset systematically expands reasoning graph diameters in tandem with performance gains, offering concrete guidelines for dataset design aimed at boosting reasoning capabilities. By bridging theoretical insights into reasoning graph structures with practical recommendations for data construction, our work advances both the interpretability and the efficacy of large reasoning models.         ",
    "url": "https://arxiv.org/abs/2506.05744",
    "authors": [
      "Gouki Minegishi",
      "Hiroki Furuta",
      "Takeshi Kojima",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.06558",
    "title": "Rapid training of Hamiltonian graph networks using random features",
    "abstract": "           Learning dynamical systems that respect physical symmetries and constraints remains a fundamental challenge in data-driven modeling. Integrating physical laws with graph neural networks facilitates principled modeling of complex N-body dynamics and yields accurate and permutation-invariant models. However, training graph neural networks with iterative, gradient-based optimization algorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training, especially for large, complex systems. In comparison to 15 different optimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained up to 600x faster--but with comparable accuracy--by replacing iterative optimization with random feature-based parameter construction. We show robust performance in diverse simulations, including N-body mass-spring and molecular systems in up to 3 dimensions and 10,000 particles with different geometries, while retaining essential physical invariances with respect to permutation, rotation, and translation. Our proposed approach is benchmarked using a NeurIPS 2022 Datasets and Benchmarks Track publication to further demonstrate its versatility. We reveal that even when trained on minimal 8-node systems, the model can generalize in a zero-shot manner to systems as large as 4096 nodes without retraining. Our work challenges the dominance of iterative gradient-descent-based optimization algorithms for training neural network models for physical systems.         ",
    "url": "https://arxiv.org/abs/2506.06558",
    "authors": [
      "Atamert Rahma",
      "Chinmay Datar",
      "Ana Cukarska",
      "Felix Dietrich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2506.08218",
    "title": "gh0stEdit: Exploiting Layer-Based Access Vulnerability Within Docker Container Images",
    "abstract": "           Containerisation is a popular deployment process for application-level virtualisation using a layer-based approach. Docker is a leading provider of containerisation, and through the Docker Hub, users can supply Docker images for sharing and re-purposing popular software application containers. Using a combination of in-built inspection commands, publicly displayed image layer content, and static image scanning, Docker images are designed to ensure end users can clearly assess the content of the image before running them. In this paper we present gh0stEdit, an exploit that fundamentally undermines the integrity of Docker images and subverts the assumed trust and transparency they utilise. The use of gh0stEdit allows an attacker to maliciously edit Docker images, in a way that is not shown within the image history, hierarchy or commands. This attack can also be carried out against signed images (Docker Content Trust) without invalidating the image signature. We present a detailed case study for this exploit, and showcase how gh0stEdit is able to poison an image in a way that is not picked up through static or dynamic scanning tools. We highlight the issues in the current approach to Docker image security and trust, and expose an attack method which could potentially be exploited in the wild without being detected. To the best of our knowledge we are the first to provide detailed discussion on the exploit of this vulnerability.         ",
    "url": "https://arxiv.org/abs/2506.08218",
    "authors": [
      "Alan Mills",
      "Jonathan White",
      "Phil Legg"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.08584",
    "title": "CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmarking of Large Language Models in Mental Health Question Answering",
    "abstract": "           Medical question answering (QA) benchmarks often focus on multiple-choice or fact-based tasks, leaving open-ended answers to real patient questions underexplored. This gap is particularly critical in mental health, where patient questions often mix symptoms, treatment concerns, and emotional needs, requiring answers that balance clinical caution with contextual sensitivity. We present CounselBench, a large-scale benchmark developed with 100 mental health professionals to evaluate and stress-test large language models (LLMs) in realistic help-seeking scenarios. The first component, CounselBench-EVAL, contains 2,000 expert evaluations of answers from GPT-4, LLaMA 3, Gemini, and human therapists on patient questions from the public forum CounselChat. Each answer is rated across six clinically grounded dimensions, with span-level annotations and written rationales. Expert evaluations show that while LLMs achieve high scores on several dimensions, they also exhibit recurring issues, including unconstructive feedback, overgeneralization, and limited personalization or relevance. Responses were frequently flagged for safety risks, most notably unauthorized medical advice. Follow-up experiments show that LLM judges systematically overrate model responses and overlook safety concerns identified by human experts. To probe failure modes more directly, we construct CounselBench-Adv, an adversarial dataset of 120 expert-authored mental health questions designed to trigger specific model issues. Evaluation of 3,240 responses from nine LLMs reveals consistent, model-specific failure patterns. Together, CounselBench establishes a clinically grounded framework for benchmarking LLMs in mental health QA.         ",
    "url": "https://arxiv.org/abs/2506.08584",
    "authors": [
      "Yahan Li",
      "Jifan Yao",
      "John Bosco S. Bunyi",
      "Adam C. Frank",
      "Angel Hwang",
      "Ruishan Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.08678",
    "title": "ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction",
    "abstract": "           Vision-language models such as CLIP have recently propelled open-vocabulary dense prediction tasks by enabling recognition of a broad range of visual concepts. However, CLIP still struggles with fine-grained, region-level understanding, hindering its effectiveness on these dense prediction tasks. We identify two pivotal factors required to address this limitation: semantic coherence and fine-grained vision-language alignment. Current adaptation methods often improve fine-grained alignment at the expense of semantic coherence, and often rely on extra modules or supervised fine-tuning. To overcome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel approach that simultaneously enhances semantic coherence and fine-grained alignment by leveraging own knowledge of a model across all representation levels. Unlike prior methods, ATAS uses only unlabeled images and an internal self-distillation process to refine representations of CLIP vision encoders, preserving local semantic consistency while sharpening local detail recognition. On open-vocabulary object detection and semantic segmentation benchmarks, ATAS achieves substantial performance gains, outperforming baseline CLIP models. These results validate the effectiveness of our approach and underscore the importance of jointly maintaining semantic coherence and fine-grained alignment for advanced open-vocabulary dense prediction.         ",
    "url": "https://arxiv.org/abs/2506.08678",
    "authors": [
      "Juan Yeo",
      "Soonwoo Cha",
      "Jiwoo Song",
      "Hyunbin Jin",
      "Taesup Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.09878",
    "title": "Virtualizing RAN: Science, Strategy, and Architecture of Software-Defined Mobile Networks",
    "abstract": "           Virtualizing the Radio-Access Network (RAN) is increasingly viewed as an enabler of affordable 5G expansion and a stepping-stone toward AI-native 6G. Most discussions, however, still approach spectrum policy, cloud engineering and organizational practice as separate topics. This paper offers an integrated perspective spanning four pillars -- science, technology, business strategy and culture. A comparative U.S.\\ case study illustrates how mid-band contiguity, complemented by selective mmWave capacity layers, can improve both coverage and churn when orchestrated through software-defined carrier aggregation. We derive analytic capacity and latency bounds for Split 7.2 $\\times$ vRAN/O-RAN deployments, quantify the throughput penalty of end-to-end 256-bit encryption, and show how GPU/FPGA off-load plus digital-twin-driven automation keeps the hybrid-automatic-repeat request (HARQ) round-trip within a 0.5 ms budget. When these technical enablers are embedded in a physics-first delivery roadmap, average vRAN cycle time drops an order of magnitude -- even in the presence of cultural head-winds such as dual-ladder'' erosion. Three cybernetic templates -- the Clock-Hierarchy Law, Ashby's Requisite Variety and a delay-cost curve -- are then used to explain why silo-constrained automation can amplify, rather than absorb, integration debt. Looking forward, silicon-paced 6G evolution (9-12 month node shrinks, sub-THz joint communication-and-sensing, chiplet architectures and optical I/O) calls for a dual-resolution planning grid that couples five-year spectrum physics with six-month silicon sprints.'' The paper closes with balanced, action-oriented recommendations for operators, vendors and researchers on sub-THz fronthaul, AI-native security, energy-proportional accelerators and zero-touch assurance.         ",
    "url": "https://arxiv.org/abs/2506.09878",
    "authors": [
      "Ryan Barker"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2506.11234",
    "title": "Poutine: Vision-Language-Trajectory Pre-Training and Reinforcement Learning Post-Training Enable Robust End-to-End Autonomous Driving",
    "abstract": "           We present Poutine, a 3B-parameter vision-language model (VLM) tailored for end-to-end autonomous driving in long-tail driving scenarios. Poutine is trained in two stages. To obtain strong base driving capabilities, we train Poutine-Base in a self-supervised vision-language-trajectory (VLT) next-token prediction fashion on 83 hours of CoVLA nominal driving and 11 hours of Waymo long-tail driving. Accompanying language annotations are auto-generated with a 72B-parameter VLM. Poutine is obtained by fine-tuning Poutine-Base with Group Relative Policy Optimization (GRPO) using less than 500 preference-labeled frames from the Waymo validation set. We show that both VLT pretraining and RL fine-tuning are critical to attain strong driving performance in the long-tail. Poutine-Base achieves a rater-feedback score (RFS) of 8.12 on the validation set, nearly matching Waymo's expert ground-truth RFS. The final Poutine model achieves an RFS of 7.99 on the official Waymo test set, placing 1st in the 2025 Waymo Vision-Based End-to-End Driving Challenge by a significant margin. These results highlight the promise of scalable VLT pre-training and lightweight RL fine-tuning to enable robust and generalizable autonomy.         ",
    "url": "https://arxiv.org/abs/2506.11234",
    "authors": [
      "Luke Rowe",
      "Rodrigue de Schaetzen",
      "Roger Girgis",
      "Christopher Pal",
      "Liam Paull"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.13032",
    "title": "AS400-DET: Detection using Deep Learning Model for IBM i (AS/400)",
    "abstract": "           This paper proposes a method for automatic GUI component detection for the IBM i system (formerly and still more commonly known as AS/400). We introduce a human-annotated dataset consisting of 1,050 system screen images, in which 381 images are screenshots of IBM i system screens in Japanese. Each image contains multiple components, including text labels, text boxes, options, tables, instructions, keyboards, and command lines. We then develop a detection system based on state-of-the-art deep learning models and evaluate different approaches using our dataset. The experimental results demonstrate the effectiveness of our dataset in constructing a system for component detection from GUI screens. By automatically detecting GUI components from the screen, AS400-DET has the potential to perform automated testing on systems that operate via GUI screens.         ",
    "url": "https://arxiv.org/abs/2506.13032",
    "authors": [
      "Thanh Tran",
      "Son T. Luu",
      "Quan Bui",
      "Shoshin Nomura"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.17253",
    "title": "MS-DFTVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Deformable Convolution",
    "abstract": "           Research on long-term time series prediction has primarily relied on Transformer and MLP models, while the potential of convolutional networks in this domain remains underexplored. To address this, we propose a novel multi-scale time series reshape module that effectively captures cross-period patch interactions and variable dependencies. Building on this, we develop MS-DFTVNet, the multi-scale 3D deformable convolutional framework tailored for long-term forecasting. Moreover, to handle the inherently uneven distribution of temporal features, we introduce a context-aware dynamic deformable convolution mechanism, which further enhances the model's ability to capture complex temporal patterns. Extensive experiments demonstrate that MS-DFTVNet not only significantly outperforms strong baselines but also achieves an average improvement of about 7.5% across six public datasets, setting new state-of-the-art results.         ",
    "url": "https://arxiv.org/abs/2506.17253",
    "authors": [
      "Chenghan Li",
      "Mingchen Li",
      "Yipu Liao",
      "Ruisheng Diao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.23274",
    "title": "Towards a Progress Bar for Reasoning: Progress Prediction in Large Reasoning Models",
    "abstract": "           Reasoning models that produce long, hidden chains of thought, have emerged as powerful tools for reasoning-intensive and agentic tasks. However, as the time horizons at which these models can operate grow exponentially, it becomes increasingly difficult to know how much progress the model is making on a task, making it challenging for users to set appropriate expectations about completion time. By probing the internal representations of Large Language Models (LLMs), we find evidence that their reasoning progress can be quantified, with simple linear probes achieving 30\\% accuracy over 10 progress classes and Mean Absolute Error (MAE) of 1.75. Rooted in this insight, we propose a two-stage fine-tuning method that trains existing reasoning models to explicitly generate progress estimates (0-100\\%) during their reasoning process. We find that the predictions of our best fine-tuned language model for sequences below 16K tokens are on average 10\\% from the true label.         ",
    "url": "https://arxiv.org/abs/2506.23274",
    "authors": [
      "Hans Peter Lynsg\u00f8e Raaschou-jensen",
      "Constanza Fierro",
      "Anders S\u00f8gaard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.09095",
    "title": "Temporal Misalignment Attacks against Multimodal Perception in Autonomous Driving",
    "abstract": "           Multimodal fusion (MMF) plays a critical role in the perception of autonomous driving, which primarily fuses camera and LiDAR streams for a comprehensive and efficient scene understanding. However, its strict reliance on precise temporal synchronization exposes it to new vulnerabilities. In this paper, we introduce DejaVu, an attack that exploits the in-vehicular network and induces delays across sensor streams to create subtle temporal misalignments, severely degrading downstream MMF-based perception tasks. Our comprehensive attack analysis across different models and datasets reveals the sensors' task-specific imbalanced sensitivities: object detection is overly dependent on LiDAR inputs, while object tracking is highly reliant on the camera inputs. Consequently, with a single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to 88.5%, while with a three-frame camera delay, multiple object tracking accuracy (MOTA) for car drops by 73%. We further demonstrated two attack scenarios using an automotive Ethernet testbed for hardware-in-the-loop validation and the Autoware stack for end-to-end AD simulation, demonstrating the feasibility of the DejaVu attack and its severe impact, such as collisions and phantom braking.         ",
    "url": "https://arxiv.org/abs/2507.09095",
    "authors": [
      "Md Hasan Shahriar",
      "Md Mohaimin Al Barat",
      "Harshavardhan Sundar",
      "Ning Zhang",
      "Naren Ramakrishnan",
      "Y. Thomas Hou",
      "Wenjing Lou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.09382",
    "title": "Fair CCA for Fair Representation Learning: An ADNI Study",
    "abstract": "           Canonical correlation analysis (CCA) is a technique for finding correlations between different data modalities and learning low-dimensional representations. As fairness becomes crucial in machine learning, fair CCA has gained attention. However, previous approaches often overlook the impact on downstream classification tasks, limiting applicability. We propose a novel fair CCA method for fair representation learning, ensuring the projected features are independent of sensitive attributes, thus enhancing fairness without compromising accuracy. We validate our method on synthetic data and real-world data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating its ability to maintain high correlation analysis performance while improving fairness in classification tasks. Our work enables fair machine learning in neuroimaging studies where unbiased analysis is essential. Code is available in this https URL.         ",
    "url": "https://arxiv.org/abs/2507.09382",
    "authors": [
      "Bojian Hou",
      "Zhanliang Wang",
      "Zhuoping Zhou",
      "Boning Tong",
      "Zexuan Wang",
      "Jingxuan Bao",
      "Duy Duong-Tran",
      "Qi Long",
      "Li Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2507.11920",
    "title": "Heterogeneous Predictor-based Risk-Aware Planning with Conformal Prediction in Dense, Uncertain Environments",
    "abstract": "           Real-time planning among many uncertain, dynamic obstacles is challenging because predicting every agent with high fidelity is both unnecessary and computationally expensive. We present Heterogeneous Predictor-based Risk-Aware Planning (H-PRAP), a framework that allocates prediction effort to where it matters. H-PRAP introduces the Probability-based Collision Risk Index (P-CRI), a closed-form, horizon-level collision index obtained by calibrating a Gaussian surrogate with conformal prediction. P-CRI drives a router that assigns high-risk obstacles to accurate but expensive predictors and low-risk obstacles to lightweight predictors, while preserving distribution-free coverage across heterogeneous predictors through conformal prediction. The selected predictions and their conformal radii are embedded in a chance-constrained model predictive control (MPC) problem, yielding receding-horizon policies with explicit safety margins. We analyze the safety-efficiency trade-off under prediction compute budget: more portion of low-fidelity predictions reduce residual risk from dropped obstacles, but in the same time induces larger conformal radii and degrades trajectory efficiency and shrinks MPC feasibility. Extensive numerical simulations in dense, uncertain environments validate that H-PRAP attains best balance between trajectory success rate (i.e., no collisions) and the time to reach the goal (i.e., trajectory efficiency) compared to single prediction architectures.         ",
    "url": "https://arxiv.org/abs/2507.11920",
    "authors": [
      "Jeongyong Yang",
      "KwangBin Lee",
      "SooJean Han"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.03199",
    "title": "Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models",
    "abstract": "           Research on bias in Text-to-Image (T2I) models has primarily focused on demographic representation and stereotypical attributes, overlooking a fundamental question: how does grammatical gender influence visual representation across languages? We introduce a cross-linguistic benchmark examining words where grammatical gender contradicts stereotypical gender associations (e.g., ``une sentinelle'' - grammatically feminine in French but referring to the stereotypically masculine concept ``guard''). Our dataset spans five gendered languages (French, Spanish, German, Italian, Russian) and two gender-neutral control languages (English, Chinese), comprising 800 unique prompts that generated 28,800 images across three state-of-the-art T2I models. Our analysis reveals that grammatical gender dramatically influences image generation: masculine grammatical markers increase male representation to 73% on average (compared to 22% with gender-neutral English), while feminine grammatical markers increase female representation to 38% (compared to 28% in English). These effects vary systematically by language resource availability and model architecture, with high-resource languages showing stronger effects. Our findings establish that language structure itself, not just content, shapes AI-generated visual outputs, introducing a new dimension for understanding bias and fairness in multilingual, multimodal systems.         ",
    "url": "https://arxiv.org/abs/2508.03199",
    "authors": [
      "Muhammed Saeed",
      "Shaina Raza",
      "Ashmal Vayani",
      "Muhammad Abdul-Mageed",
      "Ali Emami",
      "Shady Shehata"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.07452",
    "title": "Stackelberg Coupling of Online Representation Learning and Reinforcement Learning",
    "abstract": "           Deep Q-learning jointly learns representations and values within monolithic networks, promising beneficial co-adaptation between features and value estimates. Although this architecture has attained substantial success, the coupling between representation and value learning creates instability as representations must constantly adapt to non-stationary value targets, while value estimates depend on these shifting representations. This is compounded by high variance in bootstrapped targets, which causes bias in value estimation in off-policy methods. We introduce Stackelberg Coupled Representation and Reinforcement Learning (SCORER), a framework for value-based RL that views representation and Q-learning as two strategic agents in a hierarchical game. SCORER models the Q-function as the leader, which commits to its strategy by updating less frequently, while the perception network (encoder) acts as the follower, adapting more frequently to learn representations that minimize Bellman error variance given the leader's committed strategy. Through this division of labor, the Q-function minimizes MSBE while perception minimizes its variance, thereby reducing bias accordingly, with asymmetric updates allowing stable co-adaptation, unlike simultaneous parameter updates in monolithic solutions. Our proposed SCORER framework leads to a bi-level optimization problem whose solution is approximated by a two-timescale algorithm that creates an asymmetric learning dynamic between the two players. Extensive experiments on DQN and its variants demonstrate that gains stem from algorithmic insight rather than model complexity.         ",
    "url": "https://arxiv.org/abs/2508.07452",
    "authors": [
      "Fernando Martinez",
      "Tao Li",
      "Yingdong Lu",
      "Juntao Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.08172",
    "title": "Neural Logic Networks for Interpretable Classification",
    "abstract": "           Traditional neural networks have an impressive classification performance, but what they learn cannot be inspected, verified or extracted. Neural Logic Networks on the other hand have an interpretable structure that enables them to learn a logical mechanism relating the inputs and outputs with AND and OR operations. We generalize these networks with NOT operations and biases that take into account unobserved data and develop a rigorous logical and probabilistic modeling in terms of concept combinations to motivate their use. We also propose a novel factorized IF-THEN rule structure for the model as well as a modified learning algorithm. Our method improves the state-of-the-art in Boolean networks discovery and is able to learn relevant, interpretable rules in tabular classification, notably on examples from the medical and industrial fields where interpretability has tangible value.         ",
    "url": "https://arxiv.org/abs/2508.08172",
    "authors": [
      "Vincent Perreault",
      "Katsumi Inoue",
      "Richard Labib",
      "Alain Hertz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2508.09346",
    "title": "How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy",
    "abstract": "           Autonomous robots that rely on deep neural network controllers pose critical challenges for safety prediction, especially under partial observability and distribution shift. Traditional model-based verification techniques are limited in scalability and require access to low-dimensional state models, while model-free methods often lack reliability guarantees. This paper addresses these limitations by introducing a framework for calibrated safety prediction in end-to-end vision-controlled systems, where neither the state-transition model nor the observation model is accessible. Building on the foundation of world models, we leverage variational autoencoders and recurrent predictors to forecast future latent trajectories from raw image sequences and estimate the probability of satisfying safety properties. We distinguish between monolithic and composite prediction pipelines and introduce a calibration mechanism to quantify prediction confidence. In long-horizon predictions from high-dimensional observations, the forecasted inputs to the safety evaluator can deviate significantly from the training distribution due to compounding prediction errors and changing environmental conditions, leading to miscalibrated risk estimates. To address this, we incorporate unsupervised domain adaptation to ensure robustness of safety evaluation under distribution shift in predictions without requiring manual labels. Our formulation provides theoretical calibration guarantees and supports practical evaluation across long prediction horizons. Experimental results on three benchmarks show that our UDA-equipped evaluators maintain high accuracy and substantially lower false positive rates under distribution shift. Similarly, world model-based composite predictors outperform their monolithic counterparts on long-horizon tasks, and our conformal calibration provides reliable statistical bounds.         ",
    "url": "https://arxiv.org/abs/2508.09346",
    "authors": [
      "Zhenjiang Mao",
      "Mrinall Eashaan Umasudhan",
      "Ivan Ruchkin"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.14746",
    "title": "MissionHD: Hyperdimensional Refinement of Distribution-Deficient Reasoning Graphs for Video Anomaly Detection",
    "abstract": "           LLM-generated reasoning graphs, referred to as mission-specific graphs (MSGs), are increasingly used for video anomaly detection (VAD) and recognition (VAR). These MSGs are novel artifacts: they often exhibit skewed connectivity and lack large-scale datasets for pre-training, which makes existing graph structure refinement (GSR) methods ineffective. To address this challenge, we propose HDC-constrained Graph Structure Refinement (HDC-GSR), a paradigm that leverages hyperdimensional computing (HDC) to optimize decodable graph representations without relying on structural-distribution learning. Building on this paradigm, we introduce MissionHD, an HDC framework that encodes graphs with constrained graph-neural operations, aligns them directly with downstream task loss, and decodes refined structures. Experiments on VAD/VAR benchmarks demonstrate that MissionHD-refined graphs consistently improve performance, establishing HDC-GSR as an effective pre-processing step for structured reasoning in video anomaly tasks.         ",
    "url": "https://arxiv.org/abs/2508.14746",
    "authors": [
      "Sanggeon Yun",
      "Raheeb Hassan",
      "Ryozo Masukawa",
      "Nathaniel D. Bastian",
      "Mohsen Imani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.15051",
    "title": "Robust Estimation Under Heterogeneous Corruption Rates",
    "abstract": "           We study the problem of robust estimation under heterogeneous corruption rates, where each sample may be independently corrupted with a known but non-identical probability. This setting arises naturally in distributed and federated learning, crowdsourcing, and sensor networks, yet existing robust estimators typically assume uniform or worst-case corruption, ignoring structural heterogeneity. For mean estimation for multivariate bounded distributions and univariate gaussian distributions, we give tight minimax rates for all heterogeneous corruption patterns. For multivariate gaussian mean estimation and linear regression, we establish the minimax rate for squared error up to a factor of $\\sqrt{d}$, where $d$ is the dimension. Roughly, our findings suggest that samples beyond a certain corruption threshold may be discarded by the optimal estimators -- this threshold is determined by the empirical distribution of the corruption rates given.         ",
    "url": "https://arxiv.org/abs/2508.15051",
    "authors": [
      "Syomantak Chaudhuri",
      "Jerry Li",
      "Thomas A. Courtade"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2508.17324",
    "title": "CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation",
    "abstract": "           In this paper, we report our participation to the PalmX cultural evaluation shared task. Our system, CultranAI, focused on data augmentation and LoRA fine-tuning of large language models (LLMs) for Arabic cultural knowledge representation. We benchmarked several LLMs to identify the best-performing model for the task. In addition to utilizing the PalmX dataset, we augmented it by incorporating the Palm dataset and curated a new dataset of over 22K culturally grounded multiple-choice questions (MCQs). Our experiments showed that the Fanar-1-9B-Instruct model achieved the highest performance. We fine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the blind test set, our submitted system ranked 5th with an accuracy of 70.50%, while on the PalmX development set, it achieved an accuracy of 84.1%.         ",
    "url": "https://arxiv.org/abs/2508.17324",
    "authors": [
      "Hunzalah Hassan Bhatti",
      "Youssef Ahmed",
      "Md Arid Hasan",
      "Firoj Alam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.18729",
    "title": "Are All Marine Species Created Equal? Performance Disparities in Underwater Object Detection",
    "abstract": "           Underwater object detection is critical for monitoring marine ecosystems but poses unique challenges, including degraded image quality, imbalanced class distribution, and distinct visual characteristics. Not every species is detected equally well, yet underlying causes remain unclear. We address two key research questions: 1) What factors beyond data quantity drive class-specific performance disparities? 2) How can we systematically improve detection of under-performing marine species? We manipulate the DUO and RUOD datasets to separate the object detection task into localization and classification and investigate the under-performance of the scallop class. Localization analysis using YOLO11 and TIDE finds that foreground-background discrimination is the most problematic stage regardless of data quantity. Classification experiments reveal persistent precision gaps even with balanced data, indicating intrinsic feature-based challenges beyond data scarcity and inter-class dependencies. We recommend imbalanced distributions when prioritizing precision, and balanced distributions when prioritizing recall. Improving under-performing classes should focus on algorithmic advances, especially within localization modules. We publicly release our code and datasets.         ",
    "url": "https://arxiv.org/abs/2508.18729",
    "authors": [
      "Melanie Wille",
      "Tobias Fischer",
      "Scarlett Raine"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.01750",
    "title": "Communication-Aware Knowledge Distillation for Federated LLM Fine-Tuning over Wireless Networks",
    "abstract": "           Federated learning (FL) for large language models (LLMs) offers a privacy-preserving scheme, enabling clients to collaboratively fine-tune locally deployed LLMs or smaller language models (SLMs) without exchanging raw data. While parameter-sharing methods in traditional FL models solves number of technical challenges, they still incur high communication overhead and struggle with adapting to heterogeneous model architectures. Federated distillation, a framework for mutual knowledge transfer via shared logits, typically offers lower communication overhead than parameter-sharing methods. However, transmitting logits from LLMs remains challenging for bandwidth-limited clients due to their high dimensionality. In this work, we focus on a federated LLM distillation with efficient communication overhead. To achieve this, we first propose an adaptive Top-k logit selection mechanism, dynamically sparsifying logits according to real-time communication conditions. Then to tackle the dimensional inconsistency introduced by the adaptive sparsification, we design an adaptive logits aggregation scheme, effectively alleviating the artificial and uninformative inputs introduced by conventional zero-padding methods. Finally, to enhance the distillation effect, we incorporate LoRA-adapted hidden-layer projection from LLM into the distillation loss, reducing the communication overhead further while providing richer representation. Experimental results demonstrate that our scheme achieves superior performance compared to baseline methods while effectively reducing communication overhead by approximately 50%.         ",
    "url": "https://arxiv.org/abs/2509.01750",
    "authors": [
      "Xinlu Zhang",
      "Na Yan",
      "Yang Su",
      "Yansha Deng",
      "Toktam Mahmoodi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.05162",
    "title": "Verifiability and Privacy in Federated Learning through Context-Hiding Multi-Key Homomorphic Authenticators",
    "abstract": "           Federated Learning has rapidly expanded from its original inception to now have a large body of research, several frameworks, and sold in a variety of commercial offerings. Thus, its security and robustness is of significant importance. There are many algorithms that provide robustness in the case of malicious clients. However, the aggregator itself may behave maliciously, for example, by biasing the model or tampering with the weights to weaken the models privacy. In this work, we introduce a verifiable federated learning protocol that enables clients to verify the correctness of the aggregators computation without compromising the confidentiality of their updates. Our protocol uses a standard secure aggregation technique to protect individual model updates with a linearly homomorphic authenticator scheme that enables efficient, privacy-preserving verification of the aggregated result. Our construction ensures that clients can detect manipulation by the aggregator while maintaining low computational overhead. We demonstrate that our approach scales to large models, enabling verification over large neural networks with millions of parameters.         ",
    "url": "https://arxiv.org/abs/2509.05162",
    "authors": [
      "Simone Bottoni",
      "Giulio Zizzo",
      "Stefano Braghin",
      "Alberto Trombetta"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.05378",
    "title": "Code Like Humans: A Multi-Agent Solution for Medical Coding",
    "abstract": "           In medical coding, experts map unstructured clinical notes to alphanumeric codes for diagnoses and procedures. We introduce Code Like Humans: a new agentic framework for medical coding with large language models. It implements official coding guidelines for human experts, and it is the first solution that can support the full ICD-10 coding system (+70K labels). It achieves the best performance to date on rare diagnosis codes (fine-tuned discriminative classifiers retain an advantage for high-frequency codes, to which they are limited). Towards future work, we also contribute an analysis of system performance and identify its `blind spots' (codes that are systematically undercoded).         ",
    "url": "https://arxiv.org/abs/2509.05378",
    "authors": [
      "Andreas Motzfeldt",
      "Joakim Edin",
      "Casper L. Christensen",
      "Christian Hardmeier",
      "Lars Maal\u00f8e",
      "Anna Rogers"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2509.06920",
    "title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection",
    "abstract": "           Insider threats are a growing organizational problem due to the complexity of identifying their technical and behavioral elements. A large research body is dedicated to the study of insider threats from technological, psychological, and educational perspectives. However, research in this domain has been generally dependent on datasets that are static and limited access which restricts the development of adaptive detection models. This study introduces a novel, ethically grounded approach that uses the large language model (LLM) Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which contain indicators of insider threat scenarios. The messages reflect real-world data distributions by being highly imbalanced (1% insider threats). The syslogs were analyzed for insider threats by both Sonnet 3.7 and GPT-4o, with their performance evaluated through statistical metrics including accuracy, precision, recall, F1, specificity, FAR, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across nearly all metrics, particularly in reducing false alarms and improving detection accuracy. The results show strong promise for the use of LLMs in synthetic dataset generation and insider threat detection.         ",
    "url": "https://arxiv.org/abs/2509.06920",
    "authors": [
      "Haywood Gelman",
      "John D. Hastings",
      "David Kenley"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2509.10240",
    "title": "Cooperative Base Station Assignment and Resource Allocation for 6G ISAC Network",
    "abstract": "           In the upcoming 6G networks, integrated sensing and communications (ISAC) will be able to provide a performance boost in both perception and wireless connectivity. This paper considers a multiple base station (BS) architecture to support the comprehensive services of data transmission and multi-target sensing. In this context, a cooperative BS assignment and resource allocation (CBARA) strategy is proposed in this paper, aiming at jointly optimizing the communication and sensing (C&S) performance. The posterior Cramer-Rao lower bound and the achievable rate with respect to transmit power and bandwidth are derived and utilized as optimization criteria for the CBARA scheme. We develop a heuristic alternating optimization algorithm to obtain an effective sub-optimal solution for the non-convex optimization problem caused by multiple coupled variables. Numerical results show the effectiveness of the proposed solution, which achieves a performance improvement of 117% in communication rate and 40% in sensing accuracy, compared to the classic scheme.         ",
    "url": "https://arxiv.org/abs/2509.10240",
    "authors": [
      "Jiajia Liao",
      "Luping Xiang",
      "Shida Zhong",
      "Lixia Xiao",
      "Haochen Liu",
      "Kun Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2509.11149",
    "title": "RoVerFly: Robust and Versatile Implicit Hybrid Control of Quadrotor-Payload Systems",
    "abstract": "           Designing robust controllers for precise trajectory tracking with quadrotors is challenging due to nonlinear dynamics and underactuation, and becomes harder with flexible cable-suspended payloads that add degrees of freedom and hybrid dynamics. Classical model-based methods offer stability guarantees but require extensive tuning and often fail to adapt when the configuration changes-when a payload is added or removed, or when its mass or cable length varies. We present RoVerFly, a unified learning-based control framework where a single reinforcement learning (RL) policy functions as an implicit hybrid controller, managing complex dynamics without explicit mode detection or controller switching. Trained with task and domain randomization, the controller is resilient to disturbances and varying dynamics. It achieves strong zero-shot generalization across payload settings-including no payload as well as varying mass and cable length-without re-tuning, while retaining the interpretability and structure of a feedback tracking controller. Code and supplementary materials are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.11149",
    "authors": [
      "Mintae Kim",
      "Jiaze Cai",
      "Koushil Sreenath"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.12544",
    "title": "Neural Collapse-Inspired Multi-Label Federated Learning under Label-Distribution Skew",
    "abstract": "           Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy. However, the performance of deep learning often deteriorates in FL due to decentralized and heterogeneous data. This challenge is further amplified in multi-label scenarios, where data exhibit complex characteristics such as label co-occurrence, inter-label dependency, and discrepancies between local and global label relationships. While most existing FL research primarily focuses on single-label classification, many real-world applications, particularly in domains such as medical imaging, often involve multi-label settings. In this paper, we address this important yet underexplored scenario in FL, where clients hold multi-label data with skewed label distributions. Neural Collapse (NC) describes a geometric structure in the latent feature space where features of each class collapse to their class mean with vanishing intra-class variance, and the class means form a maximally separated configuration. Motivated by this theory, we propose a method to align feature distributions across clients and to learn high-quality, well-clustered representations. To make the NC-structure applicable to multi-label settings, where image-level features may contain multiple semantic concepts, we introduce a feature disentanglement module that extracts semantically specific features. The clustering of these disentangled class-wise features is guided by a predefined shared NC structure, which mitigates potential conflicts between client models due to diverse local data distributions. In addition, we design regularisation losses to encourage compact clustering in the latent feature space. Experiments conducted on four benchmark datasets across eight diverse settings demonstrate that our approach outperforms existing methods, validating its effectiveness in this challenging FL scenario.         ",
    "url": "https://arxiv.org/abs/2509.12544",
    "authors": [
      "Can Peng",
      "Yuyuan Liu",
      "Yingyu Yang",
      "Pramit Saha",
      "Qianye Yang",
      "J. Alison Noble"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.14285",
    "title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks",
    "abstract": "           Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inputs can override system prompts and induce unintended behaviors. This paper presents a novel multi-agent defense framework that employs specialized LLM agents in coordinated pipelines to detect and neutralize prompt injection attacks in real-time. We evaluate our approach using two distinct architectures: a sequential chain-of-agents pipeline and a hierarchical coordinator-based system. Our comprehensive evaluation on 55 unique prompt injection attacks, grouped into 8 categories and totaling 400 attack instances across two LLM platforms (ChatGLM and Llama2), demonstrates significant security improvements. Without defense mechanisms, baseline Attack Success Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent pipeline achieved 100% mitigation, reducing ASR to 0% across all tested scenarios. The framework demonstrates robustness across multiple attack categories including direct overrides, code execution attempts, data exfiltration, and obfuscation techniques, while maintaining system functionality for legitimate queries.         ",
    "url": "https://arxiv.org/abs/2509.14285",
    "authors": [
      "S M Asif Hossain",
      "Ruksat Khan Shayoni",
      "Mohd Ruhul Ameen",
      "Akif Islam",
      "M. F. Mridha",
      "Jungpil Shin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.15935",
    "title": "PAN: Pillars-Attention-Based Network for 3D Object Detection",
    "abstract": "           Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar fusion for the 3D object detection task in real-time under adverse weather and lighting conditions. However, currently, in the literature, it is possible to find few works focusing on this modality and, most importantly, developing new architectures to explore the advantages of the radar point cloud, such as accurate distance estimation and speed information. Therefore, this work presents a novel and efficient 3D object detection algorithm using cameras and radars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of radar before fusing the features into a detection head. A new backbone is introduced, which maps the radar pillar features into an embedded dimension. A self-attention mechanism allows the backbone to model the dependencies between the radar points. We are using a simplified convolutional layer to replace the FPN-based convolutional layers used in the PointPillars-based architectures with the main goal of reducing inference time. Our results show that with this modification, our approach achieves the new state-of-the-art in the 3D object detection problem, reaching 58.2 of the NDS metric for the use of ResNet-50, while also setting a new benchmark for inference time on the nuScenes dataset for the same category.         ",
    "url": "https://arxiv.org/abs/2509.15935",
    "authors": [
      "Ruan Bispo",
      "Dane Mitrev",
      "Letizia Mariotti",
      "Cl\u00e9ment Botty",
      "Denver Humphrey",
      "Anthony Scanlan",
      "Ciar\u00e1n Eising"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.16602",
    "title": "FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection",
    "abstract": "           Multi-step or hybrid deepfakes, created by sequentially applying different deepfake creation methods such as Face-Swapping, GAN-based generation, and Diffusion methods, can pose an emerging and unforseen technical challenge for detection models trained on single-step forgeries. While prior studies have mainly focused on detecting isolated single manipulation, little is known about the detection model behavior under such compositional, hybrid, and complex manipulation pipelines. In this work, we introduce \\textbf{FakeChain}, a large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using five state-of-the-art representative generators. Using this approach, we analyze detection performance and spectral properties across hybrid manipulation at different step, along with varying generator combinations and quality settings. Surprisingly, our findings reveal that detection performance highly depends on the final manipulation type, with F1-score dropping by up to \\textbf{58.83\\%} when it differs from training distribution. This clearly demonstrates that detectors rely on last-stage artifacts rather than cumulative manipulation traces, limiting generalization. Such findings highlight the need for detection models to explicitly consider manipulation history and sequences. Our results highlight the importance of benchmarks such as FakeChain, reflecting growing synthesis complexity and diversity in real-world scenarios. Our sample code is available here\\footnote{this https URL}.         ",
    "url": "https://arxiv.org/abs/2509.16602",
    "authors": [
      "Minji Heo",
      "Simon S. Woo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.17871",
    "title": "B-Privacy: Defining and Enforcing Privacy in Weighted Voting",
    "abstract": "           In traditional, one-vote-per-person voting systems, privacy equates with ballot secrecy: voting tallies are published, but individual voters' choices are concealed. Voting systems that weight votes in proportion to token holdings, though, are now prevalent in cryptocurrency and web3 systems. We show that these weighted-voting systems overturn existing notions of voter privacy. Our experiments demonstrate that even with secret ballots, publishing raw tallies often reveals voters' choices. Weighted voting thus requires a new framework for privacy. We introduce a notion called B-privacy whose basis is bribery, a key problem in voting systems today. B-privacy captures the economic cost to an adversary of bribing voters based on revealed voting tallies. We propose a mechanism to boost B-privacy by noising voting tallies. We prove bounds on its tradeoff between B-privacy and transparency, meaning reported-tally accuracy. Analyzing 3,582 proposals across 30 Decentralized Autonomous Organizations (DAOs), we find that the prevalence of large voters (\"whales\") limits the effectiveness of any B-Privacy-enhancing technique. However, our mechanism proves to be effective in cases without extreme voting weight concentration: among proposals requiring coalitions of $\\geq5$ voters to flip outcomes, our mechanism raises B-privacy by a geometric mean factor of $4.1\\times$. Our work offers the first principled guidance on transparency-privacy tradeoffs in weighted-voting systems, complementing existing approaches that focus on ballot secrecy and revealing fundamental constraints that voting weight concentration imposes on privacy mechanisms.         ",
    "url": "https://arxiv.org/abs/2509.17871",
    "authors": [
      "Samuel Breckenridge",
      "Dani Vilardell",
      "Andr\u00e9s F\u00e1brega",
      "Amy Zhao",
      "Patrick McCorry",
      "Rafael Solari",
      "Ari Juels"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.20460",
    "title": "Differential Privacy of Network Parameters from a System Identification Perspective",
    "abstract": "           This paper addresses the problem of protecting network information from privacy system identification (SI) attacks when sharing cyber-physical system simulations. We model analyst observations of networked states as time-series outputs of a graph filter driven by differentially private (DP) nodal excitations, with the analyst aiming to infer the underlying graph shift operator (GSO). Unlike traditional SI, which estimates system parameters, we study the inverse problem: what assumptions prevent adversaries from identifying the GSO while preserving utility for legitimate analysis. We show that applying DP mechanisms to inputs provides formal privacy guarantees for the GSO, linking the $(\\epsilon,\\delta)$-DP bound to the spectral properties of the graph filter and noise covariance. More precisely, for DP Gaussian signals, the spectral characteristics of both the filter and noise covariance determine the privacy bound, with smooth filters and low-condition-number covariance yielding greater privacy.         ",
    "url": "https://arxiv.org/abs/2509.20460",
    "authors": [
      "Andrew Campbell",
      "Anna Scaglione",
      "Hang Liu",
      "Victor Elvira",
      "Sean Peisert",
      "Daniel Arnold"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2509.23019",
    "title": "LLM Watermark Evasion via Bias Inversion",
    "abstract": "           Watermarking for large language models (LLMs) embeds a statistical signal during generation to enable detection of model-produced text. While watermarking has proven effective in benign settings, its robustness under adversarial evasion remains contested. To advance a rigorous understanding and evaluation of such vulnerabilities, we propose the \\emph{Bias-Inversion Rewriting Attack} (BIRA), which is theoretically motivated and model-agnostic. BIRA weakens the watermark signal by suppressing the logits of likely watermarked tokens during LLM-based rewriting, without any knowledge of the underlying watermarking scheme. Across recent watermarking methods, BIRA achieves over 99\\% evasion while preserving the semantic content of the original text. Beyond demonstrating an attack, our results reveal a systematic vulnerability, emphasizing the need for stress testing and robust defenses.         ",
    "url": "https://arxiv.org/abs/2509.23019",
    "authors": [
      "Jeongyeon Hwang",
      "Sangdon Park",
      "Jungseul Ok"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.23234",
    "title": "$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding",
    "abstract": "           Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments.         ",
    "url": "https://arxiv.org/abs/2509.23234",
    "authors": [
      "Runyan Tan",
      "Shuang Wu",
      "Phillip Howard"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.23652",
    "title": "ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models through Agentic Data Synthesis",
    "abstract": "           While Reinforcement Learning with Verifiable Reward (RLVR) significantly advances image reasoning in Large Vision-Language Models (LVLMs), its application to complex video reasoning remains underdeveloped. This gap stems primarily from a critical data bottleneck: existing datasets lack the challenging, multi-hop questions and high-quality, video-grounded Chain-of-Thought (CoT) data necessary to effectively bootstrap RLVR. To address this, we introduce ReWatch, a large-scale dataset built to foster advanced video reasoning. We propose a novel multi-stage synthesis pipeline to synthesize its three components: ReWatch-Caption, ReWatch-QA, and ReWatch-CoT. A core innovation is our Multi-Agent ReAct framework for CoT synthesis, which simulates a human-like \"re-watching\" process to generate video-grounded reasoning traces by explicitly modeling information retrieval and verification. Building on this dataset, we develop ReWatch-R1 by post-training a strong baseline LVLM with Supervised Fine-Tuning (SFT) and our RLVR framework. This framework incorporates a novel Observation \\& Reasoning (O\\&R) reward mechanism that evaluates both the final answer's correctness and the reasoning's alignment with video content, directly penalizing hallucination. Our experiments show that ReWatch-R1 achieves state-of-the-art average performance on five challenging video reasoning benchmarks. Project Page: this https URL ",
    "url": "https://arxiv.org/abs/2509.23652",
    "authors": [
      "Congzhi Zhang",
      "Zhibin Wang",
      "Yinchao Ma",
      "Jiawei Peng",
      "Yihan Wang",
      "Qiang Zhou",
      "Jun Song",
      "Bo Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.24159",
    "title": "Latent Collective Preference Optimization: A General Framework for Robust LLM Alignment",
    "abstract": "           Standard human preference-based alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), are a cornerstone technology for aligning Large Language Models (LLMs) with human values. However, these methods are all underpinned by a critical, yet flawed assumption: human preferences are homogeneous (representing a single, unified preference) and the collected data is noiseless (free from error). In reality, neither is true since human preference is pluralistic and annotators can make mistakes. This creates a discrepancy between the recorded data and the ground-truth preferences, which can misguide the model and degrade its performance. To address this challenge, we introduce Latent Collective Preference Optimization (LCPO). LCPO leverages an Expectation-Maximization (EM) algorithm to learn the latent collective consensus from noisy data. It operates by inferring the correctness of each preference label and using this probability as an adaptive weight to re-calibrate each data point's contribution to the training loss, thereby mitigating noise. We generalize this approach by establishing a theoretical link between arbitrary preference losses and their corresponding probabilistic models, elevating LCPO from a specific algorithm to a general framework for robust preference alignment. Theoretically, we prove that under the condition of a perfectly calibrated model, LCPO is guaranteed to converge to the true noise level of the dataset. Our experiments demonstrate LCPO's effectiveness as a general framework, consistently enhancing four state-of-the-art alignment algorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3 models, the LCPO-enhanced methods achieve substantial win rate gains on AlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% on both benchmarks.         ",
    "url": "https://arxiv.org/abs/2509.24159",
    "authors": [
      "Xiaoyang Cao",
      "Zelai Xu",
      "Mo Guang",
      "Kaiwen Long",
      "Michiel A. Bakker",
      "Yu Wang",
      "Chao Yu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.25393",
    "title": "Multi-modal Spatio-Temporal Transformer for High-resolution Land Subsidence Prediction",
    "abstract": "           Forecasting high-resolution land subsidence is a critical yet challenging task due to its complex, non-linear dynamics. While standard architectures like ConvLSTM often fail to model long-range dependencies, we argue that a more fundamental limitation of prior work lies in the uni-modal data paradigm. To address this, we propose the Multi-Modal Spatio-Temporal Transformer (MM-STT), a novel framework that fuses dynamic displacement data with static physical priors. Its core innovation is a joint spatio-temporal attention mechanism that processes all multi-modal features in a unified manner. On the public EGMS dataset, MM-STT establishes a new state-of-the-art, reducing the long-range forecast RMSE by an order of magnitude compared to all baselines, including SOTA methods like STGCN and STAEformer. Our results demonstrate that for this class of problems, an architecture's inherent capacity for deep multi-modal fusion is paramount for achieving transformative performance.         ",
    "url": "https://arxiv.org/abs/2509.25393",
    "authors": [
      "Wendong Yao",
      "Binhua Huang",
      "Soumyabrata Dev"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.25477",
    "title": "The Rise of AfricaNLP: Contributions, Contributors, and Community Impact (2005-2025)",
    "abstract": "           Natural Language Processing (NLP) is undergoing constant transformation, as Large Language Models (LLMs) are driving daily breakthroughs in research and practice. In this regard, tracking the progress of NLP research and automatically analyzing the contributions of research papers provides key insights into the nature of the field and the researchers. This study explores the progress of African NLP (AfricaNLP) by asking (and answering) basic research questions such as: i) How has the nature of NLP evolved over the last two decades?, ii) What are the contributions of AfricaNLP papers?, and iii) Which individuals and organizations (authors, affiliated institutions, and funding bodies) have been involved in the development of AfricaNLP? We quantitatively examine the contributions of AfricaNLP research using 1.9K NLP paper abstracts, 4.9K author contributors, and 7.8K human-annotated contribution sentences (AfricaNLPContributions) along with benchmark results. Our dataset and continuously existing NLP progress tracking website provide a powerful lens for tracing AfricaNLP research trends and hold potential for generating data-driven literature surveys.         ",
    "url": "https://arxiv.org/abs/2509.25477",
    "authors": [
      "Tadesse Destaw Belay",
      "Kedir Yassin Hussen",
      "Sukairaj Hafiz Imam",
      "Ibrahim Said Ahmad",
      "Isa Inuwa-Dutse",
      "Abrham Belete Haile",
      "Grigori Sidorov",
      "Iqra Ameer",
      "Idris Abdulmumin",
      "Tajuddeen Gwadabe",
      "Vukosi Marivate",
      "Seid Muhie Yimam",
      "Shamsuddeen Hassan Muhammad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.25514",
    "title": "AGNOMIN -- Architecture Agnostic Multi-Label Function Name Prediction",
    "abstract": "           Function name prediction is crucial for understanding stripped binaries in software reverse engineering, a key step for \\textbf{enabling subsequent vulnerability analysis and patching}. However, existing approaches often struggle with architecture-specific limitations, data scarcity, and diverse naming conventions. We present AGNOMIN, a novel architecture-agnostic approach for multi-label function name prediction in stripped binaries. AGNOMIN builds Feature-Enriched Hierarchical Graphs (FEHGs), combining Control Flow Graphs, Function Call Graphs, and dynamically learned \\texttt{PCode} features. A hierarchical graph neural network processes this enriched structure to generate consistent function representations across architectures, vital for \\textbf{scalable security assessments}. For function name prediction, AGNOMIN employs a Ren\u00e9e-inspired decoder, enhanced with an attention-based head layer and algorithmic improvements. We evaluate AGNOMIN on a comprehensive dataset of 9,000 ELF executable binaries across three architectures, demonstrating its superior performance compared to state-of-the-art approaches, with improvements of up to 27.17\\% in precision and 55.86\\% in recall across the testing dataset. Moreover, AGNOMIN generalizes well to unseen architectures, achieving 5.89\\% higher recall than the closest baseline. AGNOMIN's practical utility has been validated through security hackathons, where it successfully aided reverse engineers in analyzing and patching vulnerable binaries across different architectures.         ",
    "url": "https://arxiv.org/abs/2509.25514",
    "authors": [
      "Yonatan Gizachew Achamyeleh",
      "Tongtao Zhang",
      "Joshua Hyunki Kim",
      "Gabriel Garcia",
      "Shih-Yuan Yu",
      "Anton Kocheturov",
      "Mohammad Abdullah Al Faruque"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.25868",
    "title": "ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations",
    "abstract": "           Large Language Models (LLMs) frequently confabulate scientific facts, severely undermining their trustworthiness. Addressing this challenge requires benchmarks that go beyond binary factuality and enable fine-grained evaluation. We introduce ReFACT (Reddit False And Correct Texts), a benchmark of 1,001 expert-annotated question-answer pairs spanning diverse scientific domains for the detection of scientific confabulation. Each instance includes both a scientifically correct answer and a non-factual counterpart annotated with precise error spans and error types. ReFACT enables multi-stage evaluation: (1) confabulation detection, (2) fine-grained error localization, and (3) correction. We benchmark 9 state-of-the-art LLMs, revealing limited performance (about 50 percent accuracy). Even top models such as GPT-4o fail to distinguish factual from confabulated scientific answers, raising concerns about the reliability of LLM-as-judge evaluation paradigms. Our findings highlight the need for fine-grained, human-validated benchmarks to detect and correct scientific confabulation in domain-specific contexts. The dataset is available at: this https URL ",
    "url": "https://arxiv.org/abs/2509.25868",
    "authors": [
      "Yindong Wang",
      "Martin Prei\u00df",
      "Margarita Bugue\u00f1o",
      "Jan Vincent Hoffbauer",
      "Abdullatif Ghajar",
      "Tolga Buz",
      "Gerard de Melo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.25914",
    "title": "ReNF: Rethinking the Design Space of Neural Long-Term Time Series Forecasters",
    "abstract": "           Neural Forecasters (NFs) are a cornerstone of Long-term Time Series Forecasting (LTSF). However, progress has been hampered by an overemphasis on architectural complexity at the expense of fundamental forecasting principles. In this work, we return to first principles to redesign the LTSF paradigm. We begin by introducing a Multiple Neural Forecasting Theorem that provides a theoretical basis for our approach. We propose Boosted Direct Output (BDO), a novel forecasting strategy that synergistically combines the advantages of both Auto-Regressive (AR) and Direct Output (DO). In addition, we stabilize the learning process by smoothly tracking the model's parameters. Extensive experiments show that these principled improvements enable a simple MLP to achieve state-of-the-art performance, outperforming recent, complex models in nearly all cases, without any specific considerations in the area. Finally, we empirically verify our theorem, establishing a dynamic performance bound and identifying promising directions for future research. The code for review is available at: .         ",
    "url": "https://arxiv.org/abs/2509.25914",
    "authors": [
      "Yihang Lu",
      "Xianwei Meng",
      "Enhong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.26234",
    "title": "Machine Learning Detection of Lithium Plating in Lithium-ion Cells: A Gaussian Process Approach",
    "abstract": "           Lithium plating during fast charging is a critical degradation mechanism that accelerates capacity fade and can trigger catastrophic safety failures. Recent work has identified a distinctive dQ/dV peak above 4.0 V as a reliable signature of plating onset; however, conventional methods for computing dQ/dV rely on finite differencing with filtering, which amplifies sensor noise and introduces bias in peak location. In this paper, we propose a Gaussian Process (GP) framework for lithium plating detection by directly modeling the charge-voltage relationship Q(V) as a stochastic process with calibrated uncertainty. Leveraging the property that derivatives of GPs remain GPs, we infer dQ/dV analytically and probabilistically from the posterior, enabling robust detection without ad hoc smoothing. The framework provides three key benefits: (i) noise-aware inference with hyperparameters learned from data, (ii) closed-form derivatives with credible intervals for uncertainty quantification, and (iii) scalability to online variants suitable for embedded BMS. Experimental validation on Li-ion coin cells across a range of C-rates (0.2C-1C) and temperatures (0-40\u00b0C) demonstrates that the GP-based method reliably detects plating peaks under low-temperature, high-rate charging, while correctly reporting no peaks in baseline cases. The concurrence of GP-identified differential peaks, reduced charge throughput, and capacity fade measured via reference performance tests confirms the method's accuracy and robustness, establishing a practical pathway for real-time lithium plating detection.         ",
    "url": "https://arxiv.org/abs/2509.26234",
    "authors": [
      "Ayush Patnaik",
      "Adam B Zufall",
      "Stephen K Robinson",
      "Xinfan Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.26272",
    "title": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection",
    "abstract": "           The rapid rise of synthetic media has made deepfake detection a critical challenge for online safety and trust. Progress remains constrained by the scarcity of large, high-quality datasets. Although multimodal large language models (LLMs) exhibit strong reasoning capabilities, their performance on deepfake detection is poor, often producing explanations that are misaligned with visual evidence or hallucinatory. To address this limitation, we introduce a reasoning-annotated dataset for deepfake detection and propose Paragraph-level Relative Policy Optimization (PRPO), a reinforcement learning algorithm that aligns LLM reasoning with image content at the paragraph level. Experiments show that PRPO improves detection accuracy by a wide margin and achieves the highest reasoning score of 4.55/5.0. Ablation studies further demonstrate that PRPO significantly outperforms GRPO under test-time conditions. These results underscore the importance of grounding multimodal reasoning in visual evidence to enable more reliable and interpretable deepfake detection.         ",
    "url": "https://arxiv.org/abs/2509.26272",
    "authors": [
      "Tuan Nguyen",
      "Naseem Khan",
      "Khang Tran",
      "NhatHai Phan",
      "Issa Khalil"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.26383",
    "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning",
    "abstract": "           Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.26383",
    "authors": [
      "Jinyeop Song",
      "Song Wang",
      "Julian Shun",
      "Yada Zhu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2309.01363",
    "title": "Mutual information maximizing quantum generative adversarial networks",
    "abstract": "           One of the most promising applications in the era of Noisy Intermediate-Scale Quantum (NISQ) computing is quantum generative adversarial networks (QGANs), which offer significant quantum advantages over classical machine learning in various domains. However, QGANs suffer from mode collapse and lack explicit control over the features of generated outputs. To overcome these limitations, we propose InfoQGAN, a novel quantum-classical hybrid generative adversarial network that integrates the principles of InfoGAN with a QGAN architecture. Our approach employs a variational quantum circuit for data generation, a classical discriminator, and a Mutual Information Neural Estimator (MINE) to explicitly optimize the mutual information between latent codes and generated samples. Numerical simulations on synthetic 2D distributions and Iris dataset augmentation demonstrate that InfoQGAN effectively mitigates mode collapse while achieving robust feature disentanglement in the quantum generator. By leveraging these advantages, InfoQGAN not only enhances training stability but also improves data augmentation performance through controlled feature generation. These results highlight the potential of InfoQGAN as a foundational approach for advancing quantum generative modeling in the NISQ era.         ",
    "url": "https://arxiv.org/abs/2309.01363",
    "authors": [
      "Mingyu Lee",
      "Myeongjin Shin",
      "Junseo Lee",
      "Kabgyun Jeong"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Portfolio Management (q-fin.PM)"
    ]
  },
  {
    "id": "arXiv:2405.20115",
    "title": "Security of device-independent quantum key distribution via monogamy relations from multipartite information causality",
    "abstract": "           Beyond the foundational significance, the problem of bounding nonlocal correlations by reasonable physical principles has meaningful practical consequences, particularly for device-independent (DI) cryptographic security. In this work, we advance in this direction, demonstrating that the IC is enough to ensure DI security on quantum key distribution (QKD) protocols. Security is proven for a range of theoretically quantum-attainable parameters against individual attacks by a potentially post-quantum eavesdropper. This result follows as a consequence of a strong form of monogamy of Bell's inequality violations, which we have proven to be implied by the recently proposed multipartite formulation for IC. Additionally, we demonstrated that the original bipartite formulation of IC fails to imply monogamy relations and hence, ensure security of DIQKD, thus stressing the necessity of the multipartite framework.         ",
    "url": "https://arxiv.org/abs/2405.20115",
    "authors": [
      "Lucas Pollyceno",
      "Anubhav Chaturvedi",
      "Chithra Raj",
      "Pedro R. Dieguez",
      "Marcin Paw\u0142owski"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2411.16483",
    "title": "Graph Transformer Networks for Accurate Band Structure Prediction: An End-to-End Approach",
    "abstract": "           Predicting electronic band structures from crystal structures is crucial for understanding structure-property correlations in materials science. First-principles approaches are accurate but computationally intensive. Recent years, machine learning (ML) has been extensively applied to this field, while existing ML models predominantly focus on band gap predictions or indirect band structure estimation via solving predicted Hamiltonians. An end-to-end model to predict band structure accurately and efficiently is still lacking. Here, we introduce a graph Transformer-based end-to-end approach that directly predicts band structures from crystal structures with high accuracy. Our method leverages the continuity of the k-path and treat continuous bands as a sequence. We demonstrate that our model not only provides accurate band structure predictions but also can derive other properties (such as band gap, band center, and band dispersion) with high accuracy. We verify the model performance on large and diverse datasets.         ",
    "url": "https://arxiv.org/abs/2411.16483",
    "authors": [
      "Weiyi Gong",
      "Tao Sun",
      "Hexin Bai",
      "Jeng-Yuan Tsai",
      "Haibin Ling",
      "Qimin Yan"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.17481",
    "title": "Toward Foundational Model for Sleep Analysis Using a Multimodal Hybrid Self-Supervised Learning Framework",
    "abstract": "           Sleep is essential for maintaining human health and quality of life. Analyzing physiological signals during sleep is critical in assessing sleep quality and diagnosing sleep disorders. However, manual diagnoses by clinicians are time-intensive and subjective. Despite advances in deep learning that have enhanced automation, these approaches remain heavily dependent on large-scale labeled datasets. This study introduces SynthSleepNet, a multimodal hybrid self-supervised learning framework designed for analyzing polysomnography (PSG) data. SynthSleepNet effectively integrates masked prediction and contrastive learning to leverage complementary features across multiple modalities, including electroencephalogram (EEG), electrooculography (EOG), electromyography (EMG), and electrocardiogram (ECG). This approach enables the model to learn highly expressive representations of PSG data. Furthermore, a temporal context module based on Mamba was developed to efficiently capture contextual information across signals. SynthSleepNet achieved superior performance compared to state-of-the-art methods across three downstream tasks: sleep-stage classification, apnea detection, and hypopnea detection, with accuracies of 89.89%, 99.75%, and 89.60%, respectively. The model demonstrated robust performance in a semi-supervised learning environment with limited labels, achieving accuracies of 87.98%, 99.37%, and 77.52% in the same tasks. These results underscore the potential of the model as a foundational tool for the comprehensive analysis of PSG data. SynthSleepNet demonstrates comprehensively superior performance across multiple downstream tasks compared to other methodologies, making it expected to set a new standard for sleep disorder monitoring and diagnostic systems.         ",
    "url": "https://arxiv.org/abs/2502.17481",
    "authors": [
      "Cheol-Hui Lee",
      "Hakseung Kim",
      "Byung C. Yoon",
      "Dong-Joo Kim"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.02554",
    "title": "Toward a Robust R2D2 Paradigm for Radio-interferometric Imaging: Revisiting Deep Neural Network Training and Architecture",
    "abstract": "           The R2D2 Deep Neural Network (DNN) series was recently introduced for image formation in radio interferometry. It can be understood as a learned version of CLEAN, whose minor cycles are substituted with DNNs. We revisit R2D2 on the grounds of series convergence, training methodology, and DNN architecture, improving its robustness in terms of generalizability beyond training conditions, capability to deliver high data fidelity, and epistemic uncertainty. First, while still focusing on telescope-specific training, we enhance the learning process by randomizing Fourier sampling integration times, incorporating multiscan multinoise configurations, and varying imaging settings, including pixel resolution and visibility-weighting scheme. Second, we introduce a convergence criterion whereby the reconstruction process stops when the data residual is compatible with noise, rather than simply using all available DNNs. This not only increases the reconstruction efficiency by reducing its computational cost, but also refines training by pruning out the data/image pairs for which optimal data fidelity is reached before training the next DNN. Third, we substitute R2D2's early U-Net DNN with a novel architecture (U-WDSR) combining U-Net and WDSR, which leverages wide activation, dense skip connections, weight normalization, and low-rank convolution to improve feature reuse and reconstruction precision. As previously, R2D2 was trained for monochromatic intensity imaging with the Very Large Array at fixed $512 \\times 512$ image size. Simulations on a wide range of inverse problems and a case study on real data reveal that the new R2D2 model consistently outperforms its earlier version in image reconstruction quality, data fidelity, and epistemic uncertainty.         ",
    "url": "https://arxiv.org/abs/2503.02554",
    "authors": [
      "Amir Aghabiglou",
      "Chung San Chu",
      "Chao Tang",
      "Arwa Dabbech",
      "Yves Wiaux"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2505.17961",
    "title": "Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation",
    "abstract": "           Causal inference typically assumes centralized access to individual-level data. Yet, in practice, data are often decentralized across multiple sites, making centralization infeasible due to privacy, logistical, or legal constraints. We address this problem by estimating the Average Treatment Effect (ATE) from decentralized observational data via a Federated Learning (FL) approach, allowing inference through the exchange of aggregate statistics rather than individual-level data. We propose a novel method to estimate propensity scores by computing a federated weighted average of local scores with Membership Weights (MW)--probabilities of site membership conditional on covariates--which can be flexibly estimated using parametric or non-parametric classification models. Unlike density ratio weights (DW) from the transportability and generalization literature, which either rely on strong modeling assumptions or cannot be implemented in FL, MW can be estimated using standard FL algorithms and are more robust, as they support flexible, non-parametric models--making them the preferred choice in multi-site settings with strict data-sharing constraints. The resulting propensity scores are used to construct Federated Inverse Propensity Weighting (Fed-IPW) and Augmented IPW (Fed-AIPW) estimators. Unlike meta-analysis methods, which fail when any site violates positivity, our approach leverages heterogeneity in treatment assignment across sites to improve overlap. We show that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample sizes, treatment mechanisms, and covariate distributions. Both theoretical analysis and experiments on simulated and real-world data highlight their advantages over meta-analysis and related methods.         ",
    "url": "https://arxiv.org/abs/2505.17961",
    "authors": [
      "Khellaf R\u00e9mi",
      "Bellet Aur\u00e9lien",
      "Josse Julie"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)",
      "Statistics Theory (math.ST)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2506.09766",
    "title": "Vulnerability-Based Optimal Grid Defense Strategies for Enhancing Cyber-Physical Energy System Resilience",
    "abstract": "           An approach is proposed to identify optimal asset protection strategies based on vulnerability assessment outcomes. Traditional bilevel attacker-defender models emphasize worst-case scenarios but offer limited defensive guidance. In contrast, trilevel models introduce high computational complexity and rely on fixed network configurations. The proposed critical-components method leverages vulnerability assessment results to determine protection strategies, effectively outsourcing the upper-level defense decision. This enables adaptability to diverse network topologies, assessment techniques, and cyber-physical energy systems without the overhead of multi-level optimization. Case studies demonstrate the potential for improved system resilience across varying operational conditions.         ",
    "url": "https://arxiv.org/abs/2506.09766",
    "authors": [
      "Eric T\u00f6nges",
      "Martin Braun",
      "Philipp H\u00e4rtel"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.17247",
    "title": "DeepASA: An Object-Oriented One-for-All Network for Auditory Scene Analysis",
    "abstract": "           We propose DeepASA, a multi-purpose model for auditory scene analysis that performs multi-input multi-output (MIMO) source separation, dereverberation, sound event detection (SED), audio classification, and direction-of-arrival estimation (DoAE) within a unified framework. DeepASA is designed for complex auditory scenes where multiple, often similar, sound sources overlap in time and move dynamically in space. To achieve robust and consistent inference across tasks, we introduce an object-oriented processing (OOP) strategy. This approach encapsulates diverse auditory features into object-centric representations and refines them through a chain-of-inference (CoI) mechanism. The pipeline comprises a dynamic temporal kernel-based feature extractor, a transformer-based aggregator, and an object separator that yields per-object features. These features feed into multiple task-specific decoders. Our object-centric representations naturally resolve the parameter association ambiguity inherent in traditional track-wise processing. However, early-stage object separation can lead to failure in downstream ASA tasks. To address this, we implement temporal coherence matching (TCM) within the chain-of-inference, enabling multi-task fusion and iterative refinement of object features using estimated auditory parameters. We evaluate DeepASA on representative spatial audio benchmark datasets, including ASA2, MC-FUSS, and STARSS23. Experimental results show that our model achieves state-of-the-art performance across all evaluated tasks, demonstrating its effectiveness in both source separation and auditory parameter estimation under diverse spatial auditory scenes.         ",
    "url": "https://arxiv.org/abs/2509.17247",
    "authors": [
      "Dongheon Lee",
      "Younghoo Kwon",
      "Jung-Woo Choi"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2509.26371",
    "title": "Vector-Valued Reproducing Kernel Banach Spaces for Neural Networks and Operators",
    "abstract": "           Recently, there has been growing interest in characterizing the function spaces underlying neural networks. While shallow and deep scalar-valued neural networks have been linked to scalar-valued reproducing kernel Banach spaces (RKBS), $\\mathbb{R}^d$-valued neural networks and neural operator models remain less understood in the RKBS setting. To address this gap, we develop a general definition of vector-valued RKBS (vv-RKBS), which inherently includes the associated reproducing kernel. Our construction extends existing definitions by avoiding restrictive assumptions such as symmetric kernel domains, finite-dimensional output spaces, reflexivity, or separability, while still recovering familiar properties of vector-valued reproducing kernel Hilbert spaces (vv-RKHS). We then show that shallow $\\mathbb{R}^d$-valued neural networks are elements of a specific vv-RKBS, namely an instance of the integral and neural vv-RKBS. To also explore the functional structure of neural operators, we analyze the DeepONet and Hypernetwork architectures and demonstrate that they too belong to an integral and neural vv-RKBS. In all cases, we establish a Representer Theorem, showing that optimization over these function spaces recovers the corresponding neural architectures.         ",
    "url": "https://arxiv.org/abs/2509.26371",
    "authors": [
      "Sven Dummer",
      "Tjeerd Jan Heeringa",
      "Jos\u00e9 A. Iglesias"
    ],
    "subjectives": [
      "Functional Analysis (math.FA)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  }
]