[
  {
    "id": "arXiv:2201.12342",
    "title": "Error-Correcting Neural Networks for Two-Dimensional Curvature  Computation in the Level-Set Method",
    "abstract": "We present an error-neural-modeling-based strategy for approximating two-dimensional curvature in the level-set method. Our main contribution is a redesigned hybrid solver (Larios-C\\'{a}rdenas and Gibou (2021)[1]) that relies on numerical schemes to enable machine-learning operations on demand. In particular, our routine features double predicting to harness curvature symmetry invariance in favor of precision and stability. As in [1], the core of this solver is a multilayer perceptron trained on circular- and sinusoidal-interface samples. Its role is to quantify the error in numerical curvature approximations and emit corrected estimates for select grid vertices along the free boundary. These corrections arise in response to preprocessed context level-set, curvature, and gradient data. To promote neural capacity, we have adopted sample negative-curvature normalization, reorientation, and reflection-based augmentation. In the same manner, our system incorporates dimensionality reduction, well-balancedness, and regularization to minimize outlying effects. Our training approach is likewise scalable across mesh sizes. For this purpose, we have introduced dimensionless parametrization and probabilistic subsampling during data production. Together, all these elements have improved the accuracy and efficiency of curvature calculations around under-resolved regions. In most experiments, our strategy has outperformed the numerical baseline at twice the number of redistancing steps while requiring only a fraction of the cost. ",
    "url": "https://arxiv.org/abs/2201.12342",
    "authors": [
      "Luis \u00c1ngel Larios-C\u00e1rdenas",
      "Fr\u00e9d\u00e9ric Gibou"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12346",
    "title": "DiriNet: A network to estimate the spatial and spectral degradation  functions",
    "abstract": "The spatial and spectral degradation functions are critical to hyper- and multi-spectral image fusion. However, few work has been payed on the estimation of the degradation functions. To learn the spatial response function and the point spread function from the image pairs to be fused, we propose a Dirichlet network, where both functions are properly constrained. Specifically, the spatial response function is constrained with positivity, while the Dirichlet distribution along with a total variation is imposed on the point spread function. To the best of our knowledge, the neural netwrok and the Dirichlet regularization are exclusively investigated, for the first time, to estimate the degradation functions. Both image degradation and fusion experiments demonstrate the effectiveness and superiority of the proposed Dirichlet network. ",
    "url": "https://arxiv.org/abs/2201.12346",
    "authors": [
      "Ting Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2201.12347",
    "title": "Adversarial Robustness in Deep Learning: Attacks on Fragile Neurons",
    "abstract": "We identify fragile and robust neurons of deep learning architectures using nodal dropouts of the first convolutional layer. Using an adversarial targeting algorithm, we correlate these neurons with the distribution of adversarial attacks on the network. Adversarial robustness of neural networks has gained significant attention in recent times and highlights intrinsic weaknesses of deep learning networks against carefully constructed distortion applied to input images. In this paper, we evaluate the robustness of state-of-the-art image classification models trained on the MNIST and CIFAR10 datasets against the fast gradient sign method attack, a simple yet effective method of deceiving neural networks. Our method identifies the specific neurons of a network that are most affected by the adversarial attack being applied. We, therefore, propose to make fragile neurons more robust against these attacks by compressing features within robust neurons and amplifying the fragile neurons proportionally. ",
    "url": "https://arxiv.org/abs/2201.12347",
    "authors": [
      "Chandresh Pravin",
      "Ivan Martino",
      "Giuseppe Nicosia",
      "Varun Ojha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2201.12355",
    "title": "Adversarial Decisions on Complex Dynamical Systems using Game Theory",
    "abstract": "We apply computational Game Theory to a unification of physics-based models that represent decision-making across a number of agents within both cooperative and competitive processes. Here the competitors try to both positively influence their own returns, while negatively affecting those of their competitors. Modelling these interactions with the so-called Boyd-Kuramoto-Lanchester (BKL) complex dynamical system model yields results that can be applied to business, gaming and security contexts. This paper studies a class of decision problems on the BKL model, where a large set of coupled, switching dynamical systems are analysed using game-theoretic methods. Due to their size, the computational cost of solving these BKL games becomes the dominant factor in the solution process. To resolve this, we introduce a novel Nash Dominant solver, which is both numerically efficient and exact. The performance of this new solution technique is compared to traditional exact solvers, which traverse the entire game tree, as well as to approximate solvers such as Myopic and Monte Carlo Tree Search (MCTS). These techniques are assessed, and used to gain insights into both nonlinear dynamical systems and strategic decision making in adversarial environments. ",
    "url": "https://arxiv.org/abs/2201.12355",
    "authors": [
      "Andrew C. Cullen",
      "Tansu Alpcan",
      "Alexander C. Kalloniatis"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Chaotic Dynamics (nlin.CD)"
    ]
  },
  {
    "id": "arXiv:2201.12356",
    "title": "Adversarial Examples for Good: Adversarial Examples Guided Imbalanced  Learning",
    "abstract": "Adversarial examples are inputs for machine learning models that have been designed by attackers to cause the model to make mistakes. In this paper, we demonstrate that adversarial examples can also be utilized for good to improve the performance of imbalanced learning. We provide a new perspective on how to deal with imbalanced data: adjust the biased decision boundary by training with Guiding Adversarial Examples (GAEs). Our method can effectively increase the accuracy of minority classes while sacrificing little accuracy on majority classes. We empirically show, on several benchmark datasets, our proposed method is comparable to the state-of-the-art method. To our best knowledge, we are the first to deal with imbalanced learning with adversarial examples. ",
    "url": "https://arxiv.org/abs/2201.12356",
    "authors": [
      "Jie Zhang",
      "Lei Zhang",
      "Gang Li",
      "Chao Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.12360",
    "title": "Variational Neural Cellular Automata",
    "abstract": "In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms -- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, we show that the VNCA can learn a distribution of stable attractors that can recover from significant damage. ",
    "url": "https://arxiv.org/abs/2201.12360",
    "authors": [
      "Rasmus Berg Palm",
      "Miguel Gonz\u00e1lez-Duque",
      "Shyam Sudhakaran",
      "Sebastian Risi"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2201.12395",
    "title": "Competitive Algorithms and Reinforcement Learning for NOMA in IoT  Networks",
    "abstract": "This paper studies the problem of massive Internet of things (IoT) access in beyond fifth generation (B5G) networks using non-orthogonal multiple access (NOMA) technique. The problem involves massive IoT devices grouping and power allocation in order to respect the low latency as well as the limited operating energy of the IoT devices. The considered objective function, maximizing the number of successfully received IoT packets, is different from the classical sum-rate-related objective functions. The problem is first divided into multiple NOMA grouping subproblems. Then, using competitive analysis, an efficient online competitive algorithm (CA) is proposed to solve each subproblem. Next, to solve the power allocation problem, we propose a new reinforcement learning (RL) framework in which a RL agent learns to use the CA as a black box and combines the obtained solutions to each subproblem to determine the power allocation for each NOMA group. Our simulations results reveal that the proposed innovative RL framework outperforms deep-Q-learning methods and is close-to-optimal. ",
    "url": "https://arxiv.org/abs/2201.12395",
    "authors": [
      "Zoubeir Mlika",
      "Soumaya Cherkaoui"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2201.12406",
    "title": "Syfer: Neural Obfuscation for Private Data Release",
    "abstract": "Balancing privacy and predictive utility remains a central challenge for machine learning in healthcare. In this paper, we develop Syfer, a neural obfuscation method to protect against re-identification attacks. Syfer composes trained layers with random neural networks to encode the original data (e.g. X-rays) while maintaining the ability to predict diagnoses from the encoded data. The randomness in the encoder acts as the private key for the data owner. We quantify privacy as the number of attacker guesses required to re-identify a single image (guesswork). We propose a contrastive learning algorithm to estimate guesswork. We show empirically that differentially private methods, such as DP-Image, obtain privacy at a significant loss of utility. In contrast, Syfer achieves strong privacy while preserving utility. For example, X-ray classifiers built with DP-image, Syfer, and original data achieve average AUCs of 0.53, 0.78, and 0.86, respectively. ",
    "url": "https://arxiv.org/abs/2201.12406",
    "authors": [
      "Adam Yala",
      "Victor Quach",
      "Homa Esfahanizadeh",
      "Rafael G. L. D'Oliveira",
      "Ken R. Duffy",
      "Muriel M\u00e9dard",
      "Tommi S. Jaakkola",
      "Regina Barzilay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.12409",
    "title": "A Unified Approach to Entity-Centric Context Tracking in Social  Conversations",
    "abstract": "In human-human conversations, Context Tracking deals with identifying important entities and keeping track of their properties and relationships. This is a challenging problem that encompasses several subtasks such as slot tagging, coreference resolution, resolving plural mentions and entity linking. We approach this problem as an end-to-end modeling task where the conversational context is represented by an entity repository containing the entity references mentioned so far, their properties and the relationships between them. The repository is updated turn-by-turn, thus making training and inference computationally efficient even for long conversations. This paper lays the groundwork for an investigation of this framework in two ways. First, we release Contrack, a large scale human-human conversation corpus for context tracking with people and location annotations. It contains over 7000 conversations with an average of 11.8 turns, 5.8 entities and 15.2 references per conversation. Second, we open-source a neural network architecture for context tracking. Finally we compare this network to state-of-the-art approaches for the subtasks it subsumes and report results on the involved tradeoffs. ",
    "url": "https://arxiv.org/abs/2201.12409",
    "authors": [
      "Ulrich R\u00fcckert",
      "Srinivas Sunkara",
      "Abhinav Rastogi",
      "Sushant Prakash",
      "Pranav Khaitan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.12416",
    "title": "Discovering Exfiltration Paths Using Reinforcement Learning with Attack  Graphs",
    "abstract": "Reinforcement learning (RL), in conjunction with attack graphs and cyber terrain, are used to develop reward and state associated with determination of optimal paths for exfiltration of data in enterprise networks. This work builds on previous crown jewels (CJ) identification that focused on the target goal of computing optimal paths that adversaries may traverse toward compromising CJs or hosts within their proximity. This work inverts the previous CJ approach based on the assumption that data has been stolen and now must be quietly exfiltrated from the network. RL is utilized to support the development of a reward function based on the identification of those paths where adversaries desire reduced detection. Results demonstrate promising performance for a sizable network environment. ",
    "url": "https://arxiv.org/abs/2201.12416",
    "authors": [
      "Tyler Cody",
      "Abdul Rahman",
      "Christopher Redino",
      "Lanxiao Huang",
      "Ryan Clark",
      "Akshay Kakkar",
      "Deepak Kushwaha",
      "Paul Park",
      "Peter Beling",
      "Edward Bowen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2201.12425",
    "title": "CoordX: Accelerating Implicit Neural Representation with a Split MLP  Architecture",
    "abstract": "Implicit neural representations with multi-layer perceptrons (MLPs) have recently gained prominence for a wide variety of tasks such as novel view synthesis and 3D object representation and rendering. However, a significant challenge with these representations is that both training and inference with an MLP over a large number of input coordinates to learn and represent an image, video, or 3D object, require large amounts of computation and incur long processing times. In this work, we aim to accelerate inference and training of coordinate-based MLPs for implicit neural representations by proposing a new split MLP architecture, CoordX. With CoordX, the initial layers are split to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP. This approach thus aims at first learning functions that are a decomposition of the original signal and then fusing them to generate the learned signal. Our proposed architecture can be generally used for many implicit neural representation tasks with no additional memory overheads. We demonstrate a speedup of up to 2.92x compared to the baseline model for image, video, and 3D shape representation and rendering tasks. ",
    "url": "https://arxiv.org/abs/2201.12425",
    "authors": [
      "Ruofan Liang",
      "Hongyi Sun",
      "Nandita Vijaykumar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12433",
    "title": "FedGCN: Convergence and Communication Tradeoffs in Federated Training of  Graph Convolutional Networks",
    "abstract": "Distributed methods for training models on graph datasets have recently grown in popularity, due to the size of graph datasets as well as the private nature of graphical data like social networks. However, the graphical structure of this data means that it cannot be disjointly partitioned between different learning clients, leading to either significant communication overhead between clients or a loss of information available to the training method. We introduce Federated Graph Convolutional Network (FedGCN), which uses federated learning to train GCN models with optimized convergence rate and communication cost. Compared to prior methods that require communication among clients at each iteration, FedGCN preserves the privacy of client data and only needs communication at the initial step, which greatly reduces communication cost and speeds up the convergence rate. We theoretically analyze the tradeoff between FedGCN's convergence rate and communication cost under different data distributions, introducing a general framework can be generally used for the analysis of all edge-completion-based GCN training algorithms. Experimental results demonstrate the effectiveness of our algorithm and validate our theoretical analysis. ",
    "url": "https://arxiv.org/abs/2201.12433",
    "authors": [
      "Yuhang Yao",
      "Carlee Joe-Wong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2201.12436",
    "title": "Any-Play: An Intrinsic Augmentation for Zero-Shot Coordination",
    "abstract": "Cooperative artificial intelligence with human or superhuman proficiency in collaborative tasks stands at the frontier of machine learning research. Prior work has tended to evaluate cooperative AI performance under the restrictive paradigms of self-play (teams composed of agents trained together) and cross-play (teams of agents trained independently but using the same algorithm). Recent work has indicated that AI optimized for these narrow settings may make for undesirable collaborators in the real-world. We formalize an alternative criteria for evaluating cooperative AI, referred to as inter-algorithm cross-play, where agents are evaluated on teaming performance with all other agents within an experiment pool with no assumption of algorithmic similarities between agents. We show that existing state-of-the-art cooperative AI algorithms, such as Other-Play and Off-Belief Learning, under-perform in this paradigm. We propose the Any-Play learning augmentation -- a multi-agent extension of diversity-based intrinsic rewards for zero-shot coordination (ZSC) -- for generalizing self-play-based algorithms to the inter-algorithm cross-play setting. We apply the Any-Play learning augmentation to the Simplified Action Decoder (SAD) and demonstrate state-of-the-art performance in the collaborative card game Hanabi. ",
    "url": "https://arxiv.org/abs/2201.12436",
    "authors": [
      "Keane Lucas",
      "Ross E. Allen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2201.12437",
    "title": "Task-Focused Few-Shot Object Detection for Robot Manipulation",
    "abstract": "This paper addresses the problem of mobile robot manipulation of novel objects via detection. Our approach uses vision and control as complementary functions that learn from real-world tasks. We develop a manipulation method based solely on detection then introduce task-focused few-shot object detection to learn new objects and settings. The current paradigm for few-shot object detection uses existing annotated examples. In contrast, we extend this paradigm by using active data collection and annotation selection that improves performance for specific downstream tasks (e.g., depth estimation and grasping). In experiments for our interactive approach to few-shot learning, we train a robot to manipulate objects directly from detection (ClickBot). ClickBot learns visual servo control from a single click of annotation, grasps novel objects in clutter and other settings, and achieves state-of-the-art results on an existing visual servo control and depth estimation benchmark. Finally, we establish a task-focused few-shot object detection benchmark to support future research: https://github.com/griffbr/TFOD. ",
    "url": "https://arxiv.org/abs/2201.12437",
    "authors": [
      "Brent Griffin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2201.12439",
    "title": "Discriminating Defense Against DDoS Attacks; a Novel Approach",
    "abstract": "A recent paper (circa 2020) by Osterwile et al., entitled \"21 Years of Distributed Denial of Service: A Call to Action\", states: \"We are falling behind in the war against distributed denial-of-service attacks. Unless we act now, the future of the Internet could be at stake.\" And an earlier (circa 2007) paper by Peng et al. states: \"a key challenge for the defense [against DDoS attacks] is how to discriminate legitimate requests for service from malicious access attempts.\" This challenge has not been met yet, which is, arguably, a major reason for the dire situation described by Osterwile et al. -- thirteen years later. This paper attempts to meet an approximation to this challenge, by enabling a a site to define the kind of messages that it considers important, and by introducing an unambiguous criterion of discrimination between messages that a given site considers important, and all other messages sent to it. Two anti-DDoS mechanisms based on this criterion are introduced in this paper. One of these relies on lightweight support by routers; and the other one does not. ",
    "url": "https://arxiv.org/abs/2201.12439",
    "authors": [
      "Naftaly H. Minsky"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2201.12454",
    "title": "The Complexity of Approximate Pattern Matching on De Bruijn Graphs",
    "abstract": "Aligning a sequence to a walk in a labeled graph is a problem of fundamental importance to Computational Biology. For finding a walk in an arbitrary graph with $|E|$ edges that exactly matches a pattern of length $m$, a lower bound based on the Strong Exponential Time Hypothesis (SETH) implies an algorithm significantly faster than $O(|E|m)$ time is unlikely [Equi et al., ICALP 2019]. However, for many special graphs, such as de Bruijn graphs, the problem can be solved in linear time [Bowe et al., WABI 2012]. For approximate matching, the picture is more complex. When edits (substitutions, insertions, and deletions) are only allowed to the pattern, or when the graph is acyclic, the problem is again solvable in $O(|E|m)$ time. When edits are allowed to arbitrary cyclic graphs, the problem becomes NP-complete, even on binary alphabets [Jain et al., RECOMB 2019]. These results hold even when edits are restricted to only substitutions. The complexity of approximate pattern matching on de Bruijn graphs remained open. We investigate this problem and show that the properties that make de Bruijn graphs amenable to efficient exact pattern matching do not extend to approximate matching, even when restricted to the substitutions only case with alphabet size four. We prove that determining the existence of a matching walk in a de Bruijn graph is NP-complete when substitutions are allowed to the graph. In addition, we demonstrate that an algorithm significantly faster than $O(|E|m)$ is unlikely for de Bruijn graphs in the case where only substitutions are allowed to the pattern. This stands in contrast to pattern-to-text matching where exact matching is solvable in linear time, like on de Bruijn graphs, but approximate matching under substitutions is solvable in subquadratic $O(n\\sqrt{m})$ time, where $n$ is the text's length [Abrahamson, SIAM J. Computing 1987]. ",
    "url": "https://arxiv.org/abs/2201.12454",
    "authors": [
      "Daniel Gibney",
      "Sharma V. Thankachan",
      "Srinivas Aluru"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2201.12469",
    "title": "ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language  Models via Efficient Large-Batch Adversarial Noise",
    "abstract": "In recent years, large pre-trained Transformer-based language models have led to dramatic improvements in many natural language understanding tasks. To train these models with increasing sizes, many neural network practitioners attempt to increase the batch sizes in order to leverage multiple GPUs to improve training speed. However, increasing the batch size often makes the optimization more difficult, leading to slow convergence or poor generalization that can require orders of magnitude more training time to achieve the same model quality. In this paper, we explore the steepness of the loss landscape of large-batch optimization for adapting pre-trained Transformer-based language models to domain-specific tasks and find that it tends to be highly complex and irregular, posing challenges to generalization on downstream tasks. To tackle this challenge, we propose ScaLA, a novel and efficient method to accelerate the adaptation speed of pre-trained transformer networks. Different from prior methods, we take a sequential game-theoretic approach by adding lightweight adversarial noise into large-batch optimization, which significantly improves adaptation speed while preserving model generalization. Experiment results show that ScaLA attains 2.7--9.8$\\times$ adaptation speedups over the baseline for GLUE on BERT-base and RoBERTa-large, while achieving comparable and sometimes higher accuracy than the state-of-the-art large-batch optimization methods. Finally, we also address the theoretical aspect of large-batch optimization with adversarial noise and provide a theoretical convergence rate analysis for ScaLA using techniques for analyzing non-convex saddle-point problems. ",
    "url": "https://arxiv.org/abs/2201.12469",
    "authors": [
      "Minjia Zhang",
      "Niranjan Uma Naresh",
      "Yuxiong He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2201.12482",
    "title": "Collaborative Learning in General Graphs with Limited Memorization:  Learnability, Complexity and Reliability",
    "abstract": "We consider K-armed bandit problem in general graphs where agents are arbitrarily connected and each of them has limited memorization and communication bandwidth. The goal is to let each of the agents learn the best arm. Although recent studies show the power of collaboration among the agents in improving the efficacy of learning, it is assumed in these studies that the communication graphs should be complete or well-structured, whereas such an assumption is not always valid in practice. Furthermore, limited memorization and communication bandwidth also restrict the collaborations of the agents, since very few knowledge can be drawn by each agent from its experiences or the ones shared by its peers in this case. Additionally, the agents may be corrupted to share falsified experience, while the resource limit may considerably restrict the reliability of the learning process. To address the above issues, we propose a three-staged collaborative learning algorithm. In each step, the agents share their experience with each other through light-weight random walks in the general graphs, and then make decisions on which arms to pull according to the randomly memorized suggestions. The agents finally update their adoptions (i.e., preferences to the arms) based on the reward feedback of the arm pulling. Our theoretical analysis shows that, by exploiting the limited memorization and communication resources, all the agents eventually learn the best arm with high probability. We also reveal in our theoretical analysis the upper-bound on the number of corrupted agents our algorithm can tolerate. The efficacy of our proposed three-staged collaborative learning algorithm is finally verified by extensive experiments on both synthetic and real datasets. ",
    "url": "https://arxiv.org/abs/2201.12482",
    "authors": [
      "Feng Li",
      "Xuyang Yuan",
      "Lina Wang",
      "Huan Yang",
      "Dongxiao Yu",
      "Weifeng Lv",
      "Xiuzhen Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2201.12489",
    "title": "A Context-Integrated Transformer-Based Neural Network for Auction Design",
    "abstract": "One of the central problems in auction design is developing an incentive-compatible mechanism that maximizes the auctioneer's expected revenue. While theoretical approaches have encountered bottlenecks in multi-item auctions, recently, there has been much progress on finding the optimal mechanism through deep learning. However, these works either focus on a fixed set of bidders and items, or restrict the auction to be symmetric. In this work, we overcome such limitations by factoring \\emph{public} contextual information of bidders and items into the auction learning framework. We propose $\\mathtt{CITransNet}$, a context-integrated transformer-based neural network for optimal auction design, which maintains permutation-equivariance over bids and contexts while being able to find asymmetric solutions. We show by extensive experiments that $\\mathtt{CITransNet}$ can recover the known optimal solutions in single-item settings, outperform strong baselines in multi-item auctions, and generalize well to cases other than those in training. ",
    "url": "https://arxiv.org/abs/2201.12489",
    "authors": [
      "Zhijian Duan",
      "Jingwu Tang",
      "Yutong Yin",
      "Zhe Feng",
      "Xiang Yan",
      "Manzil Zaheer",
      "Xiaotie Deng"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2201.12498",
    "title": "Investigating Why Contrastive Learning Benefits Robustness Against Label  Noise",
    "abstract": "Self-supervised contrastive learning has recently been shown to be very effective in preventing deep networks from overfitting noisy labels. Despite its empirical success, the theoretical understanding of the effect of contrastive learning on boosting robustness is very limited. In this work, we rigorously prove that the representation matrix learned by contrastive learning boosts robustness, by having: (i) one prominent singular value corresponding to every sub-class in the data, and remaining significantly smaller singular values; and (ii) a large alignment between the prominent singular vector and the clean labels of each sub-class. The above properties allow a linear layer trained on the representations to quickly learn the clean labels, and prevent it from overfitting the noise for a large number of training iterations. We further show that the low-rank structure of the Jacobian of deep networks pre-trained with contrastive learning allows them to achieve a superior performance initially, when fine-tuned on noisy labels. Finally, we demonstrate that the initial robustness provided by contrastive learning enables robust training methods to achieve state-of-the-art performance under extreme noise levels, e.g., an average of 27.18\\% and 15.58\\% increase in accuracy on CIFAR-10 and CIFAR-100 with 80\\% symmetric noisy labels, and 4.11\\% increase in accuracy on WebVision. ",
    "url": "https://arxiv.org/abs/2201.12498",
    "authors": [
      "Yihao Xue",
      "Kyle Whitecross",
      "Baharan Mirzasoleiman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12507",
    "title": "AutoDistil: Few-shot Task-agnostic Neural Architecture Search for  Distilling Large Language Models",
    "abstract": "Knowledge distillation (KD) methods compress large models into smaller students with manually-designed student architectures given pre-specified computational cost. This requires several trials to find a viable student, and further repeating the process for each student or computational budget change. We use Neural Architecture Search (NAS) to automatically distill several compressed students with variable cost from a large model. Current works train a single SuperLM consisting of millions of subnetworks with weight-sharing, resulting in interference between subnetworks of different sizes. Our framework AutoDistil addresses above challenges with the following steps: (a) Incorporates inductive bias and heuristics to partition Transformer search space into K compact sub-spaces (K=3 for typical student sizes of base, small and tiny); (b) Trains one SuperLM for each sub-space using task-agnostic objective (e.g., self-attention distillation) with weight-sharing of students; (c) Lightweight search for the optimal student without re-training. Fully task-agnostic training and search allow students to be reused for fine-tuning on any downstream task. Experiments on GLUE benchmark against state-of-the-art KD and NAS methods demonstrate AutoDistil to outperform leading compression techniques with upto 2.7x reduction in computational cost and negligible loss in task performance. ",
    "url": "https://arxiv.org/abs/2201.12507",
    "authors": [
      "Dongkuan Xu",
      "Subhabrata Mukherjee",
      "Xiaodong Liu",
      "Debadeepta Dey",
      "Wenhui Wang",
      "Xiang Zhang",
      "Ahmed Hassan Awadallah",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2201.12525",
    "title": "Spherical Convolution empowered FoV Prediction in 360-degree Video  Multicast with Limited FoV Feedback",
    "abstract": "Field of view (FoV) prediction is critical in 360-degree video multicast, which is a key component of the emerging Virtual Reality (VR) and Augmented Reality (AR) applications. Most of the current prediction methods combining saliency detection and FoV information neither take into account that the distortion of projected 360-degree videos can invalidate the weight sharing of traditional convolutional networks, nor do they adequately consider the difficulty of obtaining complete multi-user FoV information, which degrades the prediction performance. This paper proposes a spherical convolution-empowered FoV prediction method, which is a multi-source prediction framework combining salient features extracted from 360-degree video with limited FoV feedback information. A spherical convolution neural network (CNN) is used instead of a traditional two-dimensional CNN to eliminate the problem of weight sharing failure caused by video projection distortion. Specifically, salient spatial-temporal features are extracted through a spherical convolution-based saliency detection model, after which the limited feedback FoV information is represented as a time-series model based on a spherical convolution-empowered gated recurrent unit network. Finally, the extracted salient video features are combined to predict future user FoVs. The experimental results show that the performance of the proposed method is better than other prediction methods. ",
    "url": "https://arxiv.org/abs/2201.12525",
    "authors": [
      "Jie Li",
      "Ling Han",
      "Cong Zhang",
      "Qiyue Li",
      "Zhi Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2201.12527",
    "title": "Scale-Invariant Adversarial Attack for Evaluating and Enhancing  Adversarial Defenses",
    "abstract": "Efficient and effective attacks are crucial for reliable evaluation of defenses, and also for developing robust models. Projected Gradient Descent (PGD) attack has been demonstrated to be one of the most successful adversarial attacks. However, the effect of the standard PGD attack can be easily weakened by rescaling the logits, while the original decision of every input will not be changed. To mitigate this issue, in this paper, we propose Scale-Invariant Adversarial Attack (SI-PGD), which utilizes the angle between the features in the penultimate layer and the weights in the softmax layer to guide the generation of adversaries. The cosine angle matrix is used to learn angularly discriminative representation and will not be changed with the rescaling of logits, thus making SI-PGD attack to be stable and effective. We evaluate our attack against multiple defenses and show improved performance when compared with existing attacks. Further, we propose Scale-Invariant (SI) adversarial defense mechanism based on the cosine angle matrix, which can be embedded into the popular adversarial defenses. The experimental results show the defense method with our SI mechanism achieves state-of-the-art performance among multi-step and single-step defenses. ",
    "url": "https://arxiv.org/abs/2201.12527",
    "authors": [
      "Mengting Xu",
      "Tao Zhang",
      "Zhongnian Li",
      "Daoqiang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.12538",
    "title": "Incorporating Commonsense Knowledge into Story Ending Generation via  Heterogeneous Graph Networks",
    "abstract": "Story ending generation is an interesting and challenging task, which aims to generate a coherent and reasonable ending given a story context. The key challenges of the task lie in how to comprehend the story context sufficiently and handle the implicit knowledge behind story clues effectively, which are still under-explored by previous work. In this paper, we propose a Story Heterogeneous Graph Network (SHGN) to explicitly model both the information of story context at different granularity levels and the multi-grained interactive relations among them. In detail, we consider commonsense knowledge, words and sentences as three types of nodes. To aggregate non-local information, a global node is also introduced. Given this heterogeneous graph network, the node representations are updated through graph propagation, which adequately utilizes commonsense knowledge to facilitate story comprehension. Moreover, we design two auxiliary tasks to implicitly capture the sentiment trend and key events lie in the context. The auxiliary tasks are jointly optimized with the primary story ending generation task in a multi-task learning strategy. Extensive experiments on the ROCStories Corpus show that the developed model achieves new state-of-the-art performances. Human study further demonstrates that our model generates more reasonable story endings. ",
    "url": "https://arxiv.org/abs/2201.12538",
    "authors": [
      "Jiaan Wang",
      "Beiqi Zou",
      "Zhixu Li",
      "Jianfeng Qu",
      "Pengpeng Zhao",
      "An Liu",
      "Lei Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.12542",
    "title": "Aper: Evolution-Aware Runtime Permission Misuse Detection for Android  Apps",
    "abstract": "The Android platform introduces the runtime permission model in version 6.0. The new model greatly improves data privacy and user experience, but brings new challenges for app developers. First, it allows users to freely revoke granted permissions. Hence, developers cannot assume that the permissions granted to an app would keep being granted. Instead, they should make their apps carefully check the permission status before invoking dangerous APIs. Second, the permission specification keeps evolving, bringing new types of compatibility issues into the ecosystem. To understand the impact of the challenges, we conducted an empirical study on 13,352 popular Google Play apps. We found that 86.0% apps used dangerous APIs asynchronously after permission management and 61.2% apps used evolving dangerous APIs. If an app does not properly handle permission revocations or platform differences, unexpected runtime issues may happen and even cause app crashes. We call such Android Runtime Permission issues as ARP bugs. Unfortunately, existing runtime permission issue detection tools cannot effectively deal with the ARP bugs induced by asynchronous permission management and permission specification evolution. To fill the gap, we designed a static analyzer, Aper, that performs reaching definition and dominator analysis on Android apps to detect the two types of ARP bugs. To compare Aper with existing tools, we built a benchmark, ARPfix, from 60 real ARP bugs. Our experiment results show that Aper significantly outperforms two academic tools, ARPDroid and RevDroid, and an industrial tool, Lint, on ARPfix, with an average improvement of 46.3% on F1-score. In addition, Aper successfully found 34 ARP bugs in 214 opensource Android apps, most of which can result in abnormal app behaviors (such as app crashes) according to our manual validation. ",
    "url": "https://arxiv.org/abs/2201.12542",
    "authors": [
      "Sinan Wang",
      "Yibo Wang",
      "Xian Zhan",
      "Ying Wang",
      "Yepang Liu",
      "Xiapu Luo",
      "Shing-Chi Cheung"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2201.12548",
    "title": "Transport Capacity Optimization for Resource Allocation in Tera-IoT  Networks",
    "abstract": "We present a new adaptive resource optimization strategy that jointly allocates the subwindow and transmit power in multi-device terahertz (THz) band Internet of Things (Tera-IoT) networks. Unlike the prior studies focusing mostly on maximizing the sum distance, we incorporate both rate and transmission distance into the objective function of our problem formulation with key features of THz bands, including the spreading and molecular absorption losses. More specifically, as a performance metric of Tera-IoT networks, we adopt the transport capacity (TC), which is defined as the sum of the rate-distance products over all users. This metric has been widely adopted in large-scale ad hoc networks, and would also be appropriate for evaluating the performance of various Tera-IoT applications. We then formulate an optimization problem that aims at maximizing the TC. Moreover, motivated by the importance of the transmission distance that is very limited due to the high path loss in THz bands, our optimization problem is extended to the case of allocating the subwindow, transmit power, and transmission distance. We show how to solve our problems via an effective two-stage resource allocation strategy. We demonstrate the superiority of our adaptive solution over benchmark methods via intensive numerical evaluations for various environmental setups of large-scale Tera-IoT networks. ",
    "url": "https://arxiv.org/abs/2201.12548",
    "authors": [
      "Cheol Jeong",
      "Chang-Jae Chun",
      "Won-Yong Shin",
      "Il-Min Kim"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2201.12558",
    "title": "The KFIoU Loss for Rotated Object Detection",
    "abstract": "Differing from the well-developed horizontal object detection area whereby the computing-friendly IoU based loss is readily adopted and well fits with the detection metrics. In contrast, rotation detectors often involve a more complicated loss based on SkewIoU which is unfriendly to gradient-based training. In this paper, we argue that one effective alternative is to devise an approximate loss who can achieve trend-level alignment with SkewIoU loss instead of the strict value-level identity. Specifically, we model the objects as Gaussian distribution and adopt Kalman filter to inherently mimic the mechanism of SkewIoU by its definition, and show its alignment with the SkewIoU at trend-level. This is in contrast to recent Gaussian modeling based rotation detectors e.g. GWD, KLD that involves a human-specified distribution distance metric which requires additional hyperparameter tuning. The resulting new loss called KFIoU is easier to implement and works better compared with exact SkewIoU, thanks to its full differentiability and ability to handle the non-overlapping cases. We further extend our technique to the 3-D case which also suffers from the same issues as 2-D detection. Extensive results on various public datasets (2-D/3-D, aerial/text/face images) with different base detectors show the effectiveness of our approach. ",
    "url": "https://arxiv.org/abs/2201.12558",
    "authors": [
      "Xue Yang",
      "Yue Zhou",
      "Gefan Zhang",
      "Jitui Yang",
      "Wentao Wang",
      "Junchi Yan",
      "Xiaopeng Zhang",
      "Qi Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12577",
    "title": "A Novel Matrix-Encoding Method for Privacy-Preserving Neural Networks  (Inference)",
    "abstract": "In this work, we present $\\texttt{Volley Revolver}$, a novel matrix-encoding method that is particularly convenient for privacy-preserving neural networks to make predictions, and use it to implement a CNN for handwritten image classification. Based on this encoding method, we develop several additional operations for putting into practice the secure matrix multiplication over encrypted data matrices. For two matrices $A$ and $B$ to perform multiplication $A \\times B$, the main idea is, in a simple version, to encrypt matrix $A$ and the transposition of the matrix $B$ into two ciphertexts respectively. Along with the additional operations, the homomorphic matrix multiplication $A \\times B$ can be calculated over encrypted data matrices efficiently. For the convolution operation in CNN, on the basis of the $\\texttt{Volley Revolver}$ encoding method, we develop a feasible and efficient evaluation strategy for performing the convolution operation. We in advance span each convolution kernel of CNN to a matrix space of the same size as the input image so as to generate several ciphertexts, each of which is later used together with the input image for calculating some part of the final convolution result. We accumulate all these part results of convolution operation and thus obtain the final convolution result. ",
    "url": "https://arxiv.org/abs/2201.12577",
    "authors": [
      "John Chiang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.12585",
    "title": "LBCF: A Large-Scale Budget-Constrained Causal Forest Algorithm",
    "abstract": "Offering incentives (e.g., coupons at Amazon, discounts at Uber and video bonuses at Tiktok) to user is a common strategy used by online platforms to increase user engagement and platform revenue. Despite its proven effectiveness, these marketing incentives incur an inevitable cost and might result in a low ROI (Return on Investment) if not used properly. On the other hand, different users respond differently to these incentives, for instance, some users never buy certain products without coupons, while others do anyway. Thus, how to select the right amount of incentives (i.e. treatment) to each user under budget constraints is an important research problem with great practical implications. In this paper, we call such problem as a budget-constrained treatment selection (BTS) problem. The challenge is how to efficiently solve BTS problem on a Large-Scale dataset and achieve improved results over the existing techniques. We propose a novel tree-based treatment selection technique under budget constraints, called Large-Scale Budget-Constrained Causal Forest (LBCF) algorithm, which is also an efficient treatment selection algorithm suitable for modern distributed computing systems. A novel offline evaluation method is also proposed to overcome an intrinsic challenge in assessing solutions' performance for BTS problem in randomized control trials (RCT) data. We deploy our approach in a real-world scenario on a large-scale video platform, where the platform gives away bonuses in order to increase users' campaign engagement duration. The simulation analysis, offline and online experiments all show that our method outperforms various tree-based state-of-the-art baselines. The proposed approach is currently serving over hundreds of millions of users on the platform and achieves one of the most tremendous improvements over these months. ",
    "url": "https://arxiv.org/abs/2201.12585",
    "authors": [
      "Meng Ai",
      "Biao Li",
      "Heyang Gong",
      "Qingwei Yu",
      "Shengjie Xue",
      "Yuan Zhang",
      "Yunzhou Zhang",
      "Peng Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2201.12594",
    "title": "Robust Imitation Learning from Corrupted Demonstrations",
    "abstract": "We consider offline Imitation Learning from corrupted demonstrations where a constant fraction of data can be noise or even arbitrary outliers. Classical approaches such as Behavior Cloning assumes that demonstrations are collected by an presumably optimal expert, hence may fail drastically when learning from corrupted demonstrations. We propose a novel robust algorithm by minimizing a Median-of-Means (MOM) objective which guarantees the accurate estimation of policy, even in the presence of constant fraction of outliers. Our theoretical analysis shows that our robust method in the corrupted setting enjoys nearly the same error scaling and sample complexity guarantees as the classical Behavior Cloning in the expert demonstration setting. Our experiments on continuous-control benchmarks validate that our method exhibits the predicted robustness and effectiveness, and achieves competitive results compared to existing imitation learning methods. ",
    "url": "https://arxiv.org/abs/2201.12594",
    "authors": [
      "Liu Liu",
      "Ziyang Tang",
      "Lanqing Li",
      "Dijun Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2201.12622",
    "title": "Hand Gesture Recognition of Dumb Person Using one Against All Neural  Network",
    "abstract": "We propose a new technique for recognition of dumb person hand gesture in real world environment. In this technique, the hand image containing the gesture is preprocessed and then hand region is segmented by convergent the RGB color image to L.a.b color space. Only few statistical features are used to classify the segmented image to different classes. Artificial Neural Network is trained in sequential manner using one against all. When the system gets trained, it becomes capable of recognition of each class in parallel manner. The result of proposed technique is much better than existing techniques. ",
    "url": "https://arxiv.org/abs/2201.12622",
    "authors": [
      "Muhammad Asim Khan",
      "Lan Hong",
      "Sajjad Ahmed"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2201.12625",
    "title": "ADC-Net: An Open-Source Deep Learning Network for Automated Dispersion  Compensation in Optical Coherence Tomography",
    "abstract": "Chromatic dispersion is a common problem to degrade the system resolution in optical coherence tomography (OCT). This study is to develop a deep learning network for automated dispersion compensation (ADC-Net) in OCT. The ADC-Net is based on a redesigned UNet architecture which employs an encoder-decoder pipeline. The input section encompasses partially compensated OCT B-scans with individual retinal layers optimized. Corresponding output is a fully compensated OCT B-scans with all retinal layers optimized. Two numeric parameters, i.e., peak signal to noise ratio (PSNR) and structural similarity index metric computed at multiple scales (MS-SSIM), were used for objective assessment of the ADC-Net performance. Comparative analysis of training models, including single, three, five, seven and nine input channels were implemented. The five-input channels implementation was observed as the optimal mode for ADC-Net training to achieve robust dispersion compensation in OCT ",
    "url": "https://arxiv.org/abs/2201.12625",
    "authors": [
      "Shaiban Ahmed",
      "David Le",
      "Taeyoon Son",
      "Tobiloba Adejumo",
      "Xincheng Yao",
      "Department of Biomedical Engineering",
      "University of Illinois at Chicago",
      "Department of Ophthalmology",
      "Visual Science",
      "University of Illinois at Chicago"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Tissues and Organs (q-bio.TO)"
    ]
  },
  {
    "id": "arXiv:2201.12633",
    "title": "Image Classification using Graph Neural Network and Multiscale Wavelet  Superpixels",
    "abstract": "Prior studies using graph neural networks (GNNs) for image classification have focused on graphs generated from a regular grid of pixels or similar-sized superpixels. In the latter, a single target number of superpixels is defined for an entire dataset irrespective of differences across images and their intrinsic multiscale structure. On the contrary, this study investigates image classification using graphs generated from an image-specific number of multiscale superpixels. We propose WaveMesh, a new wavelet-based superpixeling algorithm, where the number and sizes of superpixels in an image are systematically computed based on its content. WaveMesh superpixel graphs are structurally different from similar-sized superpixel graphs. We use SplineCNN, a state-of-the-art network for image graph classification, to compare WaveMesh and similar-sized superpixels. Using SplineCNN, we perform extensive experiments on three benchmark datasets under three local-pooling settings: 1) no pooling, 2) GraclusPool, and 3) WavePool, a novel spatially heterogeneous pooling scheme tailored to WaveMesh superpixels. Our experiments demonstrate that SplineCNN learns from multiscale WaveMesh superpixels on-par with similar-sized superpixels. In all WaveMesh experiments, GraclusPool performs poorer than no pooling / WavePool, indicating that poor choice of pooling can result in inferior performance while learning from multiscale superpixels. ",
    "url": "https://arxiv.org/abs/2201.12633",
    "authors": [
      "Varun Vasudevan",
      "Maxime Bassenne",
      "Md Tauhidul Islam",
      "Lei Xing"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12646",
    "title": "Self Semi Supervised Neural Architecture Search for Semantic  Segmentation",
    "abstract": "In this paper, we propose a Neural Architecture Search strategy based on self supervision and semi-supervised learning for the task of semantic segmentation. Our approach builds an optimized neural network (NN) model for this task by jointly solving a jigsaw pretext task discovered with self-supervised learning over unlabeled training data, and, exploiting the structure of the unlabeled data with semi-supervised learning. The search of the architecture of the NN model is performed by dynamic routing using a gradient descent algorithm. Experiments on the Cityscapes and PASCAL VOC 2012 datasets demonstrate that the discovered neural network is more efficient than a state-of-the-art hand-crafted NN model with four times less floating operations. ",
    "url": "https://arxiv.org/abs/2201.12646",
    "authors": [
      "Lo\u00efc Pauletto",
      "Massih-Reza Amini",
      "Nicolas Winckler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12649",
    "title": "Transfer Learning for Estimation of Pendubot Angular Position Using Deep  Neural Networks",
    "abstract": "In this paper, a machine learning based approach is introduced to estimate Pendubot angular position from its captured images. Initially, a baseline algorithm is introduced to estimate the angle using conventional image processing technique. The baseline algorithm performs well for the cases that the Pendubot is not moving fast. However, when moving quickly due to a free fall, the Pendubot appears as a blurred object in the captured image in a way that the baseline algorithm fails to estimate the angle. Consequently, a Deep Neural Network (DNN) based algorithm is introduced to cope with this challenge. The approach relies on the concept of transfer learning to allow the training of the DNN on a very small fine-tuning dataset. The base algorithm is used to create the ground truth labels of the fine-tuning dataset. Experimental results on the held-out evaluation set show that the proposed approach achieves a median absolute error of 0.02 and 0.06 degrees for the sharp and blurry images respectively. ",
    "url": "https://arxiv.org/abs/2201.12649",
    "authors": [
      "Sina Khanagha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2201.12650",
    "title": "New results on the robust coloring problem",
    "abstract": "Many variations of the classical graph coloring model have been intensively studied due to their multiple applications; scheduling problems and aircraft assignments, for instance, motivate the \\emph{robust coloring problem}. This model gets to capture natural constraints of those optimization problems by combining the information provided by two colorings: a vertex coloring of a graph and the induced edge coloring on a subgraph of its complement; the goal is to minimize, among all proper colorings of the graph for a fixed number of colors, the number of edges in the subgraph with the endpoints of the same color. The study of the robust coloring model has been focused on the search for heuristics due to its NP-hard character when using at least three colors, but little progress has been made in other directions. We present a new approach on the problem obtaining the first collection of non heuristic results for general graphs; among them, we prove that robust coloring is the model that better approaches the partition of any system into equal or almost equal conflict-free subsystem, relating strongly this model with the well-known equitable colorings. We also show the NP-completeness of their decision problems for the unsolved case of two colors, obtain bounds on the associated robust coloring parameter, and solve a conjecture on paths that illustrates the complexity of studying this coloring model. ",
    "url": "https://arxiv.org/abs/2201.12650",
    "authors": [
      "Delia Garijo",
      "Alberto M\u00e1rquez",
      "Rafael Robles"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2201.12657",
    "title": "Prediction of terephthalic acid (TPA) yield in aqueous hydrolysis of  polyethylene terephthalate (PET)",
    "abstract": "Aqueous hydrolysis is used to chemically recycle polyethylene terephthalate (PET) due to the production of high-quality terephthalic acid (TPA), the PET monomer. PET hydrolysis depends on various reaction conditions including PET size, catalyst concentration, reaction temperature, etc. So, modeling PET hydrolysis by considering the effective factors can provide useful information for material scientists to specify how to design and run these reactions. It will save time, energy, and materials by optimizing the hydrolysis conditions. Machine learning algorithms enable to design models to predict output results. For the first time, 381 experimental data were gathered to model the aqueous hydrolysis of PET. Effective reaction conditions on PET hydrolysis were connected to TPA yield. The logistic regression was applied to rank the reaction conditions. Two algorithms were proposed, artificial neural network multilayer perceptron (ANN-MLP) and adaptive network-based fuzzy inference system (ANFIS). The dataset was divided into training and testing sets to train and test the models, respectively. The models predicted TPA yield sufficiently where the ANFIS model outperformed. R-squared (R2) and Root Mean Square Error (RMSE) loss functions were employed to measure the efficiency of the models and evaluate their performance. ",
    "url": "https://arxiv.org/abs/2201.12657",
    "authors": [
      "Hossein Abedsoltan",
      "Zeinab Zoghi",
      "Amir H. Mohammadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.12666",
    "title": "Challenges and approaches to privacy preserving post-click conversion  prediction",
    "abstract": "Online advertising has typically been more personalized than offline advertising, through the use of machine learning models and real-time auctions for ad targeting. One specific task, predicting the likelihood of conversion (i.e.\\ the probability a user will purchase the advertised product), is crucial to the advertising ecosystem for both targeting and pricing ads. Currently, these models are often trained by observing individual user behavior, but, increasingly, regulatory and technical constraints are requiring privacy-preserving approaches. For example, major platforms are moving to restrict tracking individual user events across multiple applications, and governments around the world have shown steadily more interest in regulating the use of personal data. Instead of receiving data about individual user behavior, advertisers may receive privacy-preserving feedback, such as the number of installs of an advertised app that resulted from a group of users. In this paper we outline the recent privacy-related changes in the online advertising ecosystem from a machine learning perspective. We provide an overview of the challenges and constraints when learning conversion models in this setting. We introduce a novel approach for training these models that makes use of post-ranking signals. We show using offline experiments on real world data that it outperforms a model relying on opt-in data alone, and significantly reduces model degradation when no individual labels are available. Finally, we discuss future directions for research in this evolving area. ",
    "url": "https://arxiv.org/abs/2201.12666",
    "authors": [
      "Conor O'Brien",
      "Arvind Thiagarajan",
      "Sourav Das",
      "Rafael Barreto",
      "Chetan Verma",
      "Tim Hsu",
      "James Neufield",
      "Jonathan J Hunt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2201.12667",
    "title": "Distributed SLIDE: Enabling Training Large Neural Networks on Low  Bandwidth and Simple CPU-Clusters via Model Parallelism and Sparsity",
    "abstract": "More than 70% of cloud computing is paid for but sits idle. A large fraction of these idle compute are cheap CPUs with few cores that are not utilized during the less busy hours. This paper aims to enable those CPU cycles to train heavyweight AI models. Our goal is against mainstream frameworks, which focus on leveraging expensive specialized ultra-high bandwidth interconnect to address the communication bottleneck in distributed neural network training. This paper presents a distributed model-parallel training framework that enables training large neural networks on small CPU clusters with low Internet bandwidth. We build upon the adaptive sparse training framework introduced by the SLIDE algorithm. By carefully deploying sparsity over distributed nodes, we demonstrate several orders of magnitude faster model parallel training than Horovod, the main engine behind most commercial software. We show that with reduced communication, due to sparsity, we can train close to a billion parameter model on simple 4-16 core CPU nodes connected by basic low bandwidth interconnect. Moreover, the training time is at par with some of the best hardware accelerators. ",
    "url": "https://arxiv.org/abs/2201.12667",
    "authors": [
      "Minghao Yan",
      "Nicholas Meisburger",
      "Tharun Medini",
      "Anshumali Shrivastava"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12670",
    "title": "SMGRL: A Scalable Multi-resolution Graph Representation Learning  Framework",
    "abstract": "Graph convolutional networks (GCNs) allow us to learn topologically-aware node embeddings, which can be useful for classification or link prediction. However, by construction, they lack positional awareness and are unable to capture long-range dependencies without adding additional layers -- which in turn leads to over-smoothing and increased time and space complexity. Further, the complex dependencies between nodes make mini-batching challenging, limiting their applicability to large graphs. This paper proposes a Scalable Multi-resolution Graph Representation Learning (SMGRL) framework that enables us to learn multi-resolution node embeddings efficiently. Our framework is model-agnostic and can be applied to any existing GCN model. We dramatically reduce training costs by training only on a reduced-dimension coarsening of the original graph, then exploit self-similarity to apply the resulting algorithm at multiple resolutions. Inference of these multi-resolution embeddings can be distributed across multiple machines to reduce computational and memory requirements further. The resulting multi-resolution embeddings can be aggregated to yield high-quality node embeddings that capture both long- and short-range dependencies between nodes. Our experiments show that this leads to improved classification accuracy, without incurring high computational costs. ",
    "url": "https://arxiv.org/abs/2201.12670",
    "authors": [
      "Reza Namazi",
      "Elahe Ghalebi",
      "Sinead Williamson",
      "Hamidreza Mahyar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.12674",
    "title": "Rewiring with Positional Encodings for Graph Neural Networks",
    "abstract": "Several recent works use positional encodings to extend the receptive fields of graph neural network (GNN) layers equipped with attention mechanisms. These techniques, however, extend receptive fields to the complete graph, at substantial computational cost and risking a change in the inductive biases of conventional GNNs, or require complex architecture adjustments. As a conservative alternative, we use positional encodings to expand receptive fields to any r-ring. Our method augments the input graph with additional nodes/edges and uses positional encodings as node and/or edge features. Thus, it is compatible with many existing GNN architectures. We also provide examples of positional encodings that are non-invasive, i.e., there is a one-to-one map between the original and the modified graphs. Our experiments demonstrate that extending receptive fields via positional encodings and a virtual fully-connected node significantly improves GNN performance and alleviates over-squashing using small r. We obtain improvements across models, showing state-of-the-art performance even using older architectures than recent Transformer models adapted to graphs. ",
    "url": "https://arxiv.org/abs/2201.12674",
    "authors": [
      "Rickard Br\u00fcel-Gabrielsson",
      "Mikhail Yurochkin",
      "Justin Solomon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12675",
    "title": "Decepticons: Corrupted Transformers Breach Privacy in Federated Learning  for Language Models",
    "abstract": "A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought. ",
    "url": "https://arxiv.org/abs/2201.12675",
    "authors": [
      "Liam Fowl",
      "Jonas Geiping",
      "Steven Reich",
      "Yuxin Wen",
      "Wojtek Czaja",
      "Micah Goldblum",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2201.12678",
    "title": "A Stochastic Bundle Method for Interpolating Networks",
    "abstract": "We propose a novel method for training deep neural networks that are capable of interpolation, that is, driving the empirical loss to zero. At each iteration, our method constructs a stochastic approximation of the learning objective. The approximation, known as a bundle, is a pointwise maximum of linear functions. Our bundle contains a constant function that lower bounds the empirical loss. This enables us to compute an automatic adaptive learning rate, thereby providing an accurate solution. In addition, our bundle includes linear approximations computed at the current iterate and other linear estimates of the DNN parameters. The use of these additional approximations makes our method significantly more robust to its hyperparameters. Based on its desirable empirical properties, we term our method Bundle Optimisation for Robust and Accurate Training (BORAT). In order to operationalise BORAT, we design a novel algorithm for optimising the bundle approximation efficiently at each iteration. We establish the theoretical convergence of BORAT in both convex and non-convex settings. Using standard publicly available data sets, we provide a thorough comparison of BORAT to other single hyperparameter optimisation algorithms. Our experiments demonstrate BORAT matches the state-of-the-art generalisation performance for these methods and is the most robust. ",
    "url": "https://arxiv.org/abs/2201.12678",
    "authors": [
      "Alasdair Paren",
      "Leonard Berrada",
      "Rudra P. K. Poudel",
      "M. Pawan Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.12686",
    "title": "Robustness of Deep Recommendation Systems to Untargeted Interaction  Perturbations",
    "abstract": "While deep learning-based sequential recommender systems are widely used in practice, their sensitivity to untargeted training data perturbations is unknown. Untargeted perturbations aim to modify ranked recommendation lists for all users at test time, by inserting imperceptible input perturbations during training time. Existing perturbation methods are mostly targeted attacks optimized to change ranks of target items, but not suitable for untargeted scenarios. In this paper, we develop a novel framework in which user-item training interactions are perturbed in unintentional and adversarial settings. First, through comprehensive experiments on four datasets, we show that four popular recommender models are unstable against even one random perturbation. Second, we establish a cascading effect in which minor manipulations of early training interactions can cause extensive changes to the model and the generated recommendations for all users. Leveraging this effect, we propose an adversarial perturbation method CASPER which identifies and perturbs an interaction that induces the maximal cascading effect. Experimentally, we demonstrate that CASPER reduces the stability of recommendation models the most, compared to several baselines and state-of-the-art methods. Finally, we show the runtime and success of CASPER scale near-linearly with the dataset size and the number of perturbations, respectively. ",
    "url": "https://arxiv.org/abs/2201.12686",
    "authors": [
      "Sejoon Oh",
      "Srijan Kumar"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2201.12700",
    "title": "Coordinated Attacks against Contextual Bandits: Fundamental Limits and  Defense Mechanisms",
    "abstract": "Motivated by online recommendation systems, we propose the problem of finding the optimal policy in multitask contextual bandits when a small fraction $\\alpha < 1/2$ of tasks (users) are arbitrary and adversarial. The remaining fraction of good users share the same instance of contextual bandits with $S$ contexts and $A$ actions (items). Naturally, whether a user is good or adversarial is not known in advance. The goal is to robustly learn the policy that maximizes rewards for good users with as few user interactions as possible. Without adversarial users, established results in collaborative filtering show that $O(1/\\epsilon^2)$ per-user interactions suffice to learn a good policy, precisely because information can be shared across users. This parallelization gain is fundamentally altered by the presence of adversarial users: unless there are super-polynomial number of users, we show a lower bound of $\\tilde{\\Omega}(\\min(S,A) \\cdot \\alpha^2 / \\epsilon^2)$ {\\it per-user} interactions to learn an $\\epsilon$-optimal policy for the good users. We then show we can achieve an $\\tilde{O}(\\min(S,A)\\cdot \\alpha/\\epsilon^2)$ upper-bound, by employing efficient robust mean estimators for both uni-variate and high-dimensional random variables. We also show that this can be improved depending on the distributions of contexts. ",
    "url": "https://arxiv.org/abs/2201.12700",
    "authors": [
      "Jeongyeol Kwon",
      "Yonathan Efroni",
      "Constantine Caramanis",
      "Shie Mannor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2201.12705",
    "title": "A Robust Framework for Deep Learning Approaches to Facial Emotion  Recognition and Evaluation",
    "abstract": "Facial emotion recognition is a vast and complex problem space within the domain of computer vision and thus requires a universally accepted baseline method with which to evaluate proposed models. While test datasets have served this purpose in the academic sphere real world application and testing of such models lacks any real comparison. Therefore we propose a framework in which models developed for FER can be compared and contrasted against one another in a constant standardized fashion. A lightweight convolutional neural network is trained on the AffectNet dataset a large variable dataset for facial emotion recognition and a web application is developed and deployed with our proposed framework as a proof of concept. The CNN is embedded into our application and is capable of instant real time facial emotion recognition. When tested on the AffectNet test set this model achieves high accuracy for emotion classification of eight different emotions. Using our framework the validity of this model and others can be properly tested by evaluating a model efficacy not only based on its accuracy on a sample test dataset, but also on in the wild experiments. Additionally, our application is built with the ability to save and store any image captured or uploaded to it for emotion recognition, allowing for the curation of more quality and diverse facial emotion recognition datasets. ",
    "url": "https://arxiv.org/abs/2201.12705",
    "authors": [
      "Nyle Siddiqui",
      "Rushit Dave",
      "Tyler Bauer",
      "Thomas Reither",
      "Dylan Black",
      "Mitchell Hanson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2201.12712",
    "title": "Win the Lottery Ticket via Fourier Analysis: Frequencies Guided Network  Pruning",
    "abstract": "With the remarkable success of deep learning recently, efficient network compression algorithms are urgently demanded for releasing the potential computational power of edge devices, such as smartphones or tablets. However, optimal network pruning is a non-trivial task which mathematically is an NP-hard problem. Previous researchers explain training a pruned network as buying a lottery ticket. In this paper, we investigate the Magnitude-Based Pruning (MBP) scheme and analyze it from a novel perspective through Fourier analysis on the deep learning model to guide model designation. Besides explaining the generalization ability of MBP using Fourier transform, we also propose a novel two-stage pruning approach, where one stage is to obtain the topological structure of the pruned network and the other stage is to retrain the pruned network to recover the capacity using knowledge distillation from lower to higher on the frequency domain. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate the superiority of our novel Fourier analysis based MBP compared to other traditional MBP algorithms. ",
    "url": "https://arxiv.org/abs/2201.12712",
    "authors": [
      "Yuzhang Shang",
      "Bin Duan",
      "Ziliang Zong",
      "Liqiang Nie",
      "Yan Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.12724",
    "title": "Stochastic Neural Networks with Infinite Width are Deterministic",
    "abstract": "This work theoretically studies stochastic neural networks, a main type of neural network in use. Specifically, we prove that as the width of an optimized stochastic neural network tends to infinity, its predictive variance on the training set decreases to zero. Two common examples that our theory applies to are neural networks with dropout and variational autoencoders. Our result helps better understand how stochasticity affects the learning of neural networks and thus design better architectures for practical problems. ",
    "url": "https://arxiv.org/abs/2201.12724",
    "authors": [
      "Liu Ziyin",
      "Hanlin Zhang",
      "Xiangming Meng",
      "Yuting Lu",
      "Eric Xing",
      "Masahito Ueda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2201.12725",
    "title": "Neural Architecture Ranker",
    "abstract": "Architecture ranking has recently been advocated to design an efficient and effective performance predictor for Neural Architecture Search (NAS). The previous contrastive method solves the ranking problem by comparing pairs of architectures and predicting their relative performance, which may suffer generalization issues due to local pair-wise comparison. Inspired by the quality stratification phenomenon in the search space, we propose a predictor, namely Neural Architecture Ranker (NAR), from a new and global perspective by exploiting the quality distribution of the whole search space. The NAR learns the similar characteristics of the same quality tier (i.e., level) and distinguishes among different individuals by first matching architectures with the representation of tiers, and then classifying and scoring them. It can capture the features of different quality tiers and thus generalize its ranking ability to the entire search space. Besides, distributions of different quality tiers are also beneficial to guide the sampling procedure, which is free of training a search algorithm and thus simplifies the NAS pipeline. The proposed NAR achieves better performance than the state-of-the-art methods on two widely accepted datasets. On NAS-Bench-101, it finds the architectures with top 0.01$\\unicode{x2030}$ performance among the search space and stably focuses on the top architectures. On NAS-Bench-201, it identifies the optimal architectures on CIFAR-10, CIFAR-100 and, ImageNet-16-120. We expand and release these two datasets covering detailed cell computational information to boost the study of NAS. ",
    "url": "https://arxiv.org/abs/2201.12725",
    "authors": [
      "Bicheng Guo",
      "Shibo He",
      "Tao Chen",
      "Jiming Chen",
      "Peng Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.12738",
    "title": "AutoSNN: Towards Energy-Efficient Spiking Neural Networks",
    "abstract": "Spiking neural networks (SNNs) that mimic information transmission in the brain can energy-efficiently process spatio-temporal information through discrete and sparse spikes, thereby receiving considerable attention. To improve accuracy and energy efficiency of SNNs, most previous studies have focused solely on training methods, and the effect of architecture has rarely been studied. We investigate the design choices used in the previous studies in terms of the accuracy and number of spikes and figure out that they are not best-suited for SNNs. To further improve the accuracy and reduce the spikes generated by SNNs, we propose a spike-aware neural architecture search framework called AutoSNN. We define a search space consisting of architectures without undesirable design choices. To enable the spike-aware architecture search, we introduce a fitness that considers both the accuracy and number of spikes. AutoSNN successfully searches for SNN architectures that outperform hand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate the effectiveness of AutoSNN on various datasets including neuromorphic datasets. ",
    "url": "https://arxiv.org/abs/2201.12738",
    "authors": [
      "Byunggook Na",
      "Jisoo Mok",
      "Seongsik Park",
      "Dongjin Lee",
      "Hyeokjun Choe",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12741",
    "title": "GARNET: Reduced-Rank Topology Learning for Robust and Scalable Graph  Neural Networks",
    "abstract": "Graph neural networks (GNNs) have been increasingly deployed in various applications that involve learning on non-Euclidean data. However, recent studies show that GNNs are vulnerable to graph adversarial attacks. Although there are several defense methods to improve GNN robustness by eliminating adversarial components, they may also impair the underlying clean graph structure that contributes to GNN training. In addition, few of those defense models can scale to large graphs due to their high computational complexity and memory usage. In this paper, we propose GARNET, a scalable spectral method to boost the adversarial robustness of GNN models. GARNET first leverages weighted spectral embedding to construct a base graph, which is not only resistant to adversarial attacks but also contains critical (clean) graph structure for GNN training. Next, GARNET further refines the base graph by pruning additional uncritical edges based on probabilistic graphical model. GARNET has been evaluated on various datasets, including a large graph with millions of nodes. Our extensive experiment results show that GARNET achieves adversarial accuracy improvement and runtime speedup over state-of-the-art GNN (defense) models by up to 13.27% and 14.7x, respectively. ",
    "url": "https://arxiv.org/abs/2201.12741",
    "authors": [
      "Chenhui Deng",
      "Xiuyu Li",
      "Zhuo Feng",
      "Zhiru Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12760",
    "title": "Implicit Regularization Towards Rank Minimization in ReLU Networks",
    "abstract": "We study the conjectured relationship between the implicit regularization in neural networks, trained with gradient-based methods, and rank minimization of their weight matrices. Previously, it was proved that for linear networks (of depth 2 and vector-valued outputs), gradient flow (GF) w.r.t. the square loss acts as a rank minimization heuristic. However, understanding to what extent this generalizes to nonlinear networks is an open problem. In this paper, we focus on nonlinear ReLU networks, providing several new positive and negative results. On the negative side, we prove (and demonstrate empirically) that, unlike the linear case, GF on ReLU networks may no longer tend to minimize ranks, in a rather strong sense (even approximately, for \"most\" datasets of size 2). On the positive side, we reveal that ReLU networks of sufficient depth are provably biased towards low-rank solutions in several reasonable settings. ",
    "url": "https://arxiv.org/abs/2201.12760",
    "authors": [
      "Nadav Timor",
      "Gal Vardi",
      "Ohad Shamir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2201.12765",
    "title": "Improving Corruption and Adversarial Robustness by Enhancing Weak  Subnets",
    "abstract": "Deep neural networks have achieved great success in many computer vision tasks. However, deep networks have been shown to be very susceptible to corrupted or adversarial images, which often result in significant performance drops. In this paper, we observe that weak subnetwork (subnet) performance is correlated with a lack of robustness against corruptions and adversarial attacks. Based on that observation, we propose a novel robust training method which explicitly identifies and enhances weak subnets (EWS) during training to improve robustness. Specifically, we develop a search algorithm to find particularly weak subnets and propose to explicitly strengthen them via knowledge distillation from the full network. We show that our EWS greatly improves the robustness against corrupted images as well as the accuracy on clean data. Being complementary to many state-of-the-art data augmentation approaches, EWS consistently improves corruption robustness on top of many of these approaches. Moreover, EWS is also able to boost the adversarial robustness when combined with popular adversarial training methods. ",
    "url": "https://arxiv.org/abs/2201.12765",
    "authors": [
      "Yong Guo",
      "David Stutz",
      "Bernt Schiele"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.12771",
    "title": "Self-Supervised Moving Vehicle Detection from Audio-Visual Cues",
    "abstract": "Robust detection of moving vehicles is a critical task for any autonomously operating outdoor robot or self-driving vehicle. Most modern approaches for solving this task rely on training image-based detectors using large-scale vehicle detection datasets such as nuScenes or the Waymo Open Dataset. Providing manual annotations is an expensive and laborious exercise that does not scale well in practice. To tackle this problem, we propose a self-supervised approach that leverages audio-visual cues to detect moving vehicles in videos. Our approach employs contrastive learning for localizing vehicles in images from corresponding pairs of images and recorded audio. In extensive experiments carried out with a real-world dataset, we demonstrate that our approach provides accurate detections of moving vehicles and does not require manual annotations. We furthermore show that our model can be used as a teacher to supervise an audio-only detection model. This student model is invariant to illumination changes and thus effectively bridges the domain gap inherent to models leveraging exclusively vision as the predominant modality. ",
    "url": "https://arxiv.org/abs/2201.12771",
    "authors": [
      "Jannik Z\u00fcrn",
      "Wolfram Burgard"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.12776",
    "title": "Graph Convolution-Based Deep Reinforcement Learning for Multi-Agent  Decision-Making in Mixed Traffic Environments",
    "abstract": "An efficient and reliable multi-agent decision-making system is highly demanded for the safe and efficient operation of connected autonomous vehicles in intelligent transportation systems. Current researches mainly focus on the Deep Reinforcement Learning (DRL) methods. However, utilizing DRL methods in interactive traffic scenarios is hard to represent the mutual effects between different vehicles and model the dynamic traffic environments due to the lack of interactive information in the representation of the environments, which results in low accuracy of cooperative decisions generation. To tackle these difficulties, this research proposes a framework to enable different Graph Reinforcement Learning (GRL) methods for decision-making, and compares their performance in interactive driving scenarios. GRL methods combinate the Graph Neural Network (GNN) and DRL to achieve the better decisions generation in interactive scenarios of autonomous vehicles, where the features of interactive scenarios are extracted by the GNN, and cooperative behaviors are generated by DRL framework. Several GRL approaches are summarized and implemented in the proposed framework. To evaluate the performance of the proposed GRL methods, an interactive driving scenarios on highway with two ramps is constructed, and simulated experiment in the SUMO platform is carried out to evaluate the performance of different GRL approaches. Finally, results are analyzed in multiple perspectives and dimensions to compare the characteristic of different GRL approaches in intelligent transportation scenarios. Results show that the implementation of GNN can well represents the interaction between vehicles, and the combination of GNN and DRL is able to improve the performance of the generation of lane-change behaviors. The source code of our work can be found at https://github.com/Jacklinkk/TorchGRL. ",
    "url": "https://arxiv.org/abs/2201.12776",
    "authors": [
      "Qi Liu",
      "Zirui Li",
      "Xueyuan Li",
      "Jingda Wu",
      "Shihua Yuan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.12781",
    "title": "Low Rank Approximation of Dual Complex Matrices",
    "abstract": "Dual complex numbers can represent rigid body motion in 2D spaces. Dual complex matrices are linked with screw theory, and have potential applications in various areas. In this paper, we study low rank approximation of dual complex matrices. We define $2$-norm for dual complex vectors, and Frobenius norm for dual complex matrices. These norms are nonnegative dual numbers. We establish the unitary invariance property of dual complex matrices. We study eigenvalues of square dual complex matrices, and show that an $n \\times n$ dual complex Hermitian matrix has exactly $n$ eigenvalues, which are dual numbers. We present a singular value decomposition (SVD) theorem for dual complex matrices, define ranks and appreciable ranks for dual complex matrices, and study their properties. We establish an Eckart-Young like theorem for dual complex matrices, and present an algorithm framework for low rank approximation of dual complex matrices via truncated SVD. The SVD of dual complex matrices also provides a basic tool for Principal Component Analysis (PCA) via these matrices. Numerical experiments are reported. ",
    "url": "https://arxiv.org/abs/2201.12781",
    "authors": [
      "Liqun Qi",
      "David M. Alexander",
      "Zhongming Chen",
      "Chen Ling",
      "Ziyan Luo"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Rings and Algebras (math.RA)"
    ]
  },
  {
    "id": "arXiv:2201.12787",
    "title": "Graph Self-Attention for learning graph representation with Transformer",
    "abstract": "We propose a novel Graph Self-Attention module to enable Transformer models to learn graph representation. We aim to incorporate graph information, on the attention map and hidden representations of Transformer. To this end, we propose context-aware attention which considers the interactions between query, key and graph information. Moreover, we propose graph-embedded value to encode the graph information on the hidden representation. Our extensive experiments and ablation studies validate that our method successfully encodes graph representation on Transformer architecture. Finally, our method achieves state-of-the-art performance on multiple benchmarks of graph representation learning, such as graph classification on images and molecules to graph regression on quantum chemistry. ",
    "url": "https://arxiv.org/abs/2201.12787",
    "authors": [
      "Wonpyo Park",
      "Woonggi Chang",
      "Donggeon Lee",
      "Juntae Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.12795",
    "title": "Training Thinner and Deeper Neural Networks: Jumpstart Regularization",
    "abstract": "Neural networks are more expressive when they have multiple layers. In turn, conventional training methods are only successful if the depth does not lead to numerical issues such as exploding or vanishing gradients, which occur less frequently when the layers are sufficiently wide. However, increasing width to attain greater depth entails the use of heavier computational resources and leads to overparameterized models. These subsequent issues have been partially addressed by model compression methods such as quantization and pruning, some of which relying on normalization-based regularization of the loss function to make the effect of most parameters negligible. In this work, we propose instead to use regularization for preventing neurons from dying or becoming linear, a technique which we denote as jumpstart regularization. In comparison to conventional training, we obtain neural networks that are thinner, deeper, and - most importantly - more parameter-efficient. ",
    "url": "https://arxiv.org/abs/2201.12795",
    "authors": [
      "Carles Riera",
      "Camilo Rey",
      "Thiago Serra",
      "Eloi Puertas",
      "Oriol Pujol"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2201.12796",
    "title": "Co-Regularized Adversarial Learning for Multi-Domain Text Classification",
    "abstract": "Multi-domain text classification (MDTC) aims to leverage all available resources from multiple domains to learn a predictive model that can generalize well on these domains. Recently, many MDTC methods adopt adversarial learning, shared-private paradigm, and entropy minimization to yield state-of-the-art results. However, these approaches face three issues: (1) Minimizing domain divergence can not fully guarantee the success of domain alignment; (2) Aligning marginal feature distributions can not fully guarantee the discriminability of the learned features; (3) Standard entropy minimization may make the predictions on unlabeled data over-confident, deteriorating the discriminability of the learned features. In order to address the above issues, we propose a co-regularized adversarial learning (CRAL) mechanism for MDTC. This approach constructs two diverse shared latent spaces, performs domain alignment in each of them, and punishes the disagreements of these two alignments with respect to the predictions on unlabeled data. Moreover, virtual adversarial training (VAT) with entropy minimization is incorporated to impose consistency regularization to the CRAL method. Experiments show that our model outperforms state-of-the-art methods on two MDTC benchmarks. ",
    "url": "https://arxiv.org/abs/2201.12796",
    "authors": [
      "Yuan Wu",
      "Diana Inkpen",
      "Ahmed El-Roby"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2201.12809",
    "title": "OverChain: Building a robust overlay with a blockchain",
    "abstract": "Blockchains use peer-to-peer networks for disseminating information among peers, but these networks currently do not have any provable guarantees for desirable properties such as Byzantine fault tolerance, good connectivity and small diameter. This is not just a theoretical problem, as recent works have exploited unsafe peer connection policies and weak network synchronization to mount partitioning attacks on Bitcoin. Cryptocurrency blockchains are safety critical systems, so we need principled algorithms to maintain their networks. Our key insight is that we can leverage the blockchain itself to share information among the peers, and thus simplify the network maintenance process. Given that the peers have restricted computational resources, and at most a constant fraction of them are Byzantine, we provide communication-efficient protocols to maintain a hypercubic network for blockchains, where peers can join and leave over time. Interestingly, we discover that our design can \\emph{recover} from substantial adversarial failures. Moreover, these properties hold despite significant churn. A key contribution is a secure mechanism for joining the network that uses the blockchain to help new peers to contact existing peers. Furthermore, by examining how peers join the network, i.e., the \"bootstrapping service,\" we give a lower bound showing that (within log factors) our network tolerates the maximum churn rate possible. In fact, we can give a lower bound on churn for any fully distributed service that requires connectivity. ",
    "url": "https://arxiv.org/abs/2201.12809",
    "authors": [
      "Vijeth Aradhya",
      "Seth Gilbert",
      "Aquinas Hobor"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2201.12811",
    "title": "A DFS Algorithm for Maximum Matchings in General Graphs",
    "abstract": "In this paper, we propose a depth-first search (DFS) algorithm for searching maximum matchings in general graphs. Unlike blossom shrinking algorithms, which store all possible alternative alternating paths in the super-vertices shrunk from blossoms, the newly proposed algorithm does not involve blossom shrinking. The basic idea is to deflect the alternating path when facing blossoms. The algorithm maintains detour information in an auxiliary stack to minimize the redundant data structures. A benefit of our technique is to avoid spending time on shrinking and expanding blossoms. This DFS algorithm can determine a maximum matching of a general graph with $m$ edges and $n$ vertices in $O(mn)$ time with space complexity $O(n)$. ",
    "url": "https://arxiv.org/abs/2201.12811",
    "authors": [
      "Tony T. Lee",
      "Bojun Lu",
      "Hanli Chu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2201.12816",
    "title": "Adaptive Contraction-based Control of Uncertain Nonlinear Processes  using Neural Networks",
    "abstract": "Driven by the flexible manufacturing trend in the process control industry and the uncertain nature of chemical process models, this article aims to achieve offset-free tracking for a family of uncertain nonlinear systems (e.g., using process models with parametric uncertainties) with adaptable performance. The proposed adaptive control approach incorporates into the control loop an adaptive neural network embedded contraction-based controller (to ensure convergence to time-varying references) and an online parameter identification module coupled with reference generation (to ensure modelled parameters converge those of the physical system). The integrated learning and control approach involves training a state and parameter dependent neural network to learn a contraction metric parameterized by the uncertain parameter and a differential feedback gain. This neural network is then embedded in an adaptive contraction-based control law which is updated by parameter estimates online. As uncertain parameter estimates converge to the corresponding physical values, offset-free tracking, simultaneously with improved convergence rates, can be achieved, resulting in a flexible, efficient and less conservative approach to the reference tracking control of uncertain nonlinear processes. An illustrative example is included to demonstrate the overall approach. An illustrative example is included to demonstrate the overall approach. ",
    "url": "https://arxiv.org/abs/2201.12816",
    "authors": [
      "Lai Wei",
      "Ryan McCloy",
      "Jie Bao"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2201.12825",
    "title": "Hyperbolic Neural Networks for Molecular Generation",
    "abstract": "With the recent advance of deep learning, neural networks have been extensively used for the task of molecular generation. Many deep generators extract atomic relations from molecular graphs and ignore hierarchical information at both atom and molecule levels. In order to extract such hierarchical information, we propose a novel hyperbolic generative model. Our model contains three parts: first, a fully hyperbolic junction-tree encoder-decoder that embeds the hierarchical information of the molecules in the latent hyperbolic space; second, a latent generative adversarial network for generating the latent embeddings; third, a molecular generator that inherits the decoders from the first part and the latent generator from the second part. We evaluate our model on the ZINC dataset using the MOSES benchmarking platform and achieve competitive results, especially in metrics about structural similarity. ",
    "url": "https://arxiv.org/abs/2201.12825",
    "authors": [
      "Eric Qu",
      "Dongmian Zou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2201.12826",
    "title": "Optimizing Gradient-driven Criteria in Network Sparsity: Gradient is All  You Need",
    "abstract": "Network sparsity receives popularity mostly due to its capability to reduce the network complexity. Extensive studies excavate gradient-driven sparsity. Typically, these methods are constructed upon premise of weight independence, which however, is contrary to the fact that weights are mutually influenced. Thus, their performance remains to be improved. In this paper, we propose to further optimize gradient-driven sparsity (OptG) by solving this independence paradox. Our motive comes from the recent advances on supermask training which shows that sparse subnetworks can be located in a randomly initialized network by simply updating mask values without modifying any weight. We prove that supermask training is to accumulate the weight gradients and can partly solve the independence paradox. Consequently, OptG integrates supermask training into gradient-driven sparsity, and a specialized mask optimizer is designed to solve the independence paradox. Experiments show that OptG can well surpass many existing state-of-the-art competitors. Our code is available at \\url{https://github.com/zyxxmu/OptG}. ",
    "url": "https://arxiv.org/abs/2201.12826",
    "authors": [
      "Yuxin Zhang",
      "Mingbao Lin",
      "Mengzhao Chen",
      "Zihan Xu",
      "Fei Chao",
      "Yunhan Shen",
      "Ke Li",
      "Yongjian Wu",
      "Rongrong Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.12830",
    "title": "Over-smoothing Effect of Graph Convolutional Networks",
    "abstract": "Over-smoothing is a severe problem which limits the depth of Graph Convolutional Networks. This article gives a comprehensive analysis of the mechanism behind Graph Convolutional Networks and the over-smoothing effect. The article proposes an upper bound for the occurrence of over-smoothing, which offers insight into the key factors behind over-smoothing. The results presented in this article successfully explain the feasibility of several algorithms that alleviate over-smoothing. ",
    "url": "https://arxiv.org/abs/2201.12830",
    "authors": [
      "Fang Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12843",
    "title": "On Recoverability of Graph Neural Network Representations",
    "abstract": "Despite their growing popularity, graph neural networks (GNNs) still have multiple unsolved problems, including finding more expressive aggregation methods, propagation of information to distant nodes, and training on large-scale graphs. Understanding and solving such problems require developing analytic tools and techniques. In this work, we propose the notion of recoverability, which is tightly related to information aggregation in GNNs, and based on this concept, develop the method for GNN embedding analysis. We define recoverability theoretically and propose a method for its efficient empirical estimation. We demonstrate, through extensive experimental results on various datasets and different GNN architectures, that estimated recoverability correlates with aggregation method expressivity and graph sparsification quality. Therefore, we believe that the proposed method could provide an essential tool for understanding the roots of the aforementioned problems, and potentially lead to a GNN design that overcomes them. The code to reproduce our experiments is available at https://github.com/Anonymous1252022/Recoverability ",
    "url": "https://arxiv.org/abs/2201.12843",
    "authors": [
      "Maxim Fishman",
      "Chaim Baskin",
      "Evgenii Zheltonozhskii",
      "Ron Banner",
      "Avi Mendelson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12845",
    "title": "Potential Destination Prediction Based on Knowledge Graph Under Low  Predictability Data Condition",
    "abstract": "Destination prediction has been a critical topic in transportation research, and there are a large number of studies. However, almost all existing studies are based on high predictability data conditions while pay less attention to the data condition with low predictability, where the regularity of single individuals is not exposed. Based on a certain period of observation, there is a fact that individuals may choose destinations beyond observation, which we call \"potential destinations\". The number of potential destinations is very large and can't be ignored for the data condition with low predictability formed by short-term observation.To reveal the choice pattern of potential destination of individuals under the data condition with low predictability, we propose a global optimization method based on knowledge graph embedding. First, we joint the trip data of all individuals by constructing Trip Knowledge Graph(TKG). Next, we optimize the general algorithm of knowledge graph embedding for our data and task in training strategy and objective function, then implement it on TKG. It can achieve global optimization for association paths that exist between almost any two entities in TKG. On this basis, a method for potential destination prediction is proposed, giving the possible ranking of unobserved destinations for each individual. In addition, we improve the performance by fusing static statistical information that is not passed to TKG. Finally, we validate our method in a real-world dataset, and the prediction results are highly consistent with individuals' potential destination choice behaviour. ",
    "url": "https://arxiv.org/abs/2201.12845",
    "authors": [
      "Guilong Li",
      "Yixian Chen",
      "Qionghua Liao",
      "Zhaocheng He"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.12861",
    "title": "Neural-PIM: Efficient Processing-In-Memory with Neural Approximation of  Peripherals",
    "abstract": "Processing-in-memory (PIM) architectures have demonstrated great potential in accelerating numerous deep learning tasks. Particularly, resistive random-access memory (RRAM) devices provide a promising hardware substrate to build PIM accelerators due to their abilities to realize efficient in-situ vector-matrix multiplications (VMMs). However, existing PIM accelerators suffer from frequent and energy-intensive analog-to-digital (A/D) conversions, severely limiting their performance. This paper presents a new PIM architecture to efficiently accelerate deep learning tasks by minimizing the required A/D conversions with analog accumulation and neural approximated peripheral circuits. We first characterize the different dataflows employed by existing PIM accelerators, based on which a new dataflow is proposed to remarkably reduce the required A/D conversions for VMMs by extending shift and add (S+A) operations into the analog domain before the final quantizations. We then leverage a neural approximation method to design both analog accumulation circuits (S+A) and quantization circuits (ADCs) with RRAM crossbar arrays in a highly-efficient manner. Finally, we apply them to build an RRAM-based PIM accelerator (i.e., \\textbf{Neural-PIM}) upon the proposed analog dataflow and evaluate its system-level performance. Evaluations on different benchmarks demonstrate that Neural-PIM can improve energy efficiency by 5.36x (1.73x) and speed up throughput by 3.43x (1.59x) without losing accuracy, compared to the state-of-the-art RRAM-based PIM accelerators, i.e., ISAAC (CASCADE). ",
    "url": "https://arxiv.org/abs/2201.12861",
    "authors": [
      "Weidong Cao",
      "Yilong Zhao",
      "Adith Boloor",
      "Yinhe Han",
      "Xuan Zhang",
      "Li Jiang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12872",
    "title": "Discovering Invariant Rationales for Graph Neural Networks",
    "abstract": "Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features -- rationale -- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN. ",
    "url": "https://arxiv.org/abs/2201.12872",
    "authors": [
      "Ying-Xin Wu",
      "Xiang Wang",
      "An Zhang",
      "Xiangnan He",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12876",
    "title": "DeepCatra: Learning Flow- and Graph-based Behaviors for Android Malware  Detection",
    "abstract": "As Android malware is growing and evolving, deep learning has been introduced into malware detection, resulting in great effectiveness. Recent work is considering hybrid models and multi-view learning. However, they use only simple features, limiting the accuracy of these approaches in practice. In this paper, we propose DeepCatra, a multi-view learning approach for Android malware detection, whose model consists of a bidirectional LSTM (BiLSTM) and a graph neural network (GNN) as subnets. The two subnets rely on features extracted from statically computed call traces leading to critical APIs derived from public vulnerabilities. For each Android app, DeepCatra first constructs its call graph and computes call traces reaching critical APIs. Then, temporal opcode features used by the BiLSTM subnet are extracted from the call traces, while flow graph features used by the GNN subnet are constructed from all the call traces and inter-component communications. We evaluate the effectiveness of DeepCatra by comparing it with several state-of-the-art detection approaches. Experimental results on over 18,000 real-world apps and prevalent malware show that DeepCatra achieves considerable improvement, e.g., 2.7% to 14.6% on F1-measure, which demonstrates the feasibility of DeepCatra in practice. ",
    "url": "https://arxiv.org/abs/2201.12876",
    "authors": [
      "Yafei Wu",
      "Jian Shi",
      "Peicheng Wang",
      "Dongrui Zeng",
      "Cong Sun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2201.12879",
    "title": "Making Secure Software Insecure without Changing Its Code: The  Possibilities and Impacts of Attacks on the DevOps Pipeline",
    "abstract": "Companies are misled into thinking they solve their security issues by using a DevSecOps system. This paper aims to answer the question: Could a DevOps pipeline be misused to transform a securely developed application into an insecure one? To answer the question, we designed a typical DevOps pipeline utilizing Kubernetes (K8s} as a case study environment and analyzed the applicable threats. Then, we developed four attack scenarios against the case study environment: maliciously abusing the user's privilege of deploying containers within the K8s cluster, abusing the Jenkins instance to modify files during the continuous integration, delivery, and deployment systems (CI/CD) build phase, modifying the K8s DNS layer to expose an internal IP to external traffic, and elevating privileges from an account with create, read, update, and delete (CRUD) privileges to root privileges. The attacks answer the research question positively: companies should design and use a secure DevOps pipeline and not expect that using a DevSecOps environment alone is sufficient to deliver secure software. ",
    "url": "https://arxiv.org/abs/2201.12879",
    "authors": [
      "Nicholas Pecka",
      "Lotfi ben Othmane",
      "Altaz Valani"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2201.12884",
    "title": "A Theoretical Comparison of Graph Neural Network Extensions",
    "abstract": "We study and compare different Graph Neural Network extensions that increase the expressive power of GNNs beyond the Weisfeiler-Leman test. We focus on (i) GNNs based on higher order WL methods, (ii) GNNs that preprocess small substructures in the graph, (iii) GNNs that preprocess the graph up to a small radius, and (iv) GNNs that slightly perturb the graph to compute an embedding. We begin by presenting a simple improvement for this last extension that strictly increases the expressive power of this GNN variant. Then, as our main result, we compare the expressiveness of these extensions to each other through a series of example constructions that can be distinguished by one of the extensions, but not by another one. We also show negative examples that are particularly challenging for each of the extensions, and we prove several claims about the ability of these extensions to count cliques and cycles in the graph. ",
    "url": "https://arxiv.org/abs/2201.12884",
    "authors": [
      "P\u00e1l Andr\u00e1s Papp",
      "Roger Wattenhofer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12886",
    "title": "N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting",
    "abstract": "Recent progress in neural forecasting accelerated improvements in the performance of large-scale forecasting systems. Yet, long-horizon forecasting remains a very difficult task. Two common challenges afflicting long-horizon forecasting are the volatility of the predictions and their computational complexity. In this paper, we introduce N-HiTS, a model which addresses both challenges by incorporating novel hierarchical interpolation and multi-rate data sampling techniques. These techniques enable the proposed method to assemble its predictions sequentially, selectively emphasizing components with different frequencies and scales, while decomposing the input signal and synthesizing the forecast. We conduct an extensive empirical evaluation demonstrating the advantages of N-HiTS over the state-of-the-art long-horizon forecasting methods. On an array of multivariate forecasting tasks, the proposed method provides an average accuracy improvement of 25% over the latest Transformer architectures while reducing the computation time by an order of magnitude. Our code is available at \\href{https://github.com/cchallu/n-hits}{this repository}. ",
    "url": "https://arxiv.org/abs/2201.12886",
    "authors": [
      "Cristian Challu",
      "Kin G. Olivares",
      "Boris N. Oreshkin",
      "Federico Garza",
      "Max Mergenthaler",
      "Artur Dubrawski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.12892",
    "title": "Differences in Social Media Usage Exist Between Western and Middle-East  Countries",
    "abstract": "In this paper, we empirically analyze two examples of a Western (DE) versus Middle-East (SA) Online Social Messaging App. By focusing on the system interactions over time in comparison, we identify inherent differences in user engagement. We take a deep dive and shed light onto differences in user attention shifts and showcase their structural implications to the user experience. Our main findings show that in comparison to the German counterparts, the Saudi communities prefer creating content in longer conversations, while voting more conservative. ",
    "url": "https://arxiv.org/abs/2201.12892",
    "authors": [
      "Jens Helge Reelfs",
      "Oliver Hohlfeld",
      "Niklas Henckell"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2201.12895",
    "title": "Road User Position Prediction in Urban Environments via Locally Weighted  Learning",
    "abstract": "Prediction of the future position of a target road user given its current position, velocity and type is formulated as a weighted average. Weights are determined from data of previously observed road users, specifically from those that are most similar to the target. This formulation results in an interpretable model with few parameters. The model is validated on a dataset of vehicles, bicycles, and pedestrians in real-world traffic situations. The model outperforms the baseline constant velocity model, wheras a baseline neural network model shows comparable performance, but this model lacks the same level of interpretability. A comparison is made with state-of-the-arts, where these show superior performance on a sparse dataset, for which it is expected that the weighted average model works less well. With some further refinements a weighted average formulation could yield a reliable and interpretable model, in constrast to one which is difficult to analyze and has a huge number of uninterpretable parameters. ",
    "url": "https://arxiv.org/abs/2201.12895",
    "authors": [
      "Angelos Toytziaridis",
      "Paolo Falcone",
      "Jonas Sj\u00f6berg"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2201.12899",
    "title": "Interpretable AI-based Large-scale 3D Pathloss Prediction Model for  enabling Emerging Self-Driving Networks",
    "abstract": "In modern wireless communication systems, radio propagation modeling to estimate pathloss has always been a fundamental task in system design and optimization. The state-of-the-art empirical propagation models are based on measurements in specific environments and limited in their ability to capture idiosyncrasies of various propagation environments. To cope with this problem, ray-tracing based solutions are used in commercial planning tools, but they tend to be extremely time-consuming and expensive. We propose a Machine Learning (ML)-based model that leverages novel key predictors for estimating pathloss. By quantitatively evaluating the ability of various ML algorithms in terms of predictive, generalization and computational performance, our results show that Light Gradient Boosting Machine (LightGBM) algorithm overall outperforms others, even with sparse training data, by providing a 65% increase in prediction accuracy as compared to empirical models and 13x decrease in prediction time as compared to ray-tracing. To address the interpretability challenge that thwarts the adoption of most ML-based models, we perform extensive secondary analysis using SHapley Additive exPlanations (SHAP) method, yielding many practically useful insights that can be leveraged for intelligently tuning the network configuration, selective enrichment of training data in real networks and for building lighter ML-based propagation model to enable low-latency use-cases. ",
    "url": "https://arxiv.org/abs/2201.12899",
    "authors": [
      "Usama Masood",
      "Hasan Farooq",
      "Ali Imran",
      "Adnan Abu-Dayya"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12900",
    "title": "OpTopNET: A Learning Optimal Topology Synthesizer for Ad-hoc Robot  Networks",
    "abstract": "In this paper, we synthesize a machine-learning stacked ensemble model a vector of which predicts the optimal topology of a robot network. This problem is technically a multi-task classification problem. However, we divide it into a class of multi-class classification problems that can be more efficiently solved. For this purpose, we first compose an algorithm to create ground-truth topologies associated with various configurations of a robot network. This algorithm incorporates a complex collection of nonlinear optimality criteria that our learning model successfully manages to learn. Then, we propose a stacked ensemble model whose output is the topology prediction for the particular robot associated with it. Each stacked ensemble instance constitutes three low-level estimators whose outputs will be aggregated by a high-level boosting blender. The results of the simulations, applying our model to a network of 10 robots, represents over %80 accuracy in the prediction of optimal topologies corresponding to various configurations of this complex optimal topology learning problem. ",
    "url": "https://arxiv.org/abs/2201.12900",
    "authors": [
      "Matin Macktoobian",
      "Zhan Shu",
      "Qing Zhao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12904",
    "title": "COIN++: Data Agnostic Neural Compression",
    "abstract": "Neural compression algorithms are typically based on autoencoders that require specialized encoder and decoder architectures for different data modalities. In this paper, we propose COIN++, a neural compression framework that seamlessly handles a wide range of data modalities. Our approach is based on converting data to implicit neural representations, i.e. neural functions that map coordinates (such as pixel locations) to features (such as RGB values). Then, instead of storing the weights of the implicit neural representation directly, we store modulations applied to a meta-learned base network as a compressed code for the data. We further quantize and entropy code these modulations, leading to large compression gains while reducing encoding time by two orders of magnitude compared to baselines. We empirically demonstrate the effectiveness of our method by compressing various data modalities, from images to medical and climate data. ",
    "url": "https://arxiv.org/abs/2201.12904",
    "authors": [
      "Emilien Dupont",
      "Hrushikesh Loya",
      "Milad Alizadeh",
      "Adam Goli\u0144ski",
      "Yee Whye Teh",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2201.12905",
    "title": "Modularity-based Backbone Extraction in Weighted Complex Networks",
    "abstract": "The constantly growing size of real-world networks is a great challenge. Therefore, building a compact version of networks allowing their analyses is a must. Backbone extraction techniques are among the leading solutions to reduce network size while preserving its features. Coarse-graining merges similar nodes to reduce the network size, while filter-based methods remove nodes or edges according to a specific statistical property. Since community structure is ubiquitous in real-world networks, preserving it in the backbone extraction process is of prime interest. To this end, we propose a filter-based method. The so-called \"modularity vitality backbone\" removes nodes with the lower contribution to the network's modularity. Experimental results show that the proposed strategy outperforms the \"overlapping nodes ego backbone\" and the \"overlapping nodes and hub backbone.\" These two backbone extraction processes recently introduced have proved their efficacy to preserve better the information of the original network than the popular disparity filter. ",
    "url": "https://arxiv.org/abs/2201.12905",
    "authors": [
      "Stephany Rajeh",
      "Marinette Savonnet",
      "Eric Leclercq",
      "Hocine Cherifi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2201.12907",
    "title": "A Topological Centrality Measure for Directed Networks",
    "abstract": "Given a directed network $ G $, we are interested in studying the qualitative features of $ G $ which govern how perturbations propagate across $ G $. Various classical centrality measures have been already developed and proven useful to capture qualitative features and behaviors for undirected networks. In this paper, we use topological data analysis (TDA) to adapt measures of centrality to capture both directedness and non-local propagating behaviors in networks. We introduce a new metric for computing centrality in directed weighted networks, namely the quasi-centrality measure. We compute these metrics on trade networks to illustrate that our measure successfully captures propagating effects in the network and can also be used to identify sources of shocks that can disrupt the topology of directed networks. Moreover, we introduce a method that gives a hierarchical representation of the topological influences of nodes in a directed network. ",
    "url": "https://arxiv.org/abs/2201.12907",
    "authors": [
      "Fenghuan He"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Algebraic Topology (math.AT)"
    ]
  },
  {
    "id": "arXiv:2201.12914",
    "title": "Investigating Centrality Measures in Social Networks with Community  Structure",
    "abstract": "Centrality measures are crucial in quantifying the influence of the members of a social network. Although there has been a great deal of work dealing with this issue, the vast majority of classical centrality measures are agnostic of the community structure characterizing many social networks. Recent works have developed community-aware centrality measures that exploit features of the community structure information encountered in most real-world complex networks. In this paper, we investigate the interactions between 5 popular classical centrality measures and 5 community-aware centrality measures using 8 real-world online networks. Correlation as well as similarity measures between both type of centrality measures are computed. Results show that community-aware centrality measures can be divided into two groups. The first group, which includes Bridging centrality, Community Hub-Bridge and Participation Coefficient, provides distinctive node information as compared to classical centrality. This behavior is consistent across the networks. The second group which includes Community-based Mediator and Number of Neighboring Communities is characterized by more mixed results that vary across networks. ",
    "url": "https://arxiv.org/abs/2201.12914",
    "authors": [
      "Stephany Rajeh",
      "Marinette Savonnet",
      "Eric Leclercq",
      "Hocine Cherifi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2201.12918",
    "title": "How Correlated are Community-aware and Classical Centrality Measures in  Complex Networks?",
    "abstract": "Unlike classical centrality measures, recently developed community-aware centrality measures use a network's community structure to identify influential nodes in complex networks. This paper investigates their relationship on a set of fifty real-world networks originating from various domains. Results show that classical and community-aware centrality measures generally exhibit low to medium correlation values. These results are consistent across networks. Transitivity and efficiency are the most influential macroscopic network features driving the correlation variation between classical and community-aware centrality measures. Additionally, the mixing parameter, the modularity, and the Max-ODF are the main mesoscopic topological properties exerting the most substantial effect. ",
    "url": "https://arxiv.org/abs/2201.12918",
    "authors": [
      "Stephany Rajeh",
      "Marinette Savonnet",
      "Eric Leclercq",
      "Hocine Cherifi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2201.12923",
    "title": "Asynchronous Opinion Dynamics in Social Networks",
    "abstract": "Opinion spreading in a society decides the fate of elections, the success of products, and the impact of political or social movements. The model by Hegselmann and Krause is a well-known theoretical model to study such opinion formation processes in social networks. In contrast to many other theoretical models, it does not converge towards a situation where all agents agree on the same opinion. Instead, it assumes that people find an opinion reasonable if and only if it is close to their own. The system converges towards a stable situation where agents sharing the same opinion form a cluster, and agents in different clusters do not \\mbox{influence each other.} We focus on the social variant of the Hegselmann-Krause model where agents are connected by a social network and their opinions evolve in an iterative process. When activated, an agent adopts the average of the opinions of its neighbors having a similar opinion. By this, the set of influencing neighbors of an agent may change over time. To the best of our knowledge, social Hegselmann-Krause systems with asynchronous opinion updates have only been studied with the complete graph as social network. We show that such opinion dynamics with random agent activation are guaranteed to converge for any social network. We provide an upper bound of $\\mathcal{O}(n|E|^2 (\\varepsilon/\\delta)^2)$ on the expected number of opinion updates until convergence, where $|E|$ is the number of edges of the social network. For the complete social network we show a bound of $\\mathcal{O}(n^3(n^2 + (\\varepsilon/\\delta)^2))$ that represents a major improvement over the previously best upper bound of $\\mathcal{O}(n^9 (\\varepsilon/\\delta)^2)$. Our bounds are complemented by simulations that indicate asymptotically matching lower bounds. ",
    "url": "https://arxiv.org/abs/2201.12923",
    "authors": [
      "Petra Berenbrink",
      "Martin Hoefer",
      "Dominik Kaaser",
      "Pascal Lenzner",
      "Malin Rau",
      "Daniel Schmand"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2201.12929",
    "title": "The Geometry of Robust Value Functions",
    "abstract": "The space of value functions is a fundamental concept in reinforcement learning. Characterizing its geometric properties may provide insights for optimization and representation. Existing works mainly focus on the value space for Markov Decision Processes (MDPs). In this paper, we study the geometry of the robust value space for the more general Robust MDPs (RMDPs) setting, where transition uncertainties are considered. Specifically, since we find it hard to directly adapt prior approaches to RMDPs, we start with revisiting the non-robust case, and introduce a new perspective that enables us to characterize both the non-robust and robust value space in a similar fashion. The key of this perspective is to decompose the value space, in a state-wise manner, into unions of hypersurfaces. Through our analysis, we show that the robust value space is determined by a set of conic hypersurfaces, each of which contains the robust values of all policies that agree on one state. Furthermore, we find that taking only extreme points in the uncertainty set is sufficient to determine the robust value space. Finally, we discuss some other aspects about the robust value space, including its non-convexity and policy agreement on multiple states. ",
    "url": "https://arxiv.org/abs/2201.12929",
    "authors": [
      "Kaixin Wang",
      "Navdeep Kumar",
      "Kuangqi Zhou",
      "Bryan Hooi",
      "Jiashi Feng",
      "Shie Mannor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12950",
    "title": "Network Programming via Computable Products",
    "abstract": "The User Plane Function (UPF) aims to provide network services in the 3GPP 5G core network. These services need to be implemented on demand inexpensively with provable properties. Existing network dataplane programming languages are not up to the task. A new software paradigm is presented for the UPF. It is inspired by model checking a concurrent reactive system where conceptually each component of the system is modeled as an extended finite-state machine and their product is verified. We show how such a product can be computed for one example of a UPF and how its state invariants can be inferred, thereby eliminating the need to formally verify the product separately. Code can be generated from the product and regenerated on the fly to remain optimal for the probability distribution of network traffic the UPF must process. ",
    "url": "https://arxiv.org/abs/2201.12950",
    "authors": [
      "Dennis Volpano"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2201.12972",
    "title": "Implementation of an Elastic Reconfigurable Optical Add/Drop Multiplexer  based on Subcarriers for Application in Optical Multichannel Networks",
    "abstract": "We designed a Reconfigurable Optical Add/Drop Multiplexer (ROADM) based on a subcarrier add/drop node in an optical communication system that is suitable for all kinds of optical multiplexing signals. To achieve this goal, at first, we designed an optical comb generator based on a dual-drive Mach Zehnder. The new ROADM setup is validated by a 100 Gb/s 4-subcarrier. In the final step, we checked the performance of the system in terms of the bit error rate (BER) versus optical signal-to-noise ratio (OSNR) to verify the add/drop operation had been successfully performed at 10-9 and is suitable to apply in an all-optical multiplexing technique. ",
    "url": "https://arxiv.org/abs/2201.12972",
    "authors": [
      "Faranak Khosravi",
      "Mehdi Tarhani",
      "Shivani Kurle",
      "Mehdi Shadaram"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2201.12976",
    "title": "Heterogeneous Federated Learning via Grouped Sequential-to-Parallel  Training",
    "abstract": "Federated learning (FL) is a rapidly growing privacy-preserving collaborative machine learning paradigm. In practical FL applications, local data from each data silo reflect local usage patterns. Therefore, there exists heterogeneity of data distributions among data owners (a.k.a. FL clients). If not handled properly, this can lead to model performance degradation. This challenge has inspired the research field of heterogeneous federated learning, which currently remains open. In this paper, we propose a data heterogeneity-robust FL approach, FedGSP, to address this challenge by leveraging on a novel concept of dynamic Sequential-to-Parallel (STP) collaborative training. FedGSP assigns FL clients to homogeneous groups to minimize the overall distribution divergence among groups, and increases the degree of parallelism by reassigning more groups in each round. It is also incorporated with a novel Inter-Cluster Grouping (ICG) algorithm to assist in group assignment, which uses the centroid equivalence theorem to simplify the NP-hard grouping problem to make it solvable. Extensive experiments have been conducted on the non-i.i.d. FEMNIST dataset. The results show that FedGSP improves the accuracy by 3.7% on average compared with seven state-of-the-art approaches, and reduces the training time and communication overhead by more than 90%. ",
    "url": "https://arxiv.org/abs/2201.12976",
    "authors": [
      "Shenglai Zeng",
      "Zonghang Li",
      "Hongfang Yu",
      "Yihong He",
      "Zenglin Xu",
      "Dusit Niyato",
      "Han Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2201.12987",
    "title": "Interpretable and Generalizable Graph Learning via Stochastic Attention  Mechanism",
    "abstract": "Interpretable graph learning is in need as many scientific applications depend on learning models to collect insights from graph-structured data. Previous works mostly focused on using post-hoc approaches to interpret a pre-trained model (graph neural network models in particular). They argue against inherently interpretable models because good interpretation of these models is often at the cost of their prediction accuracy. And, the widely used attention mechanism for inherent interpretation often fails to provide faithful interpretation in graph learning tasks. In this work, we address both issues by proposing Graph Stochastic Attention (GSAT), an attention mechanism derived from the information bottleneck principle. GSAT leverages stochastic attention to block the information from the task-irrelevant graph components while learning stochasticity-reduced attention to select the task-relevant subgraphs for interpretation. GSAT can also apply to fine-tuning and interpreting pre-trained models via stochastic attention mechanism. Extensive experiments on eight datasets show that GSAT outperforms the state-of-the-art methods by up to 20%$\\uparrow$ in interpretation AUC and 5%$\\uparrow$ in prediction accuracy. ",
    "url": "https://arxiv.org/abs/2201.12987",
    "authors": [
      "Siqi Miao",
      "Miaoyuan Liu",
      "Pan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12994",
    "title": "GSN: A Universal Graph Neural Network Inspired by Spring Network",
    "abstract": "The design of universal Graph Neural Networks (GNNs) that operate on both homophilous and heterophilous graphs has received increased research attention in recent years. Existing heterophilous GNNs, particularly those designed in the spatial domain, lack a convincing theoretical or physical motivation. In this paper, we propose the Graph Spring Network (GSN), a universal GNN model that works for both homophilous and heterophilous graphs, inspired by spring networks and metric learning. We show that the GSN framework interprets many existing GNN models from the perspective of spring potential energy minimization with various metrics, which gives these models strong physical motivations. We also conduct extensive experiments to demonstrate our GSN framework's superior performance on real-world homophilous and heterophilous data sets. ",
    "url": "https://arxiv.org/abs/2201.12994",
    "authors": [
      "Guanyu Cui",
      "Zhewei Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2201.13001",
    "title": "Out-of-distribution Detection Using Kernel Density Polytopes",
    "abstract": "Any reasonable machine learning (ML) model should not only interpolate efficiently in between the training samples provided (in-distribution region), but also approach the extrapolative or out-of-distribution (OOD) region without being overconfident. Our experiment on human subjects justifies the aforementioned properties for human intelligence as well. Many state-of-the-art algorithms have tried to fix the overconfidence problem of ML models in the OOD region. However, in doing so, they have often impaired the in-distribution performance of the model. Our key insight is that ML models partition the feature space into polytopes and learn constant (random forests) or affine (ReLU networks) functions over those polytopes. This leads to the OOD overconfidence problem for the polytopes which lie in the training data boundary and extend to infinity. To resolve this issue, we propose kernel density methods that fit Gaussian kernel over the polytopes, which are learned using ML models. Specifically, we introduce two variants of kernel density polytopes: Kernel Density Forest (KDF) and Kernel Density Network (KDN) based on random forests and deep networks, respectively. Studies on various simulation settings show that both KDF and KDN achieve uniform confidence over the classes in the OOD region while maintaining good in-distribution accuracy compared to that of their respective parent models. ",
    "url": "https://arxiv.org/abs/2201.13001",
    "authors": [
      "Jayanta Dey",
      "Ashwin De Silva",
      "Will LeVine",
      "Jong Shin",
      "Haoyin Xu",
      "Ali Geisa",
      "Tiffany Chu",
      "Leyla Isik",
      "Joshua T. Vogelstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.13013",
    "title": "Filtering In Implicit Neural Networks",
    "abstract": "Implicit neural networks (INNs) are very effective for learning data representation. However, most INNs inevitably generate over-smoothed patches or obvious noisy artifacts in the results when the data has many scales of details or a wide range of frequencies, leading to significant performance reduction. Adapting the result containing both noise and over-smoothed regions usually suffers from either over smoothing or noisy issues. To overcome this challenge, we propose a new framework, coined FINN, that integrated a \\emph{filtering} module to the \\emph{implicit neural network} to perform data fitting while filtering artifacts. The filtering module has a smoothing operator that acts on the intermediate results of the network and a recovering operator that brings distinct details from the input back to the regions overly smoothed. The proposed method significantly alleviates over smoothing or noisy issues. We demonstrate the advantage of the FINN on the image regression task, considering both real and synthetic images, and showcases significant improvement on both quantitative and qualitative results compared to state-of-the-art methods. Moreover, FINN yields better performance in both convergence speed and network stability. Source code is available at https://github.com/yixin26/FINN. ",
    "url": "https://arxiv.org/abs/2201.13013",
    "authors": [
      "Yixin Zhuang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.13018",
    "title": "Monitoring Jitter in Software Defined Networks",
    "abstract": "End-to-end jitter of a flow is an important metric that indicates the Quality of Service a user is experiencing, particularly for real-time applications such as video streaming, cloud gaming, and so on. Monitoring the jitter can help controllers make routing decisions for certain flows. This paper discusses methods in which we can estimate/follow important patterns in the end-to-end jitter of a flow. Two main approaches are taken, one using a Software Defined Network controller and the other using P4 programmable data-planes. Results from the simulations of each method are discussed. ",
    "url": "https://arxiv.org/abs/2201.13018",
    "authors": [
      "Jithin Kallukalam Sojan",
      "K. Haribabu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2201.13019",
    "title": "On the Robustness of Quality Measures for GANs",
    "abstract": "This work evaluates the robustness of quality measures of generative models such as Inception Score (IS) and Fr\\'echet Inception Distance (FID). Analogous to the vulnerability of deep models against a variety of adversarial attacks, we show that such metrics can also be manipulated by additive pixel perturbations. Our experiments indicate that one can generate a distribution of images with very high scores but low perceptual quality. Conversely, one can optimize for small imperceptible perturbations that, when added to real world images, deteriorate their scores. Furthermore, we extend our evaluation to generative models themselves, including the state of the art network StyleGANv2. We show the vulnerability of both the generative model and the FID against additive perturbations in the latent space. Finally, we show that the FID can be robustified by directly replacing the Inception model by a robustly trained Inception. We validate the effectiveness of the robustified metric through extensive experiments, which show that it is more robust against manipulation. ",
    "url": "https://arxiv.org/abs/2201.13019",
    "authors": [
      "Motasem Alfarra",
      "Juan C. P\u00e9rez",
      "Anna Fr\u00fchst\u00fcck",
      "Philip H. S. Torr",
      "Peter Wonka",
      "Bernard Ghanem"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13025",
    "title": "Learning Robust Representation through Graph Adversarial Contrastive  Learning",
    "abstract": "Existing studies show that node representations generated by graph neural networks (GNNs) are vulnerable to adversarial attacks, such as unnoticeable perturbations of adjacent matrix and node features. Thus, it is requisite to learn robust representations in graph neural networks. To improve the robustness of graph representation learning, we propose a novel Graph Adversarial Contrastive Learning framework (GraphACL) by introducing adversarial augmentations into graph self-supervised learning. In this framework, we maximize the mutual information between local and global representations of a perturbed graph and its adversarial augmentations, where the adversarial graphs can be generated in either supervised or unsupervised approaches. Based on the Information Bottleneck Principle, we theoretically prove that our method could obtain a much tighter bound, thus improving the robustness of graph representation learning. Empirically, we evaluate several methods on a range of node classification benchmarks and the results demonstrate GraphACL could achieve comparable accuracy over previous supervised methods. ",
    "url": "https://arxiv.org/abs/2201.13025",
    "authors": [
      "Jiayan Guo",
      "Shangyang Li",
      "Yue Zhao",
      "Yan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13029",
    "title": "A Flexible IAB Architecture for Beyond 5G Network",
    "abstract": "IAB is an innovative wireless backhaul solution to provide cost-efficient deployment of small cells for successful 5G adoption. Besides, IAB can utilize the same spectrum for access and backhaul purposes. The 3GPP standardized IAB in Release 16 and would incorporate a few enhancements in the upcoming releases. The 3GPP IAB architecture, however, suffers from some limitations, such as it does not support mobile relays or dual-connectivity. This article presents a novel IAB architecture that addresses these limitations and is transparent to legacy operations of the 5G system. The architecture also supports multi-RAT coexistence where access and backhaul may belong to different RATs. These factors (and many others) enable operators to capitalize on the architecture for deploying IAB anywhere in a plug-and-play manner. We also show the merits of the architecture by evaluating its capacity and mobility robustness compared to the 3GPP architecture. Simulation results corroborate our design approach. Owing its robust design, the architecture can contend for standardization in B5G system. ",
    "url": "https://arxiv.org/abs/2201.13029",
    "authors": [
      "Shashi Ranjan",
      "Pranav Jha",
      "Abhay Karandikar",
      "Prasanna Chaporkar"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2201.13078",
    "title": "Lymphoma segmentation from 3D PET-CT images using a deep evidential  network",
    "abstract": "An automatic evidential segmentation method based on Dempster-Shafer theory and deep learning is proposed to segment lymphomas from three-dimensional Positron Emission Tomography (PET) and Computed Tomography (CT) images. The architecture is composed of a deep feature-extraction module and an evidential layer. The feature extraction module uses an encoder-decoder framework to extract semantic feature vectors from 3D inputs. The evidential layer then uses prototypes in the feature space to compute a belief function at each voxel quantifying the uncertainty about the presence or absence of a lymphoma at this location. Two evidential layers are compared, based on different ways of using distances to prototypes for computing mass functions. The whole model is trained end-to-end by minimizing the Dice loss function. The proposed combination of deep feature extraction and evidential segmentation is shown to outperform the baseline UNet model as well as three other state-of-the-art models on a dataset of 173 patients. ",
    "url": "https://arxiv.org/abs/2201.13078",
    "authors": [
      "Ling Huang",
      "Su Ruan",
      "Pierre Decazes",
      "Thierry Denoeux"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.13079",
    "title": "Jet noise characterization for advanced pipeline leak detection",
    "abstract": "The detection of leaks in pipeline transportation systems is a matter of serious concern for operators, who pursue the integrity of their assets, the reduction of losses and the prevention of environmental hazards. Whenever a hole occurs in a pressurized pipeline, the corresponding fluid leakage is characterized by a turbulent flow and a peculiar acoustic noise, whose characteristics depend also on the size of the hole itself. This study shows that both the presence and the size of such a leaking hole can be successfully detected, by exploiting the acoustic noise (pressure transients) generated by the fluid exiting the pipe and recorded internally by hydrophones, or by considering the corresponding vibrations (e.g., acceleration signals) propagating along the external shell of the conduit. To this purpose, several experimental campaigns of acoustic noise generation have been performed using multiple calibrated nozzles on a 16 ID connection pipeline in a fuel tanks area. Detection and classification procedures are proposed to control the presence of leakages and to estimate the size of the hole, using pressure and vibration signals. ",
    "url": "https://arxiv.org/abs/2201.13079",
    "authors": [
      "Riccardo Angelo Giro",
      "Giancarlo Bernasconi",
      "Giuseppe Giunta",
      "Simone Cesari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2201.13081",
    "title": "Unsupervised Anomaly Detection in 3D Brain MRI using Deep Learning with  Multi-Task Brain Age Prediction",
    "abstract": "Lesion detection in brain Magnetic Resonance Images (MRIs) remains a challenging task. MRIs are typically read and interpreted by domain experts, which is a tedious and time-consuming process. Recently, unsupervised anomaly detection (UAD) in brain MRI with deep learning has shown promising results to provide a quick, initial assessment. So far, these methods only rely on the visual appearance of healthy brain anatomy for anomaly detection. Another biomarker for abnormal brain development is the deviation between the brain age and the chronological age, which is unexplored in combination with UAD. We propose deep learning for UAD in 3D brain MRI considering additional age information. We analyze the value of age information during training, as an additional anomaly score, and systematically study several architecture concepts. Based on our analysis, we propose a novel deep learning approach for UAD with multi-task age prediction. We use clinical T1-weighted MRIs of 1735 healthy subjects and the publicly available BraTs 2019 data set for our study. Our novel approach significantly improves UAD performance with an AUC of 92.60% compared to an AUC-score of 84.37% using previous approaches without age information. ",
    "url": "https://arxiv.org/abs/2201.13081",
    "authors": [
      "Marcel Bengs",
      "Finn Behrendt",
      "Max-Heinrich Laves",
      "Julia Kr\u00fcger",
      "Roland Opfer",
      "Alexander Schlaefer"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.13086",
    "title": "Securing Federated Sensitive Topic Classification against Poisoning  Attacks",
    "abstract": "We present a Federated Learning (FL) based solution for building a distributed classifier capable of detecting URLs containing GDPR-sensitive content related to categories such as health, sexual preference, political beliefs, etc. Although such a classifier addresses the limitations of previous offline/centralised classifiers,it is still vulnerable to poisoning attacks from malicious users that may attempt to reduce the accuracy for benign users by disseminating faulty model updates. To guard against this, we develop a robust aggregation scheme based on subjective logic and residual-based attack detection. Employing a combination of theoretical analysis, trace-driven simulation, as well as experimental validation with a prototype and real users, we show that our classifier can detect sensitive content with high accuracy, learn new labels fast, and remain robust in view of poisoning attacks from malicious users, as well as imperfect input from non-malicious ones. ",
    "url": "https://arxiv.org/abs/2201.13086",
    "authors": [
      "Tianyue Chu",
      "Alvaro Garcia-Recuero",
      "Costas Iordanou",
      "Georgios Smaragdakis",
      "Nikolaos Laoutaris"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2201.13099",
    "title": "Max Flow Vitality of Edges and Vertices in Undirected Planar Graphs",
    "abstract": "We study the problem of computing the vitality with respect to max flow of edges and vertices in undirected planar graphs, where the vitality of an edge/vertex in a graph with respect to max flow between two fixed vertices $s,t$ is defined as the max flow decrease when the edge/vertex is removed from the graph. We show that the vitality of any $k$ selected edges can be computed in $O(kn + n\\log\\log n)$ worst-case time, and that a $\\delta$ additive approximation of the vitality of all edges with capacity at most $c$ can be computed in $O(\\frac{c}{\\delta}n +n \\log \\log n)$ worst-case time, where $n$ is the size of the graph. Similar results are given for the vitality of vertices. All our algorithms work in $O(n)$ space. ",
    "url": "https://arxiv.org/abs/2201.13099",
    "authors": [
      "Lorenzo Balzotti",
      "Paolo G. Franciosa"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2201.13100",
    "title": "Adversarial Masking for Self-Supervised Learning",
    "abstract": "We propose ADIOS, a masked image model (MIM) framework for self-supervised learning, which simultaneously learns a masking function and an image encoder using an adversarial objective. The image encoder is trained to minimise the distance between representations of the original and that of a masked image. The masking function, conversely, aims at maximising this distance. ADIOS consistently improves on state-of-the-art self-supervised learning (SSL) methods on a variety of tasks and datasets -- including classification on ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao et al., 2021) -- while generating semantically meaningful masks. Unlike modern MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch tokenisation construction of Vision Transformers, and can be implemented with convolutional backbones. We further demonstrate that the masks learned by ADIOS are more effective in improving representation learning of SSL methods than masking schemes used in popular MIM models. ",
    "url": "https://arxiv.org/abs/2201.13100",
    "authors": [
      "Yuge Shi",
      "N. Siddharth",
      "Philip H.S. Torr",
      "Adam R. Kosiorek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13102",
    "title": "GADoT: GAN-based Adversarial Training for Robust DDoS Attack Detection",
    "abstract": "Machine Learning (ML) has proven to be effective in many application domains. However, ML methods can be vulnerable to adversarial attacks, in which an attacker tries to fool the classification/prediction mechanism by crafting the input data. In the case of ML-based Network Intrusion Detection Systems (NIDSs), the attacker might use their knowledge of the intrusion detection logic to generate malicious traffic that remains undetected. One way to solve this issue is to adopt adversarial training, in which the training set is augmented with adversarial traffic samples. This paper presents an adversarial training approach called GADoT, which leverages a Generative Adversarial Network (GAN) to generate adversarial DDoS samples for training. We show that a state-of-the-art NIDS with high accuracy on popular datasets can experience more than 60% undetected malicious flows under adversarial attacks. We then demonstrate how this score drops to 1.8% or less after adversarial training using GADoT. ",
    "url": "https://arxiv.org/abs/2201.13102",
    "authors": [
      "Maged Abdelaty",
      "Sandra Scott-Hayward",
      "Roberto Doriguzzi-Corin",
      "Domenico Siracusa"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2201.13103",
    "title": "Detecting False Rumors from Retweet Dynamics on Social Media",
    "abstract": "False rumors are known to have detrimental effects on society. To prevent the spread of false rumors, social media platforms such as Twitter must detect them early. In this work, we develop a novel probabilistic mixture model that classifies true vs. false rumors based on the underlying spreading process. Specifically, our model is the first to formalize the self-exciting nature of true vs. false retweeting processes. This results in a novel mixture marked Hawkes model (MMHM). Owing to this, our model obviates the need for feature engineering; instead, it directly models the spreading process in order to make inferences of whether online rumors are incorrect. Our evaluation is based on 13,650 retweet cascades of both true. vs. false rumors from Twitter. Our model recognizes false rumors with a balanced accuracy of 64.97% and an AUC of 69.46%. It outperforms state-of-the-art baselines (both neural and feature engineering) by a considerable margin but while being fully interpretable. Our work has direct implications for practitioners: it leverages the spreading process as an implicit quality signal and, based on it, detects false content. ",
    "url": "https://arxiv.org/abs/2201.13103",
    "authors": [
      "Christof Naumzik",
      "Stefan Feuerriegel"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2201.13119",
    "title": "XNLP-completeness for Parameterized Problems on Graphs with a Linear  Structure",
    "abstract": "In this paper, we show several parameterized problems to be complete for the class XNLP: parameterized problems that can be solved with a non-deterministic algorithm that uses $f(k)\\log n$ space and $f(k)n^c$ time, with $f$ a computable function, $n$ the input size, $k$ the parameter and $c$ a constant. The problems include Maximum Regular Induced Subgraph and Max Cut parameterized by linear clique-width, Capacitated (Red-Blue) Dominating Set parameterized by pathwidth, Odd Cycle Transversal parameterized by a new parameter we call logarithmic linear clique-width (defined as $k/\\log n$ for an $n$-vertex graph of linear clique-width $k$), and Bipartite Bandwidth. ",
    "url": "https://arxiv.org/abs/2201.13119",
    "authors": [
      "Hans L. Bodlaender",
      "Carla Groenland",
      "Hugo Jacob"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2201.13128",
    "title": "Deletion Robust Submodular Maximization over Matroids",
    "abstract": "Maximizing a monotone submodular function is a fundamental task in machine learning. In this paper, we study the deletion robust version of the problem under the classic matroids constraint. Here the goal is to extract a small size summary of the dataset that contains a high value independent set even after an adversary deleted some elements. We present constant-factor approximation algorithms, whose space complexity depends on the rank $k$ of the matroid and the number $d$ of deleted elements. In the centralized setting we present a $(3.582+O(\\varepsilon))$-approximation algorithm with summary size $O(k + \\frac{d \\log k}{\\varepsilon^2})$. In the streaming setting we provide a $(5.582+O(\\varepsilon))$-approximation algorithm with summary size and memory $O(k + \\frac{d \\log k}{\\varepsilon^2})$. We complement our theoretical results with an in-depth experimental analysis showing the effectiveness of our algorithms on real-world datasets. ",
    "url": "https://arxiv.org/abs/2201.13128",
    "authors": [
      "Paul D\u00fctting",
      "Federico Fusco",
      "Silvio Lattanzi",
      "Ashkan Norouzi-Fard",
      "Morteza Zadimoghaddam"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2201.13140",
    "title": "Polynomial kernels for edge modification problems towards block and  strictly chordal graphs",
    "abstract": "We consider edge modification problems towards block and strictly chordal graphs, where one is given an undirected graph $G = (V,E)$ and an integer $k \\in \\mathbb{N}$ and seeks to edit (add or delete) at most $k$ edges from $G$ to obtain a block graph or a strictly chordal graph. The completion and deletion variants of these problems are defined similarly by only allowing edge additions for the former and only edge deletions for the latter. Block graphs are a well-studied class of graphs and admit several characterizations, e.g. they are diamond-free chordal graphs. Strictly chordal graphs, also referred to as block duplicate graphs, are a natural generalization of block graphs where one can add true twins of cut-vertices. Strictly chordal graphs are exactly dart and gem-free chordal graphs. We prove the NP-completeness for most variants of these problems and provide $O(k^2)$ vertex-kernels for Block Graph Edition and Block Graph Deletion, $O(k^3)$ vertex-kernels for Strictly Chordal Completion and Strictly Chordal Deletion and a $O(k^4)$ vertex-kernel for Strictly Chordal Edition. ",
    "url": "https://arxiv.org/abs/2201.13140",
    "authors": [
      "Ma\u00ebl Dumas",
      "Anthony Perez",
      "Mathis Rocton",
      "Ioan Todinca"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2201.13157",
    "title": "Equivariant neural networks for recovery of Hadamard matrices",
    "abstract": "We propose a message passing neural network architecture designed to be equivariant to column and row permutations of a matrix. We illustrate its advantages over traditional architectures like multi-layer perceptrons (MLPs), convolutional neural networks (CNNs) and even Transformers, on the combinatorial optimization task of recovering a set of deleted entries of a Hadamard matrix. We argue that this is a powerful application of the principles of Geometric Deep Learning to fundamental mathematics, and a potential stepping stone toward more insights on the Hadamard conjecture using Machine Learning techniques. ",
    "url": "https://arxiv.org/abs/2201.13157",
    "authors": [
      "Augusto Peres",
      "Eduardo Dias",
      "Lu\u00eds Sarmento",
      "Hugo Penedones"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2201.13164",
    "title": "Imperceptible and Multi-channel Backdoor Attack against Deep Neural  Networks",
    "abstract": "Recent researches demonstrate that Deep Neural Networks (DNN) models are vulnerable to backdoor attacks. The backdoored DNN model will behave maliciously when images containing backdoor triggers arrive. To date, existing backdoor attacks are single-trigger and single-target attacks, and the triggers of most existing backdoor attacks are obvious thus are easy to be detected or noticed. In this paper, we propose a novel imperceptible and multi-channel backdoor attack against Deep Neural Networks by exploiting Discrete Cosine Transform (DCT) steganography. Based on the proposed backdoor attack method, we implement two variants of backdoor attacks, i.e., N-to-N backdoor attack and N-to-One backdoor attack. Specifically, for a colored image, we utilize DCT steganography to construct the trigger on different channels of the image. As a result, the trigger is stealthy and natural. Based on the proposed method, we implement multi-target and multi-trigger backdoor attacks. Experimental results demonstrate that the average attack success rate of the N-to-N backdoor attack is 93.95% on CIFAR-10 dataset and 91.55% on TinyImageNet dataset, respectively. The average attack success rate of N-to-One attack is 90.22% and 89.53% on CIFAR-10 and TinyImageNet datasets, respectively. Meanwhile, the proposed backdoor attack does not affect the classification accuracy of the DNN model. Moreover, the proposed attack is demonstrated to be robust to the state-of-the-art backdoor defense (Neural Cleanse). ",
    "url": "https://arxiv.org/abs/2201.13164",
    "authors": [
      "Mingfu Xue",
      "Shifeng Ni",
      "Yinghao Wu",
      "Yushu Zhang",
      "Jian Wang",
      "Weiqiang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.13169",
    "title": "Causal Explanations and XAI",
    "abstract": "Although standard Machine Learning models are optimized for making predictions about observations, more and more they are used for making predictions about the results of actions. An important goal of Explainable Artificial Intelligence (XAI) is to compensate for this mismatch by offering explanations about the predictions of an ML-model which ensure that they are reliably action-guiding. As action-guiding explanations are causal explanations, the literature on this topic is starting to embrace insights from the literature on causal models. Here I take a step further down this path by formally defining the causal notions of sufficient explanations and counterfactual explanations. I show how these notions relate to (and improve upon) existing work, and motivate their adequacy by illustrating how different explanations are action-guiding under different circumstances. Moreover, this work is the first to offer a formal definition of actual causation that is founded entirely in action-guiding explanations. Although the definitions are motivated by a focus on XAI, the analysis of causal explanation and actual causation applies in general. I also touch upon the significance of this work for fairness in AI by showing how actual causation can be used to improve the idea of path-specific counterfactual fairness. ",
    "url": "https://arxiv.org/abs/2201.13169",
    "authors": [
      "Sander Beckers"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.13170",
    "title": "Cooperative Online Learning in Stochastic and Adversarial MDPs",
    "abstract": "We study cooperative online learning in stochastic and adversarial Markov decision process (MDP). That is, in each episode, $m$ agents interact with an MDP simultaneously and share information in order to minimize their individual regret. We consider environments with two types of randomness: \\emph{fresh} -- where each agent's trajectory is sampled i.i.d, and \\emph{non-fresh} -- where the realization is shared by all agents (but each agent's trajectory is also affected by its own actions). More precisely, with non-fresh randomness the realization of every cost and transition is fixed at the start of each episode, and agents that take the same action in the same state at the same time observe the same cost and next state. We thoroughly analyze all relevant settings, highlight the challenges and differences between the models, and prove nearly-matching regret lower and upper bounds. To our knowledge, we are the first to consider cooperative reinforcement learning (RL) with either non-fresh randomness or in adversarial MDPs. ",
    "url": "https://arxiv.org/abs/2201.13170",
    "authors": [
      "Tal Lancewicki",
      "Aviv Rosenberg",
      "Yishay Mansour"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13172",
    "title": "Near-Optimal Regret for Adversarial MDP with Delayed Bandit Feedback",
    "abstract": "The standard assumption in reinforcement learning (RL) is that agents observe feedback for their actions immediately. However, in practice feedback is often observed in delay. This paper studies online learning in episodic Markov decision process (MDP) with unknown transitions, adversarially changing costs, and unrestricted delayed bandit feedback. More precisely, the feedback for the agent in episode $k$ is revealed only in the end of episode $k + d^k$, where the delay $d^k$ can be changing over episodes and chosen by an oblivious adversary. We present the first algorithms that achieve near-optimal $\\sqrt{K + D}$ regret, where $K$ is the number of episodes and $D = \\sum_{k=1}^K d^k$ is the total delay, significantly improving upon the best known regret bound of $(K + D)^{2/3}$. ",
    "url": "https://arxiv.org/abs/2201.13172",
    "authors": [
      "Tiancheng Jin",
      "Tal Lancewicki",
      "Haipeng Luo",
      "Yishay Mansour",
      "Aviv Rosenberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13178",
    "title": "Few-Shot Backdoor Attacks on Visual Object Tracking",
    "abstract": "Visual object tracking (VOT) has been widely adopted in mission-critical applications, such as autonomous driving and intelligent surveillance systems. In current practice, third-party resources such as datasets, backbone networks, and training platforms are frequently used to train high-performance VOT models. Whilst these resources bring certain convenience, they also introduce new security threats into VOT models. In this paper, we reveal such a threat where an adversary can easily implant hidden backdoors into VOT models by tempering with the training process. Specifically, we propose a simple yet effective few-shot backdoor attack (FSBA) that optimizes two losses alternately: 1) a \\emph{feature loss} defined in the hidden feature space, and 2) the standard \\emph{tracking loss}. We show that, once the backdoor is embedded into the target model by our FSBA, it can trick the model to lose track of specific objects even when the \\emph{trigger} only appears in one or a few frames. We examine our attack in both digital and physical-world settings and show that it can significantly degrade the performance of state-of-the-art VOT trackers. We also show that our attack is resistant to potential defenses, highlighting the vulnerability of VOT models to potential backdoor attacks. ",
    "url": "https://arxiv.org/abs/2201.13178",
    "authors": [
      "Yiming Li",
      "Haoxiang Zhong",
      "Xingjun Ma",
      "Yong Jiang",
      "Shu-Tao Xia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13180",
    "title": "Learning on Arbitrary Graph Topologies via Predictive Coding",
    "abstract": "Training with backpropagation (BP) in standard deep learning consists of two main steps: a forward pass that maps a data point to its prediction, and a backward pass that propagates the error of this prediction back through the network. This process is highly effective when the goal is to minimize a specific objective function. However, it does not allow training on networks with cyclic or backward connections. This is an obstacle to reaching brain-like capabilities, as the highly complex heterarchical structure of the neural connections in the neocortex are potentially fundamental for its effectiveness. In this paper, we show how predictive coding (PC), a theory of information processing in the cortex, can be used to perform inference and learning on arbitrary graph topologies. We experimentally show how this formulation, called PC graphs, can be used to flexibly perform different tasks with the same network by simply stimulating specific neurons, and investigate how the topology of the graph influences the final performance. We conclude by comparing against simple baselines trained~with~BP. ",
    "url": "https://arxiv.org/abs/2201.13180",
    "authors": [
      "Tommaso Salvatori",
      "Luca Pinchetti",
      "Beren Millidge",
      "Yuhang Song",
      "Rafal Bogacz",
      "Thomas Lukasiewicz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13190",
    "title": "Differentiable Neural Radiosity",
    "abstract": "We introduce Differentiable Neural Radiosity, a novel method of representing the solution of the differential rendering equation using a neural network. Inspired by neural radiosity techniques, we minimize the norm of the residual of the differential rendering equation to directly optimize our network. The network is capable of outputting continuous, view-independent gradients of the radiance field with respect to scene parameters, taking into account differential global illumination effects while keeping memory and time complexity constant in path length. To solve inverse rendering problems, we use a pre-trained instance of our network that represents the differential radiance field with respect to a limited number of scene parameters. In our experiments, we leverage this to achieve faster and more accurate convergence compared to other techniques such as Automatic Differentiation, Radiative Backpropagation, and Path Replay Backpropagation. ",
    "url": "https://arxiv.org/abs/2201.13190",
    "authors": [
      "Saeed Hadadan",
      "Matthias Zwicker"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.13222",
    "title": "Perspective on Code Submission and Automated Evaluation Platforms for  University Teaching",
    "abstract": "We present a perspective on platforms for code submission and automated evaluation in the context of university teaching. Due to the COVID-19 pandemic, such platforms have become an essential asset for remote courses and a reasonable standard for structured code submission concerning increasing numbers of students in computer sciences. Utilizing automated code evaluation techniques exhibits notable positive impacts for both students and teachers in terms of quality and scalability. We identified relevant technical and non-technical requirements for such platforms in terms of practical applicability and secure code submission environments. Furthermore, a survey among students was conducted to obtain empirical data on general perception. We conclude that submission and automated evaluation involves continuous maintenance yet lowers the required workload for teachers and provides better evaluation transparency for students. ",
    "url": "https://arxiv.org/abs/2201.13222",
    "authors": [
      "Florian Auer",
      "Johann Frei",
      "Dominik M\u00fcller",
      "Frank Kramer"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2201.13226",
    "title": "Online Assessment Misconduct Detection using Internet Protocol and  Behavioural Classification",
    "abstract": "With the recent prevalence of remote education, academic assessments are often conducted online, leading to further concerns surrounding assessment misconducts. This paper investigates the potentials of online assessment misconduct (e-cheating) and proposes practical countermeasures against them. The mechanism for detecting the practices of online cheating is presented in the form of an e-cheating intelligent agent, comprising of an internet protocol (IP) detector and a behavioural monitor. The IP detector is an auxiliary detector which assigns randomised and unique assessment sets as an early procedure to reduce potential misconducts. The behavioural monitor scans for irregularities in assessment responses from the candidates, further reducing any misconduct attempts. This is highlighted through the proposal of the DenseLSTM using a deep learning approach. Additionally, a new PT Behavioural Database is presented and made publicly available. Experiments conducted on this dataset confirm the effectiveness of the DenseLSTM, resulting in classification accuracies of up to 90.7%. ",
    "url": "https://arxiv.org/abs/2201.13226",
    "authors": [
      "Leslie Ching Ow Tiong",
      "HeeJeong Jasmine Lee",
      "Kai Li Lim"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13237",
    "title": "CDNNs: The coupled deep neural networks for coupling of the Stokes and  Darcy-Forchheimer problems",
    "abstract": "In this article, we present an efficient deep learning method called coupled deep neural networks (CDNNs) for coupled physical problems. Our method compiles the interface conditions of the coupled PDEs into the networks properly and can be served as an efficient alternative to the complex coupled problems. To impose energy conservation constraints, the CDNNs utilize simple fully connected layers and a custom loss function to perform the model training process as well as the physical property of the exact solution. The approach can be beneficial for the following reasons: Firstly, we sampled randomly and only input spatial coordinates without being restricted by the nature of samples. Secondly, our method is meshfree which makes it more efficient than the traditional methods. Finally, our method is parallel and can solve multiple variables independently at the same time. We give the theory to guarantee the convergence of the loss function and the convergence of the neural networks to the exact solution. Some numerical experiments are performed and discussed to demonstrate the performance of the proposed method. ",
    "url": "https://arxiv.org/abs/2201.13237",
    "authors": [
      "Jing Yue",
      "Jian Li",
      "Wen Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2201.13266",
    "title": "Aggregation and Transformation of Vector-Valued Messages in the Shuffle  Model of Differential Privacy",
    "abstract": "Advances in communications, storage and computational technology allow significant quantities of data to be collected and processed by distributed devices. Combining the information from these endpoints can realize significant societal benefit but presents challenges in protecting the privacy of individuals, especially important in an increasingly regulated world. Differential privacy (DP) is a technique that provides a rigorous and provable privacy guarantee for aggregation and release. The Shuffle Model for DP has been introduced to overcome challenges regarding the accuracy of local-DP algorithms and the privacy risks of central-DP. In this work we introduce a new protocol for vector aggregation in the context of the Shuffle Model. The aim of this paper is twofold; first, we provide a single message protocol for the summation of real vectors in the Shuffle Model, using advanced composition results. Secondly, we provide an improvement on the bound on the error achieved through using this protocol through the implementation of a Discrete Fourier Transform, thereby minimizing the initial error at the expense of the loss in accuracy through the transformation itself. This work will further the exploration of more sophisticated structures such as matrices and higher-dimensional tensors in this context, both of which are reliant on the functionality of the vector case. ",
    "url": "https://arxiv.org/abs/2201.13266",
    "authors": [
      "Mary Scott",
      "Graham Cormode",
      "Carsten Maple"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2201.13267",
    "title": "Micro-level Reserving for General Insurance Claims using a Long  Short-Term Memory Network",
    "abstract": "Detailed information about individual claims are completely ignored when insurance claims data are aggregated and structured in development triangles for loss reserving. In the hope of extracting predictive power from the individual claims characteristics, researchers have recently proposed to move away from these macro-level methods in favor of micro-level loss reserving approaches. We introduce a discrete-time individual reserving framework incorporating granular information in a deep learning approach named Long Short-Term Memory (LSTM) neural network. At each time period, the network has two tasks: first, classifying whether there is a payment or a recovery, and second, predicting the corresponding non-zero amount, if any. We illustrate the estimation procedure on a simulated and a real general insurance dataset. We compare our approach with the chain-ladder aggregate method using the predictive outstanding loss estimates and their actual values. Based on a generalized Pareto model for excess payments over a threshold, we adjust the LSTM reserve prediction to account for extreme payments. ",
    "url": "https://arxiv.org/abs/2201.13267",
    "authors": [
      "Ihsan Chaoubi",
      "Camille Besse",
      "H\u00e9l\u00e8ne Cossette",
      "Marie-Pier C\u00f4t\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2201.13271",
    "title": "StRegA: Unsupervised Anomaly Detection in Brain MRIs using a Compact  Context-encoding Variational Autoencoder",
    "abstract": "Expert interpretation of anatomical images of the human brain is the central part of neuro-radiology. Several machine learning-based techniques have been proposed to assist in the analysis process. However, the ML models typically need to be trained to perform a specific task, e.g., brain tumour segmentation or classification. Not only do the corresponding training data require laborious manual annotations, but a wide variety of abnormalities can be present in a human brain MRI - even more than one simultaneously, which renders representation of all possible anomalies very challenging. Hence, a possible solution is an unsupervised anomaly detection (UAD) system that can learn a data distribution from an unlabelled dataset of healthy subjects and then be applied to detect out of distribution samples. Such a technique can then be used to detect anomalies - lesions or abnormalities, for example, brain tumours, without explicitly training the model for that specific pathology. Several Variational Autoencoder (VAE) based techniques have been proposed in the past for this task. Even though they perform very well on controlled artificially simulated anomalies, many of them perform poorly while detecting anomalies in clinical data. This research proposes a compact version of the \"context-encoding\" VAE (ceVAE) model, combined with pre and post-processing steps, creating a UAD pipeline (StRegA), which is more robust on clinical data, and shows its applicability in detecting anomalies such as tumours in brain MRIs. The proposed pipeline achieved a Dice score of 0.642$\\pm$0.101 while detecting tumours in T2w images of the BraTS dataset and 0.859$\\pm$0.112 while detecting artificially induced anomalies, while the best performing baseline achieved 0.522$\\pm$0.135 and 0.783$\\pm$0.111, respectively. ",
    "url": "https://arxiv.org/abs/2201.13271",
    "authors": [
      "Soumick Chatterjee",
      "Alessandro Sciarra",
      "Max D\u00fcnnwald",
      "Pavan Tummala",
      "Shubham Kumar Agrawal",
      "Aishwarya Jauhari",
      "Aman Kalra",
      "Steffen Oeltze-Jafra",
      "Oliver Speck",
      "Andreas N\u00fcrnberger"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2201.13290",
    "title": "Model-Based Engineering of CPPS Functions and Code Generation for Skills",
    "abstract": "Today's production systems are complex networks of cyber-physical systems which combine mechanical and electronic parts with software and networking capabilities. To the inherent complexity of such systems additional complexity arises from the context in which these systems operate. Manufacturing companies need to be able to adapt their production to ever changing customer demands as well as decreasing lot sizes. Engineering such systems, which need to be combined and reconfigured into different networks under changing conditions, requires engineering methods to carefully design them for possible future uses. Such engineering methods need to preserve the flexibility of functions into runtime, so that reconfiguring machines can be done with as little effort as possible. In this paper we present a model-based approach that is focused on machine functions and allows to methodically develop system functionalities for changing system networks. These functions are implemented as so-called skills using automated code-generation. ",
    "url": "https://arxiv.org/abs/2201.13290",
    "authors": [
      "Aljosha Koecher",
      "Alexander Hayward",
      "Alexander Fay"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2201.13311",
    "title": "Masked Transformer for Neighhourhood-aware Click-Through Rate Prediction",
    "abstract": "Click-Through Rate (CTR) prediction, is an essential component of online advertising. The mainstream techniques mostly focus on feature interaction or user interest modeling, which rely on users' directly interacted items. The performance of these methods are usally impeded by inactive behaviours and system's exposure, incurring that the features extracted do not contain enough information to represent all potential interests. For this sake, we propose Neighbor-Interaction based CTR prediction, which put this task into a Heterogeneous Information Network (HIN) setting, then involves local neighborhood of the target user-item pair in the HIN to predict their linkage. In order to enhance the representation of the local neighbourhood, we consider four types of topological interaction among the nodes, and propose a novel Graph-masked Transformer architecture to effectively incorporates both feature and topological information. We conduct comprehensive experiments on two real world datasets and the experimental results show that our proposed method outperforms state-of-the-art CTR models significantly. ",
    "url": "https://arxiv.org/abs/2201.13311",
    "authors": [
      "Erxue Min",
      "Yu Rong",
      "Tingyang Xu",
      "Yatao Bian",
      "Peilin Zhao",
      "Junzhou Huang",
      "Da Luo",
      "Kangyi Lin",
      "Sophia Ananiadou"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13317",
    "title": "Hyper-Class Representation of Data",
    "abstract": "Data representation is often of the natural form with their attribute values. To utilize the data efficiently, one needs to well understand the observed attribute values and identify the potential useful information in the data/smaples, or training data. In this paper, a new data representation, named as hyper-classes representation, is proposed for improving recommendation. At first, the cross entropy, KL divergence and JS divergence of features in data are defined. And then, the hyper-classes in data can be discovered with these three parameters. Finally, a kind of recommendation algorithm is used to evaluate the proposed hyper-class representation of data, and shows that the hyper-class representation is able to provide truly useful reference information for recommendation systems and makes recommendations much better than existing algorithms, i.e., this approach is efficient and promising. ",
    "url": "https://arxiv.org/abs/2201.13317",
    "authors": [
      "Shichao Zhang",
      "Jiaye Li",
      "Wenzhen Zhang",
      "Yongsong Qin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2201.13329",
    "title": "Can Adversarial Training Be Manipulated By Non-Robust Features?",
    "abstract": "Adversarial training, originally designed to resist test-time adversarial examples, has shown to be promising in mitigating training-time availability attacks. This defense ability, however, is challenged in this paper. We identify a novel threat model named stability attacks, which aims to hinder robust availability by slightly perturbing the training data. Under this threat, we find that adversarial training using a conventional defense budget $\\epsilon$ provably fails to provide test robustness in a simple statistical setting when the non-robust features of the training data are reinforced by $\\epsilon$-bounded perturbation. Further, we analyze the necessity of enlarging the defense budget to counter stability attacks. Finally, comprehensive experiments demonstrate that stability attacks are harmful on benchmark datasets, and thus the adaptive defense is necessary to maintain robustness. ",
    "url": "https://arxiv.org/abs/2201.13329",
    "authors": [
      "Lue Tao",
      "Lei Feng",
      "Hongxin Wei",
      "Jinfeng Yi",
      "Sheng-Jun Huang",
      "Songcan Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2201.13351",
    "title": "LinSyn: Synthesizing Tight Linear Bounds for Arbitrary Neural Network  Activation Functions",
    "abstract": "The most scalable approaches to certifying neural network robustness depend on computing sound linear lower and upper bounds for the network's activation functions. Current approaches are limited in that the linear bounds must be handcrafted by an expert, and can be sub-optimal, especially when the network's architecture composes operations using, for example, multiplication such as in LSTMs and the recently popular Swish activation. The dependence on an expert prevents the application of robustness certification to developments in the state-of-the-art of activation functions, and furthermore the lack of tightness guarantees may give a false sense of insecurity about a particular model. To the best of our knowledge, we are the first to consider the problem of automatically computing tight linear bounds for arbitrary n-dimensional activation functions. We propose LinSyn, the first approach that achieves tight bounds for any arbitrary activation function, while only leveraging the mathematical definition of the activation function itself. Our approach leverages an efficient heuristic approach to synthesize bounds that are tight and usually sound, and then verifies the soundness (and adjusts the bounds if necessary) using the highly optimized branch-and-bound SMT solver, dReal. Even though our approach depends on an SMT solver, we show that the runtime is reasonable in practice, and, compared with state of the art, our approach often achieves 2-5X tighter final output bounds and more than quadruple certified robustness. ",
    "url": "https://arxiv.org/abs/2201.13351",
    "authors": [
      "Brandon Paulsen",
      "Chao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2201.13354",
    "title": "Graph Set-colorings And Hypergraphs In Topological Coding",
    "abstract": "In order to make more complex number-based strings from topological coding for defending against the intelligent attacks equipped with quantum computing and providing effective protection technology for the age of quantum computing, we will introduce set-colored graphs admitting set-colorings that has been considerable cryptanalytic significance, and especially related with hypergraphs. We use the set-coloring of graphs to reflect the intersection of elements, and add other constraint requirements to express more connections between sets (as hyperedges). Since we try to find some easy and effective techniques based on graph theory for practical application, we use intersected-graphs admitting set-colorings defined on hyperedge sets to observe topological structures of hypergraphs, string-type Topcode-matrix, set-type Topcode-matrix, graph-type Topcode-matrix, hypergraph-type Topcode-matrix, matrix-type Topcode-matrix \\emph{etc}. We will show that each connected graph is the intersected-graph of some hypergraph and investigate hypergraph's connectivity, colorings of hypergraphs, hypergraph homomorphism, hypernetworks, scale-free network generator, compound hypergraphs having their intersected-graphs with vertices to be hypergraphs (for high-dimensional extension diagram). Naturally, we get various graphic lattices, such as edge-coincided intersected-graph lattice, vertex-coincided intersected-graph lattice, edge-hamiltonian graphic lattice, hypergraph lattice and intersected-network lattice. Many techniques in this article can be translated into polynomial algorithms, since we are aiming to apply hypergraphs and graph set-colorings to homomorphic encryption and asymmetric cryptograph. ",
    "url": "https://arxiv.org/abs/2201.13354",
    "authors": [
      "Bing Yao",
      "Fei Ma"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2201.13357",
    "title": "DNS: Determinantal Point Process Based Neural Network Sampler for  Ensemble Reinforcement Learning",
    "abstract": "Application of ensemble of neural networks is becoming an imminent tool for advancing the state-of-the-art in deep reinforcement learning algorithms. However, training these large numbers of neural networks in the ensemble has an exceedingly high computation cost which may become a hindrance in training large-scale systems. In this paper, we propose DNS: a Determinantal Point Process based Neural Network Sampler that specifically uses k-dpp to sample a subset of neural networks for backpropagation at every training step thus significantly reducing the training time and computation cost. We integrated DNS in REDQ for continuous control tasks and evaluated on MuJoCo environments. Our experiments show that DNS augmented REDQ outperforms baseline REDQ in terms of average cumulative reward and achieves this using less than 50% computation when measured in FLOPS. ",
    "url": "https://arxiv.org/abs/2201.13357",
    "authors": [
      "Hassam Sheikh",
      "Kizza Frisbee",
      "Mariano Phielipp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13360",
    "title": "Hydra: A Real-time Spatial Perception Engine for 3D Scene Graph  Construction and Optimization",
    "abstract": "3D scene graphs have recently emerged as a powerful high-level representation of 3D environments. A 3D scene graph describes the environment as a layered graph where nodes represent spatial concepts at multiple levels of abstraction and edges represent relations between concepts. While 3D scene graphs can serve as an advanced \"mental model\" for robots, how to build such a rich representation in real-time is still uncharted territory. This paper describes the first real-time Spatial Perception engINe (SPIN), a suite of algorithms to build a 3D scene graph from sensor data in real-time. Our first contribution is to develop real-time algorithms to incrementally construct the layers of a scene graph as the robot explores the environment; these algorithms build a local Euclidean Signed Distance Function (ESDF) around the current robot location, extract a topological map of places from the ESDF, and then segment the places into rooms using an approach inspired by community-detection techniques. Our second contribution is to investigate loop closure detection and optimization in 3D scene graphs. We show that 3D scene graphs allow defining hierarchical descriptors for loop closure detection; our descriptors capture statistics across layers in the scene graph, ranging from low-level visual appearance, to summary statistics about objects and places. We then propose the first algorithm to optimize a 3D scene graph in response to loop closures; our approach relies on embedded deformation graphs to simultaneously correct all layers of the scene graph. We implement the proposed SPIN into a highly parallelized architecture, named Hydra, that combines fast early and mid-level perception processes with slower high-level perception. We evaluate Hydra on simulated and real data and show it is able to reconstruct 3D scene graphs with an accuracy comparable with batch offline methods, while running online. ",
    "url": "https://arxiv.org/abs/2201.13360",
    "authors": [
      "Nathan Hughes",
      "Yun Chang",
      "Luca Carlone"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2201.13377",
    "title": "Neural Network Training with Asymmetric Crosspoint Elements",
    "abstract": "Analog crossbar arrays comprising programmable nonvolatile resistors are under intense investigation for acceleration of deep neural network training. However, the ubiquitous asymmetric conductance modulation of practical resistive devices critically degrades the classification performance of networks trained with conventional algorithms. Here, we describe and experimentally demonstrate an alternative fully-parallel training algorithm: Stochastic Hamiltonian Descent. Instead of conventionally tuning weights in the direction of the error function gradient, this method programs the network parameters to successfully minimize the total energy (Hamiltonian) of the system that incorporates the effects of device asymmetry. We provide critical intuition on why device asymmetry is fundamentally incompatible with conventional training algorithms and how the new approach exploits it as a useful feature instead. Our technique enables immediate realization of analog deep learning accelerators based on readily available device technologies. ",
    "url": "https://arxiv.org/abs/2201.13377",
    "authors": [
      "Murat Onen",
      "Tayfun Gokmen",
      "Teodor K. Todorov",
      "Tomasz Nowicki",
      "Jesus A. del Alamo",
      "John Rozen",
      "Wilfried Haensch",
      "Seyoung Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Emerging Technologies (cs.ET)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2201.13388",
    "title": "Compositional Multi-Object Reinforcement Learning with Linear Relation  Networks",
    "abstract": "Although reinforcement learning has seen remarkable progress over the last years, solving robust dexterous object-manipulation tasks in multi-object settings remains a challenge. In this paper, we focus on models that can learn manipulation tasks in fixed multi-object settings and extrapolate this skill zero-shot without any drop in performance when the number of objects changes. We consider the generic task of bringing a specific cube out of a set to a goal position. We find that previous approaches, which primarily leverage attention and graph neural network-based architectures, do not generalize their skills when the number of input objects changes while scaling as $K^2$. We propose an alternative plug-and-play module based on relational inductive biases to overcome these limitations. Besides exceeding performances in their training environment, we show that our approach, which scales linearly in $K$, allows agents to extrapolate and generalize zero-shot to any new object number. ",
    "url": "https://arxiv.org/abs/2201.13388",
    "authors": [
      "Davide Mambelli",
      "Frederik Tr\u00e4uble",
      "Stefan Bauer",
      "Bernhard Sch\u00f6lkopf",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2201.13389",
    "title": "Physics-informed neural networks for non-Newtonian fluid  thermo-mechanical problems: an application to rubber calendering process",
    "abstract": "Physics-Informed Neural Networks (PINNs) have gained much attention in various fields of engineering thanks to their capability of incorporating physical laws into the models. However, the assessment of PINNs in industrial applications involving coupling between mechanical and thermal fields is still an active research topic. In this work, we present an application of PINNs to a non-Newtonian fluid thermo-mechanical problem which is often considered in the rubber calendering process. We demonstrate the effectiveness of PINNs when dealing with inverse and ill-posed problems, which are impractical to be solved by classical numerical discretization methods. We study the impact of the placement of the sensors and the distribution of unsupervised points on the performance of PINNs in a problem of inferring hidden physical fields from some partial data. We also investigate the capability of PINNs to identify unknown physical parameters from the measurements captured by sensors. The effect of noisy measurements is also considered throughout this work. The results of this paper demonstrate that in the problem of identification, PINNs can successfully estimate the unknown parameters using only the measurements on the sensors. In ill-posed problems where boundary conditions are not completely defined, even though the placement of the sensors and the distribution of unsupervised points have a great impact on PINNs performance, we show that the algorithm is able to infer the hidden physics from local measurements. ",
    "url": "https://arxiv.org/abs/2201.13389",
    "authors": [
      "Thi Nguyen Khoa Nguyen",
      "Thibault Dairay",
      "Rapha\u00ebl Meunier",
      "Mathilde Mougeot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13392",
    "title": "MHSnet: Multi-head and Spatial Attention Network with False-Positive  Reduction for Pulmonary Nodules Detection",
    "abstract": "The mortality of lung cancer has ranked high among cancers for many years. Early detection of lung cancer is critical for disease prevention, cure, and mortality rate reduction. However, existing detection methods on pulmonary nodules introduce an excessive number of false positive proposals in order to achieve high sensitivity, which is not practical in clinical situations. In this paper, we propose the multi-head detection and spatial squeeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to aid doctors in the early diagnosis of lung cancers. Specifically, we first introduce multi-head detectors and skip connections to customize for the variety of nodules in sizes, shapes and types and capture multi-scale features. Then, we implement a spatial attention module to enable the network to focus on different regions differently inspired by how experienced clinicians screen CT images, which results in fewer false positive proposals. Lastly, we present a lightweight but effective false positive reduction module with the Linear Regression model to cut down the number of false positive proposals, without any constraints on the front network. Extensive experimental results compared with the state-of-the-art models have shown the superiority of the MHSnet in terms of the average FROC, sensitivity and especially false discovery rate (2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62% and 28.33% decrease in terms of false discovery rate and average candidates per scan). The false positive reduction module significantly decreases the average number of candidates generated per scan by 68.11% and the false discovery rate by 13.48%, which is promising to reduce distracted proposals for the downstream tasks based on the detection results. ",
    "url": "https://arxiv.org/abs/2201.13392",
    "authors": [
      "Juanyun Mai",
      "Minghao Wang",
      "Jiayin Zheng",
      "Yanbo Shao",
      "Zhaoqi Diao",
      "Xinliang Fu",
      "Yulong Chen",
      "Jianyu Xiao",
      "Jian You",
      "Airu Yin",
      "Yang Yang",
      "Xiangcheng Qiu",
      "Jingsheng Tao",
      "Bo Wang",
      "Hua Ji"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.13395",
    "title": "Neural Collaborative Filtering Bandits via Meta Learning",
    "abstract": "Contextual multi-armed bandits provide powerful tools to solve the exploitation-exploration dilemma in decision making, with direct applications in the personalized recommendation. In fact, collaborative effects among users carry the significant potential to improve the recommendation. In this paper, we introduce and study the problem by exploring `Neural Collaborative Filtering Bandits', where the rewards can be non-linear functions and groups are formed dynamically given different specific contents. To solve this problem, inspired by meta-learning, we propose Meta-Ban (meta-bandits), where a meta-learner is designed to represent and rapidly adapt to dynamic groups, along with a UCB-based exploration strategy. Furthermore, we analyze that Meta-Ban can achieve the regret bound of $\\mathcal{O}(\\sqrt{T \\log T})$, improving a multiplicative factor $\\sqrt{\\log T}$ over state-of-the-art related works. In the end, we conduct extensive experiments showing that Meta-Ban significantly outperforms six strong baselines. ",
    "url": "https://arxiv.org/abs/2201.13395",
    "authors": [
      "{Yikun Ban",
      "Yunzhe Qi",
      "Tianxin Wei",
      "Jingrui He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13402",
    "title": "Privacy Limitations Of Interest-based Advertising On The Web: A  Post-mortem Empirical Analysis Of Google's FLoC",
    "abstract": "In 2021, Google announced they would disable third-party cookies in the Chrome browser in order to improve user privacy. They proposed FLoC as an alternative, meant to enable interest-based advertising while mitigating risks of individualized user tracking. The FLoC algorithm assigns users to 'cohorts' that represent groups of users with similar browsing behaviors so that third-parties can serve users ads based on their group. After testing FLoC in a real world trial, Google canceled the proposal, with little explanation, in favor of new alternatives to third-party cookies. In this work, we offer a post-mortem analysis of how FLoC handled balancing utility and privacy. In particular, we analyze two potential problems raised by privacy advocates: FLoC (1) allows individualized user tracking rather than prevents it and (2) risks revealing sensitive user demographic information, presenting a new privacy risk. We test these problems by implementing FLoC and compute cohorts for users in a dataset of browsing histories collected from more than 90,000 U.S. devices over a one-year period. For (1) we investigate the uniqueness of users' cohort ID sequences over time. We find that more than 95% are uniquely identifiable after 4 weeks. We show how these risks increase when cohort IDs are combined with fingerprinting data. While these risks may be mitigated by frequently clearing first-party cookies and increasing cohort sizes, such changes would degrade utility for users and advertisers, respectively. For (2), although we find a statistically significant relationship between domain visits and racial background, we do not find that FLoC risks correlating cohort IDs with race. However, alternative clustering techniques could elevate this risk. Our contributions provide example analyses for those seeking to develop novel approaches to monetizing the web in the future. ",
    "url": "https://arxiv.org/abs/2201.13402",
    "authors": [
      "Alex Berke",
      "Dan Calacci"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2201.13444",
    "title": "Boundary Defense Against Black-box Adversarial Attacks",
    "abstract": "Black-box adversarial attacks generate adversarial samples via iterative optimizations using repeated queries. Defending deep neural networks against such attacks has been challenging. In this paper, we propose an efficient Boundary Defense (BD) method which mitigates black-box attacks by exploiting the fact that the adversarial optimizations often need samples on the classification boundary. Our method detects the boundary samples as those with low classification confidence and adds white Gaussian noise to their logits. The method's impact on the deep network's classification accuracy is analyzed theoretically. Extensive experiments are conducted and the results show that the BD method can reliably defend against both soft and hard label black-box attacks. It outperforms a list of existing defense methods. For IMAGENET models, by adding zero-mean white Gaussian noise with standard deviation 0.1 to logits when the classification confidence is less than 0.3, the defense reduces the attack success rate to almost 0 while limiting the classification accuracy degradation to around 1 percent. ",
    "url": "https://arxiv.org/abs/2201.13444",
    "authors": [
      "Manjushree B. Aithal",
      "Xiaohua Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2201.12362",
    "title": "Physics-informed neural networks to learn cardiac fiber orientation from  multiple electroanatomical maps",
    "abstract": "We propose FiberNet, a method to estimate in-vivo the cardiac fiber architecture of the human atria from multiple catheter recordings of the electrical activation. Cardiac fibers play a central rolein the electro-mechanical function of the heart, yet they aredifficult to determine in-vivo, and hence rarely truly patient-specificin existing cardiac models.FiberNet learns the fibers arrangement by solvingan inverse problem with physics-informed neural networks. The inverse problem amounts to identifyingthe conduction velocity tensor of a cardiac propagation modelfrom a set of sparse activation maps. The use of multiple mapsenables the simultaneous identification of all the componentsof the conduction velocity tensor, including the local fiber angle.We extensively test FiberNet on synthetic 2-D and 3-D examples, diffusion tensor fibers, and a patient-specific case. We show that 3 maps are sufficient to accurately capture the fibers, also in thepresence of noise. With fewer maps, the role of regularization becomesprominent. Moreover, we show that the fitted model can robustlyreproduce unseen activation maps. We envision that FiberNet will help the creation of patient-specific models for personalized medicine.The full code is available at this http URL ",
    "url": "https://arxiv.org/abs/2201.12362",
    "authors": [
      "Carlos Ruiz Herrera",
      "Thomas Grandits",
      "Gernot Plank",
      "Paris Perdikaris",
      "Francisco Sahli Costabal",
      "Simone Pezzuto"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)",
      "Medical Physics (physics.med-ph)",
      "Tissues and Organs (q-bio.TO)"
    ]
  },
  {
    "id": "arXiv:2201.12419",
    "title": "FastFlows: Flow-Based Models for Molecular Graph Generation",
    "abstract": "We propose a framework using normalizing-flow based models, SELF-Referencing Embedded Strings, and multi-objective optimization that efficiently generates small molecules. With an initial training set of only 100 small molecules, FastFlows generates thousands of chemically valid molecules in seconds. Because of the efficient sampling, substructure filters can be applied as desired to eliminate compounds with unreasonable moieties. Using easily computable and learned metrics for druglikeness, synthetic accessibility, and synthetic complexity, we perform a multi-objective optimization to demonstrate how FastFlows functions in a high-throughput virtual screening context. Our model is significantly simpler and easier to train than autoregressive molecular generative models, and enables fast generation and identification of druglike, synthesizable molecules. ",
    "url": "https://arxiv.org/abs/2201.12419",
    "authors": [
      "Nathan C. Frey",
      "Vijay Gadepally",
      "Bharath Ramsundar"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12535",
    "title": "Validation and Generalizability of Self-Supervised Image Reconstruction  Methods for Undersampled MRI",
    "abstract": "Purpose: To investigate aspects of the validation of self-supervised algorithms for reconstruction of undersampled MR images: quantitative evaluation of prospective reconstructions, potential differences between prospective and retrospective reconstructions, suitability of commonly used quantitative metrics, and generalizability. Theory and Methods: Two self-supervised algorithms based on self-supervised denoising and neural network image priors were investigated. These methods are compared to a least squares fitting and a compressed sensing reconstruction using in-vivo and phantom data. Their generalizability was tested with prospectively under-sampled data from experimental conditions different to the training. Results: Prospective reconstructions can exhibit significant distortion relative to retrospective reconstructions/ground truth. Pixel-wise quantitative metrics may not capture differences in perceptual quality accurately, in contrast to a perceptual metric. All methods showed potential for generalization; generalizability is more affected by changes in anatomy/contrast than other changes. No-reference image metrics correspond well with human rating of image quality for studying generalizability. Compressed Sensing and learned denoising perform similarly well on all data. Conclusion: Self-supervised methods show promising results for accelerating image reconstruction in clinical routines. Nonetheless, more work is required to investigate standardized methods to validate reconstruction algorithms for future clinical use. ",
    "url": "https://arxiv.org/abs/2201.12535",
    "authors": [
      "Thomas Yu",
      "Tom Hilbert",
      "Gian Franco Piredda",
      "Arun Joseph",
      "Gabriele Bonanno",
      "Salim Zenkhri",
      "Patrick Omoumi",
      "Meritxell Bach Cuadra",
      "Erick Jorge Canales-Rodr\u00edguez",
      "Tobias Kober",
      "Jean-Philippe Thiran"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2201.12584",
    "title": "Convolutional Filtering in Simplicial Complexes",
    "abstract": "This paper proposes convolutional filtering for data whose structure can be modeled by a simplicial complex (SC). SCs are mathematical tools that not only capture pairwise relationships as graphs but account also for higher-order network structures. These filters are built by following the shift-and-sum principle of the convolution operation and rely on the Hodge-Laplacians to shift the signal within the simplex. But since in SCs we have also inter-simplex coupling, we use the incidence matrices to transfer the signal in adjacent simplices and build a filter bank to jointly filter signals from different levels. We prove some interesting properties for the proposed filter bank, including permutation and orientation equivariance, a computational complexity that is linear in the SC dimension, and a spectral interpretation using the simplicial Fourier transform. We illustrate the proposed approach with numerical experiments. ",
    "url": "https://arxiv.org/abs/2201.12584",
    "authors": [
      "Elvin Isufi",
      "Maosheng Yang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12611",
    "title": "Learning Stochastic Graph Neural Networks with Constrained Variance",
    "abstract": "Stochastic graph neural networks (SGNNs) are information processing architectures that learn representations from data over random graphs. SGNNs are trained with respect to the expected performance, which comes with no guarantee about deviations of particular output realizations around the optimal expectation. To overcome this issue, we propose a variance-constrained optimization problem for SGNNs, balancing the expected performance and the stochastic deviation. An alternating primal-dual learning procedure is undertaken that solves the problem by updating the SGNN parameters with gradient descent and the dual variable with gradient ascent. To characterize the explicit effect of the variance-constrained learning, we conduct a theoretical analysis on the variance of the SGNN output and identify a trade-off between the stochastic robustness and the discrimination power. We further analyze the duality gap of the variance-constrained optimization problem and the converging behavior of the primal-dual learning procedure. The former indicates the optimality loss induced by the dual transformation and the latter characterizes the limiting error of the iterative algorithm, both of which guarantee the performance of the variance-constrained learning. Through numerical simulations, we corroborate our theoretical findings and observe a strong expected performance with a controllable standard deviation. ",
    "url": "https://arxiv.org/abs/2201.12611",
    "authors": [
      "Zhan Gao",
      "Elvin Isufi"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12755",
    "title": "HGCN: harmonic gated compensation network for speech enhancement",
    "abstract": "Mask processing in the time-frequency (T-F) domain through the neural network has been one of the mainstreams for single-channel speech enhancement. However, it is hard for most models to handle the situation when harmonics are partially masked by noise. To tackle this challenge, we propose a harmonic gated compensation network (HGCN). We design a high-resolution harmonic integral spectrum to improve the accuracy of harmonic locations prediction. Then we add voice activity detection (VAD) and voiced region detection (VRD) to the convolutional recurrent network (CRN) to filter harmonic locations. Finally, the harmonic gating mechanism is used to guide the compensation model to adjust the coarse results from CRN to obtain the refinedly enhanced results. Our experiments show HGCN achieves substantial gain over a number of advanced approaches in the community. ",
    "url": "https://arxiv.org/abs/2201.12755",
    "authors": [
      "Tianrui Wang",
      "Weibin Zhu",
      "Yingying Gao",
      "Junlan Feng",
      "Shilei Zhang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2201.12898",
    "title": "ClearingPayments in Dynamic Financial Networks",
    "abstract": "In this paper, we propose a novel dynamical model of clearing in a financial network, which stems from the classical Eisenberg- Noe model of financial contagion. The Eisenberg-Noe model assumes that at one point in time (say, at the end of a day), all liabilities are claimed and due simultaneously, and that the entire network of banks becomes aware of the claims and possible defaults and instantaneously agrees on the clearing payments. The motivation for the dynamic model we propose in this paper is that one may expect that if financial operations are allowed for a given number of time periods after the initial theoretical defaults, some nodes may actually recover and eventually manage to fulfill their obligations. We prove that the proposed model obeys the standard requirement known as the priority of debt claims, that is, each node either pays its liabilities in full, or it pays out all its balance. We also show that the requirements of ro-rata payments determines the solution uniquely. ",
    "url": "https://arxiv.org/abs/2201.12898",
    "authors": [
      "Giuseppe C. Calafiore",
      "Giulia Fracastoro",
      "Anton V. Proskurnikov"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Systems and Control (eess.SY)",
      "Mathematical Finance (q-fin.MF)",
      "Risk Management (q-fin.RM)"
    ]
  },
  {
    "id": "arXiv:2201.12942",
    "title": "The road problem and homomorphisms of directed graphs",
    "abstract": "We make progress on a generalization of the road (colouring) problem. The road problem was posed by Adler-Goodwyn-Weiss and solved by Trahtman. The generalization was posed, and solved in certain special cases, by Ashley-Marcus-Tuncel. We resolve two new families of cases, of which one generalizes the road problem and follows Trahtman's solution, and the other generalizes a result of Ashley-Marcus-Tuncel with a proof quite different from theirs. Along the way, we prove a universal property for the fiber product of certain graph homomorphisms, which may be of independent interest. We provide polynomial-time algorithms for relevant constructions and decision problems. ",
    "url": "https://arxiv.org/abs/2201.12942",
    "authors": [
      "Sophie MacDonald"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2201.12973",
    "title": "GenMod: A generative modeling approach for spectral representation of  PDEs with random inputs",
    "abstract": "We propose a method for quantifying uncertainty in high-dimensional PDE systems with random parameters, where the number of solution evaluations is small. Parametric PDE solutions are often approximated using a spectral decomposition based on polynomial chaos expansions. For the class of systems we consider (i.e., high dimensional with limited solution evaluations) the coefficients are given by an underdetermined linear system in a regression formulation. This implies additional assumptions, such as sparsity of the coefficient vector, are needed to approximate the solution. Here, we present an approach where we assume the coefficients are close to the range of a generative model that maps from a low to a high dimensional space of coefficients. Our approach is inspired be recent work examining how generative models can be used for compressed sensing in systems with random Gaussian measurement matrices. Using results from PDE theory on coefficient decay rates, we construct an explicit generative model that predicts the polynomial chaos coefficient magnitudes. The algorithm we developed to find the coefficients, which we call GenMod, is composed of two main steps. First, we predict the coefficient signs using Orthogonal Matching Pursuit. Then, we assume the coefficients are within a sparse deviation from the range of a sign-adjusted generative model. This allows us to find the coefficients by solving a nonconvex optimization problem, over the input space of the generative model and the space of sparse vectors. We obtain theoretical recovery results for a Lipschitz continuous generative model and for a more specific generative model, based on coefficient decay rate bounds. We examine three high-dimensional problems and show that, for all three examples, the generative model approach outperforms sparsity promoting methods at small sample sizes. ",
    "url": "https://arxiv.org/abs/2201.12973",
    "authors": [
      "Jacqueline Wentz",
      "Alireza Doostan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13112",
    "title": "Bayesian Optimization for Distributionally Robust Chance-constrained  Problem",
    "abstract": "In black-box function optimization, we need to consider not only controllable design variables but also uncontrollable stochastic environment variables. In such cases, it is necessary to solve the optimization problem by taking into account the uncertainty of the environmental variables. Chance-constrained (CC) problem, the problem of maximizing the expected value under a certain level of constraint satisfaction probability, is one of the practically important problems in the presence of environmental variables. In this study, we consider distributionally robust CC (DRCC) problem and propose a novel DRCC Bayesian optimization method for the case where the distribution of the environmental variables cannot be precisely specified. We show that the proposed method can find an arbitrary accurate solution with high probability in a finite number of trials, and confirm the usefulness of the proposed method through numerical experiments. ",
    "url": "https://arxiv.org/abs/2201.13112",
    "authors": [
      "Yu Inatsu",
      "Shion Takeno",
      "Masayuki Karasuyama",
      "Ichiro Takeuchi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13148",
    "title": "Threshold Independent Evaluation of Sound Event Detection Scores",
    "abstract": "Performing an adequate evaluation of sound event detection (SED) systems is far from trivial and is still subject to ongoing research. The recently proposed polyphonic sound detection (PSD)-receiver operating characteristic (ROC) and PSD score (PSDS) make an important step into the direction of an evaluation of SED systems which is independent from a certain decision threshold. This allows to obtain a more complete picture of the overall system behavior which is less biased by threshold tuning. Yet, the PSD-ROC is currently only approximated using a finite set of thresholds. The choice of the thresholds used in approximation, however, can have a severe impact on the resulting PSDS. In this paper we propose a method which allows for computing system performance on an evaluation set for all possible thresholds jointly, enabling accurate computation not only of the PSD-ROC and PSDS but also of other collar-based and intersection-based performance curves. It further allows to select the threshold which best fulfills the requirements of a given application. Source code is publicly available in our SED evaluation package sed_scores_eval. ",
    "url": "https://arxiv.org/abs/2201.13148",
    "authors": [
      "Janek Ebbers",
      "Romain Serizel",
      "Reinhold Haeb-Umbach"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2201.13299",
    "title": "Directed Weight Neural Networks for Protein Structure Representation  Learning",
    "abstract": "A protein performs biological functions by folding to a particular 3D structure. To accurately model the protein structures, both the overall geometric topology and local fine-grained relations between amino acids (e.g. side-chain torsion angles and inter-amino-acid orientations) should be carefully considered. In this work, we propose the Directed Weight Neural Network for better capturing geometric relations among different amino acids. Extending a single weight from a scalar to a 3D directed vector, our new framework supports a rich set of geometric operations on both classical and SO(3)--representation features, on top of which we construct a perceptron unit for processing amino-acid information. In addition, we introduce an equivariant message passing paradigm on proteins for plugging the directed weight perceptrons into existing Graph Neural Networks, showing superior versatility in maintaining SO(3)-equivariance at the global scale. Experiments show that our network has remarkably better expressiveness in representing geometric relations in comparison to classical neural networks and the (globally) equivariant networks. It also achieves state-of-the-art performance on various computational biology applications related to protein 3D structures. ",
    "url": "https://arxiv.org/abs/2201.13299",
    "authors": [
      "Jiahan Li",
      "Shitong Luo",
      "Congyue Deng",
      "Chaoran Cheng",
      "Jiaqi Guan",
      "Leonidas Guibas",
      "Jian Peng",
      "Jianzhu Ma"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.13344",
    "title": "The collective vs individual nature of mountaineering: a network and  simplicial approach",
    "abstract": "Mountaineering is a sport of contrary forces: teamwork plays a large role in mental fortitude and skills, but the actual act of climbing, and indeed survival, is largely individualistic. This work studies the effects of the structure and topology of relationships within climbers on the level of cooperation and success. It does so using simplicial complexes, where relationships between climbers are captured through simplexes that correspond to joint previous expeditions with dimension given by the number of climbers minus one and weight given by the number of occurrences of the simplex. First, this analysis establishes the importance of relationships and shows that chances of failure to summit reduce drastically when climbing with repeat partners. From a climber-centric perspective, climbers that belong to simplexes with large dimension were more likely to be successful, across experience levels. From an expedition-centric perspective, the distribution of relationships within a group is explored to identify collective human behavior: from polarized to cooperative. Expeditions containing simplices with large dimension, and usually low weight, i.e., a large number of people had a small number of previous joint expeditions, tended to be more cooperative, with more homogeneity in success amongst climbers. On the other hand, the existence of small, usually strong, subgroups lead to a polarized style where climbers that were not a part of the subgroup were less likely to succeed. Lastly, this work examines the effects of individual features and expedition-wide factors that may play different roles in individualistic and cooperative expeditions. Centrality indicates that individual traits of youth and oxygen use while ascending are strong drivers of success. Of expedition-wide factors, the expedition size and number of expedition days are found to be strongly correlated with success rate. ",
    "url": "https://arxiv.org/abs/2201.13344",
    "authors": [
      "Sanjukta Krishnagopal"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)",
      "General Topology (math.GN)"
    ]
  },
  {
    "id": "arXiv:2201.13372",
    "title": "Robust supervised learning with coordinate gradient descent",
    "abstract": "This paper considers the problem of supervised learning with linear methods when both features and labels can be corrupted, either in the form of heavy tailed data and/or corrupted rows. We introduce a combination of coordinate gradient descent as a learning algorithm together with robust estimators of the partial derivatives. This leads to robust statistical learning methods that have a numerical complexity nearly identical to non-robust ones based on empirical risk minimization. The main idea is simple: while robust learning with gradient descent requires the computational cost of robustly estimating the whole gradient to update all parameters, a parameter can be updated immediately using a robust estimator of a single partial derivative in coordinate gradient descent. We prove upper bounds on the generalization error of the algorithms derived from this idea, that control both the optimization and statistical errors with and without a strong convexity assumption of the risk. Finally, we propose an efficient implementation of this approach in a new python library called linlearn, and demonstrate through extensive numerical experiments that our approach introduces a new interesting compromise between robustness, statistical performance and numerical efficiency for this problem. ",
    "url": "https://arxiv.org/abs/2201.13372",
    "authors": [
      "St\u00e9phane Ga\u00efffas",
      "Ibrahim Merad"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:1804.08992",
    "title": "Infrared and visible image fusion using Latent Low-Rank Representation",
    "abstract": " Comments: 6 pages, 8 figures, 1 tables ",
    "url": "https://arxiv.org/abs/1804.08992",
    "authors": [
      "Hui Li",
      "Xiao-Jun Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:1909.07972",
    "title": "A Joint Learning and Communications Framework for Federated Learning  over Wireless Networks",
    "abstract": " Comments: This paper has been accepted by IEEE Transactions on Wireless Communications. Our code is available at: this https URL ",
    "url": "https://arxiv.org/abs/1909.07972",
    "authors": [
      "Mingzhe Chen",
      "Zhaohui Yang",
      "Walid Saad",
      "Changchuan Yin",
      "H. Vincent Poor",
      "Shuguang Cui"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:1910.09359",
    "title": "Building Efficient CNNs Using Depthwise Convolutional Eigen-Filters  (DeCEF)",
    "abstract": " Comments: key words: subspace analysis, convolutional neural networks, FLOPs, number of parameters, depthwise separable convolutions ",
    "url": "https://arxiv.org/abs/1910.09359",
    "authors": [
      "Yinan Yu",
      "Samuel Scheidegger",
      "Tomas McKelvey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2002.01102",
    "title": "Improved dual channel pulse coupled neural network and its application  to multi-focus image fusion",
    "abstract": " Comments: 16 pages, 7 figures, 9 tables ",
    "url": "https://arxiv.org/abs/2002.01102",
    "authors": [
      "Huai-Shui Tong",
      "Xiao-Jun Wu",
      "Hui Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2003.10622",
    "title": "Adaptive Cooperative Tracking and Parameter Estimation of an Uncertain  Leader over General Directed Graphs",
    "abstract": " Title: Adaptive Cooperative Tracking and Parameter Estimation of an Uncertain  Leader over General Directed Graphs ",
    "url": "https://arxiv.org/abs/2003.10622",
    "authors": [
      "Shimin Wang",
      "Hongwei Zhang",
      "Zhiyong Chen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2006.05161",
    "title": "Provable tradeoffs in adversarially robust classification",
    "abstract": " Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. 47 pages, 5 figures ",
    "url": "https://arxiv.org/abs/2006.05161",
    "authors": [
      "Edgar Dobriban",
      "Hamed Hassani",
      "David Hong",
      "Alexander Robey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2006.15221",
    "title": "Semi-discrete optimization through semi-discrete optimal transport: a  framework for neural architecture search",
    "abstract": " Title: Semi-discrete optimization through semi-discrete optimal transport: a  framework for neural architecture search ",
    "url": "https://arxiv.org/abs/2006.15221",
    "authors": [
      "Nicolas Garcia Trillos",
      "Javier Morales"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2008.07381",
    "title": "Dissecting liabilities in adversarial surgical robot failures: A  national (Danish) and European law perspective",
    "abstract": " Comments: 43 pages, 3 figures, forthcoming in CLSR ",
    "url": "https://arxiv.org/abs/2008.07381",
    "authors": [
      "Kaspar Rosager Ludvigsen",
      "Shishir Nagaraja"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Cryptography and Security (cs.CR)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2010.01792",
    "title": "Can we Generalize and Distribute Private Representation Learning?",
    "abstract": " Comments: In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022 ",
    "url": "https://arxiv.org/abs/2010.01792",
    "authors": [
      "Sheikh Shams Azam",
      "Taejin Kim",
      "Seyyedali Hosseinalipour",
      "Carlee Joe-Wong",
      "Saurabh Bagchi",
      "Christopher Brinton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multiagent Systems (cs.MA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2012.06695",
    "title": "Generating Adversarial Disturbances for Controller Verification",
    "abstract": " Title: Generating Adversarial Disturbances for Controller Verification ",
    "url": "https://arxiv.org/abs/2012.06695",
    "authors": [
      "Udaya Ghai",
      "David Snyder",
      "Anirudha Majumdar",
      "Elad Hazan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2102.06559",
    "title": "Infinitely Deep Bayesian Neural Networks with Stochastic Differential  Equations",
    "abstract": " Title: Infinitely Deep Bayesian Neural Networks with Stochastic Differential  Equations ",
    "url": "https://arxiv.org/abs/2102.06559",
    "authors": [
      "Winnie Xu",
      "Ricky T.Q. Chen",
      "Xuechen Li",
      "David Duvenaud"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2103.04046",
    "title": "Simplicial Complex Representation Learning",
    "abstract": " Comments: MACHINE LEARNING ON GRAPHS, MLoG Workshop at WSDM'22 ",
    "url": "https://arxiv.org/abs/2103.04046",
    "authors": [
      "Mustafa Hajij",
      "Ghada Zamzmi",
      "Theodore Papamarkou",
      "Vasileios Maroulas",
      "Xuanting Cai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Geometry (cs.CG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Algebraic Topology (math.AT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2103.07972",
    "title": "Open-independent, open-locating-dominating sets: structural aspects of  some classes of graphs",
    "abstract": " Comments: 18 pages, 5 figures ",
    "url": "https://arxiv.org/abs/2103.07972",
    "authors": [
      "M\u00e1rcia R. Cappelle",
      "Erika Coelho",
      "Les R. Foulds",
      "Humberto J. Longo"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2103.17235",
    "title": "FANet: A Feedback Attention Network for Improved Biomedical Image  Segmentation",
    "abstract": " Title: FANet: A Feedback Attention Network for Improved Biomedical Image  Segmentation ",
    "url": "https://arxiv.org/abs/2103.17235",
    "authors": [
      "Nikhil Kumar Tomar",
      "Debesh Jha",
      "Michael A. Riegler",
      "H\u00e5vard D. Johansen",
      "Dag Johansen",
      "Jens Rittscher",
      "P\u00e5l Halvorsen",
      "Sharib Ali"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2104.07631",
    "title": "Fair and Reliable Reconnections for Temporary Disruptions in Electric  Distribution Networks using Submodularity",
    "abstract": " Comments: 44 pages, 9 figures ",
    "url": "https://arxiv.org/abs/2104.07631",
    "authors": [
      "Cyrus Hettle",
      "Swati Gupta",
      "Daniel Molzahn"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2105.07451",
    "title": "MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image  Segmentation",
    "abstract": " Title: MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image  Segmentation ",
    "url": "https://arxiv.org/abs/2105.07451",
    "authors": [
      "Abhishek Srivastava",
      "Debesh Jha",
      "Sukalpa Chanda",
      "Umapada Pal",
      "H\u00e5vard D. Johansen",
      "Dag Johansen",
      "Michael A. Riegler",
      "Sharib Ali",
      "P\u00e5l Halvorsen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2105.08590",
    "title": "UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion  Model with Ensemble Monte Carlo Dropout for COVID-19 Detection",
    "abstract": " Comments: 16 pages, 18 figures ",
    "url": "https://arxiv.org/abs/2105.08590",
    "authors": [
      "Moloud Abdar",
      "Soorena Salari",
      "Sina Qahremani",
      "Hak-Keung Lam",
      "Fakhri Karray",
      "Sadiq Hussain",
      "Abbas Khosravi",
      "U. Rajendra Acharya",
      "Vladimir Makarenkov",
      "Saeid Nahavandi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2105.14491",
    "title": "How Attentive are Graph Attention Networks?",
    "abstract": " Comments: Published in ICLR 2022 ",
    "url": "https://arxiv.org/abs/2105.14491",
    "authors": [
      "Shaked Brody",
      "Uri Alon",
      "Eran Yahav"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2106.01981",
    "title": "ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse  Kinematics",
    "abstract": " Title: ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse  Kinematics ",
    "url": "https://arxiv.org/abs/2106.01981",
    "authors": [
      "Boris N. Oreshkin",
      "Florent Bocquelet",
      "F\u00e9lix G. Harvey",
      "Bay Raitt",
      "Dominic Laflamme"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2106.02483",
    "title": "You can't always get what you want: towards user-controlled privacy on  Android",
    "abstract": " Title: You can't always get what you want: towards user-controlled privacy on  Android ",
    "url": "https://arxiv.org/abs/2106.02483",
    "authors": [
      "Davide Caputo",
      "Francesco Pagano",
      "Giovanni Bottino",
      "Luca Verderame",
      "Alessio Merlo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2106.03087",
    "title": "Neural Implicit 3D Shapes from Single Images with Spatial Patterns",
    "abstract": " Comments: 8 pages, 7Mb ",
    "url": "https://arxiv.org/abs/2106.03087",
    "authors": [
      "Yixin Zhuang",
      "Yunzhe Liu",
      "Yujie Wang",
      "Baoquan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2106.04114",
    "title": "Theoretically Motivated Data Augmentation and Regularization for  Portfolio Construction",
    "abstract": " Comments: Preprint. Title and contents updated ",
    "url": "https://arxiv.org/abs/2106.04114",
    "authors": [
      "Liu Ziyin",
      "Kentaro Minami",
      "Kentaro Imajo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "General Finance (q-fin.GN)",
      "Portfolio Management (q-fin.PM)"
    ]
  },
  {
    "id": "arXiv:2106.06804",
    "title": "Entropy-based Logic Explanations of Neural Networks",
    "abstract": " Title: Entropy-based Logic Explanations of Neural Networks ",
    "url": "https://arxiv.org/abs/2106.06804",
    "authors": [
      "Pietro Barbiero",
      "Gabriele Ciravegna",
      "Francesco Giannini",
      "Pietro Li\u00f3",
      "Marco Gori",
      "Stefano Melacci"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2106.07830",
    "title": "On the Convergence and Calibration of Deep Learning with Differential  Privacy",
    "abstract": " Title: On the Convergence and Calibration of Deep Learning with Differential  Privacy ",
    "url": "https://arxiv.org/abs/2106.07830",
    "authors": [
      "Zhiqi Bu",
      "Hua Wang",
      "Qi Long",
      "Weijie J. Su"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2106.08609",
    "title": "Reinforcement learning for pursuit and evasion of microswimmers at low  Reynolds number",
    "abstract": " Comments: 20 pages, 5 figures (Supplementary Material in ancillary directory) ",
    "url": "https://arxiv.org/abs/2106.08609",
    "authors": [
      "Francesco Borra",
      "Luca Biferale",
      "Massimo Cencini",
      "Antonio Celani"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Chaotic Dynamics (nlin.CD)"
    ]
  },
  {
    "id": "arXiv:2106.09762",
    "title": "Causal Bias Quantification for Continuous Treatments",
    "abstract": " Title: Causal Bias Quantification for Continuous Treatments ",
    "url": "https://arxiv.org/abs/2106.09762",
    "authors": [
      "Gianluca Detommaso",
      "Michael Br\u00fcckner",
      "Philip Schulz",
      "Victor Chernozhukov"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2106.10605",
    "title": "Global and Local Contrastive Self-Supervised Learning for Semantic  Segmentation of HR Remote Sensing Images",
    "abstract": " Comments: 14 pages, 13 figures, 4 tables ",
    "url": "https://arxiv.org/abs/2106.10605",
    "authors": [
      "Haifeng Li",
      "Yi Li",
      "Guo Zhang",
      "Ruoyun Liu",
      "Haozhe Huang",
      "Qing Zhu",
      "Chao Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2106.12575",
    "title": "Weisfeiler and Lehman Go Cellular: CW Networks",
    "abstract": " Comments: NeurIPS 2021. Contains 28 pages, 9 figures ",
    "url": "https://arxiv.org/abs/2106.12575",
    "authors": [
      "Cristian Bodnar",
      "Fabrizio Frasca",
      "Nina Otter",
      "Yu Guang Wang",
      "Pietro Li\u00f2",
      "Guido Mont\u00fafar",
      "Michael Bronstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2107.03313",
    "title": "A Leap among Quantum Computing and Quantum Neural Networks: A Survey",
    "abstract": " Title: A Leap among Quantum Computing and Quantum Neural Networks: A Survey ",
    "url": "https://arxiv.org/abs/2107.03313",
    "authors": [
      "Fabio Valerio Massoli",
      "Lucia Vadicamo",
      "Giuseppe Amato",
      "Fabrizio Falchi"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2107.04276",
    "title": "Secure Consensus via Objective Coding: Robustness Analysis to Channel  Tampering",
    "abstract": " Comments: 12 pages, 5 figures, submitted to IEEE Transactions on Systems, Man and Cybernetics: Systems ",
    "url": "https://arxiv.org/abs/2107.04276",
    "authors": [
      "Marco Fabris",
      "Daniel Zelazo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2107.07511",
    "title": "A Gentle Introduction to Conformal Prediction and Distribution-Free  Uncertainty Quantification",
    "abstract": " Comments: Blog and tutorial video this http URL ",
    "url": "https://arxiv.org/abs/2107.07511",
    "authors": [
      "Anastasios N. Angelopoulos",
      "Stephen Bates"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2107.09370",
    "title": "An Embedding of ReLU Networks and an Analysis of their Identifiability",
    "abstract": " Comments: Constructive Approximation camera-ready ",
    "url": "https://arxiv.org/abs/2107.09370",
    "authors": [
      "Pierre Stock",
      "R\u00e9mi Gribonval"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.12780",
    "title": "Physics-constrained Deep Learning for Robust Inverse ECG Modeling",
    "abstract": " Title: Physics-constrained Deep Learning for Robust Inverse ECG Modeling ",
    "url": "https://arxiv.org/abs/2107.12780",
    "authors": [
      "Jianxin Xie",
      "Bing Yao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.13945",
    "title": "Effects of homophily and heterophily on preferred-degree networks:  mean-field analysis and overwhelming transition",
    "abstract": " Comments: 24 pages, 10 figures ",
    "url": "https://arxiv.org/abs/2107.13945",
    "authors": [
      "Xiang Li",
      "Mauro Mobilia",
      "Alastair M. Rucklidge",
      "R.K.P. Zia"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Social and Information Networks (cs.SI)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ]
  },
  {
    "id": "arXiv:2108.00351",
    "title": "LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic  Occlusion-Aware Data and Neural Mesh Rendering",
    "abstract": " Title: LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic  Occlusion-Aware Data and Neural Mesh Rendering ",
    "url": "https://arxiv.org/abs/2108.00351",
    "authors": [
      "Kaibing Yang",
      "Renshu Gu",
      "Maoyu Wang",
      "Masahiro Toyoura",
      "Gang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2108.02642",
    "title": "Robust interior penalty discontinuous Galerkin methods",
    "abstract": " Title: Robust interior penalty discontinuous Galerkin methods ",
    "url": "https://arxiv.org/abs/2108.02642",
    "authors": [
      "Zhaonan Dong",
      "Emmanuil H. Georgoulis"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2108.03039",
    "title": "Identifiable Energy-based Representations: An Application to Estimating  Heterogeneous Causal Effects",
    "abstract": " Comments: 20 pages, 2 figures, 9 tables ",
    "url": "https://arxiv.org/abs/2108.03039",
    "authors": [
      "Yao Zhang",
      "Jeroen Berrevoets",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2108.03702",
    "title": "BIGRoC: Boosting Image Generation via a Robust Classifier",
    "abstract": " Title: BIGRoC: Boosting Image Generation via a Robust Classifier ",
    "url": "https://arxiv.org/abs/2108.03702",
    "authors": [
      "Roy Ganz",
      "Michael Elad"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2108.04424",
    "title": "FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for  Blind Face Inpainting",
    "abstract": " Title: FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for  Blind Face Inpainting ",
    "url": "https://arxiv.org/abs/2108.04424",
    "authors": [
      "Junke Wang",
      "Shaoxiang Chen",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2108.06547",
    "title": "Spectral Detection of Simplicial Communities via Hodge Laplacians",
    "abstract": " Comments: 18 pages, 8 figures ",
    "url": "https://arxiv.org/abs/2108.06547",
    "authors": [
      "Sanjukta Krishnagopal",
      "Ginestra Bianconi"
    ],
    "subjectives": [
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Social and Information Networks (cs.SI)",
      "Adaptation and Self-Organizing Systems (nlin.AO)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2109.04150",
    "title": "Self-supervised Reinforcement Learning with Independently Controllable  Subgoals",
    "abstract": " Title: Self-supervised Reinforcement Learning with Independently Controllable  Subgoals ",
    "url": "https://arxiv.org/abs/2109.04150",
    "authors": [
      "Andrii Zadaianchuk",
      "Georg Martius",
      "Fanny Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2109.11308",
    "title": "Breaking BERT: Understanding its Vulnerabilities for Named Entity  Recognition through Adversarial Attack",
    "abstract": " Title: Breaking BERT: Understanding its Vulnerabilities for Named Entity  Recognition through Adversarial Attack ",
    "url": "https://arxiv.org/abs/2109.11308",
    "authors": [
      "Anne Dirkson",
      "Suzan Verberne",
      "Wessel Kraaij"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2109.12555",
    "title": "Self-loop Compensation in Signed Networks",
    "abstract": " Title: Self-loop Compensation in Signed Networks ",
    "url": "https://arxiv.org/abs/2109.12555",
    "authors": [
      "Haibin Shao",
      "Lulu Pan",
      "Dewei Li",
      "Yugeng Xi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2109.12556",
    "title": "Frequency Disentangled Residual Network",
    "abstract": " Title: Frequency Disentangled Residual Network ",
    "url": "https://arxiv.org/abs/2109.12556",
    "authors": [
      "Satya Rajendra Singh",
      "Roshan Reddy Yedla",
      "Shiv Ram Dubey",
      "Rakesh Sanodiya",
      "Wei-Ta Chu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2109.12888",
    "title": "Mixed Integer Neural Inverse Design",
    "abstract": " Title: Mixed Integer Neural Inverse Design ",
    "url": "https://arxiv.org/abs/2109.12888",
    "authors": [
      "Navid Ansari",
      "Hans-Peter Seidel",
      "Vahid Babaei"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.02865",
    "title": "Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural  Networks",
    "abstract": " Comments: Spotlight paper at ICLR 2022 ",
    "url": "https://arxiv.org/abs/2110.02865",
    "authors": [
      "Alan Jeffares",
      "Qinghai Guo",
      "Pontus Stenetorp",
      "Timoleon Moraitis"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2110.03608",
    "title": "How to Sense the World: Leveraging Hierarchy in Multimodal Perception  for Robust Reinforcement Learning Agents",
    "abstract": " Comments: Accepted at the International Conference on Autonomous Agents and MultiAgent Systems (AAMAS) 2022 ",
    "url": "https://arxiv.org/abs/2110.03608",
    "authors": [
      "Miguel Vasco",
      "Hang Yin",
      "Francisco S. Melo",
      "Ana Paiva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2110.04175",
    "title": "RelaySum for Decentralized Deep Learning on Heterogeneous Data",
    "abstract": " Comments: Presented at NeurIPS 2021 ",
    "url": "https://arxiv.org/abs/2110.04175",
    "authors": [
      "Thijs Vogels",
      "Lie He",
      "Anastasia Koloskova",
      "Tao Lin",
      "Sai Praneeth Karimireddy",
      "Sebastian U. Stich",
      "Martin Jaggi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2110.04743",
    "title": "ZARTS: On Zero-order Optimization for Neural Architecture Search",
    "abstract": " Title: ZARTS: On Zero-order Optimization for Neural Architecture Search ",
    "url": "https://arxiv.org/abs/2110.04743",
    "authors": [
      "Xiaoxing Wang",
      "Wenxuan Guo",
      "Junchi Yan",
      "Jianlin Su",
      "Xiaokang Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2110.04791",
    "title": "Stepwise-Refining Speech Separation Network via Fine-Grained Encoding in  High-order Latent Domain",
    "abstract": " Comments: Accepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP) ",
    "url": "https://arxiv.org/abs/2110.04791",
    "authors": [
      "Zengwei Yao",
      "Wenjie Pei",
      "Fanglin Chen",
      "Guangming Lu",
      "David Zhang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2110.05904",
    "title": "Video Is Graph: Structured Graph Module for Video Action Recognition",
    "abstract": " Comments: 9 pages ",
    "url": "https://arxiv.org/abs/2110.05904",
    "authors": [
      "Rongchang Li",
      "Xiao-Jun Wu",
      "Tianyang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2110.08105",
    "title": "Interpretable Neural Networks with Frank-Wolfe: Sparse Relevance Maps  and Relevance Orderings",
    "abstract": " Comments: 18 pages, 23 figures, 1 table ",
    "url": "https://arxiv.org/abs/2110.08105",
    "authors": [
      "Jan Macdonald",
      "Mathieu Besan\u00e7on",
      "Sebastian Pokutta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2110.09154",
    "title": "Measuring the influence of beliefs in belief networks",
    "abstract": " Comments: 14 pages, 4 figures. Earlier version of this work was presented at Networks 2021 conference. Second, shorter version of the paper ",
    "url": "https://arxiv.org/abs/2110.09154",
    "authors": [
      "Aleksandar Toma\u0161evi\u0107"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2110.11219",
    "title": "PlaneRecNet: Multi-Task Learning with Cross-Task Consistency for  Piece-Wise Plane Detection and Reconstruction from a Single RGB Image",
    "abstract": " Comments: accepted to BMVC 2021, code opensource: this https URL ",
    "url": "https://arxiv.org/abs/2110.11219",
    "authors": [
      "Yaxu Xie",
      "Fangwen Shu",
      "Jason Rambach",
      "Alain Pagani",
      "Didier Stricker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2111.00743",
    "title": "Towards the Generalization of Contrastive Self-Supervised Learning",
    "abstract": " Title: Towards the Generalization of Contrastive Self-Supervised Learning ",
    "url": "https://arxiv.org/abs/2111.00743",
    "authors": [
      "Weiran Huang",
      "Mingyang Yi",
      "Xuyang Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2111.05070",
    "title": "Almost Optimal Universal Lower Bound for Learning Causal DAGs with  Atomic Interventions",
    "abstract": " Comments: To appear in the proceedings of AISTATS 2022 ",
    "url": "https://arxiv.org/abs/2111.05070",
    "authors": [
      "Vibhor Porwal",
      "Piyush Srivastava",
      "Gaurav Sinha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2111.06236",
    "title": "Discovering and Explaining the Representation Bottleneck of DNNs",
    "abstract": " Title: Discovering and Explaining the Representation Bottleneck of DNNs ",
    "url": "https://arxiv.org/abs/2111.06236",
    "authors": [
      "Huiqi Deng",
      "Qihan Ren",
      "Hao Zhang",
      "Quanshi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2111.06346",
    "title": "Robust Moving Target Defence Against False Data Injection Attacks in  Power Grids",
    "abstract": " Title: Robust Moving Target Defence Against False Data Injection Attacks in  Power Grids ",
    "url": "https://arxiv.org/abs/2111.06346",
    "authors": [
      "Wangkun Xu",
      "Imad M. Jaimoukha",
      "Fei Teng"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2111.10095",
    "title": "An Index for Single Source All Destinations Distance Queries in Temporal  Graphs",
    "abstract": " Title: An Index for Single Source All Destinations Distance Queries in Temporal  Graphs ",
    "url": "https://arxiv.org/abs/2111.10095",
    "authors": [
      "Lutz Oettershagen",
      "Petra Mutzel"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2111.14625",
    "title": "Cyclic Graph Attentive Match Encoder (CGAME): A Novel Neural Network For  OD Estimation",
    "abstract": " Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible ",
    "url": "https://arxiv.org/abs/2111.14625",
    "authors": [
      "Guanzhou Li",
      "Yujing He",
      "Jianping Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2111.15160",
    "title": "Mitigating Adversarial Attacks by Distributing Different Copies to  Different Users",
    "abstract": " Title: Mitigating Adversarial Attacks by Distributing Different Copies to  Different Users ",
    "url": "https://arxiv.org/abs/2111.15160",
    "authors": [
      "Jiyi Zhang",
      "Wesley Joon-Wie Tann",
      "Ee-Chien Chang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2112.01197",
    "title": "Sample Prior Guided Robust Model Learning to Suppress Noisy Labels",
    "abstract": " Title: Sample Prior Guided Robust Model Learning to Suppress Noisy Labels ",
    "url": "https://arxiv.org/abs/2112.01197",
    "authors": [
      "Wenkai Chen",
      "Chuang Zhu",
      "Yi Chen",
      "Mengting Li",
      "Tiejun Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2112.04812",
    "title": "Deep Visual Constraints: Neural Implicit Models for Manipulation  Planning from Visual Input",
    "abstract": " Title: Deep Visual Constraints: Neural Implicit Models for Manipulation  Planning from Visual Input ",
    "url": "https://arxiv.org/abs/2112.04812",
    "authors": [
      "Jung-Su Ha",
      "Danny Driess",
      "Marc Toussaint"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2112.05464",
    "title": "Applying the Shuffle Model of Differential Privacy to Vector Aggregation",
    "abstract": " Comments: 17 pages, 3 figures, in: British International Conference on Databases (BICOD21), London, UK, 28 Mar 2022 ",
    "url": "https://arxiv.org/abs/2112.05464",
    "authors": [
      "Mary Scott",
      "Graham Cormode",
      "Carsten Maple"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2112.05999",
    "title": "Curvature-guided dynamic scale networks for Multi-view Stereo",
    "abstract": " Title: Curvature-guided dynamic scale networks for Multi-view Stereo ",
    "url": "https://arxiv.org/abs/2112.05999",
    "authors": [
      "Khang Truong Giang",
      "Soohwan Song",
      "Sungho Jo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2112.07054",
    "title": "Graph network for learning bi-directional physics",
    "abstract": " Title: Graph network for learning bi-directional physics ",
    "url": "https://arxiv.org/abs/2112.07054",
    "authors": [
      "Sakthi Kumar Arul Prakash",
      "Conrad Tucker"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2112.13078",
    "title": "Learning Bi-typed Multi-relational Heterogeneous Graph via Dual  Hierarchical Attention Networks",
    "abstract": " Comments: 11 pages, 8 figures and 4 tables ",
    "url": "https://arxiv.org/abs/2112.13078",
    "authors": [
      "Yu Zhao",
      "Shaopeng Wei",
      "Huaming Du",
      "Xingyan Chen",
      "Qing Li",
      "Fuzhen Zhuang",
      "Ji Liu",
      "Gang Kou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2112.14834",
    "title": "Training Quantized Deep Neural Networks via Cooperative Coevolution",
    "abstract": " Title: Training Quantized Deep Neural Networks via Cooperative Coevolution ",
    "url": "https://arxiv.org/abs/2112.14834",
    "authors": [
      "Fu Peng",
      "Shengcai Liu",
      "Ning Lu",
      "Ke Tang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.00299",
    "title": "Improving Out-of-Distribution Robustness via Selective Augmentation",
    "abstract": " Title: Improving Out-of-Distribution Robustness via Selective Augmentation ",
    "url": "https://arxiv.org/abs/2201.00299",
    "authors": [
      "Huaxiu Yao",
      "Yu Wang",
      "Sai Li",
      "Linjun Zhang",
      "Weixin Liang",
      "James Zou",
      "Chelsea Finn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.02944",
    "title": "Adaptive Performance Anomaly Detection for Online Service Systems via  Pattern Sketching",
    "abstract": " Comments: Accepted by The 44th International Conference on Software Engineering (ICSE 2022) ",
    "url": "https://arxiv.org/abs/2201.02944",
    "authors": [
      "Zhuangbin Chen",
      "Jinyang Liu",
      "Yuxin Su",
      "Hongyu Zhang",
      "Xiao Ling",
      "Yongqiang Yang",
      "Michael R. Lyu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2201.04066",
    "title": "VGAER: graph neural network reconstruction based community detection",
    "abstract": " Comments: Accepted by AAAI-22: DLG-AAAI'22 (this https URL) ",
    "url": "https://arxiv.org/abs/2201.04066",
    "authors": [
      "Chenyang Qiu",
      "Zhaoci Huang",
      "Wenzhe Xu",
      "Huijia Li"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.05610",
    "title": "When less is more: Simplifying inputs aids neural network understanding",
    "abstract": " Title: When less is more: Simplifying inputs aids neural network understanding ",
    "url": "https://arxiv.org/abs/2201.05610",
    "authors": [
      "Robin Tibor Schirrmeister",
      "Rosanne Liu",
      "Sara Hooker",
      "Tonio Ball"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.05912",
    "title": "Common Phone: A Multilingual Dataset for Robust Acoustic Modelling",
    "abstract": " Comments: Pre-print submitted to LREC 2022 Link to Common Phone: this https URL ",
    "url": "https://arxiv.org/abs/2201.05912",
    "authors": [
      "Philipp Klumpp",
      "Tom\u00e1s Arias-Vergara",
      "Paula Andrea P\u00e9rez-Toro",
      "Elmar N\u00f6th",
      "Juan Rafael Orozco-Arroyave"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2201.08802",
    "title": "Deconfounding to Explanation Evaluation in Graph Neural Networks",
    "abstract": " Comments: 18 pages ",
    "url": "https://arxiv.org/abs/2201.08802",
    "authors": [
      "Ying-Xin",
      "Xiang Wang",
      "An Zhang",
      "Xia Hu",
      "Fuli Feng",
      "Xiangnan He",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.08951",
    "title": "Visual Representation Learning with Self-Supervised Attention for  Low-Label High-data Regime",
    "abstract": " Comments: Accepted to ICASSP-2022 ",
    "url": "https://arxiv.org/abs/2201.08951",
    "authors": [
      "Prarthana Bhattacharyya",
      "Chenge Li",
      "Xiaonan Zhao",
      "Istv\u00e1n Feh\u00e9rv\u00e1ri",
      "Jason Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.10110",
    "title": "A Hybrid Quantum-Classical Algorithm for Robust Fitting",
    "abstract": " Title: A Hybrid Quantum-Classical Algorithm for Robust Fitting ",
    "url": "https://arxiv.org/abs/2201.10110",
    "authors": [
      "Anh-Dzung Doan",
      "Michele Sasdelli",
      "David Suter",
      "Tat-Jun Chin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.10145",
    "title": "MSNet: A Deep Multi-scale Submanifold Network for Visual Classification",
    "abstract": " Title: MSNet: A Deep Multi-scale Submanifold Network for Visual Classification ",
    "url": "https://arxiv.org/abs/2201.10145",
    "authors": [
      "Ziheng Chen",
      "Xiao-Jun Wu",
      "Tianyang Xu",
      "Rui Wang",
      "Zhiwu Huang",
      "Josef Kittler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.10207",
    "title": "SPIRAL: Self-supervised Perturbation-Invariant Representation Learning  for Speech Pre-Training",
    "abstract": " Comments: ICLR 2022 ",
    "url": "https://arxiv.org/abs/2201.10207",
    "authors": [
      "Wenyong Huang",
      "Zhenhe Zhang",
      "Yu Ting Yeung",
      "Xin Jiang",
      "Qun Liu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2201.10361",
    "title": "Reinforcement Learning-Based Deadline and Battery-Aware Offloading in  Smart Farm IoT-UAV Networks",
    "abstract": " Comments: Accepted Paper. Please check footnote in Page 1 for copyright ",
    "url": "https://arxiv.org/abs/2201.10361",
    "authors": [
      "Anne Catherine Nguyen",
      "Turgay Pamuklu",
      "Aisha Syed",
      "W. Sean Kennedy",
      "Melike Erol-Kantarci"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2201.10485",
    "title": "Concurrent NetKAT: Modeling and analyzing stateful, concurrent networks",
    "abstract": " Title: Concurrent NetKAT: Modeling and analyzing stateful, concurrent networks ",
    "url": "https://arxiv.org/abs/2201.10485",
    "authors": [
      "Jana Wagemaker",
      "Nate Foster",
      "Tobias Kapp\u00e9",
      "Dexter Kozen",
      "Jurriaan Rot",
      "Alexandra Silva"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2201.10751",
    "title": "Graph Neural Networks with Dynamic and Static Representations for Social  Recommendation",
    "abstract": " Comments: 17 pages, 4 figures. Extended version of paper accepted by DASFAA 2022 ",
    "url": "https://arxiv.org/abs/2201.10751",
    "authors": [
      "Junfa Lin",
      "Siyuan Chen",
      "Jiahai Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2201.11528",
    "title": "Beyond ImageNet Attack: Towards Crafting Adversarial Examples for  Black-box Domains",
    "abstract": " Comments: Accepted by ICLR 2022 ",
    "url": "https://arxiv.org/abs/2201.11528",
    "authors": [
      "Qilong Zhang",
      "Xiaodan Li",
      "Yuefeng Chen",
      "Jingkuan Song",
      "Lianli Gao",
      "Yuan He",
      "Hui Xue"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.11727",
    "title": "Multi-Agent Reinforcement Learning for Network Load Balancing in Data  Center",
    "abstract": " Title: Multi-Agent Reinforcement Learning for Network Load Balancing in Data  Center ",
    "url": "https://arxiv.org/abs/2201.11727",
    "authors": [
      "Zhiyuan Yao",
      "Zihan Ding",
      "Thomas Clausen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2201.11742",
    "title": "A neural net architecture based on principles of neural plasticity and  development evolves to effectively catch prey in a simulated environment",
    "abstract": " Comments: 7 pages, 4 figures, 1 appendix page ",
    "url": "https://arxiv.org/abs/2201.11742",
    "authors": [
      "Addison Wood",
      "Jory Schossau",
      "Nick Sabaj",
      "Richard Liu",
      "Mark Reimers"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2201.11795",
    "title": "Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG  Encoder-Decoder",
    "abstract": " Comments: Accepted in DCC 2022, 11 pages ",
    "url": "https://arxiv.org/abs/2201.11795",
    "authors": [
      "Ankur Mali",
      "Alexander Ororbia",
      "Daniel Kifer",
      "Lee Giles"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2201.12006",
    "title": "Provably Improving Expert Predictions with Conformal Prediction",
    "abstract": " Title: Provably Improving Expert Predictions with Conformal Prediction ",
    "url": "https://arxiv.org/abs/2201.12006",
    "authors": [
      "Eleni Straitouri",
      "Lequn Wang",
      "Nastaran Okati",
      "Manuel Gomez Rodriguez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (stat.ML)"
    ]
  }
]