[
  {
    "id": "arXiv:2202.08867",
    "title": "Fast online inference for nonlinear contextual bandit based on  Generative Adversarial Network",
    "abstract": "This work addresses the efficiency concern on inferring a nonlinear contextual bandit when the number of arms $n$ is very large. We propose a neural bandit model with an end-to-end training process to efficiently perform bandit algorithms such as Thompson Sampling and UCB during inference. We advance state-of-the-art time complexity to $O(\\log n)$ with approximate Bayesian inference, neural random feature mapping, approximate global maxima and approximate nearest neighbor search. We further propose a generative adversarial network to shift the bottleneck of maximizing the objective for selecting optimal arms from inference time to training time, enjoying significant speedup with additional advantage of enabling batch and parallel processing. %The generative model can inference an approximate argmax of the posterior sampling in logarithmic time complexity with the help of approximate nearest neighbor search. Extensive experiments on classification and recommendation tasks demonstrate order-of-magnitude improvement in inference time no significant degradation on the performance. ",
    "url": "https://arxiv.org/abs/2202.08867",
    "authors": [
      "Yun Da Tsai",
      "Shou De Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.08870",
    "title": "An Optimal Algorithm for Product Structure in Planar Graphs",
    "abstract": "The \\emph{Product Structure Theorem} for planar graphs (Dujmovi\\'c et al.\\ \\emph{JACM}, \\textbf{67}(4):22) states that any planar graph is contained in the strong product of a planar $3$-tree, a path, and a $3$-cycle. We give a simple linear-time algorithm for finding this decomposition as well as several related decompositions. This improves on the previous $O(n\\log n)$ time algorithm (Morin.\\ \\emph{Algorithmica}, \\textbf{85}(5):1544--1558). ",
    "url": "https://arxiv.org/abs/2202.08870",
    "authors": [
      "Prosenjit Bose",
      "Pat Morin",
      "Saeed Odak"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2202.08871",
    "title": "Graph Data Augmentation for Graph Machine Learning: A Survey",
    "abstract": "Data augmentation has recently seen increased interest in graph machine learning given its ability of creating extra training data and improving model generalization. Despite this recent upsurge, this area is still relatively underexplored, due to the challenges brought by complex, non-Euclidean structure of graph data, which limits the direct analogizing of traditional augmentation operations on other types of data. In this paper, we present a comprehensive and systematic survey of graph data augmentation that summarizes the literature in a structured manner. We first categorize graph data augmentation operations based on the components of graph data they modify or create. Next, we introduce recent advances in graph data augmentation, separating by their learning objectives and methodologies. We conclude by outlining currently unsolved challenges as well as directions for future research. Overall, this paper aims to clarify the landscape of existing literature in graph data augmentation and motivate additional work in this area. We provide a GitHub repository (https://github.com/zhao-tong/graph-data-augmentation-papers) with a reading list that will be continuously updated. ",
    "url": "https://arxiv.org/abs/2202.08871",
    "authors": [
      "Tong Zhao",
      "Gang Liu",
      "Stephan G\u00fcnnemann",
      "Meng Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.08882",
    "title": "Improving English to Sinhala Neural Machine Translation using  Part-of-Speech Tag",
    "abstract": "The performance of Neural Machine Translation (NMT) depends significantly on the size of the available parallel corpus. Due to this fact, low resource language pairs demonstrate low translation performance compared to high resource language pairs. The translation quality further degrades when NMT is performed for morphologically rich languages. Even though the web contains a large amount of information, most people in Sri Lanka are unable to read and understand English properly. Therefore, there is a huge requirement of translating English content to local languages to share information among locals. Sinhala language is the primary language in Sri Lanka and building an NMT system that can produce quality English to Sinhala translations is difficult due to the syntactic divergence between these two languages under low resource constraints. Thus, in this research, we explore effective methods of incorporating Part of Speech (POS) tags to the Transformer input embedding and positional encoding to further enhance the performance of the baseline English to Sinhala neural machine translation model. ",
    "url": "https://arxiv.org/abs/2202.08882",
    "authors": [
      "Ravinga Perera",
      "Thilakshi Fonseka",
      "Rashmini Naranpanawa",
      "Uthayasanker Thayasivam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.08892",
    "title": "Developing Imperceptible Adversarial Patches to Camouflage Military  Assets From Computer Vision Enabled Technologies",
    "abstract": "Convolutional neural networks (CNNs) have demonstrated rapid progress and a high level of success in object detection. However, recent evidence has highlighted their vulnerability to adversarial attacks. These attacks are calculated image perturbations or adversarial patches that result in object misclassification or detection suppression. Traditional camouflage methods are impractical when applied to disguise aircraft and other large mobile assets from autonomous detection in intelligence, surveillance and reconnaissance technologies and fifth generation missiles. In this paper we present a unique method that produces imperceptible patches capable of camouflaging large military assets from computer vision-enabled technologies. We developed these patches by maximising object detection loss whilst limiting the patch's colour perceptibility. This work also aims to further the understanding of adversarial examples and their effects on object detection algorithms. ",
    "url": "https://arxiv.org/abs/2202.08892",
    "authors": [
      "Christopher Wise",
      "Jo Plested"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2202.08896",
    "title": "Computing list homomorphisms in geometric intersection graphs",
    "abstract": "A homomorphism from a graph $G$ to a graph $H$ is an edge-preserving mapping from $V(G)$ to $V(H)$. Let $H$ be a fixed graph with possible loops. In the list homomorphism problem, denoted by \\textsc{LHom}($H$), the instance is a graph $G$, whose every vertex is equipped with a subset of $V(H)$, called list. We ask whether there exists a homomorphism from $G$ to $H$, such that every vertex from $G$ is mapped to a vertex from its list. We study the complexity of the \\textsc{LHom}($H$) problem in intersection graphs of various geometric objects. In particular, we are interested in answering the question for what graphs $H$ and for what types of geometric objects, the \\textsc{LHom}($H$) problem can be solved in time subexponential in the number of vertices of the instance. We fully resolve this question for string graphs, i.e., intersection graphs of continuous curves in the plane. Quite surprisingly, it turns out that the dichotomy exactly coincides with the analogous dichotomy for graphs excluding a fixed path as an induced subgraph [Okrasa, Rz\\k{a}\\.zewski, STACS 2021]. Then we turn our attention to subclasses of string graphs, defined as intersections of fat objects. We observe that the (non)existence of subexponential-time algorithms in such classes is closely related to the size $\\mathrm{mrc}(H)$ of a maximum reflexive clique in $H$, i.e., maximum number of pairwise adjacent vertices, each of which has a loop. We study the maximum value of $\\mathrm{mrc}(H)$ that guarantees the existence of a subexponential-time algorithm for \\textsc{LHom}($H$) in intersection graphs of (i) convex fat objects, (ii) fat similarly-sized objects, and (iii) disks. In the first two cases we obtain optimal results, by giving matching algorithms and lower bounds. Finally, we discuss possible extensions of our results to weighted generalizations of \\textsc{LHom}($H$). ",
    "url": "https://arxiv.org/abs/2202.08896",
    "authors": [
      "S\u00e1ndor Kisfaludi-Bak",
      "Karolina Okrasa",
      "Pawe\u0142 Rz\u0105\u017cewski"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2202.08897",
    "title": "Implementing Spiking Neural Networks on Neuromorphic Architectures: A  Review",
    "abstract": "Recently, both industry and academia have proposed several different neuromorphic systems to execute machine learning applications that are designed using Spiking Neural Networks (SNNs). With the growing complexity on design and technology fronts, programming such systems to admit and execute a machine learning application is becoming increasingly challenging. Additionally, neuromorphic systems are required to guarantee real-time performance, consume lower energy, and provide tolerance to logic and memory failures. Consequently, there is a clear need for system software frameworks that can implement machine learning applications on current and emerging neuromorphic systems, and simultaneously address performance, energy, and reliability. Here, we provide a comprehensive overview of such frameworks proposed for both, platform-based design and hardware-software co-design. We highlight challenges and opportunities that the future holds in the area of system software technology for neuromorphic computing. ",
    "url": "https://arxiv.org/abs/2202.08897",
    "authors": [
      "Phu Khanh Huynh",
      "M. Lakshmi Varshika",
      "Ankita Paul",
      "Murat Isik",
      "Adarsha Balaji",
      "Anup Das"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2202.08917",
    "title": "Discovering Fine-Grained Semantics in Knowledge Graph Relations",
    "abstract": "When it comes to comprehending and analyzing multi-relational data, the semantics of relations are crucial. Polysemous relations between different types of entities, that represent multiple semantics, are common in real-world relational datasets represented by knowledge graphs. For numerous use cases, such as entity type classification, question answering and knowledge graph completion, the correct semantic interpretation of these relations is necessary. In this work, we provide a strategy for discovering the different semantics associated with abstract relations and deriving many sub-relations with fine-grained meaning. To do this, we leverage the types of the entities associated with the relations and cluster the vector representations of entities and relations. The suggested method is able to automatically discover the best number of sub-relations for a polysemous relation and determine their semantic interpretation, according to our empirical evaluation. ",
    "url": "https://arxiv.org/abs/2202.08917",
    "authors": [
      "Nitisha Jain",
      "Ralf Krestel"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2202.08944",
    "title": "Rethinking Machine Learning Robustness via its Link with the  Out-of-Distribution Problem",
    "abstract": "Despite multiple efforts made towards robust machine learning (ML) models, their vulnerability to adversarial examples remains a challenging problem that calls for rethinking the defense strategy. In this paper, we take a step back and investigate the causes behind ML models' susceptibility to adversarial examples. In particular, we focus on exploring the cause-effect link between adversarial examples and the out-of-distribution (OOD) problem. To that end, we propose an OOD generalization method that stands against both adversary-induced and natural distribution shifts. Through an OOD to in-distribution mapping intuition, our approach translates OOD inputs to the data distribution used to train and test the model. Through extensive experiments on three benchmark image datasets of different scales (MNIST, CIFAR10, and ImageNet) and by leveraging image-to-image translation methods, we confirm that the adversarial examples problem is a special case of the wider OOD generalization problem. Across all datasets, we show that our translation-based approach consistently improves robustness to OOD adversarial inputs and outperforms state-of-the-art defenses by a significant margin, while preserving the exact accuracy on benign (in-distribution) data. Furthermore, our method generalizes on naturally OOD inputs such as darker or sharper images ",
    "url": "https://arxiv.org/abs/2202.08944",
    "authors": [
      "Abderrahmen Amich",
      "Birhanu Eshete"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2202.08959",
    "title": "Deep Interest Highlight Network for Click-Through RatePrediction in  Trigger-Induced Recommendation",
    "abstract": "In many classical e-commerce platforms, personalized recommendation has been proven to be of great business value, which can improve user satisfaction and increase the revenue of platforms. In this paper, we present a new recommendation problem, Trigger-Induced Recommendation (TIR), where users' instant interest can be explicitly induced with a trigger item and follow-up related target items are recommended accordingly. TIR has become ubiquitous and popular in e-commerce platforms. In this paper, we figure out that although existing recommendation models are effective in traditional recommendation scenarios by mining users' interests based on their massive historical behaviors, they are struggling in discovering users' instant interests in the TIR scenario due to the discrepancy between these scenarios, resulting in inferior performance. To tackle the problem, we propose a novel recommendation method named Deep Interest Highlight Network (DIHN) for Click-Through Rate (CTR) prediction in TIR scenarios. It has three main components including 1) User Intent Network (UIN), which responds to generate a precise probability score to predict user's intent on the trigger item; 2) Fusion Embedding Module (FEM), which adaptively fuses trigger item and target item embeddings based on the prediction from UIN; and (3) Hybrid Interest Extracting Module (HIEM), which can effectively highlight users' instant interest from their behaviors based on the result of FEM. Extensive offline and online evaluations on a real-world e-commerce platform demonstrate the superiority of DIHN over state-of-the-art methods. ",
    "url": "https://arxiv.org/abs/2202.08959",
    "authors": [
      "Qijie Shen",
      "Hong Wen",
      "Wanjie Tao",
      "Jing Zhang",
      "Fuyu Lv",
      "Zulong Chen",
      "Zhao Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.08975",
    "title": "Probing Pretrained Models of Source Code",
    "abstract": "Deep learning models are widely used for solving challenging code processing tasks, such as code generation or code summarization. Traditionally, a specific model architecture was carefully built to solve a particular code processing task. However, recently general pretrained models such as CodeBERT or CodeT5 have been shown to outperform task-specific models in many applications. While pretrained models are known to learn complex patterns from data, they may fail to understand some properties of source code. To test diverse aspects of code understanding, we introduce a set of diagnosting probing tasks. We show that pretrained models of code indeed contain information about code syntactic structure and correctness, the notions of identifiers, data flow and namespaces, and natural language naming. We also investigate how probing results are affected by using code-specific pretraining objectives, varying the model size, or finetuning. ",
    "url": "https://arxiv.org/abs/2202.08975",
    "authors": [
      "Sergey Troshin",
      "Nadezhda Chirkova"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.08982",
    "title": "PGCN: Progressive Graph Convolutional Networks for Spatial-Temporal  Traffic Forecasting",
    "abstract": "The complex spatial-temporal correlations in transportation networks make the traffic forecasting problem challenging. Since transportation system inherently possesses graph structures, much research efforts have been put with graph neural networks. Recently, constructing adaptive graphs to the data has shown promising results over the models relying on a single static graph structure. However, the graph adaptations are applied during the training phases, and do not reflect the data used during the testing phases. Such shortcomings can be problematic especially in traffic forecasting since the traffic data often suffers from the unexpected changes and irregularities in the time series. In this study, we propose a novel traffic forecasting framework called Progressive Graph Convolutional Network (PGCN). PGCN constructs a set of graphs by progressively adapting to input data during the training and the testing phases. Specifically, we implemented the model to construct progressive adjacency matrices by learning trend similarities among graph nodes. Then, the model is combined with the dilated causal convolution and gated activation unit to extract temporal features. With residual and skip connections, PGCN performs the traffic prediction. When applied to four real-world traffic datasets of diverse geometric nature, the proposed model achieves state-of-the-art performance with consistency in all datasets. We conclude that the ability of PGCN to progressively adapt to input data enables the model to generalize in different study sites with robustness. ",
    "url": "https://arxiv.org/abs/2202.08982",
    "authors": [
      "Yuyol Shin",
      "Yoonjin Yoon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.08985",
    "title": "Out of Distribution Data Detection Using Dropout Bayesian Neural  Networks",
    "abstract": "We explore the utility of information contained within a dropout based Bayesian neural network (BNN) for the task of detecting out of distribution (OOD) data. We first show how previous attempts to leverage the randomized embeddings induced by the intermediate layers of a dropout BNN can fail due to the distance metric used. We introduce an alternative approach to measuring embedding uncertainty, justify its use theoretically, and demonstrate how incorporating embedding uncertainty improves OOD data identification across three tasks: image classification, language classification, and malware detection. ",
    "url": "https://arxiv.org/abs/2202.08985",
    "authors": [
      "Andre T. Nguyen",
      "Fred Lu",
      "Gary Lopez Munoz",
      "Edward Raff",
      "Charles Nicholas",
      "James Holt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09006",
    "title": "KINet: Keypoint Interaction Networks for Unsupervised Forward Modeling",
    "abstract": "Object-centric representation is an essential abstraction for physical reasoning and forward prediction. Most existing approaches learn this representation through extensive supervision (e.g., object class and bounding box) although such ground-truth information is not readily accessible in reality. To address this, we introduce KINet (Keypoint Interaction Network) -- an end-to-end unsupervised framework to reason about object interactions in complex systems based on a keypoint representation. Using visual observations, our model learns to associate objects with keypoint coordinates and discovers a graph representation of the system as a set of keypoint embeddings and their relations. It then learns an action-conditioned forward model using contrastive estimation to predict future keypoint states. By learning to perform physical reasoning in the keypoint space, our model automatically generalizes to scenarios with a different number of objects, and novel object geometries. Experiments demonstrate the effectiveness of our model to accurately perform forward prediction and learn plannable object-centric representations which can also be used in downstream model-based control tasks. ",
    "url": "https://arxiv.org/abs/2202.09006",
    "authors": [
      "Alireza Rezazadeh",
      "Changhyun Choi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2202.09014",
    "title": "How Well Do Self-Supervised Methods Perform in Cross-Domain Few-Shot  Learning?",
    "abstract": "Cross-domain few-shot learning (CDFSL) remains a largely unsolved problem in the area of computer vision, while self-supervised learning presents a promising solution. Both learning methods attempt to alleviate the dependency of deep networks on the requirement of large-scale labeled data. Although self-supervised methods have recently advanced dramatically, their utility on CDFSL is relatively unexplored. In this paper, we investigate the role of self-supervised representation learning in the context of CDFSL via a thorough evaluation of existing methods. It comes as a surprise that even with shallow architectures or small training datasets, self-supervised methods can perform favorably compared to the existing SOTA methods. Nevertheless, no single self-supervised approach dominates all datasets indicating that existing self-supervised methods are not universally applicable. In addition, we find that representations extracted from self-supervised methods exhibit stronger robustness than the supervised method. Intriguingly, whether self-supervised representations perform well on the source domain has little correlation with their applicability on the target domain. As part of our study, we conduct an objective measurement of the performance for six kinds of representative classifiers. The results suggest Prototypical Classifier as the standard evaluation recipe for CDFSL. ",
    "url": "https://arxiv.org/abs/2202.09014",
    "authors": [
      "Yiyi Zhang",
      "Ying Zheng",
      "Xiaogang Xu",
      "Jun Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2202.09021",
    "title": "Effective Urban Region Representation Learning Using Heterogeneous Urban  Graph Attention Network (HUGAT)",
    "abstract": "Revealing the hidden patterns shaping the urban environment is essential to understand its dynamics and to make cities smarter. Recent studies have demonstrated that learning the representations of urban regions can be an effective strategy to uncover the intrinsic characteristics of urban areas. However, existing studies lack in incorporating diversity in urban data sources. In this work, we propose heterogeneous urban graph attention network (HUGAT), which incorporates heterogeneity of diverse urban datasets. In HUGAT, heterogeneous urban graph (HUG) incorporates both the geo-spatial and temporal people movement variations in a single graph structure. Given a HUG, a set of meta-paths are designed to capture the rich urban semantics as composite relations between nodes. Region embedding is carried out using heterogeneous graph attention network (HAN). HUGAT is designed to consider multiple learning objectives of city's geo-spatial and mobility variations simultaneously. In our extensive experiments on NYC data, HUGAT outperformed all the state-of-the-art models. Moreover, it demonstrated a robust generalization capability across the various prediction tasks of crime, average personal income, and bike flow as well as the spatial clustering task. ",
    "url": "https://arxiv.org/abs/2202.09021",
    "authors": [
      "Namwoo Kim",
      "Yoonjin Yoon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09025",
    "title": "Graph Auto-Encoder Via Neighborhood Wasserstein Reconstruction",
    "abstract": "Graph neural networks (GNNs) have drawn significant research attention recently, mostly under the setting of semi-supervised learning. When task-agnostic representations are preferred or supervision is simply unavailable, the auto-encoder framework comes in handy with a natural graph reconstruction objective for unsupervised GNN training. However, existing graph auto-encoders are designed to reconstruct the direct links, so GNNs trained in this way are only optimized towards proximity-oriented graph mining tasks, and will fall short when the topological structures matter. In this work, we revisit the graph encoding process of GNNs which essentially learns to encode the neighborhood information of each node into an embedding vector, and propose a novel graph decoder to reconstruct the entire neighborhood information regarding both proximity and structure via Neighborhood Wasserstein Reconstruction (NWR). Specifically, from the GNN embedding of each node, NWR jointly predicts its node degree and neighbor feature distribution, where the distribution prediction adopts an optimal-transport loss based on the Wasserstein distance. Extensive experiments on both synthetic and real-world network datasets show that the unsupervised node representations learned with NWR have much more advantageous in structure-oriented graph mining tasks, while also achieving competitive performance in proximity-oriented ones. ",
    "url": "https://arxiv.org/abs/2202.09025",
    "authors": [
      "Mingyue Tang",
      "Carl Yang",
      "Pan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2202.09027",
    "title": "Trusted AI in Multi-agent Systems: An Overview of Privacy and Security  for Distributed Learning",
    "abstract": "Motivated by the advancing computational capacity of distributed end-user equipments (UEs), as well as the increasing concerns about sharing private data, there has been considerable recent interest in machine learning (ML) and artificial intelligence (AI) that can be processed on on distributed UEs. Specifically, in this paradigm, parts of an ML process are outsourced to multiple distributed UEs, and then the processed ML information is aggregated on a certain level at a central server, which turns a centralized ML process into a distributed one, and brings about significant benefits. However, this new distributed ML paradigm raises new risks of privacy and security issues. In this paper, we provide a survey of the emerging security and privacy risks of distributed ML from a unique perspective of information exchange levels, which are defined according to the key steps of an ML process, i.e.: i) the level of preprocessed data, ii) the level of learning models, iii) the level of extracted knowledge and, iv) the level of intermediate results. We explore and analyze the potential of threats for each information exchange level based on an overview of the current state-of-the-art attack mechanisms, and then discuss the possible defense methods against such threats. Finally, we complete the survey by providing an outlook on the challenges and possible directions for future research in this critical area. ",
    "url": "https://arxiv.org/abs/2202.09027",
    "authors": [
      "Chuan Ma",
      "Jun Li. Kang Wei",
      "Bo Liu",
      "Ming Ding",
      "Long Yuan",
      "Zhu Han",
      "H. Vicent Poor"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2202.09028",
    "title": "A Note on the Implicit Bias Towards Minimal Depth of Deep Neural  Networks",
    "abstract": "Deep learning systems have steadily advanced the state of the art in a wide variety of benchmarks, demonstrating impressive performance in tasks ranging from image classification \\citep{taigman2014deepface,zhai2021scaling}, language processing \\citep{devlin-etal-2019-bert,NEURIPS2020_1457c0d6}, open-ended environments \\citep{SilverHuangEtAl16nature,arulkumaran2019alphastar}, to coding \\citep{chen2021evaluating}. A central aspect that enables the success of these systems is the ability to train deep models instead of wide shallow ones \\citep{7780459}. Intuitively, a neural network is decomposed into hierarchical representations from raw data to high-level, more abstract features. While training deep neural networks repetitively achieves superior performance against their shallow counterparts, an understanding of the role of depth in representation learning is still lacking. In this work, we suggest a new perspective on understanding the role of depth in deep learning. We hypothesize that {\\bf\\em SGD training of overparameterized neural networks exhibits an implicit bias that favors solutions of minimal effective depth}. Namely, SGD trains neural networks for which the top several layers are redundant. To evaluate the redundancy of layers, we revisit the recently discovered phenomenon of neural collapse \\citep{Papyan24652,han2021neural}. ",
    "url": "https://arxiv.org/abs/2202.09028",
    "authors": [
      "Tomer Galanti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09039",
    "title": "Critical Checkpoints for Evaluating Defence Models Against Adversarial  Attack and Robustness",
    "abstract": "From past couple of years there is a cycle of researchers proposing a defence model for adversaries in machine learning which is arguably defensible to most of the existing attacks in restricted condition (they evaluate on some bounded inputs or datasets). And then shortly another set of researcher finding the vulnerabilities in that defence model and breaking it by proposing a stronger attack model. Some common flaws are been noticed in the past defence models that were broken in very short time. Defence models being broken so easily is a point of concern as decision of many crucial activities are taken with the help of machine learning models. So there is an utter need of some defence checkpoints that any researcher should keep in mind while evaluating the soundness of technique and declaring it to be decent defence technique. In this paper, we have suggested few checkpoints that should be taken into consideration while building and evaluating the soundness of defence models. All these points are recommended after observing why some past defence models failed and how some model remained adamant and proved their soundness against some of the very strong attacks. ",
    "url": "https://arxiv.org/abs/2202.09039",
    "authors": [
      "Kanak Tekwani",
      "Manojkumar Parmar"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09044",
    "title": "Social Welfare Maximization in cross-silo Federated Learning",
    "abstract": "As one of the typical settings of Federated Learning (FL), cross-silo FL allows organizations to jointly train an optimal Machine Learning (ML) model. In this case, some organizations may try to obtain the global model without contributing their local training, lowering the social welfare. In this paper, we model the interactions among organizations in cross-silo FL as a public goods game for the first time and theoretically prove that there exists a social dilemma where the maximum social welfare is not achieved in Nash equilibrium. To overcome this social dilemma, we employ the Multi-player Multi-action Zero-Determinant (MMZD) strategy to maximize the social welfare. With the help of the MMZD, an individual organization can unilaterally control the social welfare without extra cost. Experimental results validate that the MMZD strategy is effective in maximizing the social welfare. ",
    "url": "https://arxiv.org/abs/2202.09044",
    "authors": [
      "Jianan Chen",
      "Qin Hu",
      "Honglu Jiang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2202.09048",
    "title": "Task Specific Attention is one more thing you need for object detection",
    "abstract": "Various models have been proposed to solve the object detection problem. However, most of them require many hand-designed components to demonstrate good performance. To mitigate these issues, Transformer based DETR and its variant Deformable DETR were suggested. They solved much of the complex issue of designing a head of object detection model but it has not been generally clear that the Transformer-based models could be considered as the state-of-the-art method in object detection without doubt. Furthermore, as DETR adapted Transformer method only for the detection head, but still with including CNN for the backbone body, it has not been certain that it would be possible to build the competent end-to-end pipeline with the combination of attention modules. In this paper, we propose that combining several attention modules with our new Task Specific Split Transformer(TSST) is a fairly good enough method to produce the best COCO results without traditionally hand-designed components. By splitting generally purposed attention module into two separated mission specific attention module, the proposed method addresses the way to design simpler object detection models than before. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/navervision/tsst ",
    "url": "https://arxiv.org/abs/2202.09048",
    "authors": [
      "Sang Yon Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2202.09096",
    "title": "A Free Lunch with Influence Functions? Improving Neural Network  Estimates with Concepts from Semiparametric Statistics",
    "abstract": "Parameter estimation in the empirical fields is usually undertaken using parametric models, and such models are convenient because they readily facilitate statistical inference. Unfortunately, they are unlikely to have a sufficiently flexible functional form to be able to adequately model real-world phenomena, and their usage may therefore result in biased estimates and invalid inference. Unfortunately, whilst non-parametric machine learning models may provide the needed flexibility to adapt to the complexity of real-world phenomena, they do not readily facilitate statistical inference, and may still exhibit residual bias. We explore the potential for semiparametric theory (in particular, the Influence Function) to be used to improve neural networks and machine learning algorithms in terms of (a) improving initial estimates without needing more data (b) increasing the robustness of our models, and (c) yielding confidence intervals for statistical inference. We propose a new neural network method MultiNet, which seeks the flexibility and diversity of an ensemble using a single architecture. Results on causal inference tasks indicate that MultiNet yields better performance than other approaches, and that all considered methods are amenable to improvement from semiparametric techniques under certain conditions. In other words, with these techniques we show that we can improve existing neural networks for `free', without needing more data, and without needing to retrain them. Finally, we provide the expression for deriving influence functions for estimands from a general graph, and the code to do so automatically. ",
    "url": "https://arxiv.org/abs/2202.09096",
    "authors": [
      "Matthew J. Vowels",
      "Sina Akbari",
      "Jalal Etesami",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2202.09097",
    "title": "Lightweight Multi-Drone Detection and 3D-Localization via YOLO",
    "abstract": "In this work, we present and evaluate a method to perform real-time multiple drone detection and three-dimensional localization using state-of-the-art tiny-YOLOv4 object detection algorithm and stereo triangulation. Our computer vision approach eliminates the need for computationally expensive stereo matching algorithms, thereby significantly reducing the memory footprint and making it deployable on embedded systems. Our drone detection system is highly modular (with support for various detection algorithms) and capable of identifying multiple drones in a system, with real-time detection accuracy of up to 77\\% with an average FPS of 332 (on Nvidia Titan Xp). We also test the complete pipeline in AirSim environment, detecting drones at a maximum distance of 8 meters, with a mean error of $23\\%$ of the distance. We also release the source code for the project, with pre-trained models and the curated synthetic stereo dataset. ",
    "url": "https://arxiv.org/abs/2202.09097",
    "authors": [
      "Aryan Sharma",
      "Nitik Jain",
      "Mangal Kothari"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2202.09115",
    "title": "Towards Simple and Accurate Human Pose Estimation with Stair Network",
    "abstract": "In this paper, we focus on tackling the precise keypoint coordinates regression task. Most existing approaches adopt complicated networks with a large number of parameters, leading to a heavy model with poor cost-effectiveness in practice. To overcome this limitation, we develop a small yet discrimicative model called STair Network, which can be simply stacked towards an accurate multi-stage pose estimation system. Specifically, to reduce computational cost, STair Network is composed of novel basic feature extraction blocks which focus on promoting feature diversity and obtaining rich local representations with fewer parameters, enabling a satisfactory balance on efficiency and performance. To further improve the performance, we introduce two mechanisms with negligible computational cost, focusing on feature fusion and replenish. We demonstrate the effectiveness of the STair Network on two standard datasets, e.g., 1-stage STair Network achieves a higher accuracy than HRNet by 5.5% on COCO test dataset with 80\\% fewer parameters and 68% fewer GFLOPs. ",
    "url": "https://arxiv.org/abs/2202.09115",
    "authors": [
      "Chenru Jiang",
      "Kaizhu Huang",
      "Shufei Zhang",
      "Shufei Zhang",
      "Jimin Xiao",
      "Zhenxing Niu",
      "Amir Hussain"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2202.09134",
    "title": "Quantifying the Effects of Data Augmentation",
    "abstract": "We provide results that exactly quantify how data augmentation affects the convergence rate and variance of estimates. They lead to some unexpected findings: Contrary to common intuition, data augmentation may increase rather than decrease uncertainty of estimates, such as the empirical prediction risk. Our main theoretical tool is a limit theorem for functions of randomly transformed, high-dimensional random vectors. The proof draws on work in probability on noise stability of functions of many variables. The pathological behavior we identify is not a consequence of complex models, but can occur even in the simplest settings -- one of our examples is a linear ridge regressor with two parameters. On the other hand, our results also show that data augmentation can have real, quantifiable benefits. ",
    "url": "https://arxiv.org/abs/2202.09134",
    "authors": [
      "Kevin H. Huang",
      "Peter Orbanz",
      "Morgane Austern"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2202.09144",
    "title": "Modelling the semantics of text in complex document layouts using graph  transformer networks",
    "abstract": "Representing structured text from complex documents typically calls for different machine learning techniques, such as language models for paragraphs and convolutional neural networks (CNNs) for table extraction, which prohibits drawing links between text spans from different content types. In this article we propose a model that approximates the human reading pattern of a document and outputs a unique semantic representation for every text span irrespective of the content type they are found in. We base our architecture on a graph representation of the structured text, and we demonstrate that not only can we retrieve semantically similar information across documents but also that the embedding space we generate captures useful semantic information, similar to language models that work only on text sequences. ",
    "url": "https://arxiv.org/abs/2202.09144",
    "authors": [
      "Thomas Roland Barillot",
      "Jacob Saks",
      "Polena Lilyanova",
      "Edward Torgas",
      "Yachen Hu",
      "Yuanqing Liu",
      "Varun Balupuri",
      "Paul Gaskell"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09153",
    "title": "Gaussian Mixture Convolution Networks",
    "abstract": "This paper proposes a novel method for deep learning based on the analytical convolution of multidimensional Gaussian mixtures. In contrast to tensors, these do not suffer from the curse of dimensionality and allow for a compact representation, as data is only stored where details exist. Convolution kernels and data are Gaussian mixtures with unconstrained weights, positions, and covariance matrices. Similar to discrete convolutional networks, each convolution step produces several feature channels, represented by independent Gaussian mixtures. Since traditional transfer functions like ReLUs do not produce Gaussian mixtures, we propose using a fitting of these functions instead. This fitting step also acts as a pooling layer if the number of Gaussian components is reduced appropriately. We demonstrate that networks based on this architecture reach competitive accuracy on Gaussian mixtures fitted to the MNIST and ModelNet data sets. ",
    "url": "https://arxiv.org/abs/2202.09153",
    "authors": [
      "Adam Celarek",
      "Pedro Hermosilla",
      "Bernhard Kerbl",
      "Timo Ropinski",
      "Michael Wimmer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09155",
    "title": "PerFED-GAN: Personalized Federated Learning via Generative Adversarial  Networks",
    "abstract": "Federated learning is gaining popularity as a distributed machine learning method that can be used to deploy AI-dependent IoT applications while protecting client data privacy and security. Due to the differences of clients, a single global model may not perform well on all clients, so the personalized federated learning method, which trains a personalized model for each client that better suits its individual needs, becomes a research hotspot. Most personalized federated learning research, however, focuses on data heterogeneity while ignoring the need for model architecture heterogeneity. Most existing federated learning methods uniformly set the model architecture of all clients participating in federated learning, which is inconvenient for each client's individual model and local data distribution requirements, and also increases the risk of client model leakage. This paper proposes a federated learning method based on co-training and generative adversarial networks(GANs) that allows each client to design its own model to participate in federated learning training independently without sharing any model architecture or parameter information with other clients or a center. In our experiments, the proposed method outperforms the existing methods in mean test accuracy by 42% when the client's model architecture and data distribution vary significantly. ",
    "url": "https://arxiv.org/abs/2202.09155",
    "authors": [
      "Xingjian Cao",
      "Gang Sun",
      "Hongfang Yu",
      "Mohsen Guizani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09157",
    "title": "Tackling A Class of Hard Subset-Sum Problems: Integration of Lattice  Attacks with Disaggregation Techniques",
    "abstract": "Subset-sum problems belong to the NP class and play an important role in both complexity theory and knapsack-based cryptosystems, which have been proved in the literature to become hardest when the so-called density approaches one. Lattice attacks, which are acknowledged in the literature as the most effective methods, fail occasionally even when the number of unknown variables is of medium size. In this paper we propose a modular disaggregation technique and a simplified lattice formulation based on which two lattice attack algorithms are further designed. We introduce the new concept \"jump points\" in our disaggregation technique, and derive inequality conditions to identify superior jump points which can more easily cut-off non-desirable short integer solutions. Empirical tests have been conducted to show that integrating the disaggregation technique with lattice attacks can effectively raise success ratios to 100% for randomly generated problems with density one and of dimensions up to 100. Finally, statistical regressions are conducted to test significant features, thus revealing reasonable factors behind the empirical success of our algorithms and techniques proposed in this paper. ",
    "url": "https://arxiv.org/abs/2202.09157",
    "authors": [
      "Bojun Lu",
      "Duan Li",
      "Rujun Jiang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2202.09177",
    "title": "Space4HGNN: A Novel, Modularized and Reproducible Platform to Evaluate  Heterogeneous Graph Neural Network",
    "abstract": "Heterogeneous Graph Neural Network (HGNN) has been successfully employed in various tasks, but we cannot accurately know the importance of different design dimensions of HGNNs due to diverse architectures and applied scenarios. Besides, in the research community of HGNNs, implementing and evaluating various tasks still need much human effort. To mitigate these issues, we first propose a unified framework covering most HGNNs, consisting of three components: heterogeneous linear transformation, heterogeneous graph transformation, and heterogeneous message passing layer. Then we build a platform Space4HGNN by defining a design space for HGNNs based on the unified framework, which offers modularized components, reproducible implementations, and standardized evaluation for HGNNs. Finally, we conduct experiments to analyze the effect of different designs. With the insights found, we distill a condensed design space and verify its effectiveness. ",
    "url": "https://arxiv.org/abs/2202.09177",
    "authors": [
      "Tianyu Zhao",
      "Cheng Yang",
      "Yibo Li",
      "Quan Gan",
      "Zhenyi Wang",
      "Fengqi Liang",
      "Huan Zhao",
      "Yingxia Shao",
      "Xiao Wang",
      "Chuan Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2202.09200",
    "title": "Geometric representation of the weighted harmonic mean of $n$ positive  values and potential uses",
    "abstract": "This paper is dedicated to the analysis and detailed study of a procedure to generate both the weighted arithmetic and harmonic means of $n$ positive real numbers. Together with this interpretation, we prove some relevant properties that will allow us to define numerical approximation methods in several dimensions adapted to discontinuities. ",
    "url": "https://arxiv.org/abs/2202.09200",
    "authors": [
      "S.Amat",
      "P. Ortiz",
      "J.Ruiz",
      "J.C.Trillo",
      "D.F. Ya\u00f1ez"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2202.09206",
    "title": "Spatio-Temporal Outdoor Lighting Aggregation on Image Sequences using  Transformer Networks",
    "abstract": "In this work, we focus on outdoor lighting estimation by aggregating individual noisy estimates from images, exploiting the rich image information from wide-angle cameras and/or temporal image sequences. Photographs inherently encode information about the scene's lighting in the form of shading and shadows. Recovering the lighting is an inverse rendering problem and as that ill-posed. Recent work based on deep neural networks has shown promising results for single image lighting estimation, but suffers from robustness. We tackle this problem by combining lighting estimates from several image views sampled in the angular and temporal domain of an image sequence. For this task, we introduce a transformer architecture that is trained in an end-2-end fashion without any statistical post-processing as required by previous work. Thereby, we propose a positional encoding that takes into account the camera calibration and ego-motion estimation to globally register the individual estimates when computing attention between visual words. We show that our method leads to improved lighting estimation while requiring less hyper-parameters compared to the state-of-the-art. ",
    "url": "https://arxiv.org/abs/2202.09206",
    "authors": [
      "Haebom Lee",
      "Christian Homeyer",
      "Robert Herzog",
      "Jan Rexilius",
      "Carsten Rother"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2202.09212",
    "title": "Molecule Generation for Drug Design: a Graph Learning Perspective",
    "abstract": "Machine learning has revolutionized many fields, and graph learning is recently receiving increasing attention. From the application perspective, one of the emerging and attractive areas is aiding the design and discovery of molecules, especially in drug industry. In this survey, we provide an overview of the state-of-the-art molecule (and mostly for de novo drug) design and discovery aiding methods whose methodology involves (deep) graph learning. Specifically, we propose to categorize these methods into three groups: i) all at once, ii) fragment-based and iii) node-by-node. We further present some representative public datasets and summarize commonly utilized evaluation metrics for generation and optimization, respectively. Finally, we discuss challenges and directions for future research, from the drug design perspective. ",
    "url": "https://arxiv.org/abs/2202.09212",
    "authors": [
      "Nianzu Yang",
      "Huaijin Wu",
      "Junchi Yan",
      "Xiaoyong Pan",
      "Ye Yuan",
      "Le Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2202.09222",
    "title": "Improving AoI via Learning-based Distributed MAC in Wireless Networks",
    "abstract": "In this work, we consider a remote monitoring scenario in which multiple sensors share a wireless channel to deliver their status updates to a process monitor via an access point (AP). Moreover, we consider that the sensors randomly arrive and depart from the network as they become active and inactive. The goal of the sensors is to devise a medium access strategy to collectively minimize the long-term mean network \\ac{AoI} of their respective processes at the remote monitor. For this purpose, we propose specific modifications to ALOHA-QT algorithm, a distributed medium access algorithm that employs a policy tree (PT) and reinforcement learning (RL) to achieve high throughput. We provide the upper bound on the mean network Age of Information (AoI) for the proposed algorithm along with pointers for selecting its key parameter. The results reveal that the proposed algorithm reduces mean network \\ac{AoI} by more than 50 percent for state of the art stationary randomized policies while successfully adjusting to a changing number of active users in the network. The algorithm needs less memory and computation than ALOHA-QT while performing better in terms of AoI. ",
    "url": "https://arxiv.org/abs/2202.09222",
    "authors": [
      "Yash Deshpande",
      "Onur Ayan",
      "Wolfgang Kellerer"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2202.09223",
    "title": "History Data Driven Distributed Consensus in Networks",
    "abstract": "The association of weights in a distributed consensus protocol quantify the trust that an agent has on its neighbors in a network. An important problem in such networked systems is the uncertainty in the estimation of trust between neighboring agents, coupled with the losses arising from mistakenly associating wrong amounts of trust with different neighboring agents. We introduce a probabilistic approach which uses the historical data collected in the network, to determine the level of trust between each agent. Specifically, using the finite history of the shared data between neighbors, we obtain a configuration which represents the confidence estimate of every neighboring agent's trustworthiness. Finally, we propose a History-Data-Driven (HDD) distributed consensus protocol which translates the computed configuration data into weights to be used in the consensus update. The approach using the historical data in the context of a distributed consensus setting marks the novel contribution of our paper. ",
    "url": "https://arxiv.org/abs/2202.09223",
    "authors": [
      "Venkatraman Renganathan",
      "Angela Fontan",
      "Karthik Ganapathy"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2202.09230",
    "title": "United Monoids: Finding Simplicial Sets and Labelled Algebraic Graphs in  Trees",
    "abstract": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (2017). It uses an algebraic data type to represent graphs and has attracted users, including from industry, due to its emphasis on equational reasoning and making a common class of bugs impossible by eliminating internal invariants. The previous formulation of algebraic graphs did not support edge labels, which was a serious practical limitation. In this paper, we redesign the main algebraic data type and remove this limitation. We follow a fairly standard approach of parameterising a data structure with a semiring of edge labels. The new formulation is both more general and simpler: the two operations for composing graphs used in the previous work can now be obtained from a single operation by fixing the semiring parameter to zero and one, respectively. By instantiating the new data type with different semirings, and working out laws for interpreting the resulting expression trees, we discover an unusual algebraic structure, which we call \"united monoids\", that is, a pair of monoids whose unit elements coincide. We believe that it is worth studying united monoids in their full generality, going beyond the graphs which prompted their discovery. To that end, we characterise united monoids with a minimal set of axioms, prove a few basic theorems, and discuss several notable examples. We validate the presented approach by implementing it in the open-source *algebraic-graphs* library. Our theoretical contributions are supported by proofs that are included in the paper and have also been machine-checked in Agda. By extending algebraic graphs with support for edge labels, we make them suitable for a much larger class of possible applications. By studying united monoids, we provide a theoretical foundation for further research in this area. ",
    "url": "https://arxiv.org/abs/2202.09230",
    "authors": [
      "Andrey Mokhov"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2202.09253",
    "title": "Sketching Distances in Monotone Graph Classes",
    "abstract": "We study the problems of adjacency sketching, small-distance sketching, and approximate distance threshold sketching for monotone classes of graphs. The problem is to obtain randomized sketches of the vertices of any graph G in the class, so that adjacency, exact distance thresholds, or approximate distance thresholds of two vertices u, v can be decided (with high probability) from the sketches of u and v, by a decoder that does not know the graph. The goal is to determine when sketches of constant size exist. We show that, for monotone classes of graphs, there is a strict hierarchy: approximate distance threshold sketches imply small-distance sketches, which imply adjacency sketches, whereas the reverse implications are each false. The existence of an adjacency sketch is equivalent to the condition of bounded arboricity, while the existence of small-distance sketches is equivalent to the condition of bounded expansion. Classes of constant expansion admit approximate distance threshold sketches, while a monotone graph class can have arbitrarily small non-constant expansion without admitting an approximate distance threshold sketch. ",
    "url": "https://arxiv.org/abs/2202.09253",
    "authors": [
      "Louis Esperet",
      "Nathaniel Harms",
      "Andrey Kupavskii"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2202.09256",
    "title": "Traffic-Aware Dynamic Functional Split for 5G Cloud Radio Access  Networks",
    "abstract": "As we are moving towards adopting virtualization technologies for next generation mobile networks, 5G base station (called gNB) is segregated into a Radio Unit (RU), a Distributed Unit (DU), and a Central Unit (CU) in order to support Cloud based Radio Access Networks (C-RAN) where RU and DU are connected through a fronthaul link while CU and DU are connected through a midhaul link. Although virtualization of CU gives benefits of centralization to the operators, there are other issues to be solved such as optimization of midhaul bandwidth and computing resources at edge cloud and central cloud where the DUs and CUs are deployed, respectively. In this paper, we propose a dynamic functional split selection for the DUs in 5G C-RAN by adopting to traffic heterogeneity where the midhaul bandwidth is limited. We proposed an optimization problem that maximizes the centralization of the C-RAN system by operating more number of DUs on split Option-7 by changing the channel bandwidth of the DUs. The dynamical selection of split options among each CU-DU pair gives 90% centralization over the static functional split for a given midhaul bandwidth. ",
    "url": "https://arxiv.org/abs/2202.09256",
    "authors": [
      "Himank Gupta",
      "Antony Franklin A",
      "Mayank Kumar",
      "Bheemarjuna Reddy Tamma"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2202.09259",
    "title": "Reduced-Order Modeling of Thermal Dynamics in District Energy Networks  using Spectral Clustering",
    "abstract": "Simulation of thermal dynamics in city-scale district energy grids often becomes computationally prohibitive for long simulation runs. Current model order reduction methods offer limited interpretability with regards to the non-reduced system, and are not in general applicable for e.g., varying flow rates, multiple producers, or changing flow directions. This article presents a novel method based on graph theory that approximates the solution of an optimization problem that minimizes the local truncation error for heat transport in the grid. It is shown that the method can be used to reduce the thermal dynamic model of a city-scale energy grid, resulting in a coarser temporal and spatial resolution. The relative root mean square error was 2.3\\% compared to the non-reduced system for the evaluation scenario at the instances of the coarser time step, for every temperature state of the original graph. ",
    "url": "https://arxiv.org/abs/2202.09259",
    "authors": [
      "Johan Simonsson",
      "Khalid Tourkey Atta",
      "Wolfgang Birk"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2202.09275",
    "title": "Rethinking Pareto Frontier for Performance Evaluation of Deep Neural  Networks",
    "abstract": "Recent efforts in deep learning show a considerable advancement in redesigning deep learning models for low-resource and edge devices. The performance optimization of deep learning models are conducted either manually or through automatic architecture search, or a combination of both. The throughput and power consumption of deep learning models strongly depend on the target hardware. We propose to use a \\emph{multi-dimensional} Pareto frontier to re-define the efficiency measure using a multi-objective optimization, where other variables such as power consumption, latency, and accuracy play a relative role in defining a dominant model. Furthermore, a random version of the multi-dimensional Pareto frontier is introduced to mitigate the uncertainty of accuracy, latency, and throughput variations of deep learning models in different experimental setups. These two breakthroughs provide an objective benchmarking method for a wide range of deep learning models. We run our novel multi-dimensional stochastic relative efficiency on a wide range of deep image classification models trained ImageNet data. Thank to this new approach we combine competing variables with stochastic nature simultaneously in a single relative efficiency measure. This allows to rank deep models that run efficiently on different computing hardware, and combines inference efficiency with training efficiency objectively. ",
    "url": "https://arxiv.org/abs/2202.09275",
    "authors": [
      "Vahid Partovi Nia",
      "Alireza Ghaffari",
      "Mahdi Zolnouri",
      "Yvon Savaria"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2202.09277",
    "title": "(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering",
    "abstract": "Spatio-temporal scene-graph approaches to video-based reasoning tasks such as video question-answering (QA) typically construct such graphs for every video frame. Such approaches often ignore the fact that videos are essentially sequences of 2D \"views\" of events happening in a 3D space, and that the semantics of the 3D scene can thus be carried over from frame to frame. Leveraging this insight, we propose a (2.5+1)D scene graph representation to better capture the spatio-temporal information flows inside the videos. Specifically, we first create a 2.5D (pseudo-3D) scene graph by transforming every 2D frame to have an inferred 3D structure using an off-the-shelf 2D-to-3D transformation module, following which we register the video frames into a shared (2.5+1)D spatio-temporal space and ground each 2D scene graph within it. Such a (2.5+1)D graph is then segregated into a static sub-graph and a dynamic sub-graph, corresponding to whether the objects within them usually move in the world. The nodes in the dynamic graph are enriched with motion features capturing their interactions with other graph nodes. Next, for the video QA task, we present a novel transformer-based reasoning pipeline that embeds the (2.5+1)D graph into a spatio-temporal hierarchical latent space, where the sub-graphs and their interactions are captured at varied granularity. To demonstrate the effectiveness of our approach, we present experiments on the NExT-QA and AVSD-QA datasets. Our results show that our proposed (2.5+1)D representation leads to faster training and inference, while our hierarchical model showcases superior performance on the video QA task versus the state of the art. ",
    "url": "https://arxiv.org/abs/2202.09277",
    "authors": [
      "Anoop Cherian",
      "Chiori Hori",
      "Tim K. Marks",
      "Jonathan Le Roux"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09282",
    "title": "FinNet: Solving Time-Independent Differential Equations with Finite  Difference Neural Network",
    "abstract": "In recent years, deep learning approaches for partial differential equations have received much attention due to their mesh-freeness and other desirable properties. However, most of the works so far concentrated on time-dependent nonlinear differential equations. In this work, we analyze potential issues with the well-known Physic Informed Neural Network for differential equations that are not time-dependent. This analysis motivates us to introduce a novel technique, namely FinNet, for solving differential equations by incorporating finite difference into deep learning. Even though we use a mesh during the training phase, the prediction phase is mesh-free. We illustrate the effectiveness of our method through experiments on solving various equations. ",
    "url": "https://arxiv.org/abs/2202.09282",
    "authors": [
      "Son N. T. Tu",
      "Thu Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09284",
    "title": "Amenable Sparse Network Investigator",
    "abstract": "As the optimization problem of pruning a neural network is nonconvex and the strategies are only guaranteed to find local solutions, a good initialization becomes paramount. To this end, we present the Amenable Sparse Network Investigator ASNI algorithm that learns a sparse network whose initialization is compressed. The learned sparse structure found by ASNI is amenable since its corresponding initialization, which is also learned by ASNI, consists of only 2L numbers, where L is the number of layers. Requiring just a few numbers for parameter initialization of the learned sparse network makes the sparse network amenable. The learned initialization set consists of L signed pairs that act as the centroids of parameter values of each layer. These centroids are learned by the ASNI algorithm after only one single round of training. We experimentally show that the learned centroids are sufficient to initialize the nonzero parameters of the learned sparse structure in order to achieve approximately the accuracy of non-sparse network. We also empirically show that in order to learn the centroids, one needs to prune the network globally and gradually. Hence, for parameter pruning we propose a novel strategy based on a sigmoid function that specifies the sparsity percentage across the network globally. Then, pruning is done magnitude-wise and after each epoch of training. We have performed a series of experiments utilizing networks such as ResNets, VGG-style, small convolutional, and fully connected ones on ImageNet, CIFAR10, and MNIST datasets. ",
    "url": "https://arxiv.org/abs/2202.09284",
    "authors": [
      "Saeed Damadi",
      "Erfan Nouri",
      "Hamed Pirsiavash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09300",
    "title": "Exploring Adversarially Robust Training for Unsupervised Domain  Adaptation",
    "abstract": "Unsupervised Domain Adaptation (UDA) methods aim to transfer knowledge from a labeled source domain to an unlabeled target domain. UDA has been extensively studied in the computer vision literature. Deep networks have been shown to be vulnerable to adversarial attacks. However, very little focus is devoted to improving the adversarial robustness of deep UDA models, causing serious concerns about model reliability. Adversarial Training (AT) has been considered to be the most successful adversarial defense approach. Nevertheless, conventional AT requires ground-truth labels to generate adversarial examples and train models, which limits its effectiveness in the unlabeled target domain. In this paper, we aim to explore AT to robustify UDA models: How to enhance the unlabeled data robustness via AT while learning domain-invariant features for UDA? To answer this, we provide a systematic study into multiple AT variants that potentially apply to UDA. Moreover, we propose a novel Adversarially Robust Training method for UDA accordingly, referred to as ARTUDA. Extensive experiments on multiple attacks and benchmarks show that ARTUDA consistently improves the adversarial robustness of UDA models. ",
    "url": "https://arxiv.org/abs/2202.09300",
    "authors": [
      "Shao-Yuan Lo",
      "Vishal M. Patel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09305",
    "title": "Masked prediction tasks: a parameter identifiability view",
    "abstract": "The vast majority of work in self-supervised learning, both theoretical and empirical (though mostly the latter), have largely focused on recovering good features for downstream tasks, with the definition of \"good\" often being intricately tied to the downstream task itself. This lens is undoubtedly very interesting, but suffers from the problem that there isn't a \"canonical\" set of downstream tasks to focus on -- in practice, this problem is usually resolved by competing on the benchmark dataset du jour. In this paper, we present an alternative lens: one of parameter identifiability. More precisely, we consider data coming from a parametric probabilistic model, and train a self-supervised learning predictor with a suitably chosen parametric form. Then, we ask whether we can read off the ground truth parameters of the probabilistic model from the optimal predictor. We focus on the widely used self-supervised learning method of predicting masked tokens, which is popular for both natural languages and visual data. While incarnations of this approach have already been successfully used for simpler probabilistic models (e.g. learning fully-observed undirected graphical models), we focus instead on latent-variable models capturing sequential structures -- namely Hidden Markov Models with both discrete and conditionally Gaussian observations. We show that there is a rich landscape of possibilities, out of which some prediction tasks yield identifiability, while others do not. Our results, borne of a theoretical grounding of self-supervised learning, could thus potentially beneficially inform practice. Moreover, we uncover close connections with uniqueness of tensor rank decompositions -- a widely used tool in studying identifiability through the lens of the method of moments. ",
    "url": "https://arxiv.org/abs/2202.09305",
    "authors": [
      "Bingbin Liu",
      "Daniel Hsu",
      "Pradeep Ravikumar",
      "Andrej Risteski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2202.09318",
    "title": "DataMUX: Data Multiplexing for Neural Networks",
    "abstract": "In this paper, we introduce data multiplexing (DataMUX), a technique that enables deep neural networks to process multiple inputs simultaneously using a single compact representation. DataMUX demonstrates that neural networks are capable of generating accurate predictions over mixtures of inputs, resulting in increased throughput with minimal extra memory requirements. Our approach uses two key components -- 1) a multiplexing layer that performs a fixed linear transformation to each input before combining them to create a mixed representation of the same size as a single input, which is then processed by the base network, and 2) a demultiplexing layer that converts the base network's output back into independent representations before producing predictions for each input. We show the viability of DataMUX for different architectures (Transformers, and to a lesser extent MLPs and CNNs) across six different tasks spanning sentence classification, named entity recognition and image classification. For instance, DataMUX for Transformers can multiplex up to $20$x/$40$x inputs, achieving $11$x/$18$x increase in throughput with minimal absolute performance drops of $<2\\%$ and $<4\\%$ respectively on MNLI, a natural language inference task. We also provide a theoretical construction for multiplexing in self-attention networks and analyze the effect of various design elements in DataMUX. ",
    "url": "https://arxiv.org/abs/2202.09318",
    "authors": [
      "Vishvak Murahari",
      "Carlos E. Jimenez",
      "Runzhe Yang",
      "Karthik Narasimhan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2202.09320",
    "title": "Distributed Transient Safety Verification via Robust Control Invariant  Sets: A Microgrid Application",
    "abstract": "Modern safety-critical energy infrastructures are increasingly operated in a hierarchical and modular control framework which allows for limited data exchange between the modules. In this context, it is important for each module to synthesize and communicate constraints on the values of exchanged information in order to assure system-wide safety. To ensure transient safety in inverter-based microgrids, we develop a set invariance-based distributed safety verification algorithm for each inverter module. Applying Nagumo's invariance condition, we construct a robust polynomial optimization problem to jointly search for safety-admissible set of control set-points and design parameters, under allowable disturbances from neighbors. We use sum-of-squares (SOS) programming to solve the verification problem and we perform numerical simulations using grid-forming inverters to illustrate the algorithm. ",
    "url": "https://arxiv.org/abs/2202.09320",
    "authors": [
      "Jean-Baptiste Bouvier",
      "Sai Pushpak Nandanoori",
      "Melkior Ornik",
      "Soumya Kundu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2202.09340",
    "title": "Learning Physics-Informed Neural Networks without Stacked  Back-propagation",
    "abstract": "Physics-Informed Neural Network (PINN) has become a commonly used machine learning approach to solve partial differential equations (PDE). But, facing high-dimensional second-order PDE problems, PINN will suffer from severe scalability issues since its loss includes second-order derivatives, the computational cost of which will grow along with the dimension during stacked back-propagation. In this paper, we develop a novel approach that can significantly accelerate the training of Physics-Informed Neural Networks. In particular, we parameterize the PDE solution by the Gaussian smoothed model and show that, derived from Stein's Identity, the second-order derivatives can be efficiently calculated without back-propagation. We further discuss the model capacity and provide variance reduction methods to address key limitations in the derivative estimation. Experimental results show that our proposed method can achieve competitive error compared to standard PINN training but is two orders of magnitude faster. ",
    "url": "https://arxiv.org/abs/2202.09340",
    "authors": [
      "Di He",
      "Wenlei Shi",
      "Shanda Li",
      "Xiaotian Gao",
      "Jia Zhang",
      "Jiang Bian",
      "Liwei Wang",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09352",
    "title": "Assessment of Cyber-Physical Intrusion Detection and Classification for  Industrial Control Systems",
    "abstract": "The increasing interaction of industrial control systems (ICSs) with public networks and digital devices introduces new cyber threats to power systems and other critical infrastructure. Recent cyber-physical attacks such as Stuxnet and Irongate revealed unexpected ICS vulnerabilities and a need for improved security measures. Intrusion detection systems constitute a key security technology, which typically monitor network data for detecting malicious activities. However, a central characteristic of modern ICSs is the increasing interdependency of physical and cyber network processes. Thus, the integration of network and physical process data is seen as a promising approach to improve predictability in intrusion detection for ICSs by accounting for physical constraints and underlying process patterns. This work systematically assesses real-time cyber-physical intrusion detection and multiclass classification, based on a comparison to its purely network data-based counterpart and evaluation of misclassifications and detection delay. Multiple supervised machine learning models are applied on a recent cyber-physical dataset, describing various cyber attacks and physical faults on a generic ICS. A key finding is that integration of physical process data improves detection and classification of all attack types. In addition, it enables simultaneous processing of attacks and faults, paving the way for holistic cross-domain cause analysis. ",
    "url": "https://arxiv.org/abs/2202.09352",
    "authors": [
      "Nils M\u00fcller",
      "Charalampos Ziras",
      "Kai Heussen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2202.08876",
    "title": "Training neural networks using monotone variational inequality",
    "abstract": "Despite the vast empirical success of neural networks, theoretical understanding of the training procedures remains limited, especially in providing performance guarantees of testing performance due to the non-convex nature of the optimization problem. Inspired by a recent work of (Juditsky & Nemirovsky, 2019), instead of using the traditional loss function minimization approach, we reduce the training of the network parameters to another problem with convex structure -- to solve a monotone variational inequality (MVI). The solution to MVI can be found by computationally efficient procedures, and importantly, this leads to performance guarantee of $\\ell_2$ and $\\ell_{\\infty}$ bounds on model recovery accuracy and prediction accuracy under the theoretical setting of training one-layer linear neural network. In addition, we study the use of MVI for training multi-layer neural networks and propose a practical algorithm called \\textit{stochastic variational inequality} (SVI), and demonstrates its applicability in training fully-connected neural networks and graph neural networks (GNN) (SVI is completely general and can be used to train other types of neural networks). We demonstrate the competitive or better performance of SVI compared to the stochastic gradient descent (SGD) on both synthetic and real network data prediction tasks regarding various performance metrics. ",
    "url": "https://arxiv.org/abs/2202.08876",
    "authors": [
      "Chen Xu",
      "Xiuyuan Cheng",
      "Yao Xie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.08916",
    "title": "Graph Convolutional Networks for Multi-modality Medical Imaging:  Methods, Architectures, and Clinical Applications",
    "abstract": "Image-based characterization and disease understanding involve integrative analysis of morphological, spatial, and topological information across biological scales. The development of graph convolutional networks (GCNs) has created the opportunity to address this information complexity via graph-driven architectures, since GCNs can perform feature aggregation, interaction, and reasoning with remarkable flexibility and efficiency. These GCNs capabilities have spawned a new wave of research in medical imaging analysis with the overarching goal of improving quantitative disease understanding, monitoring, and diagnosis. Yet daunting challenges remain for designing the important image-to-graph transformation for multi-modality medical imaging and gaining insights into model interpretation and enhanced clinical decision support. In this review, we present recent GCNs developments in the context of medical image analysis including imaging data from radiology and histopathology. We discuss the fast-growing use of graph network architectures in medical image analysis to improve disease diagnosis and patient outcomes in clinical practice. To foster cross-disciplinary research, we present GCNs technical advancements, emerging medical applications, identify common challenges in the use of image-based GCNs and their extensions in model interpretation, large-scale benchmarks that promise to transform the scope of medical image studies and related graph-driven medical research. ",
    "url": "https://arxiv.org/abs/2202.08916",
    "authors": [
      "Kexin Ding",
      "Mu Zhou",
      "Zichen Wang",
      "Qiao Liu",
      "Corey W. Arnold",
      "Shaoting Zhang",
      "Dimitri N. Metaxas"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.08936",
    "title": "Prior image-based medical image reconstruction using a style-based  generative adversarial network",
    "abstract": "Computed medical imaging systems require a computational reconstruction procedure for image formation. In order to recover a useful estimate of the object to-be-imaged when the recorded measurements are incomplete, prior knowledge about the nature of object must be utilized. In order to improve the conditioning of an ill-posed imaging inverse problem, deep learning approaches are being actively investigated for better representing object priors and constraints. This work proposes to use a style-based generative adversarial network (StyleGAN) to constrain an image reconstruction problem in the case where additional information in the form of a prior image of the sought-after object is available. An optimization problem is formulated in the intermediate latent-space of a StyleGAN, that is disentangled with respect to meaningful image attributes or \"styles\", such as the contrast used in magnetic resonance imaging (MRI). Discrepancy between the sought-after and prior images is measured in the disentangled latent-space, and is used to regularize the inverse problem in the form of constraints on specific styles of the disentangled latent-space. A stylized numerical study inspired by MR imaging is designed, where the sought-after and the prior image are structurally similar, but belong to different contrast mechanisms. The presented numerical studies demonstrate the superiority of the proposed approach as compared to classical approaches in the form of traditional metrics. ",
    "url": "https://arxiv.org/abs/2202.08936",
    "authors": [
      "Varun A. Kelkar",
      "Mark A. Anastasio"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2202.08956",
    "title": "GNN-Surrogate: A Hierarchical and Adaptive Graph Neural Network for  Parameter Space Exploration of Unstructured-Mesh Ocean Simulations",
    "abstract": "We propose GNN-Surrogate, a graph neural network-based surrogate model to explore the parameter space of ocean climate simulations. Parameter space exploration is important for domain scientists to understand the influence of input parameters (e.g., wind stress) on the simulation output (e.g., temperature). The exploration requires scientists to exhaust the complicated parameter space by running a batch of computationally expensive simulations. Our approach improves the efficiency of parameter space exploration with a surrogate model that predicts the simulation outputs accurately and efficiently. Specifically, GNN-Surrogate predicts the output field with given simulation parameters so scientists can explore the simulation parameter space with visualizations from user-specified visual mappings. Moreover, our graph-based techniques are designed for unstructured meshes, making the exploration of simulation outputs on irregular grids efficient. For efficient training, we generate hierarchical graphs and use adaptive resolutions. We give quantitative and qualitative evaluations on the MPAS-Ocean simulation to demonstrate the effectiveness and efficiency of GNN-Surrogate. Source code is publicly available at https://github.com/trainsn/GNN-Surrogate. ",
    "url": "https://arxiv.org/abs/2202.08956",
    "authors": [
      "Neng Shi",
      "Jiayi Xu",
      "Skylar W. Wurster",
      "Hanqi Guo",
      "Jonathan Woodring",
      "Luke P. Van Roekel",
      "Han-Wei Shen"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09054",
    "title": "Interpolation and Regularization for Causal Learning",
    "abstract": "We study the problem of learning causal models from observational data through the lens of interpolation and its counterpart -- regularization. A large volume of recent theoretical, as well as empirical work, suggests that, in highly complex model classes, interpolating estimators can have good statistical generalization properties and can even be optimal for statistical learning. Motivated by an analogy between statistical and causal learning recently highlighted by Janzing (2019), we investigate whether interpolating estimators can also learn good causal models. To this end, we consider a simple linearly confounded model and derive precise asymptotics for the *causal risk* of the min-norm interpolator and ridge-regularized regressors in the high-dimensional regime. Under the principle of independent causal mechanisms, a standard assumption in causal learning, we find that interpolators cannot be optimal and causal learning requires stronger regularization than statistical learning. This resolves a recent conjecture in Janzing (2019). Beyond this assumption, we find a larger range of behavior that can be precisely characterized with a new measure of *confounding strength*. If the confounding strength is negative, causal learning requires weaker regularization than statistical learning, interpolators can be optimal, and the optimal regularization can even be negative. If the confounding strength is large, the optimal regularization is infinite, and learning from observational data is actively harmful. ",
    "url": "https://arxiv.org/abs/2202.09054",
    "authors": [
      "Leena Chennuru Vankadara",
      "Luca Rendsburg",
      "Ulrike von Luxburg",
      "Debarghya Ghoshdastidar"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.09082",
    "title": "Speaker Identity Preservation in Dysarthric Speech Reconstruction by  Adversarial Speaker Adaptation",
    "abstract": "Dysarthric speech reconstruction (DSR), which aims to improve the quality of dysarthric speech, remains a challenge, not only because we need to restore the speech to be normal, but also must preserve the speaker's identity. The speaker representation extracted by the speaker encoder (SE) optimized for speaker verification has been explored to control the speaker identity. However, the SE may not be able to fully capture the characteristics of dysarthric speakers that are previously unseen. To address this research problem, we propose a novel multi-task learning strategy, i.e., adversarial speaker adaptation (ASA). The primary task of ASA fine-tunes the SE with the speech of the target dysarthric speaker to effectively capture identity-related information, and the secondary task applies adversarial training to avoid the incorporation of abnormal speaking patterns into the reconstructed speech, by regularizing the distribution of reconstructed speech to be close to that of reference speech with high quality. Experiments show that the proposed approach can achieve enhanced speaker similarity and comparable speech naturalness with a strong baseline approach. Compared with dysarthric speech, the reconstructed speech achieves 22.3% and 31.5% absolute word error rate reduction for speakers with moderate and moderate-severe dysarthria respectively. Our demo page is released here: https://wendison.github.io/ASA-DSR-demo/ ",
    "url": "https://arxiv.org/abs/2202.09082",
    "authors": [
      "Disong Wang",
      "Songxiang Liu",
      "Xixin Wu",
      "Hui Lu",
      "Lifa Sun",
      "Xunying Liu",
      "Helen Meng"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2202.09121",
    "title": "Echo-aware Adaptation of Sound Event Localization and Detection in  Unknown Environments",
    "abstract": "Our goal is to develop a sound event localization and detection (SELD) system that works robustly in unknown environments. A SELD system trained on known environment data is degraded in an unknown environment due to environmental effects such as reverberation and noise not contained in the training data. Previous studies on related tasks have shown that domain adaptation methods are effective when data on the environment in which the system will be used is available even without labels. However adaptation to unknown environments remains a difficult task. In this study, we propose echo-aware feature refinement (EAR) for SELD, which suppresses environmental effects at the feature level by using additional spatial cues of the unknown environment obtained through measuring acoustic echoes. FOA-MEIR, an impulse response dataset containing over 100 environments, was recorded to validate the proposed method. Experiments on FOA-MEIR show that the EAR effectively improves SELD performance in unknown environments. ",
    "url": "https://arxiv.org/abs/2202.09121",
    "authors": [
      "Masahiro Yasuda",
      "Yasunori Ohishi",
      "Shoichiro Saito"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2202.09124",
    "title": "Multi-view and Multi-modal Event Detection Utilizing Transformer-based  Multi-sensor fusion",
    "abstract": "We tackle a challenging task: multi-view and multi-modal event detection that detects events in a wide-range real environment by utilizing data from distributed cameras and microphones and their weak labels. In this task, distributed sensors are utilized complementarily to capture events that are difficult to capture with a single sensor, such as a series of actions of people moving in an intricate room, or communication between people located far apart in a room. For sensors to cooperate effectively in such a situation, the system should be able to exchange information among sensors and combines information that is useful for identifying events in a complementary manner. For such a mechanism, we propose a Transformer-based multi-sensor fusion (MultiTrans) which combines multi-sensor data on the basis of the relationships between features of different viewpoints and modalities. In the experiments using a dataset newly collected for this task, our proposed method using MultiTrans improved the event detection performance and outperformed comparatives. ",
    "url": "https://arxiv.org/abs/2202.09124",
    "authors": [
      "Masahiro Yasuda",
      "Yasunori Ohishi",
      "Shoichiro Saito",
      "Noboru Harada"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2202.09191",
    "title": "Heroes in orientations of chordal graphs",
    "abstract": "We characterize all digraphs $H$ such that orientations of chordal graphs with no induced copy of $H$ have bounded dichromatic number. ",
    "url": "https://arxiv.org/abs/2202.09191",
    "authors": [
      "Pierre Aboulker",
      "Guillaume Aubian",
      "Raphael Steiner"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2202.09254",
    "title": "Cell-Free Massive MIMO in Virtualized CRAN: How to Minimize the Total  Network Power?",
    "abstract": "Previous works on cell-free massive MIMO mostly consider physical-layer and fronthaul transport aspects. How to deploy cell-free massive MIMO functionality in a practical wireless system is an open problem. This paper proposes a new cell-free architecture that can be implemented on top of a virtualized cloud radio access network (V-CRAN). We aim to minimize the end-to-end power consumption by jointly considering the radio, optical fronthaul, virtualized cloud processing resources, and spectral efficiency requirements of the user equipments. The considered optimization problem is cast in a mixed binary second-order cone programming form and, thus, the global optimum can be found using a branch-and-bound algorithm. The optimal power-efficient solution of our proposed cell-free system is compared with conventional small-cell implemented using V-CRAN, to determine the benefits of cell-free networking. The numerical results demonstrate that cell-free massive MIMO increases the maximum rate substantially, which can be provided with almost the same energy per bit. We show that it is more power-efficient to activate cell-free massive MIMO already at low spectral efficiencies (above 1 bit/s/Hz). ",
    "url": "https://arxiv.org/abs/2202.09254",
    "authors": [
      "\u00d6zlem Tu\u011ffe Demir",
      "Meysam Masoudi",
      "Emil Bj\u00f6rnson",
      "Cicek Cavdar"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2202.09353",
    "title": "Model Calibration of the Liquid Mercury Spallation Target using  Evolutionary Neural Networks and Sparse Polynomial Expansions",
    "abstract": "The mercury constitutive model predicting the strain and stress in the target vessel plays a central role in improving the lifetime prediction and future target designs of the mercury targets at the Spallation Neutron Source (SNS). We leverage the experiment strain data collected over multiple years to improve the mercury constitutive model through a combination of large-scale simulations of the target behavior and the use of machine learning tools for parameter estimation. We present two interdisciplinary approaches for surrogate-based model calibration of expensive simulations using evolutionary neural networks and sparse polynomial expansions. The experiments and results of the two methods show a very good agreement for the solid mechanics simulation of the mercury spallation target. The proposed methods are used to calibrate the tensile cutoff threshold, mercury density, and mercury speed of sound during intense proton pulse experiments. Using strain experimental data from the mercury target sensors, the newly calibrated simulations achieve 7\\% average improvement on the signal prediction accuracy and 8\\% reduction in mean absolute error compared to previously reported reference parameters, with some sensors experiencing up to 30\\% improvement. The proposed calibrated simulations can significantly aid in fatigue analysis to estimate the mercury target lifetime and integrity, which reduces abrupt target failure and saves a tremendous amount of costs. However, an important conclusion from this work points out to a deficiency in the current constitutive model based on the equation of state in capturing the full physics of the spallation reaction. Given that some of the calibrated parameters that show a good agreement with the experimental data can be nonphysical mercury properties, we need a more advanced two-phase flow model to capture bubble dynamics and mercury cavitation. ",
    "url": "https://arxiv.org/abs/2202.09353",
    "authors": [
      "Majdi I. Radaideh",
      "Hoang Tran",
      "Lianshan Lin",
      "Hao Jiang",
      "Drew Winder",
      "Sarma Gorti",
      "Guannan Zhang",
      "Justin Mach",
      "Sarah Cousineau"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Machine Learning (cs.LG)",
      "Accelerator Physics (physics.acc-ph)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2202.09359",
    "title": "Machine Learning Models in Stock Market Prediction",
    "abstract": "The paper focuses on predicting the Nifty 50 Index by using 8 Supervised Machine Learning Models. The techniques used for empirical study are Adaptive Boost (AdaBoost), k-Nearest Neighbors (kNN), Linear Regression (LR), Artificial Neural Network (ANN), Random Forest (RF), Stochastic Gradient Descent (SGD), Support Vector Machine (SVM) and Decision Trees (DT). Experiments are based on historical data of Nifty 50 Index of Indian Stock Market from 22nd April, 1996 to 16th April, 2021, which is time series data of around 25 years. During the period there were 6220 trading days excluding all the non trading days. The entire trading dataset was divided into 4 subsets of different size-25% of entire data, 50% of entire data, 75% of entire data and entire data. Each subset was further divided into 2 parts-training data and testing data. After applying 3 tests- Test on Training Data, Test on Testing Data and Cross Validation Test on each subset, the prediction performance of the used models were compared and after comparison, very interesting results were found. The evaluation results indicate that Adaptive Boost, k- Nearest Neighbors, Random Forest and Decision Trees under performed with increase in the size of data set. Linear Regression and Artificial Neural Network shown almost similar prediction results among all the models but Artificial Neural Network took more time in training and validating the model. Thereafter Support Vector Machine performed better among rest of the models but with increase in the size of data set, Stochastic Gradient Descent performed better than Support Vector Machine. ",
    "url": "https://arxiv.org/abs/2202.09359",
    "authors": [
      "Gurjeet Singh"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:1810.03772",
    "title": "The existence of perfect codes in Doob graphs",
    "abstract": " Comments: 5 IEEE pages. V.2: accepted version; the introduction has been extended by a mini-survey ",
    "url": "https://arxiv.org/abs/1810.03772",
    "authors": [
      "Denis S. Krotov"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:1810.12138",
    "title": "Audio inpainting of music by means of neural networks",
    "abstract": " Comments: Presented at the 146th AES Convention [arXiv:1810.12138v2]. For the journal version, published in published in IEEE TASLP, see [arXiv:1810.12138v2] ",
    "url": "https://arxiv.org/abs/1810.12138",
    "authors": [
      "Andr\u00e9s Marafioti",
      "Nicki Holighaus",
      "Piotr Majdak",
      "Nathana\u00ebl Perraudin"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:1912.09893",
    "title": "A Fair Comparison of Graph Neural Networks for Graph Classification",
    "abstract": " Comments: Extended version of the paper published at the International Conference on Learning Representations (ICLR), 2020. Additional results are shown in the appendix ",
    "url": "https://arxiv.org/abs/1912.09893",
    "authors": [
      "Federico Errica",
      "Marco Podda",
      "Davide Bacciu",
      "Alessio Micheli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2003.09224",
    "title": "Probabilistic Visual Navigation with Bidirectional Image Prediction",
    "abstract": " Comments: 14 pages, 9 figures, 4 tables ",
    "url": "https://arxiv.org/abs/2003.09224",
    "authors": [
      "Noriaki Hirose",
      "Shun Taguchi",
      "Fei Xia",
      "Roberto Martin-Martin",
      "Kosuke Tahara",
      "Masanori Ishigaki",
      "Silvio Savarese"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2006.10325",
    "title": "When OT meets MoM: Robust estimation of Wasserstein Distance",
    "abstract": " Title: When OT meets MoM: Robust estimation of Wasserstein Distance ",
    "url": "https://arxiv.org/abs/2006.10325",
    "authors": [
      "Guillaume Staerman",
      "Pierre Laforgue",
      "Pavlo Mozharovskyi",
      "Florence d'Alch\u00e9-Buc"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2007.13819",
    "title": "Multi-Level Local SGD for Heterogeneous Hierarchical Networks",
    "abstract": " Comments: 36 pages, 10 figures, ICLR 2021 ",
    "url": "https://arxiv.org/abs/2007.13819",
    "authors": [
      "Timothy Castiglia",
      "Anirban Das",
      "Stacy Patterson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2009.06301",
    "title": "Feedback Prediction for Proactive HARQ in the Context of Industrial  Internet of Things",
    "abstract": " Title: Feedback Prediction for Proactive HARQ in the Context of Industrial  Internet of Things ",
    "url": "https://arxiv.org/abs/2009.06301",
    "authors": [
      "Baris G\u00f6ktepe",
      "Tatiana Rykova",
      "Thomas Fehrenbach",
      "Thomas Schierl",
      "Cornelius Hellge"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2010.04524",
    "title": "Multi-Objective Optimisation of Multi-Output Neural Trees",
    "abstract": " Comments: 19-24 July 2020 ",
    "url": "https://arxiv.org/abs/2010.04524",
    "authors": [
      "Varun Ojha",
      "Giuseppe Nicosia"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2101.03603",
    "title": "Target Detection and Segmentation in Circular-Scan  Synthetic-Aperture-Sonar Images using Semi-Supervised Convolutional  Encoder-Decoders",
    "abstract": " Comments: Submitted to IEEE Journal of Oceanic Engineering ",
    "url": "https://arxiv.org/abs/2101.03603",
    "authors": [
      "Isaac J. Sledge",
      "Matthew S. Emigh",
      "Jonathan L. King",
      "Denton L. Woods",
      "J. Tory Cobb",
      "Jose C. Principe"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2102.13624",
    "title": "What Doesn't Kill You Makes You Robust(er): How to Adversarially Train  against Data Poisoning",
    "abstract": " Comments: 25 pages, 15 figures ",
    "url": "https://arxiv.org/abs/2102.13624",
    "authors": [
      "Jonas Geiping",
      "Liam Fowl",
      "Gowthami Somepalli",
      "Micah Goldblum",
      "Michael Moeller",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2103.00111",
    "title": "Graph Self-Supervised Learning: A Survey",
    "abstract": " Comments: 26 pages, 9 figures, 9 tables ",
    "url": "https://arxiv.org/abs/2103.00111",
    "authors": [
      "Yixin Liu",
      "Ming Jin",
      "Shirui Pan",
      "Chuan Zhou",
      "Yu Zheng",
      "Feng Xia",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2103.00778",
    "title": "Explaining Adversarial Vulnerability with a Data Sparsity Hypothesis",
    "abstract": " Title: Explaining Adversarial Vulnerability with a Data Sparsity Hypothesis ",
    "url": "https://arxiv.org/abs/2103.00778",
    "authors": [
      "Mahsa Paknezhad",
      "Cuong Phuc Ngo",
      "Amadeus Aristo Winarto",
      "Alistair Cheong",
      "Chuen Yang Beh",
      "Jiayang Wu",
      "Hwee Kuan Lee"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2103.17203",
    "title": "Universal Prediction Band via Semi-Definite Programming",
    "abstract": " Comments: 21 pages, 4 figures ",
    "url": "https://arxiv.org/abs/2103.17203",
    "authors": [
      "Tengyuan Liang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Optimization and Control (math.OC)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2104.09124",
    "title": "DisCo: Remedy Self-supervised Learning on Lightweight Models with  Distilled Contrastive Learning",
    "abstract": " Title: DisCo: Remedy Self-supervised Learning on Lightweight Models with  Distilled Contrastive Learning ",
    "url": "https://arxiv.org/abs/2104.09124",
    "authors": [
      "Yuting Gao",
      "Jia-Xin Zhuang",
      "Shaohui Lin",
      "Hao Cheng",
      "Xing Sun",
      "Ke Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2104.14756",
    "title": "Predicting Intraoperative Hypoxemia with Joint Sequence Autoencoder  Networks",
    "abstract": " Title: Predicting Intraoperative Hypoxemia with Joint Sequence Autoencoder  Networks ",
    "url": "https://arxiv.org/abs/2104.14756",
    "authors": [
      "Hanyang Liu",
      "Michael Montana",
      "Dingwen Li",
      "Thomas Kannampallil",
      "Chenyang Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2105.03966",
    "title": "Unit Ball Model for Embedding Hierarchical Structures in the Complex  Hyperbolic Space",
    "abstract": " Title: Unit Ball Model for Embedding Hierarchical Structures in the Complex  Hyperbolic Space ",
    "url": "https://arxiv.org/abs/2105.03966",
    "authors": [
      "Huiru Xiao",
      "Caigao Jiang",
      "Yangqiu Song",
      "James Zhang",
      "Junwu Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2105.04123",
    "title": "Neural Program Repair with Execution-based Backpropagation",
    "abstract": " Title: Neural Program Repair with Execution-based Backpropagation ",
    "url": "https://arxiv.org/abs/2105.04123",
    "authors": [
      "He Ye",
      "Matias Martinez",
      "Martin Monperrus"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2105.04319",
    "title": "A Bregman Learning Framework for Sparse Neural Networks",
    "abstract": " Comments: 43 pages, 5 figures, some minor modifications, weakened assumptions ",
    "url": "https://arxiv.org/abs/2105.04319",
    "authors": [
      "Leon Bungert",
      "Tim Roith",
      "Daniel Tenbrinck",
      "Martin Burger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2105.13455",
    "title": "Perfect Matchings in the Semi-random Graph Process",
    "abstract": " Comments: Minor corrections made. Accepted to SIAM Journal on Discrete Mathematics (SIDMA) ",
    "url": "https://arxiv.org/abs/2105.13455",
    "authors": [
      "Pu Gao",
      "Calum MacRury",
      "Pawel Pralat"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2106.03403",
    "title": "Network Inference and Influence Maximization from Samples",
    "abstract": " Comments: Parellel results about the LT model are added to the conference (ICML 2021) version ",
    "url": "https://arxiv.org/abs/2106.03403",
    "authors": [
      "Zhijie Zhang",
      "Wei Chen",
      "Xiaoming Sun",
      "Jialin Zhang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2106.03480",
    "title": "A Distance Covariance-based Kernel for Nonlinear Causal Clustering in  Heterogeneous Populations",
    "abstract": " Comments: 17 pages, 3 figures; accepted to 1st Conference on Causal Learning and Reasoning (CLeaR 2022) ",
    "url": "https://arxiv.org/abs/2106.03480",
    "authors": [
      "Alex Markham",
      "Richeek Das",
      "Moritz Grosse-Wentrup"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2106.16239",
    "title": "Fixed points of nonnegative neural networks",
    "abstract": " Comments: 13 pages ",
    "url": "https://arxiv.org/abs/2106.16239",
    "authors": [
      "Tomasz Piotrowski",
      "Renato L. G. Cavalcante"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.04755",
    "title": "Beyond Low-pass Filtering: Graph Convolutional Networks with Automatic  Filtering",
    "abstract": " Comments: 12 pages ",
    "url": "https://arxiv.org/abs/2107.04755",
    "authors": [
      "Zonghan Wu",
      "Shirui Pan",
      "Guodong Long",
      "Jing Jiang",
      "Chengqi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2107.09507",
    "title": "EEG-based Cross-Subject Driver Drowsiness Recognition with an  Interpretable Convolutional Neural Network",
    "abstract": " Title: EEG-based Cross-Subject Driver Drowsiness Recognition with an  Interpretable Convolutional Neural Network ",
    "url": "https://arxiv.org/abs/2107.09507",
    "authors": [
      "Jian Cui",
      "Zirui Lan",
      "Olga Sourina",
      "Wolfgang M\u00fcller-Wittig"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2107.09899",
    "title": "Structure-Aware Long Short-Term Memory Network for 3D Cephalometric  Landmark Detection",
    "abstract": " Comments: IEEE Transactions on medical images ",
    "url": "https://arxiv.org/abs/2107.09899",
    "authors": [
      "Runnan Chen",
      "Yuexin Ma",
      "Nenglun Chen",
      "Lingjie Liu",
      "Zhiming Cui",
      "Yanhong Lin",
      "Wenping Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2107.10137",
    "title": "Improved Text Classification via Contrastive Adversarial Training",
    "abstract": " Title: Improved Text Classification via Contrastive Adversarial Training ",
    "url": "https://arxiv.org/abs/2107.10137",
    "authors": [
      "Lin Pan",
      "Chung-Wei Hang",
      "Avirup Sil",
      "Saloni Potdar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2107.13094",
    "title": "Discrete Lehmann representation of imaginary time Green's functions",
    "abstract": " Title: Discrete Lehmann representation of imaginary time Green's functions ",
    "url": "https://arxiv.org/abs/2107.13094",
    "authors": [
      "Jason Kaye",
      "Kun Chen",
      "Olivier Parcollet"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Strongly Correlated Electrons (cond-mat.str-el)"
    ]
  },
  {
    "id": "arXiv:2108.13952",
    "title": "Morphence: Moving Target Defense Against Adversarial Examples",
    "abstract": " Title: Morphence: Moving Target Defense Against Adversarial Examples ",
    "url": "https://arxiv.org/abs/2108.13952",
    "authors": [
      "Abderrahmen Amich",
      "Birhanu Eshete"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2109.04261",
    "title": "Learning cortical representations through perturbed and adversarial  dreaming",
    "abstract": " Comments: 35 pages, 15 figures; ; Jakob Jordan and Walter Senn share senior authorship ",
    "url": "https://arxiv.org/abs/2109.04261",
    "authors": [
      "Nicolas Deperrois",
      "Mihai A. Petrovici",
      "Walter Senn",
      "Jakob Jordan"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2109.05489",
    "title": "Illuminating Diverse Neural Cellular Automata for Level Generation",
    "abstract": " Comments: 9 pages, 7 figures ",
    "url": "https://arxiv.org/abs/2109.05489",
    "authors": [
      "Sam Earle",
      "Justin Snider",
      "Matthew C. Fontaine",
      "Stefanos Nikolaidis",
      "Julian Togelius"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2110.01742",
    "title": "Focal Onset Detection Using Parallel Genetic Naive Bayes Classifiers",
    "abstract": " Comments: 6 pages, 3 figures ",
    "url": "https://arxiv.org/abs/2110.01742",
    "authors": [
      "Scot Davidson",
      "Niamh McCallan",
      "Kok Yew Ng",
      "Pardis Biglarbeigi",
      "Dewar Finlay",
      "Boon Leong Lan",
      "James McLaughlin"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2110.02578",
    "title": "Decoupled Adaptation for Cross-Domain Object Detection",
    "abstract": " Title: Decoupled Adaptation for Cross-Domain Object Detection ",
    "url": "https://arxiv.org/abs/2110.02578",
    "authors": [
      "Junguang Jiang",
      "Baixu Chen",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.04398",
    "title": "The Role of Masks in Mitigating Viral Spread on Networks",
    "abstract": " Title: The Role of Masks in Mitigating Viral Spread on Networks ",
    "url": "https://arxiv.org/abs/2110.04398",
    "authors": [
      "Yurun Tian",
      "Anirudh Sridhar",
      "Chai Wah Wu",
      "Simon A. Levin",
      "H.Vincent Poor",
      "Osman Yagan"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2110.05054",
    "title": "Source Mixing and Separation Robust Audio Steganography",
    "abstract": " Comments: Accepted to ICASSP 2022 ",
    "url": "https://arxiv.org/abs/2110.05054",
    "authors": [
      "Naoya Takahashi",
      "Mayank Kumar Singh",
      "Yuki Mitsufuji"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Cryptography and Security (cs.CR)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2110.05645",
    "title": "A global convergence theory for deep ReLU implicit networks via  over-parameterization",
    "abstract": " Comments: Accepted by ICLR 2022 ",
    "url": "https://arxiv.org/abs/2110.05645",
    "authors": [
      "Tianxiang Gao",
      "Hailiang Liu",
      "Jia Liu",
      "Hridesh Rajan",
      "Hongyang Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2110.10415",
    "title": "Depth360: Self-supervised Learning for Monocular Depth Estimation using  Learnable Camera Distortion Model",
    "abstract": " Comments: 8 pages, 6 figures, 2 tables ",
    "url": "https://arxiv.org/abs/2110.10415",
    "authors": [
      "Noriaki Hirose",
      "Kosuke Tahara"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2110.12177",
    "title": "Vertebrae segmentation, identification and localization using a graph  optimization and a synergistic cycle",
    "abstract": " Comments: The iterative location-segmentation refinement scheme we claimed as one of the contributions has been found in previous reference ",
    "url": "https://arxiv.org/abs/2110.12177",
    "authors": [
      "Di Meng",
      "Eslam Mohammed",
      "Edmond Boyer",
      "Sergi Pujades"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2110.12396",
    "title": "Using Motion History Images with 3D Convolutional Networks in Isolated  Sign Language Recognition",
    "abstract": " Title: Using Motion History Images with 3D Convolutional Networks in Isolated  Sign Language Recognition ",
    "url": "https://arxiv.org/abs/2110.12396",
    "authors": [
      "Ozge Mercanoglu Sincan",
      "Hacer Yalim Keles"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2111.14616",
    "title": "DeepGate: Learning Neural Representations of Logic Gates",
    "abstract": " Comments: Accepted by DAC2022 ",
    "url": "https://arxiv.org/abs/2111.14616",
    "authors": [
      "Min Li",
      "Sadaf Khan",
      "Zhengyuan Shi",
      "Naixing Wang",
      "Yu Huang",
      "Qiang Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2112.00275",
    "title": "Learning from Mistakes based on Class Weighting with Application to  Neural Architecture Search",
    "abstract": " Title: Learning from Mistakes based on Class Weighting with Application to  Neural Architecture Search ",
    "url": "https://arxiv.org/abs/2112.00275",
    "authors": [
      "Jay Gala",
      "Pengtao Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2112.10767",
    "title": "GCN-Geo: A Graph Convolution Network-based Fine-grained IP Geolocation  Framework",
    "abstract": " Comments: Under Review ",
    "url": "https://arxiv.org/abs/2112.10767",
    "authors": [
      "Shichang Ding",
      "Xiangyang Luo",
      "Jinwei Wang",
      "Xiaoming Fu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2112.13753",
    "title": "Conversion Rate Prediction via Meta Learning in Small-Scale  Recommendation Scenarios",
    "abstract": " Title: Conversion Rate Prediction via Meta Learning in Small-Scale  Recommendation Scenarios ",
    "url": "https://arxiv.org/abs/2112.13753",
    "authors": [
      "Xiaofeng Pan",
      "Ming Li",
      "Jing Zhang",
      "Keren Yu",
      "Luping Wang",
      "Hong Wen",
      "Chengjun Mao",
      "Bo Cao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2201.00001",
    "title": "Modeling Advection on Directed Graphs using Mat\u00e9rn Gaussian Processes  for Traffic Flow",
    "abstract": " Comments: Accepted at the Machine Learning and Physical Sciences NeurIPS 2021 Workshop this https URL ",
    "url": "https://arxiv.org/abs/2201.00001",
    "authors": [
      "Danielle C Maddix",
      "Nadim Saad",
      "Yuyang Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2201.11133",
    "title": "Inference-optimized AI and high performance computing for gravitational  wave detection at scale",
    "abstract": " Comments: 19 pages, 8 figures; v2. Accepted to Frontiers in Artificial Intelligence, Special Issue: Efficient AI in Particle Physics and Astrophysics ",
    "url": "https://arxiv.org/abs/2201.11133",
    "authors": [
      "Pranshu Chaturvedi",
      "Asad Khan",
      "Minyang Tian",
      "E. A. Huerta",
      "Huihuo Zheng"
    ],
    "subjectives": [
      "General Relativity and Quantum Cosmology (gr-qc)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.12585",
    "title": "LBCF: A Large-Scale Budget-Constrained Causal Forest Algorithm",
    "abstract": " Comments: Published in Web Conference 2022 (WWW'2022) ",
    "url": "https://arxiv.org/abs/2201.12585",
    "authors": [
      "Meng Ai",
      "Biao Li",
      "Heyang Gong",
      "Qingwei Yu",
      "Shengjie Xue",
      "Yuan Zhang",
      "Yunzhou Zhang",
      "Peng Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2202.02947",
    "title": "Parallel Successive Learning for Dynamic Distributed Model Training over  Heterogeneous Wireless Networks",
    "abstract": " Title: Parallel Successive Learning for Dynamic Distributed Model Training over  Heterogeneous Wireless Networks ",
    "url": "https://arxiv.org/abs/2202.02947",
    "authors": [
      "Seyyedali Hosseinalipour",
      "Su Wang",
      "Nicolo Michelusi",
      "Vaneet Aggarwal",
      "Christopher G. Brinton",
      "David J. Love",
      "Mung Chiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2202.03484",
    "title": "Self-supervised Speaker Recognition Training Using Human-Machine  Dialogues",
    "abstract": " Comments: 5 pages, 2 figures ",
    "url": "https://arxiv.org/abs/2202.03484",
    "authors": [
      "Metehan Cekic",
      "Ruirui Li",
      "Zeya Chen",
      "Yuguang Yang",
      "Andreas Stolcke",
      "Upamanyu Madhow"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.04798",
    "title": "Augmenting Neural Networks with Priors on Function Values",
    "abstract": " Title: Augmenting Neural Networks with Priors on Function Values ",
    "url": "https://arxiv.org/abs/2202.04798",
    "authors": [
      "Hunter Nisonoff",
      "Yixin Wang",
      "Jennifer Listgarten"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2202.05089",
    "title": "Backpropagation Clipping for Deep Learning with Differential Privacy",
    "abstract": " Comments: We found a bug in our implementation code that invalidates our experimental results ",
    "url": "https://arxiv.org/abs/2202.05089",
    "authors": [
      "Timothy Stevens",
      "Ivoline C. Ngong",
      "David Darais",
      "Calvin Hirsch",
      "David Slater",
      "Joseph P. Near"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.05226",
    "title": "Deadwooding: Robust Global Pruning for Deep Neural Networks",
    "abstract": " Comments: 12 pages, 5 figures ",
    "url": "https://arxiv.org/abs/2202.05226",
    "authors": [
      "Sawinder Kaur",
      "Ferdinando Fioretto",
      "Asif Salekin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.05592",
    "title": "Video-driven Neural Physically-based Facial Asset for Production",
    "abstract": " Comments: For project page, see this https URL Notice: You may not copy, reproduce, distribute, publish, display, perform, modify, create derivative works, transmit, or in any way exploit any such content, nor may you distribute any part of this content over any network, including a local area network, sell or offer it for sale, or use such content to construct any kind of database ",
    "url": "https://arxiv.org/abs/2202.05592",
    "authors": [
      "Longwen Zhang",
      "Chuxiao Zeng",
      "Qixuan Zhang",
      "Hongyang Lin",
      "Ruixiang Cao",
      "Wei Yang",
      "Lan Xu",
      "Jingyi Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2202.06344",
    "title": "A Data Augmentation Method for Fully Automatic Brain Tumor Segmentation",
    "abstract": " Comments: 15 pages, 7 figures, 4tables ",
    "url": "https://arxiv.org/abs/2202.06344",
    "authors": [
      "Yu Wang",
      "Yarong Ji",
      "Hongbing Xiao"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.07301",
    "title": "User-Oriented Robust Reinforcement Learning",
    "abstract": " Title: User-Oriented Robust Reinforcement Learning ",
    "url": "https://arxiv.org/abs/2202.07301",
    "authors": [
      "Haoyi You",
      "Beichen Yu",
      "Haiming Jin",
      "Zhaoxing Yang",
      "Jiahui Sun",
      "Xinbing Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.07757",
    "title": "Architecture Agnostic Federated Learning for Neural Networks",
    "abstract": " Title: Architecture Agnostic Federated Learning for Neural Networks ",
    "url": "https://arxiv.org/abs/2202.07757",
    "authors": [
      "Disha Makhija",
      "Xing Han",
      "Nhat Ho",
      "Joydeep Ghosh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.07798",
    "title": "BB-ML: Basic Block Performance Prediction using Machine Learning  Techniques",
    "abstract": " Title: BB-ML: Basic Block Performance Prediction using Machine Learning  Techniques ",
    "url": "https://arxiv.org/abs/2202.07798",
    "authors": [
      "Shamminuj Aktar",
      "Hamdy Abdelkhalik",
      "Nazmul Haque Turja",
      "Yehia Arafa",
      "Atanu Barai",
      "Nishant Panda",
      "Gopinath Chennupati",
      "Nandakishore Santhi",
      "Stephan Eidenbenz",
      "Abdel-Hameed Badawy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2202.08504",
    "title": "Finding Representative Sampling Subsets in Sensor Graphs using Time  Series Similarities",
    "abstract": " Title: Finding Representative Sampling Subsets in Sensor Graphs using Time  Series Similarities ",
    "url": "https://arxiv.org/abs/2202.08504",
    "authors": [
      "Roshni Chakraborty",
      "Josefine Holm",
      "Torben Bach Pedersen",
      "Petar Popovski"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2202.08604",
    "title": "Two-Stage Architectural Fine-Tuning with Neural Architecture Search  using Early-Stopping in Image Classification",
    "abstract": " Comments: 5 pages, 6 figures ",
    "url": "https://arxiv.org/abs/2202.08604",
    "authors": [
      "Youngkee Kim",
      "Won Joon Yun",
      "Youn Kyu Lee",
      "Joongheon Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  }
]