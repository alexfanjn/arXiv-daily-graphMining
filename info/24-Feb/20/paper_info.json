[
  {
    "id": "arXiv:2402.10921",
    "title": "AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in  Conversation via Joint Embedding Learning",
    "abstract": "Human emotion can be presented in different modes i.e., audio, video, and text. However, the contribution of each mode in exhibiting each emotion is not uniform. Furthermore, the availability of complete mode-specific details may not always be guaranteed in the test time. In this work, we propose AM^2-EmoJE, a model for Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning model that is grounded on two-fold contributions: First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner. By this the model aims to prioritize the mode-invariant spatial query details of the emotion patterns, while also retaining its mode-exclusive aspects within the learned multimodal query descriptor. Second the multimodal joint embedding learning module that explicitly addresses various missing modality scenarios in test-time. By this, the model learns to emphasize on the correlated patterns across modalities, which may help align the cross-attended mode-specific descriptors pairwise within a joint-embedding space and thereby compensate for missing modalities during inference. By leveraging the spatio-temporal details at the dialogue level, the proposed AM^2-EmoJE not only demonstrates superior performance compared to the best-performing state-of-the-art multimodal methods, by effectively leveraging body language in place of face expression, it also exhibits an enhanced privacy feature. By reporting around 2-5% improvement in the weighted-F1 score, the proposed multimodal joint embedding module facilitates an impressive performance gain in a variety of missing-modality query scenarios during test time. ",
    "url": "https://arxiv.org/abs/2402.10921",
    "authors": [
      "Naresh Kumar Devulapally",
      "Sidharth Anand",
      "Sreyasee Das Bhattacharjee",
      "Junsong Yuan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.10926",
    "title": "Numerical analysis of physics-informed neural networks and related  models in physics-informed machine learning",
    "abstract": "Physics-informed neural networks (PINNs) and their variants have been very popular in recent years as algorithms for the numerical simulation of both forward and inverse problems for partial differential equations. This article aims to provide a comprehensive review of currently available results on the numerical analysis of PINNs and related models that constitute the backbone of physics-informed machine learning. We provide a unified framework in which analysis of the various components of the error incurred by PINNs in approximating PDEs can be effectively carried out. A detailed review of available results on approximation, generalization and training errors and their behavior with respect to the type of the PDE and the dimension of the underlying domain is presented. In particular, the role of the regularity of the solutions and their stability to perturbations in the error analysis is elucidated. Numerical results are also presented to illustrate the theory. We identify training errors as a key bottleneck which can adversely affect the overall performance of various models in physics-informed machine learning. ",
    "url": "https://arxiv.org/abs/2402.10926",
    "authors": [
      "Tim De Ryck",
      "Siddhartha Mishra"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.10937",
    "title": "A Lightweight Inception Boosted U-Net Neural Network for Routability  Prediction",
    "abstract": "As the modern CPU, GPU, and NPU chip design complexity and transistor counts keep increasing, and with the relentless shrinking of semiconductor technology nodes to nearly 1 nanometer, the placement and routing have gradually become the two most pivotal processes in modern very-large-scale-integrated (VLSI) circuit back-end design. How to evaluate routability efficiently and accurately in advance (at the placement and global routing stages) has grown into a crucial research area in the field of artificial intelligence (AI) assisted electronic design automation (EDA). In this paper, we propose a novel U-Net variant model boosted by an Inception embedded module to predict Routing Congestion (RC) and Design Rule Checking (DRC) hotspots. Experimental results on the recently published CircuitNet dataset benchmark show that our proposed method achieves up to 5% (RC) and 20% (DRC) rate reduction in terms of Avg-NRMSE (Average Normalized Root Mean Square Error) compared to the classic architecture. Furthermore, our approach consistently outperforms the prior model on the SSIM (Structural Similarity Index Measure) metric. ",
    "url": "https://arxiv.org/abs/2402.10937",
    "authors": [
      "Hailiang Li",
      "Yan Huo",
      "Yan Wang",
      "Xu Yang",
      "Miaohui Hao",
      "Xiao Wang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.10940",
    "title": "Neural machine translation of clinical procedure codes for medical  diagnosis and uncertainty quantification",
    "abstract": "A Clinical Decision Support System (CDSS) is designed to enhance clinician decision-making by combining system-generated recommendations with medical expertise. Given the high costs, intensive labor, and time-sensitive nature of medical treatments, there is a pressing need for efficient decision support, especially in complex emergency scenarios. In these scenarios, where information can be limited, an advanced CDSS framework that leverages AI (artificial intelligence) models to effectively reduce diagnostic uncertainty has utility. Such an AI-enabled CDSS framework with quantified uncertainty promises to be practical and beneficial in the demanding context of real-world medical care. In this study, we introduce the concept of Medical Entropy, quantifying uncertainties in patient outcomes predicted by neural machine translation based on the ICD-9 code of procedures. Our experimental results not only show strong correlations between procedure and diagnosis sequences based on the simple ICD-9 code but also demonstrate the promising capacity to model trends of uncertainties during hospitalizations through a data-driven approach. ",
    "url": "https://arxiv.org/abs/2402.10940",
    "authors": [
      "Pei-Hung Chung",
      "Shuhan He",
      "Norawit Kijpaisalratana",
      "Abdel-badih el Ariss",
      "Byung-Jun Yoon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.10948",
    "title": "Zero-shot Explainable Mental Health Analysis on Social Media by  incorporating Mental Scales",
    "abstract": "Traditional discriminative approaches in mental health analysis are known for their strong capacity but lack interpretability and demand large-scale annotated data. On the other hand, generative approaches, such as those based on large language models (LLMs),have the potential to get rid of heavy annotations and provide explanations. However, their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is a black-box process. Inspired by the psychological assessment practice of using scales to evaluate mental states, our method incorporates two procedures via LLMs. First, the patient completes mental health questionnaires, and second, the psychologist interprets the collected information from the mental health questions and makes informed decisions. Experimental results show that our method outperforms other zero-shot methods. Our method can generate more rigorous explanation based on the outputs of mental questionnaires. ",
    "url": "https://arxiv.org/abs/2402.10948",
    "authors": [
      "Wenyu Li",
      "Yinuo Zhu",
      "Xin Lin",
      "Ming Li",
      "Ziyue Jiang",
      "Ziqian Zeng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.10964",
    "title": "Optimal feature rescaling in machine learning based on neural networks",
    "abstract": "This paper proposes a novel approach to improve the training efficiency and the generalization performance of Feed Forward Neural Networks (FFNNs) resorting to an optimal rescaling of input features (OFR) carried out by a Genetic Algorithm (GA). The OFR reshapes the input space improving the conditioning of the gradient-based algorithm used for the training. Moreover, the scale factors exploration entailed by GA trials and selection corresponds to different initialization of the first layer weights at each training attempt, thus realizing a multi-start global search algorithm (even though restrained to few weights only) which fosters the achievement of a global minimum. The approach has been tested on a FFNN modeling the outcome of a real industrial process (centerless grinding). ",
    "url": "https://arxiv.org/abs/2402.10964",
    "authors": [
      "Federico Maria Vitr\u00f2",
      "Manrco Leonesio",
      "Lorenzo Fagiano"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2402.10967",
    "title": "Social network analysis for personalized characterization and risk  assessment of alcohol use disorders in adolescents using semantic  technologies",
    "abstract": "Alcohol Use Disorder (AUD) is a major concern for public health organizations worldwide, especially as regards the adolescent population. The consumption of alcohol in adolescents is known to be influenced by seeing friends and even parents drinking alcohol. Building on this fact, a number of studies into alcohol consumption among adolescents have made use of Social Network Analysis (SNA) techniques to study the different social networks (peers, friends, family, etc.) with whom the adolescent is involved. These kinds of studies need an initial phase of data gathering by means of questionnaires and a subsequent analysis phase using the SNA techniques. The process involves a number of manual data handling stages that are time consuming and error-prone. The use of knowledge engineering techniques (including the construction of a domain ontology) to represent the information, allows the automation of all the activities, from the initial data collection to the results of the SNA study. This paper shows how a knowledge model is constructed, and compares the results obtained using the traditional method with this, fully automated model, detailing the main advantages of the latter. In the case of the SNA analysis, the validity of the results obtained with the knowledge engineering approach are compared to those obtained manually using the UCINET, Cytoscape, Pajek and Gephi to test the accuracy of the knowledge model. ",
    "url": "https://arxiv.org/abs/2402.10967",
    "authors": [
      "Jos\u00e9 Alberto Ben\u00edtez-Andrades",
      "Isa\u00edas Garc\u00eda-Rodr\u00edguez",
      "Carmen Benavides",
      "H\u00e9ctor Alaiz-Moret\u00f3n",
      "Alejandro Rodr\u00edguez-Gonz\u00e1lez"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.10974",
    "title": "On the Cross-Dataset Generalization of Machine Learning for Network  Intrusion Detection",
    "abstract": "Network Intrusion Detection Systems (NIDS) are a fundamental tool in cybersecurity. Their ability to generalize across diverse networks is a critical factor in their effectiveness and a prerequisite for real-world applications. In this study, we conduct a comprehensive analysis on the generalization of machine-learning-based NIDS through an extensive experimentation in a cross-dataset framework. We employ four machine learning classifiers and utilize four datasets acquired from different networks: CIC-IDS-2017, CSE-CIC-IDS2018, LycoS-IDS2017, and LycoS-Unicas-IDS2018. Notably, the last dataset is a novel contribution, where we apply corrections based on LycoS-IDS2017 to the well-known CSE-CIC-IDS2018 dataset. The results show nearly perfect classification performance when the models are trained and tested on the same dataset. However, when training and testing the models in a cross-dataset fashion, the classification accuracy is largely commensurate with random chance except for a few combinations of attacks and datasets. We employ data visualization techniques in order to provide valuable insights on the patterns in the data. Our analysis unveils the presence of anomalies in the data that directly hinder the classifiers capability to generalize the learned knowledge to new scenarios. This study enhances our comprehension of the generalization capabilities of machine-learning-based NIDS, highlighting the significance of acknowledging data heterogeneity. ",
    "url": "https://arxiv.org/abs/2402.10974",
    "authors": [
      "Marco Cantone",
      "Claudio Marrocco",
      "Alessandro Bria"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2402.10983",
    "title": "Quantum-Inspired Analysis of Neural Network Vulnerabilities: The Role of  Conjugate Variables in System Attacks",
    "abstract": "Neural networks demonstrate inherent vulnerability to small, non-random perturbations, emerging as adversarial attacks. Such attacks, born from the gradient of the loss function relative to the input, are discerned as input conjugates, revealing a systemic fragility within the network structure. Intriguingly, a mathematical congruence manifests between this mechanism and the quantum physics' uncertainty principle, casting light on a hitherto unanticipated interdisciplinarity. This inherent susceptibility within neural network systems is generally intrinsic, highlighting not only the innate vulnerability of these networks but also suggesting potential advancements in the interdisciplinary area for understanding these black-box networks. ",
    "url": "https://arxiv.org/abs/2402.10983",
    "authors": [
      "Jun-Jie Zhang",
      "Deyu Meng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2402.10998",
    "title": "Provably Safe Neural Network Controllers via Differential Dynamic Logic",
    "abstract": "While neural networks (NNs) have a large potential as goal-oriented controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs -- especially when safety is needed for unbounded time horizons. One reason for this is the intractability of NN and hybrid system analysis. We introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first approach for the combination of differential dynamic logic (dL) and NN verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of dL. We reflect a safety proof for a controller envelope in an NN to prove the safety of concrete NNCS on an infinite-time horizon. The NN verification properties resulting from VerSAILLE typically require nonlinear arithmetic while efficient NN verification tools merely support linear arithmetic. To overcome this divide, we present Mosaic: The first sound and complete verification approach for polynomial real arithmetic properties on piece-wise linear NNs. Mosaic lifts off-the-shelf tools for linear properties to the nonlinear setting. An evaluation on case studies, including adaptive cruise control and airborne collision avoidance, demonstrates the versatility of VerSAILLE and Mosaic: It supports the certification of infinite-time horizon safety and the exhaustive enumeration of counterexample regions while significantly outperforming State-of-the-Art tools in closed-loop NNV. ",
    "url": "https://arxiv.org/abs/2402.10998",
    "authors": [
      "Samuel Teuber",
      "Stefan Mitsch",
      "Andr\u00e9 Platzer"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2402.10999",
    "title": "Analysis and Mortality Prediction using Multiclass Classification for  Older Adults with Type 2 Diabetes",
    "abstract": "Designing proper treatment plans to manage diabetes requires health practitioners to pay heed to the individuals remaining life along with the comorbidities affecting them. Older adults with Type 2 Diabetes Mellitus (T2DM) are prone to experience premature death or even hypoglycaemia. The structured dataset utilized has 68 potential mortality predictors for 275,190 diabetic U.S. military Veterans aged 65 years or older. A new target variable is invented by combining the two original target variables. Outliers are handled by discretizing the continuous variables. Categorical variables have been dummy encoded. Class balancing is achieved by random under-sampling. A benchmark regression model is built using Multinomial Logistic Regression with LASSO. Chi-Squared and Information Gain are the filter-based feature selection techniques utilized. Classifiers such as Multinomial Logistic Regression, Random Forest, Extreme Gradient Boosting (XGBoost), and One-vs-Rest classifier are employed to build various models. Contrary to expectations, all the models have constantly underperformed. XGBoost has given the highest accuracy of 53.03 percent with Chi-Squared feature selection. All the models have consistently shown an acceptable performance for Class 3 (remaining life is more than 10 years), significantly low for Class 1 (remaining life is up to 5 years), and the worst for Class 2 (remaining life is more than 5 but up to 10 years). Features analysis has deduced that almost all input variables are associated with multiple target classes. The high dimensionality of the input data after dummy encoding seems to have confused the models, leading to misclassifications. The approach taken in this study is ineffective in producing a high-performing predictive model but lays a foundation as this problem has never been viewed from a multiclass classification perspective. ",
    "url": "https://arxiv.org/abs/2402.10999",
    "authors": [
      "Ruchika Desure",
      "Gutha Jaya Krishna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11006",
    "title": "Automated Detection and Analysis of Data Practices Using A Real-World  Corpus",
    "abstract": "Privacy policies are crucial for informing users about data practices, yet their length and complexity often deter users from reading them. In this paper, we propose an automated approach to identify and visualize data practices within privacy policies at different levels of detail. Leveraging crowd-sourced annotations from the ToS;DR platform, we experiment with various methods to match policy excerpts with predefined data practice descriptions. We further conduct a case study to evaluate our approach on a real-world policy, demonstrating its effectiveness in simplifying complex policies. Experiments show that our approach accurately matches data practice descriptions with policy excerpts, facilitating the presentation of simplified privacy information to users. ",
    "url": "https://arxiv.org/abs/2402.11006",
    "authors": [
      "Mukund Srinath",
      "Pranav Venkit",
      "Maria Badillo",
      "Florian Schaub",
      "C. Lee Giles",
      "Shomir Wilson"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11025",
    "title": "Training Bayesian Neural Networks with Sparse Subspace Variational  Inference",
    "abstract": "Bayesian neural networks (BNNs) offer uncertainty quantification but come with the downside of substantially increased training and inference costs. Sparse BNNs have been investigated for efficient inference, typically by either slowly introducing sparsity throughout the training or by post-training compression of dense BNNs. The dilemma of how to cut down massive training costs remains, particularly given the requirement to learn about the uncertainty. To solve this challenge, we introduce Sparse Subspace Variational Inference (SSVI), the first fully sparse BNN framework that maintains a consistently highly sparse Bayesian model throughout the training and inference phases. Starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters. While basis selection is characterized as a non-differentiable problem, we approximate the optimal solution with a removal-and-addition strategy, guided by novel criteria based on weight distribution statistics. Our extensive experiments show that SSVI sets new benchmarks in crafting sparse BNNs, achieving, for instance, a 10-20x compression in model size with under 3\\% performance drop, and up to 20x FLOPs reduction during training compared with dense VI training. Remarkably, SSVI also demonstrates enhanced robustness to hyperparameters, reducing the need for intricate tuning in VI and occasionally even surpassing VI-trained dense BNNs on both accuracy and uncertainty metrics. ",
    "url": "https://arxiv.org/abs/2402.11025",
    "authors": [
      "Junbo Li",
      "Zichen Miao",
      "Qiang Qiu",
      "Ruqi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.11028",
    "title": "Incremental Topological Ordering and Cycle Detection with Predictions",
    "abstract": "This paper leverages the framework of algorithms-with-predictions to design data structures for two fundamental dynamic graph problems: incremental topological ordering and cycle detection. In these problems, the input is a directed graph on $n$ nodes, and the $m$ edges arrive one by one. The data structure must maintain a topological ordering of the vertices at all times and detect if the newly inserted edge creates a cycle. The theoretically best worst-case algorithms for these problems have high update cost (polynomial in $n$ and $m$). In practice, greedy heuristics (that recompute the solution from scratch each time) perform well but can have high update cost in the worst case. In this paper, we bridge this gap by leveraging predictions to design a learned new data structure for the problems. Our data structure guarantees consistency, robustness, and smoothness with respect to predictions -- that is, it has the best possible running time under perfect predictions, never performs worse than the best-known worst-case methods, and its running time degrades smoothly with the prediction error. Moreover, we demonstrate empirically that predictions, learned from a very small training dataset, are sufficient to provide significant speed-ups on real datasets. ",
    "url": "https://arxiv.org/abs/2402.11028",
    "authors": [
      "Samuel McCauley",
      "Benjamin Moseley",
      "Aidin Niaparast",
      "Shikha Singh"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2402.11039",
    "title": "Robustness to Subpopulation Shift with Domain Label Noise via  Regularized Annotation of Domains",
    "abstract": "Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA) rely heavily on well-annotated groups in the training data. We show, both in theory and practice, that annotation-based data augmentations using either downsampling or upweighting for WGA are susceptible to domain annotation noise, and in high-noise regimes approach the WGA of a model trained with vanilla empirical risk minimization. We introduce Regularized Annotation of Domains (RAD) in order to train robust last layer classifiers without the need for explicit domain annotations. Our results show that RAD is competitive with other recently proposed domain annotation-free techniques. Most importantly, RAD outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the training data for several publicly available datasets. ",
    "url": "https://arxiv.org/abs/2402.11039",
    "authors": [
      "Nathan Stromberg",
      "Rohan Ayyagari",
      "Monica Welfert",
      "Sanmi Koyejo",
      "Lalitha Sankar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.11051",
    "title": "Large Language Models Fall Short: Understanding Complex Relationships in  Detective Narratives",
    "abstract": "Existing datasets for narrative understanding often fail to represent the complexity and uncertainty of relationships in real-life social scenarios. To address this gap, we introduce a new benchmark, Conan, designed for extracting and analysing intricate character relation graphs from detective narratives. Specifically, we designed hierarchical relationship categories and manually extracted and annotated role-oriented relationships from the perspectives of various characters, incorporating both public relationships known to most characters and secret ones known to only a few. Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives. The combination of the Conan dataset and our pipeline strategy is geared towards understanding the ability of LLMs to comprehend nuanced relational dynamics in narrative contexts. ",
    "url": "https://arxiv.org/abs/2402.11051",
    "authors": [
      "Runcong Zhao",
      "Qinglin Zhu",
      "Hainiu Xu",
      "Jiazheng Li",
      "Yuxiang Zhou",
      "Yulan He",
      "Lin Gui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11060",
    "title": "Persona-DB: Efficient Large Language Model Personalization for Response  Prediction with Collaborative Data Refinement",
    "abstract": "The increasing demand for personalized interactions with large language models (LLMs) calls for the development of methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to effectively bridge knowledge gaps among users. In the task of response forecasting, Persona-DB demonstrates superior efficiency in maintaining accuracy with a significantly reduced retrieval size, a critical advantage in scenarios with extensive histories or limited context windows. Our experiments also indicate a marked improvement of over 15% under cold-start scenarios, when users have extremely sparse data. Furthermore, our analysis reveals the increasing importance of collaborative knowledge as the retrieval capacity expands. ",
    "url": "https://arxiv.org/abs/2402.11060",
    "authors": [
      "Chenkai Sun",
      "Ke Yang",
      "Revanth Gangi Reddy",
      "Yi R. Fung",
      "Hou Pong Chan",
      "ChengXiang Zhai",
      "Heng Ji"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2402.11068",
    "title": "Bridging Causal Discovery and Large Language Models: A Comprehensive  Survey of Integrative Approaches and Future Directions",
    "abstract": "Causal discovery (CD) and Large Language Models (LLMs) represent two emerging fields of study with significant implications for artificial intelligence. Despite their distinct origins, CD focuses on uncovering cause-effect relationships from data, and LLMs on processing and generating humanlike text, the convergence of these domains offers novel insights and methodologies for understanding complex systems. This paper presents a comprehensive survey of the integration of LLMs, such as GPT4, into CD tasks. We systematically review and compare existing approaches that leverage LLMs for various CD tasks and highlight their innovative use of metadata and natural language to infer causal structures. Our analysis reveals the strengths and potential of LLMs in both enhancing traditional CD methods and as an imperfect expert, alongside the challenges and limitations inherent in current practices. Furthermore, we identify gaps in the literature and propose future research directions aimed at harnessing the full potential of LLMs in causality research. To our knowledge, this is the first survey to offer a unified and detailed examination of the synergy between LLMs and CD, setting the stage for future advancements in the field. ",
    "url": "https://arxiv.org/abs/2402.11068",
    "authors": [
      "Guangya Wan",
      "Yuqi Wu",
      "Mengxuan Hu",
      "Zhixuan Chu",
      "Sheng Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11073",
    "title": "AFaCTA: Assisting the Annotation of Factual Claim Detection with  Reliable LLM Annotators",
    "abstract": "With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics. ",
    "url": "https://arxiv.org/abs/2402.11073",
    "authors": [
      "Jingwei Ni",
      "Minjing Shi",
      "Dominik Stammbach",
      "Mrinmaya Sachan",
      "Elliott Ash",
      "Markus Leippold"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11083",
    "title": "VQAttack: Transferable Adversarial Attacks on Visual Question Answering  via Pre-trained Models",
    "abstract": "Visual Question Answering (VQA) is a fundamental task in computer vision and natural language process fields. Although the ``pre-training & finetuning'' learning paradigm significantly improves the VQA performance, the adversarial robustness of such a learning paradigm has not been explored. In this paper, we delve into a new problem: using a pre-trained multimodal source model to create adversarial image-text pairs and then transferring them to attack the target VQA models. Correspondingly, we propose a novel VQAttack model, which can iteratively generate both image and text perturbations with the designed modules: the large language model (LLM)-enhanced image attack and the cross-modal joint attack module. At each iteration, the LLM-enhanced image attack module first optimizes the latent representation-based loss to generate feature-level image perturbations. Then it incorporates an LLM to further enhance the image perturbations by optimizing the designed masked answer anti-recovery loss. The cross-modal joint attack module will be triggered at a specific iteration, which updates the image and text perturbations sequentially. Notably, the text perturbation updates are based on both the learned gradients in the word embedding space and word synonym-based substitution. Experimental results on two VQA datasets with five validated models demonstrate the effectiveness of the proposed VQAttack in the transferable attack setting, compared with state-of-the-art baselines. This work reveals a significant blind spot in the ``pre-training & fine-tuning'' paradigm on VQA tasks. Source codes will be released. ",
    "url": "https://arxiv.org/abs/2402.11083",
    "authors": [
      "Ziyi Yin",
      "Muchao Ye",
      "Tianrong Zhang",
      "Jiaqi Wang",
      "Han Liu",
      "Jinghui Chen",
      "Ting Wang",
      "Fenglong Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11093",
    "title": "Modular Graph Extraction for Handwritten Circuit Diagram Images",
    "abstract": "As digitization in engineering progressed, circuit diagrams (also referred to as schematics) are typically developed and maintained in computer-aided engineering (CAE) systems, thus allowing for automated verification, simulation and further processing in downstream engineering steps. However, apart from printed legacy schematics, hand-drawn circuit diagrams are still used today in the educational domain, where they serve as an easily accessible mean for trainees and students to learn drawing this type of diagrams. Furthermore, hand-drawn schematics are typically used in examinations due to legal constraints. In order to harness the capabilities of digital circuit representations, automated means for extracting the electrical graph from raster graphics are required. While respective approaches have been proposed in literature, they are typically conducted on small or non-disclosed datasets. This paper describes a modular end-to-end solution on a larger, public dataset, in which approaches for the individual sub-tasks are evaluated to form a new baseline. These sub-tasks include object detection (for electrical symbols and texts), binary segmentation (drafter's stroke vs. background), handwritten character recognition and orientation regression for electrical symbols and texts. Furthermore, computer-vision graph assembly and rectification algorithms are presented. All methods are integrated in a publicly available prototype. ",
    "url": "https://arxiv.org/abs/2402.11093",
    "authors": [
      "Johannes Bayer",
      "Leo van Waveren",
      "Andreas Dengel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11109",
    "title": "Online Flexible Busy Time Scheduling on Heterogeneous Machines",
    "abstract": "We study the online busy time scheduling model on heterogeneous machines. In our setting, unit-length jobs arrive online with a deadline that is known to the algorithm at the job's arrival time. An algorithm has access to machines, each with different associated capacities and costs. The goal is to schedule jobs on machines before their deadline, so that the total cost incurred by the scheduling algorithm is minimized. Relatively little is known about online busy time scheduling when machines are heterogeneous (i.e., have different costs and capacities), despite this being the most practical model for clients using cloud computing services. We make significant progress in understanding this model by designing an 8-competitive algorithm for the problem on unit-length jobs, and providing a lower bound on the competitive ratio of 2. We further prove that our lower bound is tight in the natural setting when jobs have agreeable deadlines. ",
    "url": "https://arxiv.org/abs/2402.11109",
    "authors": [
      "Gruia Calinescu",
      "Sami Davies",
      "Samir Khuller",
      "Shirley Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2402.11120",
    "title": "DART: A Principled Approach to Adversarially Robust Unsupervised Domain  Adaptation",
    "abstract": "Distribution shifts and adversarial examples are two major challenges for deploying machine learning models. While these challenges have been studied individually, their combination is an important topic that remains relatively under-explored. In this work, we study the problem of adversarial robustness under a common setting of distribution shift - unsupervised domain adaptation (UDA). Specifically, given a labeled source domain $D_S$ and an unlabeled target domain $D_T$ with related but different distributions, the goal is to obtain an adversarially robust model for $D_T$. The absence of target domain labels poses a unique challenge, as conventional adversarial robustness defenses cannot be directly applied to $D_T$. To address this challenge, we first establish a generalization bound for the adversarial target loss, which consists of (i) terms related to the loss on the data, and (ii) a measure of worst-case domain divergence. Motivated by this bound, we develop a novel unified defense framework called Divergence Aware adveRsarial Training (DART), which can be used in conjunction with a variety of standard UDA methods; e.g., DANN [Ganin and Lempitsky, 2015]. DART is applicable to general threat models, including the popular $\\ell_p$-norm model, and does not require heuristic regularizers or architectural changes. We also release DomainRobust: a testbed for evaluating robustness of UDA models to adversarial attacks. DomainRobust consists of 4 multi-domain benchmark datasets (with 46 source-target pairs) and 7 meta-algorithms with a total of 11 variants. Our large-scale experiments demonstrate that on average, DART significantly enhances model robustness on all benchmarks compared to the state of the art, while maintaining competitive standard accuracy. The relative improvement in robustness from DART reaches up to 29.2% on the source-target domain pairs considered. ",
    "url": "https://arxiv.org/abs/2402.11120",
    "authors": [
      "Yunjuan Wang",
      "Hussein Hazimeh",
      "Natalia Ponomareva",
      "Alexey Kurakin",
      "Ibrahim Hammoud",
      "Raman Arora"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.11124",
    "title": "Disentanglement in Implicit Causal Models via Switch Variable",
    "abstract": "Learning causal representations from observational and interventional data in the absence of known ground-truth graph structures necessitates implicit latent causal representation learning. Implicitly learning causal mechanisms typically involves two categories of interventional data: hard and soft interventions. In real-world scenarios, soft interventions are often more realistic than hard interventions, as the latter require fully controlled environments. Unlike hard interventions, which directly force changes in a causal variable, soft interventions exert influence indirectly by affecting the causal mechanism. In this paper, we tackle implicit latent causal representation learning in a Variational Autoencoder (VAE) framework through soft interventions. Our approach models soft interventions effects by employing a causal mechanism switch variable designed to toggle between different causal mechanisms. In our experiments, we consistently observe improved learning of identifiable, causal representations, compared to baseline approaches. ",
    "url": "https://arxiv.org/abs/2402.11124",
    "authors": [
      "Shayan Shirahmad Gale Bagi",
      "Zahra Gharaee",
      "Oliver Schulte",
      "Mark Crowley"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11126",
    "title": "Kolmogorov n-Widths for Multitask Physics-Informed Machine Learning  (PIML) Methods: Towards Robust Metrics",
    "abstract": "Physics-informed machine learning (PIML) as a means of solving partial differential equations (PDE) has garnered much attention in the Computational Science and Engineering (CS&E) world. This topic encompasses a broad array of methods and models aimed at solving a single or a collection of PDE problems, called multitask learning. PIML is characterized by the incorporation of physical laws into the training process of machine learning models in lieu of large data when solving PDE problems. Despite the overall success of this collection of methods, it remains incredibly difficult to analyze, benchmark, and generally compare one approach to another. Using Kolmogorov n-widths as a measure of effectiveness of approximating functions, we judiciously apply this metric in the comparison of various multitask PIML architectures. We compute lower accuracy bounds and analyze the model's learned basis functions on various PDE problems. This is the first objective metric for comparing multitask PIML architectures and helps remove uncertainty in model validation from selective sampling and overfitting. We also identify avenues of improvement for model architectures, such as the choice of activation function, which can drastically affect model generalization to \"worst-case\" scenarios, which is not observed when reporting task-specific errors. We also incorporate this metric into the optimization process through regularization, which improves the models' generalizability over the multitask PDE problem. ",
    "url": "https://arxiv.org/abs/2402.11126",
    "authors": [
      "Michael Penwarden",
      "Houman Owhadi",
      "Robert M. Kirby"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2402.11137",
    "title": "TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks",
    "abstract": "While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs by developing context optimization techniques for PFNs. Specifically, we propose TuneTables, a novel prompt-tuning strategy that compresses large datasets into a smaller learned context. TuneTables scales TabPFN to be competitive with state-of-the-art tabular classification methods on larger datasets, while having a substantially lower inference time than TabPFN. Furthermore, we show that TuneTables can be used as an interpretability tool and can even be used to mitigate biases by optimizing a fairness objective. ",
    "url": "https://arxiv.org/abs/2402.11137",
    "authors": [
      "Benjamin Feuer",
      "Robin Tibor Schirrmeister",
      "Valeriia Cherepanova",
      "Chinmay Hegde",
      "Frank Hutter",
      "Micah Goldblum",
      "Niv Cohen",
      "Colin White"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11139",
    "title": "LiGNN: Graph Neural Networks at LinkedIn",
    "abstract": "In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks (GNNs) Framework. We share our insight on developing and deployment of GNNs at large scale at LinkedIn. We present a set of algorithmic improvements to the quality of GNN representation learning including temporal graph architectures with long term losses, effective cold start solutions via graph densification, ID embeddings and multi-hop neighbor sampling. We explain how we built and sped up by 7x our large-scale training on LinkedIn graphs with adaptive sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. We summarize our deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5% of Feed engaged daily active users, 0.2% session lift and 0.1% weekly active user lift from people recommendation. We believe that this work can provide practical solutions and insights for engineers who are interested in applying Graph neural networks at large scale. ",
    "url": "https://arxiv.org/abs/2402.11139",
    "authors": [
      "Fedor Borisyuk",
      "Shihai He",
      "Yunbo Ouyang",
      "Morteza Ramezani",
      "Peng Du",
      "Xiaochen Hou",
      "Chengming Jiang",
      "Nitin Pasumarthy",
      "Priya Bannur",
      "Birjodh Tiwana",
      "Ping Liu",
      "Siddharth Dangi",
      "Daqi Sun",
      "Zhoutao Pei",
      "Xiao Shi",
      "Sirou Zhu",
      "Qianqi Shen",
      "Kuang-Hsuan Lee",
      "David Stein",
      "Baolei Li",
      "Haichao Wei",
      "Amol Ghoting",
      "Souvik Ghosh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11141",
    "title": "Semantically-aware Neural Radiance Fields for Visual Scene  Understanding: A Comprehensive Review",
    "abstract": "This review thoroughly examines the role of semantically-aware Neural Radiance Fields (NeRFs) in visual scene understanding, covering an analysis of over 250 scholarly papers. It explores how NeRFs adeptly infer 3D representations for both stationary and dynamic objects in a scene. This capability is pivotal for generating high-quality new viewpoints, completing missing scene details (inpainting), conducting comprehensive scene segmentation (panoptic segmentation), predicting 3D bounding boxes, editing 3D scenes, and extracting object-centric 3D models. A significant aspect of this study is the application of semantic labels as viewpoint-invariant functions, which effectively map spatial coordinates to a spectrum of semantic labels, thus facilitating the recognition of distinct objects within the scene. Overall, this survey highlights the progression and diverse applications of semantically-aware neural radiance fields in the context of visual scene interpretation. ",
    "url": "https://arxiv.org/abs/2402.11141",
    "authors": [
      "Thang-Anh-Quan Nguyen",
      "Amine Bourki",
      "M\u00e1ty\u00e1s Macudzinski",
      "Anthony Brunel",
      "Mohammed Bennamoun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11153",
    "title": "Beyond Generalization: A Survey of Out-Of-Distribution Adaptation on  Graphs",
    "abstract": "Distribution shifts on graphs -- the data distribution discrepancies between training and testing a graph machine learning model, are often ubiquitous and unavoidable in real-world scenarios. Such shifts may severely deteriorate the performance of the model, posing significant challenges for reliable graph machine learning. Consequently, there has been a surge in research on graph Out-Of-Distribution (OOD) adaptation methods that aim to mitigate the distribution shifts and adapt the knowledge from one distribution to another. In our survey, we provide an up-to-date and forward-looking review of graph OOD adaptation methods, covering two main problem scenarios including training-time as well as test-time graph OOD adaptation. We start by formally formulating the two problems and then discuss different types of distribution shifts on graphs. Based on our proposed taxonomy for graph OOD adaptation, we systematically categorize the existing methods according to their learning paradigm and investigate the techniques behind them. Finally, we point out promising research directions and the corresponding challenges. We also provide a continuously updated reading list at https://github.com/kaize0409/Awesome-Graph-OOD-Adaptation.git ",
    "url": "https://arxiv.org/abs/2402.11153",
    "authors": [
      "Shuhan Liu",
      "Kaize Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11163",
    "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning  over Knowledge Graph",
    "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released. ",
    "url": "https://arxiv.org/abs/2402.11163",
    "authors": [
      "Jinhao Jiang",
      "Kun Zhou",
      "Wayne Xin Zhao",
      "Yang Song",
      "Chen Zhu",
      "Hengshu Zhu",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11166",
    "title": "GenDec: A robust generative Question-decomposition method for Multi-hop  reasoning",
    "abstract": "Multi-hop QA (MHQA) involves step-by-step reasoning to answer complex questions and find multiple relevant supporting facts. However, Existing large language models'(LLMs) reasoning ability in multi-hop question answering remains exploration, which is inadequate in answering multi-hop questions. Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach the right final answer. In this paper, we propose a \\textbf{gen}erative question \\textbf{dec}omposition method (GenDec) from the perspective of explainable QA by generating independent and complete sub-questions based on incorporating additional extracted evidence for enhancing LLMs' reasoning ability in RAG. To demonstrate the impact, generalization, and robustness of Gendec, we conduct two experiments, the first is combining GenDec with small QA systems on paragraph retrieval and QA tasks. We secondly examine the reasoning capabilities of various state-of-the-art LLMs including GPT-4 and GPT-3.5 combined with GenDec. We experiment on the HotpotQA, 2WikihopMultiHopQA, MuSiQue, and PokeMQA datasets. ",
    "url": "https://arxiv.org/abs/2402.11166",
    "authors": [
      "Jian Wu",
      "Linyi Yang",
      "Yuliang Ji",
      "Wenhao Huang",
      "B\u00f6rje F. Karlsson",
      "Manabu Okumura"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11167",
    "title": "Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated  Text Detection",
    "abstract": "The robustness of AI-content detection models against cultivated attacks (e.g., paraphrasing or word switching) remains a significant concern. This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches. We explore the ensemble attack strategy by completing the prompt with the next token generated from random candidate LLMs. We find the token-ensemble approach significantly drops the performance of AI-content detection models (The code and test sets will be released). Our findings reveal that token-ensemble generation poses a vital challenge to current detection models and underlines the need for advancing detection technologies to counter sophisticated adversarial strategies. ",
    "url": "https://arxiv.org/abs/2402.11167",
    "authors": [
      "Fan Huang",
      "Haewoon Kwak",
      "Jisun An"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11175",
    "title": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text  Detection",
    "abstract": "The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark involving multilingual, multi-domain and multi-generator for MGT detection -- M4GT-Bench. It is collected for three task formulations: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection identifies which particular model generates the text; and (3) human-machine mixed text detection, where a word boundary delimiting MGT from human-written content should be determined. Human evaluation for Task 2 shows less than random guess performance, demonstrating the challenges to distinguish unique LLMs. Promising results always occur when training and test data distribute within the same domain or generators. ",
    "url": "https://arxiv.org/abs/2402.11175",
    "authors": [
      "Yuxia Wang",
      "Jonibek Mansurov",
      "Petar Ivanov",
      "Jinyan Su",
      "Artem Shelmanov",
      "Akim Tsvigun",
      "Osama Mohanned Afzal",
      "Tarek Mahmoud",
      "Giovanni Puccetti",
      "Thomas Arnold",
      "Alham Fikri Aji",
      "Nizar Habash",
      "Iryna Gurevych",
      "Preslav Nakov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11179",
    "title": "Uncertainty Quantification of Graph Convolution Neural Network Models of  Evolving Processes",
    "abstract": "The application of neural network models to scientific machine learning tasks has proliferated in recent years. In particular, neural network models have proved to be adept at modeling processes with spatial-temporal complexity. Nevertheless, these highly parameterized models have garnered skepticism in their ability to produce outputs with quantified error bounds over the regimes of interest. Hence there is a need to find uncertainty quantification methods that are suitable for neural networks. In this work we present comparisons of the parametric uncertainty quantification of neural networks modeling complex spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational gradient descent and its projected variant. Specifically we apply these methods to graph convolutional neural network models of evolving systems modeled with recurrent neural network and neural ordinary differential equations architectures. We show that Stein variational inference is a viable alternative to Monte Carlo methods with some clear advantages for complex neural network models. For our exemplars, Stein variational interference gave similar uncertainty profiles through time compared to Hamiltonian Monte Carlo, albeit with generally more generous variance.Projected Stein variational gradient descent also produced similar uncertainty profiles to the non-projected counterpart, but large reductions in the active weight space were confounded by the stability of the neural network predictions and the convoluted likelihood landscape. ",
    "url": "https://arxiv.org/abs/2402.11179",
    "authors": [
      "Jeremiah Hauth",
      "Cosmin Safta",
      "Xun Huan",
      "Ravi G. Patel",
      "Reese E. Jones"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2402.11184",
    "title": "An efficient preconditioner for a class of non-Hermitian two-by-two  block complex system of linear equations",
    "abstract": "We present an efficient preconditioner for two-by-two block system of linear equations with the coefficient matrix $ \\begin{pmatrix} F & -G^H G & F \\end{pmatrix}$ where $F\\in\\mathbb{C}^{n\\times n}$ is Hermitian positive definite and $G\\in\\mathbb{C}^{n\\times n}$ is positive semidefinite. Spectral analysis of the preconditioned matrix is analyzed. In each iteration of a Krylov subspace method, like GMRES, for solving the preconditioned system in conjunction with proposed preconditioner two subsystems with Hermitian positive definite coefficient matrix should be solved which can be accomplished exactly using the Cholesky factorization or inexactly using the conjugate gradient method. Application of the proposed preconditioner to the systems arising from finite element discretization of PDE-constrained optimization problems is presented. Numerical results are given to demonstrate the efficiency of the preconditioner. ",
    "url": "https://arxiv.org/abs/2402.11184",
    "authors": [
      "Owe Axelsson",
      "Dovod Khojasteh Slakuyeh"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2402.11191",
    "title": "Knowledge Graph Assisted Automatic Sports News Writing",
    "abstract": "In this paper, we present a novel method for automatically generating sports news, which employs a unique algorithm that extracts pivotal moments from live text broadcasts and uses them to create an initial draft of the news. This draft is further refined by incorporating key details and background information from a specially designed sports knowledge graph. This graph contains 5,893 entities, which are classified into three distinct conceptual categories, interconnected through four relationship types, and characterized by 27 unique attributes. In addition, we create a multi-stage learning model by combining convolutional neural networks and a transformer encoder. This model expresses entity-task interactions using convolutional neural networks and enriches entity representations in the query set with the transformer encoder. It also includes a processor to compute matching scores for incomplete triples, addressing few-shot knowledge graph completion problem. The efficiency of this approach has been confirmed through both subjective and objective evaluations of 50 selected test cases, demonstrating its capability in revolutionizing the creation of sports news. ",
    "url": "https://arxiv.org/abs/2402.11191",
    "authors": [
      "Yang Cao",
      "Xinyi Chen",
      "Xin Zhang",
      "Siying Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11193",
    "title": "Privacy Impact Assessments in the Wild: A Scoping Review",
    "abstract": "Privacy Impact Assessments (PIAs) offer a systematic process for assessing the privacy impacts of a project or system. As a privacy engineering strategy, PIAs are heralded as one of the main approaches to privacy by design, supporting the early identification of threats and controls. However, there is still a shortage of empirical evidence on their uptake and proven effectiveness in practice. To better understand the current state of literature and research, this paper provides a comprehensive Scoping Review (ScR) on the topic of PIAs \"in the wild\", following the well-established Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines. As a result, this ScR includes 45 studies, providing an extensive synthesis of the existing body of knowledge, classifying types of research and publications, appraising the methodological quality of primary research, and summarising the positive and negative aspects of PIAs in practice, as reported by studies. This ScR also identifies significant research gaps (e.g., evidence gaps from contradictory results and methodological gaps from research design deficiencies), future research pathways, and implications for researchers, practitioners, and policymakers developing and evaluating PIA frameworks. As we conclude, there is still a significant need for more primary research on the topic, both qualitative and quantitative. A critical appraisal of qualitative studies (n=28) revealed deficiencies in the methodological quality, and only four quantitative studies were identified, suggesting that current primary research remains incipient. Nonetheless, PIAs can be regarded as a prominent sub-area in the broader field of Empirical Privacy Engineering, warranting further research toward more evidence-based practices. ",
    "url": "https://arxiv.org/abs/2402.11193",
    "authors": [
      "Leonardo Horn Iwaya",
      "Ala Sarah Alaqra",
      "Marit Hansen",
      "Simone Fischer-H\u00fcbner"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.11196",
    "title": "Maintaining Adversarial Robustness in Continuous Learning",
    "abstract": "Adversarial robustness is essential for security and reliability of machine learning systems. However, the adversarial robustness gained by sophisticated defense algorithms is easily erased as the neural network evolves to learn new tasks. This vulnerability can be addressed by fostering a novel capability for neural networks, termed continual robust learning, which focuses on both the (classification) performance and adversarial robustness on previous tasks during continuous learning. To achieve continuous robust learning, we propose an approach called Double Gradient Projection that projects the gradients for weight updates orthogonally onto two crucial subspaces -- one for stabilizing the smoothed sample gradients and another for stabilizing the final outputs of the neural network. The experimental results on four benchmarks demonstrate that the proposed approach effectively maintains continuous robustness against strong adversarial attacks, outperforming the baselines formed by combining the existing defense strategies and continual learning methods. ",
    "url": "https://arxiv.org/abs/2402.11196",
    "authors": [
      "Xiaolei Ru",
      "Xiaowei Cao",
      "Zijia Liu",
      "Jack Murdoch Moore",
      "Xin-Ya Zhang",
      "Xia Zhu",
      "Wenjia Wei",
      "Gang Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11198",
    "title": "Achieving Linear Speedup in Asynchronous Federated Learning with  Heterogeneous Clients",
    "abstract": "Federated learning (FL) is an emerging distributed training paradigm that aims to learn a common global model without exchanging or transferring the data that are stored locally at different clients. The Federated Averaging (FedAvg)-based algorithms have gained substantial popularity in FL to reduce the communication overhead, where each client conducts multiple localized iterations before communicating with a central server. In this paper, we focus on FL where the clients have diverse computation and/or communication capabilities. Under this circumstance, FedAvg can be less efficient since it requires all clients that participate in the global aggregation in a round to initiate iterations from the latest global model, and thus the synchronization among fast clients and straggler clients can severely slow down the overall training process. To address this issue, we propose an efficient asynchronous federated learning (AFL) framework called Delayed Federated Averaging (DeFedAvg). In DeFedAvg, the clients are allowed to perform local training with different stale global models at their own paces. Theoretical analyses demonstrate that DeFedAvg achieves asymptotic convergence rates that are on par with the results of FedAvg for solving nonconvex problems. More importantly, DeFedAvg is the first AFL algorithm that provably achieves the desirable linear speedup property, which indicates its high scalability. Additionally, we carry out extensive numerical experiments using real datasets to validate the efficiency and scalability of our approach when training deep neural networks. ",
    "url": "https://arxiv.org/abs/2402.11198",
    "authors": [
      "Xiaolu Wang",
      "Zijian Li",
      "Shi Jin",
      "Jun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2402.11199",
    "title": "Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with  Knowledge Graphs",
    "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities when prompted to generate chain-of-thought (CoT) explanations alongside answers. However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT. In this paper, we delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question answering by utilizing knowledge graphs (KGs). We propose a novel discriminative and generative CoT evaluation paradigm to assess LLMs' knowledge of reasoning and the accuracy of the generated CoT. Through experiments conducted on 5 different families of LLMs across 2 multi-hop question-answering datasets, we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorrect reasoning. ",
    "url": "https://arxiv.org/abs/2402.11199",
    "authors": [
      "Minh-Vuong Nguyen",
      "Linhao Luo",
      "Fatemeh Shiri",
      "Dinh Phung",
      "Yuan-Fang Li",
      "Thuy-Trang Vu",
      "Gholamreza Haffari"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11208",
    "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based  Agents",
    "abstract": "Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided into two subcategories based on trigger locations: the backdoor trigger can be hidden either in the user query or in an intermediate observation returned by the external environment. We propose the corresponding data poisoning mechanisms to implement the above variations of agent backdoor attacks on two typical agent tasks, web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content. ",
    "url": "https://arxiv.org/abs/2402.11208",
    "authors": [
      "Wenkai Yang",
      "Xiaohan Bi",
      "Yankai Lin",
      "Sishuo Chen",
      "Jie Zhou",
      "Xu Sun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11218",
    "title": "Controlled Text Generation for Large Language Model with Dynamic  Attribute Graphs",
    "abstract": "Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29% over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant decrease in perplexity, markedly improving text fluency. ",
    "url": "https://arxiv.org/abs/2402.11218",
    "authors": [
      "Xun Liang",
      "Hanyu Wang",
      "Shichao Song",
      "Mengting Hu",
      "Xunzhi Wang",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Bo Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11224",
    "title": "Neural Networks with (Low-Precision) Polynomial Approximations: New  Insights and Techniques for Accuracy Improvement",
    "abstract": "Replacing non-polynomial functions (e.g., non-linear activation functions such as ReLU) in a neural network with their polynomial approximations is a standard practice in privacy-preserving machine learning. The resulting neural network, called polynomial approximation of neural network (PANN) in this paper, is compatible with advanced cryptosystems to enable privacy-preserving model inference. Using ``highly precise'' approximation, state-of-the-art PANN offers similar inference accuracy as the underlying backbone model. However, little is known about the effect of approximation, and existing literature often determined the required approximation precision empirically. In this paper, we initiate the investigation of PANN as a standalone object. Specifically, our contribution is two-fold. Firstly, we provide an explanation on the effect of approximate error in PANN. In particular, we discovered that (1) PANN is susceptible to some type of perturbations; and (2) weight regularisation significantly reduces PANN's accuracy. We support our explanation with experiments. Secondly, based on the insights from our investigations, we propose solutions to increase inference accuracy for PANN. Experiments showed that combination of our solutions is very effective: at the same precision, our PANN is 10% to 50% more accurate than state-of-the-arts; and at the same accuracy, our PANN only requires a precision of $2^{-9}$ while state-of-the-art solution requires a precision of $2^{-12}$ using the ResNet-20 model on CIFAR-10 dataset. ",
    "url": "https://arxiv.org/abs/2402.11224",
    "authors": [
      "Chi Zhang",
      "Man Ho Au",
      "Siu Ming Yiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.11231",
    "title": "Enhancing Security in Blockchain Networks: Anomalies, Frauds, and  Advanced Detection Techniques",
    "abstract": "Blockchain technology, a foundational distributed ledger system, enables secure and transparent multi-party transactions. Despite its advantages, blockchain networks are susceptible to anomalies and frauds, posing significant risks to their integrity and security. This paper offers a detailed examination of blockchain's key definitions and properties, alongside a thorough analysis of the various anomalies and frauds that undermine these networks. It describes an array of detection and prevention strategies, encompassing statistical and machine learning methods, game-theoretic solutions, digital forensics, reputation-based systems, and comprehensive risk assessment techniques. Through case studies, we explore practical applications of anomaly and fraud detection in blockchain networks, extracting valuable insights and implications for both current practice and future research. Moreover, we spotlight emerging trends and challenges within the field, proposing directions for future investigation and technological development. Aimed at both practitioners and researchers, this paper seeks to provide a technical, in-depth overview of anomaly and fraud detection within blockchain networks, marking a significant step forward in the search for enhanced network security and reliability. ",
    "url": "https://arxiv.org/abs/2402.11231",
    "authors": [
      "Joerg Osterrieder",
      "Stephen Chan",
      "Jeffrey Chu",
      "Yuanyuan Zhang",
      "Branka Hadji Misheva",
      "Codruta Mare"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "General Finance (q-fin.GN)"
    ]
  },
  {
    "id": "arXiv:2402.11235",
    "title": "ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs",
    "abstract": "With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to zero-shot transferability in graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a prompt-based subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using prompting nodes and neighborhood aggregation, respectively. We further adopt a lightweight fine-tuning strategy that reduces the risk of overfitting and maintains the zero-shot learning efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset zero-shot transferability, opening pathways for the development of graph foundation models. Especially, ZeroG, as a zero-shot method, can even achieve results comparable to those of semi-supervised learning on Pubmed. ",
    "url": "https://arxiv.org/abs/2402.11235",
    "authors": [
      "Yuhan Li",
      "Peisong Wang",
      "Zhixun Li",
      "Jeffrey Xu Yu",
      "Jia Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11245",
    "title": "AI Model Placement for 6G Networks under Epistemic Uncertainty  Estimation",
    "abstract": "The adoption of Artificial Intelligence (AI) based Virtual Network Functions (VNFs) has witnessed significant growth, posing a critical challenge in orchestrating AI models within next-generation 6G networks. Finding optimal AI model placement is significantly more challenging than placing traditional software-based VNFs, due to the introduction of numerous uncertain factors by AI models, such as varying computing resource consumption, dynamic storage requirements, and changing model performance. To address the AI model placement problem under uncertainties, this paper presents a novel approach employing a sequence-to-sequence (S2S) neural network which considers uncertainty estimations. The S2S model, characterized by its encoding-decoding architecture, is designed to take the service chain with a number of AI models as input and produce the corresponding placement of each AI model. To address the introduced uncertainties, our methodology incorporates the orthonormal certificate module for uncertainty estimation and utilizes fuzzy logic for uncertainty representation, thereby enhancing the capabilities of the S2S model. Experiments demonstrate that the proposed method achieves competitive results across diverse AI model profiles, network environments, and service chain requests. ",
    "url": "https://arxiv.org/abs/2402.11245",
    "authors": [
      "Liming Huang",
      "Yulei Wu",
      "Juan Marcelo Parra-Ullauri",
      "Reza Nejabati",
      "Dimitra Simeonidou"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2402.11262",
    "title": "Mirror Gradient: Towards Robust Multimodal Recommender Systems via  Exploring Flat Local Minima",
    "abstract": "Multimodal recommender systems utilize various types of information to model user preferences and item features, helping users discover items aligned with their interests. The integration of multimodal information mitigates the inherent challenges in recommender systems, e.g., the data sparsity problem and cold-start issues. However, it simultaneously magnifies certain risks from multimodal information inputs, such as information adjustment risk and inherent noise risk. These risks pose crucial challenges to the robustness of recommendation models. In this paper, we analyze multimodal recommender systems from the novel perspective of flat local minima and propose a concise yet effective gradient strategy called Mirror Gradient (MG). This strategy can implicitly enhance the model's robustness during the optimization process, mitigating instability risks arising from multimodal information inputs. We also provide strong theoretical evidence and conduct extensive empirical experiments to show the superiority of MG across various multimodal recommendation models and benchmarks. Furthermore, we find that the proposed MG can complement existing robust training methods and be easily extended to diverse advanced recommendation models, making it a promising new and fundamental paradigm for training multimodal recommender systems. The code is released at https://github.com/Qrange-group/Mirror-Gradient. ",
    "url": "https://arxiv.org/abs/2402.11262",
    "authors": [
      "Shanshan Zhong",
      "Zhongzhan Huang",
      "Daifeng Li",
      "Wushao Wen",
      "Jinghui Qin",
      "Liang Lin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11273",
    "title": "Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo  Labeling Leveraging Strong and Weak Data Augmentation Strategies",
    "abstract": "Traditional supervised learning methods have historically encountered certain constraints in medical image segmentation due to the challenging collection process, high labeling cost, low signal-to-noise ratio, and complex features characterizing biomedical images. This paper proposes a semi-supervised model, DFCPS, which innovatively incorporates the Fixmatch concept. This significantly enhances the model's performance and generalizability through data augmentation processing, employing varied strategies for unlabeled data. Concurrently, the model design gives appropriate emphasis to the generation, filtration, and refinement processes of pseudo-labels. The novel concept of cross-pseudo-supervision is introduced, integrating consistency learning with self-training. This enables the model to fully leverage pseudo-labels from multiple perspectives, thereby enhancing training diversity. The DFCPS model is compared with both baseline and advanced models using the publicly accessible Kvasir-SEG dataset. Across all four subdivisions containing different proportions of unlabeled data, our model consistently exhibits superior performance. Our source code is available at https://github.com/JustlfC03/DFCPS. ",
    "url": "https://arxiv.org/abs/2402.11273",
    "authors": [
      "Yifei Chen",
      "Chenyan Zhang",
      "Yifan Ke",
      "Yiyu Huang",
      "Xuezhou Dai",
      "Feiwei Qin",
      "Yongquan Zhang",
      "Xiaodong Zhang",
      "Changmiao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11307",
    "title": "ICHPro: Intracerebral Hemorrhage Prognosis Classification Via  Joint-attention Fusion-based 3d Cross-modal Network",
    "abstract": "Intracerebral Hemorrhage (ICH) is the deadliest subtype of stroke, necessitating timely and accurate prognostic evaluation to reduce mortality and disability. However, the multi-factorial nature and complexity of ICH make methods based solely on computed tomography (CT) image features inadequate. Despite the capacity of cross-modal networks to fuse additional information, the effective combination of different modal features remains a significant challenge. In this study, we propose a joint-attention fusion-based 3D cross-modal network termed ICHPro that simulates the ICH prognosis interpretation process utilized by neurosurgeons. ICHPro includes a joint-attention fusion module to fuse features from CT images with demographic and clinical textual data. To enhance the representation of cross-modal features, we introduce a joint loss function. ICHPro facilitates the extraction of richer cross-modal features, thereby improving classification performance. Upon testing our method using a five-fold cross-validation, we achieved an accuracy of 89.11%, an F1 score of 0.8767, and an AUC value of 0.9429. These results outperform those obtained from other advanced methods based on the test dataset, thereby demonstrating the superior efficacy of ICHPro. The code is available at our Github: https://github.com/YU-deep/ICH. ",
    "url": "https://arxiv.org/abs/2402.11307",
    "authors": [
      "Xinlei Yu",
      "Xinyang Li",
      "Ruiquan Ge",
      "Shibin Wu",
      "Ahmed Elazab",
      "Jichao Zhu",
      "Lingyan Zhang",
      "Gangyong Jia",
      "Taosheng Xu",
      "Xiang Wan",
      "Changmiao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11317",
    "title": "Debiased Offline Representation Learning for Fast Online Adaptation in  Non-stationary Dynamics",
    "abstract": "Developing policies that can adjust to non-stationary environments is essential for real-world reinforcement learning applications. However, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called Debiased Offline Representation for fast online Adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leveraging tractable bounds of the information bottleneck principle. Our experimental evaluation across six benchmark MuJoCo tasks with variable parameters demonstrates that DORA not only achieves a more precise dynamics encoding but also significantly outperforms existing baselines in terms of performance. ",
    "url": "https://arxiv.org/abs/2402.11317",
    "authors": [
      "Xinyu Zhang",
      "Wenjie Qiu",
      "Yi-Chen Li",
      "Lei Yuan",
      "Chengxing Jia",
      "Zongzhang Zhang",
      "Yang Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11318",
    "title": "BiasBuster: a Neural Approach for Accurate Estimation of Population  Statistics using Biased Location Data",
    "abstract": "While extremely useful (e.g., for COVID-19 forecasting and policy-making, urban mobility analysis and marketing, and obtaining business insights), location data collected from mobile devices often contain data from a biased population subset, with some communities over or underrepresented in the collected datasets. As a result, aggregate statistics calculated from such datasets (as is done by various companies including Safegraph, Google, and Facebook), while ignoring the bias, leads to an inaccurate representation of population statistics. Such statistics will not only be generally inaccurate, but the error will disproportionately impact different population subgroups (e.g., because they ignore the underrepresented communities). This has dire consequences, as these datasets are used for sensitive decision-making such as COVID-19 policymaking. This paper tackles the problem of providing accurate population statistics using such biased datasets. We show that statistical debiasing, although in some cases useful, often fails to improve accuracy. We then propose BiasBuster, a neural network approach that utilizes the correlations between population statistics and location characteristics to provide accurate estimates of population statistics. Extensive experiments on real-world data show that BiasBuster improves accuracy by up to 2 times in general and up to 3 times for underrepresented populations. ",
    "url": "https://arxiv.org/abs/2402.11318",
    "authors": [
      "Sepanta Zeighami",
      "Cyrus Shahabi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2402.11319",
    "title": "Hysteresis Compensation of Flexible Continuum Manipulator using RGBD  Sensing and Temporal Convolutional Network",
    "abstract": "Flexible continuum manipulators are valued for minimally invasive surgery, offering access to confined spaces through nonlinear paths. However, cable-driven manipulators face control difficulties due to hysteresis from cabling effects such as friction, elongation, and coupling. These effects are difficult to model due to nonlinearity and the difficulties become even more evident when dealing with long and multi-segmented manipulator. This paper proposes a data-driven approach based on recurrent neural networks to capture these nonlinear and previous states-dependent characteristics of cable actuation. We design customized fiducial markers to collect physical joint configurations as a dataset. Result on a study comparing the learning performance of four Deep Neural Network (DNN) models show that the Temporal Convolution Network (TCN) demonstrates the highest predictive capability. Leveraging trained TCNs, we build a control algorithm to compensate for hysteresis. Tracking tests in task space using unseen trajectories show that the best controller reduces the mean position and orientation error by 61.39% (from 13.7 mm to 5.29 mm) and 64.04% (from 31.17{\\deg} to 11.21{\\deg}), respectively. ",
    "url": "https://arxiv.org/abs/2402.11319",
    "authors": [
      "Junhyun Park",
      "Seonghyeok Jang",
      "Hyojae Park",
      "Seongjun Bae",
      "Minho Hwang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11322",
    "title": "SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for  Spiking Neural Network Systems",
    "abstract": "Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental results show that our SpikeNAS improves the searching time and maintains high accuracy as compared to state-of-the-art while meeting the given memory budgets (e.g., 4.4x faster search with 1.3% accuracy improvement for CIFAR100, using an Nvidia RTX 6000 Ada GPU machine), thereby quickly providing the appropriate SNN architecture for memory-constrained SNN-based systems. ",
    "url": "https://arxiv.org/abs/2402.11322",
    "authors": [
      "Rachmad Vidya Wicaksana Putra",
      "Muhammad Shafique"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11333",
    "title": "A Cross-Cultural Analysis of Social Norms in Bollywood and Hollywood  Movies",
    "abstract": "Understanding how social norms vary across cultures can help us build culturally aligned NLP systems. We propose a culture agnostic approach to norm discovery, using moral emotions, shame and pride, to identify examples of normative expectations and extract corresponding social norms. We present the first cross cultural self-conscious emotions dataset, obtained from 5.4K Bollywood and Hollywood movies, along with over 10K extracted social norms. We validate our dataset using native speakers and demonstrate how our dataset reveals variations in social norms that align with the cultural dichotomy observed in these nations e.g., Bollywood movies emphasize shame due to deviation from social roles, and express pride in family honor, while Hollywood shames poverty and incompetence, and takes pride in ethical behavior. Notably, females are shamed more across both cultures and both cultures shame women for violating similar normative expectations. ",
    "url": "https://arxiv.org/abs/2402.11333",
    "authors": [
      "Sunny Rai",
      "Khushang Zilesh Zaveri",
      "Shreya Havaldar",
      "Soumna Nema",
      "Lyle Ungar",
      "Sharath Chandra Guntuku"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2402.11339",
    "title": "Expressive Higher-Order Link Prediction through Hypergraph Symmetry  Breaking",
    "abstract": "A hypergraph consists of a set of nodes along with a collection of subsets of the nodes called hyperedges. Higher-order link prediction is the task of predicting the existence of a missing hyperedge in a hypergraph. A hyperedge representation learned for higher order link prediction is fully expressive when it does not lose distinguishing power up to an isomorphism. Many existing hypergraph representation learners, are bounded in expressive power by the Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the Weisfeiler Lehman-1 algorithm. However, GWL-1 has limited expressive power. In fact, induced subhypergraphs with identical GWL-1 valued nodes are indistinguishable. Furthermore, message passing on hypergraphs can already be computationally expensive, especially on GPU memory. To address these limitations, we devise a preprocessing algorithm that can identify certain regular subhypergraphs exhibiting symmetry. Our preprocessing algorithm runs once with complexity the size of the input hypergraph. During training, we randomly replace subhypergraphs identified by the algorithm with covering hyperedges to break symmetry. We show that our method improves the expressivity of GWL-1. Our extensive experiments also demonstrate the effectiveness of our approach for higher-order link prediction on both graph and hypergraph datasets with negligible change in computation. ",
    "url": "https://arxiv.org/abs/2402.11339",
    "authors": [
      "Simon Zhang",
      "Cheng Xin",
      "Tamal K. Dey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.11342",
    "title": "Ransomware detection using stacked autoencoder for feature selection",
    "abstract": "The aim of this study is to propose and evaluate an advanced ransomware detection and classification method that combines a Stacked Autoencoder (SAE) for precise feature selection with a Long Short Term Memory (LSTM) classifier to enhance ransomware stratification accuracy. The proposed approach involves thorough pre processing of the UGRansome dataset and training an unsupervised SAE for optimal feature selection or fine tuning via supervised learning to elevate the LSTM model's classification capabilities. The study meticulously analyzes the autoencoder's learned weights and activations to identify essential features for distinguishing ransomware families from other malware and creates a streamlined feature set for precise classification. Extensive experiments, including up to 400 epochs and varying learning rates, are conducted to optimize the model's performance. The results demonstrate the outstanding performance of the SAE-LSTM model across all ransomware families, boasting high precision, recall, and F1 score values that underscore its robust classification capabilities. Furthermore, balanced average scores affirm the proposed model's ability to generalize effectively across various malware types. The proposed model achieves an exceptional 99% accuracy in ransomware classification, surpassing the Extreme Gradient Boosting (XGBoost) algorithm primarily due to its effective SAE feature selection mechanism. The model also demonstrates outstanding performance in identifying signature attacks, achieving a 98% accuracy rate. ",
    "url": "https://arxiv.org/abs/2402.11342",
    "authors": [
      "Mike Nkongolo",
      "Mahmut Tokmak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.11382",
    "title": "Secure, Robust, and Energy-Efficient Authenticated Data Sharing in  UAV-Assisted 6G Networks",
    "abstract": "This paper confronts the pressing challenges of sixth-generation (6G) wireless communication networks by harnessing the unique capabilities of Unmanned Aerial Vehicles (UAVs). With the ambitious promises of 6G, including ultra-reliable 1 Tbps data delivery and ultra-low latency, the demand for innovative solutions becomes imperative. Traditional terrestrial base stations, though effective, exhibit limitations in scenarios requiring ubiquitous connectivity, prompting the integration of UAVs. In response to these challenges, we introduce a comprehensive solution. This involves UAVs collaboratively downloading desired content from service providers, and subsequently establishing secure connections with users for efficient content exchange. Accordingly, we introduce two new protocols: a collaborative group data downloading scheme among UAVs called SeGDS, and SeDDS for secure direct data sharing through out-of-band autonomous Device-to-Device (D2D) communication. Leveraging certificateless signcryption and certificateless multi-receiver encryption, these protocols offer lightweight, certificate-free solutions with features such as user revocation, non-repudiation, and mutual authentication. Prioritizing high availability, the proposed protocols effectively detect Denial of Service (DoS) and free riding attacks. A thorough evaluation underscores the superiority of the proposed protocols in both security and efficiency over existing models; SeDDS reduces overall computation by 3x, imposing a lighter communication load on UAVs, while SeGDS meets swarm UAV security requirements, reducing communication costs by 4x with low computation cost. ",
    "url": "https://arxiv.org/abs/2402.11382",
    "authors": [
      "Atefeh Mohseni Ejiyeh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.11397",
    "title": "Random Projection Neural Networks of Best Approximation: Convergence  theory and practical applications",
    "abstract": "We investigate the concept of Best Approximation for Feedforward Neural Networks (FNN) and explore their convergence properties through the lens of Random Projection (RPNNs). RPNNs have predetermined and fixed, once and for all, internal weights and biases, offering computational efficiency. We demonstrate that there exists a choice of external weights, for any family of such RPNNs, with non-polynomial infinitely differentiable activation functions, that exhibit an exponential convergence rate when approximating any infinitely differentiable function. For illustration purposes, we test the proposed RPNN-based function approximation, with parsimoniously chosen basis functions, across five benchmark function approximation problems. Results show that RPNNs achieve comparable performance to established methods such as Legendre Polynomials, highlighting their potential for efficient and accurate function approximation. ",
    "url": "https://arxiv.org/abs/2402.11397",
    "authors": [
      "Gianluca Fabiani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2402.11399",
    "title": "k-SemStamp: A Clustering-Based Semantic Watermark for Detection of  Machine-Generated Text",
    "abstract": "Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection. ",
    "url": "https://arxiv.org/abs/2402.11399",
    "authors": [
      "Abe Bohan Hou",
      "Jingyu Zhang",
      "Yichen Wang",
      "Daniel Khashabi",
      "Tianxing He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11400",
    "title": "From Text to Map: A System Dynamics Bot for Constructing Causal Loop  Diagrams",
    "abstract": "We introduce and test the System Dynamics Bot, a computer program leveraging a large language model to automate the creation of causal loop diagrams from textual data. To evaluate its performance, we ensembled two distinct databases. The first dataset includes 20 causal loop diagrams and associated texts sourced from the system dynamics literature. The second dataset comprises responses from 30 participants to a vignette, along with causal loop diagrams coded by three system dynamics modelers. The bot uses textual data and successfully identifies approximately sixty percent of the links between variables and feedback loops in both datasets. This paper outlines our approach, provides examples, and presents evaluation results. We discuss encountered challenges and implemented solutions in developing the System Dynamics Bot. The bot can facilitate extracting mental models from textual data and improve model building processes. Moreover, the two datasets can serve as a testbed for similar programs. ",
    "url": "https://arxiv.org/abs/2402.11400",
    "authors": [
      "Niyousha Hosseinichimeh",
      "Aritra Majumdar",
      "Ross Williams",
      "Navid Ghaffarzadegan"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2402.11401",
    "title": "GraphKD: Exploring Knowledge Distillation Towards Document Object  Detection with Structured Graph Creation",
    "abstract": "Object detection in documents is a key step to automate the structural elements identification process in a digital or scanned document through understanding the hierarchical structure and relationships between different elements. Large and complex models, while achieving high accuracy, can be computationally expensive and memory-intensive, making them impractical for deployment on resource constrained devices. Knowledge distillation allows us to create small and more efficient models that retain much of the performance of their larger counterparts. Here we present a graph-based knowledge distillation framework to correctly identify and localize the document objects in a document image. Here, we design a structured graph with nodes containing proposal-level features and edges representing the relationship between the different proposal regions. Also, to reduce text bias an adaptive node sampling strategy is designed to prune the weight distribution and put more weightage on non-text nodes. We encode the complete graph as a knowledge representation and transfer it from the teacher to the student through the proposed distillation loss by effectively capturing both local and global information concurrently. Extensive experimentation on competitive benchmarks demonstrates that the proposed framework outperforms the current state-of-the-art approaches. The code will be available at: https://github.com/ayanban011/GraphKD. ",
    "url": "https://arxiv.org/abs/2402.11401",
    "authors": [
      "Ayan Banerjee",
      "Sanket Biswas",
      "Josep Llad\u00f3s",
      "Umapada Pal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11403",
    "title": "An Empirical Evaluation of Neural and Neuro-symbolic Approaches to  Real-time Multimodal Complex Event Detection",
    "abstract": "Robots and autonomous systems require an understanding of complex events (CEs) from sensor data to interact with their environments and humans effectively. Traditional end-to-end neural architectures, despite processing sensor data efficiently, struggle with long-duration events due to limited context sizes and reasoning capabilities. Recent advances in neuro-symbolic methods, which integrate neural and symbolic models leveraging human knowledge, promise improved performance with less data. This study addresses the gap in understanding these approaches' effectiveness in complex event detection (CED), especially in temporal reasoning. We investigate neural and neuro-symbolic architectures' performance in a multimodal CED task, analyzing IMU and acoustic data streams to recognize CE patterns. Our methodology includes (i) end-to-end neural architectures for direct CE detection from sensor embeddings, (ii) two-stage concept-based neural models mapping sensor embeddings to atomic events (AEs) before CE detection, and (iii) a neuro-symbolic approach using a symbolic finite-state machine for CE detection from AEs. Empirically, the neuro-symbolic architecture significantly surpasses purely neural models, demonstrating superior performance in CE recognition, even with extensive training data and ample temporal context for neural approaches. ",
    "url": "https://arxiv.org/abs/2402.11403",
    "authors": [
      "Liying Han",
      "Mani B. Srivastava"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11406",
    "title": "Don't Go To Extremes: Revealing the Excessive Sensitivity and  Calibration Limitations of LLMs in Implicit Hate Speech Detection",
    "abstract": "The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complexity. Consequently, the calibration performance is heavily reliant on primary classification accuracy. These discoveries unveil new limitations of LLMs, underscoring the need for caution when optimizing models to ensure they do not veer towards extremes. This serves as a reminder to carefully consider sensitivity and confidence in the pursuit of model fairness. ",
    "url": "https://arxiv.org/abs/2402.11406",
    "authors": [
      "Min Zhang",
      "Jianfeng He",
      "Taoran Ji",
      "Chang-Tien Lu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11431",
    "title": "A Robust Error-Resistant View Selection Method for 3D Reconstruction",
    "abstract": "To address the issue of increased triangulation uncertainty caused by selecting views with small camera baselines in Structure from Motion (SFM) view selection, this paper proposes a robust error-resistant view selection method. The method utilizes a triangulation-based computation to obtain an error-resistant model, which is then used to construct an error-resistant matrix. The sorting results of each row in the error-resistant matrix determine the candidate view set for each view. By traversing the candidate view sets of all views and completing the missing views based on the error-resistant matrix, the integrity of 3D reconstruction is ensured. Experimental comparisons between this method and the exhaustive method with the highest accuracy in the COLMAP program are conducted in terms of average reprojection error and absolute trajectory error in the reconstruction results. The proposed method demonstrates an average reduction of 29.40% in reprojection error accuracy and 5.07% in absolute trajectory error on the TUM dataset and DTU dataset. ",
    "url": "https://arxiv.org/abs/2402.11431",
    "authors": [
      "Shaojie Zhang",
      "Yinghui Wang",
      "Bin Nan",
      "Jinlong Yang",
      "Tao Yan",
      "Liangyi Huang",
      "Mingfeng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11432",
    "title": "Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark  for Deception Reasoning",
    "abstract": "Deception detection has attracted increasing attention due to its importance in many practical scenarios. Currently, data scarcity harms the development of this field. On the one hand, it is costly to hire participants to simulate deception scenarios. On the other hand, it is difficult to collect videos containing deceptive behaviors on the Internet. To address data scarcity, this paper proposes a new data collection pipeline. Specifically, we use GPT-4 to simulate a role-play between a suspect and a police officer. During interrogation, the suspect lies to the police officer to evade responsibility for the crime, while the police officer uncovers the truth and gathers evidence. Compared with previous datasets, this strategy reduces data collection costs, providing a promising way to increase the dataset size. Meanwhile, we extend the traditional deception detection task to deception reasoning, further providing evidence for deceptive parts. This dataset can also be used to evaluate the complex reasoning capability of current large language models and serve as a reasoning benchmark for further research. ",
    "url": "https://arxiv.org/abs/2402.11432",
    "authors": [
      "Kang Chen",
      "Zheng Lian",
      "Haiyang Sun",
      "Bin Liu",
      "Jianhua Tao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11441",
    "title": "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via  Infuser-Guided Knowledge Integration",
    "abstract": "Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQA domain knowledge graphs demonstrate that InfuserKI can effectively acquire new knowledge and outperform state-of-the-art baselines by 9% and 6%, respectively, in reducing knowledge forgetting. ",
    "url": "https://arxiv.org/abs/2402.11441",
    "authors": [
      "Fali Wang",
      "Runxue Bao",
      "Suhang Wang",
      "Wenchao Yu",
      "Yanchi Liu",
      "Wei Cheng",
      "Haifeng Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11454",
    "title": "Addressing Internally-Disconnected Communities in Leiden and Louvain  Community Detection Algorithms",
    "abstract": "Community detection is the problem of identifying densely connected clusters of nodes within a network. The Louvain algorithm is a widely used method for this task, but it can produce communities that are internally disconnected. To address this, the Leiden algorithm was introduced. However, our analysis and empirical observations indicate that the Leiden algorithm still identifies disconnected communities, albeit to a lesser extent. To mitigate this issue, we propose two new parallel algorithms: GSP-Leiden and GSP-Louvain, based on the Leiden and Louvain algorithms, respectively. On a system with two 16-core Intel Xeon Gold 6226R processors, we demonstrate that GSP-Leiden/GSP-Louvain not only address this issue, but also outperform the original Leiden, igraph Leiden, and NetworKit Leiden by 373x/473x, 86x/186x, and 7.2x/17.2x respectively - achieving a processing rate of 352M/652M edges/s on a 3.8B edge graph. Furthermore, GSP-Leiden/GSP-Louvain improve performance at a rate of 1.6x/1.7x for every doubling of threads. ",
    "url": "https://arxiv.org/abs/2402.11454",
    "authors": [
      "Subhajit Sahu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.11457",
    "title": "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs'  Overconfidence Helps Retrieval Augmentation",
    "abstract": "Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls. ",
    "url": "https://arxiv.org/abs/2402.11457",
    "authors": [
      "Shiyu Ni",
      "Keping Bi",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11461",
    "title": "FGeo-HyperGNet: Geometry Problem Solving Integrating Formal Symbolic  System and Hypergraph Neural Network",
    "abstract": "Geometry problem solving has always been a long-standing challenge in the fields of automated reasoning and artificial intelligence. This is the fifth article in a series of our works, we built a neural-symbolic system to automatically perform human-like geometric deductive reasoning. The symbolic part is a formal system built on FormalGeo, which can automatically perform geomertic relational reasoning and algebraic calculations and organize the solving process into a solution hypertree with conditions as hypernodes and theorems as hyperedges. The neural part, called HyperGNet, is a hypergraph neural network based on the attention mechanism, including a encoder to effectively encode the structural and semantic information of the hypertree, and a solver to provide problem-solving guidance. The neural part predicts theorems according to the hypertree, and the symbolic part applies theorems and updates the hypertree, thus forming a Predict-Apply Cycle to ultimately achieve readable and traceable automatic solving of geometric problems. Experiments demonstrate the correctness and effectiveness of this neural-symbolic architecture. We achieved a step-wised accuracy of 87.65% and an overall accuracy of 85.53% on the formalgeo7k datasets. The code and data is available at https://github.com/BitSecret/HyperGNet. ",
    "url": "https://arxiv.org/abs/2402.11461",
    "authors": [
      "Xiaokai Zhang",
      "Na Zhu",
      "Yiming He",
      "Jia Zou",
      "Cheng Qin",
      "Yang Li",
      "Zhenbing Zeng",
      "Tuo Leng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11462",
    "title": "Age of $(k,n)$-Threshold Signature Scheme on a Gossip Network",
    "abstract": "We consider information update systems on a gossip network, which consists of a single source and $n$ receiver nodes. The source encrypts the information into $n$ distinct keys with version stamps, sending a unique key to each node. For decryption in a $(k, n)$-Threshold Signature Scheme, each receiver node requires at least $k+1$ different keys with the same version, shared over peer-to-peer connections. We consider two different schemes: a memory scheme (in which the nodes keep the source's current and previous encrypted messages) and a memoryless scheme (in which the nodes are allowed to only keep the source's current message). We measure the ''timeliness'' of information updates by using the version age of information. Our work focuses on determining closed-form expressions for the time average age of information in a heterogeneous random graph. Our work not only allows to verify the expected outcome that a memory scheme results in a lower average age compared to a memoryless scheme, but also provides the quantitative difference between the two. In our numerical results, we quantify the value of memory and demonstrate that the advantages of memory diminish with infrequent source updates, frequent gossipping between nodes, or a decrease in $k$ for a fixed number of nodes. ",
    "url": "https://arxiv.org/abs/2402.11462",
    "authors": [
      "Erkan Bayram",
      "Melih Bastopcu",
      "Mohamed-Ali Belabbas",
      "Tamer Ba\u015far"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2402.11465",
    "title": "Odd Cycle Transversal on $P_5$-free Graphs in Polynomial Time",
    "abstract": "An independent set in a graph G is a set of pairwise non-adjacent vertices. A graph $G$ is bipartite if its vertex set can be partitioned into two independent sets. In the Odd Cycle Transversal problem, the input is a graph $G$ along with a weight function $w$ associating a rational weight with each vertex, and the task is to find a smallest weight vertex subset $S$ in $G$ such that $G - S$ is bipartite; the weight of $S$, $w(S) = \\sum_{v\\in S} w(v)$. We show that Odd Cycle Transversal is polynomial-time solvable on graphs excluding $P_5$ (a path on five vertices) as an induced subgraph. The problem was previously known to be polynomial-time solvable on $P_4$-free graphs and NP-hard on $P_6$-free graphs [Dabrowski, Feghali, Johnson, Paesani, Paulusma and Rz\\k{a}\\.zewski, Algorithmica 2020]. Bonamy, Dabrowski, Feghali, Johnson and Paulusma [Algorithmica 2019] posed the existence of a polynomial-time algorithm on $P_5$-free graphs as an open problem, this was later re-stated by Rz\\k{a}\\.zewski [Dagstuhl Reports, 9(6): 2019] and by Chudnovsky, King, Pilipczuk, Rz\\k{a}\\.zewski, and Spirkl [SIDMA 2021], who gave an algorithm with running time $n^{O(\\sqrt{n})}$. ",
    "url": "https://arxiv.org/abs/2402.11465",
    "authors": [
      "Akanksha Agrawal",
      "Paloma T. Lima",
      "Daniel Lokshtanov",
      "Pawel Rz\u0105\u017cewski",
      "Saket Saurabh",
      "Roohani Sharma"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2402.11469",
    "title": "A Curious Case of Searching for the Correlation between Training Data  and Adversarial Robustness of Transformer Textual Models",
    "abstract": "Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done \\textit{only after} fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate and (b) features with the most influence on the model robustness have a clear correlation with the robustness. Second, our framework can be used as a fast and effective additional tool for robustness evaluation since it (a) saves 30x-193x runtime compared to the traditional technique, (b) is transferable across models, (c) can be used under adversarial training, and (d) robust to statistical randomness. Our code will be publicly available. ",
    "url": "https://arxiv.org/abs/2402.11469",
    "authors": [
      "Cuong Dang",
      "Dung D. Le",
      "Thai Le"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.11473",
    "title": "Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery  Detection",
    "abstract": "The proliferation of face forgery techniques has raised significant concerns within society, thereby motivating the development of face forgery detection methods. These methods aim to distinguish forged faces from genuine ones and have proven effective in practical applications. However, this paper introduces a novel and previously unrecognized threat in face forgery detection scenarios caused by backdoor attack. By embedding backdoors into models and incorporating specific trigger patterns into the input, attackers can deceive detectors into producing erroneous predictions for forged faces. To achieve this goal, this paper proposes \\emph{Poisoned Forgery Face} framework, which enables clean-label backdoor attacks on face forgery detectors. Our approach involves constructing a scalable trigger generator and utilizing a novel convolving process to generate translation-sensitive trigger patterns. Moreover, we employ a relative embedding method based on landmark-based regions to enhance the stealthiness of the poisoned samples. Consequently, detectors trained on our poisoned samples are embedded with backdoors. Notably, our approach surpasses SoTA backdoor baselines with a significant improvement in attack success rate (+16.39\\% BD-AUC) and reduction in visibility (-12.65\\% $L_\\infty$). Furthermore, our attack exhibits promising performance against backdoor defenses. We anticipate that this paper will draw greater attention to the potential threats posed by backdoor attacks in face forgery detection scenarios. Our codes will be made available at \\url{https://github.com/JWLiang007/PFF} ",
    "url": "https://arxiv.org/abs/2402.11473",
    "authors": [
      "Jiawei Liang",
      "Siyuan Liang",
      "Aishan Liu",
      "Xiaojun Jia",
      "Junhao Kuang",
      "Xiaochun Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11476",
    "title": "EndoOOD: Uncertainty-aware Out-of-distribution Detection in Capsule  Endoscopy Diagnosis",
    "abstract": "Wireless capsule endoscopy (WCE) is a non-invasive diagnostic procedure that enables visualization of the gastrointestinal (GI) tract. Deep learning-based methods have shown effectiveness in disease screening using WCE data, alleviating the burden on healthcare professionals. However, existing capsule endoscopy classification methods mostly rely on pre-defined categories, making it challenging to identify and classify out-of-distribution (OOD) data, such as undefined categories or anatomical landmarks. To address this issue, we propose the Endoscopy Out-of-Distribution (EndoOOD) framework, which aims to effectively handle the OOD detection challenge in WCE diagnosis. The proposed framework focuses on improving the robustness and reliability of WCE diagnostic capabilities by incorporating uncertainty-aware mixup training and long-tailed in-distribution (ID) data calibration techniques. Additionally, virtual-logit matching is employed to accurately distinguish between OOD and ID data while minimizing information loss. To assess the performance of our proposed solution, we conduct evaluations and comparisons with 12 state-of-the-art (SOTA) methods using two publicly available datasets. The results demonstrate the effectiveness of the proposed framework in enhancing diagnostic accuracy and supporting clinical decision-making. ",
    "url": "https://arxiv.org/abs/2402.11476",
    "authors": [
      "Qiaozhi Tan",
      "Long Bai",
      "Guankun Wang",
      "Mobarakol Islam",
      "Hongliang Ren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11485",
    "title": "LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models  with Entity-based Data Augmentation",
    "abstract": "Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia. ",
    "url": "https://arxiv.org/abs/2402.11485",
    "authors": [
      "Ikuya Yamada",
      "Ryokan Ri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11492",
    "title": "Exponential Cluster Synchronization in Fast Switching Network  Topologies: A Pinning Control Approach with Necessary and Sufficient  Conditions",
    "abstract": "This research investigates the intricate domain of synchronization problem among multiple agents operating within a dynamic fast switching network topology. We concentrate on cluster synchronization within coupled linear system under pinning control, providing both necessary and sufficient conditions. As a pivotal aspect, this paper aim to president the weakest possible conditions to make the coupled linear system realize cluster synchronization exponentially. Within the context of fast switching framework, we initially examine the necessary conditions, commencing with the transformation of the consensus problem into a stability problem, introducing a new variable to make the coupled system achieve cluster synchronization if the system is controllable; communication topology switching fast enough and the coupling strength should be sufficiently robust. Then, by using the Lyapunov theorem, we also present that the state matrix controllable is necessary for cluster synchronization. Furthermore, this paper culminating in the incorporation of contraction theory and an invariant manifold, demonstrating that the switching topology has an average is imperative for achieving cluster synchronization. Finally, we introduce three simulations to validate the efficacy of the proposed approach. ",
    "url": "https://arxiv.org/abs/2402.11492",
    "authors": [
      "Ku Du",
      "Yu Kang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2402.11494",
    "title": "Graph Out-of-Distribution Generalization via Causal Intervention",
    "abstract": "Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment labels. Our method resorts to a new learning objective derived from causal inference that coordinates an environment estimator and a mixture-of-expert GNN predictor. The new approach can counteract the confounding bias in training data and facilitate learning generalizable predictive relations. Extensive experiment demonstrates that our model can effectively enhance generalization with various types of distribution shifts and yield up to 27.4\\% accuracy improvement over state-of-the-arts on graph OOD generalization benchmarks. Source codes are available at https://github.com/fannie1208/CaNet. ",
    "url": "https://arxiv.org/abs/2402.11494",
    "authors": [
      "Qitian Wu",
      "Fan Nie",
      "Chenxiao Yang",
      "Tianyi Bao",
      "Junchi Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.11495",
    "title": "URLBERT:A Contrastive and Adversarial Pre-trained Model for URL  Classification",
    "abstract": "URLs play a crucial role in understanding and categorizing web content, particularly in tasks related to security control and online recommendations. While pre-trained models are currently dominating various fields, the domain of URL analysis still lacks specialized pre-trained models. To address this gap, this paper introduces URLBERT, the first pre-trained representation learning model applied to a variety of URL classification or detection tasks. We first train a URL tokenizer on a corpus of billions of URLs to address URL data tokenization. Additionally, we propose two novel pre-training tasks: (1) self-supervised contrastive learning tasks, which strengthen the model's understanding of URL structure and the capture of category differences by distinguishing different variants of the same URL; (2) virtual adversarial training, aimed at improving the model's robustness in extracting semantic features from URLs. Finally, our proposed methods are evaluated on tasks including phishing URL detection, web page classification, and ad filtering, achieving state-of-the-art performance. Importantly, we also explore multi-task learning with URLBERT, and experimental results demonstrate that multi-task learning model based on URLBERT exhibit equivalent effectiveness compared to independently fine-tuned models, showing the simplicity of URLBERT in handling complex task requirements. The code for our work is available at https://github.com/Davidup1/URLBERT. ",
    "url": "https://arxiv.org/abs/2402.11495",
    "authors": [
      "Yujie Li",
      "Yanbin Wang",
      "Haitao Xu",
      "Zhenhao Guo",
      "Zheng Cao",
      "Lun Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11497",
    "title": "Thyroid ultrasound diagnosis improvement via multi-view self-supervised  learning and two-stage pre-training",
    "abstract": "Thyroid nodule classification and segmentation in ultrasound images are crucial for computer-aided diagnosis; however, they face limitations owing to insufficient labeled data. In this study, we proposed a multi-view contrastive self-supervised method to improve thyroid nodule classification and segmentation performance with limited manual labels. Our method aligns the transverse and longitudinal views of the same nodule, thereby enabling the model to focus more on the nodule area. We designed an adaptive loss function that eliminates the limitations of the paired data. Additionally, we adopted a two-stage pre-training to exploit the pre-training on ImageNet and thyroid ultrasound images. Extensive experiments were conducted on a large-scale dataset collected from multiple centers. The results showed that the proposed method significantly improves nodule classification and segmentation performance with limited manual labels and outperforms state-of-the-art self-supervised methods. The two-stage pre-training also significantly exceeded ImageNet pre-training. ",
    "url": "https://arxiv.org/abs/2402.11497",
    "authors": [
      "Jian Wang",
      "Xin Yang",
      "Xiaohong Jia",
      "Wufeng Xue",
      "Rusi Chen",
      "Yanlin Chen",
      "Xiliang Zhu",
      "Lian Liu",
      "Yan Cao",
      "Jianqiao Zhou",
      "Dong Ni",
      "Ning Gu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11498",
    "title": "Verifiably Following Complex Robot Instructions with Foundation Models",
    "abstract": "Enabling robots to follow complex natural language instructions is an important yet challenging problem. People want to flexibly express constraints, refer to arbitrary landmarks and verify behavior when instructing robots. Conversely, robots must disambiguate human instructions into specifications and ground instruction referents in the real world. We propose Language Instruction grounding for Motion Planning (LIMP), a system that leverages foundation models and temporal logics to generate instruction-conditioned semantic maps that enable robots to verifiably follow expressive and long-horizon instructions with open vocabulary referents and complex spatiotemporal constraints. In contrast to prior methods for using foundation models in robot task execution, LIMP constructs an explainable instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of robot behaviors that are correct-by-construction. We demonstrate LIMP in three real-world environments, across a set of 35 complex spatiotemporal instructions, showing the generality of our approach and the ease of deployment in novel unstructured domains. In our experiments, LIMP can spatially ground open-vocabulary referents and synthesize constraint-satisfying plans in 90% of object-goal navigation and 71% of mobile manipulation instructions. See supplementary videos at https://robotlimp.github.io ",
    "url": "https://arxiv.org/abs/2402.11498",
    "authors": [
      "Benedict Quartey",
      "Eric Rosen",
      "Stefanie Tellex",
      "George Konidaris"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11505",
    "title": "Federated Fine-tuning of Large Language Models under Heterogeneous  Language Tasks and Client Resources",
    "abstract": "Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the \"buckets effect\" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, our experiments validate the efficacy of FlexLoRA, with the federated global model achieving up to a 3.1% average improvement in downstream NLP task performance. FlexLoRA's practicality is further underscored by its seamless integration with existing LoRA-based FL methods and theoretical analysis, offering a path toward scalable, privacy-preserving federated tuning for LLMs. ",
    "url": "https://arxiv.org/abs/2402.11505",
    "authors": [
      "Jiamu Bai",
      "Daoyuan Chen",
      "Bingchen Qian",
      "Liuyi Yao",
      "Yaliang Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11507",
    "title": "MAL: Motion-Aware Loss with Temporal and Distillation Hints for  Self-Supervised Depth Estimation",
    "abstract": "Depth perception is crucial for a wide range of robotic applications. Multi-frame self-supervised depth estimation methods have gained research interest due to their ability to leverage large-scale, unlabeled real-world data. However, the self-supervised methods often rely on the assumption of a static scene and their performance tends to degrade in dynamic environments. To address this issue, we present Motion-Aware Loss, which leverages the temporal relation among consecutive input frames and a novel distillation scheme between the teacher and student networks in the multi-frame self-supervised depth estimation methods. Specifically, we associate the spatial locations of moving objects with the temporal order of input frames to eliminate errors induced by object motion. Meanwhile, we enhance the original distillation scheme in multi-frame methods to better exploit the knowledge from a teacher network. MAL is a novel, plug-and-play module designed for seamless integration into multi-frame self-supervised monocular depth estimation methods. Adding MAL into previous state-of-the-art methods leads to a reduction in depth estimation errors by up to 4.2% and 10.8% on KITTI and CityScapes benchmarks, respectively. ",
    "url": "https://arxiv.org/abs/2402.11507",
    "authors": [
      "Yup-Jiang Dong",
      "Fang-Lue Zhang",
      "Song-Hai Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2402.11518",
    "title": "Large Language Model-driven Meta-structure Discovery in Heterogeneous  Information Network",
    "abstract": "Heterogeneous information networks (HIN) have gained increasing popularity for being able to capture complex relations between nodes of diverse types. Meta-structure was proposed to identify important patterns of relations on HIN, which has been proven effective for extracting rich semantic information and facilitating graph neural networks to learn expressive representations. However, hand-crafted meta-structures pose challenges for scaling up, which draws wide research attention for developing automatic meta-structure search algorithms. Previous efforts concentrate on searching for meta-structures with good empirical prediction performance, overlooking explainability. Thus, they often produce meta-structures prone to overfitting and incomprehensible to humans. To address this, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose a novel REasoning meta-STRUCTure search (ReStruct) framework that integrates LLM reasoning into the evolutionary procedure. ReStruct uses a grammar translator to encode meta-structures into natural language sentences, and leverages the reasoning power of LLMs to evaluate semantically feasible meta-structures. ReStruct also employs performance-oriented evolutionary operations. These two competing forces jointly optimize for semantic explainability and empirical performance of meta-structures. We also design a differential LLM explainer that can produce natural language explanations for the discovered meta-structures, and refine the explanation by reasoning through the search history. Experiments on five datasets demonstrate ReStruct achieve SOTA performance in node classification and link recommendation tasks. Additionally, a survey study involving 73 graduate students shows that the meta-structures and natural language explanations generated by ReStruct are substantially more comprehensible. ",
    "url": "https://arxiv.org/abs/2402.11518",
    "authors": [
      "Lin Chen",
      "Fengli Xu",
      "Nian Li",
      "Zhenyu Han",
      "Meng Wang",
      "Yong Li",
      "Pan Hui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11526",
    "title": "Measuring Privacy Loss in Distributed Spatio-Temporal Data",
    "abstract": "Statistics about traffic flow and people's movement gathered from multiple geographical locations in a distributed manner are the driving force powering many applications, such as traffic prediction, demand prediction, and restaurant occupancy reports. However, these statistics are often based on sensitive location data of people, and hence privacy has to be preserved while releasing them. The standard way to do this is via differential privacy, which guarantees a form of rigorous, worst-case, person-level privacy. In this work, motivated by several counter-intuitive features of differential privacy in distributed location applications, we propose an alternative privacy loss against location reconstruction attacks by an informed adversary. Our experiments on real and synthetic data demonstrate that our privacy loss better reflects our intuitions on individual privacy violation in the distributed spatio-temporal setting. ",
    "url": "https://arxiv.org/abs/2402.11526",
    "authors": [
      "Tatsuki Koga",
      "Casey Meehan",
      "Kamalika Chaudhuri"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.11540",
    "title": "CPN: Complementary Proposal Network for Unconstrained Text Detection",
    "abstract": "Existing methods for scene text detection can be divided into two paradigms: segmentation-based and anchor-based. While Segmentation-based methods are well-suited for irregular shapes, they struggle with compact or overlapping layouts. Conversely, anchor-based approaches excel for complex layouts but suffer from irregular shapes. To strengthen their merits and overcome their respective demerits, we propose a Complementary Proposal Network (CPN) that seamlessly and parallelly integrates semantic and geometric information for superior performance. The CPN comprises two efficient networks for proposal generation: the Deformable Morphology Semantic Network, which generates semantic proposals employing an innovative deformable morphological operator, and the Balanced Region Proposal Network, which produces geometric proposals with pre-defined anchors. To further enhance the complementarity, we introduce an Interleaved Feature Attention module that enables semantic and geometric features to interact deeply before proposal generation. By leveraging both complementary proposals and features, CPN outperforms state-of-the-art approaches with significant margins under comparable computation cost. Specifically, our approach achieves improvements of 3.6%, 1.3% and 1.0% on challenging benchmarks ICDAR19-ArT, IC15, and MSRA-TD500, respectively. Code for our method will be released. ",
    "url": "https://arxiv.org/abs/2402.11540",
    "authors": [
      "Longhuang Wu",
      "Shangxuan Tian",
      "Youxin Wang",
      "Pengfei Xiong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11541",
    "title": "Counter-intuitive: Large Language Models Can Better Understand Knowledge  Graphs Than We Thought",
    "abstract": "Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension of KG. Contrary to our initial expectations, our analysis revealed that LLMs effectively handle messy, noisy, and linearized KG knowledge, outperforming methods that employ well-designed natural language (NL) textual prompts. This counter-intuitive finding provides substantial insights for future research on LLMs' comprehension of structured knowledge. ",
    "url": "https://arxiv.org/abs/2402.11541",
    "authors": [
      "Xinbang Dai",
      "Yuncheng Hua",
      "Tongtong Wu",
      "Yang Sheng",
      "Guilin Qi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11542",
    "title": "Question Answering Over Spatio-Temporal Knowledge Graph",
    "abstract": "Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge graphs (KGs) by incorporating time and location information. While the research community's focus on Knowledge Graph Question Answering (KGQA), the field of answering questions incorporating both spatio-temporal information based on STKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets also has hindered progress in this area. To address this issue, we present STQAD, a dataset comprising 10,000 natural language questions for spatio-temporal knowledge graph question answering (STKGQA). Unfortunately, various state-of-the-art KGQA approaches fall far short of achieving satisfactory performance on our dataset. In response, we propose STCQA, a new spatio-temporal KGQA approach that utilizes a novel STKG embedding method named STComplEx. By extracting temporal and spatial information from a question, our QA model can better comprehend the question and retrieve accurate answers from the STKG. Through extensive experiments, we demonstrate the quality of our dataset and the effectiveness of our STKGQA method. ",
    "url": "https://arxiv.org/abs/2402.11542",
    "authors": [
      "Xinbang Dai",
      "Huiying Li",
      "Guilin Qi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11566",
    "title": "Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data  Augmentation and Consistency Training",
    "abstract": "The 2D human pose estimation is a basic visual problem. However, supervised learning of a model requires massive labeled images, which is expensive and labor-intensive. In this paper, we aim at boosting the accuracy of a pose estimator by excavating extra unlabeled images in a semi-supervised learning (SSL) way. Most previous consistency-based SSL methods strive to constraint the model to predict consistent results for differently augmented images. Following this consensus, we revisit two core aspects including advanced data augmentation methods and concise consistency training frameworks. Specifically, we heuristically dig various collaborative combinations of existing data augmentations, and discover novel superior data augmentation schemes to more effectively add noise on unlabeled samples. They can compose easy-hard augmentation pairs with larger transformation difficulty gaps, which play a crucial role in consistency-based SSL. Moreover, we propose to strongly augment unlabeled images repeatedly with diverse augmentations, generate multi-path predictions sequentially, and optimize corresponding unsupervised consistency losses using one single network. This simple and compact design is on a par with previous methods consisting of dual or triple networks. Furthermore, it can also be integrated with multiple networks to produce better performance. Comparing to state-of-the-art SSL approaches, our method brings substantial improvements on public datasets. Code is released for academic use in \\url{https://github.com/hnuzhy/MultiAugs}. ",
    "url": "https://arxiv.org/abs/2402.11566",
    "authors": [
      "Huayi Zhou",
      "Mukun Luo",
      "Fei Jiang",
      "Yue Ding",
      "Hongtao Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11568",
    "title": "A novel Fourier neural operator framework for classification of  multi-sized images: Application to 3D digital porous media",
    "abstract": "Fourier neural operators (FNOs) are invariant with respect to the size of input images, and thus images with any size can be fed into FNO-based frameworks without any modification of network architectures, in contrast to traditional convolutional neural networks (CNNs). Leveraging the advantage of FNOs, we propose a novel deep-learning framework for classifying images with varying sizes. Particularly, we simultaneously train the proposed network on multi-sized images. As a practical application, we consider the problem of predicting the label (e.g., permeability) of three-dimensional digital porous media. To construct the framework, an intuitive approach is to connect FNO layers to a classifier using adaptive max pooling. First, we show that this approach is only effective for porous media with fixed sizes, whereas it fails for porous media of varying sizes. To overcome this limitation, we introduce our approach: instead of using adaptive max pooling, we use static max pooling with the size of channel width of FNO layers. Since the channel width of the FNO layers is independent of input image size, the introduced framework can handle multi-sized images during training. We show the effectiveness of the introduced framework and compare its performance with the intuitive approach through the example of the classification of three-dimensional digital porous media of varying sizes. ",
    "url": "https://arxiv.org/abs/2402.11568",
    "authors": [
      "Ali Kashefi",
      "Tapan Mukerji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11573",
    "title": "BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval  Augmented Long-Context Large Language Models",
    "abstract": "Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we proposeExtensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way. 3) Superior compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component. Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context. ",
    "url": "https://arxiv.org/abs/2402.11573",
    "authors": [
      "Kun Luo",
      "Zheng Liu",
      "Shitao Xiao",
      "Kang Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11585",
    "title": "PolypNextLSTM: A lightweight and fast polyp video segmentation network  using ConvNext and ConvLSTM",
    "abstract": "Commonly employed in polyp segmentation, single image UNet architectures lack the temporal insight clinicians gain from video data in diagnosing polyps. To mirror clinical practices more faithfully, our proposed solution, PolypNextLSTM, leverages video-based deep learning, harnessing temporal information for superior segmentation performance with the least parameter overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting the last two layers to reduce parameter overhead. Our temporal fusion module, a Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal features. Our primary novelty lies in PolypNextLSTM, which stands out as the leanest in parameters and the fastest model, surpassing the performance of five state-of-the-art image and video-based deep learning models. The evaluation of the SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios, along with videos containing challenging artefacts like fast motion and occlusion. ",
    "url": "https://arxiv.org/abs/2402.11585",
    "authors": [
      "Debayan Bhattacharya",
      "Konrad Reuter",
      "Finn Behrendnt",
      "Lennart Maack",
      "Sarah Grube",
      "Alexander Schlaefer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11618",
    "title": "On Network Design and Planning 2.0 for Optical-computing-enabled  Networks",
    "abstract": "In accommodating the continued explosive growth in Internet traffic, optical core networks have been evolving accordingly thanks to numerous technological and architectural innovations. From an architectural perspective, the adoption of optical-bypass networking in the last two decades has resulted in substantial cost savings, owning to the elimination of massive optical-electrical optical interfaces. In optical-bypass framework, the basic functions of optical nodes include adding (dropping) and cross-connecting transitional lightpaths. Moreover, in the process of cross-connecting transiting lightpaths through an intermediate node, these lightpaths must be separated from each other in either time, frequency or spatial domain, to avoid unwanted interference which deems to deteriorate the signal qualities. In light of recently enormous advances in photonic signal processing / computing technologies enabling the precisely controlled interference of optical channels for various computing functions, we propose a new architectural paradigm for future optical networks, namely, optical-computing-enabled networks. Our proposal is defined by the added capability of optical nodes permitting the superposition of transitional lightpaths for computing purposes to achieve greater capacity efficiency. Specifically, we present two illustrative examples highlighting the potential benefits of bringing about in-network optical computing functions which are relied on optical aggregation and optical XOR gate. The new optical computing capabilities armed at optical nodes therefore call for a radical change in formulating networking problems and designing accompanying algorithms, which are collectively referred to as optical network design and planning 2.0 so that the capital and operational efficiency could be fully unlocked. ",
    "url": "https://arxiv.org/abs/2402.11618",
    "authors": [
      "Dao Thanh Hai",
      "Isaac Woungang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2402.11621",
    "title": "Decoding News Narratives: A Critical Analysis of Large Language Models  in Framing Bias Detection",
    "abstract": "This work contributes to the expanding research on the applicability of LLMs in social sciences by examining the performance of GPT-3.5 Turbo, GPT-4, and Flan-T5 models in detecting framing bias in news headlines through zero-shot, few-shot, and explainable prompting methods. A key insight from our evaluation is the notable efficacy of explainable prompting in enhancing the reliability of these models, highlighting the importance of explainable settings for social science research on framing bias. GPT-4, in particular, demonstrated enhanced performance in few-shot scenarios when presented with a range of relevant, in-domain examples. FLAN-T5's poor performance indicates that smaller models may require additional task-specific fine-tuning for identifying framing bias detection. Our study also found that models, particularly GPT-4, often misinterpret emotional language as an indicator of framing bias, underscoring the challenge of distinguishing between reporting genuine emotional expression and intentionally use framing bias in news headlines. We further evaluated the models on two subsets of headlines where the presence or absence of framing bias was either clear-cut or more contested, with the results suggesting that these models' can be useful in flagging potential annotation inaccuracies within existing or new datasets. Finally, the study evaluates the models in real-world conditions (\"in the wild\"), moving beyond the initial dataset focused on U.S. Gun Violence, assessing the models' performance on framed headlines covering a broad range of topics. ",
    "url": "https://arxiv.org/abs/2402.11621",
    "authors": [
      "Valeria Pastorino",
      "Jasivan A. Sivakumar",
      "Nafise Sadat Moosavi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11628",
    "title": "Discrete Neural Algorithmic Reasoning",
    "abstract": "Neural algorithmic reasoning aims to capture computations with neural networks via learning the models to imitate the execution of classical algorithms. While common architectures are expressive enough to contain the correct model in the weights space, current neural reasoners are struggling to generalize well on out-of-distribution data. On the other hand, classical computations are not affected by distribution shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on the SALSA-CLRS benchmark, where we get perfect test scores for all tasks. Moreover, the proposed architectural choice allows us to prove the correctness of the learned algorithms for any test data. ",
    "url": "https://arxiv.org/abs/2402.11628",
    "authors": [
      "Gleb Rodionov",
      "Liudmila Prokhorenkova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11637",
    "title": "Poisoning Federated Recommender Systems with Fake Users",
    "abstract": "Federated recommendation is a prominent use case within federated learning, yet it remains susceptible to various attacks, from user to server-side vulnerabilities. Poisoning attacks are particularly notable among user-side attacks, as participants upload malicious model updates to deceive the global model, often intending to promote or demote specific targeted items. This study investigates strategies for executing promotion attacks in federated recommender systems. Current poisoning attacks on federated recommender systems often rely on additional information, such as the local training data of genuine users or item popularity. However, such information is challenging for the potential attacker to obtain. Thus, there is a need to develop an attack that requires no extra information apart from item embeddings obtained from the server. In this paper, we introduce a novel fake user based poisoning attack named PoisonFRS to promote the attacker-chosen targeted item in federated recommender systems without requiring knowledge about user-item rating data, user attributes, or the aggregation rule used by the server. Extensive experiments on multiple real-world datasets demonstrate that PoisonFRS can effectively promote the attacker-chosen targeted item to a large portion of genuine users and outperform current benchmarks that rely on additional information about the system. We further observe that the model updates from both genuine and fake users are indistinguishable within the latent space. ",
    "url": "https://arxiv.org/abs/2402.11637",
    "authors": [
      "Ming Yin",
      "Yichang Xu",
      "Minghong Fang",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11638",
    "title": "Stumbling Blocks: Stress Testing the Robustness of Machine-Generated  Text Detectors Under Attacks",
    "abstract": "The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors' robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating. Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches to improve robustness. ",
    "url": "https://arxiv.org/abs/2402.11638",
    "authors": [
      "Yichen Wang",
      "Shangbin Feng",
      "Abe Bohan Hou",
      "Xiao Pu",
      "Chao Shen",
      "Xiaoming Liu",
      "Yulia Tsvetkov",
      "Tianxing He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11641",
    "title": "Towards Versatile Graph Learning Approach: from the Perspective of Large  Language Models",
    "abstract": "Graph-structured data are the commonly used and have wide application scenarios in the real world. For these diverse applications, the vast variety of learning tasks, graph domains, and complex graph learning procedures present challenges for human experts when designing versatile graph learning approaches. Facing these challenges, large language models (LLMs) offer a potential solution due to the extensive knowledge and the human-like intelligence. This paper proposes a novel conceptual prototype for designing versatile graph learning methods with LLMs, with a particular focus on the ``where'' and ``how'' perspectives. From the ``where'' perspective, we summarize four key graph learning procedures, including task definition, graph data feature engineering, model selection and optimization, deployment and serving. We then explore the application scenarios of LLMs in these procedures across a wider spectrum. In the ``how'' perspective, we align the abilities of LLMs with the requirements of each procedure. Finally, we point out the promising directions that could better leverage the strength of LLMs towards versatile graph learning methods. ",
    "url": "https://arxiv.org/abs/2402.11641",
    "authors": [
      "Lanning Wei",
      "Jun Gao",
      "Huan Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11662",
    "title": "TDE-3: An improved prior for optical flow computation in spiking neural  networks",
    "abstract": "Motion detection is a primary task required for robotic systems to perceive and navigate in their environment. Proposed in the literature bioinspired neuromorphic Time-Difference Encoder (TDE-2) combines event-based sensors and processors with spiking neural networks to provide real-time and energy-efficient motion detection through extracting temporal correlations between two points in space. However, on the algorithmic level, this design leads to loss of direction-selectivity of individual TDEs in textured environments. Here we propose an augmented 3-point TDE (TDE-3) with additional inhibitory input that makes TDE-3 direction-selectivity robust in textured environments. We developed a procedure to train the new TDE-3 using backpropagation through time and surrogate gradients to linearly map input velocities into an output spike count or an Inter-Spike Interval (ISI). Our work is the first instance of training a spiking neuron to have a specific ISI. Using synthetic data we compared training and inference with spike count and ISI with respect to changes in stimuli dynamic range, spatial frequency, and level of noise. ISI turns out to be more robust towards variation in spatial frequency, whereas the spike count is a more reliable training signal in the presence of noise. We performed the first in-depth quantitative investigation of optical flow coding with TDE and compared TDE-2 vs TDE-3 in terms of energy-efficiency and coding precision. Results show that on the network level both detectors show similar precision (20 degree angular error, 88% correlation with ground truth). Yet, due to the more robust direction-selectivity of individual TDEs, TDE-3 based network spike less and hence is more energy-efficient. Reported precision is on par with model-based methods but the spike-based processing of the TDEs provides allows more energy-efficient inference with neuromorphic hardware. ",
    "url": "https://arxiv.org/abs/2402.11662",
    "authors": [
      "Matthew Yedutenko",
      "Federico Paredes-Valles",
      "Lyes Khacef",
      "Guido C.H.E. De Croon"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2402.11674",
    "title": "A Fast Algorithm to Simulate Nonlinear Resistive Networks",
    "abstract": "In the quest for energy-efficient artificial intelligence systems, resistor networks are attracting interest as an alternative to conventional GPU-based neural networks. These networks leverage the physics of electrical circuits for inference and can be optimized with local training techniques such as equilibrium propagation. Despite their potential advantage in terms of power consumption, the challenge of efficiently simulating these resistor networks has been a significant bottleneck to assess their scalability, with current methods either being limited to linear networks or relying on realistic, yet slow circuit simulators like SPICE. Assuming ideal circuit elements, we introduce a novel approach for the simulation of nonlinear resistive networks, which we frame as a quadratic programming problem with linear inequality constraints, and which we solve using a fast, exact coordinate descent algorithm. Our simulation methodology significantly outperforms existing SPICE-based simulations, enabling the training of networks up to 325 times larger at speeds 150 times faster, resulting in a 50,000-fold improvement in the ratio of network size to epoch duration. Our approach, adaptable to other electrical components, can foster more rapid progress in the simulations of nonlinear electrical networks. ",
    "url": "https://arxiv.org/abs/2402.11674",
    "authors": [
      "Benjamin Scellier"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11677",
    "title": "MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of  LiDAR-Camera Fusion for 3D Object Detection",
    "abstract": "Multi-modal 3D object detection models for automated driving have demonstrated exceptional performance on computer vision benchmarks like nuScenes. However, their reliance on densely sampled LiDAR point clouds and meticulously calibrated sensor arrays poses challenges for real-world applications. Issues such as sensor misalignment, miscalibration, and disparate sampling frequencies lead to spatial and temporal misalignment in data from LiDAR and cameras. Additionally, the integrity of LiDAR and camera data is often compromised by adverse environmental conditions such as inclement weather, leading to occlusions and noise interference. To address this challenge, we introduce MultiCorrupt, a comprehensive benchmark designed to evaluate the robustness of multi-modal 3D object detectors against ten distinct types of corruptions. We evaluate five state-of-the-art multi-modal detectors on MultiCorrupt and analyze their performance in terms of their resistance ability. Our results show that existing methods exhibit varying degrees of robustness depending on the type of corruption and their fusion strategy. We provide insights into which multi-modal design choices make such models robust against certain perturbations. The dataset generation code and benchmark are open-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt. ",
    "url": "https://arxiv.org/abs/2402.11677",
    "authors": [
      "Till Beemelmanns",
      "Quan Zhang",
      "Lutz Eckstein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11680",
    "title": "3D Point Cloud Compression with Recurrent Neural Network and Image  Compression Methods",
    "abstract": "Storing and transmitting LiDAR point cloud data is essential for many AV applications, such as training data collection, remote control, cloud services or SLAM. However, due to the sparsity and unordered structure of the data, it is difficult to compress point cloud data to a low volume. Transforming the raw point cloud data into a dense 2D matrix structure is a promising way for applying compression algorithms. We propose a new lossless and calibrated 3D-to-2D transformation which allows compression algorithms to efficiently exploit spatial correlations within the 2D representation. To compress the structured representation, we use common image compression methods and also a self-supervised deep compression approach using a recurrent neural network. We also rearrange the LiDAR's intensity measurements to a dense 2D representation and propose a new metric to evaluate the compression performance of the intensity. Compared to approaches that are based on generic octree point cloud compression or based on raw point cloud data compression, our approach achieves the best quantitative and visual performance. Source code and dataset are available at https://github.com/ika-rwth-aachen/Point-Cloud-Compression. ",
    "url": "https://arxiv.org/abs/2402.11680",
    "authors": [
      "Till Beemelmanns",
      "Yuchen Tao",
      "Bastian Lampe",
      "Lennart Reiher",
      "Raphael van Kempen",
      "Timo Woopen",
      "Lutz Eckstein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2402.11695",
    "title": "Enabling Software Defined Optical Networks",
    "abstract": "This paper gives an overview of Software Defined Optical Networks or SDONs and how they can be implemented. It traces the evolution of Optical networks upto GMPLS and traces the idea of SDN and builds upto OpenFlow. The paper explores the need for SDONs and explains what a SDON solution could look like, including the hardware. It also seeks to explain how OpenFlow could be used as a part of this solution to overcome the limitations of GMPLS. ",
    "url": "https://arxiv.org/abs/2402.11695",
    "authors": [
      "Deven Panchal"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Computers and Society (cs.CY)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2402.11702",
    "title": "Can ChatGPT Support Developers? An Empirical Evaluation of Large  Language Models for Code Generation",
    "abstract": "Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development. ",
    "url": "https://arxiv.org/abs/2402.11702",
    "authors": [
      "Kailun Jin",
      "Chung-Yu Wang",
      "Hung Viet Pham",
      "Hadi Hemmati"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11709",
    "title": "GNNavi: Navigating the Information Flow in Large Language Models by  Graph Neural Network",
    "abstract": "Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-based fine-tuning methods in few-shot settings by updating just 0.2% to 0.5% of parameters. We compare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and Adapter in terms of performance and efficiency. Our analysis reveals that GNNavi enhances information flow and ensures a clear aggregation process. ",
    "url": "https://arxiv.org/abs/2402.11709",
    "authors": [
      "Shuzhou Yuan",
      "Ercong Nie",
      "Michael F\u00e4rber",
      "Helmut Schmid",
      "Hinrich Sch\u00fctze"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11722",
    "title": "Invertible Fourier Neural Operators for Tackling Both Forward and  Inverse Problems",
    "abstract": "Fourier Neural Operator (FNO) is a popular operator learning method, which has demonstrated state-of-the-art performance across many tasks. However, FNO is mainly used in forward prediction, yet a large family of applications rely on solving inverse problems. In this paper, we propose an invertible Fourier Neural Operator (iFNO) that tackles both the forward and inverse problems. We designed a series of invertible Fourier blocks in the latent channel space to share the model parameters, efficiently exchange the information, and mutually regularize the learning for the bi-directional tasks. We integrated a variational auto-encoder to capture the intrinsic structures within the input space and to enable posterior inference so as to overcome challenges of illposedness, data shortage, noises, etc. We developed a three-step process for pre-training and fine tuning for efficient training. The evaluations on five benchmark problems have demonstrated the effectiveness of our approach. ",
    "url": "https://arxiv.org/abs/2402.11722",
    "authors": [
      "Da Long",
      "Shandian Zhe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11728",
    "title": "Numerical Claim Detection in Finance: A New Financial Dataset,  Weak-Supervision Model, and Market Analysis",
    "abstract": "In this paper, we investigate the influence of claims in analyst reports and earnings calls on financial market returns, considering them as significant quarterly events for publicly traded companies. To facilitate a comprehensive analysis, we construct a new financial dataset for the claim detection task in the financial domain. We benchmark various language models on this dataset and propose a novel weak-supervision model that incorporates the knowledge of subject matter experts (SMEs) in the aggregation function, outperforming existing approaches. Furthermore, we demonstrate the practical utility of our proposed model by constructing a novel measure ``optimism\". Furthermore, we observed the dependence of earnings surprise and return on our optimism measure. Our dataset, models, and code will be made publicly (under CC BY 4.0 license) available on GitHub and Hugging Face. ",
    "url": "https://arxiv.org/abs/2402.11728",
    "authors": [
      "Agam Shah",
      "Arnav Hiray",
      "Pratvi Shah",
      "Arkaprabha Banerjee",
      "Anushka Singh",
      "Dheeraj Eidnani",
      "Bhaskar Chaudhury",
      "Sudheer Chava"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)"
    ]
  },
  {
    "id": "arXiv:2402.11733",
    "title": "The Effectiveness of Random Forgetting for Robust Generalization",
    "abstract": "Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \"Forget to Mitigate Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios. ",
    "url": "https://arxiv.org/abs/2402.11733",
    "authors": [
      "Vijaya Raghavan T Ramkumar",
      "Bahram Zonooz",
      "Elahe Arani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11735",
    "title": "LiRaFusion: Deep Adaptive LiDAR-Radar Fusion for 3D Object Detection",
    "abstract": "We propose LiRaFusion to tackle LiDAR-radar fusion for 3D object detection to fill the performance gap of existing LiDAR-radar detectors. To improve the feature extraction capabilities from these two modalities, we design an early fusion module for joint voxel feature encoding, and a middle fusion module to adaptively fuse feature maps via a gated network. We perform extensive evaluation on nuScenes to demonstrate that LiRaFusion leverages the complementary information of LiDAR and radar effectively and achieves notable improvement over existing methods. ",
    "url": "https://arxiv.org/abs/2402.11735",
    "authors": [
      "Jingyu Song",
      "Lingjun Zhao",
      "Katherine A. Skinner"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11737",
    "title": "Compression Repair for Feedforward Neural Networks Based on Model  Equivalence Evaluation",
    "abstract": "In this paper, we propose a method of repairing compressed Feedforward Neural Networks (FNNs) based on equivalence evaluation of two neural networks. In the repairing framework, a novel neural network equivalence evaluation method is developed to compute the output discrepancy between two neural networks. The output discrepancy can quantitatively characterize the output difference produced by compression procedures. Based on the computed output discrepancy, the repairing method first initializes a new training set for the compressed networks to narrow down the discrepancy between the two neural networks and improve the performance of the compressed network. Then, we repair the compressed FNN by re-training based on the training set. We apply our developed method to the MNIST dataset to demonstrate the effectiveness and advantages of our proposed repair method. ",
    "url": "https://arxiv.org/abs/2402.11737",
    "authors": [
      "Zihao Mo",
      "Yejiang Yang",
      "Shuaizheng Lu",
      "Weiming Xiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11739",
    "title": "A Transition System Abstraction Framework for Neural Network Dynamical  System Models",
    "abstract": "This paper proposes a transition system abstraction framework for neural network dynamical system models to enhance the model interpretability, with applications to complex dynamical systems such as human behavior learning and verification. To begin with, the localized working zone will be segmented into multiple localized partitions under the data-driven Maximum Entropy (ME) partitioning method. Then, the transition matrix will be obtained based on the set-valued reachability analysis of neural networks. Finally, applications to human handwriting dynamics learning and verification are given to validate our proposed abstraction framework, which demonstrates the advantages of enhancing the interpretability of the black-box model, i.e., our proposed framework is able to abstract a data-driven neural network model into a transition system, making the neural network model interpretable through verifying specifications described in Computational Tree Logic (CTL) languages. ",
    "url": "https://arxiv.org/abs/2402.11739",
    "authors": [
      "Yejiang Yang",
      "Zihao Mo",
      "Hoang-Dung Tran",
      "Weiming Xiang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11740",
    "title": "Extraction of nonlinearity in neural networks and model compression with  Koopman operator",
    "abstract": "Nonlinearity plays a crucial role in deep neural networks. In this paper, we first investigate the degree to which the nonlinearity of the neural network is essential. For this purpose, we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format. The results imply that restricted nonlinearity is enough for the classification of handwritten numbers. Then, we propose a model compression method for deep neural networks, which could be beneficial to handling large networks in resource-constrained environments. Leveraging the Koopman operator, the proposed method enables us to use linear algebra in the internal processing of neural networks. We numerically show that the proposed method performs comparably or better than conventional methods in highly compressed model settings for the handwritten number recognition task. ",
    "url": "https://arxiv.org/abs/2402.11740",
    "authors": [
      "Naoki Sugishita",
      "Kayo Kinjo",
      "Jun Ohkubo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11741",
    "title": "To Store or Not to Store: a graph theoretical approach for Dataset  Versioning",
    "abstract": "In this work, we study the cost efficient data versioning problem, where the goal is to optimize the storage and reconstruction (retrieval) costs of data versions, given a graph of datasets as nodes and edges capturing edit/delta information. One central variant we study is MinSum Retrieval (MSR) where the goal is to minimize the total retrieval costs, while keeping the storage costs bounded. This problem (along with its variants) was introduced by Bhattacherjee et al. [VLDB'15]. While such problems are frequently encountered in collaborative tools (e.g., version control systems and data analysis pipelines), to the best of our knowledge, no existing research studies the theoretical aspects of these problems. We establish that the currently best-known heuristic, LMG, can perform arbitrarily badly in a simple worst case. Moreover, we show that it is hard to get $o(n)$-approximation for MSR on general graphs even if we relax the storage constraints by an $O(\\log n)$ factor. Similar hardness results are shown for other variants. Meanwhile, we propose poly-time approximation schemes for tree-like graphs, motivated by the fact that the graphs arising in practice from typical edit operations are often not arbitrary. As version graphs typically have low treewidth, we further develop new algorithms for bounded treewidth graphs. Furthermore, we propose two new heuristics and evaluate them empirically. First, we extend LMG by considering more potential ``moves'', to propose a new heuristic LMG-All. LMG-All consistently outperforms LMG while having comparable run time on a wide variety of datasets, i.e., version graphs. Secondly, we apply our tree algorithms on the minimum-storage arborescence of an instance, yielding algorithms that are qualitatively better than all previous heuristics for MSR, as well as for another variant BoundedMin Retrieval (BMR). ",
    "url": "https://arxiv.org/abs/2402.11741",
    "authors": [
      "Anxin Guo",
      "Jingwei Li",
      "Pattara Sukprasert",
      "Samir Khuller",
      "Amol Deshpande",
      "Koyel Mukherjee"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Databases (cs.DB)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2402.11753",
    "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
    "abstract": "Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we develop the jailbreak attack ArtPrompt, which leverages the poor performance of LLMs in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from LLMs. ArtPrompt only requires black-box access to the victim LLMs, making it a practical attack. We evaluate ArtPrompt on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five LLMs. ",
    "url": "https://arxiv.org/abs/2402.11753",
    "authors": [
      "Fengqing Jiang",
      "Zhangchen Xu",
      "Luyao Niu",
      "Zhen Xiang",
      "Bhaskar Ramasubramanian",
      "Bo Li",
      "Radha Poovendran"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11755",
    "title": "SPML: A DSL for Defending Language Models Against Prompt Attacks",
    "abstract": "Large language models (LLMs) have profoundly transformed natural language applications, with a growing reliance on instruction-based definitions for designing chatbots. However, post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users, emphasizing the need to prevent unethical applications and financial losses. Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored. This paper presents System Prompt Meta Language (SPML), a domain-specific language for refining prompts and monitoring the inputs to the LLM-based chatbots. SPML actively checks attack prompts, ensuring user inputs align with chatbot definitions to prevent malicious execution on the LLM backbone, optimizing costs. It also streamlines chatbot definition crafting with programming language capabilities, overcoming natural language design challenges. Additionally, we introduce a groundbreaking benchmark with 1.8k system prompts and 20k user inputs, offering the inaugural language and benchmark for chatbot definition evaluation. Experiments across datasets demonstrate SPML's proficiency in understanding attacker prompts, surpassing models like GPT-4, GPT-3.5, and LLAMA. Our data and codes are publicly available at: https://prompt-compiler.github.io/SPML/. ",
    "url": "https://arxiv.org/abs/2402.11755",
    "authors": [
      "Reshabh K Sharma",
      "Vinayak Gupta",
      "Dan Grossman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2402.11760",
    "title": "Reinforcement Learning as a Parsimonious Alternative to Prediction  Cascades: A Case Study on Image Segmentation",
    "abstract": "Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation. This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets. Although such architectures lead to increased accuracy, this is usually accompanied by a large increase in computation and memory requirements during inference. While this is a non-issue in traditional machine learning pipelines, the recent confluence of machine learning and fields like the Internet of Things has rendered such large architectures infeasible for execution in low-resource settings. In such settings, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved. However, we argue that cascaded prediction leads to increased computational cost due to wasteful intermediate computations. To address this, we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a non-cascading, cost-aware learning pipeline as an alternative to cascaded architectures. Through experimental evaluation on real-world and standard datasets, we demonstrate that PaSeR achieves better accuracy while minimizing computational cost relative to cascaded models. Further, we introduce a new metric IoU/GigaFlop to evaluate the balance between cost and performance. On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174% on the IoU/GigaFlop metric with respect to baselines. We also demonstrate PaSeR's adaptability to complementary models trained on a noisy MNIST dataset, where it achieved a minimum performance improvement on IoU/GigaFlop of 13.4% over SOTA models. Code and data are available at https://github.com/scailab/paser . ",
    "url": "https://arxiv.org/abs/2402.11760",
    "authors": [
      "Bharat Srikishan",
      "Anika Tabassum",
      "Srikanth Allu",
      "Ramakrishnan Kannan",
      "Nikhil Muralidhar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11764",
    "title": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient  Debiasing of LLMs",
    "abstract": "Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost. ",
    "url": "https://arxiv.org/abs/2402.11764",
    "authors": [
      "Pengrui Han",
      "Rafal Kocielnik",
      "Adhithya Saravanan",
      "Roy Jiang",
      "Or Sharir",
      "Anima Anandkumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2402.11765",
    "title": "Guide to Numerical Experiments on Elections in Computational Social  Choice",
    "abstract": "We analyze how numerical experiments regarding elections were conducted within the computational social choice literature (focusing on papers published in the IJCAI, AAAI, and AAMAS conferences). We analyze the sizes of the studied elections and the methods used for generating preference data, thereby making previously hidden standards and practices explicit. In particular, we survey a number of statistical cultures for generating elections and their commonly used parameters. ",
    "url": "https://arxiv.org/abs/2402.11765",
    "authors": [
      "Niclas Boehmer",
      "Piotr Faliszewski",
      "\u0141ukasz Janeczko",
      "Andrzej Kaczmarczyk",
      "Grzegorz Lisowski",
      "Grzegorz Pierczy\u0144ski",
      "Simon Rey",
      "Dariusz Stolicki",
      "Stanis\u0142aw Szufa",
      "Tomasz W\u0105s"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2402.11773",
    "title": "Dynamic Multi-Network Mining of Tensor Time Series",
    "abstract": "Subsequence clustering of time series is an essential task in data mining, and interpreting the resulting clusters is also crucial since we generally do not have prior knowledge of the data. Thus, given a large collection of tensor time series consisting of multiple modes, including timestamps, how can we achieve subsequence clustering for tensor time series and provide interpretable insights? In this paper, we propose a new method, Dynamic Multi-network Mining (DMM), that converts a tensor time series into a set of segment groups of various lengths (i.e., clusters) characterized by a dependency network constrained with l1-norm. Our method has the following properties. (a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships. (b) Accurate: it discovers the clusters with distinct networks from tensor time series according to the minimum description length (MDL). (c) Scalable: it scales linearly in terms of the input data size when solving a non-convex problem to optimize the number of segments and clusters, and thus it is applicable to long-range and high-dimensional tensors. Extensive experiments with synthetic datasets confirm that our method outperforms the state-of-the-art methods in terms of clustering accuracy. We then use real datasets to demonstrate that DMM is useful for providing interpretable insights from tensor time series. ",
    "url": "https://arxiv.org/abs/2402.11773",
    "authors": [
      "Kohei Obata",
      "Koki Kawabata",
      "Yasuko Matsubara",
      "Yasushi Sakurai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2402.11793",
    "title": "Generative Kaleidoscopic Networks",
    "abstract": "We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\\in\\mathbb{R}^D$ to itself $f_\\mathcal{N}(x)\\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\\in\\mathbb{R}^D$ and recursively applies $f_\\mathcal{N}(\\cdots f_\\mathcal{N}(z)\\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper the MLP, higher is the quality of samples recovered. Scope: We observed this phenomenon to various degrees for the other deep learning architectures like CNNs, Transformers & U-Nets and we are currently investigating them further. ",
    "url": "https://arxiv.org/abs/2402.11793",
    "authors": [
      "Harsh Shrivastava"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11804",
    "title": "LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge  Graphs",
    "abstract": "Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-resource KG datasets and find that ProLINK outperforms previous methods in three-shot, one-shot, and zero-shot reasoning tasks, exhibiting average performance improvements by 20%, 45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong robustness for various LLM promptings as well as full-shot scenarios. ",
    "url": "https://arxiv.org/abs/2402.11804",
    "authors": [
      "Kai Wang",
      "Yuwei Xu",
      "Zhiyong Wu",
      "Siqiang Luo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.11812",
    "title": "Interpretable Embedding for Ad-hoc Video Search",
    "abstract": "Answering query with semantic concepts has long been the mainstream approach for video search. Until recently, its performance is surpassed by concept-free approach, which embeds queries in a joint space as videos. Nevertheless, the embedded features as well as search results are not interpretable, hindering subsequent steps in video browsing and query reformulation. This paper integrates feature embedding and concept interpretation into a neural network for unified dual-task learning. In this way, an embedding is associated with a list of semantic concepts as an interpretation of video content. This paper empirically demonstrates that, by using either the embedding features or concepts, considerable search improvement is attainable on TRECVid benchmarked datasets. Concepts are not only effective in pruning false positive videos, but also highly complementary to concept-free search, leading to large margin of improvement compared to state-of-the-art approaches. ",
    "url": "https://arxiv.org/abs/2402.11812",
    "authors": [
      "Jiaxin Wu",
      "Chong-Wah Ngo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2402.11821",
    "title": "Microstructures and Accuracy of Graph Recall by Large Language Models",
    "abstract": "Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structural patterns) in their recall. We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from -- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain. ",
    "url": "https://arxiv.org/abs/2402.11821",
    "authors": [
      "Yanbang Wang",
      "Hejie Cui",
      "Jon Kleinberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.11831",
    "title": "Rock Classification Based on Residual Networks",
    "abstract": "Rock Classification is an essential geological problem since it provides important formation information. However, exploration on this problem using convolutional neural networks is not sufficient. To tackle this problem, we propose two approaches using residual neural networks. We first adopt data augmentation methods to enlarge our dataset. By modifying kernel sizes, normalization methods and composition based on ResNet34, we achieve an accuracy of 70.1% on the test dataset, with an increase of 3.5% compared to regular Resnet34. Furthermore, using a similar backbone like BoTNet that incorporates multihead self attention, we additionally use internal residual connections in our model. This boosts the model's performance, achieving an accuracy of 73.7% on the test dataset. We also explore how the number of bottleneck transformer blocks may influence model performance. We discover that models with more than one bottleneck transformer block may not further improve performance. Finally, we believe that our approach can inspire future work related to this problem and our model design can facilitate the development of new residual model architectures. ",
    "url": "https://arxiv.org/abs/2402.11831",
    "authors": [
      "Sining Zhoubian",
      "Yuyang Wang",
      "Zhihuan Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11837",
    "title": "Self-Guided Robust Graph Structure Refinement",
    "abstract": "Recent studies have revealed that GNNs are vulnerable to adversarial attacks. To defend against such attacks, robust graph structure refinement (GSR) methods aim at minimizing the effect of adversarial edges based on node features, graph structure, or external information. However, we have discovered that existing GSR methods are limited by narrowassumptions, such as assuming clean node features, moderate structural attacks, and the availability of external clean graphs, resulting in the restricted applicability in real-world scenarios. In this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a clean sub-graph found within the given attacked graph itself. Furthermore, we propose a novel graph augmentation and a group-training strategy to handle the two technical challenges in the clean sub-graph extraction: 1) loss of structural information, and 2) imbalanced node degree distribution. Extensive experiments demonstrate the effectiveness of SG-GSR under various scenarios including non-targeted attacks, targeted attacks, feature attacks, e-commerce fraud, and noisy node labels. Our code is available at https://github.com/yeonjun-in/torch-SG-GSR. ",
    "url": "https://arxiv.org/abs/2402.11837",
    "authors": [
      "Yeonjun In",
      "Kanghoon Yoon",
      "Kibum Kim",
      "Kijung Shin",
      "Chanyoung Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11838",
    "title": "UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal  Prediction",
    "abstract": "Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal knowledge-guided prompts that align and leverage intrinsic and shared knowledge across scenarios. These designs together unlock the potential of a one-for-all model for spatio-temporal prediction with powerful generalization capability. Extensive experiments on 15 cities and 6 domains demonstrate the universality of UniST in advancing state-of-the-art prediction performance, especially in few-shot and zero-shot scenarios. ",
    "url": "https://arxiv.org/abs/2402.11838",
    "authors": [
      "Yuan Yuan",
      "Jingtao Ding",
      "Jie Feng",
      "Depeng Jin",
      "Yong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11841",
    "title": "ASGNet: Adaptive Semantic Gate Networks for Log-Based Anomaly Diagnosis",
    "abstract": "Logs are widely used in the development and maintenance of software systems. Logs can help engineers understand the runtime behavior of systems and diagnose system failures. For anomaly diagnosis, existing methods generally use log event data extracted from historical logs to build diagnostic models. However, we find that existing methods do not make full use of two types of features, (1) statistical features: some inherent statistical features in log data, such as word frequency and abnormal label distribution, are not well exploited. Compared with log raw data, statistical features are deterministic and naturally compatible with corresponding tasks. (2) semantic features: Logs contain the execution logic behind software systems, thus log statements share deep semantic relationships. How to effectively combine statistical features and semantic features in log data to improve the performance of log anomaly diagnosis is the key point of this paper. In this paper, we propose an adaptive semantic gate networks (ASGNet) that combines statistical features and semantic features to selectively use statistical features to consolidate log text semantic representation. Specifically, ASGNet encodes statistical features via a variational encoding module and fuses useful information through a well-designed adaptive semantic threshold mechanism. The threshold mechanism introduces the information flow into the classifier based on the confidence of the semantic features in the decision, which is conducive to training a robust classifier and can solve the overfitting problem caused by the use of statistical features. The experimental results on the real data set show that our method proposed is superior to all baseline methods in terms of various performance indicators. ",
    "url": "https://arxiv.org/abs/2402.11841",
    "authors": [
      "Haitian Yang",
      "Degang Sun",
      "Wen Liu",
      "Yanshu Li",
      "Yan Wang",
      "Weiqing Huang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2402.11842",
    "title": "CodeArt: Better Code Models by Attention Regularization When Symbols Are  Lacking",
    "abstract": "Transformer based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more important compared to others. To realize the idea, we enhance the vanilla tokenization and model architecture of a BERT model, construct and utilize attention masks, and introduce a new pre-training algorithm. We pre-train this BERT-like model from scratch, using a dataset of 26 million stripped binary functions with explicit program dependence information extracted by our tool. We apply the model in three downstream tasks: binary similarity, type inference, and malware family classification. Our pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49% to 60%, and 74% to 94%, respectively. It also substantially outperforms other general pre-training techniques of code understanding models. ",
    "url": "https://arxiv.org/abs/2402.11842",
    "authors": [
      "Zian Su",
      "Xiangzhe Xu",
      "Ziyang Huang",
      "Zhuo Zhang",
      "Yapeng Ye",
      "Jianjun Huang",
      "Xiangyu Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11843",
    "title": "WildFake: A Large-scale Challenging Dataset for AI-Generated Images  Detection",
    "abstract": "The extraordinary ability of generative models enabled the generation of images with such high quality that human beings cannot distinguish Artificial Intelligence (AI) generated images from real-life photographs. The development of generation techniques opened up new opportunities but concurrently introduced potential risks to privacy, authenticity, and security. Therefore, the task of detecting AI-generated imagery is of paramount importance to prevent illegal activities. To assess the generalizability and robustness of AI-generated image detection, we present a large-scale dataset, referred to as WildFake, comprising state-of-the-art generators, diverse object categories, and real-world applications. WildFake dataset has the following advantages: 1) Rich Content with Wild collection: WildFake collects fake images from the open-source community, enriching its diversity with a broad range of image classes and image styles. 2) Hierarchical structure: WildFake contains fake images synthesized by different types of generators from GANs, diffusion models, to other generative models. These key strengths enhance the generalization and robustness of detectors trained on WildFake, thereby demonstrating WildFake's considerable relevance and effectiveness for AI-generated detectors in real-world scenarios. Moreover, our extensive evaluation experiments are tailored to yield profound insights into the capabilities of different levels of generative models, a distinctive advantage afforded by WildFake's unique hierarchical structure. ",
    "url": "https://arxiv.org/abs/2402.11843",
    "authors": [
      "Yan Hong",
      "Jianfu Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11845",
    "title": "Modularized Networks for Few-shot Hateful Meme Detection",
    "abstract": "In this paper, we address the challenge of detecting hateful memes in the low-resource setting where only a few labeled examples are available. Our approach leverages the compositionality of Low-rank adaptation (LoRA), a widely used parameter-efficient tuning technique. We commence by fine-tuning large language models (LLMs) with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules. These modules are capable of essential reasoning skills for hateful meme detection. We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference. ",
    "url": "https://arxiv.org/abs/2402.11845",
    "authors": [
      "Rui Cao",
      "Roy Ka-Wei Lee",
      "Jing Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11853",
    "title": "Beyond Voice Assistants: Exploring Advantages and Risks of an In-Car  Social Robot in Real Driving Scenarios",
    "abstract": "In-car Voice Assistants (VAs) play an increasingly critical role in automotive user interface design. However, existing VAs primarily perform simple 'query-answer' tasks, limiting their ability to sustain drivers' long-term attention. In this study, we investigate the effectiveness of an in-car Robot Assistant (RA) that offers functionalities beyond voice interaction. We aim to answer the question: How does the presence of a social robot impact user experience in real driving scenarios? Our study begins with a user survey to understand perspectives on in-car VAs and their influence on driving experiences. We then conduct non-driving and on-road experiments with selected participants to assess user experiences with an RA. Additionally, we conduct subjective ratings to evaluate user perceptions of the RA's personality, which is crucial for robot design. We also explore potential concerns regarding ethical risks. Finally, we provide a comprehensive discussion and recommendations for the future development of in-car RAs. ",
    "url": "https://arxiv.org/abs/2402.11853",
    "authors": [
      "Yuanchao Li",
      "Lachlan Urquhart",
      "Nihan Karatas",
      "Shun Shao",
      "Hiroshi Ishiguro",
      "Xun Shen"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2402.11879",
    "title": "Incipient Slip Detection by Vibration Injection into Soft Sensor",
    "abstract": "In robotic manipulation, preventing objects from slipping and establishing a secure grip on them is critical. Successful manipulation requires tactile sensors that detect the microscopic incipient slip phenomenon at the contact surface. Unfortunately, the tiny signals generated by incipient slip are quickly buried by environmental noise, and precise stress-distribution measurement requires an extensive optical system and integrated circuits. In this study, we focus on the macroscopic deformation of the entire fingertip's soft structure instead of directly observing the contact surface and its role as a vibration medium for sensing. The proposed method compresses the stick ratio's information into a one-dimensional pressure signal using the change in the propagation characteristics by vibration injection into the soft structure, which magnifies the microscopic incipient slip phenomena into the entire deformation. This mechanism allows a tactile sensor to use just a single vibration sensor. In the implemented system, a biomimetic tactile sensor is vibrated using a white signal from a PZT motor and utilizes frequency spectrum change of the propagated vibration as features. We investigated the proposed method's effectiveness on stick-ratio estimation and \\red{stick-ratio stabilization} control during incipient slip. Our estimation error and the control performance results significantly outperformed the conventional methods. ",
    "url": "https://arxiv.org/abs/2402.11879",
    "authors": [
      "Naoto Komeno",
      "Takamitsu Matsubara"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2402.11887",
    "title": "Generative Semi-supervised Graph Anomaly Detection",
    "abstract": "This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and as a result, they fail to take account of the graph structure information. Our approach tackles this problem by generating graph structure-aware outlier nodes that have asymmetric affinity separability from normal nodes while being enforced to achieve egocentric closeness to normal nodes in the node representation space. Comprehensive experiments on four real-world datasets are performed to establish a benchmark for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes. Code will be made available at https://github.com/mala-lab/GGAD. ",
    "url": "https://arxiv.org/abs/2402.11887",
    "authors": [
      "Hezhe Qiao",
      "Qingsong Wen",
      "Xiaoli Li",
      "Ee-Peng Lim",
      "Guansong Pang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11897",
    "title": "Enhancing Power Prediction of Photovoltaic Systems: Leveraging Dynamic  Physical Model for Irradiance-to-Power Conversion",
    "abstract": "Power prediction is crucial to the efficiency and reliability of Photovoltaic (PV) systems. For the model-chain-based (also named indirect or physical) power prediction, the conversion of ground environmental data (plane-of-array irradiance and module temperature) to the output power is a fundamental step, commonly accomplished through physical modeling. The core of the physical model lies in the parameters. However, traditional parameter estimation either relies on datasheet information that cannot reflect the system's current health status or necessitates additional I-V characterization of the entire array, which is not commonly available. To address this, our paper introduces PVPro, a dynamic physical modeling method for irradiance-to-power conversion. It extracts model parameters from the recent production data without requiring I-V curve measurements. This dynamic model, periodically-updated (as short as daily), can closely capture the actual health status, enabling precise power estimation. To evaluate the performance, PVPro is compared with the smart persistence, nominal physical, and various machine learning models for day-ahead power prediction. The results indicate that PVPro achieves an outstanding power estimation performance with the average nMAE =1.4% across four field PV systems, reducing the error by 17.6% compared to the best of other techniques. Furthermore, PVPro demonstrates robustness across different seasons and weather conditions. More importantly, PVPro can also perform well with a limited amount of historical production data (3 days), rendering it applicable for new PV systems. The tool is available as a Python package at: https://github.com/DuraMAT/pvpro. ",
    "url": "https://arxiv.org/abs/2402.11897",
    "authors": [
      "Baojie Li",
      "Xin Chen",
      "Anubhav Jain"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2402.11913",
    "title": "PhySU-Net: Long Temporal Context Transformer for rPPG with  Self-Supervised Pre-training",
    "abstract": "Remote photoplethysmography (rPPG) is a promising technology that consists of contactless measuring of cardiac activity from facial videos. Most recent approaches utilize convolutional networks with limited temporal modeling capability or ignore long temporal context. Supervised rPPG methods are also severely limited by scarce data availability. In this work, we propose PhySU-Net, the first long spatial-temporal map rPPG transformer network and a self-supervised pre-training strategy that exploits unlabeled data to improve our model. Our strategy leverages traditional methods and image masking to provide pseudo-labels for self-supervised pre-training. Our model is tested on two public datasets (OBF and VIPL-HR) and shows superior performance in supervised training. Furthermore, we demonstrate that our self-supervised pre-training strategy further improves our model's performance by leveraging representations learned from unlabeled data. ",
    "url": "https://arxiv.org/abs/2402.11913",
    "authors": [
      "Marko Savic",
      "Guoying Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11919",
    "title": "Unraveling Complex Data Diversity in Underwater Acoustic Target  Recognition through Convolution-based Mixture of Experts",
    "abstract": "Underwater acoustic target recognition is a difficult task owing to the intricate nature of underwater acoustic signals. The complex underwater environments, unpredictable transmission channels, and dynamic motion states greatly impact the real-world underwater acoustic signals, and may even obscure the intrinsic characteristics related to targets. Consequently, the data distribution of underwater acoustic signals exhibits high intra-class diversity, thereby compromising the accuracy and robustness of recognition systems.To address these issues, this work proposes a convolution-based mixture of experts (CMoE) that recognizes underwater targets in a fine-grained manner. The proposed technique introduces multiple expert layers as independent learners, along with a routing layer that determines the assignment of experts according to the characteristics of inputs. This design allows the model to utilize independent parameter spaces, facilitating the learning of complex underwater signals with high intra-class diversity. Furthermore, this work optimizes the CMoE structure by balancing regularization and an optional residual module. To validate the efficacy of our proposed techniques, we conducted detailed experiments and visualization analyses on three underwater acoustic databases across several acoustic features. The experimental results demonstrate that our CMoE consistently achieves significant performance improvements, delivering superior recognition accuracy when compared to existing advanced methods. ",
    "url": "https://arxiv.org/abs/2402.11919",
    "authors": [
      "Yuan Xie",
      "Jiawei Ren",
      "Ji Xu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2402.11922",
    "title": "A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer  Learning",
    "abstract": "Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning. Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities. We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/PLUTO-SCY/GPDiff. ",
    "url": "https://arxiv.org/abs/2402.11922",
    "authors": [
      "Yuan Yuan",
      "Chenyang Shao",
      "Jingtao Ding",
      "Depeng Jin",
      "Yong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11928",
    "title": "Separating common from salient patterns with Contrastive Representation  Learning",
    "abstract": "Contrastive Analysis is a sub-field of Representation Learning that aims at separating common factors of variation between two datasets, a background (i.e., healthy subjects) and a target (i.e., diseased subjects), from the salient factors of variation, only present in the target dataset. Despite their relevance, current models based on Variational Auto-Encoders have shown poor performance in learning semantically-expressive representations. On the other hand, Contrastive Representation Learning has shown tremendous performance leaps in various applications (classification, clustering, etc.). In this work, we propose to leverage the ability of Contrastive Learning to learn semantically expressive representations well adapted for Contrastive Analysis. We reformulate it under the lens of the InfoMax Principle and identify two Mutual Information terms to maximize and one to minimize. We decompose the first two terms into an Alignment and a Uniformity term, as commonly done in Contrastive Learning. Then, we motivate a novel Mutual Information minimization strategy to prevent information leakage between common and salient distributions. We validate our method, called SepCLR, on three visual datasets and three medical datasets, specifically conceived to assess the pattern separation capability in Contrastive Analysis. Code available at https://github.com/neurospin-projects/2024_rlouiset_sep_clr. ",
    "url": "https://arxiv.org/abs/2402.11928",
    "authors": [
      "Robin Louiset",
      "Edouard Duchesnay",
      "Antoine Grigis",
      "Pietro Gori"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11931",
    "title": "Soft-Weighted CrossEntropy Loss for Continous Alzheimer's Disease  Detection",
    "abstract": "Alzheimer's disease is a common cognitive disorder in the elderly. Early and accurate diagnosis of Alzheimer's disease (AD) has a major impact on the progress of research on dementia. At present, researchers have used machine learning methods to detect Alzheimer's disease from the speech of participants. However, the recognition accuracy of current methods is unsatisfactory, and most of them focus on using low-dimensional handcrafted features to extract relevant information from audios. This paper proposes an Alzheimer's disease detection system based on the pre-trained framework Wav2vec 2.0 (Wav2vec2). In addition, by replacing the loss function with the Soft-Weighted CrossEntropy loss function, we achieved 85.45\\% recognition accuracy on the same test dataset. ",
    "url": "https://arxiv.org/abs/2402.11931",
    "authors": [
      "Xiaohui Zhang",
      "Wenjie Fu",
      "Mangui Liang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2402.11933",
    "title": "SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via  Self-Supervised Learning",
    "abstract": "To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones. Failure in these tasks for a node signals its deviation from the norm. Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t. the graph size) in response to each new edge in the input stream. In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision. ",
    "url": "https://arxiv.org/abs/2402.11933",
    "authors": [
      "Jongha Lee",
      "Sunwoo Kim",
      "Kijung Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.11940",
    "title": "AICAttack: Adversarial Image Captioning Attack with Attention-Based  Optimization",
    "abstract": "Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchmark datasets with multiple victim models. The experimental results demonstrate that our method surpasses current leading-edge techniques by effectively distributing the alignment and semantics of words in the output. ",
    "url": "https://arxiv.org/abs/2402.11940",
    "authors": [
      "Jiyao Li",
      "Mingze Ni",
      "Yifei Dong",
      "Tianqing Zhu",
      "Wei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11942",
    "title": "The effect of Leaky ReLUs on the training and generalization of  overparameterized networks",
    "abstract": "We investigate the training and generalization errors of overparameterized neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU) functions. More specifically, we carefully upper bound both the convergence rate of the training error and the generalization error of such NNs and investigate the dependence of these bounds on the Leaky ReLU parameter, $\\alpha$. We show that $\\alpha =-1$, which corresponds to the absolute value activation function, is optimal for the training error bound. Furthermore, in special settings, it is also optimal for the generalization error bound. Numerical experiments empirically support the practical choices guided by the theory. ",
    "url": "https://arxiv.org/abs/2402.11942",
    "authors": [
      "Yinglong Guo",
      "Shaohan Li",
      "Gilad Lerman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11943",
    "title": "LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with  External Knowledge Augmentation",
    "abstract": "The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. In this paper, we first investigate the potential of LVLM on multimodal misinformation detection. We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation. LEMMA leverages LVLM intuition and reasoning capabilities while augmenting them with external knowledge to enhance the accuracy of misinformation detection. Our method improves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and Fakeddit datasets respectively. ",
    "url": "https://arxiv.org/abs/2402.11943",
    "authors": [
      "Keyang Xuan",
      "Li Yi",
      "Fan Yang",
      "Ruochen Wu",
      "Yi R. Fung",
      "Heng Ji"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.11953",
    "title": "Stealing the Invisible: Unveiling Pre-Trained CNN Models through  Adversarial Examples and Timing Side-Channels",
    "abstract": "Machine learning, with its myriad applications, has become an integral component of numerous technological systems. A common practice in this domain is the use of transfer learning, where a pre-trained model's architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it's crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method. Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerprint several renowned Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures. We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required. We have presented our results over 27 pre-trained models of different CNN and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under 20. ",
    "url": "https://arxiv.org/abs/2402.11953",
    "authors": [
      "Shubhi Shukla",
      "Manaar Alam",
      "Pabitra Mitra",
      "Debdeep Mukhopadhyay"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11984",
    "title": "Hebbian Learning based Orthogonal Projection for Continual Learning of  Spiking Neural Networks",
    "abstract": "Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting activity traces of neurons into an orthogonal subspace so that synaptic weight update will not interfere with old tasks. We show that Hebbian and anti-Hebbian learning on recurrent lateral connections can effectively extract the principal subspace of neural activities and enable orthogonal projection. This provides new insights into how neural circuits and Hebbian learning can help continual learning, and also how the concept of orthogonal projection can be realized in neuronal systems. Our method is also flexible to utilize arbitrary training methods based on presynaptic activities/traces. Experiments show that our method consistently solves forgetting for spiking neural networks with nearly zero forgetting under various supervised training methods with different error propagation approaches, and outperforms previous approaches under various settings. Our method can pave a solid path for building continual neuromorphic computing systems. ",
    "url": "https://arxiv.org/abs/2402.11984",
    "authors": [
      "Mingqing Xiao",
      "Qingyan Meng",
      "Zongpeng Zhang",
      "Di He",
      "Zhouchen Lin"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11985",
    "title": "Weakly Supervised Object Detection in Chest X-Rays with Differentiable  ROI Proposal Networks and Soft ROI Pooling",
    "abstract": "Weakly supervised object detection (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision. The successes of multiple instance learning in this task for natural images, however, do not translate well to medical images due to the very different characteristics of their objects (i.e. pathologies). In this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new method for generating bounding box proposals on the fly using a specialized region of interest-attention (ROI-attention) module. WSRPN integrates well with classic backbone-head classification algorithms and is end-to-end trainable with only image-label supervision. We experimentally demonstrate that our new method outperforms existing methods in the challenging task of disease localization in chest X-ray images. Code: https://github.com/philip-mueller/wsrpn ",
    "url": "https://arxiv.org/abs/2402.11985",
    "authors": [
      "Philip M\u00fcller",
      "Felix Meissen",
      "Georgios Kaissis",
      "Daniel Rueckert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11995",
    "title": "Network Inversion of Binarised Neural Nets",
    "abstract": "While the deployment of neural networks, yielding impressive results, becomes more prevalent in various applications, their interpretability and understanding remain a critical challenge. Network inversion, a technique that aims to reconstruct the input space from the model's learned internal representations, plays a pivotal role in unraveling the black-box nature of input to output mappings in neural networks. In safety-critical scenarios, where model outputs may influence pivotal decisions, the integrity of the corresponding input space is paramount, necessitating the elimination of any extraneous \"garbage\" to ensure the trustworthiness of the network. Binarised Neural Networks (BNNs), characterized by binary weights and activations, offer computational efficiency and reduced memory requirements, making them suitable for resource-constrained environments. This paper introduces a novel approach to invert a trained BNN by encoding it into a CNF formula that captures the network's structure, allowing for both inference and inversion. ",
    "url": "https://arxiv.org/abs/2402.11995",
    "authors": [
      "Pirzada Suhail",
      "Supratik Chakraborty",
      "Amit Sethi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11996",
    "title": "ISCUTE: Instance Segmentation of Cables Using Text Embedding",
    "abstract": "In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a foundation model-based DLO instance segmentation technique that is text-promptable and user-friendly. Specifically, our approach combines the text-conditioned semantic segmentation capabilities of CLIPSeg model with the zero-shot generalization capabilities of Segment Anything Model (SAM). We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21\\%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation. ",
    "url": "https://arxiv.org/abs/2402.11996",
    "authors": [
      "Shir Kozlovsky",
      "Omkar Joglekar",
      "Dotan Di Castro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.12001",
    "title": "A Survey on Extractive Knowledge Graph Summarization: Applications,  Approaches, Evaluation, and Future Directions",
    "abstract": "With the continuous growth of large Knowledge Graphs (KGs), extractive KG summarization becomes a trending task. Aiming at distilling a compact subgraph with condensed information, it facilitates various downstream KG-based tasks. In this survey paper, we are among the first to provide a systematic overview of its applications and define a taxonomy for existing methods from its interdisciplinary studies. Future directions are also laid out based on our extensive and comparative review. ",
    "url": "https://arxiv.org/abs/2402.12001",
    "authors": [
      "Xiaxia Wang",
      "Gong Cheng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)",
      "Information Retrieval (cs.IR)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.12015",
    "title": "An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart  Target Tracking",
    "abstract": "In solving the non-myopic radar scheduling for multiple smart target tracking within an active and passive radar network, we need to consider both short-term enhanced tracking performance and a higher probability of target maneuvering in the future with active tracking. Acquiring the long-term tracking performance while scheduling the beam resources of active and passive radars poses a challenge. To address this challenge, we model this problem as a Markov decision process consisting of parallel restless bandit processes. Each bandit process is associated with a smart target, of which the estimation state evolves according to different discrete dynamic models for different actions - whether or not the target is being tracked. The discrete state is defined by the dynamic mode. The problem exhibits the curse of dimensionality, where optimal solutions are in general intractable. We resort to heuristics through the famous restless multi-armed bandit techniques. It follows with efficient scheduling policies based on the indices that are real numbers representing the marginal rewards of taking different actions. For the inevitable practical case with unknown transition matrices, we propose a new method that utilizes the forward Sarsa and backward Q-learning to approximate the indices through adapting the state-action value functions, or equivalently the Q-functions, and propose a new policy, namely ISQ, aiming to maximize the long-term tracking rewards. Numerical results demonstrate that the proposed ISQ policy outperforms conventional Q-learning-based methods and rapidly converges to the well-known Whittle index policy with revealed state transition models, which is considered the benchmark. ",
    "url": "https://arxiv.org/abs/2402.12015",
    "authors": [
      "Yuhang Hao",
      "Zengfu Wang",
      "Jing Fu",
      "Quan Pan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.12018",
    "title": "Even-Cycle Detection in the Randomized and Quantum CONGEST Model",
    "abstract": "We show that, for every $k\\geq 2$, $C_{2k}$-freeness can be decided in $O(n^{1-1/k})$ rounds in the \\CONGEST{} model by a randomized Monte-Carlo distributed algorithm with one-sided error probability $1/3$. This matches the best round-complexities of previously known algorithms for $k\\in\\{2,3,4,5\\}$ by Drucker et al. [PODC'14] and Censor-Hillel et al. [DISC'20], but improves the complexities of the known algorithms for $k>5$ by Eden et al. [DISC'19], which were essentially of the form $\\tilde O(n^{1-2/k^2})$. Our algorithm uses colored BFS-explorations with threshold, but with an original \\emph{global} approach that enables to overcome a recent impossibility result by Fraigniaud et al. [SIROCCO'23] about using colored BFS-exploration with \\emph{local} threshold for detecting cycles. We also show how to quantize our algorithm for achieving a round-complexity $\\tilde O(n^{\\frac{1}{2}-\\frac{1}{2k}})$ in the quantum setting for deciding $C_{2k}$ freeness. Furthermore, this allows us to improve the known quantum complexities of the simpler problem of detecting cycles of length \\emph{at most}~$2k$ by van Apeldoorn and de Vos [PODC'22]. Our quantization is in two steps. First, the congestion of our randomized algorithm is reduced, to the cost of reducing its success probability too. Second, the success probability is boosted using a new quantum framework derived from sequential algorithms, namely Monte-Carlo quantum amplification. ",
    "url": "https://arxiv.org/abs/2402.12018",
    "authors": [
      "Pierre Fraigniaud",
      "Mael Luce",
      "Frederic Magniez",
      "Ioan Todinca"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2402.12022",
    "title": "Distilling Large Language Models for Text-Attributed Graph Learning",
    "abstract": "Text-Attributed Graphs (TAGs) are graphs of connected textual documents. Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning. To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale. Extensive experiments validate the efficacy of our proposed framework. ",
    "url": "https://arxiv.org/abs/2402.12022",
    "authors": [
      "Bo Pan",
      "Zheng Zhang",
      "Yifei Zhang",
      "Yuntong Hu",
      "Liang Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.12026",
    "title": "Acquiring Clean Language Models from Backdoor Poisoned Datasets by  Downscaling Frequency Space",
    "abstract": "Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, and Llama2. The codes are available at https://github.com/ZrW00/MuScleLoRA. ",
    "url": "https://arxiv.org/abs/2402.12026",
    "authors": [
      "Zongru Wu",
      "Zhuosheng Zhang",
      "Pengzhou Cheng",
      "Gongshen Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.12032",
    "title": "Flexible Robust Optimal Bidding of Renewable Virtual Power Plants in  Sequential Markets",
    "abstract": "In this paper, a novel approach to define the optimal bidding of renewable-only virtual power plants (RVPPs) in the day-ahead, secondary reserve, and intra-day markets is proposed. To this aim, a robust optimization algorithm is developed to account for the asymmetric nature of the uncertainties that characterize the market prices, as well as the energy production of the RVPP stochastic sources and flexible demand consumption. Simulation results show increased RVPP benefits compared to other existing solutions and demonstrate the potential of renewable sources to further increase their economic competitiveness. The simplicity of the implementation, the computational efficiency, and the flexible robustness are also verified. ",
    "url": "https://arxiv.org/abs/2402.12032",
    "authors": [
      "Hadi Nemati",
      "Pedro S\u00e1nchez-Mart\u00edn",
      "\u00c1lvaro Ortega",
      "Lukas Sigrist",
      "Enrique Lobato",
      "Luis Rouco"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2402.12040",
    "title": "Attack Tree Generation via Process Mining",
    "abstract": "Attack Trees are a graphical model of security used to study threat scenarios. While visually appealing and supported by solid theories and effective tools, one of their main drawbacks remains the amount of effort required by security experts to design them from scratch. This work aims to remedy this by providing a method for the automatic generation of Attack Trees from attack logs. The main original feature of our approach w.r.t existing ones is the use of Process Mining algorithms to synthesize Attack Trees, which allow users to customize the way a set of logs are summarized as an Attack Tree, for example by discarding statistically irrelevant events. Our approach is supported by a prototype that, apart from the derivation and translation of the model, provides the user with an Attack Tree in the RisQFLan format, a tool used for quantitative risk modeling and analysis with Attack Trees. We illustrate our approach with the case study of attacks on a communication protocol, produced by a state-of-the-art protocol analyzer. ",
    "url": "https://arxiv.org/abs/2402.12040",
    "authors": [
      "Alyzia-Maria Konsta",
      "Gemma Di Federico",
      "Alberto Lluch Lafuente",
      "Andrea Burattin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Formal Languages and Automata Theory (cs.FL)"
    ]
  },
  {
    "id": "arXiv:2402.12062",
    "title": "Causal Equal Protection as Algorithmic Fairness",
    "abstract": "Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classification parity, but also fails to model our moral intuitions in a number of common scenarios, for example, when the predictor is causally downstream relative to the protected characteristic. To address these difficulties, we defend a novel principle, causal equal protection, that models the fair allocation of the risks of erroneous classification through the lenses of causality. ",
    "url": "https://arxiv.org/abs/2402.12062",
    "authors": [
      "Marcello Di Bello",
      "Nicol\u00f2 Cangiotti",
      "Michele Loi"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.12074",
    "title": "HIP Network: Historical Information Passing Network for Extrapolation  Reasoning on Temporal Knowledge Graph",
    "abstract": "In recent years, temporal knowledge graph (TKG) reasoning has received significant attention. Most existing methods assume that all timestamps and corresponding graphs are available during training, which makes it difficult to predict future events. To address this issue, recent works learn to infer future events based on historical information. However, these methods do not comprehensively consider the latent patterns behind temporal changes, to pass historical information selectively, update representations appropriately and predict events accurately. In this paper, we propose the Historical Information Passing (HIP) network to predict future events. HIP network passes information from temporal, structural and repetitive perspectives, which are used to model the temporal evolution of events, the interactions of events at the same time step, and the known events respectively. In particular, our method considers the updating of relation representations and adopts three scoring functions corresponding to the above dimensions. Experimental results on five benchmark datasets show the superiority of HIP network, and the significant improvements on Hits@1 prove that our method can more accurately predict what is going to happen. ",
    "url": "https://arxiv.org/abs/2402.12074",
    "authors": [
      "Yongquan He",
      "Peng Zhang",
      "Luchen Liu",
      "Qi Liang",
      "Wenyuan Zhang",
      "Chuang Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.12100",
    "title": "Groot: Adversarial Testing for Generative Text-to-Image Models with  Tree-based Semantic Transformation",
    "abstract": "With the prevalence of text-to-image generative models, their safety becomes a critical concern. adversarial testing techniques have been developed to probe whether such models can be prompted to produce Not-Safe-For-Work (NSFW) content. However, existing solutions face several challenges, including low success rate and inefficiency. We introduce Groot, the first automated framework leveraging tree-based semantic transformation for adversarial testing of text-to-image models. Groot employs semantic decomposition and sensitive element drowning strategies in conjunction with LLMs to systematically refine adversarial prompts. Our comprehensive evaluation confirms the efficacy of Groot, which not only exceeds the performance of current state-of-the-art approaches but also achieves a remarkable success rate (93.66%) on leading text-to-image models such as DALL-E 3 and Midjourney. ",
    "url": "https://arxiv.org/abs/2402.12100",
    "authors": [
      "Yi Liu",
      "Guowei Yang",
      "Gelei Deng",
      "Feiyue Chen",
      "Yuqi Chen",
      "Ling Shi",
      "Tianwei Zhang",
      "Yang Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2402.12132",
    "title": "SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and  Versatile Dynamic Information Embedding",
    "abstract": "Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations. This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors. ",
    "url": "https://arxiv.org/abs/2402.12132",
    "authors": [
      "Ruiyi Yang",
      "Flora D. Salim",
      "Hao Xue"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.12142",
    "title": "Federated Bayesian Network Ensembles",
    "abstract": "Federated learning allows us to run machine learning algorithms on decentralized data when data sharing is not permitted due to privacy concerns. Ensemble-based learning works by training multiple (weak) classifiers whose output is aggregated. Federated ensembles are ensembles applied to a federated setting, where each classifier in the ensemble is trained on one data location. In this article, we explore the use of federated ensembles of Bayesian networks (FBNE) in a range of experiments and compare their performance with locally trained models and models trained with VertiBayes, a federated learning algorithm to train Bayesian networks from decentralized data. Our results show that FBNE outperforms local models and provides a significant increase in training speed compared with VertiBayes while maintaining a similar performance in most settings, among other advantages. We show that FBNE is a potentially useful tool within the federated learning toolbox, especially when local populations are heavily biased, or there is a strong imbalance in population size across parties. We discuss the advantages and disadvantages of this approach in terms of time complexity, model accuracy, privacy protection, and model interpretability. ",
    "url": "https://arxiv.org/abs/2402.12142",
    "authors": [
      "Florian van Daalen",
      "Lianne Ippel",
      "Andre Dekker",
      "Inigo Bermejo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.12144",
    "title": "Connectivity Labeling in Faulty Colored Graphs",
    "abstract": "Fault-tolerant connectivity labelings are schemes that, given an $n$-vertex graph $G=(V,E)$ and $f\\geq 1$, produce succinct yet informative labels for the elements of the graph. Given only the labels of two vertices $u,v$ and of the elements in a faulty-set $F$ with $|F|\\leq f$, one can determine if $u,v$ are connected in $G-F$, the surviving graph after removing $F$. For the edge or vertex faults models, i.e., $F\\subseteq E$ or $F\\subseteq V$, a sequence of recent work established schemes with $poly(f,\\log n)$-bit labels. This paper considers the color faults model, recently introduced in the context of spanners [Petruschka, Sapir and Tzalik, ITCS'24], which accounts for known correlations between failures. Here, the edges (or vertices) of the input $G$ are arbitrarily colored, and the faulty elements in $F$ are colors; a failing color causes all edges (vertices) of that color to crash. Our main contribution is settling the label length complexity for connectivity under one color fault ($f=1$). The existing implicit solution, by applying the state-of-the-art scheme for edge faults of [Dory and Parter, PODC'21], might yield labels of $\\Omega(n)$ bits. We provide a deterministic scheme with labels of $\\tilde{O}(\\sqrt{n})$ bits in the worst case, and a matching lower bound. Moreover, our scheme is universally optimal: even schemes tailored to handle only colorings of one specific graph topology cannot produce asymptotically smaller labels. We extend our labeling approach to yield a routing scheme avoiding a single forbidden color. We also consider the centralized setting, and show an $\\tilde{O}(n)$-space oracle, answering connectivity queries under one color fault in $\\tilde{O}(1)$ time. Turning to $f\\geq 2$ color faults, we give a randomized labeling scheme with $\\tilde{O}(n^{1-1/2^f})$-bit labels, along with a lower bound of $\\Omega(n^{1-1/(f+1)})$ bits. ",
    "url": "https://arxiv.org/abs/2402.12144",
    "authors": [
      "Asaf Petruschka",
      "Shay Sapir",
      "Elad Tzalik"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2402.12151",
    "title": "Transformer-based Causal Language Models Perform Clustering",
    "abstract": "Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting. ",
    "url": "https://arxiv.org/abs/2402.12151",
    "authors": [
      "Xinbo Wu",
      "Lav R. Varshney"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.12161",
    "title": "Endowing Pre-trained Graph Models with Provable Fairness",
    "abstract": "Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained \\textbf{Graph} models with \\textbf{P}rovable f\\textbf{A}i\\textbf{R}ness (called GraphPAR). GraphPAR freezes the parameters of PGMs and trains a parameter-efficient adapter to flexibly improve the fairness of PGMs in downstream tasks. Specifically, we design a sensitive semantic augmenter on node representations, to extend the node representations with different sensitive attribute semantics for each node. The extended representations will be used to further train an adapter, to prevent the propagation of sensitive attribute semantics from PGMs to task predictions. Furthermore, with GraphPAR, we quantify whether the fairness of each node is provable, i.e., predictions are always fair within a certain range of sensitive attribute semantics. Experimental evaluations on real-world datasets demonstrate that GraphPAR achieves state-of-the-art prediction performance and fairness on node classification task. Furthermore, based on our GraphPAR, around 90\\% nodes have provable fairness. ",
    "url": "https://arxiv.org/abs/2402.12161",
    "authors": [
      "Zhongjian Zhang",
      "Mengmei Zhang",
      "Yue Yu",
      "Cheng Yang",
      "Jiawei Liu",
      "Chuan Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.12162",
    "title": "SCARF: Securing Chips with a Robust Framework against Fabrication-time  Hardware Trojans",
    "abstract": "The globalization of the semiconductor industry has introduced security challenges to Integrated Circuits (ICs), particularly those related to the threat of Hardware Trojans (HTs) - malicious logic that can be introduced during IC fabrication. While significant efforts are directed towards verifying the correctness and reliability of ICs, their security is often overlooked. In this paper, we propose a comprehensive approach to enhance IC security from the front-end to back-end stages of design. Initially, we outline a systematic method to transform existing verification assets into potent security checkers by repurposing verification assertions. To further improve security, we introduce an innovative technique for integrating online monitors during physical synthesis - a back-end insertion providing an additional layer of defense. Experimental results demonstrate a significant increase in security, measured by our introduced metric, Security Coverage (SC), with a marginal rise in area and power consumption, typically under 20\\%. The insertion of online monitors during physical synthesis enhances security metrics by up to 33.5\\%. This holistic approach offers a comprehensive and resilient defense mechanism across the entire spectrum of IC design. ",
    "url": "https://arxiv.org/abs/2402.12162",
    "authors": [
      "Mohammad Eslami",
      "Tara Ghasempouri",
      "Samuel Pagliarini"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2402.12168",
    "title": "Defending Against Weight-Poisoning Backdoor Attacks for  Parameter-Efficient Fine-Tuning",
    "abstract": "Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference process, extreme confidence serves as an indicator for poisoned samples, while others are clean. We conduct experiments on text classification tasks, five fine-tuning strategies, and three weight-poisoning backdoor attack methods. Experiments show near 100% success rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore, our defensive approach exhibits overall competitive performance in mitigating weight-poisoning backdoor attacks. ",
    "url": "https://arxiv.org/abs/2402.12168",
    "authors": [
      "Shuai Zhao",
      "Leilei Gan",
      "Luu Anh Tuan",
      "Jie Fu",
      "Lingjuan Lyu",
      "Meihuizi Jia",
      "Jinming Wen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.12175",
    "title": "Learning Discretized Bayesian Networks with GOMEA",
    "abstract": "Bayesian networks model relationships between random variables under uncertainty and can be used to predict the likelihood of events and outcomes while incorporating observed evidence. From an eXplainable AI (XAI) perspective, such models are interesting as they tend to be compact. Moreover, captured relations can be directly inspected by domain experts. In practice, data is often real-valued. Unless assumptions of normality can be made, discretization is often required. The optimal discretization, however, depends on the relations modelled between the variables. This complicates learning Bayesian networks from data. For this reason, most literature focuses on learning conditional dependencies between sets of variables, called structure learning. In this work, we extend an existing state-of-the-art structure learning approach based on the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) to jointly learn variable discretizations. The proposed Discretized Bayesian Network GOMEA (DBN-GOMEA) obtains similar or better results than the current state-of-the-art when tasked to retrieve randomly generated ground-truth networks. Moreover, leveraging a key strength of evolutionary algorithms, we can straightforwardly perform DBN learning multi-objectively. We show how this enables incorporating expert knowledge in a uniquely insightful fashion, finding multiple DBNs that trade-off complexity, accuracy, and the difference with a pre-determined expert network. ",
    "url": "https://arxiv.org/abs/2402.12175",
    "authors": [
      "Damy M.F. Ha",
      "Tanja Alderliesten",
      "Peter A.N. Bosman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2402.12181",
    "title": "Revisiting Data Augmentation in Deep Reinforcement Learning",
    "abstract": "Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge. We evaluate our proposition and validate our analysis in several domains. Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments. ",
    "url": "https://arxiv.org/abs/2402.12181",
    "authors": [
      "Jianshu Hu",
      "Yunpeng Jiang",
      "Paul Weng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.12187",
    "title": "Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep  Learning via Adversarial Training",
    "abstract": "Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples. Adversarial training is used to mitigate this problem by increasing robustness against these attacks. However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples. The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial. AFA mitigates this risk by employing a novel optimization algorithm based on contrastive learning to alleviate potential feature misalignment. Through our evaluations, we demonstrate the superior performance of AFA. The baseline AFA delivers higher robust accuracy than previous adversarial contrastive learning methods while minimizing the drop in clean accuracy to 1.86% and 8.91% on CIFAR10 and CIFAR100, respectively, in comparison to cross-entropy. We also show that joint optimization of AFA and TRADES, accompanied by data augmentation using a recent diffusion model, achieves state-of-the-art accuracy and robustness. ",
    "url": "https://arxiv.org/abs/2402.12187",
    "authors": [
      "Leo Hyun Park",
      "Jaeuk Kim",
      "Myung Gyo Oh",
      "Jaewoo Park",
      "Taekyoung Kwon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.12238",
    "title": "Mixed Gaussian Flow for Diverse Trajectory Prediction",
    "abstract": "Existing trajectory prediction studies intensively leverage generative models. Normalizing flow is one of the genres with the advantage of being invertible to derive the probability density of predicted trajectories. However, mapping from a standard Gaussian by a flow-based model hurts the capacity to capture complicated patterns of trajectories, ignoring the under-represented motion intentions in the training data. To solve the problem, we propose a flow-based model to transform a mixed Gaussian prior into the future trajectory manifold. The model shows a better capacity for generating diverse trajectory patterns. Also, by associating each sub-Gaussian with a certain subspace of trajectories, we can generate future trajectories with controllable motion intentions. In such a fashion, the flow-based model is not encouraged to simply seek the most likelihood of the intended manifold anymore but a family of controlled manifolds with explicit interpretability. Our proposed method is demonstrated to show state-of-the-art performance in the quantitative evaluation of sampling well-aligned trajectories in top-M generated candidates. We also demonstrate that it can generate diverse, controllable, and out-of-distribution trajectories. Code is available at https://github.com/mulplue/MGF. ",
    "url": "https://arxiv.org/abs/2402.12238",
    "authors": [
      "Jiahe Chen",
      "Jinkun Cao",
      "Dahua Lin",
      "Kris Kitani",
      "Jiangmiao Pang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.12241",
    "title": "Convergence of Gradient Descent for Recurrent Neural Networks: A  Nonasymptotic Analysis",
    "abstract": "We analyze recurrent neural networks trained with gradient descent in the supervised learning setting for dynamical systems, and prove that gradient descent can achieve optimality \\emph{without} massive overparameterization. Our in-depth nonasymptotic analysis (i) provides sharp bounds on the network size $m$ and iteration complexity $\\tau$ in terms of the sequence length $T$, sample size $n$ and ambient dimension $d$, and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the Lipschitz continuity of the activation function. Remarkably, this analysis reveals that an appropriately-initialized recurrent neural network trained with $n$ samples can achieve optimality with a network size $m$ that scales only logarithmically with $n$. This sharply contrasts with the prior works that require high-order polynomial dependency of $m$ on $n$ to establish strong regularity conditions. Our results are based on an explicit characterization of the class of dynamical systems that can be approximated and learned by recurrent neural networks via norm-constrained transportation mappings, and establishing local smoothness properties of the hidden state with respect to the learnable parameters. ",
    "url": "https://arxiv.org/abs/2402.12241",
    "authors": [
      "Semih Cayci",
      "Atilla Eryilmaz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.12259",
    "title": "Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with  Queryable Objects and Open-Set Relationships",
    "abstract": "Current approaches for 3D scene graph prediction rely on labeled datasets to train models for a fixed set of known object classes and relationship categories. We present Open3DSG, an alternative approach to learn 3D scene graph prediction in an open world without requiring labeled scene graph data. We co-embed the features from a 3D scene graph prediction backbone with the feature space of powerful open world 2D vision language foundation models. This enables us to predict 3D scene graphs from 3D point clouds in a zero-shot manner by querying object classes from an open vocabulary and predicting the inter-object relationships from a grounded LLM with scene graph features and queried object classes as context. Open3DSG is the first 3D point cloud method to predict not only explicit open-vocabulary object classes, but also open-set relationships that are not limited to a predefined label set, making it possible to express rare as well as specific objects and relationships in the predicted 3D scene graph. Our experiments show that Open3DSG is effective at predicting arbitrary object classes as well as their complex inter-object relationships describing spatial, supportive, semantic and comparative relationships. ",
    "url": "https://arxiv.org/abs/2402.12259",
    "authors": [
      "Sebastian Koch",
      "Narunas Vaskevicius",
      "Mirco Colosi",
      "Pedro Hermosilla",
      "Timo Ropinski"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.12261",
    "title": "NEO-BENCH: Evaluating Robustness of Large Language Models with  Neologisms",
    "abstract": "The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments. ",
    "url": "https://arxiv.org/abs/2402.12261",
    "authors": [
      "Jonathan Zheng",
      "Alan Ritter",
      "Wei Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.12269",
    "title": "End-to-end Supervised Prediction of Arbitrary-size Graphs with  Partially-Masked Fused Gromov-Wasserstein Matching",
    "abstract": "We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors. ",
    "url": "https://arxiv.org/abs/2402.12269",
    "authors": [
      "Paul Krzakala",
      "Junjie Yang",
      "R\u00e9mi Flamary",
      "Florence d'Alch\u00e9 Buc",
      "Charlotte Laclau",
      "Matthieu Labeau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.12271",
    "title": "Secure Federated Learning Across Heterogeneous Cloud and  High-Performance Computing Resources -- A Case Study on Federated Fine-tuning  of LLaMA 2",
    "abstract": "Federated learning enables multiple data owners to collaboratively train robust machine learning models without transferring large or sensitive local datasets by only sharing the parameters of the locally trained models. In this paper, we elaborate on the design of our Advanced Privacy-Preserving Federated Learning (APPFL) framework, which streamlines end-to-end secure and reliable federated learning experiments across cloud computing facilities and high-performance computing resources by leveraging Globus Compute, a distributed function as a service platform, and Amazon Web Services. We further demonstrate the use case of APPFL in fine-tuning a LLaMA 2 7B model using several cloud resources and supercomputers. ",
    "url": "https://arxiv.org/abs/2402.12271",
    "authors": [
      "Zilinghan Li",
      "Shilan He",
      "Pranshu Chaturvedi",
      "Volodymyr Kindratenko",
      "Eliu A Huerta",
      "Kibaek Kim",
      "Ravi Madduri"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.12275",
    "title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing  Code and Interacting with the Environment",
    "abstract": "We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents. ",
    "url": "https://arxiv.org/abs/2402.12275",
    "authors": [
      "Hao Tang",
      "Darren Key",
      "Kevin Ellis"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.12276",
    "title": "Explain then Rank: Scale Calibration of Neural Rankers Using Natural  Language Explanations from Large Language Models",
    "abstract": "The process of scale calibration in ranking systems involves adjusting the outputs of rankers to correspond with significant qualities like click-through rates or relevance, crucial for mirroring real-world value and thereby boosting the system's effectiveness and reliability. Although there has been research on calibrated ranking losses within learning-to-rank models, the particular issue of adjusting the scale for neural rankers, which excel in handling textual information, has not been thoroughly examined. Neural ranking models are adept at processing text data, yet the application of existing scale calibration techniques to these models poses significant challenges due to their complexity and the intensive training they require, often resulting in suboptimal outcomes. This study delves into the potential of large language models (LLMs) to provide uncertainty measurements for a query and document pair that correlate with the scale-calibrated scores. By employing Monte Carlo sampling to gauge relevance probabilities from LLMs and incorporating natural language explanations (NLEs) to articulate this uncertainty, we carry out comprehensive tests on two major document ranking datasets. Our findings reveal that the approach leveraging NLEs outperforms existing calibration methods under various training scenarios, leading to better calibrated neural rankers. ",
    "url": "https://arxiv.org/abs/2402.12276",
    "authors": [
      "Puxuan Yu",
      "Daniel Cohen",
      "Hemank Lamba",
      "Joel Tetreault",
      "Alex Jaimes"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2402.12280",
    "title": "Adaptive Skeleton Graph Decoding",
    "abstract": "Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. Additionally, we leverage difficulty estimates for each sub-problem to select an appropriately-sized model, improving performance without significantly reducing quality. Compared to standard autoregressive generation and SoT, SGD achieves a 1.69x speedup while improving quality by up to 51%. ",
    "url": "https://arxiv.org/abs/2402.12280",
    "authors": [
      "Shuowei Jin",
      "Yongji Wu",
      "Haizhong Zheng",
      "Qingzhao Zhang",
      "Matthew Lentz",
      "Z. Morley Mao",
      "Atul Prakash",
      "Feng Qian",
      "Danyang Zhuo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.12282",
    "title": "Ontology Enhanced Claim Detection",
    "abstract": "We propose an ontology enhanced model for sentence based claim detection. We fused ontology embeddings from a knowledge base with BERT sentence embeddings to perform claim detection for the ClaimBuster and the NewsClaims datasets. Our ontology enhanced approach showed the best results with these small-sized unbalanced datasets, compared to other statistical and neural machine learning models. The experiments demonstrate that adding domain specific features (either trained word embeddings or knowledge graph metadata) can improve traditional ML methods. In addition, adding domain knowledge in the form of ontology embeddings helps avoid the bias encountered in neural network based models, for example the pure BERT model bias towards larger classes in our small corpus. ",
    "url": "https://arxiv.org/abs/2402.12282",
    "authors": [
      "Zehra Melce H\u00fcs\u00fcnbeyi",
      "Tatjana Scheffler"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.12303",
    "title": "UncertaintyTrack: Exploiting Detection and Localization Uncertainty in  Multi-Object Tracking",
    "abstract": "Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods. The majority of tracking methods follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty. This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT. While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking. We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors. Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19\\% and improves mMOTA by 2-3%. The source code is available at https://github.com/TRAILab/UncertaintyTrack ",
    "url": "https://arxiv.org/abs/2402.12303",
    "authors": [
      "Chang Won Lee",
      "Steven L. Waslander"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2402.12307",
    "title": "Multi-View Conformal Learning for Heterogeneous Sensor Fusion",
    "abstract": "Being able to assess the confidence of individual predictions in machine learning models is crucial for decision making scenarios. Specially, in critical applications such as medical diagnosis, security, and unmanned vehicles, to name a few. In the last years, complex predictive models have had great success in solving hard tasks and new methods are being proposed every day. While the majority of new developments in machine learning models focus on improving the overall performance, less effort is put on assessing the trustworthiness of individual predictions, and even to a lesser extent, in the context of sensor fusion. To this end, we build and test multi-view and single-view conformal models for heterogeneous sensor fusion. Our models provide theoretical marginal confidence guarantees since they are based on the conformal prediction framework. We also propose a multi-view semi-conformal model based on sets intersection. Through comprehensive experimentation, we show that multi-view models perform better than single-view models not only in terms of accuracy-based performance metrics (as it has already been shown in several previous works) but also in conformal measures that provide uncertainty estimation. Our results also showed that multi-view models generate prediction sets with less uncertainty compared to single-view models. ",
    "url": "https://arxiv.org/abs/2402.12307",
    "authors": [
      "Enrique Garcia-Ceja"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.12309",
    "title": "TILP: Differentiable Learning of Temporal Logical Rules on Knowledge  Graphs",
    "abstract": "Compared with static knowledge graphs, temporal knowledge graphs (tKG), which can capture the evolution and change of information over time, are more realistic and general. However, due to the complexity that the notion of time introduces to the learning of the rules, an accurate graph reasoning, e.g., predicting new links between entities, is still a difficult problem. In this paper, we propose TILP, a differentiable framework for temporal logical rules learning. By designing a constrained random walk mechanism and the introduction of temporal operators, we ensure the efficiency of our model. We present temporal features modeling in tKG, e.g., recurrence, temporal order, interval between pair of relations, and duration, and incorporate it into our learning process. We compare TILP with state-of-the-art methods on two benchmark datasets. We show that our proposed framework can improve upon the performance of baseline methods while providing interpretable results. In particular, we consider various scenarios in which training samples are limited, data is biased, and the time range between training and inference are different. In all these cases, TILP works much better than the state-of-the-art methods. ",
    "url": "https://arxiv.org/abs/2402.12309",
    "authors": [
      "Siheng Xiong",
      "Yuan Yang",
      "Faramarz Fekri",
      "James Clayton Kerce"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.12317",
    "title": "ARKS: Active Retrieval in Knowledge Soup for Code Generation",
    "abstract": "Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training. While widely explored in natural language applications, its utilization in code generation remains under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code. In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets. We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on ChatGPT and CodeLlama demonstrate a substantial improvement in the average execution accuracy of ARKS on LLMs. The analysis confirms the effectiveness of our proposed knowledge soup and active retrieval strategies, offering rich insights into the construction of effective retrieval-augmented code generation (RACG) pipelines. Our model, code, and data are available at https://arks-codegen.github.io. ",
    "url": "https://arxiv.org/abs/2402.12317",
    "authors": [
      "Hongjin Su",
      "Shuyang Jiang",
      "Yuhang Lai",
      "Haoyuan Wu",
      "Boao Shi",
      "Che Liu",
      "Qian Liu",
      "Tao Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.12329",
    "title": "Query-Based Adversarial Prompt Generation",
    "abstract": "Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability. ",
    "url": "https://arxiv.org/abs/2402.12329",
    "authors": [
      "Jonathan Hayase",
      "Ema Borevkovic",
      "Nicholas Carlini",
      "Florian Tram\u00e8r",
      "Milad Nasr"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.12336",
    "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings  for Robust Large Vision-Language Models",
    "abstract": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the VLM is required. The code and robust models are available at https://github.com/chs20/RobustVLM ",
    "url": "https://arxiv.org/abs/2402.12336",
    "authors": [
      "Christian Schlarmann",
      "Naman Deep Singh",
      "Francesco Croce",
      "Matthias Hein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.12338",
    "title": "An Adversarial Approach to Evaluating the Robustness of Event  Identification Models",
    "abstract": "Intelligent machine learning approaches are finding active use for event detection and identification that allow real-time situational awareness. Yet, such machine learning algorithms have been shown to be susceptible to adversarial attacks on the incoming telemetry data. This paper considers a physics-based modal decomposition method to extract features for event classification and focuses on interpretable classifiers including logistic regression and gradient boosting to distinguish two types of events: load loss and generation loss. The resulting classifiers are then tested against an adversarial algorithm to evaluate their robustness. The adversarial attack is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classification model. Thorough experiments on the synthetic South Carolina 500-bus system highlight that a relatively simpler model such as logistic regression is more susceptible to adversarial attacks than gradient boosting. ",
    "url": "https://arxiv.org/abs/2402.12338",
    "authors": [
      "Obai Bahwal",
      "Oliver Kosut",
      "Lalitha Sankar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.12357",
    "title": "Flip Graphs of Pseudo-Triangulations With Face Degree at Most 4",
    "abstract": "A pseudo-triangle is a simple polygon with exactly three convex vertices, and all other vertices (if any) are distributed on three concave chains. A pseudo-triangulation~$\\mathcal{T}$ of a point set~$P$ in~$\\mathbb{R}^2$ is a partitioning of the convex hull of~$P$ into pseudo-triangles, such that the union of the vertices of the pseudo-triangles is exactly~$P$. We call a size-4 pseudo-triangle a dart. For a fixed $k\\geq 1$, we study $k$-dart pseudo-triangulations ($k$-DPTs), that is, pseudo-triangulations in which exactly $k$ faces are darts and all other faces are triangles. We study the flip graph for such pseudo-triangulations, in which a flip exchanges the diagonals of a pseudo-quadrilatral. Our results are as follows. We prove that the flip graph of $1$-DPTs is generally not connected, and show how to compute its connected components. Furthermore, for $k$-DPTs on a point configuration called the double chain we analyze the structure of the flip graph on a more fine-grained level. ",
    "url": "https://arxiv.org/abs/2402.12357",
    "authors": [
      "Maarten L\u00f6ffler",
      "Tamara Mchedlidze",
      "David Orden",
      "Josef Tkadlec",
      "Jules Wulms"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2402.12360",
    "title": "Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks",
    "abstract": "We use Physics-Informed Neural Networks (PINNs) to solve the discrete-time nonlinear observer state estimation problem. Integrated within a single-step exact observer linearization framework, the proposed PINN approach aims at learning a nonlinear state transformation map by solving a system of inhomogeneous functional equations. The performance of the proposed PINN approach is assessed via two illustrative case studies for which the observer linearizing transformation map can be derived analytically. We also perform an uncertainty quantification analysis for the proposed PINN scheme and we compare it with conventional power-series numerical implementations, which rely on the computation of a power series solution. ",
    "url": "https://arxiv.org/abs/2402.12360",
    "authors": [
      "Hector Vargas Alvarez",
      "Gianluca Fabiani",
      "Ioannis G. Kevrekidis",
      "Nikolaos Kazantzis",
      "Constantinos Siettos"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Artificial Intelligence (cs.AI)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2402.10649",
    "title": "Hermite Neural Network Simulation for Solving the 2D Schrodinger  Equation",
    "abstract": "The Schrodinger equation is a mathematical equation describing the wave function's behavior in a quantum-mechanical system. It is a partial differential equation that provides valuable insights into the fundamental principles of quantum mechanics. In this paper, the aim was to solve the Schrodinger equation with sufficient accuracy by using a mixture of neural networks with the collocation method base Hermite functions. Initially, the Hermite functions roots were employed as collocation points, enhancing the efficiency of the solution. The Schrodinger equation is defined in an infinite domain, the use of Hermite functions as activation functions resulted in excellent precision. Finally, the proposed method was simulated using MATLAB's Simulink tool. The results were then compared with those obtained using Physics-informed neural networks and the presented method. ",
    "url": "https://arxiv.org/abs/2402.10649",
    "authors": [
      "Kourosh Parand",
      "Aida Pakniyat"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.10972",
    "title": "Modeling methodology for the accurate and prompt prediction of  symptomatic events in chronic diseases",
    "abstract": "Prediction of symptomatic crises in chronic diseases allows to take decisions before the symptoms occur, such as the intake of drugs to avoid the symptoms or the activation of medical alarms. The prediction horizon is in this case an important parameter in order to fulfill the pharmacokinetics of medications, or the time response of medical services. This paper presents a study about the prediction limits of a chronic disease with symptomatic crises: the migraine. For that purpose, this work develops a methodology to build predictive migraine models and to improve these predictions beyond the limits of the initial models. The maximum prediction horizon is analyzed, and its dependency on the selected features is studied. A strategy for model selection is proposed to tackle the trade off between conservative but robust predictive models, with respect to less accurate predictions with higher horizons. The obtained results show a prediction horizon close to 40 minutes, which is in the time range of the drug pharmacokinetics. Experiments have been performed in a realistic scenario where input data have been acquired in an ambulatory clinical study by the deployment of a non-intrusive Wireless Body Sensor Network. Our results provide an effective methodology for the selection of the future horizon in the development of prediction algorithms for diseases experiencing symptomatic crises. ",
    "url": "https://arxiv.org/abs/2402.10972",
    "authors": [
      "Josu\u00e9 Pag\u00e1n",
      "Jos\u00e9 L. Risco-Mart\u00edn",
      "Jos\u00e9 M. Moya",
      "Jos\u00e9 L. Ayala"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11044",
    "title": "Star-Forest Decompositions of Complete Graphs",
    "abstract": "We deal with the problem of decomposing a complete geometric graph into plane star-forests. In particular, we disprove a recent conjecture by Pach, Saghafian and Schnider by constructing for each $n$ a complete geometric graph on $n$ vertices which can be decomposed into $\\frac{n}{2}+1$ plane star-forests. Additionally we prove that for even $n$, every decomposition of complete abstract graph on $n$ vertices into $\\frac{n}{2}+1$ star-forests is composed of a perfect matching and $\\frac{n}{2}$ star-forests with two edge-balanced components, which we call broken double stars. ",
    "url": "https://arxiv.org/abs/2402.11044",
    "authors": [
      "Todor Anti\u0107",
      "Jelena Gli\u0161i\u0107",
      "Milan Milivoj\u010devi\u0107"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2402.11216",
    "title": "Feedback Delay Network Optimization",
    "abstract": "A common bane of artificial reverberation algorithms is spectral coloration, typically manifesting as metallic ringing, leading to a degradation in the perceived sound quality. This paper presents an optimization framework where a differentiable feedback delay network is used to learn a set of parameters to reduce coloration iteratively. The parameters under optimization include the feedback matrix, as well as the input and output gains. The optimization objective is twofold: to maximize spectral flatness through a spectral loss while maintaining temporal density by penalizing sparseness in the parameter values. A favorable narrower distribution of modal excitation is achieved while maintaining the desired impulse response density. In a subjective assessment, the new method proves effective in reducing perceptual coloration of late reverberation. The proposed method achieves computational savings compared to the baseline while preserving its performance. The effectiveness of this work is demonstrated through two application scenarios where natural-sounding synthetic impulse responses are obtained via the introduction of attenuation filters and an optimizable scattering feedback matrix. ",
    "url": "https://arxiv.org/abs/2402.11216",
    "authors": [
      "Gloria Dal Santo",
      "Karolina Prawda",
      "Sebastian J. Schlecht",
      "Vesa V\u00e4lim\u00e4ki"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2402.11222",
    "title": "Treewidth versus clique number. IV. Tree-independence number of graphs  excluding an induced star",
    "abstract": "Many recent works address the question of characterizing induced obstructions to bounded treewidth. In 2022, Lozin and Razgon completely answered this question for graph classes defined by finitely many forbidden induced subgraphs. Their result also implies a characterization of graph classes defined by finitely many forbidden induced subgraphs that are $(tw,\\omega)$-bounded, that is, treewidth can only be large due to the presence of a large clique. This condition is known to be satisfied for any graph class with bounded tree-independence number, a graph parameter introduced independently by Yolov in 2018 and by Dallard, Milani\\v{c}, and \\v{S}torgel in 2024. Dallard et al. conjectured that $(tw,\\omega)$-boundedness is actually equivalent to bounded tree-independence number. We address this conjecture in the context of graph classes defined by finitely many forbidden induced subgraphs and prove it for the case of graph classes excluding an induced star. We also prove it for subclasses of the class of line graphs, determine the exact values of the tree-independence numbers of line graphs of complete graphs and line graphs of complete bipartite graphs, and characterize the tree-independence number of $P_4$-free graphs, which implies a linear-time algorithm for its computation. Applying the algorithmic framework provided in a previous paper of the series leads to polynomial-time algorithms for the Maximum Weight Independent Set problem in an infinite family of graph classes. ",
    "url": "https://arxiv.org/abs/2402.11222",
    "authors": [
      "Cl\u00e9ment Dallard",
      "Matja\u017e Krnc",
      "O-joung Kwon",
      "Martin Milani\u010d",
      "Andrea Munaro",
      "Kenny \u0160torgel",
      "Sebastian Wiederrecht"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2402.11464",
    "title": "Weighted Myerson value for Network games",
    "abstract": "We study the weighted Myerson value for Network games extending a similar concept for communication situations. Network games, unlike communication situations, treat direct and indirect links among players differently and distinguish their effects in both worth generation and allocation processes. The weighted Myerson value is an allocation rule for Network games that generalizes the Myerson value of Network games. Here, the players are assumed to have some weights measuring their capacity to form links with other players. Two characterization of the weighted Myerson value are provided. Finally, we propose a bidding mechanism to show that the weighted Myerson value is a subgame-perfect Nash equilibrium under a non-cooperative framework. ",
    "url": "https://arxiv.org/abs/2402.11464",
    "authors": [
      "Niharika Kakoty",
      "Surajit Borkotokey",
      "Rajnish Kumar",
      "Abhijit Bora"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2402.11472",
    "title": "DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt  Learning",
    "abstract": "Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issues by leveraging the intrinsic knowledge from pre-trained models, which can be efficiently deployed with minimal downstream data. Specifically, to solve the first challenge, DDIPrompt employs augmented links between drugs, considering both structural and interactive proximity. It features a hierarchical pre-training strategy that comprehends intra-molecular structures and inter-molecular interactions, fostering a comprehensive and unbiased understanding of drug properties. For the second challenge, we implement a prototype-enhanced prompting mechanism during inference. This mechanism, refined by few-shot examples from each category, effectively harnesses the rich pre-training knowledge to enhance prediction accuracy, particularly for these rare but crucial interactions. Comprehensive evaluations on two benchmark datasets demonstrate the superiority of DDIPrompt, particularly in predicting rare DDI events. ",
    "url": "https://arxiv.org/abs/2402.11472",
    "authors": [
      "Yingying Wang",
      "Yun Xiong",
      "Xixi Wu",
      "Xiangguo Sun",
      "Jiawei Zhang"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11538",
    "title": "PASCL: Supervised Contrastive Learning with Perturbative Augmentation  for Particle Decay Reconstruction",
    "abstract": "In high-energy physics, particles produced in collision events decay in a format of a hierarchical tree structure, where only the final decay products can be observed using detectors. However, the large combinatorial space of possible tree structures makes it challenging to recover the actual decay process given a set of final particles. To better analyse the hierarchical tree structure, we propose a graph-based deep learning model to infer the tree structure to reconstruct collision events. In particular, we use a compact matrix representation termed as lowest common ancestor generations (LCAG) matrix, to encode the particle decay tree structure. Then, we introduce a perturbative augmentation technique applied to node features, aiming to mimic experimental uncertainties and increase data diversity. We further propose a supervised graph contrastive learning algorithm to utilize the information of inter-particle relations from multiple decay processes. Extensive experiments show that our proposed supervised graph contrastive learning with perturbative augmentation (PASCL) method outperforms state-of-the-art baseline models on an existing physics-based dataset, significantly improving the reconstruction accuracy. This method provides a more effective training strategy for models with the same parameters and makes way for more accurate and efficient high-energy particle physics data analysis. ",
    "url": "https://arxiv.org/abs/2402.11538",
    "authors": [
      "Junjian Lu",
      "Siwei Liu",
      "Dmitrii Kobylianski",
      "Etienne Dreyer",
      "Eilam Gross",
      "Shangsong Liang"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11557",
    "title": "Evaluating Adversarial Robustness of Low dose CT Recovery",
    "abstract": "Low dose computed tomography (CT) acquisition using reduced radiation or sparse angle measurements is recommended to decrease the harmful effects of X-ray radiation. Recent works successfully apply deep networks to the problem of low dose CT recovery on bench-mark datasets. However, their robustness needs a thorough evaluation before use in clinical settings. In this work, we evaluate the robustness of different deep learning approaches and classical methods for CT recovery. We show that deep networks, including model-based networks encouraging data consistency, are more susceptible to untargeted attacks. Surprisingly, we observe that data consistency is not heavily affected even for these poor quality reconstructions, motivating the need for better regularization for the networks. We demonstrate the feasibility of universal attacks and study attack transferability across different methods. We analyze robustness to attacks causing localized changes in clinically relevant regions. Both classical approaches and deep networks are affected by such attacks leading to changes in the visual appearance of localized lesions, for extremely small perturbations. As the resulting reconstructions have high data consistency with the original measurements, these localized attacks can be used to explore the solution space of the CT recovery problem. ",
    "url": "https://arxiv.org/abs/2402.11557",
    "authors": [
      "Kanchana Vaishnavi Gandikota",
      "Paramanand Chandramouli",
      "Hannah Droege",
      "Michael Moeller"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.11630",
    "title": "Towards Distributed and Intelligent Integrated Sensing and  Communications for 6G Networks",
    "abstract": "This paper introduces the distributed and intelligent integrated sensing and communications (DISAC) concept, a transformative approach for 6G wireless networks that extends the emerging concept of integrated sensing and communications (ISAC). DISAC addresses the limitations of the existing ISAC models and, to overcome them, it introduces two novel foundational functionalities for both sensing and communications: a distributed architecture and a semantic and goal-oriented framework. The distributed architecture enables large-scale and energy-efficient tracking of connected users and objects, leveraging the fusion of heterogeneous sensors. The semantic and goal-oriented intelligent and parsimonious framework, enables the transition from classical data fusion to the composition of semantically selected information, offering new paradigms for the optimization of resource utilization and exceptional multi-modal sensing performance across various use cases. This paper details DISAC's principles, architecture, and potential applications. ",
    "url": "https://arxiv.org/abs/2402.11630",
    "authors": [
      "Emilio Calvanese Strinati",
      "George C. Alexandropoulos",
      "Navid Amani",
      "Maurizio Crozzoli",
      "Giyyarpuram Madhusudan",
      "Sami Mekki",
      "Francois Rivet",
      "Vincenzo Sciancalepore",
      "Philippe Sehier",
      "Maximilian Stark",
      "Henk Wymeersch"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2402.11652",
    "title": "Doubly Robust Inference in Causal Latent Factor Models",
    "abstract": "This article introduces a new framework for estimating average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. Simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article. ",
    "url": "https://arxiv.org/abs/2402.11652",
    "authors": [
      "Alberto Abadie",
      "Anish Agarwal",
      "Raaz Dwivedi",
      "Abhin Shah"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.11687",
    "title": "Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum  Neural Networks",
    "abstract": "Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack. In this study, we assess the efficacy of such attacks in the realm of quantum computing. We conducted comprehensive experiments on various datasets with multiple QML model architectures. Our findings revealed that model stealing attacks can produce clone models achieving up to $0.9\\times$ and $0.99\\times$ clone test accuracy when trained using Top-$1$ and Top-$k$ labels, respectively ($k:$ num\\_classes). To defend against these attacks, we leverage the unique properties of current noisy hardware and perturb the victim model outputs and hinder the attacker's training process. In particular, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP). Although noise and architectural variability can provide up to $\\sim16\\%$ output obfuscation, our comprehensive analysis revealed that models cloned under noisy conditions tend to be resilient, suffering little to no performance degradation due to such obfuscations. Despite limited success with our defense techniques, this outcome has led to an important discovery: QML models trained on noisy hardwares are naturally resistant to perturbation or obfuscation-based defenses or attacks. ",
    "url": "https://arxiv.org/abs/2402.11687",
    "authors": [
      "Satwik Kundu",
      "Debarshi Kundu",
      "Swaroop Ghosh"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11758",
    "title": "Conformally rigid graphs",
    "abstract": "Given a finite, simple, connected graph $G=(V,E)$ with $|V|=n$, we consider the associated graph Laplacian matrix $L = D - A$ with eigenvalues $0 = \\lambda_1 < \\lambda_2 \\leq \\dots \\leq \\lambda_n$. One can also consider the same graph equipped with positive edge weights $w:E \\rightarrow \\mathbb{R}_{> 0}$ normalized to $\\sum_{e \\in E} w_e = |E|$ and the associated weighted Laplacian matrix $L_w$. We say that $G$ is conformally rigid if constant edge-weights maximize the second eigenvalue $\\lambda_2(w)$ of $L_w$ over all $w$, and minimize $\\lambda_n(w')$ of $L_{w'}$ over all $w'$, i.e., for all $w,w'$, $$ \\lambda_2(w) \\leq \\lambda_2(1) \\leq \\lambda_n(1) \\leq \\lambda_n(w').$$ Conformal rigidity requires an extraordinary amount of symmetry in $G$. Every edge-transitive graph is conformally rigid. We prove that every distance-regular graph, and hence every strongly-regular graph, is conformally rigid. Certain special graph embeddings can be used to characterize conformal rigidity. Cayley graphs can be conformally rigid but need not be, we prove a sufficient criterion. We also find a small set of conformally rigid graphs that do not belong into any of the above categories; these include the Hoffman graph, the crossing number graph 6B and others. Conformal rigidity can be certified via semidefinite programming, we provide explicit examples. ",
    "url": "https://arxiv.org/abs/2402.11758",
    "authors": [
      "Stefan Steinerberger",
      "Rekha R. Thomas"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Optimization and Control (math.OC)",
      "Spectral Theory (math.SP)"
    ]
  },
  {
    "id": "arXiv:2402.12072",
    "title": "Robustness and Exploration of Variational and Machine Learning  Approaches to Inverse Problems: An Overview",
    "abstract": "This paper attempts to provide an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning. A special focus lies on point estimators and their robustness against adversarial perturbations. In this context results of numerical experiments for a one-dimensional toy problem are provided, showing the robustness of different approaches and empirically verifying theoretical guarantees. Another focus of this review is the exploration of the subspace of data consistent solutions through explicit guidance to satisfy specific semantic or textural properties. ",
    "url": "https://arxiv.org/abs/2402.12072",
    "authors": [
      "Alexander Auras",
      "Kanchana Vaishnavi Gandikota",
      "Hannah Droege",
      "Michael Moeller"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2402.12188",
    "title": "Structure of activity in multiregion recurrent neural networks",
    "abstract": "Neural circuits are composed of multiple regions, each with rich dynamics and engaging in communication with other regions. The combination of local, within-region dynamics and global, network-level dynamics is thought to provide computational flexibility. However, the nature of such multiregion dynamics and the underlying synaptic connectivity patterns remain poorly understood. Here, we study the dynamics of recurrent neural networks with multiple interconnected regions. Within each region, neurons have a combination of random and structured recurrent connections. Motivated by experimental evidence of communication subspaces between cortical areas, these networks have low-rank connectivity between regions, enabling selective routing of activity. These networks exhibit two interacting forms of dynamics: high-dimensional fluctuations within regions and low-dimensional signal transmission between regions. To characterize this interaction, we develop a dynamical mean-field theory to analyze such networks in the limit where each region contains infinitely many neurons, with cross-region currents as key order parameters. Regions can act as both generators and transmitters of activity, roles that we show are in conflict. Specifically, taming the complexity of activity within a region is necessary for it to route signals to and from other regions. Unlike previous models of routing in neural circuits, which suppressed the activities of neuronal groups to control signal flow, routing in our model is achieved by exciting different high-dimensional activity patterns through a combination of connectivity structure and nonlinear recurrent dynamics. This theory provides insight into the interpretation of both multiregion neural data and trained neural networks. ",
    "url": "https://arxiv.org/abs/2402.12188",
    "authors": [
      "David G. Clark",
      "Manuel Beiran"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2402.12208",
    "title": "Language-Codec: Reducing the Gaps Between Discrete Codec Representation  and Speech Language Models",
    "abstract": "In recent years, large language models have achieved significant success in generative tasks (e.g., speech cloning and audio generation) related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serves as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) most codec models are trained on only 1,000 hours of data, whereas most speech language models are trained on 60,000 hours; 2) Achieving good reconstruction performance requires the utilization of numerous codebooks, which increases the burden on downstream speech language models; 3) The initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Mask Channel Residual Vector Quantization (MCRVQ) mechanism along with improved Fourier transform structures and larger training datasets to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/speechnovateur/languagecodec_tmp . ",
    "url": "https://arxiv.org/abs/2402.12208",
    "authors": [
      "Shengpeng Ji",
      "Minghui Fang",
      "Ziyue Jiang",
      "Rongjie Huang",
      "Jialung Zuo",
      "Shulei Wang",
      "Zhou Zhao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2402.12223",
    "title": "Second Order Meanfield Approximation for calculating Dynamics in  Au-Nanoparticle Networks",
    "abstract": "Exploiting physical processes for fast and energy-efficient computation bears great potential in the advancement of modern hardware components. This paper explores non-linear charge tunneling in nanoparticle networks, controlled by external voltages. The dynamics are described by a master equation, which describes the development of a distribution function over the set of charge occupation numbers. The driving force behind this evolution are charge tunneling events among nanoparticles and their associated rates. In this paper, we introduce two meanfield approximations to this master equation. By parametrization of the distribution function using its first- and second-order statistical moments, and a subsequent projection of the dynamics onto the resulting moment manifold, one can deterministically calculate expected charges and currents. Unlike a kinetic Monte Carlo approach, which extracts samples from the distribution function, this meanfield approach avoids any random elements. A comparison of results between the meanfield approximation and an already available kinetic Monte Carlo simulation demonstrates great accuracy. Our analysis also reveals that transitioning from a first-order to a second-order approximation significantly enhances the accuracy. Furthermore, we demonstrate the applicability of our approach to time-dependent simulations, using eulerian time-integration schemes. ",
    "url": "https://arxiv.org/abs/2402.12223",
    "authors": [
      "Evan Wonisch",
      "Jonas Mensing",
      "Andreas Heuer"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Emerging Technologies (cs.ET)",
      "Numerical Analysis (math.NA)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2402.12369",
    "title": "Short-Period Variables in TESS Full-Frame Image Light Curves Identified  via Convolutional Neural Networks",
    "abstract": "The Transiting Exoplanet Survey Satellite (TESS) mission measured light from stars in ~85% of the sky throughout its two-year primary mission, resulting in millions of TESS 30-minute cadence light curves to analyze in the search for transiting exoplanets. To search this vast dataset, we aim to provide an approach that is both computationally efficient, produces highly performant predictions, and minimizes the required human search effort. We present a convolutional neural network that we train to identify short period variables. To make a prediction for a given light curve, our network requires no prior target parameters identified using other methods. Our network performs inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large scale archival searches. We present a collection of 14156 short-period variables identified by our network. The majority of our identified variables fall into two prominent populations, one of short-period main sequence binaries and another of Delta Scuti stars. Our neural network model and related code is additionally provided as open-source code for public use and extension. ",
    "url": "https://arxiv.org/abs/2402.12369",
    "authors": [
      "Greg Olmschenk",
      "Richard K. Barry",
      "Stela Ishitani Silva",
      "Brian P. Powell",
      "Ethan Kruse",
      "Jeremy D. Schnittman",
      "Agnieszka M. Cieplak",
      "Thomas Barclay",
      "Siddhant Solanki",
      "Bianca Ortega",
      "John Baker",
      "Yesenia Helem Salinas Mamani"
    ],
    "subjectives": [
      "Solar and Stellar Astrophysics (astro-ph.SR)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:1910.08597",
    "title": "Robust Learning Rate Selection for Stochastic Optimization via Splitting  Diagnostic",
    "abstract": " Title: Robust Learning Rate Selection for Stochastic Optimization via Splitting  Diagnostic ",
    "url": "https://arxiv.org/abs/1910.08597",
    "authors": [
      "Matteo Sordello",
      "Niccol\u00f2 Dalmasso",
      "Hangfeng He",
      "Weijie Su"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2008.05966",
    "title": "Deep-Lock: Secure Authorization for Deep Neural Networks",
    "abstract": " Title: Deep-Lock: Secure Authorization for Deep Neural Networks ",
    "url": "https://arxiv.org/abs/2008.05966",
    "authors": [
      "Manaar Alam",
      "Sayandeep Saha",
      "Debdeep Mukhopadhyay",
      "Sandip Kundu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2108.04786",
    "title": "Tangled Paths: A Random Graph Model from Mallows Permutations",
    "abstract": " Comments: 36 pages, 7 figures. Strengthened Theorems 1.1 & 1.4 ",
    "url": "https://arxiv.org/abs/2108.04786",
    "authors": [
      "Jessica Enright",
      "Kitty Meeks",
      "William Pettersson",
      "John Sylvester"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2108.06663",
    "title": "HCR-Net: A deep learning based script independent handwritten character  recognition network",
    "abstract": " Comments: accepted to Multimedia Tools and Applications ",
    "url": "https://arxiv.org/abs/2108.06663",
    "authors": [
      "Vinod Kumar Chauhan",
      "Sukhdeep Singh",
      "Anuj Sharma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2201.11653",
    "title": "Representations learnt by SGD and Adaptive learning rules: Conditions  that vary sparsity and selectivity in neural network",
    "abstract": " Title: Representations learnt by SGD and Adaptive learning rules: Conditions  that vary sparsity and selectivity in neural network ",
    "url": "https://arxiv.org/abs/2201.11653",
    "authors": [
      "Jin Hyun Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2203.02511",
    "title": "Self-Supervised Learning for Joint Pushing and Grasping Policies in  Highly Cluttered Environments",
    "abstract": " Comments: This paper has been accepted for publication at the ICRA2024 conference ",
    "url": "https://arxiv.org/abs/2203.02511",
    "authors": [
      "Kamal Mokhtar",
      "Cock Heemskerk",
      "Hamidreza Kasaei"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2203.12446",
    "title": "SMEMO: Social Memory for Trajectory Forecasting",
    "abstract": " Comments: Accepted for publication in IEEE Transaction on Pattern Analysis and Machine Intelligence (PAMI) ",
    "url": "https://arxiv.org/abs/2203.12446",
    "authors": [
      "Francesco Marchetti",
      "Federico Becattini",
      "Lorenzo Seidenari",
      "Alberto Del Bimbo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.01900",
    "title": "Estimating counterfactual treatment outcomes over time in complex  multiagent scenarios",
    "abstract": " Comments: 16 pages, 11 figures. Accepted in IEEE Transactions on Neural Networks and Learning Systems ",
    "url": "https://arxiv.org/abs/2206.01900",
    "authors": [
      "Keisuke Fujii",
      "Koh Takeuchi",
      "Atsushi Kuribayashi",
      "Naoya Takeishi",
      "Yoshinobu Kawahara",
      "Kazuya Takeda"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.12943",
    "title": "Multi-view Feature Augmentation with Adaptive Class Activation Mapping",
    "abstract": " Comments: An arxiv version of the paper published in Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21). See this https URL ",
    "url": "https://arxiv.org/abs/2206.12943",
    "authors": [
      "Xiang Gao",
      "Yingjie Tian",
      "Zhiquan Qi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2207.12638",
    "title": "Variance estimation in graphs with the fused lasso",
    "abstract": " Title: Variance estimation in graphs with the fused lasso ",
    "url": "https://arxiv.org/abs/2207.12638",
    "authors": [
      "Oscar Hernan Madrid Padilla"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2208.01113",
    "title": "On the Evaluation of User Privacy in Deep Neural Networks using Timing  Side Channel",
    "abstract": " Comments: 15 pages, 20 figures ",
    "url": "https://arxiv.org/abs/2208.01113",
    "authors": [
      "Shubhi Shukla",
      "Manaar Alam",
      "Sarani Bhattacharya",
      "Debdeep Mukhopadhyay",
      "Pabitra Mitra"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2211.16335",
    "title": "X-ICP: Localizability-Aware LiDAR Registration for Robust Localization  in Extreme Environments",
    "abstract": " Comments: 20 Pages, 20 Figures Submitted to IEEE Transactions On Robotics. Supplementary Video: this https URL Project Website: this https URL ",
    "url": "https://arxiv.org/abs/2211.16335",
    "authors": [
      "Turcan Tuna",
      "Julian Nubert",
      "Yoshua Nava",
      "Shehryar Khattak",
      "Marco Hutter"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2212.04548",
    "title": "STLGRU: Spatio-Temporal Lightweight Graph GRU for Traffic Flow  Prediction",
    "abstract": " Comments: PAKDD 2024 (Oral) ",
    "url": "https://arxiv.org/abs/2212.04548",
    "authors": [
      "Kishor Kumar Bhaumik",
      "Fahim Faisal Niloy",
      "Saif Mahmud",
      "Simon Woo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.05861",
    "title": "Joint Counting, Detection and Re-Identification for Multi-Object  Tracking",
    "abstract": " Title: Joint Counting, Detection and Re-Identification for Multi-Object  Tracking ",
    "url": "https://arxiv.org/abs/2212.05861",
    "authors": [
      "Weihong Ren",
      "Denglu Wu",
      "Hui Cao",
      "Xi'ai Chen",
      "Zhi Han",
      "Honghai Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2301.11237",
    "title": "The Hazards and Benefits of Condescension in Social Learning",
    "abstract": " Comments: 34 pages ",
    "url": "https://arxiv.org/abs/2301.11237",
    "authors": [
      "Itai Arieli",
      "Yakov Babichenko",
      "Stephan M\u00fcller",
      "Farzad Pourbabaee",
      "Omer Tamuz"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2303.00492",
    "title": "Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized  Devices",
    "abstract": " Comments: 13 pages, 7 figures, published in the Proceedings of the 39th IEEE International Conference on Data Engineering (ICDE 2023) ",
    "url": "https://arxiv.org/abs/2303.00492",
    "authors": [
      "Qiying Pan",
      "Yifei Zhu",
      "Lingyang Chu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2303.01385",
    "title": "Hyperlink communities in higher-order networks",
    "abstract": " Comments: Accepted for publication in the Journal of Complex Networks ",
    "url": "https://arxiv.org/abs/2303.01385",
    "authors": [
      "Quintino Francesco Lotito",
      "Federico Musciotto",
      "Alberto Montresor",
      "Federico Battiston"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2303.02586",
    "title": "A Complex Quasi-Newton Proximal Method for Image Reconstruction in  Compressed Sensing MRI",
    "abstract": " Comments: 26 pages, 26 figures ",
    "url": "https://arxiv.org/abs/2303.02586",
    "authors": [
      "Tao Hong",
      "Luis Hernandez-Garcia",
      "Jeffrey A. Fessler"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2303.08103",
    "title": "Multi-task Meta Label Correction for Time Series Prediction",
    "abstract": " Title: Multi-task Meta Label Correction for Time Series Prediction ",
    "url": "https://arxiv.org/abs/2303.08103",
    "authors": [
      "Luxuan Yang",
      "Ting Gao",
      "Wei Wei",
      "Min Dai",
      "Cheng Fang",
      "Jinqiao Duan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2304.03343",
    "title": "Spintronic Physical Reservoir for Autonomous Prediction and Long-Term  Household Energy Load Forecasting",
    "abstract": " Title: Spintronic Physical Reservoir for Autonomous Prediction and Long-Term  Household Energy Load Forecasting ",
    "url": "https://arxiv.org/abs/2304.03343",
    "authors": [
      "Walid Al Misba",
      "Harindra S. Mavikumbure",
      "Md Mahadi Rajib",
      "Daniel L. Marino",
      "Victor Cobilean",
      "Milos Manic",
      "Jayasimha Atulasimha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.05875",
    "title": "Quantization Aware Attack: Enhancing Transferable Adversarial Attacks by  Model Quantization",
    "abstract": " Comments: Accepted by IEEE Transactions on Information Forensics and Security in 2024 ",
    "url": "https://arxiv.org/abs/2305.05875",
    "authors": [
      "Yulong Yang",
      "Chenhao Lin",
      "Qian Li",
      "Zhengyu Zhao",
      "Haoran Fan",
      "Dawei Zhou",
      "Nannan Wang",
      "Tongliang Liu",
      "Chao Shen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2305.12050",
    "title": "AI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed  methods evaluation",
    "abstract": " Title: AI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed  methods evaluation ",
    "url": "https://arxiv.org/abs/2305.12050",
    "authors": [
      "Vijayaraghavan Murali",
      "Chandra Maddila",
      "Imad Ahmad",
      "Michael Bolin",
      "Daniel Cheng",
      "Negar Ghorbani",
      "Renuka Fernandez",
      "Nachiappan Nagappan",
      "Peter C. Rigby"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.13718",
    "title": "LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large  Language Models",
    "abstract": " Comments: 11 pages ",
    "url": "https://arxiv.org/abs/2305.13718",
    "authors": [
      "Fangkai Jiao",
      "Zhiyang Teng",
      "Bosheng Ding",
      "Zhengyuan Liu",
      "Nancy F. Chen",
      "Shafiq Joty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18485",
    "title": "Autoencoding Conditional Neural Processes for Representation Learning",
    "abstract": " Title: Autoencoding Conditional Neural Processes for Representation Learning ",
    "url": "https://arxiv.org/abs/2305.18485",
    "authors": [
      "Victor Prokhorov",
      "Ivan Titov",
      "N. Siddharth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.19306",
    "title": "A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets  Spiking Neural Networks",
    "abstract": " Comments: Accepted to ICLR 2024; Code is available at this https URL ",
    "url": "https://arxiv.org/abs/2305.19306",
    "authors": [
      "Jintang Li",
      "Huizhe Zhang",
      "Ruofan Wu",
      "Zulun Zhu",
      "Baokun Wang",
      "Changhua Meng",
      "Zibin Zheng",
      "Liang Chen"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.04325",
    "title": "A Study on Chinese Social Perspective regarding ChatGPT for Education  and Beyond",
    "abstract": " Title: A Study on Chinese Social Perspective regarding ChatGPT for Education  and Beyond ",
    "url": "https://arxiv.org/abs/2306.04325",
    "authors": [
      "Yao Tian",
      "Chengwei Tong",
      "Lik-Hang Lee",
      "Reza Hadi Mogavi",
      "Yong Liao",
      "Pengyuan Zhou"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2306.10001",
    "title": "Group Orthogonalization Regularization For Vision Models Adaptation and  Robustness",
    "abstract": " Comments: BMVC 2023 ",
    "url": "https://arxiv.org/abs/2306.10001",
    "authors": [
      "Yoav Kurtz",
      "Noga Bar",
      "Raja Giryes"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.11157",
    "title": "Human Limits in Machine Learning: Prediction of Plant Phenotypes Using  Soil Microbiome Data",
    "abstract": " Title: Human Limits in Machine Learning: Prediction of Plant Phenotypes Using  Soil Microbiome Data ",
    "url": "https://arxiv.org/abs/2306.11157",
    "authors": [
      "Rosa Aghdam",
      "Xudong Tang",
      "Shan Shan",
      "Richard Lankau",
      "Claudia Sol\u00eds-Lemus"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2306.11282",
    "title": "Phase Repair for Time-Domain Convolutional Neural Networks in Music  Super-Resolution",
    "abstract": " Comments: Under review ",
    "url": "https://arxiv.org/abs/2306.11282",
    "authors": [
      "Yenan Zhang",
      "Guilly Kolkman",
      "Hiroshi Watanabe"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2306.13941",
    "title": "Grassroots Social Networking: Where People have Agency over their  Personal Information and Social Graph",
    "abstract": " Title: Grassroots Social Networking: Where People have Agency over their  Personal Information and Social Graph ",
    "url": "https://arxiv.org/abs/2306.13941",
    "authors": [
      "Ehud Shapiro"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Computers and Society (cs.CY)",
      "Multiagent Systems (cs.MA)",
      "Networking and Internet Architecture (cs.NI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2306.16060",
    "title": "Dynamic Path-Controllable Deep Unfolding Network for Compressive Sensing",
    "abstract": " Comments: TIP, 2023 ",
    "url": "https://arxiv.org/abs/2306.16060",
    "authors": [
      "Jiechong Song",
      "Bin Chen",
      "Jian Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2306.16261",
    "title": "SE-PQA: Personalized Community Question Answering",
    "abstract": " Title: SE-PQA: Personalized Community Question Answering ",
    "url": "https://arxiv.org/abs/2306.16261",
    "authors": [
      "Pranav Kasela",
      "Marco Braga",
      "Gabriella Pasi",
      "Raffaele Perego"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2307.01649",
    "title": "Nonparametric Classification on Low Dimensional Manifolds using  Overparameterized Convolutional Residual Networks",
    "abstract": " Comments: 20 pages, 1 figure ",
    "url": "https://arxiv.org/abs/2307.01649",
    "authors": [
      "Kaiqi Zhang",
      "Zixuan Zhang",
      "Minshuo Chen",
      "Yuma Takeda",
      "Mengdi Wang",
      "Tuo Zhao",
      "Yu-Xiang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.04866",
    "title": "Gait Event Detection and Travel Distance Using Waist-Worn Accelerometers  across a Range of Speeds: Automated Approach",
    "abstract": " Title: Gait Event Detection and Travel Distance Using Waist-Worn Accelerometers  across a Range of Speeds: Automated Approach ",
    "url": "https://arxiv.org/abs/2307.04866",
    "authors": [
      "Albara Ah Ramli",
      "Xin Liu",
      "Kelly Berndt",
      "Chen-Nee Chuah",
      "Erica Goude",
      "Lynea B. Kaethler",
      "Amanda Lopez",
      "Alina Nicorici",
      "Corey Owens",
      "David Rodriguez",
      "Jane Wang",
      "Daniel Aranki",
      "Craig M. McDonald",
      "Erik K. Henricson"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.06976",
    "title": "On the Complexity of Target Set Selection in Simple Geometric Networks",
    "abstract": " Comments: This is an extended and revised version of a preliminary conference report that was presented in WAW 2023 ",
    "url": "https://arxiv.org/abs/2307.06976",
    "authors": [
      "Michal Dvo\u0159\u00e1k",
      "Du\u0161an Knop",
      "\u0160imon Schierreich"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2307.07494",
    "title": "TALL: Thumbnail Layout for Deepfake Video Detection",
    "abstract": " Comments: Accepted by ICCV 2023; We revised the first paragraph of section 3 ",
    "url": "https://arxiv.org/abs/2307.07494",
    "authors": [
      "Yuting Xu",
      "Jian Liang",
      "Gengyun Jia",
      "Ziming Yang",
      "Yanhao Zhang",
      "Ran He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2307.08433",
    "title": "From random-walks to graph-sprints: a low-latency node embedding  framework on continuous-time dynamic graphs",
    "abstract": " Comments: 9 pages, 5 figures, 7 tables ",
    "url": "https://arxiv.org/abs/2307.08433",
    "authors": [
      "Ahmad Naser Eddin",
      "Jacopo Bono",
      "David Apar\u00edcio",
      "Hugo Ferreira",
      "Jo\u00e3o Ascens\u00e3o",
      "Pedro Ribeiro",
      "Pedro Bizarro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.09110",
    "title": "Cut Sparsification and Succinct Representation of Submodular Hypergraphs",
    "abstract": " Title: Cut Sparsification and Succinct Representation of Submodular Hypergraphs ",
    "url": "https://arxiv.org/abs/2307.09110",
    "authors": [
      "Yotam Kenneth",
      "Robert Krauthgamer"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2307.09529",
    "title": "QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum  Neural Networks",
    "abstract": " Title: QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum  Neural Networks ",
    "url": "https://arxiv.org/abs/2307.09529",
    "authors": [
      "Cheng Chu",
      "Fan Chen",
      "Philip Richerme",
      "Lei Jiang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2307.10655",
    "title": "A Survey of What to Share in Federated Learning: Perspectives on Model  Utility, Privacy Leakage, and Communication Efficiency",
    "abstract": " Title: A Survey of What to Share in Federated Learning: Perspectives on Model  Utility, Privacy Leakage, and Communication Efficiency ",
    "url": "https://arxiv.org/abs/2307.10655",
    "authors": [
      "Jiawei Shao",
      "Zijian Li",
      "Wenqiang Sun",
      "Tailin Zhou",
      "Yuchang Sun",
      "Lumin Liu",
      "Zehong Lin",
      "Yuyi Mao",
      "Jun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2307.11471",
    "title": "Robust Visual Question Answering: Datasets, Methods, and Future  Challenges",
    "abstract": " Comments: Accepted by IEEE TPAMI ",
    "url": "https://arxiv.org/abs/2307.11471",
    "authors": [
      "Jie Ma",
      "Pinghui Wang",
      "Dechen Kong",
      "Zewei Wang",
      "Jun Liu",
      "Hongbin Pei",
      "Junzhou Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2307.11729",
    "title": "OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with  Adversarially Generated Examples",
    "abstract": " Comments: AAAI 2024 camera ready. Code and dataset available at this https URL ",
    "url": "https://arxiv.org/abs/2307.11729",
    "authors": [
      "Ryuto Koike",
      "Masahiro Kaneko",
      "Naoaki Okazaki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2307.16847",
    "title": "CroSSL: Cross-modal Self-Supervised Learning for Time-series through  Latent Masking",
    "abstract": " Comments: Accepted in WSDM24. Short version presented in ML4MHD @ICML23 ",
    "url": "https://arxiv.org/abs/2307.16847",
    "authors": [
      "Shohreh Deldari",
      "Dimitris Spathis",
      "Mohammad Malekzadeh",
      "Fahim Kawsar",
      "Flora Salim",
      "Akhil Mathur"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.04882",
    "title": "Multipacking and broadcast domination on cactus graph and its impact on  hyperbolic graph",
    "abstract": " Title: Multipacking and broadcast domination on cactus graph and its impact on  hyperbolic graph ",
    "url": "https://arxiv.org/abs/2308.04882",
    "authors": [
      "Sandip Das",
      "Sk Samim Islam"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2308.05724",
    "title": "Optimizing Performance of Feedforward and Convolutional Neural Networks  through Dynamic Activation Functions",
    "abstract": " Comments: Tiny ICLR Approved ",
    "url": "https://arxiv.org/abs/2308.05724",
    "authors": [
      "Chinmay Rane",
      "Kanishka Tyagi",
      "Michael Manry"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2308.07124",
    "title": "OctoPack: Instruction Tuning Code Large Language Models",
    "abstract": " Comments: 60 pages (9 main), 40 figures, 19 tables ",
    "url": "https://arxiv.org/abs/2308.07124",
    "authors": [
      "Niklas Muennighoff",
      "Qian Liu",
      "Armel Zebaze",
      "Qinkai Zheng",
      "Binyuan Hui",
      "Terry Yue Zhuo",
      "Swayam Singh",
      "Xiangru Tang",
      "Leandro von Werra",
      "Shayne Longpre"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.11804",
    "title": "Adversarial Illusions in Multi-Modal Embeddings",
    "abstract": " Title: Adversarial Illusions in Multi-Modal Embeddings ",
    "url": "https://arxiv.org/abs/2308.11804",
    "authors": [
      "Tingwei Zhang",
      "Rishi Jha",
      "Eugene Bagdasaryan",
      "Vitaly Shmatikov"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.13916",
    "title": "Exploring Large Language Models for Knowledge Graph Completion",
    "abstract": " Comments: Work in progress ",
    "url": "https://arxiv.org/abs/2308.13916",
    "authors": [
      "Liang Yao",
      "Jiazhen Peng",
      "Chengsheng Mao",
      "Yuan Luo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.15307",
    "title": "Compositional maps for registration in complex geometries",
    "abstract": " Title: Compositional maps for registration in complex geometries ",
    "url": "https://arxiv.org/abs/2308.15307",
    "authors": [
      "Tommaso Taddei"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2309.09737",
    "title": "RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud",
    "abstract": " Comments: Accepted to ICRA 2024. 8 pages, 4 figures. Co-first authorship for Zhijun Pan, Fangqiang Ding and Hantao Zhong, listed randomly ",
    "url": "https://arxiv.org/abs/2309.09737",
    "authors": [
      "Zhijun Pan",
      "Fangqiang Ding",
      "Hantao Zhong",
      "Chris Xiaoxuan Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2309.10479",
    "title": "RECALL+: Adversarial Web-based Replay for Continual Learning in Semantic  Segmentation",
    "abstract": " Title: RECALL+: Adversarial Web-based Replay for Continual Learning in Semantic  Segmentation ",
    "url": "https://arxiv.org/abs/2309.10479",
    "authors": [
      "Chang Liu",
      "Giulia Rizzoli",
      "Francesco Barbato",
      "Andrea Maracani",
      "Marco Toldo",
      "Umberto Michieli",
      "Yi Niu",
      "Pietro Zanuttigh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2309.11015",
    "title": "3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images",
    "abstract": " Comments: The paper is rejected ",
    "url": "https://arxiv.org/abs/2309.11015",
    "authors": [
      "Yifu Zhang",
      "Zuozhu Liu",
      "Yang Feng",
      "Renjing Xu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2309.13443",
    "title": "Early-Exit with Class Exclusion for Efficient Inference of Neural  Networks",
    "abstract": " Title: Early-Exit with Class Exclusion for Efficient Inference of Neural  Networks ",
    "url": "https://arxiv.org/abs/2309.13443",
    "authors": [
      "Jingcun Wang",
      "Bing Li",
      "Grace Li Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2309.13893",
    "title": "Scene Informer: Anchor-based Occlusion Inference and Trajectory  Prediction in Partially Observable Environments",
    "abstract": " Comments: Accepted to 2024 IEEE International Conference on Robotics and Automation (ICRA) ",
    "url": "https://arxiv.org/abs/2309.13893",
    "authors": [
      "Bernard Lange",
      "Jiachen Li",
      "Mykel J. Kochenderfer"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2309.17055",
    "title": "An Application Driven Method for Assembling Numerical Schemes for the  Solution of Complex Multiphysics Problems",
    "abstract": " Title: An Application Driven Method for Assembling Numerical Schemes for the  Solution of Complex Multiphysics Problems ",
    "url": "https://arxiv.org/abs/2309.17055",
    "authors": [
      "Patrick Zimbrod",
      "Michael Fleck",
      "Johannes Schilp"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2310.00797",
    "title": "Going Beyond Familiar Features for Deep Anomaly Detection",
    "abstract": " Title: Going Beyond Familiar Features for Deep Anomaly Detection ",
    "url": "https://arxiv.org/abs/2310.00797",
    "authors": [
      "Sarath Sivaprasad",
      "Mario Fritz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.04910",
    "title": "Faithful Knowledge Graph Explanations for Commonsense Reasoning",
    "abstract": " Title: Faithful Knowledge Graph Explanations for Commonsense Reasoning ",
    "url": "https://arxiv.org/abs/2310.04910",
    "authors": [
      "Weihe Zhai",
      "Arkaitz Zubiaga",
      "Bingquan Liu",
      "Chengjie Sun",
      "Yalong Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.06549",
    "title": "Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield  but Also a Catalyst for Model Inversion Attacks",
    "abstract": " Comments: Published as a conference paper at ICLR 2024 ",
    "url": "https://arxiv.org/abs/2310.06549",
    "authors": [
      "Lukas Struppek",
      "Dominik Hintersdorf",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.11730",
    "title": "Federated Heterogeneous Graph Neural Network for Privacy-preserving  Recommendation",
    "abstract": " Comments: Accepted by WWW 2024 ",
    "url": "https://arxiv.org/abs/2310.11730",
    "authors": [
      "Bo Yan",
      "Yang Cao",
      "Haoyu Wang",
      "Wenchuan Yang",
      "Junping Du",
      "Chuan Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2310.12081",
    "title": "Robust Graph Matching Using An Unbalanced Hierarchical Optimal Transport  Framework",
    "abstract": " Title: Robust Graph Matching Using An Unbalanced Hierarchical Optimal Transport  Framework ",
    "url": "https://arxiv.org/abs/2310.12081",
    "authors": [
      "Haoran Cheng",
      "Dixin Luo",
      "Hongteng Xu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.13505",
    "title": "Robust Training for Conversational Question Answering Models with  Reinforced Reformulation Generation",
    "abstract": " Comments: WSDM 2024 Research Paper, 11 pages ",
    "url": "https://arxiv.org/abs/2310.13505",
    "authors": [
      "Magdalena Kaiser",
      "Rishiraj Saha Roy",
      "Gerhard Weikum"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2310.13828",
    "title": "Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models",
    "abstract": " Title: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models ",
    "url": "https://arxiv.org/abs/2310.13828",
    "authors": [
      "Shawn Shan",
      "Wenxin Ding",
      "Josephine Passananti",
      "Stanley Wu",
      "Haitao Zheng",
      "Ben Y. Zhao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2311.01740",
    "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via  Semantic-aware Cross-check Consistency",
    "abstract": " Comments: EMNLP 2023 ",
    "url": "https://arxiv.org/abs/2311.01740",
    "authors": [
      "Jiaxin Zhang",
      "Zhuohang Li",
      "Kamalika Das",
      "Bradley A. Malin",
      "Sricharan Kumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2311.06835",
    "title": "Open-Set Graph Anomaly Detection via Normal Structure Regularisation",
    "abstract": " Title: Open-Set Graph Anomaly Detection via Normal Structure Regularisation ",
    "url": "https://arxiv.org/abs/2311.06835",
    "authors": [
      "Qizhou Wang",
      "Guansong Pang",
      "Mahsa Salehi",
      "Christopher Leckie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2311.07632",
    "title": "ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical  Interactions Discovering",
    "abstract": " Comments: In progress ",
    "url": "https://arxiv.org/abs/2311.07632",
    "authors": [
      "Zecheng Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Molecular Networks (q-bio.MN)"
    ]
  },
  {
    "id": "arXiv:2311.07879",
    "title": "Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting  Volunteer Content Moderators",
    "abstract": " Title: Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting  Volunteer Content Moderators ",
    "url": "https://arxiv.org/abs/2311.07879",
    "authors": [
      "Yang Trista Cao",
      "Lovely-Frances Domingo",
      "Sarah Ann Gilbert",
      "Michelle Mazurek",
      "Katie Shilton",
      "Hal Daum\u00e9 III"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2311.08045",
    "title": "Adversarial Preference Optimization",
    "abstract": " Comments: In process ",
    "url": "https://arxiv.org/abs/2311.08045",
    "authors": [
      "Pengyu Cheng",
      "Yifan Yang",
      "Jian Li",
      "Yong Dai",
      "Tianhao Hu",
      "Peixin Cao",
      "Nan Du"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.08598",
    "title": "DALA: A Distribution-Aware LoRA-Based Adversarial Attack against  Language Models",
    "abstract": " Comments: First two authors contribute equally ",
    "url": "https://arxiv.org/abs/2311.08598",
    "authors": [
      "Yibo Wang",
      "Xiangjue Dong",
      "James Caverlee",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2311.09090",
    "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
    "abstract": " Title: Social Bias Probing: Fairness Benchmarking for Language Models ",
    "url": "https://arxiv.org/abs/2311.09090",
    "authors": [
      "Marta Marchiori Manerba",
      "Karolina Sta\u0144czak",
      "Riccardo Guidotti",
      "Isabelle Augenstein"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2311.09195",
    "title": "Self-Supervised Curriculum Generation for Autonomous Reinforcement  Learning without Task-Specific Knowledge",
    "abstract": " Comments: 8 pages, 8 figures ",
    "url": "https://arxiv.org/abs/2311.09195",
    "authors": [
      "Sang-Hyun Lee",
      "Seung-Woo Seo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2311.09603",
    "title": "Self-Contradictory Reasoning Evaluation and Detection",
    "abstract": " Title: Self-Contradictory Reasoning Evaluation and Detection ",
    "url": "https://arxiv.org/abs/2311.09603",
    "authors": [
      "Ziyi Liu",
      "Isabelle Lee",
      "Yongkang Du",
      "Soumya Sanyal",
      "Jieyu Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2311.10801",
    "title": "Reinforcement Learning with Maskable Stock Representation for Portfolio  Management in Customizable Stock Pools",
    "abstract": " Title: Reinforcement Learning with Maskable Stock Representation for Portfolio  Management in Customizable Stock Pools ",
    "url": "https://arxiv.org/abs/2311.10801",
    "authors": [
      "Wentao Zhang",
      "Yilei Zhao",
      "Shuo Sun",
      "Jie Ying",
      "Yonggang Xie",
      "Zitao Song",
      "Xinrun Wang",
      "Bo An"
    ],
    "subjectives": [
      "Portfolio Management (q-fin.PM)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.11509",
    "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures  and Contextual Information",
    "abstract": " Title: Token-Level Adversarial Prompt Detection Based on Perplexity Measures  and Contextual Information ",
    "url": "https://arxiv.org/abs/2311.11509",
    "authors": [
      "Zhengmian Hu",
      "Gang Wu",
      "Saayan Mitra",
      "Ruiyi Zhang",
      "Tong Sun",
      "Heng Huang",
      "Viswanathan Swaminathan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.13010",
    "title": "Beyond Catoni: Sharper Rates for Heavy-Tailed and Robust Mean Estimation",
    "abstract": " Title: Beyond Catoni: Sharper Rates for Heavy-Tailed and Robust Mean Estimation ",
    "url": "https://arxiv.org/abs/2311.13010",
    "authors": [
      "Shivam Gupta",
      "Samuel B. Hopkins",
      "Eric Price"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Data Structures and Algorithms (cs.DS)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2311.14778",
    "title": "Anomaly detection in cross-country money transfer temporal networks",
    "abstract": " Title: Anomaly detection in cross-country money transfer temporal networks ",
    "url": "https://arxiv.org/abs/2311.14778",
    "authors": [
      "Salvatore Vilella",
      "Arthur Thomas Edward Capozzi Lupi",
      "Marco Fornasiero",
      "Dario Moncalvo",
      "Valeria Ricci",
      "Silvia Ronchiadin",
      "Giancarlo Ruffo"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2311.14901",
    "title": "Code Search Debiasing:Improve Search Results beyond Overall Ranking  Performance",
    "abstract": " Comments: 11 pages ",
    "url": "https://arxiv.org/abs/2311.14901",
    "authors": [
      "Sheng Zhang",
      "Hui Li",
      "Yanlin Wang",
      "Zhao Wei",
      "Yong Xiu",
      "Juhong Wang",
      "Rongong Ji"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2311.16121",
    "title": "Real-Time Neural Materials using Block-Compressed Features",
    "abstract": " Comments: Eurographics 2024 ",
    "url": "https://arxiv.org/abs/2311.16121",
    "authors": [
      "Cl\u00e9ment Weinreich",
      "Louis de Oliveira",
      "Antoine Houdard",
      "Georges Nader"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2311.16716",
    "title": "GraphPro: Graph Pre-training and Prompt Learning for Recommendation",
    "abstract": " Comments: Accepted by WWW'2024, full paper ",
    "url": "https://arxiv.org/abs/2311.16716",
    "authors": [
      "Yuhao Yang",
      "Lianghao Xia",
      "Da Luo",
      "Kangyi Lin",
      "Chao Huang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2312.00078",
    "title": "Enhancing Cross-domain Click-Through Rate Prediction via Explicit  Feature Augmentation",
    "abstract": " Comments: accepted by WWW 2024. arXiv admin note: substantial text overlap with arXiv:2305.03953 ",
    "url": "https://arxiv.org/abs/2312.00078",
    "authors": [
      "Xu Chen",
      "Zida Cheng",
      "Jiangchao Yao",
      "Chen Ju",
      "Weilin Huang",
      "Jinsong Lan",
      "Xiaoyi Zeng",
      "Shuai Xiao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2312.00519",
    "title": "The Impact of Privacy and Security Attitudes and Concerns of Travellers  on Their Willingness to Use Mobility-as-a-Service Systems",
    "abstract": " Comments: Please cite this paper as follows: Maria Sophia Heering, Haiyue Yuan and Shujun Li (2023) The Impact of Privacy and Security Attitudes and Concerns of Travellers on Their Willingness to Use Mobility-as-a-Service Systems. Proceedings of the 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC 2023), pp.5573-5578, IEEE, doi: 10.1109/ITSC57777.2023.10422468 ",
    "url": "https://arxiv.org/abs/2312.00519",
    "authors": [
      "Maria Sophia Heering",
      "Haiyue Yuan",
      "Shujun Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2312.05583",
    "title": "Better Neural PDE Solvers Through Data-Free Mesh Movers",
    "abstract": " Title: Better Neural PDE Solvers Through Data-Free Mesh Movers ",
    "url": "https://arxiv.org/abs/2312.05583",
    "authors": [
      "Peiyan Hu",
      "Yue Wang",
      "Zhi-Ming Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2312.05875",
    "title": "Class-Aware Pruning for Efficient Neural Networks",
    "abstract": " Comments: Accepted by Design Automation and Test in Europe (DATE) 2024 ",
    "url": "https://arxiv.org/abs/2312.05875",
    "authors": [
      "Mengnan Jiang",
      "Jingcun Wang",
      "Amro Eldebiky",
      "Xunzhao Yin",
      "Cheng Zhuo",
      "Ing-Chao Lin",
      "Grace Li Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2312.08207",
    "title": "Black-box Membership Inference Attacks against Fine-tuned Diffusion  Models",
    "abstract": " Title: Black-box Membership Inference Attacks against Fine-tuned Diffusion  Models ",
    "url": "https://arxiv.org/abs/2312.08207",
    "authors": [
      "Yan Pang",
      "Tianhao Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2312.10040",
    "title": "Robust Errant Beam Prognostics with Conditional Modeling for Particle  Accelerators",
    "abstract": " Comments: Under review at Machine Learning: Science and Technology Journal ",
    "url": "https://arxiv.org/abs/2312.10040",
    "authors": [
      "Kishansingh Rajput",
      "Malachi Schram",
      "Willem Blokland",
      "Yasir Alanazi",
      "Pradeep Ramuhalli",
      "Alexander Zhukov",
      "Charles Peters",
      "Ricardo Vilalta"
    ],
    "subjectives": [
      "Accelerator Physics (physics.acc-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2312.14922",
    "title": "Learning from higher-order statistics, efficiently: hypothesis tests,  random features, and neural networks",
    "abstract": " Title: Learning from higher-order statistics, efficiently: hypothesis tests,  random features, and neural networks ",
    "url": "https://arxiv.org/abs/2312.14922",
    "authors": [
      "Eszter Sz\u00e9kely",
      "Lorenzo Bardone",
      "Federica Gerace",
      "Sebastian Goldt"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2312.16819",
    "title": "Hidden Minima in Two-Layer ReLU Networks",
    "abstract": " Title: Hidden Minima in Two-Layer ReLU Networks ",
    "url": "https://arxiv.org/abs/2312.16819",
    "authors": [
      "Yossi Arjevani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2401.05018",
    "title": "AdvMT: Adversarial Motion Transformer for Long-term Human Motion  Prediction",
    "abstract": " Comments: The paper is under consideration at Pattern Recognition Letters ",
    "url": "https://arxiv.org/abs/2401.05018",
    "authors": [
      "Sarmad Idrees",
      "Jongeun Choi",
      "Seokman Sohn"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2401.06801",
    "title": "Graph-of-Thought: Utilizing Large Language Models to Solve Complex and  Dynamic Business Problems",
    "abstract": " Comments: Keywords: Graph-of-Thought (GoT), Workflow Automation, Large Language Models (LLMs), Task Execution, Data-Driven Decision Making, Complexity Management ",
    "url": "https://arxiv.org/abs/2401.06801",
    "authors": [
      "Ye Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.06824",
    "title": "Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation  Engineering",
    "abstract": " Comments: 13 pages, 9 figures ",
    "url": "https://arxiv.org/abs/2401.06824",
    "authors": [
      "Tianlong Li",
      "Shihan Dou",
      "Wenhao Liu",
      "Muling Wu",
      "Changze Lv",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.07105",
    "title": "Graph Language Models",
    "abstract": " Comments: 8 pages, 10 figures, 9 tables ",
    "url": "https://arxiv.org/abs/2401.07105",
    "authors": [
      "Moritz Plenz",
      "Anette Frank"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2401.08876",
    "title": "Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image  Labeling",
    "abstract": " Comments: 19 pages, 11 figures, 9 tables. Accepted by ACM CHI 2024 ",
    "url": "https://arxiv.org/abs/2401.08876",
    "authors": [
      "Dongping Zhang",
      "Angelos Chatzimparmpas",
      "Negar Kamali",
      "Jessica Hullman"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2401.11610",
    "title": "Note on k-Planar and Min-k-Planar Drawings of Graphs",
    "abstract": " Title: Note on k-Planar and Min-k-Planar Drawings of Graphs ",
    "url": "https://arxiv.org/abs/2401.11610",
    "authors": [
      "Petr Hlin\u011bn\u00fd"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2401.11772",
    "title": "LightDiC: A Simple yet Effective Approach for Large-scale Digraph  Representation Learning",
    "abstract": " Comments: Accepted by VLDB 2024 ",
    "url": "https://arxiv.org/abs/2401.11772",
    "authors": [
      "Xunkai Li",
      "Meihao Liao",
      "Zhengyu Wu",
      "Daohan Su",
      "Wentao Zhang",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2401.11963",
    "title": "Bridging Evolutionary Algorithms and Reinforcement Learning: A  Comprehensive Survey",
    "abstract": " Title: Bridging Evolutionary Algorithms and Reinforcement Learning: A  Comprehensive Survey ",
    "url": "https://arxiv.org/abs/2401.11963",
    "authors": [
      "Pengyi Li",
      "Jianye Hao",
      "Hongyao Tang",
      "Xian Fu",
      "Yan Zheng",
      "Ke Tang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2401.13789",
    "title": "A Unified Approach to Emotion Detection and Task-Oriented Dialogue  Modeling",
    "abstract": " Comments: Accepted @ IWSDS 2024 ",
    "url": "https://arxiv.org/abs/2401.13789",
    "authors": [
      "Armand Stricker",
      "Patrick Paroubek"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.01341",
    "title": "Fundamental Properties of Causal Entropy and Information Gain",
    "abstract": " Comments: In Proceedings of the conference CLeaR (Causal Learning and Reasoning) 2024 ",
    "url": "https://arxiv.org/abs/2402.01341",
    "authors": [
      "Francisco N. F. Q. Simoes",
      "Mehdi Dastani",
      "Thijs van Ommen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.01967",
    "title": "MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate  Speech and Target Detection Using Transformer Ensembles",
    "abstract": " Title: MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate  Speech and Target Detection Using Transformer Ensembles ",
    "url": "https://arxiv.org/abs/2402.01967",
    "authors": [
      "Amrita Ganguly",
      "Al Nahian Bin Emran",
      "Sadiya Sayara Chowdhury Puspo",
      "Md Nishat Raihan",
      "Dhiman Goswami",
      "Marcos Zampieri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.02041",
    "title": "$\u03b1$-Divergence Loss Function for Neural Density Ratio Estimation",
    "abstract": " Comments: $\\mathcal{T}_{\\text{Lip}}$ in Theorem 7.1 (Theorem B.15.) was changed to the set of all locally Lipschitz continuous functions. In the previous version, $\\mathcal{T}_{\\text{Lip}}$ was defined as the set of all Lipschitz continuous functions, which is unsuitable for the statement of case (ii) in the theorem ",
    "url": "https://arxiv.org/abs/2402.02041",
    "authors": [
      "Yoshiaki Kitazawa"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.02047",
    "title": "Calibration and Correctness of Language Models for Code",
    "abstract": " Title: Calibration and Correctness of Language Models for Code ",
    "url": "https://arxiv.org/abs/2402.02047",
    "authors": [
      "Claudio Spiess",
      "David Gros",
      "Kunal Suresh Pai",
      "Michael Pradel",
      "Md Rafiqul Islam Rabin",
      "Amin Alipour",
      "Susmit Jha",
      "Prem Devanbu",
      "Toufique Ahmed"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.02130",
    "title": "Rendering Graphs for Graph Reasoning in Multimodal Large Language Models",
    "abstract": " Title: Rendering Graphs for Graph Reasoning in Multimodal Large Language Models ",
    "url": "https://arxiv.org/abs/2402.02130",
    "authors": [
      "Yanbin Wei",
      "Shuai Fu",
      "Weisen Jiang",
      "James T. Kwok",
      "Yu Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.03358",
    "title": "A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening,  and Condensation",
    "abstract": " Comments: 16 pages, 3 tables, 2 figures ",
    "url": "https://arxiv.org/abs/2402.03358",
    "authors": [
      "Mohammad Hashemi",
      "Shengbo Gong",
      "Juntong Ni",
      "Wenqi Fan",
      "B. Aditya Prakash",
      "Wei Jin"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.04296",
    "title": "LightHGNN: Distilling Hypergraph Neural Networks into MLPs for  $100\\times$ Faster Inference",
    "abstract": " Comments: Some details are missing. The method of this paper is not complete ",
    "url": "https://arxiv.org/abs/2402.04296",
    "authors": [
      "Yifan Feng",
      "Yihe Luo",
      "Shihui Ying",
      "Yue Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.04821",
    "title": "E(3)-Equivariant Mesh Neural Networks",
    "abstract": " Title: E(3)-Equivariant Mesh Neural Networks ",
    "url": "https://arxiv.org/abs/2402.04821",
    "authors": [
      "Thuan Trang",
      "Nhat Khang Ngo",
      "Daniel Levy",
      "Thieu N. Vo",
      "Siamak Ravanbakhsh",
      "Truong Son Hy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05396",
    "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph  Representation Learning",
    "abstract": " Comments: IPDPS 2024 ",
    "url": "https://arxiv.org/abs/2402.05396",
    "authors": [
      "Gangda Deng",
      "Hongkuan Zhou",
      "Hanqing Zeng",
      "Yinglong Xia",
      "Christopher Leung",
      "Jianbo Li",
      "Rajgopal Kannan",
      "Viktor Prasanna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05493",
    "title": "Investigating White-Box Attacks for On-Device Models",
    "abstract": " Comments: The International Conference on Software Engineering 2024 (ICSE'24) ",
    "url": "https://arxiv.org/abs/2402.05493",
    "authors": [
      "Mingyi Zhou",
      "Xiang Gao",
      "Jing Wu",
      "Kui Liu",
      "Hailong Sun",
      "Li Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.06430",
    "title": "Polyarc bounded complex interval arithmetic",
    "abstract": " Comments: 38 pages (plus 48 pages of supplemetary material), 8 figures (plus 21 supplementary) ",
    "url": "https://arxiv.org/abs/2402.06430",
    "authors": [
      "G\u00e1bor Ger\u00e9b",
      "Andr\u00e1s S\u00e1ndor"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Algebraic Geometry (math.AG)"
    ]
  },
  {
    "id": "arXiv:2402.06512",
    "title": "Multimodal Clinical Trial Outcome Prediction with Large Language Models",
    "abstract": " Title: Multimodal Clinical Trial Outcome Prediction with Large Language Models ",
    "url": "https://arxiv.org/abs/2402.06512",
    "authors": [
      "Wenhao Zheng",
      "Dongsheng Peng",
      "Hongxia Xu",
      "Hongtu Zhu",
      "Tianfan Fu",
      "Huaxiu Yao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.06716",
    "title": "Dynamic Graph Information Bottleneck",
    "abstract": " Comments: Accepted by the research tracks of The Web Conference 2024 (WWW 2024) ",
    "url": "https://arxiv.org/abs/2402.06716",
    "authors": [
      "Haonan Yuan",
      "Qingyun Sun",
      "Xingcheng Fu",
      "Cheng Ji",
      "Jianxin Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.07092",
    "title": "Generalizing Conversational Dense Retrieval via LLM-Cognition Data  Augmentation",
    "abstract": " Title: Generalizing Conversational Dense Retrieval via LLM-Cognition Data  Augmentation ",
    "url": "https://arxiv.org/abs/2402.07092",
    "authors": [
      "Haonan Chen",
      "Zhicheng Dou",
      "Kelong Mao",
      "Jiongnan Liu",
      "Ziliang Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2402.07570",
    "title": "Only the Curve Shape Matters: Training Foundation Models for Zero-Shot  Multivariate Time Series Forecasting through Next Curve Shape Prediction",
    "abstract": " Title: Only the Curve Shape Matters: Training Foundation Models for Zero-Shot  Multivariate Time Series Forecasting through Next Curve Shape Prediction ",
    "url": "https://arxiv.org/abs/2402.07570",
    "authors": [
      "Cheng Feng",
      "Long Huang",
      "Denis Krompass"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.08170",
    "title": "LLaGA: Large Language and Graph Assistant",
    "abstract": " Title: LLaGA: Large Language and Graph Assistant ",
    "url": "https://arxiv.org/abs/2402.08170",
    "authors": [
      "Runjin Chen",
      "Tong Zhao",
      "Ajay Jaiswal",
      "Neil Shah",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.08223",
    "title": "The Limits of Price Discrimination Under Privacy Constraints",
    "abstract": " Title: The Limits of Price Discrimination Under Privacy Constraints ",
    "url": "https://arxiv.org/abs/2402.08223",
    "authors": [
      "Alireza Fallah",
      "Michael I. Jordan",
      "Ali Makhdoumi",
      "Azarakhsh Malekian"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2402.08653",
    "title": "SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds",
    "abstract": " Title: SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds ",
    "url": "https://arxiv.org/abs/2402.08653",
    "authors": [
      "Wuxinlin Cheng",
      "Chenhui Deng",
      "Ali Aghdaei",
      "Zhiru Zhang",
      "Zhuo Feng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.08678",
    "title": "Graph Mamba: Towards Learning on Graphs with State Space Models",
    "abstract": " Title: Graph Mamba: Towards Learning on Graphs with State Space Models ",
    "url": "https://arxiv.org/abs/2402.08678",
    "authors": [
      "Ali Behrouz",
      "Farnoosh Hashemi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.08971",
    "title": "Structured Language Generation Model for Robust Structure Prediction",
    "abstract": " Comments: 8 pages, 4 figures, 5 tables, 7 pages of appendix with 9 additional tables ",
    "url": "https://arxiv.org/abs/2402.08971",
    "authors": [
      "Minho Lee",
      "Junghyun Min",
      "Woochul Lee",
      "Yeonsoo Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.09264",
    "title": "UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers",
    "abstract": " Title: UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers ",
    "url": "https://arxiv.org/abs/2402.09264",
    "authors": [
      "Hong Jia",
      "Young D. Kwon",
      "Dong Ma",
      "Nhat Pham",
      "Lorena Qendro",
      "Tam Vu",
      "Cecilia Mascolo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2402.09303",
    "title": "Immediate generalisation in humans but a generalisation lag in deep  neural networks -- evidence for representational divergence?",
    "abstract": " Comments: Under review at the ICLR 2024 Workshop on Representational Alignment (Re-Align) ",
    "url": "https://arxiv.org/abs/2402.09303",
    "authors": [
      "Lukas S. Huber",
      "Fred W. Mast",
      "Felix A. Wichmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2402.09329",
    "title": "YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture  Detection",
    "abstract": " Title: YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture  Detection ",
    "url": "https://arxiv.org/abs/2402.09329",
    "authors": [
      "Chun-Tse Chien",
      "Rui-Yang Ju",
      "Kuang-Yi Chou",
      "Jen-Shiun Chiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.09495",
    "title": "On the Potential of Network-Based Features for Fraud Detection",
    "abstract": " Title: On the Potential of Network-Based Features for Fraud Detection ",
    "url": "https://arxiv.org/abs/2402.09495",
    "authors": [
      "Catayoun Azarm",
      "Erman Acar",
      "Mickey van Zeelt"
    ],
    "subjectives": [
      "Risk Management (q-fin.RM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.09820",
    "title": "Utilizing Deep Learning for Enhancing Network Resilience in Finance",
    "abstract": " Title: Utilizing Deep Learning for Enhancing Network Resilience in Finance ",
    "url": "https://arxiv.org/abs/2402.09820",
    "authors": [
      "Yulu Gong",
      "Mengran Zhu",
      "Shuning Huo",
      "Yafei Xiang",
      "Hanyi Yu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "General Finance (q-fin.GN)"
    ]
  },
  {
    "id": "arXiv:2402.10074",
    "title": "Class-Balanced and Reinforced Active Learning on Graphs",
    "abstract": " Title: Class-Balanced and Reinforced Active Learning on Graphs ",
    "url": "https://arxiv.org/abs/2402.10074",
    "authors": [
      "Chengcheng Yu",
      "Jiapeng Zhu",
      "Xiang Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.10163",
    "title": "Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural  Networks",
    "abstract": " Title: Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural  Networks ",
    "url": "https://arxiv.org/abs/2402.10163",
    "authors": [
      "Arjun Karuvally",
      "Terrence J. Sejnowski",
      "Hava T. Siegelmann"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2402.10184",
    "title": "Rethinking Information Structures in RLHF: Reward Generalization from a  Graph Theory Perspective",
    "abstract": " Title: Rethinking Information Structures in RLHF: Reward Generalization from a  Graph Theory Perspective ",
    "url": "https://arxiv.org/abs/2402.10184",
    "authors": [
      "Tianyi Qiu",
      "Fanzhi Zeng",
      "Jiaming Ji",
      "Dong Yan",
      "Kaile Wang",
      "Jiayi Zhou",
      "Han Yang",
      "Josef Dai",
      "Xuehai Pan",
      "Yaodong Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Discrete Mathematics (cs.DM)"
    ]
  }
]