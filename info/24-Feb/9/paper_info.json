[
  {
    "id": "arXiv:2402.05126",
    "title": "Graph Neural Network and NER-Based Text Summarization",
    "abstract": "With the abundance of data and information in todays time, it is nearly impossible for man, or, even machine, to go through all of the data line by line. What one usually does is to try to skim through the lines and retain the absolutely important information, that in a more formal term is called summarization. Text summarization is an important task that aims to compress lengthy documents or articles into shorter, coherent representations while preserving the core information and meaning. This project introduces an innovative approach to text summarization, leveraging the capabilities of Graph Neural Networks (GNNs) and Named Entity Recognition (NER) systems. GNNs, with their exceptional ability to capture and process the relational data inherent in textual information, are adept at understanding the complex structures within large documents. Meanwhile, NER systems contribute by identifying and emphasizing key entities, ensuring that the summarization process maintains a focus on the most critical aspects of the text. By integrating these two technologies, our method aims to enhances the efficiency of summarization and also tries to ensures a high degree relevance in the condensed content. This project, therefore, offers a promising direction for handling the ever increasing volume of textual data in an information-saturated world. ",
    "url": "https://arxiv.org/abs/2402.05126",
    "authors": [
      "Imaad Zaffar Khan",
      "Amaan Aijaz Sheikh",
      "Utkarsh Sinha"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05127",
    "title": "Illuminate: A novel approach for depression detection with explainable  analysis and proactive therapy using prompt engineering",
    "abstract": "This paper introduces a novel paradigm for depression detection and treatment using advanced Large Language Models (LLMs): Generative Pre-trained Transformer 4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized prompts to diagnose, explain, and suggest therapeutic interventions for depression. A unique few-shot prompting method enhances the models' ability to analyze and explain depressive symptoms based on the DSM-5 criteria. In the interaction phase, the models engage in empathetic dialogue management, drawing from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide, fostering supportive interactions with individuals experiencing major depressive disorders. Additionally, the research introduces the Illuminate Database, enriched with various CBT modules, aiding in personalized therapy recommendations. The study evaluates LLM performance using metrics such as F1 scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) across different test sets, demonstrating their effectiveness. This comprehensive approach blends cutting-edge AI with established psychological methods, offering new possibilities in mental health care and showcasing the potential of LLMs in revolutionizing depression diagnosis and treatment strategies. ",
    "url": "https://arxiv.org/abs/2402.05127",
    "authors": [
      "Aryan Agrawal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05132",
    "title": "TexShape: Information Theoretic Sentence Embedding for Language Models",
    "abstract": "With the exponential growth in data volume and the emergence of data-intensive applications, particularly in the field of machine learning, concerns related to resource utilization, privacy, and fairness have become paramount. This paper focuses on the textual domain of data and addresses challenges regarding encoding sentences to their optimized representations through the lens of information-theory. In particular, we use empirical estimates of mutual information, using the Donsker-Varadhan definition of Kullback-Leibler divergence. Our approach leverages this estimation to train an information-theoretic sentence embedding, called TexShape, for (task-based) data compression or for filtering out sensitive information, enhancing privacy and fairness. In this study, we employ a benchmark language model for initial text representation, complemented by neural networks for information-theoretic compression and mutual information estimations. Our experiments demonstrate significant advancements in preserving maximal targeted information and minimal sensitive information over adverse compression ratios, in terms of predictive accuracy of downstream models that are trained using the compressed data. ",
    "url": "https://arxiv.org/abs/2402.05132",
    "authors": [
      "H. Kaan Kale",
      "Homa Esfahanizadeh",
      "Noel Elias",
      "Oguzhan Baser",
      "Muriel Medard",
      "Sriram Vishwanath"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2402.05135",
    "title": "CADReN: Contextual Anchor-Driven Relational Network for Controllable  Cross-Graphs Node Importance Estimation",
    "abstract": "Node Importance Estimation (NIE) is crucial for integrating external information into Large Language Models through Retriever-Augmented Generation. Traditional methods, focusing on static, single-graph characteristics, lack adaptability to new graphs and user-specific requirements. CADReN, our proposed method, addresses these limitations by introducing a Contextual Anchor (CA) mechanism. This approach enables the network to assess node importance relative to the CA, considering both structural and semantic features within Knowledge Graphs (KGs). Extensive experiments show that CADReN achieves better performance in cross-graph NIE task, with zero-shot prediction ability. CADReN is also proven to match the performance of previous models on single-graph NIE task. Additionally, we introduce and opensource two new datasets, RIC200 and WK1K, specifically designed for cross-graph NIE research, providing a valuable resource for future developments in this domain. ",
    "url": "https://arxiv.org/abs/2402.05135",
    "authors": [
      "Zijie Zhong",
      "Yunhui Zhang",
      "Ziyi Chang",
      "Zengchang Qin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2402.05144",
    "title": "A Bandit Approach with Evolutionary Operators for Model Selection",
    "abstract": "This paper formulates model selection as an infinite-armed bandit problem. The models are arms, and picking an arm corresponds to a partial training of the model (resource allocation). The reward is the accuracy of the selected model after its partial training. In this best arm identification problem, regret is the gap between the expected accuracy of the optimal model and that of the model finally chosen. We first consider a straightforward generalization of UCB-E to the stochastic infinite-armed bandit problem and show that, under basic assumptions, the expected regret order is $T^{-\\alpha}$ for some $\\alpha \\in (0,1/5)$ and $T$ the number of resources to allocate. From this vanilla algorithm, we introduce the algorithm Mutant-UCB that incorporates operators from evolutionary algorithms. Tests carried out on three open source image classification data sets attest to the relevance of this novel combining approach, which outperforms the state-of-the-art for a fixed budget. ",
    "url": "https://arxiv.org/abs/2402.05144",
    "authors": [
      "Margaux Br\u00e9g\u00e8re",
      "Julie Keisler"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2402.05146",
    "title": "Compressing Deep Reinforcement Learning Networks with a Dynamic  Structured Pruning Method for Autonomous Driving",
    "abstract": "Deep reinforcement learning (DRL) has shown remarkable success in complex autonomous driving scenarios. However, DRL models inevitably bring high memory consumption and computation, which hinders their wide deployment in resource-limited autonomous driving devices. Structured Pruning has been recognized as a useful method to compress and accelerate DRL models, but it is still challenging to estimate the contribution of a parameter (i.e., neuron) to DRL models. In this paper, we introduce a novel dynamic structured pruning approach that gradually removes a DRL model's unimportant neurons during the training stage. Our method consists of two steps, i.e. training DRL models with a group sparse regularizer and removing unimportant neurons with a dynamic pruning threshold. To efficiently train the DRL model with a small number of important neurons, we employ a neuron-importance group sparse regularizer. In contrast to conventional regularizers, this regularizer imposes a penalty on redundant groups of neurons that do not significantly influence the output of the DRL model. Furthermore, we design a novel structured pruning strategy to dynamically determine the pruning threshold and gradually remove unimportant neurons with a binary mask. Therefore, our method can remove not only redundant groups of neurons of the DRL model but also achieve high and robust performance. Experimental results show that the proposed method is competitive with existing DRL pruning methods on discrete control environments (i.e., CartPole-v1 and LunarLander-v2) and MuJoCo continuous environments (i.e., Hopper-v3 and Walker2D-v3). Specifically, our method effectively compresses $93\\%$ neurons and $96\\%$ weights of the DRL model in four challenging DRL environments with slight accuracy degradation. ",
    "url": "https://arxiv.org/abs/2402.05146",
    "authors": [
      "Wensheng Su",
      "Zhenni Li",
      "Minrui Xu",
      "Jiawen Kang",
      "Dusit Niyato",
      "Shengli Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2402.05150",
    "title": "Designing deep neural networks for driver intention recognition",
    "abstract": "Driver intention recognition studies increasingly rely on deep neural networks. Deep neural networks have achieved top performance for many different tasks, but it is not a common practice to explicitly analyse the complexity and performance of the network's architecture. Therefore, this paper applies neural architecture search to investigate the effects of the deep neural network architecture on a real-world safety critical application with limited computational capabilities. We explore a pre-defined search space for three deep neural network layer types that are capable to handle sequential data (a long-short term memory, temporal convolution, and a time-series transformer layer), and the influence of different data fusion strategies on the driver intention recognition performance. A set of eight search strategies are evaluated for two driver intention recognition datasets. For the two datasets, we observed that there is no search strategy clearly sampling better deep neural network architectures. However, performing an architecture search does improve the model performance compared to the original manually designed networks. Furthermore, we observe no relation between increased model complexity and higher driver intention recognition performance. The result indicate that multiple architectures yield similar performance, regardless of the deep neural network layer type or fusion strategy. ",
    "url": "https://arxiv.org/abs/2402.05150",
    "authors": [
      "Koen Vellenga",
      "H. Joe Steinhauer",
      "Alexander Karlsson",
      "G\u00f6ran Falkman",
      "Asli Rhodin",
      "Ashok Koppisetty"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2402.05153",
    "title": "Estimating On-road Transportation Carbon Emissions from Open Data of  Road Network and Origin-destination Flow Data",
    "abstract": "Accounting for over 20% of the total carbon emissions, the precise estimation of on-road transportation carbon emissions is crucial for carbon emission monitoring and efficient mitigation policy formulation. However, existing estimation methods typically depend on hard-to-collect individual statistics of vehicle miles traveled to calculate emissions, thereby suffering from high data collection difficulty. To relieve this issue by utilizing the strong pattern recognition of artificial intelligence, we incorporate two sources of open data representative of the transportation demand and capacity factors, the origin-destination (OD) flow data and the road network data, to build a hierarchical heterogeneous graph learning method for on-road carbon emission estimation (HENCE). Specifically, a hierarchical graph consisting of the road network level, community level, and region level is constructed to model the multi-scale road network-based connectivity and travel connection between spatial areas. Heterogeneous graphs consisting of OD links and spatial links are further built at both the community level and region level to capture the intrinsic interactions between travel demand and road network accessibility. Extensive experiments on two large-scale real-world datasets demonstrate HENCE's effectiveness and superiority with R-squared exceeding 0.75 and outperforming baselines by 9.60% on average, validating its success in pioneering the use of artificial intelligence to empower carbon emission management and sustainability development. The implementation codes are available at this link: https://github.com/tsinghua-fib-lab/HENCE. ",
    "url": "https://arxiv.org/abs/2402.05153",
    "authors": [
      "Jinwei Zeng",
      "Yu Liu",
      "Jingtao Ding",
      "Jian Yuan",
      "Yong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05154",
    "title": "Adaptive Hypergraph Network for Trust Prediction",
    "abstract": "Trust plays an essential role in an individual's decision-making. Traditional trust prediction models rely on pairwise correlations to infer potential relationships between users. However, in the real world, interactions between users are usually complicated rather than pairwise only. Hypergraphs offer a flexible approach to modeling these complex high-order correlations (not just pairwise connections), since hypergraphs can leverage hyperedeges to link more than two nodes. However, most hypergraph-based methods are generic and cannot be well applied to the trust prediction task. In this paper, we propose an Adaptive Hypergraph Network for Trust Prediction (AHNTP), a novel approach that improves trust prediction accuracy by using higher-order correlations. AHNTP utilizes Motif-based PageRank to capture high-order social influence information. In addition, it constructs hypergroups from both node-level and structure-level attributes to incorporate complex correlation information. Furthermore, AHNTP leverages adaptive hypergraph Graph Convolutional Network (GCN) layers and multilayer perceptrons (MLPs) to generate comprehensive user embeddings, facilitating trust relationship prediction. To enhance model generalization and robustness, we introduce a novel supervised contrastive learning loss for optimization. Extensive experiments demonstrate the superiority of our model over the state-of-the-art approaches in terms of trust prediction accuracy. The source code of this work can be accessed via https://github.com/Sherry-XU1995/AHNTP. ",
    "url": "https://arxiv.org/abs/2402.05154",
    "authors": [
      "Rongwei Xu",
      "Guanfeng Liu",
      "Yan Wang",
      "Xuyun Zhang",
      "Kai Zheng",
      "Xiaofang Zhou"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05159",
    "title": "Threats and Limitations of Terrestrial Broadcast Attacks",
    "abstract": "The DVB standard does not mandate the use of authentication and integrity protection for transport streams. This allows malicious third parties to replace legitimate broadcasts by overpowering terrestrial transmissions. The rogue signal can then deliver a malicious broadcast stream to exploit security vulnerabilities on Smart TVs (STVs) in range. We implemented a proof-of-concept attack based on a malicious Hybrid Broadcast Broadband TV app, able to acquire permanent system-level access to an STV over the air, in less than 10 s. These attacks, however, are severely limited in range due to required co-channel protection ratios (CCPRs), which is in direct contradiction to previous publications. We present evidence for these limitations in form of laboratory experiments, extensive simulations, and field measurements. To this end, we developed an automated, low-cost method for CCPR determination, as well as a method for non-disruptive attack range measurements based on a gap filler and the resulting channel impulse response. ",
    "url": "https://arxiv.org/abs/2402.05159",
    "authors": [
      "Benjamin Michele",
      "Ivan Pena",
      "Pablo Angueira"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2402.05164",
    "title": "A Resource Model For Neural Scaling Law",
    "abstract": "Neural scaling laws characterize how model performance improves as the model size scales up. Inspired by empirical observations, we introduce a resource model of neural scaling. A task is usually composite hence can be decomposed into many subtasks, which compete for resources (measured by the number of neurons allocated to subtasks). On toy problems, we empirically find that: (1) The loss of a subtask is inversely proportional to its allocated neurons. (2) When multiple subtasks are present in a composite task, the resources acquired by each subtask uniformly grow as models get larger, keeping the ratios of acquired resources constants. We hypothesize these findings to be generally true and build a model to predict neural scaling laws for general composite tasks, which successfully replicates the neural scaling law of Chinchilla models reported in arXiv:2203.15556. We believe that the notion of resource used in this paper will be a useful tool for characterizing and diagnosing neural networks. ",
    "url": "https://arxiv.org/abs/2402.05164",
    "authors": [
      "Jinyeop Song",
      "Ziming Liu",
      "Max Tegmark",
      "Jeff Gore"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2402.05203",
    "title": "Bellman Conformal Inference: Calibrating Prediction Intervals For Time  Series",
    "abstract": "We introduce Bellman Conformal Inference (BCI), a framework that wraps around any time series forecasting models and provides calibrated prediction intervals. Unlike the existing methods, BCI is able to leverage multi-step ahead forecasts and explicitly optimize the average interval lengths by solving a one-dimensional stochastic control problem (SCP) at each time step. In particular, we use the dynamic programming algorithm to find the optimal policy for the SCP. We prove that BCI achieves long-term coverage under arbitrary distribution shifts and temporal dependence, even with poor multi-step ahead forecasts. We find empirically that BCI avoids uninformative intervals that have infinite lengths and generates substantially shorter prediction intervals on volatility forecasting problems when compared with existing methods. ",
    "url": "https://arxiv.org/abs/2402.05203",
    "authors": [
      "Zitong Yang",
      "Emmanuel Cand\u00e8s",
      "Lihua Lei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.05232",
    "title": "Universal Neural Functionals",
    "abstract": "A challenging problem in many modern machine learning tasks is to process weight-space features, i.e., to transform or extract information from the weights and gradients of a neural network. Recent works have developed promising weight-space models that are equivariant to the permutation symmetries of simple feedforward networks. However, they are not applicable to general architectures, since the permutation symmetries of a weight space can be complicated by recurrence or residual connections. This work proposes an algorithm that automatically constructs permutation equivariant models, which we refer to as universal neural functionals (UNFs), for any weight space. Among other applications, we demonstrate how UNFs can be substituted into existing learned optimizer designs, and find promising improvements over prior methods when optimizing small image classifiers and language models. Our results suggest that learned optimizers can benefit from considering the (symmetry) structure of the weight space they optimize. We open-source our library for constructing UNFs at https://github.com/AllanYangZhou/universal_neural_functional. ",
    "url": "https://arxiv.org/abs/2402.05232",
    "authors": [
      "Allan Zhou",
      "Chelsea Finn",
      "James Harrison"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05255",
    "title": "An Overview of Machine Learning-Enabled Network Softwarization for the  Internet of Things",
    "abstract": "The Internet of Things (IoT) has evolved from a novel technology to an integral part of our everyday lives. It encompasses a multitude of heterogeneous devices that collect valuable data through various sensors. The sheer volume of these interconnected devices poses significant challenges as IoT provides complex network services with diverse requirements on a shared infrastructure. Network softwarization could help address these issues as it has emerged as a paradigm that enhances traditional networking by decoupling hardware from software and leveraging enabling technologies such as Software Defined Networking (SDN) and Network Function Virtualization (NFV). In networking, Machine Learning (ML) has demonstrated impressive results across multiple domains. By smoothly integrating with network softwarization, ML plays a pivotal role in building efficient and intelligent IoT networks. This paper explores the fundamentals of IoT, network softwarization, and ML, while reviewing the latest advances in ML-enabled network softwarization for IoT. ",
    "url": "https://arxiv.org/abs/2402.05255",
    "authors": [
      "Mohamed Ali Zormati",
      "Hicham Lakhlef"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2402.05256",
    "title": "IRFuzzer: Specialized Fuzzing for LLVM Backend Code Generation",
    "abstract": "Modern compilers, such as LLVM, are complex pieces of software. Due to their complexity, manual testing is unlikely to suffice, yet formal verification is difficult to scale. End-to-end fuzzing can be used, but it has difficulties in achieving high coverage of some components of LLVM. In this paper, we implement IRFuzzer to investigate the effectiveness of specialized fuzzing of the LLVM compiler backend. We focus on two approaches to improve the fuzzer: guaranteed input validity using constrained mutations and improved feedback quality. The mutator in IRFuzzer is capable of generating a wide range of LLVM IR inputs, including structured control flow, vector types, and function definitions. The system instruments coding patterns in the compiler to monitor the execution status of instruction selection. The instrumentation not only provides a new coverage feedback called matcher table coverage, but also provides an architecture specific guidance to the mutator. We show that IRFuzzer is more effective than existing fuzzers by fuzzing on 29 mature LLVM backend targets. In the process, we reported 74 confirmed new bugs in LLVM upstream, out of which 49 have been fixed, five have been back ported to LLVM 15, showing that specialized fuzzing provides useful and actionable insights to LLVM developers. ",
    "url": "https://arxiv.org/abs/2402.05256",
    "authors": [
      "Yuyang Rong",
      "Zhanghan Yu",
      "Zhenkai Weng",
      "Stephen Neuendorffer",
      "Hao Chen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2402.05259",
    "title": "Enabling Architecture for Distributed Intelligent Network Softwarization  for the Internet of Things",
    "abstract": "The Internet of Things (IoT) is becoming a part of everyday life through its various sensing devices that collect valuable information. The huge number of interconnected heterogeneous IoT devices poses immense challenges, and network softwarization techniques are an adequate solution to these concerns. Software Defined Networking (SDN) and Network Function Virtualization (NFV) are two key softwarization techniques that enable the realization of efficient, agile IoT networks, especially when combined with Machine Learning (ML), mainly Federated Learning (FL). Unfortunately, existing solutions do not take advantage of such a combination to strengthen IoT networks in terms of efficiency and scalability. In this paper, we propose a novel architecture to achieve distributed intelligent network softwarization for IoT, in which SDN, NFV, and ML combine forces to enhance IoT constrained networks. ",
    "url": "https://arxiv.org/abs/2402.05259",
    "authors": [
      "Mohamed Ali Zormati",
      "Hicham Lakhlef"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2402.05270",
    "title": "Review and Analysis of Recent Advances in Intelligent Network  Softwarization for the Internet of Things",
    "abstract": "The Internet of Things (IoT) is an emerging technology that aims to connect heterogeneous and constrained objects to each other and to the Internet. It has grown significantly in a wide variety of applications such as smart homes, smart cities, smart vehicles, etc. The huge number of connected devices increases the challenges, as IoT provides diverse and complex network services with different requirements on a common infrastructure. Network Softwarization is the latest network paradigm that transforms traditional network processes to the separation of hardware and software by using some enabling network technologies such as Software Defined Networking (SDN) and Network Function Virtualization (NFV). Machine Learning (ML) plays an essential role in creating smarter IoT networks, as it has shown remarkable results in various domains. Given that the network softwarization allows it to be easily integrated, ML can play a crucial role in efficient and self-adaptive IoT networks. In this paper, we provide a detailed overview of the concepts of IoT, network softwarization, and ML, and we study and discuss the state of the art of intelligent ML-enabled network softwarization for IoT. We also identify the most prominent future research directions to be considered. ",
    "url": "https://arxiv.org/abs/2402.05270",
    "authors": [
      "Mohamed Ali Zormati",
      "Hicham Lakhlef",
      "Sofiane Ouni"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2402.05284",
    "title": "Analyzing Adversarial Inputs in Deep Reinforcement Learning",
    "abstract": "In recent years, Deep Reinforcement Learning (DRL) has become a popular paradigm in machine learning due to its successful applications to real-world and complex systems. However, even the state-of-the-art DRL models have been shown to suffer from reliability concerns -- for example, their susceptibility to adversarial inputs, i.e., small and abundant input perturbations that can fool the models into making unpredictable and potentially dangerous decisions. This drawback limits the deployment of DRL systems in safety-critical contexts, where even a small error cannot be tolerated. In this work, we present a comprehensive analysis of the characterization of adversarial inputs, through the lens of formal verification. Specifically, we introduce a novel metric, the Adversarial Rate, to classify models based on their susceptibility to such perturbations, and present a set of tools and algorithms for its computation. Our analysis empirically demonstrates how adversarial inputs can affect the safety of a given DRL system with respect to such perturbations. Moreover, we analyze the behavior of these configurations to suggest several useful practices and guidelines to help mitigate the vulnerability of trained DRL networks. ",
    "url": "https://arxiv.org/abs/2402.05284",
    "authors": [
      "Davide Corsi",
      "Guy Amir",
      "Guy Katz",
      "Alessandro Farinelli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05291",
    "title": "Graph Neural Networks as Fast and High-fidelity Emulators for  Finite-Element Ice Sheet Modeling",
    "abstract": "Although the finite element approach of the Ice-sheet and Sea-level System Model (ISSM) solves ice dynamics problems governed by Stokes equations quickly and accurately, such numerical modeling requires intensive computation on central processing units (CPU). In this study, we develop graph neural networks (GNN) as fast surrogate models to preserve the finite element structure of ISSM. Using the 20-year transient simulations in the Pine Island Glacier (PIG), we train and test three GNNs: graph convolutional network (GCN), graph attention network (GAT), and equivariant graph convolutional network (EGCN). These GNNs reproduce ice thickness and velocity with better accuracy than the classic convolutional neural network (CNN) and multi-layer perception (MLP). In particular, GNNs successfully capture the ice mass loss and acceleration induced by higher basal melting rates in the PIG. When our GNN emulators are implemented on graphic processing units (GPUs), they show up to 50 times faster computational time than the CPU-based ISSM simulation. ",
    "url": "https://arxiv.org/abs/2402.05291",
    "authors": [
      "Maryam Rahnemoonfar",
      "Younghyun Koo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2402.05293",
    "title": "A comparative study on feature selection for a risk prediction model for  colorectal cancer",
    "abstract": "Background and objective Risk prediction models aim at identifying people at higher risk of developing a target disease. Feature selection is particularly important to improve the prediction model performance avoiding overfitting and to identify the leading cancer risk (and protective) factors. Assessing the stability of feature selection/ranking algorithms becomes an important issue when the aim is to analyze the features with more prediction power. Methods This work is focused on colorectal cancer, assessing several feature ranking algorithms in terms of performance for a set of risk prediction models (Neural Networks, Support Vector Machines (SVM), Logistic Regression, k-Nearest Neighbors and Boosted Trees). Additionally, their robustness is evaluated following a conventional approach with scalar stability metrics and a visual approach proposed in this work to study both similarity among feature ranking techniques as well as their individual stability. A comparative analysis is carried out between the most relevant features found out in this study and features provided by the experts according to the state-of-the-art knowledge. Results The two best performance results in terms of Area Under the ROC Curve (AUC) are achieved with a SVM classifier using the top-41 features selected by the SVM wrapper approach (AUC=0.693) and Logistic Regression with the top-40 features selected by the Pearson (AUC=0.689). Experiments showed that performing feature selection contributes to classification performance with a 3.9% and 1.9% improvement in AUC for the SVM and Logistic Regression classifier, respectively, with respect to the results using the full feature set. The visual approach proposed in this work allows to see that the Neural Network-based wrapper ranking is the most unstable while the Random Forest is the most stable. ",
    "url": "https://arxiv.org/abs/2402.05293",
    "authors": [
      "N. Cueto-L\u00f3pez",
      "M. T. Garc\u00eda-Ord\u00e1s",
      "V. D\u00e1vila-Batista",
      "V. Moreno",
      "N. Aragon\u00e9s",
      "R. Alaiz-Rodr\u00edguez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05294",
    "title": "Examining Modality Incongruity in Multimodal Federated Learning for  Medical Vision and Language-based Disease Detection",
    "abstract": "Multimodal Federated Learning (MMFL) utilizes multiple modalities in each client to build a more powerful Federated Learning (FL) model than its unimodal counterpart. However, the impact of missing modality in different clients, also called modality incongruity, has been greatly overlooked. This paper, for the first time, analyses the impact of modality incongruity and reveals its connection with data heterogeneity across participating clients. We particularly inspect whether incongruent MMFL with unimodal and multimodal clients is more beneficial than unimodal FL. Furthermore, we examine three potential routes of addressing this issue. Firstly, we study the effectiveness of various self-attention mechanisms towards incongruity-agnostic information fusion in MMFL. Secondly, we introduce a modality imputation network (MIN) pre-trained in a multimodal client for modality translation in unimodal clients and investigate its potential towards mitigating the missing modality problem. Thirdly, we assess the capability of client-level and server-level regularization techniques towards mitigating modality incongruity effects. Experiments are conducted under several MMFL settings on two publicly available real-world datasets, MIMIC-CXR and Open-I, with Chest X-Ray and radiology reports. ",
    "url": "https://arxiv.org/abs/2402.05294",
    "authors": [
      "Pramit Saha",
      "Divyanshu Mishra",
      "Felix Wagner",
      "Konstantinos Kamnitsas",
      "J. Alison Noble"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05302",
    "title": "Training DNN Models over Heterogeneous Clusters with Optimal Performance",
    "abstract": "Adjusting batch sizes and adaptively tuning other hyperparameters can significantly speed up deep neural network (DNN) training. Despite the ubiquity of heterogeneous clusters, existing adaptive DNN training techniques solely consider homogeneous environments. Optimizing distributed DNN training over heterogeneous clusters is technically challenging, and directly adapting existing techniques results in low utilization and poor performance. To solve this problem, we introduce Cannikin -- a novel data-parallel distributed training system. Cannikin achieves efficient and near-optimal performance by accurately modeling the optimal system performance and predicting adaptive batch size training metrics for DNNs in heterogeneous clusters. We implemented Cannikin in PyTorch and conducted experiments over 16 GPUs in Chameleon. Empirical results show that Cannikin reduces DNN training in heterogeneous clusters by up to $52\\%$ compared to the state-of-the-art adaptive training system and up to $85\\%$ compared to native PyTorch DistributedDataParallel. ",
    "url": "https://arxiv.org/abs/2402.05302",
    "authors": [
      "Chengyi Nie",
      "Jessica Maghakian",
      "Zhenhua Liu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2402.05305",
    "title": "Knowledge Distillation for Road Detection based on cross-model  Semi-Supervised Learning",
    "abstract": "The advancement of knowledge distillation has played a crucial role in enabling the transfer of knowledge from larger teacher models to smaller and more efficient student models, and is particularly beneficial for online and resource-constrained applications. The effectiveness of the student model heavily relies on the quality of the distilled knowledge received from the teacher. Given the accessibility of unlabelled remote sensing data, semi-supervised learning has become a prevalent strategy for enhancing model performance. However, relying solely on semi-supervised learning with smaller models may be insufficient due to their limited capacity for feature extraction. This limitation restricts their ability to exploit training data. To address this issue, we propose an integrated approach that combines knowledge distillation and semi-supervised learning methods. This hybrid approach leverages the robust capabilities of large models to effectively utilise large unlabelled data whilst subsequently providing the small student model with rich and informative features for enhancement. The proposed semi-supervised learning-based knowledge distillation (SSLKD) approach demonstrates a notable improvement in the performance of the student model, in the application of road segmentation, surpassing the effectiveness of traditional semi-supervised learning methods. ",
    "url": "https://arxiv.org/abs/2402.05305",
    "authors": [
      "Wanli Ma",
      "Oktay Karakus",
      "Paul L. Rosin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05307",
    "title": "Three Pathways to Neurosymbolic Reinforcement Learning with  Interpretable Model and Policy Networks",
    "abstract": "Neurosymbolic AI combines the interpretability, parsimony, and explicit reasoning of classical symbolic approaches with the statistical learning of data-driven neural approaches. Models and policies that are simultaneously differentiable and interpretable may be key enablers of this marriage. This paper demonstrates three pathways to implementing such models and policies in a real-world reinforcement learning setting. Specifically, we study a broad class of neural networks that build interpretable semantics directly into their architecture. We reveal and highlight both the potential and the essential difficulties of combining logic, simulation, and learning. One lesson is that learning benefits from continuity and differentiability, but classical logic is discrete and non-differentiable. The relaxation to real-valued, differentiable representations presents a trade-off; the more learnable, the less interpretable. Another lesson is that using logic in the context of a numerical simulation involves a non-trivial mapping from raw (e.g., real-valued time series) simulation data to logical predicates. Some open questions this note exposes include: What are the limits of rule-based controllers, and how learnable are they? Do the differentiable interpretable approaches discussed here scale to large, complex, uncertain systems? Can we truly achieve interpretability? We highlight these and other themes across the three approaches. ",
    "url": "https://arxiv.org/abs/2402.05307",
    "authors": [
      "Peter Graf",
      "Patrick Emami"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05309",
    "title": "Investigating Generalization Behaviours of Generative Flow Networks",
    "abstract": "Generative Flow Networks (GFlowNets, GFNs) are a generative framework for learning unnormalized probability mass functions over discrete spaces. Since their inception, GFlowNets have proven to be useful for learning generative models in applications where the majority of the discrete space is unvisited during training. This has inspired some to hypothesize that GFlowNets, when paired with deep neural networks (DNNs), have favourable generalization properties. In this work, we empirically verify some of the hypothesized mechanisms of generalization of GFlowNets. In particular, we find that the functions that GFlowNets learn to approximate have an implicit underlying structure which facilitate generalization. We also find that GFlowNets are sensitive to being trained offline and off-policy; however, the reward implicitly learned by GFlowNets is robust to changes in the training distribution. ",
    "url": "https://arxiv.org/abs/2402.05309",
    "authors": [
      "Lazar Atanackovic",
      "Emmanuel Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05312",
    "title": "SplitSim: Large-Scale Simulations for Evaluating Network Systems  Research",
    "abstract": "When physical testbeds are out of reach for evaluating a networked system, we frequently turn to simulation. In today's datacenter networks, bottlenecks are rarely at the network protocol level, but instead in end-host software or hardware components, thus current protocol-level simulations are inadequate means of evaluation. End-to-end simulations covering these components on the other hand, simply cannot achieve the required scale with feasible simulation performance and computational resources. In this paper, we address this with SplitSim, a simulation framework for end-to-end evaluation for large-scale network and distributed systems. To this end, SplitSim builds on prior work on modular end-to-end simulations and combines this with key elements to achieve scalability. First, mixed fidelity simulations judiciously reduce detail in simulation of parts of the system where this can be tolerated, while retaining the necessary detail elsewhere. SplitSim then parallelizes bottleneck simulators by decomposing them into multiple parallel but synchronized processes. Next, SplitSim provides a profiler to help users understand simulation performance and where the bottlenecks are, so users can adjust the configuration. Finally SplitSim provides abstractions to make it easy for users to build complex large-scale simulations. Our evaluation demonstrates SplitSim in multiple large-scale case studies. ",
    "url": "https://arxiv.org/abs/2402.05312",
    "authors": [
      "Hejing Li",
      "Praneeth Balasubramanian",
      "Marvin Meiers",
      "Jialin Li",
      "Antoine Kaufmann"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2402.05337",
    "title": "Investigating the Impact of SOLID Design Principles on Machine Learning  Code Understanding",
    "abstract": "[Context] Applying design principles has long been acknowledged as beneficial for understanding and maintainability in traditional software projects. These benefits may similarly hold for Machine Learning (ML) projects, which involve iterative experimentation with data, models, and algorithms. However, ML components are often developed by data scientists with diverse educational backgrounds, potentially resulting in code that doesn't adhere to software design best practices. [Goal] In order to better understand this phenomenon, we investigated the impact of the SOLID design principles on ML code understanding. [Method] We conducted a controlled experiment with three independent trials involving 100 data scientists. We restructured real industrial ML code that did not use SOLID principles. Within each trial, one group was presented with the original ML code, while the other was presented with ML code incorporating SOLID principles. Participants of both groups were asked to analyze the code and fill out a questionnaire that included both open-ended and closed-ended questions on their understanding. [Results] The study results provide statistically significant evidence that the adoption of the SOLID design principles can improve code understanding within the realm of ML projects. [Conclusion] We put forward that software engineering design principles should be spread within the data science community and considered for enhancing the maintainability of ML code. ",
    "url": "https://arxiv.org/abs/2402.05337",
    "authors": [
      "Raphael Cabral",
      "Marcos Kalinowski",
      "Maria Teresa Baldassarre",
      "Hugo Villamizar",
      "Tatiana Escovedo",
      "H\u00e9lio Lopes"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2402.05347",
    "title": "Robust Implicit Adaptive Low Rank Time-Stepping Methods for Matrix  Differential Equations",
    "abstract": "In this work, we develop implicit rank-adaptive schemes for time-dependent matrix differential equations. The dynamic low rank approximation (DLRA) is a well-known technique to capture the dynamic low rank structure based on Dirac-Frenkel time-dependent variational principle. In recent years, it has attracted a lot of attention due to its wide applicability. Our schemes are inspired by the three-step procedure used in the rank adaptive version of the unconventional robust integrator (the so called BUG integrator) for DLRA. First, a prediction (basis update) step is made computing the approximate column and row spaces at the next time level. Second, a Galerkin evolution step is invoked using a base implicit solve for the small core matrix. Finally, a truncation is made according to a prescribed error threshold. Since the DLRA is evolving the differential equation projected on to the tangent space of the low rank manifold, the error estimate of the BUG integrator contains the tangent projection (modeling) error which cannot be easily controlled by mesh refinement. This can cause convergence issue for equations with cross terms. To address this issue, we propose a simple modification, consisting of merging the row and column spaces from the explicit step truncation method together with the BUG spaces in the prediction step. In addition, we propose an adaptive strategy where the BUG spaces are only computed if the residual for the solution obtained from the prediction space by explicit step truncation method, is too large. We prove stability and estimate the local truncation error of the schemes under assumptions. We benchmark the schemes in several tests, such as anisotropic diffusion, solid body rotation and the combination of the two, to show robust convergence properties. ",
    "url": "https://arxiv.org/abs/2402.05347",
    "authors": [
      "Daniel Appel\u00f6",
      "Yingda Cheng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2402.05348",
    "title": "Are We Asking the Right Questions?: Designing for Community  Stakeholders' Interactions with AI in Policing",
    "abstract": "Research into recidivism risk prediction in the criminal legal system has garnered significant attention from HCI, critical algorithm studies, and the emerging field of human-AI decision-making. This study focuses on algorithmic crime mapping, a prevalent yet underexplored form of algorithmic decision support (ADS) in this context. We conducted experiments and follow-up interviews with 60 participants, including community members, technical experts, and law enforcement agents (LEAs), to explore how lived experiences, technical knowledge, and domain expertise shape interactions with the ADS, impacting human-AI decision-making. Surprisingly, we found that domain experts (LEAs) often exhibited anchoring bias, readily accepting and engaging with the first crime map presented to them. Conversely, community members and technical experts were more inclined to engage with the tool, adjust controls, and generate different maps. Our findings highlight that all three stakeholders were able to provide critical feedback regarding AI design and use - community members questioned the core motivation of the tool, technical experts drew attention to the elastic nature of data science practice, and LEAs suggested redesign pathways such that the tool could complement their domain expertise. ",
    "url": "https://arxiv.org/abs/2402.05348",
    "authors": [
      "MD Romael Haque",
      "Devansh Saxena",
      "Katy Weathington",
      "Joseph Chudzik",
      "Shion Guha"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2402.05349",
    "title": "Scrapping The Web For Early Wildfire Detection",
    "abstract": "Early wildfire detection is of the utmost importance to enable rapid response efforts, and thus minimize the negative impacts of wildfire spreads. To this end, we present \\Pyro, a web-scraping-based dataset composed of videos of wildfires from a network of cameras that were enhanced with manual bounding-box-level annotations. Our dataset was filtered based on a strategy to improve the quality and diversity of the data, reducing the final data to a set of 10,000 images. We ran experiments using a state-of-the-art object detection model and found out that the proposed dataset is challenging and its use in concordance with other public dataset helps to reach higher results overall. We will make our code and data publicly available. ",
    "url": "https://arxiv.org/abs/2402.05349",
    "authors": [
      "Mateo Lostanlen",
      "Felix Veith",
      "Cristian Buc",
      "Valentin Barriere"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05370",
    "title": "Attention as Robust Representation for Time Series Forecasting",
    "abstract": "Time series forecasting is essential for many practical applications, with the adoption of transformer-based models on the rise due to their impressive performance in NLP and CV. Transformers' key feature, the attention mechanism, dynamically fusing embeddings to enhance data representation, often relegating attention weights to a byproduct role. Yet, time series data, characterized by noise and non-stationarity, poses significant forecasting challenges. Our approach elevates attention weights as the primary representation for time series, capitalizing on the temporal relationships among data points to improve forecasting accuracy. Our study shows that an attention map, structured using global landmarks and local windows, acts as a robust kernel representation for data points, withstanding noise and shifts in distribution. Our method outperforms state-of-the-art models, reducing mean squared error (MSE) in multivariate time series forecasting by a notable 3.6% without altering the core neural network architecture. It serves as a versatile component that can readily replace recent patching based embedding schemes in transformer-based models, boosting their performance. ",
    "url": "https://arxiv.org/abs/2402.05370",
    "authors": [
      "PeiSong Niu",
      "Tian Zhou",
      "Xue Wang",
      "Liang Sun",
      "Rong Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05376",
    "title": "Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms  in Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks and exhibited impressive reasoning abilities by applying zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature of sentence prefixes during the pre-training phase, existing zero-shot CoT prompting methods that employ identical CoT prompting across all task instances may not be optimal. In this paper, we introduce a novel zero-shot prompting method that leverages evolutionary algorithms to generate diverse promptings for LLMs dynamically. Our approach involves initializing two CoT promptings, performing evolutionary operations based on LLMs to create a varied set, and utilizing the LLMs to select a suitable CoT prompting for a given problem. Additionally, a rewriting operation, guided by the selected CoT prompting, enhances the understanding of the LLMs about the problem. Extensive experiments conducted across ten reasoning datasets demonstrate the superior performance of our proposed method compared to current zero-shot CoT prompting methods on GPT-3.5-turbo and GPT-4. Moreover, in-depth analytical experiments underscore the adaptability and effectiveness of our method in various reasoning tasks. ",
    "url": "https://arxiv.org/abs/2402.05376",
    "authors": [
      "Feihu Jin",
      "Yifan Liu",
      "Ying Tan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.05388",
    "title": "Form-From: A Design Space of Social Media Systems",
    "abstract": "Social media systems are as varied as they are pervasive. They have been almost universally adopted for a broad range of purposes including work, entertainment, activism, and decision making. As a result, they have also diversified, with many distinct designs differing in content type, organization, delivery mechanism, access control, and many other dimensions. In this work, we aim to characterize and then distill a concise design space of social media systems that can help us understand similarities and differences, recognize potential consequences of design choice, and identify spaces for innovation. Our model, which we call Form-From, characterizes social media based on (1) the form of the content, either threaded or flat, and (2) from where or from whom one might receive content, ranging from spaces to networks to the commons. We derive Form-From inductively from a larger set of 62 dimensions organized into 10 categories. To demonstrate the utility of our model, we trace the history of social media systems as they traverse the Form-From space over time, and we identify common design patterns within cells of the model. ",
    "url": "https://arxiv.org/abs/2402.05388",
    "authors": [
      "Amy X. Zhang",
      "Michael S. Bernstein",
      "David R. Karger",
      "Mark S. Ackerman"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.05390",
    "title": "Integrated Sensing and Communication Driven Digital Twin for Intelligent  Machine Network",
    "abstract": "Intelligent machines (IMs), including industrial machines, unmanned aerial vehicles (UAVs), and unmanned vehicles, etc., could perform effective cooperation in complex environment when they form IM network. The efficient environment sensing and communication are crucial for IM network, enabling the real-time and stable control of IMs. With the emergence of integrated sensing and communication (ISAC) technology, IM network is empowered with ubiquitous sensing capabilities, which is helpful in improving the efficiency of communication and sensing with the mutual benefit of them. However, the massive amount of sensing information brings challenges for the processing, storage and application of sensing information. In this article, ISAC driven digital twin (DT) is proposed for IM network, and the architecture and enabling technologies are revealed. ISAC driven DT structurally stores the sensing information, which is further applied to optimize communication, networking and control schemes of IMs, promoting the widespread applications of IMs. ",
    "url": "https://arxiv.org/abs/2402.05390",
    "authors": [
      "Zhiqing Wei",
      "Yucong Du",
      "Qixun Zhang",
      "Wangjun Jiang",
      "Yanpeng Cui",
      "Zeyang Meng",
      "Huici Wu",
      "Zhiyong Feng"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2402.05391",
    "title": "Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey",
    "abstract": "Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss current challenges and identify emerging trends, such as progress in Large Language Modeling and Multi-modal Pre-training strategies. This survey aims to serve as a comprehensive reference for researchers already involved in or considering delving into KG and multi-modal learning research, offering insights into the evolving landscape of MMKG research and supporting future work. ",
    "url": "https://arxiv.org/abs/2402.05391",
    "authors": [
      "Zhuo Chen",
      "Yichi Zhang",
      "Yin Fang",
      "Yuxia Geng",
      "Lingbing Guo",
      "Xiang Chen",
      "Qian Li",
      "Wen Zhang",
      "Jiaoyan Chen",
      "Yushan Zhu",
      "Jiaqi Li",
      "Xiaoze Liu",
      "Jeff Z. Pan",
      "Ningyu Zhang",
      "Huajun Chen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05396",
    "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph  Representation Learning",
    "abstract": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and scalability. TASER adapts its mini-batch selection based on training dynamics and temporal neighbor selection based on the contextual, structural, and temporal properties of past interactions. To alleviate the bottleneck in mini-batch generation, TASER implements a pure GPU-based temporal neighbor finder and a dedicated GPU feature cache. We evaluate the performance of TASER using two state-of-the-art backbone TGNNs. On five popular datasets, TASER outperforms the corresponding baselines by an average of 2.3% in Mean Reciprocal Rank (MRR) while achieving an average of 5.1x speedup in training time. ",
    "url": "https://arxiv.org/abs/2402.05396",
    "authors": [
      "Gangda Deng",
      "Hongkuan Zhou",
      "Hanqing Zeng",
      "Yinglong Xia",
      "Christopher Leung",
      "Jianbo Li",
      "Rajgopal Kannan",
      "Viktor Prasanna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05402",
    "title": "A State-of-the-art Survey on Full-duplex Network Design",
    "abstract": "Full-duplex (FD) technology is gaining popularity for integration into a wide range of wireless networks due to its demonstrated potential in recent studies. In contrast to half-duplex (HD) technology, the implementation of FD in networks necessitates considering inter-node interference (INI) from various network perspectives. When deploying FD technology in networks, several critical factors must be taken into account. These include self-interference (SI) and the requisite SI cancellation (SIC) processes, as well as the selection of multiple user equipment (UE) per time slot. Additionally, inter-node interference (INI), including cross-link interference (CLI) and inter-cell interference (ICI), become crucial issues during concurrent uplink (UL) and downlink (DL) transmission and reception, similar to SI. Since most INI is challenging to eliminate, a comprehensive investigation that covers radio resource control (RRC), medium access control (MAC), and the physical layer (PHY) is essential in the context of FD network design, rather than focusing on individual network layers and types. This paper covers state-of-the-art studies, including protocols and documents from 3GPP for FD, MAC protocol, user scheduling, and CLI handling. The methods are also compared through a network-level system simulation based on 3D ray-tracing. ",
    "url": "https://arxiv.org/abs/2402.05402",
    "authors": [
      "Yonghwi Kim",
      "Hyung-Joo Moon",
      "Hanju Yoo",
      "Byoungnam",
      "Kai-Kit Wong",
      "Chan-Byoung Chae"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2402.05412",
    "title": "Multi-Network Constrained Operational Optimization in Community  Integrated Energy Systems: A Safe Reinforcement Learning Approach",
    "abstract": "The integrated community energy system (ICES) has emerged as a promising solution for enhancing the efficiency of the distribution system by effectively coordinating multiple energy sources. However, the operational optimization of ICES is hindered by the physical constraints of heterogeneous networks including electricity, natural gas, and heat. These challenges are difficult to address due to the non-linearity of network constraints and the high complexity of multi-network coordination. This paper, therefore, proposes a novel Safe Reinforcement Learning (SRL) algorithm to optimize the multi-network constrained operation problem of ICES. Firstly, a comprehensive ICES model is established considering integrated demand response (IDR), multiple energy devices, and network constraints. The multi-network operational optimization problem of ICES is then presented and reformulated as a constrained Markov Decision Process (C-MDP) accounting for violating physical network constraints. The proposed novel SRL algorithm, named Primal-Dual Twin Delayed Deep Deterministic Policy Gradient (PD-TD3), solves the C-MDP by employing a Lagrangian multiplier to penalize the multi-network constraint violation, ensuring that violations are within a tolerated range and avoid over-conservative strategy with a low reward at the same time. The proposed algorithm accurately estimates the cumulative reward and cost of the training process, thus achieving a fair balance between improving profits and reducing constraint violations in a privacy-protected environment with only partial information. A case study comparing the proposed algorithm with benchmark RL algorithms demonstrates the computational performance in increasing total profits and alleviating the network constraint violations. ",
    "url": "https://arxiv.org/abs/2402.05412",
    "authors": [
      "Ze Hu",
      "Ka Wing Chan",
      "Ziqing Zhu",
      "Xiang Wei",
      "Siqi Bu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2402.05423",
    "title": "MTSA-SNN: A Multi-modal Time Series Analysis Model Based on Spiking  Neural Network",
    "abstract": "Time series analysis and modelling constitute a crucial research area. Traditional artificial neural networks struggle with complex, non-stationary time series data due to high computational complexity, limited ability to capture temporal information, and difficulty in handling event-driven data. To address these challenges, we propose a Multi-modal Time Series Analysis Model Based on Spiking Neural Network (MTSA-SNN). The Pulse Encoder unifies the encoding of temporal images and sequential information in a common pulse-based representation. The Joint Learning Module employs a joint learning function and weight allocation mechanism to fuse information from multi-modal pulse signals complementary. Additionally, we incorporate wavelet transform operations to enhance the model's ability to analyze and evaluate temporal information. Experimental results demonstrate that our method achieved superior performance on three complex time-series tasks. This work provides an effective event-driven approach to overcome the challenges associated with analyzing intricate temporal information. Access to the source code is available at https://github.com/Chenngzz/MTSA-SNN}{https://github.com/Chenngzz/MTSA-SNN ",
    "url": "https://arxiv.org/abs/2402.05423",
    "authors": [
      "Chengzhi Liu",
      "Chong Zhong",
      "Mingyu Jin",
      "Zheng Tao",
      "Zihong Luo",
      "Chenghao Liu",
      "Shuliang Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05424",
    "title": "Neural Circuit Diagrams: Robust Diagrams for the Communication,  Implementation, and Analysis of Deep Learning Architectures",
    "abstract": "Diagrams matter. Unfortunately, the deep learning community has no standard method for diagramming architectures. The current combination of linear algebra notation and ad-hoc diagrams fails to offer the necessary precision to understand architectures in all their detail. However, this detail is critical for faithful implementation, mathematical analysis, further innovation, and ethical assurances. I present neural circuit diagrams, a graphical language tailored to the needs of communicating deep learning architectures. Neural circuit diagrams naturally keep track of the changing arrangement of data, precisely show how operations are broadcast over axes, and display the critical parallel behavior of linear operations. A lingering issue with existing diagramming methods is the inability to simultaneously express the detail of axes and the free arrangement of data, which neural circuit diagrams solve. Their compositional structure is analogous to code, creating a close correspondence between diagrams and implementation. In this work, I introduce neural circuit diagrams for an audience of machine learning researchers. After introducing neural circuit diagrams, I cover a host of architectures to show their utility and breed familiarity. This includes the transformer architecture, convolution (and its difficult-to-explain extensions), residual networks, the U-Net, and the vision transformer. I include a Jupyter notebook that provides evidence for the close correspondence between diagrams and code. Finally, I examine backpropagation using neural circuit diagrams. I show their utility in providing mathematical insight and analyzing algorithms' time and space complexities. ",
    "url": "https://arxiv.org/abs/2402.05424",
    "authors": [
      "Vincent Abbott"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05427",
    "title": "A Sampling Theory Perspective on Activations for Implicit Neural  Representations",
    "abstract": "Implicit Neural Representations (INRs) have gained popularity for encoding signals as compact, differentiable entities. While commonly using techniques like Fourier positional encodings or non-traditional activation functions (e.g., Gaussian, sinusoid, or wavelets) to capture high-frequency content, their properties lack exploration within a unified theoretical framework. Addressing this gap, we conduct a comprehensive analysis of these activations from a sampling theory perspective. Our investigation reveals that sinc activations, previously unused in conjunction with INRs, are theoretically optimal for signal encoding. Additionally, we establish a connection between dynamical systems and INRs, leveraging sampling theory to bridge these two paradigms. ",
    "url": "https://arxiv.org/abs/2402.05427",
    "authors": [
      "Hemanth Saratchandran",
      "Sameera Ramasinghe",
      "Violetta Shevchenko",
      "Alexander Long",
      "Simon Lucey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05428",
    "title": "Mixture Density Networks for Classification with an Application to  Product Bundling",
    "abstract": "While mixture density networks (MDNs) have been extensively used for regression tasks, they have not been used much for classification tasks. One reason for this is that the usability of MDNs for classification is not clear and straightforward. In this paper, we propose two MDN-based models for classification tasks. Both models fit mixtures of Gaussians to the the data and use the fitted distributions to classify a given sample by evaluating the learnt cumulative distribution function for the given input features. While the proposed MDN-based models perform slightly better than, or on par with, five baseline classification models on three publicly available datasets, the real utility of our models comes out through a real-world product bundling application. Specifically, we use our MDN-based models to learn the willingness-to-pay (WTP) distributions for two products from synthetic sales data of the individual products. The Gaussian mixture representation of the learnt WTP distributions is then exploited to obtain the WTP distribution of the bundle consisting of both the products. The proposed MDN-based models are able to approximate the true WTP distributions of both products and the bundle well. ",
    "url": "https://arxiv.org/abs/2402.05428",
    "authors": [
      "Narendhar Gugulothu",
      "Sanjay P. Bhat",
      "Tejas Bodas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05441",
    "title": "Spiking Neural Network Enhanced Hand Gesture Recognition Using Low-Cost  Single-photon Avalanche Diode Array",
    "abstract": "We present a compact spiking convolutional neural network (SCNN) and spiking multilayer perceptron (SMLP) to recognize ten different gestures in dark and bright light environments, using a $9.6 single-photon avalanche diode (SPAD) array. In our hand gesture recognition (HGR) system, photon intensity data was leveraged to train and test the network. A vanilla convolutional neural network (CNN) was also implemented to compare the performance of SCNN with the same network topologies and training strategies. Our SCNN was trained from scratch instead of being converted from the CNN. We tested the three models in dark and ambient light (AL)-corrupted environments. The results indicate that SCNN achieves comparable accuracy (90.8%) to CNN (92.9%) and exhibits lower floating operations with only 8 timesteps. SMLP also presents a trade-off between computational workload and accuracy. The code and collected datasets of this work are available at https://github.com/zzy666666zzy/TinyLiDAR_NET_SNN. ",
    "url": "https://arxiv.org/abs/2402.05441",
    "authors": [
      "Zhenya Zang",
      "Xingda Li",
      "David Day Uei Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2402.05453",
    "title": "Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss",
    "abstract": "Machine learning models are susceptible to membership inference attacks (MIAs), which aim to infer whether a sample is in the training set. Existing work utilizes gradient ascent to enlarge the loss variance of training data, alleviating the privacy risk. However, optimizing toward a reverse direction may cause the model parameters to oscillate near local minima, leading to instability and suboptimal performance. In this work, we propose a novel method -- Convex-Concave Loss, which enables a high variance of training loss distribution by gradient descent. Our method is motivated by the theoretical analysis that convex losses tend to decrease the loss variance during training. Thus, our key idea behind CCL is to reduce the convexity of loss functions with a concave term. Trained with CCL, neural networks produce losses with high variance for training data, reinforcing the defense against MIAs. Extensive experiments demonstrate the superiority of CCL, achieving state-of-the-art balance in the privacy-utility trade-off. ",
    "url": "https://arxiv.org/abs/2402.05453",
    "authors": [
      "Zhenlong Liu",
      "Lei Feng",
      "Huiping Zhuang",
      "Xiaofeng Cao",
      "Hongxin Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.05460",
    "title": "I-FENN with Temporal Convolutional Networks: expediting the load-history  analysis of non-local gradient damage propagation",
    "abstract": "In this paper, we demonstrate for the first time how the Integrated Finite Element Neural Network (I-FENN) framework, previously proposed by the authors, can efficiently simulate the entire loading history of non-local gradient damage propagation. To achieve this goal, we first adopt a Temporal Convolutional Network (TCN) as the neural network of choice to capture the history-dependent evolution of the non-local strain in a coarsely meshed domain. The quality of the network predictions governs the computational performance of I-FENN, and therefore we perform an extended investigation aimed at enhancing them. We explore a data-driven vs. physics-informed TCN setup to arrive at an optimum network training, evaluating the network based on a coherent set of relevant performance metrics. We address the crucial issue of training a physics-informed network with input data that span vastly different length scales by proposing a systematic way of input normalization and output un-normalization. We then integrate the trained TCN within the nonlinear iterative FEM solver and apply I-FENN to simulate the damage propagation analysis. I-FENN is always applied in mesh idealizations different from the one used for the TCN training, showcasing the framework's ability to be used at progressively refined mesh resolutions. We illustrate several cases that I-FENN completes the simulation using either a modified or a full Newton-Raphson scheme, and we showcase its computational savings compared to both the classical monolithic and staggered FEM solvers. We underline that we satisfy very strict convergence criteria for every increment across the entire simulation, providing clear evidence of the robustness and accuracy of I-FENN. All the code and data used in this work will be made publicly available upon publication of the article. ",
    "url": "https://arxiv.org/abs/2402.05460",
    "authors": [
      "Panos Pantidis",
      "Habiba Eldababy",
      "Diab Abueidda",
      "Mostafa E. Mobasher"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2402.05480",
    "title": "Kontextbasierte Aktivit\u00e4tserkennung -- Synergie von Mensch und Technik  in der Social Networked Industry",
    "abstract": "In a social networked industry, the focus is on collaboration between humans and technology. Communication is the basic prerequisite for synergetic collaboration between all players. It includes non-verbal as well as verbal interactions. To enable non-verbal interaction, machines must be able to detect and understand human movements. This article presents the ongoing fundamental research on the analysis of human movements using sensor-based activity recognition and identifies potential for a transfer to industrial applications. The focus is on the practical feasibility of activity recognition by adding further data streams such as the position data of logistical objects and tools, meaning the context in which a certain activity is carried out. -- In der Social Networked Industry steht die Zusammenarbeit von Mensch und Technik im Vordergrund. Grundvoraussetzung f\\\"ur eine synergetische Zusammenarbeit aller Akteure ist die Kommunikation, welche neben verbalen auch nonverbale Interaktionen umfasst. Um eine nonverbale Interaktion zu erm\\\"oglichen, m\\\"ussen Maschinen in der Lage sein, menschliche Bewegungen zu erfassen und zu verstehen. Dieser Beitrag stellt die laufende Grundlagenforschung zur Analyse menschlicher Bewegungen mittels sensorgest\\\"utzter Aktivit\\\"atserkennung vor und zeigt Ankn\\\"upfungspunkte f\\\"ur einen Transfer in industrielle Anwendungen. Im Fokus steht die Praxistauglichkeit der Aktivit\\\"atserkennung durch die Hinzunahme weiterer Datenstr\\\"ome wie beispielsweise den Positionsdaten logistischer Objekte und Hilfsmitteln, d. h. dem Kontext, in dem eine gewisse Aktivit\\\"at ausgef\\\"uhrt wird. ",
    "url": "https://arxiv.org/abs/2402.05480",
    "authors": [
      "Friedrich Niemann",
      "Christopher Reining"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2402.05489",
    "title": "Multispecies bird sound recognition using a fully convolutional neural  network",
    "abstract": "This study proposes a method based on fully convolutional neural networks (FCNs) to identify migratory birds from their songs, with the objective of recognizing which birds pass through certain areas and at what time. To determine the best FCN architecture, extensive experimentation was conducted through a grid search, exploring the optimal depth, width, and activation function of the network. The results showed that the optimal number of filters is 400 in the widest layer, with 4 convolutional blocks with maxpooling and an adaptive activation function. The proposed FCN offers a significant advantage over other techniques, as it can recognize the sound of a bird in audio of any length with an accuracy greater than 85%. Furthermore, due to its architecture, the network can detect more than one species from audio and can carry out near-real-time sound recognition. Additionally, the proposed method is lightweight, making it ideal for deployment and use in IoT devices. The study also presents a comparative analysis of the proposed method against other techniques, demonstrating an improvement of over 67% in the best-case scenario. These findings contribute to advancing the field of bird sound recognition and provide valuable insights into the practical application of FCNs in real-world scenarios. ",
    "url": "https://arxiv.org/abs/2402.05489",
    "authors": [
      "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s",
      "Sergio Rubio-Mart\u00edn",
      "Jos\u00e9 Alberto Ben\u00edtez-Andrades",
      "Hector Alaiz-Moret\u00f3n",
      "Isa\u00edas Garc\u00eda-Rodr\u00edguez"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2402.05491",
    "title": "Determining the severity of Parkinson's disease in patients using a  multi task neural network",
    "abstract": "Parkinson's disease is easy to diagnose when it is advanced, but it is very difficult to diagnose in its early stages. Early diagnosis is essential to be able to treat the symptoms. It impacts on daily activities and reduces the quality of life of both the patients and their families and it is also the second most prevalent neurodegenerative disorder after Alzheimer in people over the age of 60. Most current studies on the prediction of Parkinson's severity are carried out in advanced stages of the disease. In this work, the study analyzes a set of variables that can be easily extracted from voice analysis, making it a very non-intrusive technique. In this paper, a method based on different deep learning techniques is proposed with two purposes. On the one hand, to find out if a person has severe or non-severe Parkinson's disease, and on the other hand, to determine by means of regression techniques the degree of evolution of the disease in a given patient. The UPDRS (Unified Parkinson's Disease Rating Scale) has been used by taking into account both the motor and total labels, and the best results have been obtained using a mixed multi-layer perceptron (MLP) that classifies and regresses at the same time and the most important features of the data obtained are taken as input, using an autoencoder. A success rate of 99.15% has been achieved in the problem of predicting whether a person suffers from severe Parkinson's disease or non-severe Parkinson's disease. In the degree of disease involvement prediction problem case, a MSE (Mean Squared Error) of 0.15 has been obtained. Using a full deep learning pipeline for data preprocessing and classification has proven to be very promising in the field Parkinson's outperforming the state-of-the-art proposals. ",
    "url": "https://arxiv.org/abs/2402.05491",
    "authors": [
      "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s",
      "Jos\u00e9 Alberto Ben\u00edtez-Andrades",
      "Jose Aveleira-Mata",
      "Jos\u00e9-Manuel Alija-P\u00e9rez",
      "Carmen Benavides"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2402.05493",
    "title": "Investigating White-Box Attacks for On-Device Models",
    "abstract": "Numerous mobile apps have leveraged deep learning capabilities. However, on-device models are vulnerable to attacks as they can be easily extracted from their corresponding mobile apps. Existing on-device attacking approaches only generate black-box attacks, which are far less effective and efficient than white-box strategies. This is because mobile deep learning frameworks like TFLite do not support gradient computing, which is necessary for white-box attacking algorithms. Thus, we argue that existing findings may underestimate the harmfulness of on-device attacks. To this end, we conduct a study to answer this research question: Can on-device models be directly attacked via white-box strategies? We first systematically analyze the difficulties of transforming the on-device model to its debuggable version, and propose a Reverse Engineering framework for On-device Models (REOM), which automatically reverses the compiled on-device TFLite model to the debuggable model. Specifically, REOM first transforms compiled on-device models into Open Neural Network Exchange format, then removes the non-debuggable parts, and converts them to the debuggable DL models format that allows attackers to exploit in a white-box setting. Our experimental results show that our approach is effective in achieving automated transformation among 244 TFLite models. Compared with previous attacks using surrogate models, REOM enables attackers to achieve higher attack success rates with a hundred times smaller attack perturbations. In addition, because the ONNX platform has plenty of tools for model format exchanging, the proposed method based on the ONNX platform can be adapted to other model formats. Our findings emphasize the need for developers to carefully consider their model deployment strategies, and use white-box methods to evaluate the vulnerability of on-device models. ",
    "url": "https://arxiv.org/abs/2402.05493",
    "authors": [
      "Mingyi Zhou",
      "Xiang Gao",
      "Jing Wu",
      "Kui Liu",
      "Hailong Sun",
      "Li Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.05495",
    "title": "Heart disease risk prediction using deep learning techniques with  feature augmentation",
    "abstract": "Cardiovascular diseases state as one of the greatest risks of death for the general population. Late detection in heart diseases highly conditions the chances of survival for patients. Age, sex, cholesterol level, sugar level, heart rate, among other factors, are known to have an influence on life-threatening heart problems, but, due to the high amount of variables, it is often difficult for an expert to evaluate each patient taking this information into account. In this manuscript, the authors propose using deep learning methods, combined with feature augmentation techniques for evaluating whether patients are at risk of suffering cardiovascular disease. The results of the proposed methods outperform other state of the art methods by 4.4%, leading to a precision of a 90%, which presents a significant improvement, even more so when it comes to an affliction that affects a large population. ",
    "url": "https://arxiv.org/abs/2402.05495",
    "authors": [
      "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s",
      "Mart\u00edn Bay\u00f3n-Guti\u00e9rrez",
      "Carmen Benavides",
      "Jose Aveleira-Mata",
      "Jos\u00e9 Alberto Ben\u00edtez-Andrades"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05521",
    "title": "Linearizing Models for Efficient yet Robust Private Inference",
    "abstract": "The growing concern about data privacy has led to the development of private inference (PI) frameworks in client-server applications which protects both data privacy and model IP. However, the cryptographic primitives required yield significant latency overhead which limits its wide-spread application. At the same time, changing environments demand the PI service to be robust against various naturally occurring and gradient-based perturbations. Despite several works focused on the development of latency-efficient models suitable for PI, the impact of these models on robustness has remained unexplored. Towards this goal, this paper presents RLNet, a class of robust linearized networks that can yield latency improvement via reduction of high-latency ReLU operations while improving the model performance on both clean and corrupted images. In particular, RLNet models provide a \"triple win ticket\" of improved classification accuracy on clean, naturally perturbed, and gradient-based perturbed images using a shared-mask shared-weight architecture with over an order of magnitude fewer ReLUs than baseline models. To demonstrate the efficacy of RLNet, we perform extensive experiments with ResNet and WRN model variants on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. Our experimental evaluations show that RLNet can yield models with up to 11.14x fewer ReLUs, with accuracy close to the all-ReLU models, on clean, naturally perturbed, and gradient-based perturbed images. Compared with the SoTA non-robust linearized models at similar ReLU budgets, RLNet achieves an improvement in adversarial accuracy of up to ~47%, naturally perturbed accuracy up to ~16.4%, while improving clean image accuracy up to ~1.5%. ",
    "url": "https://arxiv.org/abs/2402.05521",
    "authors": [
      "Sreetama Sarkar",
      "Souvik Kundu",
      "Peter A. Beerel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.05532",
    "title": "NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of  Hand-Object Interaction",
    "abstract": "Modeling hand-object interactions is a fundamentally challenging task in 3D computer vision. Despite remarkable progress that has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between the hand and the object, and inaccurate hand-object pose estimation. To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos. In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between the hand and the object. (b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences. We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction that achieves photo-realistic novel view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy. ",
    "url": "https://arxiv.org/abs/2402.05532",
    "authors": [
      "Zhongqun Zhang",
      "Jifei Song",
      "Eduardo P\u00e9rez-Pellitero",
      "Yiren Zhou",
      "Hyung Jin Chang",
      "Ale\u0161 Leonardis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05534",
    "title": "Robust Parameter Fitting to Realistic Network Models via Iterative  Stochastic Approximation",
    "abstract": "Random graph models are widely used to understand network properties and graph algorithms. Key to such analyses are the different parameters of each model, which affect various network features, such as its size, clustering, or degree distribution. The exact effect of the parameters on these features is not well understood, mainly because we lack tools to thoroughly investigate this relation. Moreover, the parameters cannot be considered in isolation, as changing one affects multiple features. Existing approaches for finding the best model parameters of desired features, such as a grid search or estimating the parameter-feature relations, are not well suited, as they are inaccurate or computationally expensive. We introduce an efficient iterative fitting method, named ParFit, that finds parameters using only a few network samples, based on the Robbins-Monro algorithm. We test ParFit on three well-known graph models, namely Erd\\H{o}s-R\\'enyi, Chung-Lu, and geometric inhomogeneous random graphs, as well as on real-world networks, including web networks. We find that ParFit performs well in terms of quality and running time across most parameter configurations. ",
    "url": "https://arxiv.org/abs/2402.05534",
    "authors": [
      "Thomas Bl\u00e4sius",
      "Sarel Cohen",
      "Philipp Fischbeck",
      "Tobias Friedrich",
      "Martin S. Krejca"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2402.05536",
    "title": "Empowering machine learning models with contextual knowledge for  enhancing the detection of eating disorders in social media posts",
    "abstract": "Social networks are vital for information sharing, especially in the health sector for discussing diseases and treatments. These platforms, however, often feature posts as brief texts, posing challenges for Artificial Intelligence (AI) in understanding context. We introduce a novel hybrid approach combining community-maintained knowledge graphs (like Wikidata) with deep learning to enhance the categorization of social media posts. This method uses advanced entity recognizers and linkers (like Falcon 2.0) to connect short post entities to knowledge graphs. Knowledge graph embeddings (KGEs) and contextualized word embeddings (like BERT) are then employed to create rich, context-based representations of these posts. Our focus is on the health domain, particularly in identifying posts related to eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in early diagnosis. We tested our approach on a dataset of 2,000 tweets about eating disorders, finding that merging word embeddings with knowledge graph information enhances the predictive models' reliability. This methodology aims to assist health experts in spotting patterns indicative of mental disorders, thereby improving early detection and accurate diagnosis for personalized medicine. ",
    "url": "https://arxiv.org/abs/2402.05536",
    "authors": [
      "Jos\u00e9 Alberto Ben\u00edtez-Andrades",
      "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s",
      "Mayra Russo",
      "Ahmad Sakor",
      "Luis Daniel Fernandes Rotger",
      "Maria-Esther Vidal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.05540",
    "title": "Tightly Coupled Range Inertial Localization on a 3D Prior Map Based on  Sliding Window Factor Graph Optimization",
    "abstract": "This paper presents a range inertial localization algorithm for a 3D prior map. The proposed algorithm tightly couples scan-to-scan and scan-to-map point cloud registration factors along with IMU factors on a sliding window factor graph. The tight coupling of the scan-to-scan and scan-to-map registration factors enables a smooth fusion of sensor ego-motion estimation and map-based trajectory correction that results in robust tracking of the sensor pose under severe point cloud degeneration and defective regions in a map. We also propose an initial sensor state estimation algorithm that robustly estimates the gravity direction and IMU state and helps perform global localization in 3- or 4-DoF for system initialization without prior position information. Experimental results show that the proposed method outperforms existing state-of-the-art methods in extremely severe situations where the point cloud data becomes degenerate, there are momentary sensor interruptions, or the sensor moves along the map boundary or into unmapped regions. ",
    "url": "https://arxiv.org/abs/2402.05540",
    "authors": [
      "Kenji Koide",
      "Shuji Oishi",
      "Masashi Yokozuka",
      "Atsuhiko Banno"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2402.05541",
    "title": "Reinforcement Learning as a Catalyst for Robust and Fair Federated  Learning: Deciphering the Dynamics of Client Contributions",
    "abstract": "Recent advancements in federated learning (FL) have produced models that retain user privacy by training across multiple decentralized devices or systems holding local data samples. However, these strategies often neglect the inherent challenges of statistical heterogeneity and vulnerability to adversarial attacks, which can degrade model robustness and fairness. Personalized FL strategies offer some respite by adjusting models to fit individual client profiles, yet they tend to neglect server-side aggregation vulnerabilities. To address these issues, we propose Reinforcement Federated Learning (RFL), a novel framework that leverages deep reinforcement learning to adaptively optimize client contribution during aggregation, thereby enhancing both model robustness against malicious clients and fairness across participants under non-identically distributed settings. To achieve this goal, we propose a meticulous approach involving a Deep Deterministic Policy Gradient-based algorithm for continuous control of aggregation weights, an innovative client selection method based on model parameter distances, and a reward mechanism guided by validation set performance. Empirically, extensive experiments demonstrate that, in terms of robustness, RFL outperforms the state-of-the-art methods, while maintaining comparable levels of fairness, offering a promising solution to build resilient and fair federated systems. ",
    "url": "https://arxiv.org/abs/2402.05541",
    "authors": [
      "Jialuo He",
      "Wei Chen",
      "Xiaojin Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2402.05548",
    "title": "Efficient Expression Neutrality Estimation with Application to Face  Recognition Utility Prediction",
    "abstract": "The recognition performance of biometric systems strongly depends on the quality of the compared biometric samples. Motivated by the goal of establishing a common understanding of face image quality and enabling system interoperability, the committee draft of ISO/IEC 29794-5 introduces expression neutrality as one of many component quality elements affecting recognition performance. In this study, we train classifiers to assess facial expression neutrality using seven datasets. We conduct extensive performance benchmarking to evaluate their classification and face recognition utility prediction abilities. Our experiments reveal significant differences in how each classifier distinguishes \"neutral\" from \"non-neutral\" expressions. While Random Forests and AdaBoost classifiers are most suitable for distinguishing neutral from non-neutral facial expressions with high accuracy, they underperform compared to Support Vector Machines in predicting face recognition utility. ",
    "url": "https://arxiv.org/abs/2402.05548",
    "authors": [
      "Marcel Grimmer",
      "Raymond N. J. Veldhuis",
      "Christoph Busch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2402.05557",
    "title": "On Convolutional Vision Transformers for Yield Prediction",
    "abstract": "While a variety of methods offer good yield prediction on histogrammed remote sensing data, vision Transformers are only sparsely represented in the literature. The Convolution vision Transformer (CvT) is being tested to evaluate vision Transformers that are currently achieving state-of-the-art results in many other vision tasks. CvT combines some of the advantages of convolution with the advantages of dynamic attention and global context fusion of Transformers. It performs worse than widely tested methods such as XGBoost and CNNs, but shows that Transformers have potential to improve yield prediction. ",
    "url": "https://arxiv.org/abs/2402.05557",
    "authors": [
      "Alvin Inderka",
      "Florian Huber",
      "Volker Steinhage"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05563",
    "title": "Neural Multigrid Architectures",
    "abstract": "We propose a convenient matrix-free neural architecture for the multigrid method. The architecture is simple enough to be implemented in less than fifty lines of code, yet it encompasses a large number of distinct multigrid solvers. We argue that a fixed neural network without dense layers can not realize an efficient iterative method. Because of that, standard training protocols do not lead to competitive solvers. To overcome this difficulty, we use parameter sharing and serialization of layers. The resulting network can be trained on linear problems with thousands of unknowns and retains its efficiency on problems with millions of unknowns. From the point of view of numerical linear algebra network's training corresponds to finding optimal smoothers for the geometric multigrid method. We demonstrate our approach on a few second-order elliptic equations. For tested linear systems, we obtain from two to five times smaller spectral radius of the error propagation matrix compare to a basic linear multigrid with Jacobi smoother. ",
    "url": "https://arxiv.org/abs/2402.05563",
    "authors": [
      "Vladimir Fanaskov"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05567",
    "title": "Listening Between the Lines: Synthetic Speech Detection Disregarding  Verbal Content",
    "abstract": "Recent advancements in synthetic speech generation have led to the creation of forged audio data that are almost indistinguishable from real speech. This phenomenon poses a new challenge for the multimedia forensics community, as the misuse of synthetic media can potentially cause adverse consequences. Several methods have been proposed in the literature to mitigate potential risks and detect synthetic speech, mainly focusing on the analysis of the speech itself. However, recent studies have revealed that the most crucial frequency bands for detection lie in the highest ranges (above 6000 Hz), which do not include any speech content. In this work, we extensively explore this aspect and investigate whether synthetic speech detection can be performed by focusing only on the background component of the signal while disregarding its verbal content. Our findings indicate that the speech component is not the predominant factor in performing synthetic speech detection. These insights provide valuable guidance for the development of new synthetic speech detectors and their interpretability, together with some considerations on the existing work in the audio forensics field. ",
    "url": "https://arxiv.org/abs/2402.05567",
    "authors": [
      "Davide Salvi",
      "Temesgen Semu Balcha",
      "Paolo Bestagini",
      "Stefano Tubaro"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2402.05569",
    "title": "Hypergraph Node Classification With Graph Neural Networks",
    "abstract": "Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not only higher classification accuracy compared to state-of-the-art HyperGNNs, but also superior memory and runtime efficiency. ",
    "url": "https://arxiv.org/abs/2402.05569",
    "authors": [
      "Bohan Tang",
      "Zexi Liu",
      "Keyue Jiang",
      "Siheng Chen",
      "Xiaowen Dong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.05583",
    "title": "On the Spectral Efficiency of Indoor Wireless Networks with a Rotary  Uniform Linear Array",
    "abstract": "Contemporary wireless communication systems rely on Multi-User Multiple-Input Multiple-Output (MU-MIMO) techniques. In such systems, each Access Point (AP) is equipped with multiple antenna elements and serves multiple devices simultaneously. Notably, traditional systems utilize fixed antennas, i.e., antennas without any movement capabilities, while the idea of movable antennas has recently gained traction among the research community. By moving in a confined region, movable antennas are able to exploit the wireless channel variation in the continuous domain. This additional degree of freedom may enhance the quality of the wireless links, and consequently the communication performance. However, movable antennas for MU-MIMO proposed in the literature are complex, bulky, expensive and present a high power consumption. In this paper, we propose an alternative to such systems that has lower complexity and lower cost. More specifically, we propose the incorporation of rotation capabilities to APs equipped with Uniform Linear Arrays (ULAs) of antennas. We consider the uplink of an indoor scenario where the AP serves multiple devices simultaneously. The optimal rotation of the ULA is computed based on estimates of the positions of the active devices and aiming at maximizing the per-user mean achievable Spectral Efficiency (SE). Adopting a spatially correlated Rician channel model, our numerical results show that the rotation capabilities of the AP can bring substantial improvements in the SE in scenarios where the line-of-sight component of the channel vectors is strong. Moreover, our proposed system is robust against imperfect positioning estimates. ",
    "url": "https://arxiv.org/abs/2402.05583",
    "authors": [
      "Eduardo Noboro Tominaga",
      "Onel Luis Alcaraz L\u00f3pez",
      "Tommy Svensson",
      "Richard Demo Souza",
      "Hirley Alves"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2402.05584",
    "title": "AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods  in Low-resource Regimes",
    "abstract": "Text data augmentation is a complex problem due to the discrete nature of sentences. Although rule-based augmentation methods are widely adopted in real-world applications because of their simplicity, they suffer from potential semantic damage. Previous researchers have suggested easy data augmentation with soft labels (softEDA), employing label smoothing to mitigate this problem. However, finding the best factor for each model and dataset is challenging; therefore, using softEDA in real-world applications is still difficult. In this paper, we propose adapting AutoAugment to solve this problem. The experimental results suggest that the proposed method can boost existing augmentation methods and that rule-based methods can enhance cutting-edge pre-trained language models. We offer the source code. ",
    "url": "https://arxiv.org/abs/2402.05584",
    "authors": [
      "Juhwan Choi",
      "Kyohoon Jin",
      "Junho Lee",
      "Sangmin Song",
      "Youngbin Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05585",
    "title": "Neural functional a posteriori error estimates",
    "abstract": "We propose a new loss function for supervised and physics-informed training of neural networks and operators that incorporates a posteriori error estimate. More specifically, during the training stage, the neural network learns additional physical fields that lead to rigorous error majorants after a computationally cheap postprocessing stage. Theoretical results are based upon the theory of functional a posteriori error estimates, which allows for the systematic construction of such loss functions for a diverse class of practically relevant partial differential equations. From the numerical side, we demonstrate on a series of elliptic problems that for a variety of architectures and approaches (physics-informed neural networks, physics-informed neural operators, neural operators, and classical architectures in the regression and physics-informed settings), we can reach better or comparable accuracy and in addition to that cheaply recover high-quality upper bounds on the error after training. ",
    "url": "https://arxiv.org/abs/2402.05585",
    "authors": [
      "Vladimir Fanaskov",
      "Alexander Rudikov",
      "Ivan Oseledets"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2402.05591",
    "title": "SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels",
    "abstract": "Rule-based text data augmentation is widely used for NLP tasks due to its simplicity. However, this method can potentially damage the original meaning of the text, ultimately hurting the performance of the model. To overcome this limitation, we propose a straightforward technique for applying soft labels to augmented data. We conducted experiments across seven different classification tasks and empirically demonstrated the effectiveness of our proposed approach. We have publicly opened our source code for reproducibility. ",
    "url": "https://arxiv.org/abs/2402.05591",
    "authors": [
      "Juhwan Choi",
      "Kyohoon Jin",
      "Junho Lee",
      "Sangmin Song",
      "Youngbin Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05598",
    "title": "Neural operators meet conjugate gradients: The FCG-NO method for  efficient PDE solving",
    "abstract": "Deep learning solvers for partial differential equations typically have limited accuracy. We propose to overcome this problem by using them as preconditioners. More specifically, we apply discretization-invariant neural operators to learn preconditioners for the flexible conjugate gradient method (FCG). Architecture paired with novel loss function and training scheme allows for learning efficient preconditioners that can be used across different resolutions. On the theoretical side, FCG theory allows us to safely use nonlinear preconditioners that can be applied in $O(N)$ operations without constraining the form of the preconditioners matrix. To justify learning scheme components (the loss function and the way training data is collected) we perform several ablation studies. Numerical results indicate that our approach favorably compares with classical preconditioners and allows to reuse of preconditioners learned for lower resolution to the higher resolution data. ",
    "url": "https://arxiv.org/abs/2402.05598",
    "authors": [
      "Alexander Rudikov",
      "Vladimir Fanaskov",
      "Ekaterina Muravleva",
      "Yuri M. Laevsky",
      "Ivan Oseledets"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2402.05607",
    "title": "Internal Model Control design for systems learned by Control Affine  Neural Nonlinear Autoregressive Exogenous Models",
    "abstract": "This paper explores the use of Control Affine Neural Nonlinear AutoRegressive eXogenous (CA-NNARX) models for nonlinear system identification and model-based control design. The idea behind this architecture is to match the known control-affine structure of the system to achieve improved performance. Coherently with recent literature of neural networks for data-driven control, we first analyze the stability properties of CA-NNARX models, devising sufficient conditions for their incremental Input-to-State Stability ($\\delta$ISS) that can be enforced at the model training stage. The model's stability property is then leveraged to design a stable Internal Model Control (IMC) architecture. The proposed control scheme is tested on a simulated Quadruple Tank benchmark system to address the output reference tracking problem. The results achieved show that (i) the modeling accuracy of CA-NNARX is superior to the one of a standard NNARX model for given weight size and training epochs, and (ii) the proposed IMC law provides performance comparable to the ones of a standard Model Predictive Controller (MPC) at a significantly lower computational burden. ",
    "url": "https://arxiv.org/abs/2402.05607",
    "authors": [
      "Jing Xie",
      "Fabio Bonassi",
      "Riccardo Scattolini"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2402.05611",
    "title": "An Implementation for Dynamic Application Allocation in Shared Sensor  Networks",
    "abstract": "We present a system architecture implementation to perform dynamic application allocation in shared sensor networks, where highly integrated wireless sensor systems are used to support multiple applications. The architecture is based on a central controller that collects the received data from the sensor nodes, dynamically decides which applications must be simultaneously deployed in each node and, accordingly, over-the-air reprograms the sensor nodes. Waspmote devices are used as sensor nodes that communicate with the controller using ZigBee protocol. Experimental results show the viability of the proposal. ",
    "url": "https://arxiv.org/abs/2402.05611",
    "authors": [
      "Carmen Delgado",
      "Sergio Batista",
      "Mar\u00eda Canales",
      "Jos\u00e9 Ram\u00f3n G\u00e1llego",
      "Jorge Ort\u00edn",
      "Matteo Cesana"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2402.05624",
    "title": "Efficient Models for the Detection of Hate, Abuse and Profanity",
    "abstract": "Large Language Models (LLMs) are the cornerstone for many Natural Language Processing (NLP) tasks like sentiment analysis, document classification, named entity recognition, question answering, summarization, etc. LLMs are often trained on data which originates from the web. This data is prone to having content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP, please refer to the Appendix. Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content. For example, when the open-source RoBERTa model (specifically, the RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted to replace the mask token in `I do not know that Persian people are that MASK` it returns the word `stupid` with the highest score. This is unacceptable in civil discourse.The detection of Hate, Abuse and Profanity in text is a vital component of creating civil and unbiased LLMs, which is needed not only for English, but for all languages. In this article, we briefly describe the creation of HAP detectors and various ways of using them to make models civil and acceptable in the output they generate. ",
    "url": "https://arxiv.org/abs/2402.05624",
    "authors": [
      "Christoph Tillmann",
      "Aashka Trivedi",
      "Bishwaranjan Bhattacharjee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2402.05626",
    "title": "The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary  Points, Saddle Escaping, and Network Embedding",
    "abstract": "In this paper, we investigate the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss. As the activation function is non-differentiable, it is so far unclear how to completely characterize the stationary points. We propose the conditions for stationarity that apply to both non-differentiable and differentiable cases. Additionally, we show that, if a stationary point does not contain \"escape neurons\", which are defined with first-order conditions, then it must be a local minimum. Moreover, for the scalar-output case, the presence of an escape neuron guarantees that the stationary point is not a local minimum. Our results refine the description of the saddle-to-saddle training process starting from infinitesimally small (vanishing) initialization for shallow ReLU-like networks, linking saddle escaping directly with the parameter changes of escape neurons. Moreover, we are also able to fully discuss how network embedding, which is to instantiate a narrower network within a wider network, reshapes the stationary points. ",
    "url": "https://arxiv.org/abs/2402.05626",
    "authors": [
      "Zhengqing Wu",
      "Berfin Simsek",
      "Francois Ged"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05641",
    "title": "Boosting Dynamic TDD in Small Cell Networks by the Multiplicative Weight  Update Method",
    "abstract": "We leverage the Multiplicative Weight Update (MWU) method to develop a decentralized algorithm that significantly improves the performance of dynamic time division duplexing (D-TDD) in small cell networks. The proposed algorithm adaptively adjusts the time portion allocated to uplink (UL) and downlink (DL) transmissions at every node during each scheduled time slot, aligning the packet transmissions toward the most appropriate link directions according to the feedback of signal-to-interference ratio information. Our simulation results reveal that compared to the (conventional) fixed configuration of UL/DL transmission probabilities in D-TDD, incorporating MWU into D-TDD brings about a two-fold improvement of mean packet throughput in the DL and a three-fold improvement of the same performance metric in the UL, resulting in the D-TDD even outperforming Static-TDD in the UL. It also shows that the proposed scheme maintains a consistent performance gain in the presence of an ascending traffic load, validating its effectiveness in boosting the network performance. This work also demonstrates an approach that accounts for algorithmic considerations at the forefront when solving stochastic problems. ",
    "url": "https://arxiv.org/abs/2402.05641",
    "authors": [
      "Jiaqi Zhu",
      "Nikolaos Pappas",
      "Howard H. Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2402.05643",
    "title": "Improving Token-Based World Models with Parallel Observation Prediction",
    "abstract": "Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours. Our code is available at \\url{https://github.com/leor-c/REM}. ",
    "url": "https://arxiv.org/abs/2402.05643",
    "authors": [
      "Lior Cohen",
      "Kaixin Wang",
      "Bingyi Kang",
      "Shie Mannor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05644",
    "title": "FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single  Annotated Example Object",
    "abstract": "We present FuncGrasp, a framework that can infer dense yet reliable grasp configurations for unseen objects using one annotated object and single-view RGB-D observation via categorical priors. Unlike previous works that only transfer a set of grasp poses, FuncGrasp aims to transfer infinite configurations parameterized by an object-centric continuous grasp function across varying instances. To ease the transfer process, we propose Neural Surface Grasping Fields (NSGF), an effective neural representation defined on the surface to densely encode grasp configurations. Further, we exploit function-to-function transfer using sphere primitives to establish semantically meaningful categorical correspondences, which are learned in an unsupervised fashion without any expert knowledge. We showcase the effectiveness through extensive experiments in both simulators and the real world. Remarkably, our framework significantly outperforms several strong baseline methods in terms of density and reliability for generated grasps. ",
    "url": "https://arxiv.org/abs/2402.05644",
    "authors": [
      "Hanzhi Chen",
      "Binbin Xu",
      "Stefan Leutenegger"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05645",
    "title": "Investigating Reproducibility in Deep Learning-Based Software Fault  Prediction",
    "abstract": "Over the past few years, deep learning methods have been applied for a wide range of Software Engineering (SE) tasks, including in particular for the important task of automatically predicting and localizing faults in software. With the rapid adoption of increasingly complex machine learning models, it however becomes more and more difficult for scholars to reproduce the results that are reported in the literature. This is in particular the case when the applied deep learning models and the evaluation methodology are not properly documented and when code and data are not shared. Given some recent -- and very worrying -- findings regarding reproducibility and progress in other areas of applied machine learning, the goal of this work is to analyze to what extent the field of software engineering, in particular in the area of software fault prediction, is plagued by similar problems. We have therefore conducted a systematic review of the current literature and examined the level of reproducibility of 56 research articles that were published between 2019 and 2022 in top-tier software engineering conferences. Our analysis revealed that scholars are apparently largely aware of the reproducibility problem, and about two thirds of the papers provide code for their proposed deep learning models. However, it turned out that in the vast majority of cases, crucial elements for reproducibility are missing, such as the code of the compared baselines, code for data pre-processing or code for hyperparameter tuning. In these cases, it therefore remains challenging to exactly reproduce the results in the current research literature. Overall, our meta-analysis therefore calls for improved research practices to ensure the reproducibility of machine-learning based research. ",
    "url": "https://arxiv.org/abs/2402.05645",
    "authors": [
      "Adil Mukhtar",
      "Dietmar Jannach",
      "Franz Wotawa"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05660",
    "title": "Rethinking Propagation for Unsupervised Graph Domain Adaptation",
    "abstract": "Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a labelled source graph to an unlabelled target graph in order to address the distribution shifts between graph domains. Previous works have primarily focused on aligning data from the source and target graph in the representation space learned by graph neural networks (GNNs). However, the inherent generalization capability of GNNs has been largely overlooked. Motivated by our empirical analysis, we reevaluate the role of GNNs in graph domain adaptation and uncover the pivotal role of the propagation process in GNNs for adapting to different graph domains. We provide a comprehensive theoretical analysis of UGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter by removing propagation layers in source graph and stacking multiple propagation layers in target graph. Based on the empirical and theoretical analysis mentioned above, we propose a simple yet effective approach called A2GNN for graph domain adaptation. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed A2GNN framework. ",
    "url": "https://arxiv.org/abs/2402.05660",
    "authors": [
      "Meihan Liu",
      "Zeyu Fang",
      "Zhen Zhang",
      "Ming Gu",
      "Sheng Zhou",
      "Xin Wang",
      "Jiajun Bu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05663",
    "title": "Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave  Prediction",
    "abstract": "Accurate real-time traffic state forecasting plays a pivotal role in traffic control research. In particular, the CIRCLES consortium project necessitates predictive techniques to mitigate the impact of data source delays. After the success of the MegaVanderTest experiment, this paper aims at overcoming the current system limitations and develop a more suited approach to improve the real-time traffic state estimation for the next iterations of the experiment. In this paper, we introduce the SA-LSTM, a deep forecasting method integrating Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM) yielding state-of-the-art results in real-time mesoscale traffic forecasting. We extend this approach to multi-step forecasting with the n-step SA-LSTM, which outperforms traditional multi-step forecasting methods in the trade-off between short-term and long-term predictions, all while operating in real-time. ",
    "url": "https://arxiv.org/abs/2402.05663",
    "authors": [
      "Raphael Chekroun",
      "Han Wang",
      "Jonathan Lee",
      "Marin Toromanoff",
      "Sascha Hornauer",
      "Fabien Moutarde",
      "Maria Laura Delle Monache"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2402.05668",
    "title": "Comprehensive Assessment of Jailbreak Attacks Against LLMs",
    "abstract": "Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhibit robustness across different LLMs. Some jailbreak prompt datasets, available from the Internet, can also achieve high attack success rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the claims from many organizations regarding the coverage of violation categories in their policies, the attack success rates from these categories remain high, indicating the challenges of effectively aligning LLM policies and the ability to counter jailbreak attacks. We also discuss the trade-off between the attack performance and efficiency, as well as show that the transferability of the jailbreak prompts is still viable, becoming an option for black-box models. Overall, our research highlights the necessity of evaluating different jailbreak methods. We hope our study can provide insights for future research on jailbreak attacks and serve as a benchmark tool for evaluating them for practitioners. ",
    "url": "https://arxiv.org/abs/2402.05668",
    "authors": [
      "Junjie Chu",
      "Yugeng Liu",
      "Ziqing Yang",
      "Xinyue Shen",
      "Michael Backes",
      "Yang Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05675",
    "title": "Is Adversarial Training with Compressed Datasets Effective?",
    "abstract": "Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset. This synthetic dataset retains the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset. Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of adversarial robustness. In this work, we investigate the impact of adversarial robustness on models trained with compressed datasets. We show that the compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models. As a solution to improve dataset compression efficiency and adversarial robustness simultaneously, we propose a novel robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset. The proposed method is (1) obtained by one-time computation and is applicable for any model, (2) more effective than DC methods when applying adversarial training over MFC, (3) provably robust by minimizing the generalized adversarial loss. Additionally, empirical evaluation on three datasets shows that the proposed method is able to achieve better robustness and performance trade-off compared to DC methods such as distribution matching. ",
    "url": "https://arxiv.org/abs/2402.05675",
    "authors": [
      "Tong Chen",
      "Raghavendra Selvan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05687",
    "title": "Assessment of the Sparsity-Diversity Trade-offs in Active Users  Detection for mMTC",
    "abstract": "Wireless communication systems must increasingly support a multitude of machine-type communications (MTC) devices, thus calling for advanced strategies for active user detection (AUD). Recent literature has delved into AUD techniques based on compressed sensing, highlighting the critical role of signal sparsity. This study investigates the relationship between frequency diversity and signal sparsity in the AUD problem. Single-antenna users transmit multiple copies of non-orthogonal pilots across multiple frequency channels and the base station independently performs AUD in each channel using the orthogonal matching pursuit algorithm. We note that, although frequency diversity may improve the likelihood of successful reception of the signals, it may also damage the channel sparsity level, leading to important trade-offs. We show that a sparser signal significantly benefits AUD, surpassing the advantages brought by frequency diversity in scenarios with limited temporal resources and/or high numbers of receive antennas. Conversely, with longer pilots and fewer receive antennas, investing in frequency diversity becomes more impactful, resulting in a tenfold AUD performance improvement. ",
    "url": "https://arxiv.org/abs/2402.05687",
    "authors": [
      "Gabriel Martins de Jesus",
      "Onel Luis Alcaraz Lopez",
      "Richard Demo Souza",
      "Nurul Huda Mahmood",
      "Markku Juntti",
      "Matti Latva-Aho"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2402.05698",
    "title": "Evolving AI for Wellness: Dynamic and Personalized Real-time Loneliness  Detection Using Passive Sensing",
    "abstract": "Loneliness is a growing health concern as it can lead to depression and other associated mental health problems for people who experience feelings of loneliness over prolonged periods of time. Utilizing passive sensing methods that use smartphone and wearable sensor data to capture daily behavioural patterns offers a promising approach for the early detection of loneliness. Given the subjective nature of loneliness and people's varying daily routines, past detection approaches using machine learning models often face challenges with effectively detecting loneliness. This paper proposes a methodologically novel approach, particularly developing a loneliness detection system that evolves over time, adapts to new data, and provides real-time detection. Our study utilized the Globem dataset, a comprehensive collection of passive sensing data acquired over 10 weeks from university students. The base of our approach is the continuous identification and refinement of similar behavioural groups among students using an incremental clustering method. As we add new data, the model improves based on changing behavioural patterns. Parallel to this, we create and update classification models to detect loneliness among the evolving behavioural groups of students. When unique behavioural patterns are observed among student data, specialized classification models have been created. For predictions of loneliness, a collaborative effort between the generalized and specialized models is employed, treating each prediction as a vote. This study's findings reveal that group-based loneliness detection models exhibit superior performance compared to generic models, underscoring the necessity for more personalized approaches tailored to specific behavioural patterns. These results pave the way for future research, emphasizing the development of finely-tuned, individualized mental health interventions. ",
    "url": "https://arxiv.org/abs/2402.05698",
    "authors": [
      "Malik Muhammad Qirtas",
      "Evi Zafeiridi",
      "Eleanor Bantry White",
      "Dirk Pesch"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2402.05699",
    "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social  Scene Simulation",
    "abstract": "Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 benchmarks. As evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning with human values. Code is available at https://github.com/pangxianghe/MATRIX. ",
    "url": "https://arxiv.org/abs/2402.05699",
    "authors": [
      "Xianghe Pang",
      "Shuo Tang",
      "Rui Ye",
      "Yuxin Xiong",
      "Bolun Zhang",
      "Yanfeng Wang",
      "Siheng Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2402.05707",
    "title": "Neumann-Neumann type domain decomposition of elliptic problems on metric  graphs",
    "abstract": "In this paper we develop a Neumann-Neumann type domain decomposition method for elliptic problems on metric graphs. We describe the iteration in the continuous and discrete setting, reformulate the latter in the abstract additive Schwarz framework and prove its convergence to the finite element solution. We provide an implementation and test it on various examples of interest. ",
    "url": "https://arxiv.org/abs/2402.05707",
    "authors": [
      "Mih\u00e1ly Kov\u00e1cs",
      "Mih\u00e1ly Andr\u00e1s V\u00e1ghy"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2402.05713",
    "title": "Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on  Vulnerable Patient Populations",
    "abstract": "The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data. ",
    "url": "https://arxiv.org/abs/2402.05713",
    "authors": [
      "Pranav Kulkarni",
      "Andrew Chan",
      "Nithya Navarathna",
      "Skylar Chan",
      "Paul H. Yi",
      "Vishwa S. Parekh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05797",
    "title": "TaE: Task-aware Expandable Representation for Long Tail Class  Incremental Learning",
    "abstract": "Class-incremental learning (CIL) aims to train classifiers that learn new classes without forgetting old ones. Most CIL methods focus on balanced data distribution for each task, overlooking real-world long-tailed distributions. Therefore, Long-Tailed Class-Incremental Learning (LT-CIL) has been introduced, which trains on data where head classes have more samples than tail classes. Existing methods mainly focus on preserving representative samples from previous classes to combat catastrophic forgetting. Recently, dynamic network algorithms frozen old network structures and expanded new ones, achieving significant performance. However, with the introduction of the long-tail problem, merely extending task-specific parameters can lead to miscalibrated predictions, while expanding the entire model results in an explosion of memory size. To address these issues, we introduce a novel Task-aware Expandable (TaE) framework, dynamically allocating and updating task-specific trainable parameters to learn diverse representations from each incremental task, while resisting forgetting through the majority of frozen model parameters. To further encourage the class-specific feature representation, we develop a Centroid-Enhanced (CEd) method to guide the update of these task-aware parameters. This approach is designed to adaptively minimize the distances between intra-class features while simultaneously maximizing the distances between inter-class features across all seen classes. The utility of this centroid-enhanced method extends to all \"training from scratch\" CIL algorithms. Extensive experiments were conducted on CIFAR-100 and ImageNet100 under different settings, which demonstrates that TaE achieves state-of-the-art performance. ",
    "url": "https://arxiv.org/abs/2402.05797",
    "authors": [
      "Linjie Li",
      "S. Liu",
      "Zhenyu Wu",
      "JI yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05806",
    "title": "On Calibration and Conformal Prediction of Deep Classifiers",
    "abstract": "In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied with some confidence indication. Two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier's softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (CP): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced. While in practice both types of indications can be desired, so far the interplay between them has not been investigated. Toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent CP methods. We start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive CP methods: it frequently leads to larger prediction sets. Then, we turn to theoretically analyze this behavior. We reveal several mathematical properties of the procedure, according to which we provide a reasoning for the phenomenon. Our study suggests that it may be worthwhile to utilize adaptive CP methods, chosen for their enhanced conditional coverage, based on softmax values prior to (or after canceling) temperature scaling calibration. ",
    "url": "https://arxiv.org/abs/2402.05806",
    "authors": [
      "Lahav Dabah",
      "Tom Tirer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.05809",
    "title": "You Only Need One Color Space: An Efficient Network for Low-light Image  Enhancement",
    "abstract": "Low-Light Image Enhancement (LLIE) task tends to restore the details and visual information from corrupted low-light images. Most existing methods learn the mapping function between low/normal-light images by Deep Neural Networks (DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves amplifying image signals, and applying these color spaces to low-light images with a low signal-to-noise ratio can introduce sensitivity and instability into the enhancement process. Consequently, this results in the presence of color artifacts and brightness artifacts in the enhanced images. To alleviate this problem, we propose a novel trainable color space, named Horizontal/Vertical-Intensity (HVI). It not only decouples brightness and color from RGB channels to mitigate the instability during enhancement but also adapts to low-light images in different illumination ranges due to the trainable parameters. Further, we design a novel Color and Intensity Decoupling Network (CIDNet) with two branches dedicated to processing the decoupled image brightness and color in the HVI space. Within CIDNet, we introduce the Lightweight Cross-Attention (LCA) module to facilitate interaction between image structure and content information in both branches, while also suppressing noise in low-light images. Finally, we conducted 22 quantitative and qualitative experiments to show that the proposed CIDNet outperforms the state-of-the-art methods on 11 datasets. The code will be available at https://github.com/Fediory/HVI-CIDNet. ",
    "url": "https://arxiv.org/abs/2402.05809",
    "authors": [
      "Yixu Feng",
      "Cheng Zhang",
      "Pei Wang",
      "Peng Wu",
      "Qingsen Yan",
      "Yanning Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05823",
    "title": "FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework  for Robust Solar Power Forecasting",
    "abstract": "Accurate solar power forecasting is crucial to integrate photovoltaic plants into the electric grid, schedule and secure the power grid safety. This problem becomes more demanding for those newly installed solar plants which lack sufficient data. Current research predominantly relies on historical solar power data or numerical weather prediction in a single-modality format, ignoring the complementary information provided in different modalities. In this paper, we propose a multi-modality fusion framework to integrate historical power data, numerical weather prediction, and satellite images, significantly improving forecast performance. We introduce a vector quantized framework that aligns modalities with varying information densities, striking a balance between integrating sufficient information and averting model overfitting. Our framework demonstrates strong zero-shot forecasting capability, which is especially useful for those newly installed plants. Moreover, we collect and release a multi-modal solar power (MMSP) dataset from real-world plants to further promote the research of multi-modal solar forecasting algorithms. Our extensive experiments show that our model not only operates with robustness but also boosts accuracy in both zero-shot forecasting and scenarios rich with training data, surpassing leading models. We have incorporated it into our eForecaster platform and deployed it for more than 300 solar plants with a capacity of over 15GW. ",
    "url": "https://arxiv.org/abs/2402.05823",
    "authors": [
      "Ziqing Ma",
      "Wenwei Wang",
      "Tian Zhou",
      "Chao Chen",
      "Bingqing Peng",
      "Liang Sun",
      "Rong Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05840",
    "title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception  Uncertainties",
    "abstract": "The availability of a reliable map and a robust localization system is critical for the operation of an autonomous vehicle. In a modern system, both mapping and localization solutions generally employ convolutional neural network (CNN) --based perception. Hence, any algorithm should consider potential errors in perception for safe and robust functioning. In this work, we present uncertainty-aware panoptic Localization and Mapping (uPLAM), which employs perception uncertainty as a bridge to fuse the perception information with classical localization and mapping approaches. We introduce an uncertainty-based map aggregation technique to create a long-term panoptic bird's eye view map and provide an associated mapping uncertainty. Our map consists of surface semantics and landmarks with unique IDs. Moreover, we present panoptic uncertainty-aware particle filter-based localization. To this end, we propose an uncertainty-based particle importance weight calculation for the adaptive incorporation of perception information into localization. We also present a new dataset for evaluating long-term panoptic mapping and map-based localization. Extensive evaluations showcase that our proposed uncertainty incorporation leads to better mapping with reliable uncertainty estimates and accurate localization. We make our dataset and code available at: \\url{this http URL} ",
    "url": "https://arxiv.org/abs/2402.05840",
    "authors": [
      "Kshitij Sirohi",
      "Daniel B\u00fcscher",
      "Wolfram Burgard"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2402.05862",
    "title": "Let Your Graph Do the Talking: Encoding Structured Data for LLMs",
    "abstract": "How can we best encode structured data into sequential form for use in large language models (LLMs)? In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs. Our method, GraphToken, learns an encoding function to extend prompts with explicit structured information. Unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. Specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the GraphQA benchmark. ",
    "url": "https://arxiv.org/abs/2402.05862",
    "authors": [
      "Bryan Perozzi",
      "Bahare Fatemi",
      "Dustin Zelle",
      "Anton Tsitsulin",
      "Mehran Kazemi",
      "Rami Al-Rfou",
      "Jonathan Halcrow"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.05864",
    "title": "Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs",
    "abstract": "In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding ",
    "url": "https://arxiv.org/abs/2402.05864",
    "authors": [
      "Xuandong Zhao",
      "Lei Li",
      "Yu-Xiang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05872",
    "title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for  Semantic and Property Prediction",
    "abstract": "Robots must be able to understand their surroundings to perform complex tasks in challenging environments and many of these complex tasks require estimates of physical properties such as friction or weight. Estimating such properties using learning is challenging due to the large amounts of labelled data required for training and the difficulty of updating these learned models online at run time. To overcome these challenges, this paper introduces a novel, multi-modal approach for representing semantic predictions and physical property estimates jointly in a probabilistic manner. By using conjugate pairs, the proposed method enables closed-form Bayesian updates given visual and tactile measurements without requiring additional training data. The efficacy of the proposed algorithm is demonstrated through several hardware experiments. In particular, this paper illustrates that by conditioning semantic classifications on physical properties, the proposed method quantitatively outperforms state-of-the-art semantic classification methods that rely on vision alone. To further illustrate its utility, the proposed method is used in several applications including to represent affordance-based properties probabilistically and a challenging terrain traversal task using a legged robot. In the latter task, the proposed method represents the coefficient of friction of the terrain probabilistically, which enables the use of an on-line risk-aware planner that switches the legged robot from a dynamic gait to a static, stable gait when the expected value of the coefficient of friction falls below a given threshold. Videos of these case studies are presented in the multimedia attachment. The proposed framework includes an open-source C++ and ROS interface. ",
    "url": "https://arxiv.org/abs/2402.05872",
    "authors": [
      "Parker Ewen",
      "Hao Chen",
      "Yuzhen Chen",
      "Anran Li",
      "Anup Bagali",
      "Gitesh Gunjal",
      "Ram Vasudevan"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2402.05884",
    "title": "Cutsets and EF1 Fair Division of Graphs",
    "abstract": "In fair division of a connected graph $G = (V, E)$, each of $n$ agents receives a share of $G$'s vertex set $V$. These shares partition $V$, with each share required to induce a connected subgraph. Agents use their own valuation functions to determine the non-negative numerical values of the shares, which determine whether the allocation is fair in some specified sense. We introduce forbidden substructures called graph cutsets, which block divisions that are fair in the EF1 (envy-free up to one item) sense by cutting the graph into \"too many pieces\". Two parameters - gap and valence - determine blocked values of $n$. If $G$ guarantees connected EF1 allocations for $n$ agents with valuations that are CA (common and additive), then $G$ contains no elementary cutset of gap $k \\ge 2$ and valence in the interval $\\[n - k + 1, n - 1\\]$. If $G$ guarantees connected EF1 allocations for $n$ agents with valuations in the broader CM (common and monotone) class, then $G$ contains no cutset of gap $k \\ge 2$ and valence in the interval $\\[n - k + 1, n - 1\\]$. These results rule out the existence of connected EF1 allocations in a variety of situations. For some graphs $G$ we can, with help from some new positive results, pin down $G$'s spectrum - the list of exactly which values of $n$ do/do not guarantee connected EF1 allocations. Examples suggest a conjectured common spectral pattern for all graphs. Further, we show that it is NP-hard to determine whether a graph admits a cutset. We also provide an example of a (non-traceable) graph on eight vertices that has no cutsets of gap $\\ge 2$ at all, yet fails to guarantee connected EF1 allocations for three agents with CA preferences. ",
    "url": "https://arxiv.org/abs/2402.05884",
    "authors": [
      "Jiehua Chen",
      "William S. Zwicker"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2402.05885",
    "title": "EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance",
    "abstract": "The need to identify graphs having small structural distance from a query arises in biology, chemistry, recommender systems, and social network analysis. Among several methods to measure inter graph distance, Graph Edit Distance (GED) is preferred for its comprehensibility, yet hindered by the NP-hardness of its computation. State-of-the-art GED approximations predominantly employ neural methods, which, however, (i) lack an explanatory edit path corresponding to the approximated GED; (ii) require the NP-hard generation of ground-truth GEDs for training; and (iii) necessitate separate training on each dataset. In this paper, we propose an efficient algebraic unsuper vised method, EUGENE, that approximates GED and yields edit paths corresponding to the approx imated cost, while eliminating the need for ground truth generation and data-specific training. Extensive experimental evaluation demonstrates that the aforementioned benefits of EUGENE do not come at the cost of efficacy. Specifically, EUGENE consistently ranks among the most accurate methods across all of the benchmark datasets and outperforms majority of the neural approaches. ",
    "url": "https://arxiv.org/abs/2402.05885",
    "authors": [
      "Aditya Bommakanti",
      "Harshith Reddy Vonteri",
      "Sayan Ranu",
      "Panagiotis Karras"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05894",
    "title": "Large Language Model Meets Graph Neural Network in Knowledge  Distillation",
    "abstract": "Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the teacher LLM and the student GNN in latent space, employing a layer-adaptive contrastive learning strategy. Through extensive experiments on a variety of LLM and GNN models and multiple benchmark datasets, the proposed LinguGKD significantly boosts the student GNN's predictive accuracy and convergence rate, without the need of extra data or model parameters. Compared to teacher LLM, distilled GNN achieves superior inference speed equipped with much fewer computing and storage demands, when surpassing the teacher LLM's classification performance on some of benchmark datasets. ",
    "url": "https://arxiv.org/abs/2402.05894",
    "authors": [
      "Shengxiang Hu",
      "Guobing Zou",
      "Song Yang",
      "Bofeng Zhang",
      "Yixin Chen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05904",
    "title": "FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs",
    "abstract": "Our society is facing rampant misinformation harming public health and trust. To address the societal challenge, we introduce FACT-GPT, a system leveraging Large Language Models (LLMs) to automate the claim matching stage of fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims. Our evaluation shows that our specialized LLMs can match the accuracy of larger models in identifying related claims, closely mirroring human judgment. This research provides an automated solution for efficient claim matching, demonstrates the potential of LLMs in supporting fact-checkers, and offers valuable resources for further research in the field. ",
    "url": "https://arxiv.org/abs/2402.05904",
    "authors": [
      "Eun Cheol Choi",
      "Emilio Ferrara"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.05906",
    "title": "Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative  Markov Games",
    "abstract": "Classical multi-agent reinforcement learning (MARL) assumes risk neutrality and complete objectivity for agents. However, in settings where agents need to consider or model human economic or social preferences, a notion of risk must be incorporated into the RL optimization problem. This will be of greater importance in MARL where other human or non-human agents are involved, possibly with their own risk-sensitive policies. In this work, we consider risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT), a non-convex risk measure and a generalization of coherent measures of risk. CPT is capable of explaining loss aversion in humans and their tendency to overestimate/underestimate small/large probabilities. We propose a distributed sampling-based actor-critic (AC) algorithm with CPT risk for network aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC. Under a set of assumptions, we prove the convergence of the algorithm to a subjective notion of Markov perfect Nash equilibrium in NAMGs. The experimental results show that subjective CPT policies obtained by our algorithm can be different from the risk-neutral ones, and agents with a higher loss aversion are more inclined to socially isolate themselves in an NAMG. ",
    "url": "https://arxiv.org/abs/2402.05906",
    "authors": [
      "Hafez Ghaemi",
      "Hamed Kebriaei",
      "Alireza Ramezani Moghaddam",
      "Majid Nili Ahamdabadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2402.05934",
    "title": "Classifying Nodes in Graphs without GNNs",
    "abstract": "Graph neural networks (GNNs) are the dominant paradigm for classifying nodes in a graph, but they have several undesirable attributes stemming from their message passing architecture. Recently, distillation methods succeeded in eliminating the use of GNNs at test time but they still require them during training. We perform a careful analysis of the role that GNNs play in distillation methods. This analysis leads us to propose a fully GNN-free approach for node classification, not requiring them at train or test time. Our method consists of three key components: smoothness constraints, pseudo-labeling iterations and neighborhood-label histograms. Our final approach can match the state-of-the-art accuracy on standard popular benchmarks such as citation and co-purchase networks, without training a GNN. ",
    "url": "https://arxiv.org/abs/2402.05934",
    "authors": [
      "Daniel Winter",
      "Niv Cohen",
      "Yedid Hoshen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.05937",
    "title": "InstaGen: Enhancing Object Detection by Training on Synthetic Dataset",
    "abstract": "In this paper, we introduce a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pre-trained, generative diffusion model, to augment it with the ability of localising arbitrary instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. This enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer for object detection. We conduct thorough experiments to show that, object detector can be enhanced while training on the synthetic dataset from InstaGen, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to 5.2 AP) scenarios. ",
    "url": "https://arxiv.org/abs/2402.05937",
    "authors": [
      "Chengjian Feng",
      "Yujie Zhong",
      "Zequn Jie",
      "Weidi Xie",
      "Lin Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05155",
    "title": "Non-convergence to global minimizers for Adam and stochastic gradient  descent optimization and constructions of local minimizers in the training of  artificial neural networks",
    "abstract": "Stochastic gradient descent (SGD) optimization methods such as the plain vanilla SGD method and the popular Adam optimizer are nowadays the method of choice in the training of artificial neural networks (ANNs). Despite the remarkable success of SGD methods in the ANN training in numerical simulations, it remains in essentially all practical relevant scenarios an open problem to rigorously explain why SGD methods seem to succeed to train ANNs. In particular, in most practically relevant supervised learning problems, it seems that SGD methods do with high probability not converge to global minimizers in the optimization landscape of the ANN training problem. Nevertheless, it remains an open problem of research to disprove the convergence of SGD methods to global minimizers. In this work we solve this research problem in the situation of shallow ANNs with the rectified linear unit (ReLU) and related activations with the standard mean square error loss by disproving in the training of such ANNs that SGD methods (such as the plain vanilla SGD, the momentum SGD, the AdaGrad, the RMSprop, and the Adam optimizers) can find a global minimizer with high probability. Even stronger, we reveal in the training of such ANNs that SGD methods do with high probability fail to converge to global minimizers in the optimization landscape. The findings of this work do, however, not disprove that SGD methods succeed to train ANNs since they do not exclude the possibility that SGD methods find good local minimizers whose risk values are close to the risk values of the global minimizers. In this context, another key contribution of this work is to establish the existence of a hierarchical structure of local minimizers with distinct risk values in the optimization landscape of ANN training problems with ReLU and related activations. ",
    "url": "https://arxiv.org/abs/2402.05155",
    "authors": [
      "Arnulf Jentzen",
      "Adrian Riekert"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05271",
    "title": "Gradient descent induces alignment between weights and the empirical NTK  for deep non-linear networks",
    "abstract": "Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of simple statistics of the inputs and labels. Finally, we introduce a simple intervention to increase NFA correlation at any given layer, which dramatically improves the quality of features learned. ",
    "url": "https://arxiv.org/abs/2402.05271",
    "authors": [
      "Daniel Beaglehole",
      "Ioannis Mitliagkas",
      "Atish Agarwala"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05276",
    "title": "Spreading Information via Social Networks: An Irrelevance Result",
    "abstract": "An informed planner wishes to spread information among a group of agents in order to induce efficient coordination -- say the adoption of a new technology with positive externalities. The agents are connected via a social network. The planner informs a seed and then the information spreads via the network. While the structure of the network affects the rate of diffusion, we show that the rate of adoption is the same for all acyclic networks. ",
    "url": "https://arxiv.org/abs/2402.05276",
    "authors": [
      "Yu Awaya",
      "Vijay Krishna"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.05372",
    "title": "Reduced-order modeling of unsteady fluid flow using neural network  ensembles",
    "abstract": "The use of deep learning has become increasingly popular in reduced-order models (ROMs) to obtain low-dimensional representations of full-order models. Convolutional autoencoders (CAEs) are often used to this end as they are adept at handling data that are spatially distributed, including solutions to partial differential equations. When applied to unsteady physics problems, ROMs also require a model for time-series prediction of the low-dimensional latent variables. Long short-term memory (LSTM) networks, a type of recurrent neural network useful for modeling sequential data, are frequently employed in data-driven ROMs for autoregressive time-series prediction. When making predictions at unseen design points over long time horizons, error propagation is a frequently encountered issue, where errors made early on can compound over time and lead to large inaccuracies. In this work, we propose using bagging, a commonly used ensemble learning technique, to develop a fully data-driven ROM framework referred to as the CAE-eLSTM ROM that uses CAEs for spatial reconstruction of the full-order model and LSTM ensembles for time-series prediction. When applied to two unsteady fluid dynamics problems, our results show that the presented framework effectively reduces error propagation and leads to more accurate time-series prediction of latent variables at unseen points. ",
    "url": "https://arxiv.org/abs/2402.05372",
    "authors": [
      "Rakesh Halder",
      "Mohammadmehdi Ataei",
      "Hesam Salehipour",
      "Krzysztof Fidkowski",
      "Kevin Maki"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05378",
    "title": "Graph Neural Networks for Physical-Layer Security in Multi-User  Flexible-Duplex Networks",
    "abstract": "This paper explores Physical-Layer Security (PLS) in Flexible Duplex (FlexD) networks, considering scenarios involving eavesdroppers. Our investigation revolves around the intricacies of the sum secrecy rate maximization problem, particularly when faced with coordinated and distributed eavesdroppers employing a Minimum Mean Square Error (MMSE) receiver. Our contributions include an iterative classical optimization solution and an unsupervised learning strategy based on Graph Neural Networks (GNNs). To the best of our knowledge, this work marks the initial exploration of GNNs for PLS applications. Additionally, we extend the GNN approach to address the absence of eavesdroppers' channel knowledge. Extensive numerical simulations highlight FlexD's superiority over Half-Duplex (HD) communications and the GNN approach's superiority over the classical method in both performance and time complexity. ",
    "url": "https://arxiv.org/abs/2402.05378",
    "authors": [
      "Tharaka Perera",
      "Saman Atapattu",
      "Yuting Fang",
      "Jamie Evans"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05422",
    "title": "Memory-efficient deep end-to-end posterior network (DEEPEN) for inverse  problems",
    "abstract": "End-to-End (E2E) unrolled optimization frameworks show promise for Magnetic Resonance (MR) image recovery, but suffer from high memory usage during training. In addition, these deterministic approaches do not offer opportunities for sampling from the posterior distribution. In this paper, we introduce a memory-efficient approach for E2E learning of the posterior distribution. We represent this distribution as the combination of a data-consistency-induced likelihood term and an energy model for the prior, parameterized by a Convolutional Neural Network (CNN). The CNN weights are learned from training data in an E2E fashion using maximum likelihood optimization. The learned model enables the recovery of images from undersampled measurements using the Maximum A Posteriori (MAP) optimization. In addition, the posterior model can be sampled to derive uncertainty maps about the reconstruction. Experiments on parallel MR image reconstruction show that our approach performs comparable to the memory-intensive E2E unrolled algorithm, performs better than its memory-efficient counterpart, and can provide uncertainty maps. Our framework paves the way towards MR image reconstruction in 3D and higher dimensions ",
    "url": "https://arxiv.org/abs/2402.05422",
    "authors": [
      "Jyothi Rikhab Chand",
      "Mathews Jacob"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05482",
    "title": "A Non-Intrusive Neural Quality Assessment Model for Surface  Electromyography Signals",
    "abstract": "In practical scenarios involving the measurement of surface electromyography (sEMG) in muscles, particularly those areas near the heart, one of the primary sources of contamination is the presence of electrocardiogram (ECG) signals. To assess the quality of real-world sEMG data more effectively, this study proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG signals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an end-to-end training strategy. Our experimental framework utilizes real-world sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database, respectively. The experimental results demonstrate the superiority of QASE-net over the previous assessment model, exhibiting significantly reduced prediction errors and notably higher linear correlations with the ground truth. These findings show the potential of QASE-net to substantially enhance the reliability and precision of sEMG quality assessment in practical applications. ",
    "url": "https://arxiv.org/abs/2402.05482",
    "authors": [
      "Cho-Yuan Lee",
      "Kuan-Chen Wang",
      "Kai-Chun Liu",
      "Xugang Lu",
      "Ping-Chen Yeh",
      "Yu Tsao"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05568",
    "title": "Neural Graphics Primitives-based Deformable Image Registration for  On-the-fly Motion Extraction",
    "abstract": "Intra-fraction motion in radiotherapy is commonly modeled using deformable image registration (DIR). However, existing methods often struggle to balance speed and accuracy, limiting their applicability in clinical scenarios. This study introduces a novel approach that harnesses Neural Graphics Primitives (NGP) to optimize the displacement vector field (DVF). Our method leverages learned primitives, processed as splats, and interpolates within space using a shallow neural network. Uniquely, it enables self-supervised optimization at an ultra-fast speed, negating the need for pre-training on extensive datasets and allowing seamless adaptation to new cases. We validated this approach on the 4D-CT lung dataset DIR-lab, achieving a target registration error (TRE) of 1.15\\pm1.15 mm within a remarkable time of 1.77 seconds. Notably, our method also addresses the sliding boundary problem, a common challenge in conventional DIR methods. ",
    "url": "https://arxiv.org/abs/2402.05568",
    "authors": [
      "Xia Li",
      "Fabian Zhang",
      "Muheng Li",
      "Damien Weber",
      "Antony Lomax",
      "Joachim Buhmann",
      "Ye Zhang"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.05674",
    "title": "A High Dimensional Model for Adversarial Training: Geometry and  Trade-Offs",
    "abstract": "This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\\alpha = n / d$. We introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses. Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. In particular, we unveil the existence of directions which can be defended without penalising accuracy. Finally, we show the advantage of defending non-robust features during training, identifying a uniform protection as an inherently effective defence mechanism. ",
    "url": "https://arxiv.org/abs/2402.05674",
    "authors": [
      "Kasimir Tanner",
      "Matteo Vilucchio",
      "Bruno Loureiro",
      "Florent Krzakala"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05696",
    "title": "Fixed width treelike neural networks capacity analysis -- generic  activations",
    "abstract": "We consider the capacity of \\emph{treelike committee machines} (TCM) neural networks. Relying on Random Duality Theory (RDT), \\cite{Stojnictcmspnncaprdt23} recently introduced a generic framework for their capacity analysis. An upgrade based on the so-called \\emph{partially lifted} RDT (pl RDT) was then presented in \\cite{Stojnictcmspnncapliftedrdt23}. Both lines of work focused on the networks with the most typical, \\emph{sign}, activations. Here, on the other hand, we focus on networks with other, more general, types of activations and show that the frameworks of \\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} are sufficiently powerful to enable handling of such scenarios as well. In addition to the standard \\emph{linear} activations, we uncover that particularly convenient results can be obtained for two very commonly used activations, namely, the \\emph{quadratic} and \\emph{rectified linear unit (ReLU)} ones. In more concrete terms, for each of these activations, we obtain both the RDT and pl RDT based memory capacities upper bound characterization for \\emph{any} given (even) number of the hidden layer neurons, $d$. In the process, we also uncover the following two, rather remarkable, facts: 1) contrary to the common wisdom, both sets of results show that the bounding capacity decreases for large $d$ (the width of the hidden layer) while converging to a constant value; and 2) the maximum bounding capacity is achieved for the networks with precisely \\textbf{\\emph{two}} hidden layer neurons! Moreover, the large $d$ converging values are observed to be in excellent agrement with the statistical physics replica theory based predictions. ",
    "url": "https://arxiv.org/abs/2402.05696",
    "authors": [
      "Mihailo Stojnic"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2402.05718",
    "title": "REMEDI: Corrective Transformations for Improved Neural Entropy  Estimation",
    "abstract": "Information theoretic quantities play a central role in machine learning. The recent surge in the complexity of data and models has increased the demand for accurate estimation of these quantities. However, as the dimension grows the estimation presents significant challenges, with existing methods struggling already in relatively low dimensions. To address this issue, in this work, we introduce $\\texttt{REMEDI}$ for efficient and accurate estimation of differential entropy, a fundamental information theoretic quantity. The approach combines the minimization of the cross-entropy for simple, adaptive base models and the estimation of their deviation, in terms of the relative entropy, from the data density. Our approach demonstrates improvement across a broad spectrum of estimation tasks, encompassing entropy estimation on both synthetic and natural data. Further, we extend important theoretical consistency results to a more generalized setting required by our approach. We illustrate how the framework can be naturally extended to information theoretic supervised learning models, with a specific focus on the Information Bottleneck approach. It is demonstrated that the method delivers better accuracy compared to the existing methods in Information Bottleneck. In addition, we explore a natural connection between $\\texttt{REMEDI}$ and generative modeling using rejection sampling and Langevin dynamics. ",
    "url": "https://arxiv.org/abs/2402.05718",
    "authors": [
      "Viktor Nilsson",
      "Anirban Samaddar",
      "Sandeep Madireddy",
      "Pierre Nyquist"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05719",
    "title": "Exact capacity of the \\emph{wide} hidden layer treelike neural networks  with generic activations",
    "abstract": "Recent progress in studying \\emph{treelike committee machines} (TCM) neural networks (NN) in \\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23} showed that the Random Duality Theory (RDT) and its a \\emph{partially lifted}(pl RDT) variant are powerful tools that can be used for very precise networks capacity analysis. Here, we consider \\emph{wide} hidden layer networks and uncover that certain aspects of numerical difficulties faced in \\cite{Stojnictcmspnncapdiffactrdt23} miraculously disappear. In particular, we employ recently developed \\emph{fully lifted} (fl) RDT to characterize the \\emph{wide} ($d\\rightarrow \\infty$) TCM nets capacity. We obtain explicit, closed form, capacity characterizations for a very generic class of the hidden layer activations. While the utilized approach significantly lowers the amount of the needed numerical evaluations, the ultimate fl RDT usefulness and success still require a solid portion of the residual numerical work. To get the concrete capacity values, we take four very famous activations examples: \\emph{\\textbf{ReLU}}, \\textbf{\\emph{quadratic}}, \\textbf{\\emph{erf}}, and \\textbf{\\emph{tanh}}. After successfully conducting all the residual numerical work for all of them, we uncover that the whole lifting mechanism exhibits a remarkably rapid convergence with the relative improvements no better than $\\sim 0.1\\%$ happening already on the 3-rd level of lifting. As a convenient bonus, we also uncover that the capacity characterizations obtained on the first and second level of lifting precisely match those obtained through the statistical physics replica theory methods in \\cite{ZavPeh21} for the generic and in \\cite{BalMalZech19} for the ReLU activations. ",
    "url": "https://arxiv.org/abs/2402.05719",
    "authors": [
      "Mihailo Stojnic"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2402.05819",
    "title": "Integrating Self-supervised Speech Model with Pseudo Word-level Targets  from Visually-grounded Speech Model",
    "abstract": "Recent advances in self-supervised speech models have shown significant improvement in many downstream tasks. However, these models predominantly centered on frame-level training objectives, which can fall short in spoken language understanding tasks that require semantic comprehension. Existing works often rely on additional speech-text data as intermediate targets, which is costly in the real-world setting. To address this challenge, we propose Pseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-level targets into the training process, where the targets are derived from a visually-ground speech model, notably eliminating the need for speech-text paired data. Our experimental results on four spoken language understanding (SLU) benchmarks suggest the superiority of our model in capturing semantic information. ",
    "url": "https://arxiv.org/abs/2402.05819",
    "authors": [
      "Hung-Chieh Fang",
      "Nai-Xuan Ye",
      "Yi-Jen Shih",
      "Puyuan Peng",
      "Hsuan-Fu Wang",
      "Layne Berry",
      "Hung-yi Lee",
      "David Harwath"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05887",
    "title": "Sandwiched Compression: Repurposing Standard Codecs with Neural Network  Wrappers",
    "abstract": "We propose sandwiching standard image and video codecs between pre- and post-processing neural networks. The networks are jointly trained through a differentiable codec proxy to minimize a given rate-distortion loss. This sandwich architecture not only improves the standard codec's performance on its intended content, it can effectively adapt the codec to other types of image/video content and to other distortion measures. Essentially, the sandwich learns to transmit ``neural code images'' that optimize overall rate-distortion performance even when the overall problem is well outside the scope of the codec's design. Through a variety of examples, we apply the sandwich architecture to sources with different numbers of channels, higher resolution, higher dynamic range, and perceptual distortion measures. The results demonstrate substantial improvements (up to 9 dB gains or up to 30\\% bitrate reductions) compared to alternative adaptations. We derive VQ equivalents for the sandwich, establish optimality properties, and design differentiable codec proxies approximating current standard codecs. We further analyze model complexity, visual quality under perceptual metrics, as well as sandwich configurations that offer interesting potentials in image/video compression and streaming. ",
    "url": "https://arxiv.org/abs/2402.05887",
    "authors": [
      "Onur G. Guleryuz",
      "Philip A. Chou",
      "Berivan Isik",
      "Hugues Hoppe",
      "Danhang Tang",
      "Ruofei Du",
      "Jonathan Taylor",
      "Philip Davidson",
      "Sean Fanello"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2108.04567",
    "title": "Robust and Dexterous Dual-arm Tele-Cooperation using Adaptable Impedance  Control",
    "abstract": " Title: Robust and Dexterous Dual-arm Tele-Cooperation using Adaptable Impedance  Control ",
    "url": "https://arxiv.org/abs/2108.04567",
    "authors": [
      "Keyhan Kouhkiloui Babarahmati",
      "Mohammadreza Kasaei",
      "Carlo Tiseo",
      "Michael Mistry",
      "Sethu Vijayakumar"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2205.13147",
    "title": "Matryoshka Representation Learning",
    "abstract": " Comments: Edited related work to include intrinsic dimensionality works ",
    "url": "https://arxiv.org/abs/2205.13147",
    "authors": [
      "Aditya Kusupati",
      "Gantavya Bhatt",
      "Aniket Rege",
      "Matthew Wallingford",
      "Aditya Sinha",
      "Vivek Ramanujan",
      "William Howard-Snyder",
      "Kaifeng Chen",
      "Sham Kakade",
      "Prateek Jain",
      "Ali Farhadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2207.04950",
    "title": "Neural and spectral operator surrogates: unified construction and  expression rate bounds",
    "abstract": " Title: Neural and spectral operator surrogates: unified construction and  expression rate bounds ",
    "url": "https://arxiv.org/abs/2207.04950",
    "authors": [
      "Lukas Herrmann",
      "Christoph Schwab",
      "Jakob Zech"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2208.04508",
    "title": "Training Overparametrized Neural Networks in Sublinear Time",
    "abstract": " Title: Training Overparametrized Neural Networks in Sublinear Time ",
    "url": "https://arxiv.org/abs/2208.04508",
    "authors": [
      "Yichuan Deng",
      "Hang Hu",
      "Zhao Song",
      "Omri Weinstein",
      "Danyang Zhuo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2210.07996",
    "title": "Degeneracy is OK: Logarithmic Regret for Network Revenue Management with  Indiscrete Distributions",
    "abstract": " Title: Degeneracy is OK: Logarithmic Regret for Network Revenue Management with  Indiscrete Distributions ",
    "url": "https://arxiv.org/abs/2210.07996",
    "authors": [
      "Jiashuo Jiang",
      "Will Ma",
      "Jiawei Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2212.00773",
    "title": "FakeOut: Leveraging Out-of-domain Self-supervision for Multi-modal Video  Deepfake Detection",
    "abstract": " Title: FakeOut: Leveraging Out-of-domain Self-supervision for Multi-modal Video  Deepfake Detection ",
    "url": "https://arxiv.org/abs/2212.00773",
    "authors": [
      "Gil Knafo",
      "Ohad Fried"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2301.11048",
    "title": "Decidability of well quasi-order and atomicity for equivalence relations  under embedding orderings",
    "abstract": " Title: Decidability of well quasi-order and atomicity for equivalence relations  under embedding orderings ",
    "url": "https://arxiv.org/abs/2301.11048",
    "authors": [
      "V. Ironmonger",
      "N. Ruskuc"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2302.05652",
    "title": "On Structural and Spectral Properties of Distance Magic Graphs",
    "abstract": " Title: On Structural and Spectral Properties of Distance Magic Graphs ",
    "url": "https://arxiv.org/abs/2302.05652",
    "authors": [
      "Himadri Mukherjee",
      "Ravindra Pawar",
      "Tarkeshwar Singh"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2303.01213",
    "title": "DSD$^2$: Can We Dodge Sparse Double Descent and Compress the Neural  Network Worry-Free?",
    "abstract": " Title: DSD$^2$: Can We Dodge Sparse Double Descent and Compress the Neural  Network Worry-Free? ",
    "url": "https://arxiv.org/abs/2303.01213",
    "authors": [
      "Victor Qu\u00e9tu",
      "Enzo Tartaglione"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.10555",
    "title": "LiDAR Spoofing Meets the New-Gen: Capability Improvements, Broken  Assumptions, and New Attack Strategies",
    "abstract": " Comments: The first 3 authors are co-first ",
    "url": "https://arxiv.org/abs/2303.10555",
    "authors": [
      "Takami Sato",
      "Yuki Hayakawa",
      "Ryo Suzuki",
      "Yohsuke Shiiki",
      "Kentaro Yoshioka",
      "Qi Alfred Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.14535",
    "title": "EfficientAD: Accurate Visual Anomaly Detection at Millisecond-Level  Latencies",
    "abstract": " Comments: Accepted as Oral to WACV 2024 ",
    "url": "https://arxiv.org/abs/2303.14535",
    "authors": [
      "Kilian Batzner",
      "Lars Heckler",
      "Rebecca K\u00f6nig"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2304.13105",
    "title": "Attention-Enhanced Deep Learning for Device-Free Through-the-Wall  Presence Detection Using Indoor WiFi Systems",
    "abstract": " Comments: Accepted by IEEE Sensors Journal ",
    "url": "https://arxiv.org/abs/2304.13105",
    "authors": [
      "Li-Hsiang Shen",
      "An-Hung Hsiao",
      "Kuan-I Lu",
      "Kai-Ten Feng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.05772",
    "title": "Spiking Neural Networks in the Alexiewicz Topology: A New Perspective on  Analysis and Error Bounds",
    "abstract": " Title: Spiking Neural Networks in the Alexiewicz Topology: A New Perspective on  Analysis and Error Bounds ",
    "url": "https://arxiv.org/abs/2305.05772",
    "authors": [
      "Bernhard A. Moser",
      "Michael Lunglmayr"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Discrete Mathematics (cs.DM)",
      "Signal Processing (eess.SP)",
      "Metric Geometry (math.MG)"
    ]
  },
  {
    "id": "arXiv:2305.06344",
    "title": "Orthogonal Transforms in Neural Networks Amount to Effective  Regularization",
    "abstract": " Title: Orthogonal Transforms in Neural Networks Amount to Effective  Regularization ",
    "url": "https://arxiv.org/abs/2305.06344",
    "authors": [
      "Krzysztof Zaj\u0105c",
      "Wojciech Sopot",
      "Pawe\u0142 Wachel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2305.08012",
    "title": "Quantization in Spiking Neural Networks",
    "abstract": " Comments: arXiv admin note: text overlap with arXiv:2305.05772 ",
    "url": "https://arxiv.org/abs/2305.08012",
    "authors": [
      "Bernhard A. Moser",
      "Michael Lunglmayr"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Discrete Mathematics (cs.DM)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2305.14405",
    "title": "NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix  Operations for Efficient Inference",
    "abstract": " Comments: 11 pages, 6figures, Submitted to 41st International Conference on Machine Learning ",
    "url": "https://arxiv.org/abs/2305.14405",
    "authors": [
      "Ruiqi Sun",
      "Siwei Ye",
      "Jie Zhao",
      "Xin He",
      "Yiran Li",
      "An Zou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2305.19510",
    "title": "Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape",
    "abstract": " Comments: 40 pages ",
    "url": "https://arxiv.org/abs/2305.19510",
    "authors": [
      "Kedar Karhadkar",
      "Michael Murray",
      "Hanna Tseran",
      "Guido Mont\u00fafar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Combinatorics (math.CO)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.01710",
    "title": "A Data-Driven Measure of Relative Uncertainty for Misclassification  Detection",
    "abstract": " Comments: Accepted in ICLR2024 ",
    "url": "https://arxiv.org/abs/2306.01710",
    "authors": [
      "Eduardo Dadalto",
      "Marco Romanelli",
      "Georg Pichler",
      "Pablo Piantanida"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.05957",
    "title": "DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic  Latent Particles",
    "abstract": " Comments: TMLR 2024. Project site: this https URL ",
    "url": "https://arxiv.org/abs/2306.05957",
    "authors": [
      "Tal Daniel",
      "Aviv Tamar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.13149",
    "title": "\"Filling the Blanks'': Identifying Micro-activities that Compose Complex  Human Activities of Daily Living",
    "abstract": " Comments: 23 pages, 4 tables, 7 figures ",
    "url": "https://arxiv.org/abs/2306.13149",
    "authors": [
      "Soumyajit Chatterjee",
      "Bivas Mitra",
      "Sandip Chakraborty"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.14263",
    "title": "Revolutionizing Cyber Threat Detection with Large Language Models: A  privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices",
    "abstract": " Comments: This paper has been accepted for publication in IEEE Access: this http URL ",
    "url": "https://arxiv.org/abs/2306.14263",
    "authors": [
      "Mohamed Amine Ferrag",
      "Mthandazo Ndhlovu",
      "Norbert Tihanyi",
      "Lucas C. Cordeiro",
      "Merouane Debbah",
      "Thierry Lestable",
      "Narinderjit Singh Thandi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2307.04804",
    "title": "S2vNTM: Semi-supervised vMF Neural Topic Modeling",
    "abstract": " Comments: 17 pages, 9 figures, ICLR Workshop 2023. arXiv admin note: text overlap with arXiv:2307.01226 ",
    "url": "https://arxiv.org/abs/2307.04804",
    "authors": [
      "Weijie Xu",
      "Jay Desai",
      "Srinivasan Sengamedu",
      "Xiaoyu Jiang",
      "Francis Iannacci"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.03970",
    "title": "Dependent Cluster Mapping (DCMAP): Optimal clustering of directed  acyclic graphs for statistical inference",
    "abstract": " Title: Dependent Cluster Mapping (DCMAP): Optimal clustering of directed  acyclic graphs for statistical inference ",
    "url": "https://arxiv.org/abs/2308.03970",
    "authors": [
      "Paul Pao-Yen Wu",
      "Fabrizio Ruggeri",
      "Kerrie Mengersen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2308.07688",
    "title": "Enhancing Network Initialization for Medical AI Models Using  Large-Scale, Unlabeled Natural Images",
    "abstract": " Comments: Published in European Radiology Experimental ",
    "url": "https://arxiv.org/abs/2308.07688",
    "authors": [
      "Soroosh Tayebi Arasteh",
      "Leo Misera",
      "Jakob Nikolas Kather",
      "Daniel Truhn",
      "Sven Nebelung"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.10966",
    "title": "Deadlock-free, Safe, and Decentralized Multi-Robot Navigation in Social  Mini-Games via Discrete-Time Control Barrier Functions",
    "abstract": " Comments: major update since last revision ",
    "url": "https://arxiv.org/abs/2308.10966",
    "authors": [
      "Rohan Chandra",
      "Vrushabh Zinage",
      "Efstathios Bakolas",
      "Peter Stone",
      "Joydeep Biswas"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2308.13401",
    "title": "Min-$k$-planar Drawings of Graphs",
    "abstract": " Comments: Appears in the Proceedings of the 31st International Symposium on Graph Drawing and Network Visualization (GD 2023) ",
    "url": "https://arxiv.org/abs/2308.13401",
    "authors": [
      "Carla Binucci",
      "Aaron B\u00fcngener",
      "Giuseppe Di Battista",
      "Walter Didimo",
      "Vida Dujmovi\u0107",
      "Seok-Hee Hong",
      "Michael Kaufmann",
      "Giuseppe Liotta",
      "Pat Morin",
      "Alessandra Tappini"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2308.16264",
    "title": "Resource Allocation for Rate and Fidelity Maximization in Quantum  Networks",
    "abstract": " Comments: 18 pages, 8 figures, 3 appendices ",
    "url": "https://arxiv.org/abs/2308.16264",
    "authors": [
      "Shahrooz Pouryousef",
      "Hassan Shapourian",
      "Alireza Shabani",
      "Ramana Kompella",
      "Don Towsley"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2309.05102",
    "title": "Is Learning in Biological Neural Networks based on Stochastic Gradient  Descent? An analysis using stochastic processes",
    "abstract": " Title: Is Learning in Biological Neural Networks based on Stochastic Gradient  Descent? An analysis using stochastic processes ",
    "url": "https://arxiv.org/abs/2309.05102",
    "authors": [
      "S\u00f6ren Christensen",
      "Jan Kallsen"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2309.09323",
    "title": "Answering Causal Queries at Layer 3 with DiscoSCMs-Embracing  Heterogeneity",
    "abstract": " Title: Answering Causal Queries at Layer 3 with DiscoSCMs-Embracing  Heterogeneity ",
    "url": "https://arxiv.org/abs/2309.09323",
    "authors": [
      "Heyang Gong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2309.14891",
    "title": "RE-SORT: Removing Spurious Correlation in Multilevel Interaction for CTR  Prediction",
    "abstract": " Comments: 15 pages, 7 figures ",
    "url": "https://arxiv.org/abs/2309.14891",
    "authors": [
      "Songli Wu",
      "Liang Du",
      "Jia-Qi Yang",
      "Yuai Wang",
      "De-Chuan Zhan",
      "Shuang Zhao",
      "Zixun Sun"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2310.03821",
    "title": "WLST: Weak Labels Guided Self-training for Weakly-supervised Domain  Adaptation on 3D Object Detection",
    "abstract": " Comments: Accepted to ICRA 2024. Code is available at this https URL ",
    "url": "https://arxiv.org/abs/2310.03821",
    "authors": [
      "Tsung-Lin Tsou",
      "Tsung-Han Wu",
      "Winston H. Hsu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2310.04521",
    "title": "Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie  Algebras",
    "abstract": " Title: Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie  Algebras ",
    "url": "https://arxiv.org/abs/2310.04521",
    "authors": [
      "Tzu-Yuan Lin",
      "Minghan Zhu",
      "Maani Ghaffari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.06312",
    "title": "Discovering Mixtures of Structural Causal Models from Time Series Data",
    "abstract": " Title: Discovering Mixtures of Structural Causal Models from Time Series Data ",
    "url": "https://arxiv.org/abs/2310.06312",
    "authors": [
      "Sumanth Varambally",
      "Yi-An Ma",
      "Rose Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2310.13367",
    "title": "VFedMH: Vertical Federated Learning for Training Multiple Heterogeneous  Models",
    "abstract": " Title: VFedMH: Vertical Federated Learning for Training Multiple Heterogeneous  Models ",
    "url": "https://arxiv.org/abs/2310.13367",
    "authors": [
      "Shuo Wang",
      "Keke Gai",
      "Jing Yu",
      "Liehuang Zhu",
      "Kim-Kwang Raymond Choo",
      "Bin Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2310.14450",
    "title": "TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings",
    "abstract": " Comments: Accepted to EMNLP 2023; Updated citations ",
    "url": "https://arxiv.org/abs/2310.14450",
    "authors": [
      "Hans W. A. Hanley",
      "Zakir Durumeric"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.12944",
    "title": "DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution  Mechanism for 5G and Beyond Solar Small Cell Networks",
    "abstract": " Title: DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution  Mechanism for 5G and Beyond Solar Small Cell Networks ",
    "url": "https://arxiv.org/abs/2311.12944",
    "authors": [
      "Daksh Dave",
      "Vinay Chamola",
      "Sandeep Joshi",
      "Sherali Zeadally"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2311.17954",
    "title": "Transformer-empowered Multi-modal Item Embedding for Enhanced Image  Search in E-Commerce",
    "abstract": " Comments: Accepted by IAAI 2024 ",
    "url": "https://arxiv.org/abs/2311.17954",
    "authors": [
      "Chang Liu",
      "Peng Hou",
      "Anxiang Zeng",
      "Han Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.01210",
    "title": "When accurate prediction models yield harmful self-fulfilling prophecies",
    "abstract": " Title: When accurate prediction models yield harmful self-fulfilling prophecies ",
    "url": "https://arxiv.org/abs/2312.01210",
    "authors": [
      "Wouter A.C. van Amsterdam",
      "Nan van Geloven",
      "Jesse H. Krijthe",
      "Rajesh Ranganath",
      "Giovanni Cin\u00e1"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2312.06999",
    "title": "DGNet: Dynamic Gradient-Guided Network for Water-Related Optics Image  Enhancement",
    "abstract": " Title: DGNet: Dynamic Gradient-Guided Network for Water-Related Optics Image  Enhancement ",
    "url": "https://arxiv.org/abs/2312.06999",
    "authors": [
      "Jingchun Zhou",
      "Zongxin He",
      "Qiuping Jiang",
      "Kui Jiang",
      "Xianping Fu",
      "Xuelong Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.07364",
    "title": "Collapse-Aware Triplet Decoupling for Adversarially Robust Image  Retrieval",
    "abstract": " Title: Collapse-Aware Triplet Decoupling for Adversarially Robust Image  Retrieval ",
    "url": "https://arxiv.org/abs/2312.07364",
    "authors": [
      "Qiwei Tian",
      "Chenhao Lin",
      "Zhengyu Zhao",
      "Qian Li",
      "Chao Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.08213",
    "title": "Accelerated Event-Based Feature Detection and Compression for  Surveillance Video Systems",
    "abstract": " Comments: Accepted for publication in the proceedings of ACM Multimedia Systems '24 ",
    "url": "https://arxiv.org/abs/2312.08213",
    "authors": [
      "Andrew C. Freeman",
      "Ketan Mayer-Patel",
      "Montek Singh"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.11441",
    "title": "Social Learning: Towards Collaborative Learning with Large Language  Models",
    "abstract": " Title: Social Learning: Towards Collaborative Learning with Large Language  Models ",
    "url": "https://arxiv.org/abs/2312.11441",
    "authors": [
      "Amirkeivan Mohtashami",
      "Florian Hartmann",
      "Sian Gooding",
      "Lukas Zilka",
      "Matt Sharifi",
      "Blaise Aguera y Arcas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2312.12112",
    "title": "Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation  in ultra low-data regimes",
    "abstract": " Comments: *Seedat & Huynh contributed equally ",
    "url": "https://arxiv.org/abs/2312.12112",
    "authors": [
      "Nabeel Seedat",
      "Nicolas Huynh",
      "Boris van Breugel",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2312.14877",
    "title": "Robust Knowledge Extraction from Large Language Models using Social  Choice Theory",
    "abstract": " Comments: Accepted by AAMAS 2024 as a full paper ",
    "url": "https://arxiv.org/abs/2312.14877",
    "authors": [
      "Nico Potyka",
      "Yuqicheng Zhu",
      "Yunjie He",
      "Evgeny Kharlamov",
      "Steffen Staab"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.01984",
    "title": "AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed  and Low Tolerance",
    "abstract": " Comments: This research has been conducted during Google Summer of Code 2023 (GSoC 2023) at OpenVINO (Intel). GSoC 2023 page: this https URL ",
    "url": "https://arxiv.org/abs/2401.01984",
    "authors": [
      "Joao P. C. Bertoldo",
      "Dick Ameln",
      "Ashwin Vaidya",
      "Samet Ak\u00e7ay"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2401.04679",
    "title": "RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation",
    "abstract": " Title: RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation ",
    "url": "https://arxiv.org/abs/2401.04679",
    "authors": [
      "Mahdi Nikdan",
      "Soroush Tabesh",
      "Elvir Crn\u010devi\u0107",
      "Dan Alistarh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2401.09071",
    "title": "Rethinking Spectral Graph Neural Networks with Spatially Adaptive  Filtering",
    "abstract": " Title: Rethinking Spectral Graph Neural Networks with Spatially Adaptive  Filtering ",
    "url": "https://arxiv.org/abs/2401.09071",
    "authors": [
      "Jingwei Guo",
      "Kaizhu Huang",
      "Xinping Yi",
      "Zixian Su",
      "Rui Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.13169",
    "title": "ReposVul: A Repository-Level High-Quality Vulnerability Dataset",
    "abstract": " Comments: Accepted by ICSE 2024 Industry Challenge Track ",
    "url": "https://arxiv.org/abs/2401.13169",
    "authors": [
      "Xinchen Wang",
      "Ruida Hu",
      "Cuiyun Gao",
      "Xin-Cheng Wen",
      "Yujia Chen",
      "Qing Liao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2401.16191",
    "title": "From Tripods to Bipods: Reducing the Queue Number of Planar Graphs Costs  Just One Leg",
    "abstract": " Comments: The presented decomposition technique (Theorems 1.2/1.3) has been already independently shown by T. Ueckerdt, D.R. Wood, W. Yi (this https URL); a circumstance that I missed due to the result not being advertised in the corresponding abstract. Moreover, Lemma 4.2 is wrong, thus new technical details are necessary. I would like to thank Sergey Pupyrev for pointing this out ",
    "url": "https://arxiv.org/abs/2401.16191",
    "authors": [
      "Henry F\u00f6rster"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2402.00418",
    "title": "Benchmarking Transferable Adversarial Attacks",
    "abstract": " Comments: Accepted by NDSS 2024 Workshop ",
    "url": "https://arxiv.org/abs/2402.00418",
    "authors": [
      "Zhibo Jin",
      "Jiayu Zhang",
      "Zhiyu Zhu",
      "Huaming Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.00746",
    "title": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model",
    "abstract": " Title: Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model ",
    "url": "https://arxiv.org/abs/2402.00746",
    "authors": [
      "Mingyu Jin",
      "Qinkai Yu",
      "Chong Zhang",
      "Dong Shu",
      "Suiyuan Zhu",
      "Mengnan Du",
      "Yongfeng Zhang",
      "Yanda Meng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.01397",
    "title": "A survey on robustness in trajectory prediction for autonomous vehicles",
    "abstract": " Comments: 8 pages, 1 figure, 2 tables ",
    "url": "https://arxiv.org/abs/2402.01397",
    "authors": [
      "Jeroen Hagenus",
      "Frederik Baymler Mathiesen",
      "Julian F. Schumann",
      "Arkady Zgonnikov"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2402.01697",
    "title": "APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data  Annotation",
    "abstract": " Comments: Just accepted by WWW 2024 ",
    "url": "https://arxiv.org/abs/2402.01697",
    "authors": [
      "Yiming Zhu",
      "Zhizhuo Yin",
      "Gareth Tyson",
      "Ehsan-Ul Haq",
      "Lik-Hang Lee",
      "Pan Hui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.02275",
    "title": "SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing  Applications using a Generative Approach",
    "abstract": " Comments: Published in ACM Conference on Embedded Networked Sensor Systems (SenSys 23), November, 2023, Istanbul, Turkiye. This is the author's version of the work. It is posted here for your personal use. Not for redistribution. Publication rights licensed to the Association for Computing Machinery ",
    "url": "https://arxiv.org/abs/2402.02275",
    "authors": [
      "Tianshi Wang",
      "Jinyang Li",
      "Ruijie Wang",
      "Denizhan Kara",
      "Shengzhong Liu",
      "Davis Wertheimer",
      "Antoni Viros-i-Martin",
      "Raghu Ganti",
      "Mudhakar Srivatsa",
      "Tarek Abdelzaher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.03916",
    "title": "Can Large Language Models Detect Rumors on Social Media?",
    "abstract": " Title: Can Large Language Models Detect Rumors on Social Media? ",
    "url": "https://arxiv.org/abs/2402.03916",
    "authors": [
      "Qiang Liu",
      "Xiang Tao",
      "Junfei Wu",
      "Shu Wu",
      "Liang Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.04163",
    "title": "Tempered Calculus for ML: Application to Hyperbolic Model Embedding",
    "abstract": " Title: Tempered Calculus for ML: Application to Hyperbolic Model Embedding ",
    "url": "https://arxiv.org/abs/2402.04163",
    "authors": [
      "Richard Nock",
      "Ehsan Amid",
      "Frank Nielsen",
      "Alexander Soen",
      "Manfred K. Warmuth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.04663",
    "title": "CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural  Networks",
    "abstract": " Title: CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural  Networks ",
    "url": "https://arxiv.org/abs/2402.04663",
    "authors": [
      "Yulong Huang",
      "Xiaopeng Lin",
      "Hongwei Ren",
      "Yue Zhou",
      "Zunchang Liu",
      "Haotian Fu",
      "Biao Pan",
      "Bojun Cheng"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2402.04924",
    "title": "Two Trades is not Baffled: Condensing Graph via Crafting Rational  Gradient Matching",
    "abstract": " Comments: An effective method for graph condensation ",
    "url": "https://arxiv.org/abs/2402.04924",
    "authors": [
      "Tianle Zhang",
      "Yuchen Zhang",
      "Kun Wang",
      "Kai Wang",
      "Beining Yang",
      "Kaipeng Zhang",
      "Wenqi Shao",
      "Ping Liu",
      "Joey Tianyi Zhou",
      "Yang You"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05067",
    "title": "Multiscale Modelling with Physics-informed Neural Network: from  Large-scale Dynamics to Small-scale Predictions in Complex Systems",
    "abstract": " Title: Multiscale Modelling with Physics-informed Neural Network: from  Large-scale Dynamics to Small-scale Predictions in Complex Systems ",
    "url": "https://arxiv.org/abs/2402.05067",
    "authors": [
      "Jing Wang",
      "Zheng Li",
      "Pengyu Lai",
      "Rui Wang",
      "Di Yang",
      "Dewu Yang",
      "Hui Xu"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2402.05088",
    "title": "Domination and packing in graphs",
    "abstract": " Comments: 12 pages, 6 figures ",
    "url": "https://arxiv.org/abs/2402.05088",
    "authors": [
      "Renzo G\u00f3mez",
      "Juan Guti\u00e9rrez"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  }
]