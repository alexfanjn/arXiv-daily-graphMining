[
  {
    "id": "arXiv:2305.18300",
    "title": "Investigation of how social media influenced the endsars protests in  Lagos, Nigeria",
    "abstract": "The research assessed the role of social media in the unfolding of the EndSARS demonstrations in Nigeria. The study was necessary given the persistent call by governments for the strict regulation of social medium platforms. This is given governments' claim that social media is being misused. The study, however, reveals that social media use is determined by users' social experiences, including those caused by governments. ",
    "url": "https://arxiv.org/abs/2305.18300",
    "authors": [
      "Christopher Augustine"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2305.18320",
    "title": "Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4  mirroring math anxiety in high-school students",
    "abstract": "Large language models are becoming increasingly integrated into our lives. Hence, it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes, which originate in our own flawed ways of thinking. This challenge requires developing new benchmarks and methods for quantifying affective and semantic bias, keeping in mind that LLMs act as psycho-social mirrors that reflect the views and tendencies that are prevalent in society. One such tendency that has harmful negative effects is the global phenomenon of anxiety toward math and STEM subjects. Here, we investigate perceptions of math and STEM fields provided by cutting-edge language models, namely GPT-3, Chat-GPT, and GPT-4, by applying an approach from network science and cognitive psychology. Specifically, we use behavioral forma mentis networks (BFMNs) to understand how these LLMs frame math and STEM disciplines in relation to other concepts. We use data obtained by probing the three LLMs in a language generation task that has previously been applied to humans. Our findings indicate that LLMs have an overall negative perception of math and STEM fields, with math being perceived most negatively. We observe significant differences across the three LLMs. We observe that newer versions (i.e. GPT-4) produce richer, more complex perceptions as well as less negative perceptions compared to older versions and N=159 high-school students. These findings suggest that advances in the architecture of LLMs may lead to increasingly less biased models that could even perhaps someday aid in reducing harmful stereotypes in society rather than perpetuating them. ",
    "url": "https://arxiv.org/abs/2305.18320",
    "authors": [
      "Katherine Abramski",
      "Salvatore Citraro",
      "Luigi Lombardi",
      "Giulio Rossetti",
      "Massimo Stella"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18327",
    "title": "A Study on Deep CNN Structures for Defect Detection From Laser  Ultrasonic Visualization Testing Images",
    "abstract": "The importance of ultrasonic nondestructive testing has been increasing in recent years, and there are high expectations for the potential of laser ultrasonic visualization testing, which combines laser ultrasonic testing with scattered wave visualization technology. Even if scattered waves are visualized, inspectors still need to carefully inspect the images. To automate this, this paper proposes a deep neural network for automatic defect detection and localization in LUVT images. To explore the structure of a neural network suitable to this task, we compared the LUVT image analysis problem with the generic object detection problem. Numerical experiments using real-world data from a SUS304 flat plate showed that the proposed method is more effective than the general object detection model in terms of prediction performance. We also show that the computational time required for prediction is faster than that of the general object detection model. ",
    "url": "https://arxiv.org/abs/2305.18327",
    "authors": [
      "Miya Nakajima",
      "Takahiro Saitoh",
      "Tsuyoshi Kato"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18341",
    "title": "Tuning Models of Code with Compiler-Generated Reinforcement Learning  Feedback",
    "abstract": "Large Language Models (LLMs) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, the code that these models produce can violate basic language-level invariants, leading to lower performance in downstream tasks. We address this issue through an approach, called RLCF, that further trains a pre-trained LLM using feedback from a code compiler. RLCF views the LLM as an RL agent that generates code step by step and receives: (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different LLM on whether the generated code is similar to a set of reference programs in the training corpus. Together, these feedback mechanisms help the generated code remain within the target distribution while passing all static correctness checks. RLCF is model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA tasks for Java. Our experiments show that RLCF significantly raises the odds that an LLM-generated program compiles, is executable, and produces the right output on tests, often allowing LLMs to match the performance of 2x-8x larger LLMs. ",
    "url": "https://arxiv.org/abs/2305.18341",
    "authors": [
      "Abhinav Jain",
      "Chima Adiole",
      "Swarat Chaudhuri",
      "Thomas Reps",
      "Chris Jermaine"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18342",
    "title": "Neural Task Synthesis for Visual Programming",
    "abstract": "Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visual tasks for these codes. We demonstrate the effectiveness of NeurTaskSyn through an extensive empirical evaluation and a qualitative study on reference tasks taken from the Hour of Code: Classic Maze challenge by Code-dot-org and the Intro to Programming with Karel course by CodeHS-dot-com. ",
    "url": "https://arxiv.org/abs/2305.18342",
    "authors": [
      "Victor-Alexandru P\u0103durean",
      "Georgios Tzannetos",
      "Adish Singla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2305.18353",
    "title": "Emergent representations in networks trained with the Forward-Forward  algorithm",
    "abstract": "The Backpropagation algorithm, widely used to train neural networks, has often been criticised for its lack of biological realism. In an attempt to find a more biologically plausible alternative, and avoid to back-propagate gradients in favour of using local learning rules, the recently introduced Forward-Forward algorithm replaces the traditional forward and backward passes of Backpropagation with two forward passes. In this work, we show that internal representations obtained with the Forward-Forward algorithm organize into robust, category-specific ensembles, composed by an extremely low number of active units (high sparsity). This is remarkably similar to what is observed in cortical representations during sensory processing. While not found in models trained with standard Backpropagation, sparsity emerges also in networks optimized by Backpropagation, on the same training objective of Forward-Forward. These results suggest that the learning procedure proposed by Forward-Forward may be superior to Backpropagation in modelling learning in the cortex, even when a backward pass is used. ",
    "url": "https://arxiv.org/abs/2305.18353",
    "authors": [
      "Niccol\u00f2 Tosato",
      "Lorenzo Basile",
      "Emanuele Ballarin",
      "Giuseppe de Alteriis",
      "Alberto Cazzaniga",
      "Alessio Ansuini"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18355",
    "title": "An Efficient Membership Inference Attack for the Diffusion Model by  Proximal Initialization",
    "abstract": "Recently, diffusion models have achieved remarkable success in generating tasks, including image and audio generation. However, like other generative models, diffusion models are prone to privacy issues. In this paper, we propose an efficient query-based membership inference attack (MIA), namely Proximal Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by $\\epsilon$ initialized in $t=0$ and predicted point to infer memberships. Experimental results indicate that the proposed method can achieve competitive performance with only two queries on both discrete-time and continuous-time diffusion models. Moreover, previous works on the privacy of diffusion models have focused on vision tasks without considering audio tasks. Therefore, we also explore the robustness of diffusion models to MIA in the text-to-speech (TTS) task, which is an audio generation task. To the best of our knowledge, this work is the first to study the robustness of diffusion models to MIA in the TTS task. Experimental results indicate that models with mel-spectrogram (image-like) output are vulnerable to MIA, while models with audio output are relatively robust to MIA. {Code is available at \\url{https://github.com/kong13661/PIA}}. ",
    "url": "https://arxiv.org/abs/2305.18355",
    "authors": [
      "Fei Kong",
      "Jinhao Duan",
      "RuiPeng Ma",
      "Hengtao Shen",
      "Xiaofeng Zhu",
      "Xiaoshuang Shi",
      "Kaidi Xu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2305.18360",
    "title": "Sharing Leaky-Integrate-and-Fire Neurons for Memory-Efficient Spiking  Neural Networks",
    "abstract": "Spiking Neural Networks (SNNs) have gained increasing attention as energy-efficient neural networks owing to their binary and asynchronous computation. However, their non-linear activation, that is Leaky-Integrate-and-Fire (LIF) neuron, requires additional memory to store a membrane voltage to capture the temporal dynamics of spikes. Although the required memory cost for LIF neurons significantly increases as the input dimension goes larger, a technique to reduce memory for LIF neurons has not been explored so far. To address this, we propose a simple and effective solution, EfficientLIF-Net, which shares the LIF neurons across different layers and channels. Our EfficientLIF-Net achieves comparable accuracy with the standard SNNs while bringing up to ~4.3X forward memory efficiency and ~21.9X backward memory efficiency for LIF neurons. We conduct experiments on various datasets including CIFAR10, CIFAR100, TinyImageNet, ImageNet-100, and N-Caltech101. Furthermore, we show that our approach also offers advantages on Human Activity Recognition (HAR) datasets, which heavily rely on temporal information. ",
    "url": "https://arxiv.org/abs/2305.18360",
    "authors": [
      "Youngeun Kim",
      "Yuhang Li",
      "Abhishek Moitra",
      "Ruokai Yin",
      "Priyadarshini Panda"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2305.18374",
    "title": "Pure Spectral Graph Embeddings: Reinterpreting Graph Convolution for  Top-N Recommendation",
    "abstract": "The use of graph convolution in the development of recommender system algorithms has recently achieved state-of-the-art results in the collaborative filtering task (CF). While it has been demonstrated that the graph convolution operation is connected to a filtering operation on the graph spectral domain, the theoretical rationale for why this leads to higher performance on the collaborative filtering problem remains unknown. The presented work makes two contributions. First, we investigate the effect of using graph convolution throughout the user and item representation learning processes, demonstrating how the latent features learned are pushed from the filtering operation into the subspace spanned by the eigenvectors associated with the highest eigenvalues of the normalised adjacency matrix, and how vectors lying on this subspace are the optimal solutions for an objective function related to the sum of the prediction function over the training data. Then, we present an approach that directly leverages the eigenvectors to emulate the solution obtained through graph convolution, eliminating the requirement for a time-consuming gradient descent training procedure while also delivering higher performance on three real-world datasets. ",
    "url": "https://arxiv.org/abs/2305.18374",
    "authors": [
      "Edoardo D'Amico",
      "Aonghus Lawlor",
      "Neil Hurley"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18377",
    "title": "BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise  Learning",
    "abstract": "Label-noise learning (LNL) aims to increase the model's generalization given training data with noisy labels. To facilitate practical LNL algorithms, researchers have proposed different label noise types, ranging from class-conditional to instance-dependent noises. In this paper, we introduce a novel label noise type called BadLabel, which can significantly degrade the performance of existing LNL algorithms by a large margin. BadLabel is crafted based on the label-flipping attack against standard classification, where specific samples are selected and their labels are flipped to other labels so that the loss values of clean and noisy labels become indistinguishable. To address the challenge posed by BadLabel, we further propose a robust LNL method that perturbs the labels in an adversarial manner at each epoch to make the loss values of clean and noisy labels again distinguishable. Once we select a small set of (mostly) clean labeled data, we can apply the techniques of semi-supervised learning to train the model accurately. Empirically, our experimental results demonstrate that existing LNL algorithms are vulnerable to the newly introduced BadLabel noise type, while our proposed robust LNL method can effectively improve the generalization performance of the model under various types of label noise. The new dataset of noisy labels and the source codes of robust LNL algorithms are available at https://github.com/zjfheart/BadLabels. ",
    "url": "https://arxiv.org/abs/2305.18377",
    "authors": [
      "Jingfeng Zhang",
      "Bo Song",
      "Haohan Wang",
      "Bo Han",
      "Tongliang Liu",
      "Lei Liu",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18384",
    "title": "Backdoor Attacks Against Incremental Learners: An Empirical Evaluation  Study",
    "abstract": "Large amounts of incremental learning algorithms have been proposed to alleviate the catastrophic forgetting issue arises while dealing with sequential data on a time series. However, the adversarial robustness of incremental learners has not been widely verified, leaving potential security risks. Specifically, for poisoning-based backdoor attacks, we argue that the nature of streaming data in IL provides great convenience to the adversary by creating the possibility of distributed and cross-task attacks -- an adversary can affect \\textbf{any unknown} previous or subsequent task by data poisoning \\textbf{at any time or time series} with extremely small amount of backdoor samples injected (e.g., $0.1\\%$ based on our observations). To attract the attention of the research community, in this paper, we empirically reveal the high vulnerability of 11 typical incremental learners against poisoning-based backdoor attack on 3 learning scenarios, especially the cross-task generalization effect of backdoor knowledge, while the poison ratios range from $5\\%$ to as low as $0.1\\%$. Finally, the defense mechanism based on activation clustering is found to be effective in detecting our trigger pattern to mitigate potential security risks. ",
    "url": "https://arxiv.org/abs/2305.18384",
    "authors": [
      "Yiqi Zhong",
      "Xianming Liu",
      "Deming Zhai",
      "Junjun Jiang",
      "Xiangyang Ji"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18385",
    "title": "Self-attention Dual Embedding for Graphs with Heterophily",
    "abstract": "Graph Neural Networks (GNNs) have been highly successful for the node classification task. GNNs typically assume graphs are homophilic, i.e. neighboring nodes are likely to belong to the same class. However, a number of real-world graphs are heterophilic, and this leads to much lower classification accuracy using standard GNNs. In this work, we design a novel GNN which is effective for both heterophilic and homophilic graphs. Our work is based on three main observations. First, we show that node features and graph topology provide different amounts of informativeness in different graphs, and therefore they should be encoded independently and prioritized in an adaptive manner. Second, we show that allowing negative attention weights when propagating graph topology information improves accuracy. Finally, we show that asymmetric attention weights between nodes are helpful. We design a GNN which makes use of these observations through a novel self-attention mechanism. We evaluate our algorithm on real-world graphs containing thousands to millions of nodes and show that we achieve state-of-the-art results compared to existing GNNs. We also analyze the effectiveness of the main components of our design on different graphs. ",
    "url": "https://arxiv.org/abs/2305.18385",
    "authors": [
      "Yurui Lai",
      "Taiyan Zhang",
      "Rui Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2305.18387",
    "title": "Augmenting Character Designers Creativity Using Generative Adversarial  Networks",
    "abstract": "Recent advances in Generative Adversarial Networks (GANs) continue to attract the attention of researchers in different fields due to the wide range of applications devised to take advantage of their key features. Most recent GANs are focused on realism, however, generating hyper-realistic output is not a priority for some domains, as in the case of this work. The generated outcomes are used here as cognitive components to augment character designers creativity while conceptualizing new characters for different multimedia projects. To select the best-suited GANs for such a creative context, we first present a comparison between different GAN architectures and their performance when trained from scratch on a new visual characters dataset using a single Graphics Processing Unit. We also explore alternative techniques, such as transfer learning and data augmentation, to overcome computational resource limitations, a challenge faced by many researchers in the domain. Additionally, mixed methods are used to evaluate the cognitive value of the generated visuals on character designers agency conceptualizing new characters. The results discussed proved highly effective for this context, as demonstrated by early adaptations to the characters design process. As an extension for this work, the presented approach will be further evaluated as a novel co-design process between humans and machines to investigate where and how the generated concepts are interacting with and influencing the design process outcome. ",
    "url": "https://arxiv.org/abs/2305.18387",
    "authors": [
      "Mohammad Lataifeh",
      "Xavier Carrasco",
      "Ashraf Elnagar",
      "Naveed Ahmed"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18389",
    "title": "AnoRand: A Semi Supervised Deep Learning Anomaly Detection Method by  Random Labeling",
    "abstract": "Anomaly detection or more generally outliers detection is one of the most popular and challenging subject in theoretical and applied machine learning. The main challenge is that in general we have access to very few labeled data or no labels at all. In this paper, we present a new semi-supervised anomaly detection method called \\textbf{AnoRand} by combining a deep learning architecture with random synthetic label generation. The proposed architecture has two building blocks: (1) a noise detection (ND) block composed of feed forward ferceptron and (2) an autoencoder (AE) block. The main idea of this new architecture is to learn one class (e.g. the majority class in case of anomaly detection) as well as possible by taking advantage of the ability of auto encoders to represent data in a latent space and the ability of Feed Forward Perceptron (FFP) to learn one class when the data is highly imbalanced. First, we create synthetic anomalies by randomly disturbing (add noise) few samples (e.g. 2\\%) from the training set. Second, we use the normal and the synthetic samples as input to our model. We compared the performance of the proposed method to 17 state-of-the-art unsupervised anomaly detection method on synthetic datasets and 57 real-world datasets. Our results show that this new method generally outperforms most of the state-of-the-art methods and has the best performance (AUC ROC and AUC PR) on the vast majority of reference datasets. We also tested our method in a supervised way by using the actual labels to train the model. The results show that it has very good performance compared to most of state-of-the-art supervised algorithms. ",
    "url": "https://arxiv.org/abs/2305.18389",
    "authors": [
      "Mansour Zoubeirou A Mayaki",
      "Michel Riveill"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18391",
    "title": "MemeGraphs: Linking Memes to Knowledge Graphs",
    "abstract": "Memes are a popular form of communicating trends and ideas in social media and on the internet in general, combining the modalities of images and text. They can express humor and sarcasm but can also have offensive content. Analyzing and classifying memes automatically is challenging since their interpretation relies on the understanding of visual elements, language, and background knowledge. Thus, it is important to meaningfully represent these sources and the interaction between them in order to classify a meme as a whole. In this work, we propose to use scene graphs, that express images in terms of objects and their visual relations, and knowledge graphs as structured representations for meme classification with a Transformer-based architecture. We compare our approach with ImgBERT, a multimodal model that uses only learned (instead of structured) representations of the meme, and observe consistent improvements. We further provide a dataset with human graph annotations that we compare to automatically generated graphs and entity linking. Analysis shows that automatic methods link more entities than human annotators and that automatically generated graphs are better suited for hatefulness classification in memes. ",
    "url": "https://arxiv.org/abs/2305.18391",
    "authors": [
      "Vasiliki Kougia",
      "Simon Fetzel",
      "Thomas Kirchmair",
      "Erion \u00c7ano",
      "Sina Moayed Baharlou",
      "Sahand Sharifzadeh",
      "Benjamin Roth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18397",
    "title": "Prediction of the 2023 Turkish Presidential Election Results Using  Social Media Data",
    "abstract": "Social media platforms influence the way political campaigns are run and therefore they have become an increasingly important tool for politicians to directly interact with citizens. Previous elections in various countries have shown that social media data may significantly impact election results. In this study, we aim to predict the vote shares of parties participating in the 2023 elections in Turkey by combining social media data from various platforms together with traditional polling data. Our approach is a volume-based approach that considers the number of social media interactions rather than content. We compare several prediction models across varying time windows. Our results show that for all time windows, the ARIMAX model outperforms the other algorithms. ",
    "url": "https://arxiv.org/abs/2305.18397",
    "authors": [
      "Aysun Bozanta",
      "Fuad Bayrak",
      "Ayse Basar"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18402",
    "title": "Neural Sculpting: Uncovering hierarchically modular task structure  through pruning and network analysis",
    "abstract": "Natural target functions and tasks typically exhibit hierarchical modularity - they can be broken down into simpler sub-functions that are organized in a hierarchy. Such sub-functions have two important features: they have a distinct set of inputs (input-separability) and they are reused as inputs higher in the hierarchy (reusability). Previous studies have established that hierarchically modular neural networks, which are inherently sparse, offer benefits such as learning efficiency, generalization, multi-task learning, and transferability. However, identifying the underlying sub-functions and their hierarchical structure for a given task can be challenging. The high-level question in this work is: if we learn a task using a sufficiently deep neural network, how can we uncover the underlying hierarchy of sub-functions in that task? As a starting point, we examine the domain of Boolean functions, where it is easier to determine whether a task is hierarchically modular. We propose an approach based on iterative unit and edge pruning (during training), combined with network analysis for module detection and hierarchy inference. Finally, we demonstrate that this method can uncover the hierarchical modularity of a wide range of Boolean functions and two vision tasks based on the MNIST digits dataset. ",
    "url": "https://arxiv.org/abs/2305.18402",
    "authors": [
      "Shreyas Malakarjun Patil",
      "Loizos Michael",
      "Constantine Dovrolis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18404",
    "title": "Conformal Prediction with Large Language Models for Multi-Choice  Question Answering",
    "abstract": "As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required. ",
    "url": "https://arxiv.org/abs/2305.18404",
    "authors": [
      "Bhawesh Kumar",
      "Charlie Lu",
      "Gauri Gupta",
      "Anil Palepu",
      "David Bellamy",
      "Ramesh Raskar",
      "Andrew Beam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.18405",
    "title": "Dink-Net: Neural Clustering on Large Graphs",
    "abstract": "Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designed loss functions adopt the mini-batch data to optimize the clustering distribution even without performance drops. Both experimental results and theoretical analyses demonstrate the superiority of our method. Compared to the runner-up, Dink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111 million nodes and 1.6 billion edges. The source code is released at https://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes, and datasets) of deep graph clustering is shared at https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering. ",
    "url": "https://arxiv.org/abs/2305.18405",
    "authors": [
      "Yue Liu",
      "Ke Liang",
      "Jun Xia",
      "Sihang Zhou",
      "Xihong Yang",
      "Xinwang Liu",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18410",
    "title": "Understanding Breast Cancer Survival: Using Causality and Language  Models on Multi-omics Data",
    "abstract": "The need for more usable and explainable machine learning models in healthcare increases the importance of developing and utilizing causal discovery algorithms, which aim to discover causal relations by analyzing observational data. Explainable approaches aid clinicians and biologists in predicting the prognosis of diseases and suggesting proper treatments. However, very little research has been conducted at the crossroads between causal discovery, genomics, and breast cancer, and we aim to bridge this gap. Moreover, evaluation of causal discovery methods on real data is in general notoriously difficult because ground-truth causal relations are usually unknown, and accordingly, in this paper, we also propose to address the evaluation problem with large language models. In particular, we exploit suitable causal discovery algorithms to investigate how various perturbations in the genome can affect the survival of patients diagnosed with breast cancer. We used three main causal discovery algorithms: PC, Greedy Equivalence Search (GES), and a Generalized Precision Matrix-based one. We experiment with a subset of The Cancer Genome Atlas, which contains information about mutations, copy number variations, protein levels, and gene expressions for 705 breast cancer patients. Our findings reveal important factors related to the vital status of patients using causal discovery algorithms. However, the reliability of these results remains a concern in the medical domain. Accordingly, as another contribution of the work, the results are validated through language models trained on biomedical literature, such as BlueBERT and other large language models trained on medical corpora. Our results profess proper utilization of causal discovery algorithms and language models for revealing reliable causal relations for clinical applications. ",
    "url": "https://arxiv.org/abs/2305.18410",
    "authors": [
      "Mugariya Farooq",
      "Shahad Hardan",
      "Aigerim Zhumbhayeva",
      "Yujia Zheng",
      "Preslav Nakov",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Genomics (q-bio.GN)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2305.18411",
    "title": "Feature-Learning Networks Are Consistent Across Widths At Realistic  Scales",
    "abstract": "We study the effect of width on the dynamics of feature-learning neural networks across a variety of architectures and datasets. Early in training, wide neural networks trained on online data have not only identical loss curves but also agree in their point-wise test predictions throughout training. For simple tasks such as CIFAR-5m this holds throughout training for networks of realistic widths. We also show that structural properties of the models, including internal representations, preactivation distributions, edge of stability phenomena, and large learning rate effects are consistent across large widths. This motivates the hypothesis that phenomena seen in realistic models can be captured by infinite-width, feature-learning limits. For harder tasks (such as ImageNet and language modeling), and later training times, finite-width deviations grow systematically. Two distinct effects cause these deviations across widths. First, the network output has initialization-dependent variance scaling inversely with width, which can be removed by ensembling networks. We observe, however, that ensembles of narrower networks perform worse than a single wide network. We call this the bias of narrower width. We conclude with a spectral perspective on the origin of this finite-width bias. ",
    "url": "https://arxiv.org/abs/2305.18411",
    "authors": [
      "Nikhil Vyas",
      "Alexander Atanasov",
      "Blake Bordelon",
      "Depen Morwani",
      "Sabarish Sainathan",
      "Cengiz Pehlevan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18414",
    "title": "StEik: Stabilizing the Optimization of Neural Signed Distance Functions  and Finer Shape Representation",
    "abstract": "We present new insights and a novel paradigm (StEik) for learning implicit neural representations (INR) of shapes. In particular, we shed light on the popular eikonal loss used for imposing a signed distance function constraint in INR. We show analytically that as the representation power of the network increases, the optimization approaches a partial differential equation (PDE) in the continuum limit that is unstable. We show that this instability can manifest in existing network optimization, leading to irregularities in the reconstructed surface and/or convergence to sub-optimal local minima, and thus fails to capture fine geometric and topological structure. We show analytically how other terms added to the loss, currently used in the literature for other purposes, can actually eliminate these instabilities. However, such terms can over-regularize the surface, preventing the representation of fine shape detail. Based on a similar PDE theory for the continuum limit, we introduce a new regularization term that still counteracts the eikonal instability but without over-regularizing. Furthermore, since stability is now guaranteed in the continuum limit, this stabilization also allows for considering new network structures that are able to represent finer shape detail. We introduce such a structure based on quadratic layers. Experiments on multiple benchmark data sets show that our new regularization and network are able to capture more precise shape details and more accurate topology than existing state-of-the-art. ",
    "url": "https://arxiv.org/abs/2305.18414",
    "authors": [
      "Huizong Yang",
      "Yuxin Sun",
      "Ganesh Sundaramoorthi",
      "Anthony Yezzi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2305.18420",
    "title": "Sample Complexity of Variance-reduced Distributionally Robust Q-learning",
    "abstract": "Dynamic decision making under distributional shifts is of fundamental interest in theory and applications of reinforcement learning: The distribution of the environment on which the data is collected can differ from that of the environment on which the model is deployed. This paper presents two novel model-free algorithms, namely the distributionally robust Q-learning and its variance-reduced counterpart, that can effectively learn a robust policy despite distributional shifts. These algorithms are designed to efficiently approximate the $q$-function of an infinite-horizon $\\gamma$-discounted robust Markov decision process with Kullback-Leibler uncertainty set to an entry-wise $\\epsilon$-degree of precision. Further, the variance-reduced distributionally robust Q-learning combines the synchronous Q-learning with variance-reduction techniques to enhance its performance. Consequently, we establish that it attains a minmax sample complexity upper bound of $\\tilde O(|S||A|(1-\\gamma)^{-4}\\epsilon^{-2})$, where $S$ and $A$ denote the state and action spaces. This is the first complexity result that is independent of the uncertainty size $\\delta$, thereby providing new complexity theoretic insights. Additionally, a series of numerical experiments confirm the theoretical findings and the efficiency of the algorithms in handling distributional shifts. ",
    "url": "https://arxiv.org/abs/2305.18420",
    "authors": [
      "Shengbo Wang",
      "Nian Si",
      "Jose Blanchet",
      "Zhengyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.18444",
    "title": "Continual Task Allocation in Meta-Policy Network via Sparse Prompting",
    "abstract": "How to train a generalizable meta-policy by continually learning a sequence of tasks? It is a natural human skill yet challenging to achieve by current reinforcement learning: the agent is expected to quickly adapt to new tasks (plasticity) meanwhile retaining the common knowledge from previous tasks (stability). We address it by \"Continual Task Allocation via Sparse Prompting (CoTASP)\", which learns over-complete dictionaries to produce sparse masks as prompts extracting a sub-network for each task from a meta-policy network. By optimizing the sub-network and prompts alternatively, CoTASP updates the meta-policy via training a task-specific policy. The dictionary is then updated to align the optimized prompts with tasks' embedding, thereby capturing their semantic correlations. Hence, relevant tasks share more neurons in the meta-policy network via similar prompts while cross-task interference causing forgetting is effectively restrained. Given a trained meta-policy with updated dictionaries, new task adaptation reduces to highly efficient sparse prompting and sub-network finetuning. In experiments, CoTASP achieves a promising plasticity-stability trade-off without storing or replaying any past tasks' experiences and outperforms existing continual and multi-task RL methods on all seen tasks, forgetting reduction, and generalization to unseen tasks. ",
    "url": "https://arxiv.org/abs/2305.18444",
    "authors": [
      "Yijun Yang",
      "Tianyi Zhou",
      "Jing Jiang",
      "Guodong Long",
      "Yuhui Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18445",
    "title": "Intelligent gradient amplification for deep neural networks",
    "abstract": "Deep learning models offer superior performance compared to other machine learning techniques for a variety of tasks and domains, but pose their own challenges. In particular, deep learning models require larger training times as the depth of a model increases, and suffer from vanishing gradients. Several solutions address these problems independently, but there have been minimal efforts to identify an integrated solution that improves the performance of a model by addressing vanishing gradients, as well as accelerates the training process to achieve higher performance at larger learning rates. In this work, we intelligently determine which layers of a deep learning model to apply gradient amplification to, using a formulated approach that analyzes gradient fluctuations of layers during training. Detailed experiments are performed for simpler and deeper neural networks using two different intelligent measures and two different thresholds that determine the amplification layers, and a training strategy where gradients are amplified only during certain epochs. Results show that our amplification offers better performance compared to the original models, and achieves accuracy improvement of around 2.5% on CIFAR- 10 and around 4.5% on CIFAR-100 datasets, even when the models are trained with higher learning rates. ",
    "url": "https://arxiv.org/abs/2305.18445",
    "authors": [
      "Sunitha Basodi",
      "Krishna Pusuluri",
      "Xueli Xiao",
      "Yi Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18446",
    "title": "Trompt: Towards a Better Deep Neural Network for Tabular Data",
    "abstract": "Tabular data is arguably one of the most commonly used data structures in various practical domains, including finance, healthcare and e-commerce. The inherent heterogeneity allows tabular data to store rich information. However, based on a recently published tabular benchmark, we can see deep neural networks still fall behind tree-based models on tabular datasets. In this paper, we propose Trompt--which stands for Tabular Prompt--a novel architecture inspired by prompt learning of language models. The essence of prompt learning is to adjust a large pre-trained model through a set of prompts outside the model without directly modifying the model. Based on this idea, Trompt separates the learning strategy of tabular data into two parts. The first part, analogous to pre-trained models, focus on learning the intrinsic information of a table. The second part, analogous to prompts, focus on learning the variations among samples. Trompt is evaluated with the benchmark mentioned above. The experimental results demonstrate that Trompt outperforms state-of-the-art deep neural networks and is comparable to tree-based models. ",
    "url": "https://arxiv.org/abs/2305.18446",
    "authors": [
      "Kuan-Yu Chen",
      "Ping-Han Chiang",
      "Hsin-Rung Chou",
      "Ting-Wei Chen",
      "Tien-Hao Chang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18448",
    "title": "Neural Network Reduction with Guided Regularizers",
    "abstract": "Regularization techniques such as $\\mathcal{L}_1$ and $\\mathcal{L}_2$ regularizers are effective in sparsifying neural networks (NNs). However, to remove a certain neuron or channel in NNs, all weight elements related to that neuron or channel need to be prunable, which is not guaranteed by traditional regularization. This paper proposes a simple new approach named \"Guided Regularization\" that prioritizes the weights of certain NN units more than others during training, which renders some of the units less important and thus, prunable. This is different from the scattered sparsification of $\\mathcal{L}_1$ and $\\mathcal{L}_2$ regularizers where the the components of a weight matrix that are zeroed out can be located anywhere. The proposed approach offers a natural reduction of NN in the sense that a model is being trained while also neutralizing unnecessary units. We empirically demonstrate that our proposed method is effective in pruning NNs while maintaining performance. ",
    "url": "https://arxiv.org/abs/2305.18448",
    "authors": [
      "Ali Haisam Muhammad Rafid",
      "Adrian Sandu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18449",
    "title": "Taming AI Bots: Controllability of Neural States in Large Language  Models",
    "abstract": "We tackle the question of whether an agent can, by suitable choice of prompts, control an AI bot to any state. To that end, we first introduce a formal definition of ``meaning'' that is amenable to analysis. Then, we characterize ``meaningful data'' on which large language models (LLMs) are ostensibly trained, and ``well-trained LLMs'' through conditions that are largely met by today's LLMs. While a well-trained LLM constructs an embedding space of meanings that is Euclidean, meanings themselves do not form a vector (linear) subspace, but rather a quotient space within. We then characterize the subset of meanings that can be reached by the state of the LLMs for some input prompt, and show that a well-trained bot can reach any meaning albeit with small probability. We then introduce a stronger notion of controllability as {\\em almost certain reachability}, and show that, when restricted to the space of meanings, an AI bot is controllable. We do so after introducing a functional characterization of attentive AI bots, and finally derive necessary and sufficient conditions for controllability. The fact that AI bots are controllable means that an adversary could steer them towards any state. However, the sampling process can be designed to counteract adverse actions and avoid reaching undesirable regions of state space before their boundary is crossed. ",
    "url": "https://arxiv.org/abs/2305.18449",
    "authors": [
      "Stefano Soatto",
      "Paulo Tabuada",
      "Pratik Chaudhari",
      "Tian Yu Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2305.18451",
    "title": "Shift-Robust Molecular Relational Learning with Causal Substructure",
    "abstract": "Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and synthetic datasets demonstrate the superiority of CMRL over state-of-the-art baseline models. Our code is available at https://github.com/Namkyeong/CMRL. ",
    "url": "https://arxiv.org/abs/2305.18451",
    "authors": [
      "Namkyeong Lee",
      "Kanghoon Yoon",
      "Gyoung S. Na",
      "Sein Kim",
      "Chanyoung Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)",
      "Molecular Networks (q-bio.MN)"
    ]
  },
  {
    "id": "arXiv:2305.18457",
    "title": "Learning Strong Graph Neural Networks with Weak Information",
    "abstract": "Graph Neural Networks (GNNs) have exhibited impressive performance in many graph learning tasks. Nevertheless, the performance of GNNs can deteriorate when the input graph data suffer from weak information, i.e., incomplete structure, incomplete features, and insufficient labels. Most prior studies, which attempt to learn from the graph data with a specific type of weak information, are far from effective in dealing with the scenario where diverse data deficiencies exist and mutually affect each other. To fill the gap, in this paper, we aim to develop an effective and principled approach to the problem of graph learning with weak information (GLWI). Based on the findings from our empirical analysis, we derive two design focal points for solving the problem of GLWI, i.e., enabling long-range propagation in GNNs and allowing information propagation to those stray nodes isolated from the largest connected component. Accordingly, we propose D$^2$PT, a dual-channel GNN framework that performs long-range information propagation not only on the input graph with incomplete structure, but also on a global graph that encodes global semantic similarities. We further develop a prototype contrastive alignment algorithm that aligns the class-level prototypes learned from two channels, such that the two different information propagation processes can mutually benefit from each other and the finally learned model can well handle the GLWI problem. Extensive experiments on eight real-world benchmark datasets demonstrate the effectiveness and efficiency of our proposed methods in various GLWI scenarios. ",
    "url": "https://arxiv.org/abs/2305.18457",
    "authors": [
      "Yixin Liu",
      "Kaize Ding",
      "Jianling Wang",
      "Vincent Lee",
      "Huan Liu",
      "Shirui Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18460",
    "title": "Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal  Approximation",
    "abstract": "The study of universal approximation properties (UAP) for neural networks (NN) has a long history. When the network width is unlimited, only a single hidden layer is sufficient for UAP. In contrast, when the depth is unlimited, the width for UAP needs to be not less than the critical width $w^*_{\\min}=\\max(d_x,d_y)$, where $d_x$ and $d_y$ are the dimensions of the input and output, respectively. Recently, \\cite{cai2022achieve} shows that a leaky-ReLU NN with this critical width can achieve UAP for $L^p$ functions on a compact domain $K$, \\emph{i.e.,} the UAP for $L^p(K,\\mathbb{R}^{d_y})$. This paper examines a uniform UAP for the function class $C(K,\\mathbb{R}^{d_y})$ and gives the exact minimum width of the leaky-ReLU NN as $w_{\\min}=\\max(d_x+1,d_y)+1_{d_y=d_x+1}$, which involves the effects of the output dimensions. To obtain this result, we propose a novel lift-flow-discretization approach that shows that the uniform UAP has a deep connection with topological theory. ",
    "url": "https://arxiv.org/abs/2305.18460",
    "authors": [
      "Li'ang Li",
      "Yifei Duan",
      "Guanghua Ji",
      "Yongqiang Cai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2305.18462",
    "title": "Membership Inference Attacks against Language Models via Neighbourhood  Comparison",
    "abstract": "Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. To investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks. ",
    "url": "https://arxiv.org/abs/2305.18462",
    "authors": [
      "Justus Mattern",
      "Fatemehsadat Mireshghallah",
      "Zhijing Jin",
      "Bernhard Sch\u00f6lkopf",
      "Mrinmaya Sachan",
      "Taylor Berg-Kirkpatrick"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18465",
    "title": "Federated Learning of Gboard Language Models with Differential Privacy",
    "abstract": "We train language models (LMs) with federated learning (FL) and differential privacy (DP) in the Google Keyboard (Gboard). We apply the DP-Follow-the-Regularized-Leader (DP-FTRL)~\\citep{kairouz21b} algorithm to achieve meaningfully formal DP guarantees without requiring uniform sampling of client devices. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation~\\citep{andrew2019differentially} can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation for training. With the help of pretraining on public data, we train and deploy more than twenty Gboard LMs that achieve high utility and $\\rho-$zCDP privacy guarantees with $\\rho \\in (0.2, 2)$, with two models additionally trained with secure aggregation~\\citep{bonawitz2017practical}. We are happy to announce that all the next word prediction neural network LMs in Gboard now have DP guarantees, and all future launches of Gboard neural network LMs will require DP guarantees. We summarize our experience and provide concrete suggestions on DP training for practitioners. ",
    "url": "https://arxiv.org/abs/2305.18465",
    "authors": [
      "Zheng Xu",
      "Yanxiang Zhang",
      "Galen Andrew",
      "Christopher A. Choquette-Choo",
      "Peter Kairouz",
      "H. Brendan McMahan",
      "Jesse Rosenstock",
      "Yuanbo Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2305.18467",
    "title": "Geometric Graph Filters and Neural Networks: Limit Properties and  Discriminability Trade-offs",
    "abstract": "This paper studies the relationship between a graph neural network (GNN) and a manifold neural network (MNN) when the graph is constructed from a set of points sampled from the manifold, thus encoding geometric information. We consider convolutional MNNs and GNNs where the manifold and the graph convolutions are respectively defined in terms of the Laplace-Beltrami operator and the graph Laplacian. Using the appropriate kernels, we analyze both dense and moderately sparse graphs. We prove non-asymptotic error bounds showing that convolutional filters and neural networks on these graphs converge to convolutional filters and neural networks on the continuous manifold. As a byproduct of this analysis, we observe an important trade-off between the discriminability of graph filters and their ability to approximate the desired behavior of manifold filters. We then discuss how this trade-off is ameliorated in neural networks due to the frequency mixing property of nonlinearities. We further derive a transferability corollary for geometric graphs sampled from the same manifold. We validate our results numerically on a navigation control problem and a point cloud classification task. ",
    "url": "https://arxiv.org/abs/2305.18467",
    "authors": [
      "Zhiyang Wang",
      "Luana Ruiz",
      "Alejandro Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2305.18475",
    "title": "Approximation theory of transformer networks for sequence modeling",
    "abstract": "The transformer is a widely applied architecture in sequence modeling applications, but the theoretical understanding of its working principles is limited. In this work, we investigate the ability of transformers to approximate sequential relationships. We first prove a universal approximation theorem for the transformer hypothesis space. From its derivation, we identify a novel notion of regularity under which we can prove an explicit approximation rate estimate. This estimate reveals key structural properties of the transformer and suggests the types of sequence relationships that the transformer is adapted to approximating. In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks. Our findings are supported by numerical experiments. ",
    "url": "https://arxiv.org/abs/2305.18475",
    "authors": [
      "Haotian Jiang",
      "Qianxiao Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18478",
    "title": "Forward and Inverse Approximation Theory for Linear Temporal  Convolutional Networks",
    "abstract": "We present a theoretical analysis of the approximation properties of convolutional architectures when applied to the modeling of temporal sequences. Specifically, we prove an approximation rate estimate (Jackson-type result) and an inverse approximation theorem (Bernstein-type result), which together provide a comprehensive characterization of the types of sequential relationships that can be efficiently captured by a temporal convolutional architecture. The rate estimate improves upon a previous result via the introduction of a refined complexity measure, whereas the inverse approximation theorem is new. ",
    "url": "https://arxiv.org/abs/2305.18478",
    "authors": [
      "Haotian Jiang",
      "Qianxiao Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18482",
    "title": "Fashion Object Detection for Tops & Bottoms",
    "abstract": "Fashion is one of the largest world's industries and computer vision techniques have been becoming more popular in recent years, in particular, for tasks such as object detection and apparel segmentation. Even with the rapid growth in computer vision solutions, specifically for the fashion industry, many problems are far for being resolved. Therefore, not at all times, adjusting out-of-the-box pre-trained computer vision models will provide the desired solution. In the present paper is proposed a pipeline that takes a noisy image with a person and specifically detects the regions with garments that are bottoms or tops. Our solution implements models that are capable of finding human parts in an image e.g. full-body vs half-body, or no human is found. Then, other models knowing that there's a human and its composition (e.g. not always we have a full-body) finds the bounding boxes/regions of the image that very likely correspond to a bottom or a top. For the creation of bounding boxes/regions task, a benchmark dataset was specifically prepared. The results show that the Mask RCNN solution is robust, and generalized enough to be used and scalable in unseen apparel/fashion data. ",
    "url": "https://arxiv.org/abs/2305.18482",
    "authors": [
      "Andreas Petridis",
      "Mirela Popa",
      "Filipa Peleja",
      "Dario Dotti",
      "Alberto de Santos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18485",
    "title": "Autoencoding Conditional Neural Processes for Representation Learning",
    "abstract": "Conditional neural processes (CNPs) are a flexible and efficient family of models that learn to learn a stochastic process from observations. In the visual domain, they have seen particular application in contextual image completion - observing pixel values at some locations to predict a distribution over values at other unobserved locations. However, the choice of pixels in learning such a CNP is typically either random or derived from a simple statistical measure (e.g. pixel variance). Here, we turn the problem on its head and ask: which pixels would a CNP like to observe? That is, which pixels allow fitting CNP, and do such pixels tell us something about the underlying image? Viewing the context provided to the CNP as fixed-size latent representations, we construct an amortised variational framework, Partial Pixel Space Variational Autoencoder (PPS-VAE), for predicting this context simultaneously with learning a CNP. We evaluate PPS-VAE on a set of vision datasets, and find that not only is it possible to learn context points while also fitting CNPs, but that their spatial arrangement and values provides strong signal for the information contained in the image - evaluated through the lens of classification. We believe the PPS-VAE provides a promising avenue to explore learning interpretable and effective visual representations. ",
    "url": "https://arxiv.org/abs/2305.18485",
    "authors": [
      "Victor Prokhorov",
      "Ivan Titov",
      "N. Siddharth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18491",
    "title": "Towards a Better Understanding of Representation Dynamics under  TD-learning",
    "abstract": "TD-learning is a foundation reinforcement learning (RL) algorithm for value prediction. Critical to the accuracy of value predictions is the quality of state representations. In this work, we consider the question: how does end-to-end TD-learning impact the representation over time? Complementary to prior work, we provide a set of analysis that sheds further light on the representation dynamics under TD-learning. We first show that when the environments are reversible, end-to-end TD-learning strictly decreases the value approximation error over time. Under further assumptions on the environments, we can connect the representation dynamics with spectral decomposition over the transition matrix. This latter finding establishes fitting multiple value functions from randomly generated rewards as a useful auxiliary task for representation learning, as we empirically validate on both tabular and Atari game suites. ",
    "url": "https://arxiv.org/abs/2305.18491",
    "authors": [
      "Yunhao Tang",
      "R\u00e9mi Munos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18495",
    "title": "Hardware-aware Training Techniques for Improving Robustness of Ex-Situ  Neural Network Transfer onto Passive TiO2 ReRAM Crossbars",
    "abstract": "Passive resistive random access memory (ReRAM) crossbar arrays, a promising emerging technology used for analog matrix-vector multiplications, are far superior to their active (1T1R) counterparts in terms of the integration density. However, current transfers of neural network weights into the conductance state of the memory devices in the crossbar architecture are accompanied by significant losses in precision due to hardware variabilities such as sneak path currents, biasing scheme effects and conductance tuning imprecision. In this work, training approaches that adapt techniques such as dropout, the reparametrization trick and regularization to TiO2 crossbar variabilities are proposed in order to generate models that are better adapted to their hardware transfers. The viability of this approach is demonstrated by comparing the outputs and precision of the proposed hardware-aware network with those of a regular fully connected network over a few thousand weight transfers using the half moons dataset in a simulation based on experimental data. For the neural network trained using the proposed hardware-aware method, 79.5% of the test set's data points can be classified with an accuracy of 95% or higher, while only 18.5% of the test set's data points can be classified with this accuracy by the regularly trained neural network. ",
    "url": "https://arxiv.org/abs/2305.18495",
    "authors": [
      "Philippe Drolet",
      "Rapha\u00ebl Dawant",
      "Victor Yon",
      "Pierre-Antoine Mouny",
      "Matthieu Valdenaire",
      "Javier Arias Zapata",
      "Pierre Gliech",
      "Sean U. N. Wood",
      "Serge Ecoffey",
      "Fabien Alibart",
      "Yann Beilliard",
      "Dominique Drouin"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18497",
    "title": "Collaborative Learning via Prediction Consensus",
    "abstract": "We consider a collaborative learning setting where each agent's goal is to improve their own model by leveraging the expertise of collaborators, in addition to their own training data. To facilitate the exchange of expertise among agents, we propose a distillation-based method leveraging unlabeled auxiliary data, which is pseudo-labeled by the collective. Central to our method is a trust weighting scheme which serves to adaptively weigh the influence of each collaborator on the pseudo-labels until a consensus on how to label the auxiliary data is reached. We demonstrate that our collaboration scheme is able to significantly boost individual model's performance with respect to the global distribution, compared to local training. At the same time, the adaptive trust weights can effectively identify and mitigate the negative impact of bad models on the collective. We find that our method is particularly effective in the presence of heterogeneity among individual agents, both in terms of training data as well as model architectures. ",
    "url": "https://arxiv.org/abs/2305.18497",
    "authors": [
      "Dongyang Fan",
      "Celestine Mendler-D\u00fcnner",
      "Martin Jaggi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18503",
    "title": "From Adversarial Arms Race to Model-centric Evaluation: Motivating a  Unified Automatic Robustness Evaluation Framework",
    "abstract": "Textual adversarial attacks can discover models' weaknesses by adding semantic-preserved but misleading perturbations to the inputs. The long-lasting adversarial attack-and-defense arms race in Natural Language Processing (NLP) is algorithm-centric, providing valuable techniques for automatic robustness evaluation. However, the existing practice of robustness evaluation may exhibit issues of incomprehensive evaluation, impractical evaluation protocol, and invalid adversarial samples. In this paper, we aim to set up a unified automatic robustness evaluation framework, shifting towards model-centric evaluation to further exploit the advantages of adversarial attacks. To address the above challenges, we first determine robustness evaluation dimensions based on model capabilities and specify the reasonable algorithm to generate adversarial samples for each dimension. Then we establish the evaluation protocol, including evaluation settings and metrics, under realistic demands. Finally, we use the perturbation degree of adversarial samples to control the sample validity. We implement a toolkit RobTest that realizes our automatic robustness evaluation framework. In our experiments, we conduct a robustness evaluation of RoBERTa models to demonstrate the effectiveness of our evaluation framework, and further show the rationality of each component in the framework. The code will be made public at \\url{https://github.com/thunlp/RobTest}. ",
    "url": "https://arxiv.org/abs/2305.18503",
    "authors": [
      "Yangyi Chen",
      "Hongcheng Gao",
      "Ganqu Cui",
      "Lifan Yuan",
      "Dehan Kong",
      "Hanlu Wu",
      "Ning Shi",
      "Bo Yuan",
      "Longtao Huang",
      "Hui Xue",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Heng Ji"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18507",
    "title": "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large  Language Models",
    "abstract": "Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show through experiments how code annotations and their locations affect code prompting. ",
    "url": "https://arxiv.org/abs/2305.18507",
    "authors": [
      "Yi Hu",
      "Haotong Yang",
      "Zhouchen Lin",
      "Muhan Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18512",
    "title": "A Rainbow in Deep Network Black Boxes",
    "abstract": "We introduce rainbow networks as a probabilistic model of trained deep neural networks. The model cascades random feature maps whose weight distributions are learned. It assumes that dependencies between weights at different layers are reduced to rotations which align the input activations. Neuron weights within a layer are independent after this alignment. Their activations define kernels which become deterministic in the infinite-width limit. This is verified numerically for ResNets trained on the ImageNet dataset. We also show that the learned weight distributions have low-rank covariances. Rainbow networks thus alternate between linear dimension reductions and non-linear high-dimensional embeddings with white random features. Gaussian rainbow networks are defined with Gaussian weight distributions. These models are validated numerically on image classification on the CIFAR-10 dataset, with wavelet scattering networks. We further show that during training, SGD updates the weight covariances while mostly preserving the Gaussian initialization. ",
    "url": "https://arxiv.org/abs/2305.18512",
    "authors": [
      "Florentin Guth",
      "Brice M\u00e9nard",
      "Gaspar Rochette",
      "St\u00e9phane Mallat"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2305.18543",
    "title": "Robust Lipschitz Bandits to Adversarial Corruptions",
    "abstract": "Lipschitz bandit is a variant of stochastic bandits that deals with a continuous arm set defined on a metric space, where the reward function is subject to a Lipschitz constraint. In this paper, we introduce a new problem of Lipschitz bandits in the presence of adversarial corruptions where an adaptive adversary corrupts the stochastic rewards up to a total budget $C$. The budget is measured by the sum of corruption levels across the time horizon $T$. We consider both weak and strong adversaries, where the weak adversary is unaware of the current action before the attack, while the strong one can observe it. Our work presents the first line of robust Lipschitz bandit algorithms that can achieve sub-linear regret under both types of adversary, even when the total budget of corruption $C$ is unrevealed to the agent. We provide a lower bound under each type of adversary, and show that our algorithm is optimal under the strong case. Finally, we conduct experiments to illustrate the effectiveness of our algorithms against two classic kinds of attacks. ",
    "url": "https://arxiv.org/abs/2305.18543",
    "authors": [
      "Yue Kang",
      "Cho-Jui Hsieh",
      "Thomas C. M. Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.18552",
    "title": "Learning Linear Groups in Neural Networks",
    "abstract": "Employing equivariance in neural networks leads to greater parameter efficiency and improved generalization performance through the encoding of domain knowledge in the architecture; however, the majority of existing approaches require an a priori specification of the desired symmetries. We present a neural network architecture, Linear Group Networks (LGNs), for learning linear groups acting on the weight space of neural networks. Linear groups are desirable due to their inherent interpretability, as they can be represented as finite matrices. LGNs learn groups without any supervision or knowledge of the hidden symmetries in the data and the groups can be mapped to well known operations in machine learning. We use LGNs to learn groups on multiple datasets while considering different downstream tasks; we demonstrate that the linear group structure depends on both the data distribution and the considered task. ",
    "url": "https://arxiv.org/abs/2305.18552",
    "authors": [
      "Emmanouil Theodosis",
      "Karim Helwani",
      "Demba Ba"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2305.18557",
    "title": "Evaluating 3D Shape Analysis Methods for Robustness to Rotation  Invariance",
    "abstract": "This paper analyzes the robustness of recent 3D shape descriptors to SO(3) rotations, something that is fundamental to shape modeling. Specifically, we formulate the task of rotated 3D object instance detection. To do so, we consider a database of 3D indoor scenes, where objects occur in different orientations. We benchmark different methods for feature extraction and classification in the context of this task. We systematically contrast different choices in a variety of experimental settings investigating the impact on the performance of different rotation distributions, different degrees of partial observations on the object, and the different levels of difficulty of negative pairs. Our study, on a synthetic dataset of 3D scenes where objects instances occur in different orientations, reveals that deep learning-based rotation invariant methods are effective for relatively easy settings with easy-to-distinguish pairs. However, their performance decreases significantly when the difference in rotations on the input pair is large, or when the degree of observation of input objects is reduced, or the difficulty level of input pair is increased. Finally, we connect feature encodings designed for rotation-invariant methods to 3D geometry that enable them to acquire the property of rotation invariance. ",
    "url": "https://arxiv.org/abs/2305.18557",
    "authors": [
      "Supriya Gadi Patil",
      "Angel X. Chang",
      "Manolis Savva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18558",
    "title": "DelBugV: Delta-Debugging Neural Network Verifiers",
    "abstract": "Deep neural networks (DNNs) are becoming a key component in diverse systems across the board. However, despite their success, they often err miserably; and this has triggered significant interest in formally verifying them. Unfortunately, DNN verifiers are intricate tools, and are themselves susceptible to soundness bugs. Due to the complexity of DNN verifiers, as well as the sizes of the DNNs being verified, debugging such errors is a daunting task. Here, we present a novel tool, named DelBugV, that uses automated delta debugging techniques on DNN verifiers. Given a malfunctioning DNN verifier and a correct verifier as a point of reference (or, in some cases, just a single, malfunctioning verifier), DelBugV can produce much simpler DNN verification instances that still trigger undesired behavior -- greatly facilitating the task of debugging the faulty verifier. Our tool is modular and extensible, and can easily be enhanced with additional network simplification methods and strategies. For evaluation purposes, we ran DelBugV on 4 DNN verification engines, which were observed to produce incorrect results at the 2021 neural network verification competition (VNN-COMP'21). We were able to simplify many of the verification queries that trigger these faulty behaviors, by as much as 99%. We regard our work as a step towards the ultimate goal of producing reliable and trustworthy DNN-based software. ",
    "url": "https://arxiv.org/abs/2305.18558",
    "authors": [
      "Raya Elsaleh",
      "Guy Katz"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18568",
    "title": "Performance of affine-splitting pseudo-spectral methods for fractional  complex Ginzburg-Landau equations",
    "abstract": "In this paper, we evaluate the performance of novel numerical methods for solving one-dimensional nonlinear fractional dispersive and dissipative evolution equations. The methods are based on affine combinations of time-splitting integrators and pseudo-spectral discretizations using Hermite and Fourier expansions. We show the effectiveness of the proposed methods by numerically computing the dynamics of soliton solutions of the the standard and fractional variants of the nonlinear Schr\\\"odinger equation (NLSE) and the complex Ginzburg-Landau equation (CGLE), and by comparing the results with those obtained by standard splitting integrators. An exhaustive numerical investigation shows that the new technique is competitive with traditional composition-splitting schemes for the case of Hamiltonian problems both in terms accuracy and computational cost. Moreover, it is applicable straightforwardly to irreversible models, outperforming high-order symplectic integrators which could become unstable due to their need of negative time steps. Finally, we discuss potential improvements of the numerical methods aimed to increase their efficiency, and possible applications to the investigation of dissipative solitons that arise in nonlinear optical systems of contemporary interest. Overall, our method offers a promising alternative for solving a wide range of evolutionary partial differential equations. ",
    "url": "https://arxiv.org/abs/2305.18568",
    "authors": [
      "Lisandro A. Raviola",
      "Mariano F. De Leo"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2305.18576",
    "title": "TreeMAN: Tree-enhanced Multimodal Attention Network for ICD Coding",
    "abstract": "ICD coding is designed to assign the disease codes to electronic health records (EHRs) upon discharge, which is crucial for billing and clinical statistics. In an attempt to improve the effectiveness and efficiency of manual coding, many methods have been proposed to automatically predict ICD codes from clinical notes. However, most previous works ignore the decisive information contained in structured medical data in EHRs, which is hard to be captured from the noisy clinical notes. In this paper, we propose a Tree-enhanced Multimodal Attention Network (TreeMAN) to fuse tabular features and textual features into multimodal representations by enhancing the text representations with tree-based features via the attention mechanism. Tree-based features are constructed according to decision trees learned from structured multimodal medical data, which capture the decisive information about ICD coding. We can apply the same multi-label classifier from previous text models to the multimodal representations to predict ICD codes. Experiments on two MIMIC datasets show that our method outperforms prior state-of-the-art ICD coding approaches. The code is available at https://github.com/liu-zichen/TreeMAN. ",
    "url": "https://arxiv.org/abs/2305.18576",
    "authors": [
      "Zichen Liu",
      "Xuyuan Liu",
      "Yanlong Wen",
      "Guoqing Zhao",
      "Fen Xia",
      "Xiaojie Yuan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18584",
    "title": "Coeditor: Leveraging Contextual Changes for Multi-round Code  Auto-editing",
    "abstract": "Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, neglecting the unique requirements of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned CodeT5 model with enhancements specifically designed for code editing tasks. We encode code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms the best code completion approach -- nearly doubling its exact-match accuracy, despite using a much smaller model -- demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively prompting the model with additional user edits. We open-source our code, data, and model weights to encourage future research and release a VSCode extension powered by our model for interactive usage. ",
    "url": "https://arxiv.org/abs/2305.18584",
    "authors": [
      "Jiayi Wei",
      "Greg Durrett",
      "Isil Dillig"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2305.18585",
    "title": "Exploiting Explainability to Design Adversarial Attacks and Evaluate  Attack Resilience in Hate-Speech Detection Models",
    "abstract": "The advent of social media has given rise to numerous ethical challenges, with hate speech among the most significant concerns. Researchers are attempting to tackle this problem by leveraging hate-speech detection and employing language models to automatically moderate content and promote civil discourse. Unfortunately, recent studies have revealed that hate-speech detection systems can be misled by adversarial attacks, raising concerns about their resilience. While previous research has separately addressed the robustness of these models under adversarial attacks and their interpretability, there has been no comprehensive study exploring their intersection. The novelty of our work lies in combining these two critical aspects, leveraging interpretability to identify potential vulnerabilities and enabling the design of targeted adversarial attacks. We present a comprehensive and comparative analysis of adversarial robustness exhibited by various hate-speech detection models. Our study evaluates the resilience of these models against adversarial attacks using explainability techniques. To gain insights into the models' decision-making processes, we employ the Local Interpretable Model-agnostic Explanations (LIME) framework. Based on the explainability results obtained by LIME, we devise and execute targeted attacks on the text by leveraging the TextAttack tool. Our findings enhance the understanding of the vulnerabilities and strengths exhibited by state-of-the-art hate-speech detection models. This work underscores the importance of incorporating explainability in the development and evaluation of such models to enhance their resilience against adversarial attacks. Ultimately, this work paves the way for creating more robust and reliable hate-speech detection systems, fostering safer online environments and promoting ethical discourse on social media platforms. ",
    "url": "https://arxiv.org/abs/2305.18585",
    "authors": [
      "Pranath Reddy Kumbam",
      "Sohaib Uddin Syed",
      "Prashanth Thamminedi",
      "Suhas Harish",
      "Ian Perera",
      "Bonnie J. Dorr"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18593",
    "title": "On Diffusion Modeling for Anomaly Detection",
    "abstract": "Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Probabilistic Model (DTPM). DTPM estimates the posterior distribution over diffusion time for a given input, enabling the identification of anomalies due to their higher posterior density at larger timesteps. We derive an analytical form for this posterior density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods perform competitively. Notably, DTPM achieves orders of magnitude faster inference time than DDPM, while outperforming it on this benchmark. These results establish diffusion-based anomaly detection as an interpretable and scalable alternative to traditional methods and recent deep-learning techniques. ",
    "url": "https://arxiv.org/abs/2305.18593",
    "authors": [
      "Victor Livernoche",
      "Vineet Jain",
      "Yashar Hezaveh",
      "Siamak Ravanbakhsh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18598",
    "title": "A Method for Studying Semantic Construal in Grammatical Constructions  with Interpretable Contextual Embedding Spaces",
    "abstract": "We study semantic construal in grammatical constructions using large language models. First, we project contextual word embeddings into three interpretable semantic spaces, each defined by a different set of psycholinguistic feature norms. We validate these interpretable spaces and then use them to automatically derive semantic characterizations of lexical items in two grammatical constructions: nouns in subject or object position within the same sentence, and the AANN construction (e.g., `a beautiful three days'). We show that a word in subject position is interpreted as more agentive than the very same word in object position, and that the nouns in the AANN construction are interpreted as more measurement-like than when in the canonical alternation. Our method can probe the distributional meaning of syntactic constructions at a templatic level, abstracted away from specific lexemes. ",
    "url": "https://arxiv.org/abs/2305.18598",
    "authors": [
      "Gabriella Chronis",
      "Kyle Mahowald",
      "Katrin Erk"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18599",
    "title": "Improving Generalization for Multimodal Fake News Detection",
    "abstract": "The increasing proliferation of misinformation and its alarming impact have motivated both industry and academia to develop approaches for fake news detection. However, state-of-the-art approaches are usually trained on datasets of smaller size or with a limited set of specific topics. As a consequence, these models lack generalization capabilities and are not applicable to real-world data. In this paper, we propose three models that adopt and fine-tune state-of-the-art multimodal transformers for multimodal fake news detection. We conduct an in-depth analysis by manipulating the input data aimed to explore models performance in realistic use cases on social media. Our study across multiple models demonstrates that these systems suffer significant performance drops against manipulated data. To reduce the bias and improve model generalization, we suggest training data augmentation to conduct more meaningful experiments for fake news detection on social media. The proposed data augmentation techniques enable models to generalize better and yield improved state-of-the-art results. ",
    "url": "https://arxiv.org/abs/2305.18599",
    "authors": [
      "Sahar Tahmasebi",
      "Sherzod Hakimov",
      "Ralph Ewerth",
      "Eric M\u00fcller-Budack"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2305.18601",
    "title": "BRIGHT: Bi-level Feature Representation of Image Collections using  Groups of Hash Tables",
    "abstract": "We present BRIGHT, a bi-levelfeature representation for an imagecollection, consisting of a per-image latent space on top of a multi-scale feature grid space. Our representation is learned by an autoencoder to encode images intocontinuouskey codes, which are used to retrieve features fromgroups of multi-resolution hashtables. Our key codes and hash tables are trained together continuously with well-defined gradient flows, leading to high usage of the hash table entries and improved generative modeling compared to discrete Vector Quantization (VQ). Differently from existing continuous representations such as KL-regularized latent codes, our key codes are strictly bounded in scale and variance. Overall, feature encoding by BRIGHT is compact, efficient to train, and enables generative modeling over the image codes using state-of-the-art generators such as latent diffusion models(LDMs). Experimental results show that our method achieves comparable recon-struction results to VQ methods while having a smaller and more efficient decoder network. By applying LDM over our key code space, we achieve state-of-the-art performance on image synthesis on the LSUN-Church and human-face datasets. ",
    "url": "https://arxiv.org/abs/2305.18601",
    "authors": [
      "Dingdong Yang",
      "Yizhi Wang",
      "Ali Mahdavi-Amiri",
      "Hao Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18602",
    "title": "From `Snippet-lects' to Doculects and Dialects: Leveraging Neural  Representations of Speech for Placing Audio Signals in a Language Landscape",
    "abstract": "XLSR-53 a multilingual model of speech, builds a vector representation from audio, which allows for a range of computational treatments. The experiments reported here use this neural representation to estimate the degree of closeness between audio files, ultimately aiming to extract relevant linguistic properties. We use max-pooling to aggregate the neural representations from a \"snippet-lect\" (the speech in a 5-second audio snippet) to a \"doculect\" (the speech in a given resource), then to dialects and languages. We use data from corpora of 11 dialects belonging to 5 less-studied languages. Similarity measurements between the 11 corpora bring out greatest closeness between those that are known to be dialects of the same language. The findings suggest that (i) dialect/language can emerge among the various parameters characterizing audio files and (ii) estimates of overall phonetic/phonological closeness can be obtained for a little-resourced or fully unknown language. The findings help shed light on the type of information captured by neural representations of speech and how it can be extracted from these representations ",
    "url": "https://arxiv.org/abs/2305.18602",
    "authors": [
      "S\u00e9verine Guillaume",
      "Guillaume Wisniewski",
      "Alexis Michaud"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2305.18607",
    "title": "How Effective Are Neural Networks for Fixing Security Vulnerabilities",
    "abstract": "Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. (2) Fine-tuning with general APR data improves LLMs' vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes 8.3 transformed vulnerabilities, outperforming all the other LLMs and APR models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning LLMs with such data, and applying code simplification transformation to facilitate vulnerability repair. ",
    "url": "https://arxiv.org/abs/2305.18607",
    "authors": [
      "Yi Wu",
      "Nan Jiang",
      "Hung Viet Pham",
      "Thibaud Lutellier",
      "Jordan Davis",
      "Lin Tan",
      "Petr Babkin",
      "Sameena Shah"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2305.18612",
    "title": "Networked Time Series Imputation via Position-aware Graph Enhanced  Variational Autoencoders",
    "abstract": "Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new model named PoGeVon which leverages variational autoencoder (VAE) to predict missing values over both node time series features and graph structures. In particular, we propose a new node position embedding based on random walk with restart (RWR) in the encoder with provable higher expressive power compared with message-passing based graph neural networks (GNNs). We further design a decoder with 3-stage predictions from the perspective of multi-task learning to impute missing values in both time series and graph structures reciprocally. Experiment results demonstrate the effectiveness of our model over baselines. ",
    "url": "https://arxiv.org/abs/2305.18612",
    "authors": [
      "Dingsu Wang",
      "Yuchen Yan",
      "Ruizhong Qiu",
      "Yada Zhu",
      "Kaiyu Guan",
      "Andrew J Margenot",
      "Hanghang Tong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18622",
    "title": "Instant Representation Learning for Recommendation over Large Dynamic  Graphs",
    "abstract": "Recommender systems are able to learn user preferences based on user and item representations via their historical behaviors. To improve representation learning, recent recommendation models start leveraging information from various behavior types exhibited by users. In real-world scenarios, the user behavioral graph is not only multiplex but also dynamic, i.e., the graph evolves rapidly over time, with various types of nodes and edges added or deleted, which causes the Neighborhood Disturbance. Nevertheless, most existing methods neglect such streaming dynamics and thus need to be retrained once the graph has significantly evolved, making them unsuitable in the online learning environment. Furthermore, the Neighborhood Disturbance existing in dynamic graphs deteriorates the performance of neighbor-aggregation based graph models. To this end, we propose SUPA, a novel graph neural network for dynamic multiplex heterogeneous graphs. Compared to neighbor-aggregation architecture, SUPA develops a sample-update-propagate architecture to alleviate neighborhood disturbance. Specifically, for each new edge, SUPA samples an influenced subgraph, updates the representations of the two interactive nodes, and propagates the interaction information to the sampled subgraph. Furthermore, to train SUPA incrementally online, we propose InsLearn, an efficient workflow for single-pass training of large dynamic graphs. Extensive experimental results on six real-world datasets show that SUPA has a good generalization ability and is superior to sixteen state-of-the-art baseline methods. The source code is available at https://github.com/shatter15/SUPA. ",
    "url": "https://arxiv.org/abs/2305.18622",
    "authors": [
      "Cheng Wu",
      "Chaokun Wang",
      "Jingcao Xu",
      "Ziwei Fang",
      "Tiankai Gu",
      "Changping Wang",
      "Yang Song",
      "Kai Zheng",
      "Xiaowei Wang",
      "Guorui Zhou"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18632",
    "title": "Graph Rewriting for Graph Neural Networks",
    "abstract": "Given graphs as input, Graph Neural Networks (GNNs) support the inference of nodes, edges, attributes, or graph properties. Graph Rewriting investigates the rule-based manipulation of graphs to model complex graph transformations. We propose that, therefore, (i) graph rewriting subsumes GNNs and could serve as formal model to study and compare them, and (ii) the representation of GNNs as graph rewrite systems can help to design and analyse GNNs, their architectures and algorithms. Hence we propose Graph Rewriting Neural Networks (GReNN) as both novel semantic foundation and engineering discipline for GNNs. We develop a case study reminiscent of a Message Passing Neural Network realised as a Groove graph rewriting model and explore its incremental operation in response to dynamic updates. ",
    "url": "https://arxiv.org/abs/2305.18632",
    "authors": [
      "Adam Machowczyk",
      "Reiko Heckel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2305.18651",
    "title": "UMD: Unsupervised Model Detection for X2X Backdoor Attacks",
    "abstract": "Backdoor (Trojan) attack is a common threat to deep neural networks, where samples from one or more source classes embedded with a backdoor trigger will be misclassified to adversarial target classes. Existing methods for detecting whether a classifier is backdoor attacked are mostly designed for attacks with a single adversarial target (e.g., all-to-one attack). To the best of our knowledge, without supervision, no existing methods can effectively address the more general X2X attack with an arbitrary number of source classes, each paired with an arbitrary target class. In this paper, we propose UMD, the first Unsupervised Model Detection method that effectively detects X2X backdoor attacks via a joint inference of the adversarial (source, target) class pairs. In particular, we first define a novel transferability statistic to measure and select a subset of putative backdoor class pairs based on a proposed clustering approach. Then, these selected class pairs are jointly assessed based on an aggregation of their reverse-engineered trigger size for detection inference, using a robust and unsupervised anomaly detector we proposed. We conduct comprehensive evaluations on CIFAR-10, GTSRB, and Imagenette dataset, and show that our unsupervised UMD outperforms SOTA detectors (even with supervision) by 17%, 4%, and 8%, respectively, in terms of the detection accuracy against diverse X2X attacks. We also show the strong detection performance of UMD against several strong adaptive attacks. ",
    "url": "https://arxiv.org/abs/2305.18651",
    "authors": [
      "Zhen Xiang",
      "Zidi Xiong",
      "Bo Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18657",
    "title": "Representation Of Lexical Stylistic Features In Language Models'  Embedding Space",
    "abstract": "The representation space built by pretrained Language Models (LMs) encodes rich information about words and their relationships (e.g., similarity, hypernymy/hyponymy, polysemy) as well as abstract semantic notions (e.g., intensity). In this paper, we demonstrate that lexical stylistic notions such as complexity, formality, and figurativeness, can also be identified in this space. We show that it is possible to derive a vector representation for each of these stylistic notions, from only a small number of seed text pairs. Using these vectors, we can characterize new texts in terms of these dimensions using simple calculations in the corresponding embedding space. We perform experiments on five datasets and find that static embeddings encode these features more accurately at the level of words and phrases, whereas contextualized LMs perform better on longer texts. The lower performance of contextualized representations at the word level is partially attributable to the anisotropy of their vector space, which can be corrected through techniques like standardization to further improve performance. ",
    "url": "https://arxiv.org/abs/2305.18657",
    "authors": [
      "Qing Lyu",
      "Marianna Apidianaki",
      "Chris Callison-Burch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18665",
    "title": "E-PANNs: Sound Recognition Using Efficient Pre-trained Audio Neural  Networks",
    "abstract": "Sounds carry an abundance of information about activities and events in our everyday environment, such as traffic noise, road works, music, or people talking. Recent machine learning methods, such as convolutional neural networks (CNNs), have been shown to be able to automatically recognize sound activities, a task known as audio tagging. One such method, pre-trained audio neural networks (PANNs), provides a neural network which has been pre-trained on over 500 sound classes from the publicly available AudioSet dataset, and can be used as a baseline or starting point for other tasks. However, the existing PANNs model has a high computational complexity and large storage requirement. This could limit the potential for deploying PANNs on resource-constrained devices, such as on-the-edge sound sensors, and could lead to high energy consumption if many such devices were deployed. In this paper, we reduce the computational complexity and memory requirement of the PANNs model by taking a pruning approach to eliminate redundant parameters from the PANNs model. The resulting Efficient PANNs (E-PANNs) model, which requires 36\\% less computations and 70\\% less memory, also slightly improves the sound recognition (audio tagging) performance. The code for the E-PANNs model has been released under an open source license. ",
    "url": "https://arxiv.org/abs/2305.18665",
    "authors": [
      "Arshdeep Singh",
      "Haohe Liu",
      "Mark D. Plumbley"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2305.18667",
    "title": "Lost at Sea: Assessment and Evaluation of Rootkit Attacks on Shipboard  Microgrids",
    "abstract": "Increased dependence of the maritime industry on information and communication networks has made shipboard power systems vulnerable to stealthy cyber-attacks. One such attack variant, called rootkit, can leverage system knowledge to hide its presence and allow remotely located malware handlers to gain complete control of infected subsystems. This paper presents a comprehensive evaluation of the threat landscape imposed by such attack variants on Medium Voltage DC (MVDC) shipboard microgrids, including a discussion of their impact on the overall maritime sector in general, and provides several simulation results to demonstrate the same. It also analyzes and presents the actions of possible defense mechanisms, with specific emphasis on evasion, deception, and detection frameworks, that will help ship operators and maritime cybersecurity professionals protect their systems from such attacks. ",
    "url": "https://arxiv.org/abs/2305.18667",
    "authors": [
      "Suman Rath",
      "Andres Intriago",
      "Shamik Sengupta",
      "Charalambos Konstantinou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2305.18668",
    "title": "Fine-Grained is Too Coarse: A Novel Data-Centric Approach for Efficient  Scene Graph Generation",
    "abstract": "Learning to compose visual relationships from raw images in the form of scene graphs is a highly challenging task due to contextual dependencies, but it is essential in computer vision applications that depend on scene understanding. However, no current approaches in Scene Graph Generation (SGG) aim at providing useful graphs for downstream tasks. Instead, the main focus has primarily been on the task of unbiasing the data distribution for predicting more fine-grained relations. That being said, all fine-grained relations are not equally relevant and at least a part of them are of no use for real-world applications. In this work, we introduce the task of Efficient SGG that prioritizes the generation of relevant relations, facilitating the use of Scene Graphs in downstream tasks such as Image Generation. To support further approaches in this task, we present a new dataset, VG150-curated, based on the annotations of the popular Visual Genome dataset. We show through a set of experiments that this dataset contains more high-quality and diverse annotations than the one usually adopted by approaches in SGG. Finally, we show the efficiency of this dataset in the task of Image Generation from Scene Graphs. Our approach can be easily replicated to improve the quality of other Scene Graph Generation datasets. ",
    "url": "https://arxiv.org/abs/2305.18668",
    "authors": [
      "Neau Ma\u00eblic",
      "Paulo Santos",
      "Anne-Gwenn Bosser",
      "C\u00e9dric Buche"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18675",
    "title": "History Repeats: Overcoming Catastrophic Forgetting For Event-Centric  Temporal Knowledge Graph Completion",
    "abstract": "Temporal knowledge graph (TKG) completion models typically rely on having access to the entire graph during training. However, in real-world scenarios, TKG data is often received incrementally as events unfold, leading to a dynamic non-stationary data distribution over time. While one could incorporate fine-tuning to existing methods to allow them to adapt to evolving TKG data, this can lead to forgetting previously learned patterns. Alternatively, retraining the model with the entire updated TKG can mitigate forgetting but is computationally burdensome. To address these challenges, we propose a general continual training framework that is applicable to any TKG completion method, and leverages two key ideas: (i) a temporal regularization that encourages repurposing of less important model parameters for learning new knowledge, and (ii) a clustering-based experience replay that reinforces the past knowledge by selectively preserving only a small portion of the past data. Our experimental results on widely used event-centric TKG datasets demonstrate the effectiveness of our proposed continual training framework in adapting to new events while reducing catastrophic forgetting. Further, we perform ablation studies to show the effectiveness of each component of our proposed framework. Finally, we investigate the relation between the memory dedicated to experience replay and the benefit gained from our clustering-based sampling strategy. ",
    "url": "https://arxiv.org/abs/2305.18675",
    "authors": [
      "Mehrnoosh Mirtaheri",
      "Mohammad Rostami",
      "Aram Galstyan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18680",
    "title": "Improving Deep Representation Learning via Auxiliary Learnable Target  Coding",
    "abstract": "Deep representation learning is a subfield of machine learning that focuses on learning meaningful and useful representations of data through deep neural networks. However, existing methods for semantic classification typically employ pre-defined target codes such as the one-hot and the Hadamard codes, which can either fail or be less flexible to model inter-class correlation. In light of this, this paper introduces a novel learnable target coding as an auxiliary regularization of deep representation learning, which can not only incorporate latent dependency across classes but also impose geometric properties of target codes into representation space. Specifically, a margin-based triplet loss and a correlation consistency loss on the proposed target codes are designed to encourage more discriminative representations owing to enlarging between-class margins in representation space and favoring equal semantic correlation of learnable target codes respectively. Experimental results on several popular visual classification and retrieval benchmarks can demonstrate the effectiveness of our method on improving representation learning, especially for imbalanced data. ",
    "url": "https://arxiv.org/abs/2305.18680",
    "authors": [
      "Kangjun Liu",
      "Ke Chen",
      "Yaowei Wang",
      "Kui Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18687",
    "title": "Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic  Forecasting",
    "abstract": "There is a recent surge in the development of spatio-temporal forecasting models in the transportation domain. Long-range traffic forecasting, however, remains a challenging task due to the intricate and extensive spatio-temporal correlations observed in traffic networks. Current works primarily rely on road networks with graph structures and learn representations using graph neural networks (GNNs), but this approach suffers from over-smoothing problem in deep architectures. To tackle this problem, recent methods introduced the combination of GNNs with residual connections or neural ordinary differential equations (ODE). However, current graph ODE models face two key limitations in feature extraction: (1) they lean towards global temporal patterns, overlooking local patterns that are important for unexpected events; and (2) they lack dynamic semantic edges in their architectural design. In this paper, we propose a novel architecture called Graph-based Multi-ODE Neural Networks (GRAM-ODE) which is designed with multiple connective ODE-GNN modules to learn better representations by capturing different views of complex local and global dynamic spatio-temporal dependencies. We also add some techniques like shared weights and divergence constraints into the intermediate layers of distinct ODE-GNN modules to further improve their communication towards the forecasting task. Our extensive set of experiments conducted on six real-world datasets demonstrate the superior performance of GRAM-ODE compared with state-of-the-art baselines as well as the contribution of different components to the overall performance. The code is available at https://github.com/zbliu98/GRAM-ODE ",
    "url": "https://arxiv.org/abs/2305.18687",
    "authors": [
      "Zibo Liu",
      "Parshin Shojaee",
      "Chandan K. Reddy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18698",
    "title": "AutoMM: Energy-Efficient Multi-Data-Type Matrix Multiply Design on  Heterogeneous Programmable System-on-Chip",
    "abstract": "As the increasing complexity of Neural Network(NN) models leads to high demands for computation, AMD introduces a heterogeneous programmable system-on-chip (SoC), i.e., Versal ACAP architectures featured with programmable logic (PL), CPUs, and dedicated AI engines (AIE) ASICs which has a theoretical throughput up to 6.4 TFLOPs for FP32, 25.6 TOPs for INT16 and 102.4 TOPs for INT8. However, the higher level of complexity makes it non-trivial to achieve the theoretical performance even for well-studied applications like matrix-matrix multiply. In this paper, we provide AutoMM, an automatic white-box framework that can systematically generate the design for MM accelerators on Versal which achieves 3.7 TFLOPs, 7.5 TOPs, and 28.2 TOPs for FP32, INT16, and INT8 data type respectively. Our designs are tested on board and achieve gains of 7.20x (FP32), 3.26x (INT16), 6.23x (INT8) energy efficiency than AMD U250 FPGA, 2.32x (FP32) than Nvidia Jetson TX2 GPU, 1.06x (FP32), 1.70x (INT8) than Nvidia A100 GPU. ",
    "url": "https://arxiv.org/abs/2305.18698",
    "authors": [
      "Jinming Zhuang",
      "Zhuoping Yang",
      "Peipei Zhou"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2305.18706",
    "title": "HQDec: Self-Supervised Monocular Depth Estimation Based on a  High-Quality Decoder",
    "abstract": "Decoders play significant roles in recovering scene depths. However, the decoders used in previous works ignore the propagation of multilevel lossless fine-grained information, cannot adaptively capture local and global information in parallel, and cannot perform sufficient global statistical analyses on the final output disparities. In addition, the process of mapping from a low-resolution feature space to a high-resolution feature space is a one-to-many problem that may have multiple solutions. Therefore, the quality of the recovered depth map is low. To this end, we propose a high-quality decoder (HQDec), with which multilevel near-lossless fine-grained information, obtained by the proposed adaptive axial-normalized position-embedded channel attention sampling module (AdaAxialNPCAS), can be adaptively incorporated into a low-resolution feature map with high-level semantics utilizing the proposed adaptive information exchange scheme. In the HQDec, we leverage the proposed adaptive refinement module (AdaRM) to model the local and global dependencies between pixels in parallel and utilize the proposed disparity attention module to model the distribution characteristics of disparity values from a global perspective. To recover fine-grained high-resolution features with maximal accuracy, we adaptively fuse the high-frequency information obtained by constraining the upsampled solution space utilizing the local and global dependencies between pixels into the high-resolution feature map generated from the nonlearning method. Extensive experiments demonstrate that each proposed component improves the quality of the depth estimation results over the baseline results, and the developed approach achieves state-of-the-art results on the KITTI and DDAD datasets. The code and models will be publicly available at \\href{https://github.com/fwucas/HQDec}{HQDec}. ",
    "url": "https://arxiv.org/abs/2305.18706",
    "authors": [
      "Fei Wang",
      "Jun Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18710",
    "title": "High-Performance Inference Graph Convolutional Networks for  Skeleton-Based Action Recognition",
    "abstract": "Recently, significant achievements have been made in skeleton-based human action recognition with the emergence of graph convolutional networks (GCNs). However, the state-of-the-art (SOTA) models used for this task focus on constructing more complex higher-order connections between joint nodes to describe skeleton information, which leads to complex inference processes and high computational costs, resulting in reduced model's practicality. To address the slow inference speed caused by overly complex model structures, we introduce re-parameterization and over-parameterization techniques to GCNs, and propose two novel high-performance inference graph convolutional networks, namely HPI-GCN-RP and HPI-GCN-OP. HPI-GCN-RP uses re-parameterization technique to GCNs to achieve a higher inference speed with competitive model performance. HPI-GCN-OP further utilizes over-parameterization technique to bring significant performance improvement with inference speed slightly decreased. Experimental results on the two skeleton-based action recognition datasets demonstrate the effectiveness of our approach. Our HPI-GCN-OP achieves an accuracy of 93% on the cross-subject split of the NTU-RGB+D 60 dataset, and 90.1% on the cross-subject benchmark of the NTU-RGB+D 120 dataset and is 4.5 times faster than HD-GCN at the same accuracy. ",
    "url": "https://arxiv.org/abs/2305.18710",
    "authors": [
      "Ziao Li",
      "Junyi Wang",
      "Guhong Nie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18714",
    "title": "Align, Perturb and Decouple: Toward Better Leverage of Difference  Information for RSI Change Detection",
    "abstract": "Change detection is a widely adopted technique in remote sense imagery (RSI) analysis in the discovery of long-term geomorphic evolution. To highlight the areas of semantic changes, previous effort mostly pays attention to learning representative feature descriptors of a single image, while the difference information is either modeled with simple difference operations or implicitly embedded via feature interactions. Nevertheless, such difference modeling can be noisy since it suffers from non-semantic changes and lacks explicit guidance from image content or context. In this paper, we revisit the importance of feature difference for change detection in RSI, and propose a series of operations to fully exploit the difference information: Alignment, Perturbation and Decoupling (APD). Firstly, alignment leverages contextual similarity to compensate for the non-semantic difference in feature space. Next, a difference module trained with semantic-wise perturbation is adopted to learn more generalized change estimators, which reversely bootstraps feature extraction and prediction. Finally, a decoupled dual-decoder structure is designed to predict semantic changes in both content-aware and content-agnostic manners. Extensive experiments are conducted on benchmarks of LEVIR-CD, WHU-CD and DSIFN-CD, demonstrating our proposed operations bring significant improvement and achieve competitive results under similar comparative conditions. Code is available at https://github.com/wangsp1999/CD-Research/tree/main/openAPD ",
    "url": "https://arxiv.org/abs/2305.18714",
    "authors": [
      "Supeng Wang",
      "Yuxi Li",
      "Ming Xie",
      "Mingmin Chi",
      "Yabiao Wang",
      "Chengjie Wang",
      "Wenbing Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18719",
    "title": "Graph Neural Processes for Spatio-Temporal Extrapolation",
    "abstract": "We study the task of spatio-temporal extrapolation that generates data at target locations from surrounding contexts in a graph. This task is crucial as sensors that collect data are sparsely deployed, resulting in a lack of fine-grained information due to high deployment and maintenance costs. Existing methods either use learning-based models like Neural Networks or statistical approaches like Gaussian Processes for this task. However, the former lacks uncertainty estimates and the latter fails to capture complex spatial and temporal correlations effectively. To address these issues, we propose Spatio-Temporal Graph Neural Processes (STGNP), a neural latent variable model which commands these capabilities simultaneously. Specifically, we first learn deterministic spatio-temporal representations by stacking layers of causal convolutions and cross-set graph neural networks. Then, we learn latent variables for target locations through vertical latent state transitions along layers and obtain extrapolations. Importantly during the transitions, we propose Graph Bayesian Aggregation (GBA), a Bayesian graph aggregator that aggregates contexts considering uncertainties in context data and graph structure. Extensive experiments show that STGNP has desirable properties such as uncertainty estimates and strong learning capabilities, and achieves state-of-the-art results by a clear margin. ",
    "url": "https://arxiv.org/abs/2305.18719",
    "authors": [
      "Junfeng Hu",
      "Yuxuan Liang",
      "Zhencheng Fan",
      "Hongyang Chen",
      "Yu Zheng",
      "Roger Zimmermann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18727",
    "title": "An Annotated Dataset for Explainable Interpersonal Risk Factors of  Mental Disturbance in Social Media Posts",
    "abstract": "With a surge in identifying suicidal risk and its severity in social media posts, we argue that a more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare. The success of computational intelligence techniques for inferring mental illness from social media resources, points to natural language processing as a lens for determining Interpersonal Risk Factors (IRF) in human writings. Motivated with limited availability of datasets for social NLP research community, we construct and release a new annotated dataset with human-labelled explanations and classification of IRF affecting mental disturbance on social media: (i) Thwarted Belongingness (TBe), and (ii) Perceived Burdensomeness (PBu). We establish baseline models on our dataset facilitating future research directions to develop real-time personalized AI models by detecting patterns of TBe and PBu in emotional spectrum of user's historical social media profile. ",
    "url": "https://arxiv.org/abs/2305.18727",
    "authors": [
      "Muskan Garg",
      "Amirmohammad Shahbandegan",
      "Amrit Chadha",
      "Vijay Mago"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2305.18731",
    "title": "Hybrid Representation Learning via Epistemic Graph",
    "abstract": "In recent years, deep models have achieved remarkable success in many vision tasks. Unfortunately, their performance largely depends on intensive training samples. In contrast, human beings typically perform hybrid learning, e.g., spontaneously integrating structured knowledge for cross-domain recognition or on a much smaller amount of data samples for few-shot learning. Thus it is very attractive to extend hybrid learning for the computer vision tasks by seamlessly integrating structured knowledge with data samples to achieve more effective representation learning. However, such a hybrid learning approach remains a great challenge due to the huge gap between the structured knowledge and the deep features (learned from data samples) on both dimensions and knowledge granularity. In this paper, a novel Epistemic Graph Layer (EGLayer) is developed to enable hybrid learning, such that the information can be exchanged more effectively between the deep features and a structured knowledge graph. Our EGLayer is composed of three major parts: (a) a local graph module to establish a local prototypical graph through the learned deep features, i.e., aligning the deep features with the structured knowledge graph at the same granularity; (b) a query aggregation model to aggregate useful information from the local graphs, and using such representations to compute their similarity with global node embeddings for final prediction; and (c) a novel correlation loss function to constrain the linear consistency between the local and global adjacency matrices. ",
    "url": "https://arxiv.org/abs/2305.18731",
    "authors": [
      "Jin Yuan",
      "Yang Zhang",
      "Yangzhou Du",
      "Zhongchao Shi",
      "Xin Geng",
      "Jianping Fan",
      "Yong Rui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18742",
    "title": "Graph Reasoning for Question Answering with Triplet Retrieval",
    "abstract": "Answering complex questions often requires reasoning over knowledge graphs (KGs). State-of-the-art methods often utilize entities in questions to retrieve local subgraphs, which are then fed into KG encoder, e.g. graph neural networks (GNNs), to model their local structures and integrated into language models for question answering. However, this paradigm constrains retrieved knowledge in local subgraphs and discards more diverse triplets buried in KGs that are disconnected but useful for question answering. In this paper, we propose a simple yet effective method to first retrieve the most relevant triplets from KGs and then rerank them, which are then concatenated with questions to be fed into language models. Extensive results on both CommonsenseQA and OpenbookQA datasets show that our method can outperform state-of-the-art up to 4.6% absolute accuracy. ",
    "url": "https://arxiv.org/abs/2305.18742",
    "authors": [
      "Shiyang Li",
      "Yifan Gao",
      "Haoming Jiang",
      "Qingyu Yin",
      "Zheng Li",
      "Xifeng Yan",
      "Chao Zhang",
      "Bing Yin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18743",
    "title": "Decomposed Human Motion Prior for Video Pose Estimation via Adversarial  Training",
    "abstract": "Estimating human pose from video is a task that receives considerable attention due to its applicability in numerous 3D fields. The complexity of prior knowledge of human body movements poses a challenge to neural network models in the task of regressing keypoints. In this paper, we address this problem by incorporating motion prior in an adversarial way. Different from previous methods, we propose to decompose holistic motion prior to joint motion prior, making it easier for neural networks to learn from prior knowledge thereby boosting the performance on the task. We also utilize a novel regularization loss to balance accuracy and smoothness introduced by motion prior. Our method achieves 9\\% lower PA-MPJPE and 29\\% lower acceleration error than previous methods tested on 3DPW. The estimator proves its robustness by achieving impressive performance on in-the-wild dataset. ",
    "url": "https://arxiv.org/abs/2305.18743",
    "authors": [
      "Wenshuo Chen",
      "Xiang Zhou",
      "Zhengdi Yu",
      "Zhaoyu Zheng",
      "Weixi Gu",
      "Kai Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18745",
    "title": "Multi-objective Anti-swing Trajectory Planning of Double-pendulum Tower  Crane Operations using Opposition-based Evolutionary Algorithm",
    "abstract": "Underactuated tower crane lifting requires time-energy optimal trajectories for the trolley/slew operations and reduction of the unactuated swings resulting from the trolley/jib motion. In scenarios involving non-negligible hook mass or long rig-cable, the hook-payload unit exhibits double-pendulum behaviour, making the problem highly challenging. This article introduces an offline multi-objective anti-swing trajectory planning module for a Computer-Aided Lift Planning (CALP) system of autonomous double-pendulum tower cranes, addressing all the transient state constraints. A set of auxiliary outputs are selected by methodically analyzing the payload swing dynamics and are used to prove the differential flatness property of the crane operations. The flat outputs are parameterized via suitable B\\'{e}zier curves to formulate the multi-objective trajectory optimization problems in the flat output space. A novel multi-objective evolutionary algorithm called Collective Oppositional Generalized Differential Evolution 3 (CO-GDE3) is employed as the optimizer. To obtain faster convergence and better consistency in getting a wide range of good solutions, a new population initialization strategy is integrated into the conventional GDE3. The computationally efficient initialization method incorporates various concepts of computational opposition. Statistical comparisons based on trolley and slew operations verify the superiority of convergence and reliability of CO-GDE3 over the standard GDE3. Trolley and slew operations of a collision-free lifting path computed via the path planner of the CALP system are selected for a simulation study. The simulated trajectories demonstrate that the proposed planner can produce time-energy optimal solutions, keeping all the state variables within their respective limits and restricting the hook and payload swings. ",
    "url": "https://arxiv.org/abs/2305.18745",
    "authors": [
      "Souravik Dutta",
      "Yiyu Cai",
      "Jianmin Zheng"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2305.18758",
    "title": "Task-Equivariant Graph Few-shot Learning",
    "abstract": "Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strategies using a limited number of training meta-tasks, allowing it to acquire meta-knowledge for a wide range of meta-tasks. By incorporating equivariant neural networks, TEG can utilize their strong generalization abilities to learn highly adaptable task-specific strategies. As a result, TEG achieves state-of-the-art performance with limited training meta-tasks. Our experiments on various benchmark datasets demonstrate TEG's superiority in terms of accuracy and generalization ability, even when using minimal meta-training data, highlighting the effectiveness of our proposed approach in addressing the challenges of meta-learning based few-shot node classification. Our code is available at the following link: https://github.com/sung-won-kim/TEG ",
    "url": "https://arxiv.org/abs/2305.18758",
    "authors": [
      "Sungwon Kim",
      "Junseok Lee",
      "Namkyeong Lee",
      "Wonjoong Kim",
      "Seungyoon Choi",
      "Chanyoung Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18771",
    "title": "SFCNeXt: a simple fully convolutional network for effective brain age  estimation with small sample size",
    "abstract": "Deep neural networks (DNN) have been designed to predict the chronological age of a healthy brain from T1-weighted magnetic resonance images (T1 MRIs), and the predicted brain age could serve as a valuable biomarker for the early detection of development-related or aging-related disorders. Recent DNN models for brain age estimations usually rely too much on large sample sizes and complex network structures for multi-stage feature refinement. However, in clinical application scenarios, researchers usually cannot obtain thousands or tens of thousands of MRIs in each data center for thorough training of these complex models. This paper proposes a simple fully convolutional network (SFCNeXt) for brain age estimation in small-sized cohorts with biased age distributions. The SFCNeXt consists of Single Pathway Encoded ConvNeXt (SPEC) and Hybrid Ranking Loss (HRL), aiming to estimate brain ages in a lightweight way with a sufficient exploration of MRI, age, and ranking features of each batch of subjects. Experimental results demonstrate the superiority and efficiency of our approach. ",
    "url": "https://arxiv.org/abs/2305.18771",
    "authors": [
      "Yu Fu",
      "Yanyan Huang",
      "Shunjie Dong",
      "Yalin Wang",
      "Tianbai Yu",
      "Meng Niu",
      "Cheng Zhuo"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.18773",
    "title": "On a neural network approach for solving potential control problem of  the semiclassical Schr\u00f6dinger equation",
    "abstract": "Robust control design for quantum systems is a challenging and key task for practical technology. In this work, we apply neural networks to learn the control problem for the semiclassical Schr\\\"odinger equation, where the control variable is the potential given by an external field that may contain uncertainties. Inspired by a relevant work [29], we incorporate the sampling-based learning process into the training of networks, while combining with the fast time-splitting spectral method for the Schr\\\"odinger equation in the semiclassical regime. The numerical results have shown the efficiency and accuracy of our proposed deep learning approach. ",
    "url": "https://arxiv.org/abs/2305.18773",
    "authors": [
      "Yating Wang",
      "Liu Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2305.18774",
    "title": "Bayesian Decision Trees Inspired from Evolutionary Algorithms",
    "abstract": "Bayesian Decision Trees (DTs) are generally considered a more advanced and accurate model than a regular Decision Tree (DT) because they can handle complex and uncertain data. Existing work on Bayesian DTs uses Markov Chain Monte Carlo (MCMC) with an accept-reject mechanism and sample using naive proposals to proceed to the next iteration, which can be slow because of the burn-in time needed. We can reduce the burn-in period by proposing a more sophisticated way of sampling or by designing a different numerical Bayesian approach. In this paper, we propose a replacement of the MCMC with an inherently parallel algorithm, the Sequential Monte Carlo (SMC), and a more effective sampling strategy inspired by the Evolutionary Algorithms (EA). Experiments show that SMC combined with the EA can produce more accurate results compared to MCMC in 100 times fewer iterations. ",
    "url": "https://arxiv.org/abs/2305.18774",
    "authors": [
      "Efthyvoulos Drousiotis",
      "Alexander M. Phillips",
      "Paul G. Spirakis",
      "Simon Maskell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2305.18777",
    "title": "Adaptive Conditional Quantile Neural Processes",
    "abstract": "Neural processes are a family of probabilistic models that inherit the flexibility of neural networks to parameterize stochastic processes. Despite providing well-calibrated predictions, especially in regression problems, and quick adaptation to new tasks, the Gaussian assumption that is commonly used to represent the predictive likelihood fails to capture more complicated distributions such as multimodal ones. To overcome this limitation, we propose Conditional Quantile Neural Processes (CQNPs), a new member of the neural processes family, which exploits the attractive properties of quantile regression in modeling the distributions irrespective of their form. By introducing an extension of quantile regression where the model learns to focus on estimating informative quantiles, we show that the sampling efficiency and prediction accuracy can be further enhanced. Our experiments with real and synthetic datasets demonstrate substantial improvements in predictive performance compared to the baselines, and better modeling of heterogeneous distributions' characteristics such as multimodality. ",
    "url": "https://arxiv.org/abs/2305.18777",
    "authors": [
      "Peiman Mohseni",
      "Nick Duffield",
      "Bani Mallick",
      "Arman Hasanzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.18778",
    "title": "CN2F: A Cloud-Native Cellular Network Framework",
    "abstract": "Upcoming 5G and Beyond 5G (B5G) cellular networks aim to improve the efficiency and flexibility of mobile networks by incorporating various technologies, such as Software Defined Networking (SDN), Network Function Virtualization (NFV), and Network Slicing (NS). In this paper, we share our findings, accompanied by a comprehensive online codebase, about the best practice of using different open-source projects in order to realize a flexible testbed for academia and industrial Research and Development (R&D) activities on the future generation of cellular networks. In particular, a Cloud-Native Cellular Network Framework (CN2F) is presented which uses OpenAirInterface's codebase to generate cellular Virtual Network Functions (VNFs) and deploys Kubernetes to disperse and manage them among some worker nodes. Moreover, CN2F leverages ONOS and Mininet to emulate the effect of the IP transport networks in the fronthaul and backhaul of real cellular networks. In this paper, we also showcase two use cases of CN2F to demonstrate the importance of Edge Computing (EC) and the capability of Radio Access Network (RAN) slicing. ",
    "url": "https://arxiv.org/abs/2305.18778",
    "authors": [
      "Sepehr Ganji",
      "Shirin Behnaminia",
      "Ali Ahangarpour",
      "Erfan Mazaheri",
      "Sara Baradaran",
      "Zeinab Zali",
      "Mohammad Reza Heidarpour",
      "Ali Rakhshan",
      "Mahsa Faraji Shoyari"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2305.18779",
    "title": "It begins with a boundary: A geometric view on probabilistically robust  learning",
    "abstract": "Although deep neural networks have achieved super-human performance on many classification tasks, they often exhibit a worrying lack of robustness towards adversarially generated examples. Thus, considerable effort has been invested into reformulating Empirical Risk Minimization (ERM) into an adversarially robust framework. Recently, attention has shifted towards approaches which interpolate between the robustness offered by adversarial training and the higher clean accuracy and faster training times of ERM. In this paper, we take a fresh and geometric view on one such method -- Probabilistically Robust Learning (PRL) (Robey et al., ICML, 2022). We propose a geometric framework for understanding PRL, which allows us to identify a subtle flaw in its original formulation and to introduce a family of probabilistic nonlocal perimeter functionals to address this. We prove existence of solutions using novel relaxation methods and study properties as well as local limits of the introduced perimeters. ",
    "url": "https://arxiv.org/abs/2305.18779",
    "authors": [
      "Leon Bungert",
      "Nicol\u00e1s Garc\u00eda Trillos",
      "Matt Jacobs",
      "Daniel McKenzie",
      "\u0110or\u0111e Nikoli\u0107",
      "Qingsong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.18780",
    "title": "Who Would be Interested in Services? An Entity Graph Learning System for  User Targeting",
    "abstract": "With the growing popularity of various mobile devices, user targeting has received a growing amount of attention, which aims at effectively and efficiently locating target users that are interested in specific services. Most pioneering works for user targeting tasks commonly perform similarity-based expansion with a few active users as seeds, suffering from the following major issues: the unavailability of seed users for newcoming services and the unfriendliness of black-box procedures towards marketers. In this paper, we design an Entity Graph Learning (EGL) system to provide explainable user targeting ability meanwhile applicable to addressing the cold-start issue. EGL System follows the hybrid online-offline architecture to satisfy the requirements of scalability and timeliness. Specifically, in the offline stage, the system focuses on the heavyweight entity graph construction and user entity preference learning, in which we propose a Three-stage Relation Mining Procedure (TRMP), breaking loose from the expensive seed users. At the online stage, the system offers the ability of user targeting in real-time based on the entity graph from the offline stage. Since the user targeting process is based on graph reasoning, the whole process is transparent and operation-friendly to marketers. Finally, extensive offline experiments and online A/B testing demonstrate the superior performance of the proposed EGL System. ",
    "url": "https://arxiv.org/abs/2305.18780",
    "authors": [
      "Dan Yang",
      "Binbin Hu",
      "Xiaoyan Yang",
      "Yue Shen",
      "Zhiqiang Zhang",
      "Jinjie Gu",
      "Guannan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2305.18782",
    "title": "VVC Extension Scheme for Object Detection Using Contrast Reduction",
    "abstract": "In recent years, video analysis using Artificial Intelligence (AI) has been widely used, due to the remarkable development of image recognition technology using deep learning. In 2019, the Moving Picture Experts Group (MPEG) has started standardization of Video Coding for Machines (VCM) as a video coding technology for image recognition. In the framework of VCM, both higher image recognition accuracy and video compression performance are required. In this paper, we propose an extention scheme of video coding for object detection using Versatile Video Coding (VVC). Unlike video for human vision, video used for object detection does not require a large image size or high contrast. Since downsampling of the image can reduce the amount of information to be transmitted. Due to the decrease in image contrast, entropy of the image becomes smaller. Therefore, in our proposed scheme, the original image is reduced in size and contrast, then coded with VVC encoder to achieve high compression performance. Then, the output image from the VVC decoder is restored to its original image size using the bicubic method. Experimental results show that the proposed video coding scheme achieves better coding performance than regular VVC in terms of object detection accuracy. ",
    "url": "https://arxiv.org/abs/2305.18782",
    "authors": [
      "Takahiro Shindo",
      "Taiju Watanabe",
      "Kein Yamada",
      "Hiroshi Watanabe"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18784",
    "title": "Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits",
    "abstract": "The study of collaborative multi-agent bandits has attracted significant attention recently. In light of this, we initiate the study of a new collaborative setting, consisting of $N$ agents such that each agent is learning one of $M$ stochastic multi-armed bandits to minimize their group cumulative regret. We develop decentralized algorithms which facilitate collaboration between the agents under two scenarios. We characterize the performance of these algorithms by deriving the per agent cumulative regret and group regret upper bounds. We also prove lower bounds for the group regret in this setting, which demonstrates the near-optimal behavior of the proposed algorithms. ",
    "url": "https://arxiv.org/abs/2305.18784",
    "authors": [
      "Ronshee Chawla",
      "Daniel Vial",
      "Sanjay Shakkottai",
      "R. Srikant"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.18797",
    "title": "Learning Weakly Supervised Audio-Visual Violence Detection in Hyperbolic  Space",
    "abstract": "In recent years, the task of weakly supervised audio-visual violence detection has gained considerable attention. The goal of this task is to identify violent segments within multimodal data based on video-level labels. Despite advances in this field, traditional Euclidean neural networks, which have been used in prior research, encounter difficulties in capturing highly discriminative representations due to limitations of the feature space. To overcome this, we propose HyperVD, a novel framework that learns snippet embeddings in hyperbolic space to improve model discrimination. Our framework comprises a detour fusion module for multimodal fusion, effectively alleviating modality inconsistency between audio and visual signals. Additionally, we contribute two branches of fully hyperbolic graph convolutional networks that excavate feature similarities and temporal relationships among snippets in hyperbolic space. By learning snippet representations in this space, the framework effectively learns semantic discrepancies between violent and normal events. Extensive experiments on the XD-Violence benchmark demonstrate that our method outperforms state-of-the-art methods by a sizable margin. ",
    "url": "https://arxiv.org/abs/2305.18797",
    "authors": [
      "Xiaogang Peng",
      "Hao Wen",
      "Yikai Luo",
      "Xiao Zhou",
      "Keyang Yu",
      "Yigang Wang",
      "Zizhao Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18798",
    "title": "AnoOnly: Semi-Supervised Anomaly Detection without Loss on Normal Data",
    "abstract": "Semi-supervised anomaly detection (SSAD) methods have demonstrated their effectiveness in enhancing unsupervised anomaly detection (UAD) by leveraging few-shot but instructive abnormal instances. However, the dominance of homogeneous normal data over anomalies biases the SSAD models against effectively perceiving anomalies. To address this issue and achieve balanced supervision between heavily imbalanced normal and abnormal data, we develop a novel framework called AnoOnly (Anomaly Only). Unlike existing SSAD methods that resort to strict loss supervision, AnoOnly suspends it and introduces a form of weak supervision for normal data. This weak supervision is instantiated through the utilization of batch normalization, which implicitly performs cluster learning on normal data. When integrated into existing SSAD methods, the proposed AnoOnly demonstrates remarkable performance enhancements across various models and datasets, achieving new state-of-the-art performance. Additionally, our AnoOnly is natively robust to label noise when suffering from data contamination. Our code is publicly available at https://github.com/cool-xuan/AnoOnly. ",
    "url": "https://arxiv.org/abs/2305.18798",
    "authors": [
      "Yixuan Zhou",
      "Peiyu Yang",
      "Yi Qu",
      "Xing Xu",
      "Fumin Shen",
      "Heng Tao Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18806",
    "title": "Prediction Error-based Classification for Class-Incremental Learning",
    "abstract": "Class-incremental learning (CIL) is a particularly challenging variant of continual learning, where the goal is to learn to discriminate between all classes presented in an incremental fashion. Existing approaches often suffer from excessive forgetting and imbalance of the scores assigned to classes that have not been seen together during training. In this study, we introduce a novel approach, Prediction Error-based Classification (PEC), which differs from traditional discriminative and generative classification paradigms. PEC computes a class score by measuring the prediction error of a model trained to replicate the outputs of a frozen random neural network on data from that class. The method can be interpreted as approximating a classification rule based on Gaussian Process posterior variance. PEC offers several practical advantages, including sample efficiency, ease of tuning, and effectiveness even when data are presented one class at a time. Our empirical results show that PEC performs strongly in single-pass-through-data CIL, outperforming other rehearsal-free baselines in all cases and rehearsal-based methods with moderate replay buffer size in most cases across multiple benchmarks. ",
    "url": "https://arxiv.org/abs/2305.18806",
    "authors": [
      "Micha\u0142 Zaj\u0105c",
      "Tinne Tuytelaars",
      "Gido M. van de Ven"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.18808",
    "title": "CTSN: Predicting Cloth Deformation for Skeleton-based Characters with a  Two-stream Skinning Network",
    "abstract": "We present a novel learning method to predict the cloth deformation for skeleton-based characters with a two-stream network. The characters processed in our approach are not limited to humans, and can be other skeletal-based representations of non-human targets such as fish or pets. We use a novel network architecture which consists of skeleton-based and mesh-based residual networks to learn the coarse and wrinkle features as the overall residual from the template cloth mesh. Our network is used to predict the deformation for loose or tight-fitting clothing or dresses. We ensure that the memory footprint of our network is low, and thereby result in reduced storage and computational requirements. In practice, our prediction for a single cloth mesh for the skeleton-based character takes about 7 milliseconds on an NVIDIA GeForce RTX 3090 GPU. Compared with prior methods, our network can generate fine deformation results with details and wrinkles. ",
    "url": "https://arxiv.org/abs/2305.18808",
    "authors": [
      "Yudi Li",
      "Min Tang",
      "Yun Yang",
      "Ruofeng Tong",
      "Shuangcai Yang",
      "Yao Li",
      "Bailin An",
      "Qilong Kou"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18811",
    "title": "PyPOTS: A Python Toolbox for Data Mining on Partially-Observed Time  Series",
    "abstract": "PyPOTS is an open-source Python library dedicated to data mining and analysis on multivariate partially-observed time series, i.e. incomplete time series with missing values, A.K.A. irregularlysampled time series. Particularly, it provides easy access to diverse algorithms categorized into four tasks: imputation, classification, clustering, and forecasting. The included models contain probabilistic approaches as well as neural-network methods, with a well-designed and fully-documented programming interface for both academic researchers and industrial professionals to use. With robustness and scalability in its design philosophy, best practices of software construction, for example, unit testing, continuous integration (CI) and continuous delivery (CD), code coverage, maintainability evaluation, interactive tutorials, and parallelization, are carried out as principles during the development of PyPOTS. The toolkit is available on both Python Package Index (PyPI) and Anaconda. PyPOTS is open-source and publicly available on GitHub https://github.com/WenjieDu/PyPOTS. ",
    "url": "https://arxiv.org/abs/2305.18811",
    "authors": [
      "Wenjie Du"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.18820",
    "title": "Robust Reinforcement Learning Objectives for Sequential Recommender  Systems",
    "abstract": "Attention-based sequential recommendation methods have demonstrated promising results by accurately capturing users' dynamic interests from historical interactions. In addition to generating superior user representations, recent studies have begun integrating reinforcement learning (RL) into these models. Framing sequential recommendation as an RL problem with reward signals, unlocks developing recommender systems (RS) that consider a vital aspect-incorporating direct user feedback in the form of rewards to deliver a more personalized experience. Nonetheless, employing RL algorithms presents challenges, including off-policy training, expansive combinatorial action spaces, and the scarcity of datasets with sufficient reward signals. Contemporary approaches have attempted to combine RL and sequential modeling, incorporating contrastive-based objectives and negative sampling strategies for training the RL component. In this study, we further emphasize the efficacy of contrastive-based objectives paired with augmentation to address datasets with extended horizons. Additionally, we recognize the potential instability issues that may arise during the application of negative sampling. These challenges primarily stem from the data imbalance prevalent in real-world datasets, which is a common issue in offline RL contexts. While our established baselines attempt to mitigate this through various techniques, instability remains an issue. Therefore, we introduce an enhanced methodology aimed at providing a more effective solution to these challenges. ",
    "url": "https://arxiv.org/abs/2305.18820",
    "authors": [
      "Melissa Mozifian",
      "Tristan Sylvain",
      "Dave Evans",
      "Lili Meng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2305.18823",
    "title": "Language-independent speaker anonymization using orthogonal Householder  neural network",
    "abstract": "Speaker anonymization aims to conceal a speaker's identity while preserving content information in speech. Current mainstream neural-network speaker anonymization systems disentangle speech into prosody-related, content, and speaker representations. The speaker representation is then anonymized by a selection-based speaker anonymizer that uses a mean vector over a set of randomly selected speaker vectors from an external pool of English speakers. However, the resulting anonymized vectors are subject to severe privacy leakage against powerful attackers, reduction in speaker diversity, and language mismatch problems for unseen language speaker anonymization. To generate diverse, language-neutral speaker vectors, this paper proposes an anonymizer based on an orthogonal Householder neural network (OHNN). Specifically, the OHNN acts like a rotation to transform the original speaker vectors into anonymized speaker vectors, which are constrained to follow the distribution over the original speaker vector space. A basic classification loss is introduced to ensure that anonymized speaker vectors from different speakers have unique speaker identities. To further protect speaker identities, an improved classification loss and similarity loss are used to push original-anonymized sample pairs away from each other. Experiments on VoicePrivacy Challenge datasets in English and the AISHELL-3 dataset in Mandarin demonstrate the proposed anonymizer's effectiveness. ",
    "url": "https://arxiv.org/abs/2305.18823",
    "authors": [
      "Xiaoxiao Miao",
      "Xin Wang",
      "Erica Cooper",
      "Junichi Yamagishi",
      "Natalia Tomashenko"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2305.18824",
    "title": "Graph Neural Networks for Contextual ASR with the Tree-Constrained  Pointer Generator",
    "abstract": "The incorporation of biasing words obtained through contextual knowledge is of paramount importance in automatic speech recognition (ASR) applications. This paper proposes an innovative method for achieving end-to-end contextual ASR using graph neural network (GNN) encodings based on the tree-constrained pointer generator method. GNN node encodings facilitate lookahead for future word pieces in the process of ASR decoding at each tree node by incorporating information about all word pieces on the tree branches rooted from it. This results in a more precise prediction of the generation probability of the biasing words. The study explores three GNN encoding techniques, namely tree recursive neural networks, graph convolutional network (GCN), and GraphSAGE, along with different combinations of the complementary GCN and GraphSAGE structures. The performance of the systems was evaluated using the Librispeech and AMI corpus, following the visual-grounded contextual ASR pipeline. The findings indicate that using GNN encodings achieved consistent and significant reductions in word error rate (WER), particularly for words that are rare or have not been seen during the training process. Notably, the most effective combination of GNN encodings obtained more than 60% WER reduction for rare and unseen words compared to standard end-to-end systems. ",
    "url": "https://arxiv.org/abs/2305.18824",
    "authors": [
      "Guangzhi Sun",
      "Chao Zhang",
      "Phil Woodland"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2305.18832",
    "title": "Rethinking Rendering in Generalizable Neural Surface Reconstruction: A  Learning-based Solution",
    "abstract": "Generalizable neural surface reconstruction techniques have attracted great attention in recent years. However, they encounter limitations of low confidence depth distribution and inaccurate surface reasoning due to the oversimplified volume rendering process employed. In this paper, we present Reconstruction TRansformer (ReTR), a novel framework that leverages the transformer architecture to redesign the rendering process, enabling complex photon-particle interaction modeling. It introduces a learnable meta-ray token and utilizes the cross-attention mechanism to simulate the interaction of photons with sampled points and render the observed color. Meanwhile, by operating within a high-dimensional feature space rather than the color space, ReTR mitigates sensitivity to projected colors in source views. Such improvements result in accurate surface assessment with high confidence. We demonstrate the effectiveness of our approach on various datasets, showcasing how our method outperforms the current state-of-the-art approaches in terms of reconstruction quality and generalization ability. ",
    "url": "https://arxiv.org/abs/2305.18832",
    "authors": [
      "Yixun Liang",
      "Hao He",
      "Ying-cong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18845",
    "title": "How Generative Models Improve LOS Estimation in 6G Non-Terrestrial  Networks",
    "abstract": "With the advent of 5G and the anticipated arrival of 6G, there has been a growing research interest in combining mobile networks with Non-Terrestrial Network platforms such as low earth orbit satellites and Geosynchronous Equatorial Orbit satellites to provide broader coverage for a wide range of applications. However, integrating these platforms is challenging because Line-Of-Sight (LOS) estimation is required for both inter satellite and satellite-to-terrestrial segment links. Machine Learning (ML) techniques have shown promise in channel modeling and LOS estimation, but they require large datasets for model training, which can be difficult to obtain. In addition, network operators may be reluctant to disclose their network data due to privacy concerns. Therefore, alternative data collection techniques are needed. In this paper, a framework is proposed that uses generative models to generate synthetic data for LOS estimation in non-terrestrial 6G networks. Specifically, the authors show that generative models can be trained with a small available dataset to generate large datasets that can be used to train ML models for LOS estimation. Furthermore, since the generated synthetic data does not contain identifying information of the original dataset, it can be made publicly available without violating privacy ",
    "url": "https://arxiv.org/abs/2305.18845",
    "authors": [
      "Saira Bano",
      "Achilles Machumilane",
      "Pietro Cassar\u00e0",
      "Alberto Gotta"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18851",
    "title": "Data Augmentation Methods of Parameter Identification of a Dynamic Model  for Harbor Maneuvers",
    "abstract": "A dynamic model for an automatic berthing and unberthing controller has to estimate harbor maneuvers, which include berthing, unberthing, approach maneuvers to berths, and entering and leaving the port. When the dynamic model is estimated by the system identification, a large number of tests or trials are required to measure the various motions of harbor maneuvers. However, the amount of data that can be obtained is limited due to the high costs and time-consuming nature of full-scale ship trials. In this paper, we improve the generalization performance of the dynamic model for the automatic berthing and unberthing controller by introducing data augmentation. This study used slicing and jittering as data augmentation methods and confirmed their effectiveness by numerical experiments using the free-running model tests. The dynamic model is represented by a neural network-based model in numerical experiments. Results of numerical experiments demonstrated that slicing and jittering are effective data augmentation methods but could not improve generalization performance for extrapolation states of the original dataset. ",
    "url": "https://arxiv.org/abs/2305.18851",
    "authors": [
      "Kouki Wakita",
      "Yoshiki Miyauchi",
      "Youhei Akimoto",
      "Atsuo Maki"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2305.18852",
    "title": "Majority Voting Approach to Ransomware Detection",
    "abstract": "Crypto-ransomware remains a significant threat to governments and companies alike, with high-profile cyber security incidents regularly making headlines. Many different detection systems have been proposed as solutions to the ever-changing dynamic landscape of ransomware detection. In the majority of cases, these described systems propose a method based on the result of a single test performed on either the executable code, the process under investigation, its behaviour, or its output. In a small subset of ransomware detection systems, the concept of a scorecard is employed where multiple tests are performed on various aspects of a process under investigation and their results are then analysed using machine learning. The purpose of this paper is to propose a new majority voting approach to ransomware detection by developing a method that uses a cumulative score derived from discrete tests based on calculations using algorithmic rather than heuristic techniques. The paper describes 23 candidate tests, as well as 9 Windows API tests which are validated to determine both their accuracy and viability for use within a ransomware detection system. Using a cumulative score calculation approach to ransomware detection has several benefits, such as the immunity to the occasional inaccuracy of individual tests when making its final classification. The system can also leverage multiple tests that can be both comprehensive and complimentary in an attempt to achieve a broader, deeper, and more robust analysis of the program under investigation. Additionally, the use of multiple collaborative tests also significantly hinders ransomware from masking or modifying its behaviour in an attempt to bypass detection. ",
    "url": "https://arxiv.org/abs/2305.18852",
    "authors": [
      "Simon R Davies",
      "Richard Macfarlane",
      "William J Buchanan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2305.18856",
    "title": "A Federated Channel Modeling System using Generative Neural Networks",
    "abstract": "The paper proposes a data-driven approach to air-to-ground channel estimation in a millimeter-wave wireless network on an unmanned aerial vehicle. Unlike traditional centralized learning methods that are specific to certain geographical areas and inappropriate for others, we propose a generalized model that uses Federated Learning (FL) for channel estimation and can predict the air-to-ground path loss between a low-altitude platform and a terrestrial terminal. To this end, our proposed FL-based Generative Adversarial Network (FL-GAN) is designed to function as a generative data model that can learn different types of data distributions and generate realistic patterns from the same distributions without requiring prior data analysis before the training phase. To evaluate the effectiveness of the proposed model, we evaluate its performance using Kullback-Leibler divergence (KL), and Wasserstein distance between the synthetic data distribution generated by the model and the actual data distribution. We also compare the proposed technique with other generative models, such as FL-Variational Autoencoder (FL-VAE) and stand-alone VAE and GAN models. The results of the study show that the synthetic data generated by FL-GAN has the highest similarity in distribution with the real data. This shows the effectiveness of the proposed approach in generating data-driven channel models that can be used in different regions ",
    "url": "https://arxiv.org/abs/2305.18856",
    "authors": [
      "Saira Bano",
      "Pietro Cassar\u00e0",
      "Nicola Tonellotto",
      "Alberto Gotta"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18865",
    "title": "Elongated Physiological Structure Segmentation via Spatial and Scale  Uncertainty-aware Network",
    "abstract": "Robust and accurate segmentation for elongated physiological structures is challenging, especially in the ambiguous region, such as the corneal endothelium microscope image with uneven illumination or the fundus image with disease interference. In this paper, we present a spatial and scale uncertainty-aware network (SSU-Net) that fully uses both spatial and scale uncertainty to highlight ambiguous regions and integrate hierarchical structure contexts. First, we estimate epistemic and aleatoric spatial uncertainty maps using Monte Carlo dropout to approximate Bayesian networks. Based on these spatial uncertainty maps, we propose the gated soft uncertainty-aware (GSUA) module to guide the model to focus on ambiguous regions. Second, we extract the uncertainty under different scales and propose the multi-scale uncertainty-aware (MSUA) fusion module to integrate structure contexts from hierarchical predictions, strengthening the final prediction. Finally, we visualize the uncertainty map of final prediction, providing interpretability for segmentation results. Experiment results show that the SSU-Net performs best on cornea endothelial cell and retinal vessel segmentation tasks. Moreover, compared with counterpart uncertainty-based methods, SSU-Net is more accurate and robust. ",
    "url": "https://arxiv.org/abs/2305.18865",
    "authors": [
      "Yinglin Zhang",
      "Ruiling Xi",
      "Huazhu Fu",
      "Dave Towey",
      "RuiBin Bai",
      "Risa Higashita",
      "Jiang Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18885",
    "title": "Criteria Tell You More than Ratings: Criteria Preference-Aware Light  Graph Convolution for Effective Multi-Criteria Recommendation",
    "abstract": "The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborative signal in MC ratings. Next, to strengthen the capability of criteria preference awareness, CPA-LGC incorporates newly characterized embeddings, including user-specific criteria-preference embeddings and item-specific criterion embeddings, into our graph convolution model. Through comprehensive evaluations using four real-world datasets, we demonstrate (a) the superiority over benchmark MC recommendation methods and benchmark recommendation methods using GNNs with tremendous gains, (b) the effectiveness of core components in CPA-LGC, and (c) the computational efficiency. ",
    "url": "https://arxiv.org/abs/2305.18885",
    "authors": [
      "Jin-Duk Park",
      "Siqing Li",
      "Xin Cao",
      "Won-Yong Shin"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18888",
    "title": "Contrastive Shapelet Learning for Unsupervised Multivariate Time Series  Representation Learning",
    "abstract": "Recent studies have shown great promise in unsupervised representation learning (URL) for multivariate time series, because URL has the capability in learning generalizable representation for many downstream tasks without using inaccessible labels. However, existing approaches usually adopt the models originally designed for other domains (e.g., computer vision) to encode the time series data and rely on strong assumptions to design learning objectives, which limits their ability to perform well. To deal with these problems, we propose a novel URL framework for multivariate time series by learning time-series-specific shapelet-based representation through a popular contrasting learning paradigm. To the best of our knowledge, this is the first work that explores the shapelet-based embedding in the unsupervised general-purpose representation learning. A unified shapelet-based encoder and a novel learning objective with multi-grained contrasting and multi-scale alignment are particularly designed to achieve our goal, and a data augmentation library is employed to improve the generalization. We conduct extensive experiments using tens of real-world datasets to assess the representation quality on many downstream tasks, including classification, clustering, and anomaly detection. The results demonstrate the superiority of our method against not only URL competitors, but also techniques specially designed for downstream tasks. Our code has been made publicly available at https://github.com/real2fish/CSL. ",
    "url": "https://arxiv.org/abs/2305.18888",
    "authors": [
      "Zhiyu Liang",
      "Jianfeng Zhang",
      "Chen Liang",
      "Hongzhi Wang",
      "Zheng Liang",
      "Lujia Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18889",
    "title": "Split Federated Learning: Speed up Model Training in Resource-Limited  Wireless Networks",
    "abstract": "In this paper, we propose a novel distributed learning scheme, named group-based split federated learning (GSFL), to speed up artificial intelligence (AI) model training. Specifically, the GSFL operates in a split-then-federated manner, which consists of three steps: 1) Model distribution, in which the access point (AP) splits the AI models and distributes the client-side models to clients; 2) Model training, in which each client executes forward propagation and transmit the smashed data to the edge server. The edge server executes forward and backward propagation and then returns the gradient to the clients for updating local client-side models; and 3) Model aggregation, in which edge servers aggregate the server-side and client-side models. Simulation results show that the GSFL outperforms vanilla split learning and federated learning schemes in terms of overall training latency while achieving satisfactory accuracy. ",
    "url": "https://arxiv.org/abs/2305.18889",
    "authors": [
      "Songge Zhang",
      "Wen Wu",
      "Penghui Hu",
      "Shaofeng Li",
      "Ning Zhang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2305.18893",
    "title": "Where's the Point? Self-Supervised Multilingual Punctuation-Agnostic  Sentence Segmentation",
    "abstract": "Many NLP pipelines split text into sentences as one of the crucial preprocessing steps. Prior sentence segmentation tools either rely on punctuation or require a considerable amount of sentence-segmented training data: both central assumptions might fail when porting sentence segmenters to diverse languages on a massive scale. In this work, we thus introduce a multilingual punctuation-agnostic sentence segmentation method, currently covering 85 languages, trained in a self-supervised fashion on unsegmented text, by making use of newline characters which implicitly perform segmentation into paragraphs. We further propose an approach that adapts our method to the segmentation in a given corpus by using only a small number (64-256) of sentence-segmented examples. The main results indicate that our method outperforms all the prior best sentence-segmentation tools by an average of 6.1% F1 points. Furthermore, we demonstrate that proper sentence segmentation has a point: the use of a (powerful) sentence segmenter makes a considerable difference for a downstream application such as machine translation (MT). By using our method to match sentence segmentation to the segmentation used during training of MT models, we achieve an average improvement of 2.3 BLEU points over the best prior segmentation tool, as well as massive gains over a trivial segmenter that splits text into equally sized blocks. ",
    "url": "https://arxiv.org/abs/2305.18893",
    "authors": [
      "Benjamin Minixhofer",
      "Jonas Pfeiffer",
      "Ivan Vuli\u0107"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18897",
    "title": "HuMoR: Human Motion Representation using Topology-Agnostic Transformers  for Character Animation Retargeting",
    "abstract": "Motion retargeting is a long-standing problem in character animation, which consists in transferring and adapting the motion of a source character to another target character. A typical application is the creation of motion sequences from off-the-shelf motions by transferring them onto new characters. Motion retargeting is also promising to increase interoperability of existing animation systems and motion databases, as they often differ in the structure of the skeleton(s) considered. Moreover, since the goal of motion retargeting is to abstract and transfer motion dynamics, effective solutions might coincide with expressive and powerful human motion models in which operations such as cleaning or editing are easier. In this article, we present a novel abstract representation of human motion agnostic to skeleton topology and morphology. Based on transformers, our model is able to encode and decode motion sequences with variable morphology and topology -- extending the scope of retargeting -- while supporting skeleton topologies not seen during the training phase. More specifically, our model is structured as an autoencoder and encoding and decoding are separately conditioned on skeleton templates to extract and control morphology and topology. Beyond motion retargeting, our model has many applications since our abstract representation is a convenient space to embed motion data from different sources. It may potentially be benefical to a number of data-driven methods, allowing them to combine scarce specialised motion datasets (e.g. with style or contact annotations) and larger general motion datasets for improved performance and generalisation ability. Moreover, we show that our model can be useful for other applications beyond retargeting, including motion denoising and joint upsampling. ",
    "url": "https://arxiv.org/abs/2305.18897",
    "authors": [
      "Lucas Mourot",
      "Ludovic Hoyet",
      "Fran\u00e7ois Le Clerc",
      "Pierre Hellier"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2305.18907",
    "title": "Multitask learning for recognizing stress and depression in social media",
    "abstract": "Stress and depression are prevalent nowadays across people of all ages due to the quick paces of life. People use social media to express their feelings. Thus, social media constitute a valuable form of information for the early detection of stress and depression. Although many research works have been introduced targeting the early recognition of stress and depression, there are still limitations. There have been proposed multi-task learning settings, which use depression and emotion (or figurative language) as the primary and auxiliary tasks respectively. However, although stress is inextricably linked with depression, researchers face these two tasks as two separate tasks. To address these limitations, we present the first study, which exploits two different datasets collected under different conditions, and introduce two multitask learning frameworks, which use depression and stress as the main and auxiliary tasks respectively. Specifically, we use a depression dataset and a stressful dataset including stressful posts from ten subreddits of five domains. In terms of the first approach, each post passes through a shared BERT layer, which is updated by both tasks. Next, two separate BERT encoder layers are exploited, which are updated by each task separately. Regarding the second approach, it consists of shared and task-specific layers weighted by attention fusion networks. We conduct a series of experiments and compare our approaches with existing research initiatives, single-task learning, and transfer learning. Experiments show multiple advantages of our approaches over state-of-the-art ones. ",
    "url": "https://arxiv.org/abs/2305.18907",
    "authors": [
      "Loukas Ilias",
      "Dimitris Askounis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2305.18917",
    "title": "Fighting Bias with Bias: Promoting Model Robustness by Amplifying  Dataset Biases",
    "abstract": "NLP models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. Recent work sought to develop robust, unbiased models by filtering biased examples from training sets. In this work, we argue that such filtering can obscure the true capabilities of models to overcome biases, which might never be removed in full from the dataset. We suggest that in order to drive the development of models robust to subtle biases, dataset biases should be amplified in the training set. We introduce an evaluation framework defined by a bias-amplified training set and an anti-biased test set, both automatically extracted from existing datasets. Experiments across three notions of bias, four datasets and two models show that our framework is substantially more challenging for models than the original data splits, and even more challenging than hand-crafted challenge sets. Our evaluation framework can use any existing dataset, even those considered obsolete, to test model robustness. We hope our work will guide the development of robust models that do not rely on superficial biases and correlations. To this end, we publicly release our code and data. ",
    "url": "https://arxiv.org/abs/2305.18917",
    "authors": [
      "Yuval Reif",
      "Roy Schwartz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18933",
    "title": "A Multilingual Evaluation of NER Robustness to Adversarial Inputs",
    "abstract": "Adversarial evaluations of language models typically focus on English alone. In this paper, we performed a multilingual evaluation of Named Entity Recognition (NER) in terms of its robustness to small perturbations in the input. Our results showed the NER models we explored across three languages (English, German and Hindi) are not very robust to such changes, as indicated by the fluctuations in the overall F1 score as well as in a more fine-grained evaluation. With that knowledge, we further explored whether it is possible to improve the existing NER models using a part of the generated adversarial data sets as augmented training data to train a new NER model or as fine-tuning data to adapt an existing NER model. Our results showed that both these approaches improve performance on the original as well as adversarial test sets. While there is no significant difference between the two approaches for English, re-training is significantly better than fine-tuning for German and Hindi. ",
    "url": "https://arxiv.org/abs/2305.18933",
    "authors": [
      "Akshay Srinivasan",
      "Sowmya Vajjala"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18936",
    "title": "The Isomorphism Problem of Power Graphs and a Question of Cameron",
    "abstract": "The isomorphism problem for graphs (GI) and the isomorphism problem for groups (GrISO) have been studied extensively by researchers. The current best algorithms for both these problems run in quasipolynomial time. In this paper, we study the isomorphism problem of graphs that are defined in terms of groups, namely power graphs, directed power graphs, and enhanced power graphs. It is not enough to check the isomorphism of the underlying groups to solve the isomorphism problem of such graphs as the power graphs (or the directed power graphs or the enhanced power graphs) of two nonisomorphic groups can be isomorphic. Nevertheless, it is interesting to ask if the underlying group structure can be exploited to design better isomorphism algorithms for these graphs. We design polynomial time algorithms for the isomorphism problems for the power graphs, the directed power graphs and the enhanced power graphs arising from finite nilpotent groups. In contrast, no polynomial time algorithm is known for the group isomorphism problem, even for nilpotent groups of class 2. We note that our algorithm does not require the underlying groups of the input graphs to be given. The isomorphism problems of power graphs and enhanced power graphs are solved by first computing the directed power graphs from the input graphs. The problem of efficiently computing the directed power graph from the power graph or the enhanced power graph is due to Cameron [IJGT'22]. Therefore, we give a solution to Cameron's question. ",
    "url": "https://arxiv.org/abs/2305.18936",
    "authors": [
      "Bireswar Das",
      "Jinia Ghosh",
      "Anant Kumar"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Group Theory (math.GR)"
    ]
  },
  {
    "id": "arXiv:2305.18937",
    "title": "WDM/TDM over Passive Optical Networks with Cascaded-AWGRs for Data  Centers",
    "abstract": "Data centers based on Passive Optical Networks (PONs) can provide high capacity, low cost, scalability, elasticity and high energy-efficiency. This paper introduces the use of WDM-TDM multiple access in a PON-based data center that offers multipath routing via two-tier cascaded Arrayed Waveguide Grating Routers (AWGRs) to improve the utilization of resources. A Mixed Integer Linear Programming (MILP) model is developed to optimize resource allocation while considering multipath routing. The results show that all-to-all connectivity is achieved in the architecture through the use of two different wavelength within different time slots for the communication between racks in the same or different cells, as well as with the OLT switches. ",
    "url": "https://arxiv.org/abs/2305.18937",
    "authors": [
      "Mohammed Alharthi",
      "Sanaa H. Mohamed",
      "Taisir E. H. El-Gorashi",
      "Jaafar M. H. Elmirghani"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2305.18947",
    "title": "A Probabilistic Rotation Representation for Symmetric Shapes With an  Efficiently Computable Bingham Loss Function",
    "abstract": "In recent years, a deep learning framework has been widely used for object pose estimation. While quaternion is a common choice for rotation representation, it cannot represent the ambiguity of the observation. In order to handle the ambiguity, the Bingham distribution is one promising solution. However, it requires complicated calculation when yielding the negative log-likelihood (NLL) loss. An alternative easy-to-implement loss function has been proposed to avoid complex computations but has difficulty expressing symmetric distribution. In this paper, we introduce a fast-computable and easy-to-implement NLL loss function for Bingham distribution. We also create the inference network and show that our loss function can capture the symmetric property of target objects from their point clouds. ",
    "url": "https://arxiv.org/abs/2305.18947",
    "authors": [
      "Hiroya Sato",
      "Takuya Ikeda",
      "Koichi Nishiwaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18951",
    "title": "Subequivariant Graph Reinforcement Learning in 3D Environments",
    "abstract": "Learning a shared policy that guides the locomotion of different agents is of core interest in Reinforcement Learning (RL), which leads to the study of morphology-agnostic RL. However, existing benchmarks are highly restrictive in the choice of starting point and target point, constraining the movement of the agents within 2D space. In this work, we propose a novel setup for morphology-agnostic RL, dubbed Subequivariant Graph RL in 3D environments (3D-SGRL). Specifically, we first introduce a new set of more practical yet challenging benchmarks in 3D space that allows the agent to have full Degree-of-Freedoms to explore in arbitrary directions starting from arbitrary configurations. Moreover, to optimize the policy over the enlarged state-action space, we propose to inject geometric symmetry, i.e., subequivariance, into the modeling of the policy and Q-function such that the policy can generalize to all directions, improving exploration efficiency. This goal is achieved by a novel SubEquivariant Transformer (SET) that permits expressive message exchange. Finally, we evaluate the proposed method on the proposed benchmarks, where our method consistently and significantly outperforms existing approaches on single-task, multi-task, and zero-shot generalization scenarios. Extensive ablations are also conducted to verify our design. Code and videos are available on our project page: https://alpc91.github.io/SGRL/. ",
    "url": "https://arxiv.org/abs/2305.18951",
    "authors": [
      "Runfa Chen",
      "Jiaqi Han",
      "Fuchun Sun",
      "Wenbing Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2305.18955",
    "title": "On the Impact of Operators and Populations within Evolutionary  Algorithms for the Dynamic Weighted Traveling Salesperson Problem",
    "abstract": "Evolutionary algorithms have been shown to obtain good solutions for complex optimization problems in static and dynamic environments. It is important to understand the behaviour of evolutionary algorithms for complex optimization problems that also involve dynamic and/or stochastic components in a systematic way in order to further increase their applicability to real-world problems. We investigate the node weighted traveling salesperson problem (W-TSP), which provides an abstraction of a wide range of weighted TSP problems, in dynamic settings. In the dynamic setting of the problem, items that have to be collected as part of a TSP tour change over time. We first present a dynamic setup for the dynamic W-TSP parameterized by different types of changes that are applied to the set of items to be collected when traversing the tour. Our first experimental investigations study the impact of such changes on resulting optimized tours in order to provide structural insights of optimization solutions. Afterwards, we investigate simple mutation-based evolutionary algorithms and study the impact of the mutation operators and the use of populations with dealing with the dynamic changes to the node weights of the problem. ",
    "url": "https://arxiv.org/abs/2305.18955",
    "authors": [
      "Jakob Bossek",
      "Aneta Neumann",
      "Frank Neumann"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2305.18962",
    "title": "Hyperbolic Diffusion Embedding and Distance for Hierarchical  Representation Learning",
    "abstract": "Finding meaningful representations and distances of hierarchical data is important in many fields. This paper presents a new method for hierarchical data embedding and distance. Our method relies on combining diffusion geometry, a central approach to manifold learning, and hyperbolic geometry. Specifically, using diffusion geometry, we build multi-scale densities on the data, aimed to reveal their hierarchical structure, and then embed them into a product of hyperbolic spaces. We show theoretically that our embedding and distance recover the underlying hierarchical structure. In addition, we demonstrate the efficacy of the proposed method and its advantages compared to existing methods on graph embedding benchmarks and hierarchical datasets. ",
    "url": "https://arxiv.org/abs/2305.18962",
    "authors": [
      "Ya-Wei Eileen Lin",
      "Ronald R. Coifman",
      "Gal Mishne",
      "Ronen Talmon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18965",
    "title": "Node Embedding from Neural Hamiltonian Orbits in Graph Neural Networks",
    "abstract": "In the graph node embedding problem, embedding spaces can vary significantly for different data types, leading to the need for different GNN model types. In this paper, we model the embedding update of a node feature as a Hamiltonian orbit over time. Since the Hamiltonian orbits generalize the exponential maps, this approach allows us to learn the underlying manifold of the graph in training, in contrast to most of the existing literature that assumes a fixed graph embedding manifold with a closed exponential map solution. Our proposed node embedding strategy can automatically learn, without extensive tuning, the underlying geometry of any given graph dataset even if it has diverse geometries. We test Hamiltonian functions of different forms and verify the performance of our approach on two graph node embedding downstream tasks: node classification and link prediction. Numerical experiments demonstrate that our approach adapts better to different types of graph datasets than popular state-of-the-art graph node embedding GNNs. The code is available at \\url{https://github.com/zknus/Hamiltonian-GNN}. ",
    "url": "https://arxiv.org/abs/2305.18965",
    "authors": [
      "Qiyu Kang",
      "Kai Zhao",
      "Yang Song",
      "Sijie Wang",
      "Wee Peng Tay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Classical Physics (physics.class-ph)"
    ]
  },
  {
    "id": "arXiv:2305.18980",
    "title": "Multi-modal Queried Object Detection in the Wild",
    "abstract": "We introduce MQ-Det, an efficient architecture and pre-training strategy design to utilize both textual description with open-set generalization and visual exemplars with rich description granularity as category queries, namely, Multi-modal Queried object Detection, for real-world detection with both open-vocabulary categories and various granularity. MQ-Det incorporates vision queries into existing well-established language-queried-only detectors. A plug-and-play gated class-scalable perceiver module upon the frozen detector is proposed to augment category text with class-wise visual information. To address the learning inertia problem brought by the frozen detector, a vision conditioned masked language prediction strategy is proposed. MQ-Det's simple yet effective architecture and training strategy design is compatible with most language-queried object detectors, thus yielding versatile applications. Experimental results demonstrate that multi-modal queries largely boost open-world detection. For instance, MQ-Det significantly improves the state-of-the-art open-set detector GLIP by +7.8% zero-shot AP on the LVIS benchmark and averagely +6.3% AP on 13 few-shot downstream tasks, with merely 3% pre-training time required by GLIP. Code is available at https://github.com/YifanXu74/MQ-Det. ",
    "url": "https://arxiv.org/abs/2305.18980",
    "authors": [
      "Yifan Xu",
      "Mengdan Zhang",
      "Chaoyou Fu",
      "Peixian Chen",
      "Xiaoshan Yang",
      "Ke Li",
      "Changsheng Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18985",
    "title": "Robust Multimodal Failure Detection for Microservice Systems",
    "abstract": "Proactive failure detection of instances is vitally essential to microservice systems because an instance failure can propagate to the whole system and degrade the system's performance. Over the years, many single-modal (i.e., metrics, logs, or traces) data-based nomaly detection methods have been proposed. However, they tend to miss a large number of failures and generate numerous false alarms because they ignore the correlation of multimodal data. In this work, we propose AnoFusion, an unsupervised failure detection approach, to proactively detect instance failures through multimodal data for microservice systems. It applies a Graph Transformer Network (GTN) to learn the correlation of the heterogeneous multimodal data and integrates a Graph Attention Network (GAT) with Gated Recurrent Unit (GRU) to address the challenges introduced by dynamically changing multimodal data. We evaluate the performance of AnoFusion through two datasets, demonstrating that it achieves the F1-score of 0.857 and 0.922, respectively, outperforming the state-of-the-art failure detection approaches. ",
    "url": "https://arxiv.org/abs/2305.18985",
    "authors": [
      "Chenyu Zhao",
      "Minghua Ma",
      "Zhenyu Zhong",
      "Shenglin Zhang",
      "Zhiyuan Tan",
      "Xiao Xiong",
      "LuLu Yu",
      "Jiayi Feng",
      "Yongqian Sun",
      "Yuzhi Zhang",
      "Dan Pei",
      "Qingwei Lin",
      "Dongmei Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2305.18993",
    "title": "ConES: Concept Embedding Search for Parameter Efficient Tuning Large  Vision Language Models",
    "abstract": "Large pre-trained vision-language models have shown great prominence in transferring pre-acquired knowledge to various domains and downstream tasks with appropriate prompting or tuning. Existing prevalent tuning methods can be generally categorized into three genres: 1) prompt engineering by creating suitable prompt texts, which is time-consuming and requires domain expertise; 2) or simply fine-tuning the whole model, which is extremely inefficient; 3) prompt tuning through parameterized prompt embeddings with the text encoder. Nevertheless, all methods rely on the text encoder for bridging the modality gap between vision and language. In this work, we question the necessity of the cumbersome text encoder for a more lightweight and efficient tuning paradigm as well as more representative prompt embeddings closer to the image representations. To achieve this, we propose a Concept Embedding Search (ConES) approach by optimizing prompt embeddings -- without the need of the text encoder -- to capture the 'concept' of the image modality through a variety of task objectives. By dropping the text encoder, we are able to significantly speed up the learning process, \\eg, from about an hour to just ten minutes in our experiments for personalized text-to-image generation without impairing the generation quality. Moreover, our proposed approach is orthogonal to current existing tuning methods since the searched concept embeddings can be further utilized in the next stage of fine-tuning the pre-trained large models for boosting performance. Extensive experiments show that our approach can beat the prompt tuning and textual inversion methods in a variety of downstream tasks including objection detection, instance segmentation, and image generation. Our approach also shows better generalization capability for unseen concepts in specialized domains, such as the medical domain. ",
    "url": "https://arxiv.org/abs/2305.18993",
    "authors": [
      "Huahui Yi",
      "Ziyuan Qin",
      "Wei Xu",
      "Miaotian Guo",
      "Kun Wang",
      "Shaoting Zhang",
      "Kang Li",
      "Qicheng Lao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.19020",
    "title": "Pseudo-Siamese Network based Timbre-reserved Black-box Adversarial  Attack in Speaker Identification",
    "abstract": "In this study, we propose a timbre-reserved adversarial attack approach for speaker identification (SID) to not only exploit the weakness of the SID model but also preserve the timbre of the target speaker in a black-box attack setting. Particularly, we generate timbre-reserved fake audio by adding an adversarial constraint during the training of the voice conversion model. Then, we leverage a pseudo-Siamese network architecture to learn from the black-box SID model constraining both intrinsic similarity and structural similarity simultaneously. The intrinsic similarity loss is to learn an intrinsic invariance, while the structural similarity loss is to ensure that the substitute SID model shares a similar decision boundary to the fixed black-box SID model. The substitute model can be used as a proxy to generate timbre-reserved fake audio for attacking. Experimental results on the Audio Deepfake Detection (ADD) challenge dataset indicate that the attack success rate of our proposed approach yields up to 60.58% and 55.38% in the white-box and black-box scenarios, respectively, and can deceive both human beings and machines. ",
    "url": "https://arxiv.org/abs/2305.19020",
    "authors": [
      "Qing Wang",
      "Jixun Yao",
      "Ziqian Wang",
      "Pengcheng Guo",
      "Lei Xie"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2305.19035",
    "title": "Solving Robust MDPs through No-Regret Dynamics",
    "abstract": "Reinforcement Learning is a powerful framework for training agents to navigate different situations, but it is susceptible to changes in environmental dynamics. However, solving Markov Decision Processes that are robust to changes is difficult due to nonconvexity and size of action or state spaces. While most works have analyzed this problem by taking different assumptions on the problem, a general and efficient theoretical analysis is still missing. However, we generate a simple framework for improving robustness by solving a minimax iterative optimization problem where a policy player and an environmental dynamics player are playing against each other. Leveraging recent results in online nonconvex learning and techniques from improving policy gradient methods, we yield an algorithm that maximizes the robustness of the Value Function on the order of $\\mathcal{O}\\left(\\frac{1}{T^{\\frac{1}{2}}}\\right)$ where $T$ is the number of iterations of the algorithm. ",
    "url": "https://arxiv.org/abs/2305.19035",
    "authors": [
      "Etash Kumar Guha",
      "Jason D. Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.19056",
    "title": "Get Out of the Nest! Drivers of Social Influence in the  #TwitterMigration to Mastodon",
    "abstract": "The migration of Twitter users to Mastodon following Elon Musk's acquisition presents a unique opportunity to study collective behavior and gain insights into the drivers of coordinated behavior in online media. We analyzed the social network and the public conversations of about 75,000 migrated users and observed that the temporal trace of their migrations is compatible with a phenomenon of social influence, as described by a compartmental epidemic model of information diffusion. Drawing from prior research on behavioral change, we delved into the factors that account for variations across different Twitter communities in the effectiveness of the spreading of the influence to migrate. Communities in which the influence process unfolded more rapidly exhibit lower density of social connections, higher levels of signaled commitment to migrating, and more emphasis on shared identity and exchange of factual knowledge in the community discussion. These factors account collectively for 57% of the variance in the observed data. Our results highlight the joint importance of network structure, commitment, and psycho-linguistic aspects of social interactions in describing grassroots collective action, and contribute to deepen our understanding of the mechanisms driving processes of behavior change of online groups. ",
    "url": "https://arxiv.org/abs/2305.19056",
    "authors": [
      "Lucio La Cava",
      "Luca Maria Aiello",
      "Andrea Tagarelli"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2305.19059",
    "title": "Rank-adaptive spectral pruning of convolutional layers during training",
    "abstract": "The computing cost and memory demand of deep learning pipelines have grown fast in recent years and thus a variety of pruning techniques have been developed to reduce model parameters. The majority of these techniques focus on reducing inference costs by pruning the network after a pass of full training. A smaller number of methods address the reduction of training costs, mostly based on compressing the network via low-rank layer factorizations. Despite their efficiency for linear layers, these methods fail to effectively handle convolutional filters. In this work, we propose a low-parametric training method that factorizes the convolutions into tensor Tucker format and adaptively prunes the Tucker ranks of the convolutional kernel during training. Leveraging fundamental results from geometric integration theory of differential equations on tensor manifolds, we obtain a robust training algorithm that provably approximates the full baseline performance and guarantees loss descent. A variety of experiments against the full model and alternative low-rank baselines are implemented, showing that the proposed method drastically reduces the training costs, while achieving high performance, comparable to or better than the full baseline, and consistently outperforms competing low-rank approaches. ",
    "url": "https://arxiv.org/abs/2305.19059",
    "authors": [
      "Emanuele Zangrando",
      "Steffen Schotth\u00f6fer",
      "Gianluca Ceruti",
      "Jonas Kusch",
      "Francesco Tudisco"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.19063",
    "title": "Scale-aware Super-resolution Network with Dual Affinity Learning for  Lesion Segmentation from Medical Images",
    "abstract": "Convolutional Neural Networks (CNNs) have shown remarkable progress in medical image segmentation. However, lesion segmentation remains a challenge to state-of-the-art CNN-based algorithms due to the variance in scales and shapes. On the one hand, tiny lesions are hard to be delineated precisely from the medical images which are often of low resolutions. On the other hand, segmenting large-size lesions requires large receptive fields, which exacerbates the first challenge. In this paper, we present a scale-aware super-resolution network to adaptively segment lesions of various sizes from the low-resolution medical images. Our proposed network contains dual branches to simultaneously conduct lesion mask super-resolution and lesion image super-resolution. The image super-resolution branch will provide more detailed features for the segmentation branch, i.e., the mask super-resolution branch, for fine-grained segmentation. Meanwhile, we introduce scale-aware dilated convolution blocks into the multi-task decoders to adaptively adjust the receptive fields of the convolutional kernels according to the lesion sizes. To guide the segmentation branch to learn from richer high-resolution features, we propose a feature affinity module and a scale affinity module to enhance the multi-task learning of the dual branches. On multiple challenging lesion segmentation datasets, our proposed network achieved consistent improvements compared to other state-of-the-art methods. ",
    "url": "https://arxiv.org/abs/2305.19063",
    "authors": [
      "Yanwen Li",
      "Luyang Luo",
      "Huangjing Lin",
      "Pheng-Ann Heng",
      "Hao Chen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.19065",
    "title": "Template-free Articulated Neural Point Clouds for Reposable View  Synthesis",
    "abstract": "Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when synthesizing novel views of time-evolving 3D scenes. However, the common reliance on backward deformation fields makes reanimation of the captured object poses challenging. Moreover, the state of the art dynamic models are often limited by low visual fidelity, long reconstruction time or specificity to narrow application domains. In this paper, we present a novel method utilizing a point-based representation and Linear Blend Skinning (LBS) to jointly learn a Dynamic NeRF and an associated skeletal model from even sparse multi-view video. Our forward-warping approach achieves state-of-the-art visual fidelity when synthesizing novel views and poses while significantly reducing the necessary learning time when compared to existing work. We demonstrate the versatility of our representation on a variety of articulated objects from common datasets and obtain reposable 3D reconstructions without the need of object-specific skeletal templates. Code will be made available at https://github.com/lukasuz/Articulated-Point-NeRF. ",
    "url": "https://arxiv.org/abs/2305.19065",
    "authors": [
      "Lukas Uzolas",
      "Elmar Eisemann",
      "Petr Kellnhofer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2305.19067",
    "title": "Multi-source adversarial transfer learning based on similar source  domains with local features",
    "abstract": "Transfer learning leverages knowledge from other domains and has been successful in many applications. Transfer learning methods rely on the overall similarity of the source and target domains. However, in some cases, it is impossible to provide an overall similar source domain, and only some source domains with similar local features can be provided. Can transfer learning be achieved? In this regard, we propose a multi-source adversarial transfer learning method based on local feature similarity to the source domain to handle transfer scenarios where the source and target domains have only local similarities. This method extracts transferable local features between a single source domain and the target domain through a sub-network. Specifically, the feature extractor of the sub-network is induced by the domain discriminator to learn transferable knowledge between the source domain and the target domain. The extracted features are then weighted by an attention module to suppress non-transferable local features while enhancing transferable local features. In order to ensure that the data from the target domain in different sub-networks in the same batch is exactly the same, we designed a multi-source domain independent strategy to provide the possibility for later local feature fusion to complete the key features required. In order to verify the effectiveness of the method, we made the dataset \"Local Carvana Image Masking Dataset\". Applying the proposed method to the image segmentation task of the proposed dataset achieves better transfer performance than other multi-source transfer learning methods. It is shown that the designed transfer learning method is feasible for transfer scenarios where the source and target domains have only local similarities. ",
    "url": "https://arxiv.org/abs/2305.19067",
    "authors": [
      "Yifu Zhang",
      "Hongru Li",
      "Shimeng Shi",
      "Youqi Li",
      "Jiansong Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.19068",
    "title": "Complex Query Answering on Eventuality Knowledge Graph with Implicit  Logical Constraints",
    "abstract": "Querying incomplete knowledge graphs (KGs) using deep learning approaches can naturally leverage the reasoning and generalization ability to learn to infer better answers. Traditional neural complex query answering (CQA) approaches mostly work on entity-centric KGs. However, in the real world, we also need to make logical inferences about events, states, and activities (i.e., eventualities or situations) to push learning systems from System I to System II, as proposed by Yoshua Bengio. Querying logically from an EVentuality-centric KG (EVKG) can naturally provide references to such kind of intuitive and logical inference. Thus, in this paper, we propose a new framework to leverage neural methods to answer complex logical queries based on an EVKG, which can satisfy not only traditional first-order logic constraints but also implicit logical constraints over eventualities concerning their occurrences and orders. For instance, if we know that ``Food is bad'' happens before ``PersonX adds soy sauce,'' then ``PersonX adds soy sauce'' is unlikely to be the cause of ``Food is bad'' due to implicit temporal constraint. To facilitate consistent reasoning on EVKGs, we propose Complex Eventuality Query Answering (CEQA), a more rigorous definition of CQA that considers the implicit logical constraints governing the temporal order and occurrence of eventualities. In this manner, we propose to leverage theorem provers for constructing benchmark datasets to ensure the answers satisfy implicit logical constraints. We also propose a Memory-Enhanced Query Encoding (MEQE) approach to significantly improve the performance of state-of-the-art neural query encoders on the CEQA task. ",
    "url": "https://arxiv.org/abs/2305.19068",
    "authors": [
      "Jiaxin Bai",
      "Xin Liu",
      "Weiqi Wang",
      "Chen Luo",
      "Yangqiu Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2305.19069",
    "title": "Multi-source adversarial transfer learning for ultrasound image  segmentation with limited similarity",
    "abstract": "Lesion segmentation of ultrasound medical images based on deep learning techniques is a widely used method for diagnosing diseases. Although there is a large amount of ultrasound image data in medical centers and other places, labeled ultrasound datasets are a scarce resource, and it is likely that no datasets are available for new tissues/organs. Transfer learning provides the possibility to solve this problem, but there are too many features in natural images that are not related to the target domain. As a source domain, redundant features that are not conducive to the task will be extracted. Migration between ultrasound images can avoid this problem, but there are few types of public datasets, and it is difficult to find sufficiently similar source domains. Compared with natural images, ultrasound images have less information, and there are fewer transferable features between different ultrasound images, which may cause negative transfer. To this end, a multi-source adversarial transfer learning network for ultrasound image segmentation is proposed. Specifically, to address the lack of annotations, the idea of adversarial transfer learning is used to adaptively extract common features between a certain pair of source and target domains, which provides the possibility to utilize unlabeled ultrasound data. To alleviate the lack of knowledge in a single source domain, multi-source transfer learning is adopted to fuse knowledge from multiple source domains. In order to ensure the effectiveness of the fusion and maximize the use of precious data, a multi-source domain independent strategy is also proposed to improve the estimation of the target domain data distribution, which further increases the learning ability of the multi-source adversarial migration learning network in multiple domains. ",
    "url": "https://arxiv.org/abs/2305.19069",
    "authors": [
      "Yifu Zhang",
      "Hongru Li",
      "Tao Yang",
      "Rui Tao",
      "Zhengyuan Liu",
      "Shimeng Shi",
      "Jiansong Zhang",
      "Ning Ma",
      "Wujin Feng",
      "Zhanhu Zhang",
      "Xinyu Zhang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.19083",
    "title": "Defense Against Shortest Path Attacks",
    "abstract": "Identifying shortest paths between nodes in a network is an important task in applications involving routing of resources. Recent work has shown that a malicious actor can manipulate a graph to make traffic between two nodes of interest follow their target path. In this paper, we develop a defense against such attacks by modifying the weights of the graph that users observe. The defender must balance inhibiting the attacker against any negative effects of the defense on benign users. Specifically, the defender's goals are: (a) to recommend the shortest paths possible to users, (b) for the lengths of the shortest paths in the published graph to be close to those of the same paths in the true graph, and (c) to minimize the probability of an attack. We formulate the defense as a Stackelberg game in which the defender is the leader and the attacker is the follower. In this context, we also consider a zero-sum version of the game, in which the defender's goal is to minimize cost while achieving the minimum possible attack probability. We show that this problem is NP-hard and propose heuristic solutions based on increasing edge weights along target paths in both the zero-sum and non-zero-sum settings. Relaxing some constraints of the original problem, we formulate a linear program for local optimization around a feasible point. We present defense results with both synthetic and real network datasets and show that these methods often reach the lower bound of the defender's cost. ",
    "url": "https://arxiv.org/abs/2305.19083",
    "authors": [
      "Benjamin A. Miller",
      "Zohair Shafi",
      "Wheeler Ruml",
      "Yevgeniy Vorobeychik",
      "Tina Eliassi-Rad",
      "Scott Alfeld"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2305.19084",
    "title": "Joint Optimization of Class-Specific Training- and Test-Time Data  Augmentation in Segmentation",
    "abstract": "This paper presents an effective and general data augmentation framework for medical image segmentation. We adopt a computationally efficient and data-efficient gradient-based meta-learning scheme to explicitly align the distribution of training and validation data which is used as a proxy for unseen test data. We improve the current data augmentation strategies with two core designs. First, we learn class-specific training-time data augmentation (TRA) effectively increasing the heterogeneity within the training subsets and tackling the class imbalance common in segmentation. Second, we jointly optimize TRA and test-time data augmentation (TEA), which are closely connected as both aim to align the training and test data distribution but were so far considered separately in previous works. We demonstrate the effectiveness of our method on four medical image segmentation tasks across different scenarios with two state-of-the-art segmentation models, DeepMedic and nnU-Net. Extensive experimentation shows that the proposed data augmentation framework can significantly and consistently improve the segmentation performance when compared to existing solutions. Code is publicly available. ",
    "url": "https://arxiv.org/abs/2305.19084",
    "authors": [
      "Zeju Li",
      "Konstantinos Kamnitsas",
      "Qi Dou",
      "Chen Qin",
      "Ben Glocker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.19088",
    "title": "TrueDeep: A systematic approach of crack detection with less data",
    "abstract": "Supervised and semi-supervised semantic segmentation algorithms require significant amount of annotated data to achieve a good performance. In many situations, the data is either not available or the annotation is expensive. The objective of this work is to show that by incorporating domain knowledge along with deep learning architectures, we can achieve similar performance with less data. We have used publicly available crack segmentation datasets and shown that selecting the input images using knowledge can significantly boost the performance of deep-learning based architectures. Our proposed approaches have many fold advantages such as low annotation and training cost, and less energy consumption. We have measured the performance of our algorithm quantitatively in terms of mean intersection over union (mIoU) and F score. Our algorithms, developed with 23% of the overall data; have a similar performance on the test data and significantly better performance on multiple blind datasets. ",
    "url": "https://arxiv.org/abs/2305.19088",
    "authors": [
      "Ram Krishna Pandey",
      "Akshit Achara"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.19101",
    "title": "Which Models have Perceptually-Aligned Gradients? An Explanation via  Off-Manifold Robustness",
    "abstract": "One of the remarkable properties of robust computer vision models is that their input-gradients are often aligned with human perception, referred to in the literature as perceptually-aligned gradients (PAGs). Despite only being trained for classification, PAGs cause robust models to have rudimentary generative capabilities, including image generation, denoising, and in-painting. However, the underlying mechanisms behind these phenomena remain unknown. In this work, we provide a first explanation of PAGs via \\emph{off-manifold robustness}, which states that models must be more robust off- the data manifold than they are on-manifold. We first demonstrate theoretically that off-manifold robustness leads input gradients to lie approximately on the data manifold, explaining their perceptual alignment. We then show that Bayes optimal models satisfy off-manifold robustness, and confirm the same empirically for robust models trained via gradient norm regularization, noise augmentation, and randomized smoothing. Quantifying the perceptual alignment of model gradients via their similarity with the gradients of generative models, we show that off-manifold robustness correlates well with perceptual alignment. Finally, based on the levels of on- and off-manifold robustness, we identify three different regimes of robustness that affect both perceptual alignment and model accuracy: weak robustness, bayes-aligned robustness, and excessive robustness. ",
    "url": "https://arxiv.org/abs/2305.19101",
    "authors": [
      "Suraj Srinivas",
      "Sebastian Bordt",
      "Hima Lakkaraju"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.19103",
    "title": "Does Conceptual Representation Require Embodiment? Insights From Large  Language Models",
    "abstract": "Recent advances in large language models (LLM) have the potential to shed light on the debate regarding the extent to which knowledge representation requires the grounding of embodied experience. Despite learning from limited modalities (e.g., text for GPT-3.5, and text+image for GPT-4), LLMs have nevertheless demonstrated human-like behaviors in various psychology tasks, which may provide an alternative interpretation of the acquisition of conceptual knowledge. We compared lexical conceptual representations between humans and ChatGPT (GPT-3.5 and GPT-4) on subjective ratings of various lexical conceptual features or dimensions (e.g., emotional arousal, concreteness, haptic, etc.). The results show that both GPT-3.5 and GPT-4 were strongly correlated with humans in some abstract dimensions, such as emotion and salience. In dimensions related to sensory and motor domains, GPT-3.5 shows weaker correlations while GPT-4 has made significant progress compared to GPT-3.5. Still, GPT-4 struggles to fully capture motor aspects of conceptual knowledge such as actions with foot/leg, mouth/throat, and torso. Moreover, we found that GPT-4's progress can largely be associated with its training in the visual domain. Certain aspects of conceptual representation appear to exhibit a degree of independence from sensory capacities, but others seem to necessitate them. Our findings provide insights into the complexities of knowledge representation from diverse perspectives and highlights the potential influence of embodied experience in shaping language and cognition. ",
    "url": "https://arxiv.org/abs/2305.19103",
    "authors": [
      "Qihui Xu",
      "Yingying Peng",
      "Minghua Wu",
      "Feng Xiao",
      "Martin Chodorow",
      "Ping Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.19105",
    "title": "A Versatile Wireless Network Protocol for Spectrum Sharing with Passive  Radio Services",
    "abstract": "With the proliferation of wideband active services in bands shared with passive receivers for remote sensing and radio astronomy, new methods are needed for deconflicting active and passive users. We have developed a technique for active/passive user coordination that is compatible with essentially any existing wireless communications protocol. The passive user transmits an on-off keying modulated signal that can be detected by active radios using simple channel power estimates. Using off-the-shelf WiFi and LoRa hardware and on a software defined radio implementation of LTE, we show that Dynamic Passive to Active Spectrum Sharing (DPASS) is effective on a wide range of frequencies and physical layer implementations. We validate the protocol using these three technologies by demonstrating that each device receives a DPASS packet and dynamically takes an appropriate spectrum coordination action, including shutting off transmissions or switching frequencies. ",
    "url": "https://arxiv.org/abs/2305.19105",
    "authors": [
      "Ashton Palacios",
      "Dinah Bronson",
      "Jon Backman",
      "Karl Warnick",
      "Philip Lundrigan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2305.19112",
    "title": "DENTEX: An Abnormal Tooth Detection with Dental Enumeration and  Diagnosis Benchmark for Panoramic X-rays",
    "abstract": "Panoramic X-rays are frequently used in dentistry for treatment planning, but their interpretation can be both time-consuming and prone to error. Artificial intelligence (AI) has the potential to aid in the analysis of these X-rays, thereby improving the accuracy of dental diagnoses and treatment plans. Nevertheless, designing automated algorithms for this purpose poses significant challenges, mainly due to the scarcity of annotated data and variations in anatomical structure. To address these issues, the Dental Enumeration and Diagnosis on Panoramic X-rays Challenge (DENTEX) has been organized in association with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2023. This challenge aims to promote the development of algorithms for multi-label detection of abnormal teeth, using three types of hierarchically annotated data: partially annotated quadrant data, partially annotated quadrant-enumeration data, and fully annotated quadrant-enumeration-diagnosis data, inclusive of four different diagnoses. In this paper, we present the results of evaluating participant algorithms on the fully annotated data, additionally investigating performance variation for quadrant, enumeration, and diagnosis labels in the detection of abnormal teeth. The provision of this annotated dataset, alongside the results of this challenge, may lay the groundwork for the creation of AI-powered tools that can offer more precise and efficient diagnosis and treatment planning in the field of dentistry. The evaluation code and datasets can be accessed at https://github.com/ibrahimethemhamamci/DENTEX ",
    "url": "https://arxiv.org/abs/2305.19112",
    "authors": [
      "Ibrahim Ethem Hamamci",
      "Sezgin Er",
      "Enis Simsar",
      "Atif Emre Yuksel",
      "Sadullah Gultekin",
      "Serife Damla Ozdemir",
      "Kaiyuan Yang",
      "Hongwei Bran Li",
      "Sarthak Pati",
      "Bernd Stadlinger",
      "Albert Mehl",
      "Mustafa Gundogar",
      "Bjoern Menze"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.19115",
    "title": "High-Gain Disturbance Observer for Robust Trajectory Tracking of  Quadrotors",
    "abstract": "This paper presents a simple method to boost the robustness of quadrotors in trajectory tracking. The presented method features a high-gain disturbance observer (HGDO) that provides disturbance estimates in real-time. The estimates are then used in a trajectory control law to compensate for disturbance effects. We present theoretical convergence results showing that the proposed HGDO can quickly converge to an adjustable neighborhood of actual disturbance values. We will then integrate the disturbance estimates with a typical robust trajectory controller, namely sliding mode control (SMC), and present Lyapunov stability analysis to establish the boundedness of trajectory tracking errors. However, our stability analysis can be easily extended to other Lyapunov-based controllers to develop different HGDO-based controllers with formal stability guarantees. We evaluate the proposed HGDO-based control method using both simulation and laboratory experiments in various scenarios and in the presence of external disturbances. Our results indicate that the addition of HGDO to a quadrotor trajectory controller can significantly improve the accuracy and precision of trajectory tracking in the presence of external disturbances. ",
    "url": "https://arxiv.org/abs/2305.19115",
    "authors": [
      "Mohammadreza Izadi",
      "Reza Faieghi"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2305.19125",
    "title": "Hierarchical Graph Generation with $K^2$-trees",
    "abstract": "Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation which was originally designed for lossless graph compression. Our motivation stems from the ability of the $K^2$-trees to enable compact generation while concurrently capturing the inherent hierarchical structure of a graph. In addition, we make further contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation. ",
    "url": "https://arxiv.org/abs/2305.19125",
    "authors": [
      "Yunhui Jang",
      "Dongwoo Kim",
      "Sungsoo Ahn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2305.19130",
    "title": "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using  Spatial Transformer Networks",
    "abstract": "Thanks to the latest deep learning algorithms, silent speech interfaces (SSI) are now able to synthesize intelligible speech from articulatory movement data under certain conditions. However, the resulting models are rather speaker-specific, making a quick switch between users troublesome. Even for the same speaker, these models perform poorly cross-session, i.e. after dismounting and re-mounting the recording equipment. To aid quick speaker and session adaptation of ultrasound tongue imaging-based SSI models, we extend our deep networks with a spatial transformer network (STN) module, capable of performing an affine transformation on the input images. Although the STN part takes up only about 10\\% of the network, our experiments show that adapting just the STN module might allow to reduce MSE by 88\\% on the average, compared to retraining the whole network. The improvement is even larger (around 92\\%) when adapting the network to different recording sessions from the same speaker. ",
    "url": "https://arxiv.org/abs/2305.19130",
    "authors": [
      "L\u00e1szl\u00f3 T\u00f3th",
      "Amin Honarmandi Shandiz",
      "G\u00e1bor Gosztolya",
      "Csap\u00f3 Tam\u00e1s G\u00e1bor"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2305.19144",
    "title": "BLEU Meets COMET: Combining Lexical and Neural Metrics Towards Robust  Machine Translation Evaluation",
    "abstract": "Although neural-based machine translation evaluation metrics, such as COMET or BLEURT, have achieved strong correlations with human judgements, they are sometimes unreliable in detecting certain phenomena that can be considered as critical errors, such as deviations in entities and numbers. In contrast, traditional evaluation metrics, such as BLEU or chrF, which measure lexical or character overlap between translation hypotheses and human references, have lower correlations with human judgements but are sensitive to such deviations. In this paper, we investigate several ways of combining the two approaches in order to increase robustness of state-of-the-art evaluation methods to translations with critical errors. We show that by using additional information during training, such as sentence-level features and word-level tags, the trained metrics improve their capability to penalize translations with specific troublesome phenomena, which leads to gains in correlation with human judgments and on recent challenge sets on several language pairs. ",
    "url": "https://arxiv.org/abs/2305.19144",
    "authors": [
      "Taisiya Glushkova",
      "Chrysoula Zerva",
      "Andr\u00e9 F. T. Martins"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.19153",
    "title": "FERN: Leveraging Graph Attention Networks for Failure Evaluation and  Robust Network Design",
    "abstract": "Robust network design, which aims to guarantee network availability under various failure scenarios while optimizing performance/cost objectives, has received significant attention. Existing approaches often rely on model-based mixed-integer optimization that is hard to scale or employ deep learning to solve specific engineering problems yet with limited generalizability. In this paper, we show that failure evaluation provides a common kernel to improve the tractability and scalability of existing solutions. By providing a neural network function approximation of this common kernel using graph attention networks, we develop a unified learning-based framework, FERN, for scalable Failure Evaluation and Robust Network design. FERN represents rich problem inputs as a graph and captures both local and global views by attentively performing feature extraction from the graph. It enables a broad range of robust network design problems, including robust network validation, network upgrade optimization, and fault-tolerant traffic engineering that are discussed in this paper, to be recasted with respect to the common kernel and thus computed efficiently using neural networks and over a small set of critical failure scenarios. Extensive experiments on real-world network topologies show that FERN can efficiently and accurately identify key failure scenarios for both OSPF and optimal routing scheme, and generalizes well to different topologies and input traffic patterns. It can speed up multiple robust network design problems by more than 80x, 200x, 10x, respectively with negligible performance gap. ",
    "url": "https://arxiv.org/abs/2305.19153",
    "authors": [
      "Chenyi Liu",
      "Vaneet Aggarwal",
      "Tian Lan",
      "Nan Geng",
      "Yuan Yang",
      "Mingwei Xu",
      "Qing Li"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.19157",
    "title": "Sensor Fault Detection and Compensation with Performance Prescription  for Robotic Manipulators",
    "abstract": "This paper focuses on sensor fault detection and compensation for robotic manipulators. The proposed method features a new adaptive observer and a new terminal sliding mode control law established on a second-order integral sliding surface. The method enables sensor fault detection without the need to impose known bounds on fault value and/or its derivative. It also enables fast and fixed-time fault-tolerant control whose performance can be prescribed beforehand by defining funnel bounds on the tracking error. The ultimate boundedness of the estimation errors for the proposed observer and the fixed-time stability of the control system are shown using Lyapunov stability analysis. The effectiveness of the proposed method is verified using numerical simulations on two different robotic manipulators, and the results are compared with existing methods. Our results demonstrate performance gains obtained by the proposed method compared to the existing results. ",
    "url": "https://arxiv.org/abs/2305.19157",
    "authors": [
      "S. Mohammadreza Ebrahimi",
      "Farid Norouzi",
      "Hossein Dastres",
      "Reza Faieghi",
      "Mehdi Naderi",
      "Milad Malekzadeh"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2305.19160",
    "title": "Recognizing People by Body Shape Using Deep Networks of Images and Words",
    "abstract": "Common and important applications of person identification occur at distances and viewpoints in which the face is not visible or is not sufficiently resolved to be useful. We examine body shape as a biometric across distance and viewpoint variation. We propose an approach that combines standard object classification networks with representations based on linguistic (word-based) descriptions of bodies. Algorithms with and without linguistic training were compared on their ability to identify people from body shape in images captured across a large range of distances/views (close-range, 100m, 200m, 270m, 300m, 370m, 400m, 490m, 500m, 600m, and at elevated pitch in images taken by an unmanned aerial vehicle [UAV]). Accuracy, as measured by identity-match ranking and false accept errors in an open-set test, was surprisingly good. For identity-ranking, linguistic models were more accurate for close-range images, whereas non-linguistic models fared better at intermediary distances. Fusion of the linguistic and non-linguistic embeddings improved performance at all, but the farthest distance. Although the non-linguistic model yielded fewer false accepts at all distances, fusion of the linguistic and non-linguistic models decreased false accepts for all, but the UAV images. We conclude that linguistic and non-linguistic representations of body shape can offer complementary identity information for bodies that can improve identification in applications of interest. ",
    "url": "https://arxiv.org/abs/2305.19160",
    "authors": [
      "Blake A. Myers",
      "Lucas Jaggernauth",
      "Thomas M. Metz",
      "Matthew Q. Hill",
      "Veda Nandan Gandi",
      "Carlos D. Castillo",
      "Alice J. O'Toole"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.19162",
    "title": "An AMR-based Link Prediction Approach for Document-level Event Argument  Extraction",
    "abstract": "Recent works have introduced Abstract Meaning Representation (AMR) for Document-level Event Argument Extraction (Doc-level EAE), since AMR provides a useful interpretation of complex semantic structures and helps to capture long-distance dependency. However, in these works AMR is used only implicitly, for instance, as additional features or training signals. Motivated by the fact that all event structures can be inferred from AMR, this work reformulates EAE as a link prediction problem on AMR graphs. Since AMR is a generic structure and does not perfectly suit EAE, we propose a novel graph structure, Tailored AMR Graph (TAG), which compresses less informative subgraphs and edge types, integrates span information, and highlights surrounding events in the same document. With TAG, we further propose a novel method using graph neural networks as a link prediction model to find event arguments. Our extensive experiments on WikiEvents and RAMS show that this simpler approach outperforms the state-of-the-art models by 3.63pt and 2.33pt F1, respectively, and do so with reduced 56% inference time. The code is availabel at https://github.com/ayyyq/TARA. ",
    "url": "https://arxiv.org/abs/2305.19162",
    "authors": [
      "Yuqing Yang",
      "Qipeng Guo",
      "Xiangkun Hu",
      "Yue Zhang",
      "Xipeng Qiu",
      "Zheng Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.19167",
    "title": "Reduced Precision Floating-Point Optimization for Deep Neural Network  On-Device Learning on MicroControllers",
    "abstract": "Enabling On-Device Learning (ODL) for Ultra-Low-Power Micro-Controller Units (MCUs) is a key step for post-deployment adaptation and fine-tuning of Deep Neural Network (DNN) models in future TinyML applications. This paper tackles this challenge by introducing a novel reduced precision optimization technique for ODL primitives on MCU-class devices, leveraging the State-of-Art advancements in RISC-V RV32 architectures with support for vectorized 16-bit floating-point (FP16) Single-Instruction Multiple-Data (SIMD) operations. Our approach for the Forward and Backward steps of the Back-Propagation training algorithm is composed of specialized shape transform operators and Matrix Multiplication (MM) kernels, accelerated with parallelization and loop unrolling. When evaluated on a single training step of a 2D Convolution layer, the SIMD-optimized FP16 primitives result up to 1.72$\\times$ faster than the FP32 baseline on a RISC-V-based 8+1-core MCU. An average computing efficiency of 3.11 Multiply and Accumulate operations per clock cycle (MAC/clk) and 0.81 MAC/clk is measured for the end-to-end training tasks of a ResNet8 and a DS-CNN for Image Classification and Keyword Spotting, respectively -- requiring 17.1 ms and 6.4 ms on the target platform to compute a training step on a single sample. Overall, our approach results more than two orders of magnitude faster than existing ODL software frameworks for single-core MCUs and outperforms by 1.6 $\\times$ previous FP32 parallel implementations on a Continual Learning setup. ",
    "url": "https://arxiv.org/abs/2305.19167",
    "authors": [
      "Davide Nadalini",
      "Manuele Rusci",
      "Luca Benini",
      "Francesco Conti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2305.19170",
    "title": "Forward-Forward Training of an Optical Neural Network",
    "abstract": "Neural networks (NN) have demonstrated remarkable capabilities in various tasks, but their computation-intensive nature demands faster and more energy-efficient hardware implementations. Optics-based platforms, using technologies such as silicon photonics and spatial light modulators, offer promising avenues for achieving this goal. However, training multiple trainable layers in tandem with these physical systems poses challenges, as they are difficult to fully characterize and describe with differentiable functions, hindering the use of error backpropagation algorithm. The recently introduced Forward-Forward Algorithm (FFA) eliminates the need for perfect characterization of the learning system and shows promise for efficient training with large numbers of programmable parameters. The FFA does not require backpropagating an error signal to update the weights, rather the weights are updated by only sending information in one direction. The local loss function for each set of trainable weights enables low-power analog hardware implementations without resorting to metaheuristic algorithms or reinforcement learning. In this paper, we present an experiment utilizing multimode nonlinear wave propagation in an optical fiber demonstrating the feasibility of the FFA approach using an optical system. The results show that incorporating optical transforms in multilayer NN architectures trained with the FFA, can lead to performance improvements, even with a relatively small number of trainable weights. The proposed method offers a new path to the challenge of training optical NNs and provides insights into leveraging physical transformations for enhancing NN performance. ",
    "url": "https://arxiv.org/abs/2305.19170",
    "authors": [
      "Ilker Oguz",
      "Junjie Ke",
      "Qifei Wang",
      "Feng Yang",
      "Mustafa Yildirim",
      "Niyazi Ulas Dinc",
      "Jih-Liang Hsieh",
      "Christophe Moser",
      "Demetri Psaltis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2305.19181",
    "title": "Table Detection for Visually Rich Document Images",
    "abstract": "Table Detection (TD) is a fundamental task towards visually rich document understanding. Current studies usually formulate the TD problem as an object detection problem, then leverage Intersection over Union (IoU) based metrics to evaluate the model performance and IoU-based loss functions to optimize the model. TD applications usually require the prediction results to cover all the table contents and avoid information loss. However, IoU and IoU-based loss functions cannot directly reflect the degree of information loss for the prediction results. Therefore, we propose to decouple IoU into a ground truth coverage term and a prediction coverage term, in which the former can be used to measure the information loss of the prediction results. Besides, tables in the documents are usually large, sparsely distributed, and have no overlaps because they are designed to summarize essential information to make it easy to read and interpret for human readers. Therefore, in this study, we use SparseR-CNN as the base model, and further improve the model by using Gaussian Noise Augmented Image Size region proposals and many-to-one label assignments. To demonstrate the effectiveness of proposed method and compare with state-of-the-art methods fairly, we conduct experiments and use IoU-based evaluation metrics to evaluate the model performance. The experimental results show that the proposed method can consistently outperform state-of-the-art methods under different IoU-based metric on a variety of datasets. We conduct further experiments to show the superiority of the proposed decoupled IoU for the TD applications by replacing the IoU-based loss functions and evaluation metrics with proposed decoupled IoU counterparts. The experimental results show that our proposed decoupled IoU loss can encourage the model to alleviate information loss. ",
    "url": "https://arxiv.org/abs/2305.19181",
    "authors": [
      "Bin Xiao",
      "Murat Simsek",
      "Burak Kantarci",
      "Ala Abu Alkheir"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2305.19182",
    "title": "Optimal Hub Placement and Deadlock-Free Routing for Payment Channel  Network Scalability",
    "abstract": "As a promising implementation model of payment channel network (PCN), payment channel hub (PCH) could achieve high throughput by providing stable off-chain transactions through powerful hubs. However, existing PCH schemes assume hubs preplaced in advance, not considering payment requests' distribution and may affect network scalability, especially network load balancing. In addition, current source routing protocols with PCH allow each sender to make routing decision on his/her own request, which may have a bad effect on performance scalability (e.g., deadlock) for not considering other senders' requests. This paper proposes a novel multi-PCHs solution with high scalability. First, we are the first to study the PCH placement problem and propose optimal/approximation solutions with load balancing for small-scale and large-scale scenarios, by trading off communication costs among participants and turning the original NP-hard problem into a mixed-integer linear programming (MILP) problem solving by supermodular techniques. Then, on global network states and local directly connected clients' requests, a routing protocol is designed for each PCH with a dynamic adjustment strategy on request processing rates, enabling high-performance deadlock-free routing. Extensive experiments show that our work can effectively balance the network load, and improve the performance on throughput by 29.3% on average compared with state-of-the-arts. ",
    "url": "https://arxiv.org/abs/2305.19182",
    "authors": [
      "Lingxiao Yang",
      "Xuewen Dong",
      "Sheng Gao",
      "Qiang Qu",
      "Xiaodong Zhang",
      "Wensheng Tian",
      "Yulong Shen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2305.19185",
    "title": "Compression with Bayesian Implicit Neural Representations",
    "abstract": "Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a progressive refinement process for the variational posterior that significantly enhances performance. Experiments show that our method achieves strong performance on image and audio compression while retaining simplicity. ",
    "url": "https://arxiv.org/abs/2305.19185",
    "authors": [
      "Zongyu Guo",
      "Gergely Flamich",
      "Jiajun He",
      "Zhibo Chen",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.19190",
    "title": "Inverse Approximation Theory for Nonlinear Recurrent Neural Networks",
    "abstract": "We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using RNNs. This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships, viewed as functional sequences, that can be stably approximated by RNNs with hardtanh/tanh activations must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. Based on the analysis, we propose a principled reparameterization method to overcome the limitations. Our theoretical results are confirmed by numerical experiments. ",
    "url": "https://arxiv.org/abs/2305.19190",
    "authors": [
      "Shida Wang",
      "Zhong Li",
      "Qianxiao Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2305.19191",
    "title": "Method of Exact Solutions Code Verification of a Superelastic  Constitutive Model in a Commercial Finite Element Solver",
    "abstract": "The superelastic constitutive model implemented in the commercial finite element code ABAQUS is verified using the method of exact solutions (MES). An analytical solution for uniaxial strain is first developed under a set of simplifying assumptions including von Mises-like transformation surfaces, symmetric transformation behavior, and monotonic loading. Numerical simulations are then performed, and simulation predictions are compared to the exact analytical solutions. Results reveal the superelasticity model agrees with the analytical solution to within one ten-thousandth of a percent (0.0001%) or less for stress and strain quantities of interest when using displacement-driven boundary conditions. Full derivation of the analytical solution is provided in an Appendix, and simulation input files and post-processing scripts are provided as supplemental material. ",
    "url": "https://arxiv.org/abs/2305.19191",
    "authors": [
      "Kenneth I. Aycock",
      "Nuno Rebelo",
      "Brent A. Craven"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Applied Physics (physics.app-ph)"
    ]
  },
  {
    "id": "arXiv:2305.19194",
    "title": "FakeSwarm: Improving Fake News Detection with Swarming Characteristics",
    "abstract": "The proliferation of fake news poses a serious threat to society, as it can misinform and manipulate the public, erode trust in institutions, and undermine democratic processes. To address this issue, we present FakeSwarm, a fake news identification system that leverages the swarming characteristics of fake news. To extract the swarm behavior, we propose a novel concept of fake news swarming characteristics and design three types of swarm features, including principal component analysis, metric representation, and position encoding. We evaluate our system on a public dataset and demonstrate the effectiveness of incorporating swarm features in fake news identification, achieving an f1-score and accuracy of over 97% by combining all three types of swarm features. Furthermore, we design an online learning pipeline based on the hypothesis of the temporal distribution pattern of fake news emergence, validated on a topic with early emerging fake news and a shortage of text samples, showing that swarm features can significantly improve recall rates in such cases. Our work provides a new perspective and approach to fake news detection and highlights the importance of considering swarming characteristics in detecting fake news. ",
    "url": "https://arxiv.org/abs/2305.19194",
    "authors": [
      "Jun Wu",
      "Xuesong Ye"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.19211",
    "title": "COVID-19 Detection from Mass Spectra of Exhaled Breath",
    "abstract": "According to the World Health Organization, the SARS-CoV-2 virus generated a global emergency between 2020 and 2023 resulting in about 7 million deaths out of more than 750 million individuals diagnosed with COVID-19. During these years, polymerase-chain-reaction and antigen testing played a prominent role in disease control. In this study, we propose a fast and non-invasive detection system exploiting a proprietary mass spectrometer to measure ions in exhaled breath. We demonstrated that infected individuals, even if asymptomatic, exhibit characteristics in the air expelled from the lungs that can be detected by a nanotech-based technology and then recognized by soft-computing algorithms. A clinical trial was ran on about 300 patients: the mass spectra in the 10-351 mass-to-charge range were measured, suitably pre-processed, and analyzed by different classification models; eventually, the system shown an accuracy of 95% and a recall of 94% in identifying cases of COVID-19. With performances comparable to traditional methodologies, the proposed system could play a significant role in both routine examination for common diseases and emergency response for new epidemics. ",
    "url": "https://arxiv.org/abs/2305.19211",
    "authors": [
      "Nicol\u00f2 Bellarmino",
      "Giorgio Bozzini",
      "Riccardo Cantoro",
      "Francesco Castelletti",
      "Michele Castelluzzo",
      "Carla Ciricugno",
      "Raffaele Correale",
      "Daniela Dalla Gasperina",
      "Francesco Dentali",
      "Giovanni Poggialini",
      "Piergiorgio Salerno",
      "Giovanni Squillero",
      "Stefano Taborelli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2305.19213",
    "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large  Language Models of Code",
    "abstract": "Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations. ",
    "url": "https://arxiv.org/abs/2305.19213",
    "authors": [
      "Xiao Liu",
      "Da Yin",
      "Chen Zhang",
      "Yansong Feng",
      "Dongyan Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.19218",
    "title": "Adversarial Attacks on Online Learning to Rank with Stochastic Click  Models",
    "abstract": "We propose the first study of adversarial attacks on online learning to rank. The goal of the adversary is to misguide the online learning to rank algorithm to place the target item on top of the ranking list linear times to time horizon $T$ with a sublinear attack cost. We propose generalized list poisoning attacks that perturb the ranking list presented to the user. This strategy can efficiently attack any no-regret ranker in general stochastic click models. Furthermore, we propose a click poisoning-based strategy named attack-then-quit that can efficiently attack two representative OLTR algorithms for stochastic click models. We theoretically analyze the success and cost upper bound of the two proposed methods. Experimental results based on synthetic and real-world data further validate the effectiveness and cost-efficiency of the proposed attack strategies. ",
    "url": "https://arxiv.org/abs/2305.19218",
    "authors": [
      "Zichen Wang",
      "Rishab Balasubramanian",
      "Hui Yuan",
      "Chenyu Song",
      "Mengdi Wang",
      "Huazheng Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2305.19230",
    "title": "Controlled Text Generation with Hidden Representation Transformations",
    "abstract": "We propose CHRT (Control Hidden Representation Transformation) - a controlled language generation framework that steers large language models to generate text pertaining to certain attributes (such as toxicity). CHRT gains attribute control by modifying the hidden representation of the base model through learned transformations. We employ a contrastive-learning framework to learn these transformations that can be combined to gain multi-attribute control. The effectiveness of CHRT is experimentally shown by comparing it with seven baselines over three attributes. CHRT outperforms all the baselines in the task of detoxification, positive sentiment steering, and text simplification while minimizing the loss in linguistic qualities. Further, our approach has the lowest inference latency of only 0.01 seconds more than the base model, making it the most suitable for high-performance production environments. We open-source our code and release two novel datasets to further propel controlled language generation research. ",
    "url": "https://arxiv.org/abs/2305.19230",
    "authors": [
      "Vaibhav Kumar",
      "Hana Koorehdavoudi",
      "Masud Moshtaghi",
      "Amita Misra",
      "Ankit Chadha",
      "Emilio Ferrara"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.19235",
    "title": "On the Stability of Gated Graph Neural Networks",
    "abstract": "In this paper, we aim to find the conditions for input-state stability (ISS) and incremental input-state stability ($\\delta$ISS) of Gated Graph Neural Networks (GGNNs). We show that this recurrent version of Graph Neural Networks (GNNs) can be expressed as a dynamical distributed system and, as a consequence, can be analysed using model-based techniques to assess its stability and robustness properties. Then, the stability criteria found can be exploited as constraints during the training process to enforce the internal stability of the neural network. Two distributed control examples, flocking and multi-robot motion control, show that using these conditions increases the performance and robustness of the gated GNNs. ",
    "url": "https://arxiv.org/abs/2305.19235",
    "authors": [
      "Antonio Marino",
      "Claudio Pacchierotti",
      "Paolo Robuffo Giordano"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2305.19241",
    "title": "Accountable authentication with privacy protection: The Larch system for  universal login",
    "abstract": "Credential compromise is hard to detect and hard to mitigate. To address this problem, we present larch, an accountable authentication framework with strong security and privacy properties. Larch protects user privacy while ensuring that the larch log server correctly records every authentication. Specifically, an attacker who compromises a user's device cannot authenticate without creating evidence in the log, and the log cannot learn which web service (relying party) the user is authenticating to. To enable fast adoption, larch is backwards-compatible with relying parties that support FIDO2, TOTP, and password-based login. Furthermore, larch does not degrade the security and privacy a user already expects: the log server cannot authenticate on behalf of a user, and larch does not allow relying parties to link a user across accounts. We implement larch for FIDO2, TOTP, and password-based login. Given a client with four cores and a log server with eight cores, an authentication with larch takes 150ms for FIDO2, 91ms for TOTP, and 74ms for passwords (excluding preprocessing, which takes 1.23s for TOTP). ",
    "url": "https://arxiv.org/abs/2305.19241",
    "authors": [
      "Emma Dauterman",
      "Danny Lin",
      "Henry Corrigan-Gibbs",
      "David Mazi\u00e8res"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2305.19271",
    "title": "Concise Answers to Complex Questions: Summarization of Long-form Answers",
    "abstract": "Long-form question answering systems provide rich information by presenting paragraph-level answers, often containing optional background or auxiliary information. While such comprehensive answers are helpful, not all information is required to answer the question (e.g. users with domain knowledge do not need an explanation of background). Can we provide a concise version of the answer by summarizing it, while still addressing the question? We conduct a user study on summarized answers generated from state-of-the-art models and our newly proposed extract-and-decontextualize approach. We find a large proportion of long-form answers (over 90%) in the ELI5 domain can be adequately summarized by at least one system, while complex and implicit answers are challenging to compress. We observe that decontextualization improves the quality of the extractive summary, exemplifying its potential in the summarization task. To promote future work, we provide an extractive summarization dataset covering 1K long-form answers and our user study annotations. Together, we present the first study on summarizing long-form answers, taking a step forward for QA agents that can provide answers at multiple granularities. ",
    "url": "https://arxiv.org/abs/2305.19271",
    "authors": [
      "Abhilash Potluri",
      "Fangyuan Xu",
      "Eunsol Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18361",
    "title": "Deep learning network to correct axial and coronal eye motion in 3D OCT  retinal imaging",
    "abstract": "Optical Coherence Tomography (OCT) is one of the most important retinal imaging technique. However, involuntary motion artifacts still pose a major challenge in OCT imaging that compromises the quality of downstream analysis, such as retinal layer segmentation and OCT Angiography. We propose deep learning based neural networks to correct axial and coronal motion artifacts in OCT based on a single volumetric scan. The proposed method consists of two fully-convolutional neural networks that predict Z and X dimensional displacement maps sequentially in two stages. The experimental result shows that the proposed method can effectively correct motion artifacts and achieve smaller error than other methods. Specifically, the method can recover the overall curvature of the retina, and can be generalized well to various diseases and resolutions. ",
    "url": "https://arxiv.org/abs/2305.18361",
    "authors": [
      "Yiqian Wang",
      "Alexandra Warter",
      "Melina Cavichini",
      "Varsha Alex",
      "Dirk-Uwe G. Bartsch",
      "William R. Freeman",
      "Truong Q. Nguyen",
      "Cheolhong An"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.18370",
    "title": "Explainable Brain Age Prediction using coVariance Neural Networks",
    "abstract": "In computational neuroscience, there has been an increased interest in developing machine learning algorithms that leverage brain imaging data to provide estimates of \"brain age\" for an individual. Importantly, the discordance between brain age and chronological age (referred to as \"brain age gap\") can capture accelerated aging due to adverse health conditions and therefore, can reflect increased vulnerability towards neurological disease or cognitive impairments. However, widespread adoption of brain age for clinical decision support has been hindered due to lack of transparency and methodological justifications in most existing brain age prediction algorithms. In this paper, we leverage coVariance neural networks (VNN) to propose an anatomically interpretable framework for brain age prediction using cortical thickness features. Specifically, our brain age prediction framework extends beyond the coarse metric of brain age gap in Alzheimer's disease (AD) and we make two important observations: (i) VNNs can assign anatomical interpretability to elevated brain age gap in AD by identifying contributing brain regions, (ii) the interpretability offered by VNNs is contingent on their ability to exploit specific eigenvectors of the anatomical covariance matrix. Together, these observations facilitate an explainable perspective to the task of brain age prediction. ",
    "url": "https://arxiv.org/abs/2305.18370",
    "authors": [
      "Saurabh Sihag",
      "Gonzalo Mateos",
      "Corey T. McMillan",
      "Alejandro Ribeiro"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2305.18383",
    "title": "A Three-regime Model of Network Pruning",
    "abstract": "Recent work has highlighted the complex influence training hyperparameters, e.g., the number of training epochs, can have on the prunability of machine learning models. Perhaps surprisingly, a systematic approach to predict precisely how adjusting a specific hyperparameter will affect prunability remains elusive. To address this gap, we introduce a phenomenological model grounded in the statistical mechanics of learning. Our approach uses temperature-like and load-like parameters to model the impact of neural network (NN) training hyperparameters on pruning performance. A key empirical result we identify is a sharp transition phenomenon: depending on the value of a load-like parameter in the pruned model, increasing the value of a temperature-like parameter in the pre-pruned model may either enhance or impair subsequent pruning performance. Based on this transition, we build a three-regime model by taxonomizing the global structure of the pruned NN loss landscape. Our model reveals that the dichotomous effect of high temperature is associated with transitions between distinct types of global structures in the post-pruned model. Based on our results, we present three case-studies: 1) determining whether to increase or decrease a hyperparameter for improved pruning; 2) selecting the best model to prune from a family of models; and 3) tuning the hyperparameter of the Sharpness Aware Minimization method for better pruning performance. ",
    "url": "https://arxiv.org/abs/2305.18383",
    "authors": [
      "Yefan Zhou",
      "Yaoqing Yang",
      "Arin Chang",
      "Michael W. Mahoney"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18386",
    "title": "A Synergistic Framework Leveraging Autoencoders and Generative  Adversarial Networks for the Synthesis of Computational Fluid Dynamics  Results in Aerofoil Aerodynamics",
    "abstract": "In the realm of computational fluid dynamics (CFD), accurate prediction of aerodynamic behaviour plays a pivotal role in aerofoil design and optimization. This study proposes a novel approach that synergistically combines autoencoders and Generative Adversarial Networks (GANs) for the purpose of generating CFD results. Our innovative framework harnesses the intrinsic capabilities of autoencoders to encode aerofoil geometries into a compressed and informative 20-length vector representation. Subsequently, a conditional GAN network adeptly translates this vector into precise pressure-distribution plots, accounting for fixed wind velocity, angle of attack, and turbulence level specifications. The training process utilizes a meticulously curated dataset acquired from JavaFoil software, encompassing a comprehensive range of aerofoil geometries. The proposed approach exhibits profound potential in reducing the time and costs associated with aerodynamic prediction, enabling efficient evaluation of aerofoil performance. The findings contribute to the advancement of computational techniques in fluid dynamics and pave the way for enhanced design and optimization processes in aerodynamics. ",
    "url": "https://arxiv.org/abs/2305.18386",
    "authors": [
      "Tanishk Nandal",
      "Vaibhav Fulara",
      "Raj Kumar Singh"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18406",
    "title": "A machine learning approach to the prediction of heat-transfer  coefficients in micro-channels",
    "abstract": "The accurate prediction of the two-phase heat transfer coefficient (HTC) as a function of working fluids, channel geometries and process conditions is key to the optimal design and operation of compact heat exchangers. Advances in artificial intelligence research have recently boosted the application of machine learning (ML) algorithms to obtain data-driven surrogate models for the HTC. For most supervised learning algorithms, the task is that of a nonlinear regression problem. Despite the fact that these models have been proven capable of outperforming traditional empirical correlations, they have key limitations such as overfitting the data, the lack of uncertainty estimation, and interpretability of the results. To address these limitations, in this paper, we use a multi-output Gaussian process regression (GPR) to estimate the HTC in microchannels as a function of the mass flow rate, heat flux, system pressure and channel diameter and length. The model is trained using the Brunel Two-Phase Flow database of high-fidelity experimental data. The advantages of GPR are data efficiency, the small number of hyperparameters to be trained (typically of the same order of the number of input dimensions), and the automatic trade-off between data fit and model complexity guaranteed by the maximization of the marginal likelihood (Bayesian approach). Our paper proposes research directions to improve the performance of the GPR-based model in extrapolation. ",
    "url": "https://arxiv.org/abs/2305.18406",
    "authors": [
      "Tullio Traverso",
      "Francesco Coletti",
      "Luca Magri",
      "Tassos G. Karayiannis",
      "Omar K. Matar"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2305.18412",
    "title": "Short-term Temporal Dependency Detection under Heterogeneous Event  Dynamic with Hawkes Processes",
    "abstract": "Many event sequence data exhibit mutually exciting or inhibiting patterns. Reliable detection of such temporal dependency is crucial for scientific investigation. The de facto model is the Multivariate Hawkes Process (MHP), whose impact function naturally encodes a causal structure in Granger causality. However, the vast majority of existing methods use direct or nonlinear transform of standard MHP intensity with constant baseline, inconsistent with real-world data. Under irregular and unknown heterogeneous intensity, capturing temporal dependency is hard as one struggles to distinguish the effect of mutual interaction from that of intensity fluctuation. In this paper, we address the short-term temporal dependency detection issue. We show the maximum likelihood estimation (MLE) for cross-impact from MHP has an error that can not be eliminated but may be reduced by order of magnitude, using heterogeneous intensity not of the target HP but of the interacting HP. Then we proposed a robust and computationally-efficient method modified from MLE that does not rely on the prior estimation of the heterogeneous intensity and is thus applicable in a data-limited regime (e.g., few-shot, no repeated observations). Extensive experiments on various datasets show that our method outperforms existing ones by notable margins, with highlighted novel applications in neuroscience. ",
    "url": "https://arxiv.org/abs/2305.18412",
    "authors": [
      "Yu Chen",
      "Fengpei Li",
      "Anderson Schneider",
      "Yuriy Nevmyvaka",
      "Asohan Amarasingham",
      "Henry Lam"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18423",
    "title": "On the Role of Noise in the Sample Complexity of Learning Recurrent  Neural Networks: Exponential Gaps for Long Sequences",
    "abstract": "We consider the class of noisy multi-layered sigmoid recurrent neural networks with $w$ (unbounded) weights for classification of sequences of length $T$, where independent noise distributed according to $\\mathcal{N}(0,\\sigma^2)$ is added to the output of each neuron in the network. Our main result shows that the sample complexity of PAC learning this class can be bounded by $O (w\\log(T/\\sigma))$. For the non-noisy version of the same class (i.e., $\\sigma=0$), we prove a lower bound of $\\Omega (wT)$ for the sample complexity. Our results indicate an exponential gap in the dependence of sample complexity on $T$ for noisy versus non-noisy networks. Moreover, given the mild logarithmic dependence of the upper bound on $1/\\sigma$, this gap still holds even for numerically negligible values of $\\sigma$. ",
    "url": "https://arxiv.org/abs/2305.18423",
    "authors": [
      "Alireza Fathollah Pour",
      "Hassan Ashtiani"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18484",
    "title": "Neural Fourier Transform: A General Approach to Equivariant  Representation Learning",
    "abstract": "Symmetry learning has proven to be an effective approach for extracting the hidden structure of data, with the concept of equivariance relation playing the central role. However, most of the current studies are built on architectural theory and corresponding assumptions on the form of data. We propose Neural Fourier Transform (NFT), a general framework of learning the latent linear action of the group without assuming explicit knowledge of how the group acts on data. We present the theoretical foundations of NFT and show that the existence of a linear equivariant feature, which has been assumed ubiquitously in equivariance learning, is equivalent to the existence of a group invariant kernel on the dataspace. We also provide experimental results to demonstrate the application of NFT in typical scenarios with varying levels of knowledge about the acting group. ",
    "url": "https://arxiv.org/abs/2305.18484",
    "authors": [
      "Masanori Koyama",
      "Kenji Fukumizu",
      "Kohei Hayashi",
      "Takeru Miyato"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18502",
    "title": "Escaping mediocrity: how two-layer networks learn hard single-index  models with SGD",
    "abstract": "This study explores the sample complexity for two-layer neural networks to learn a single-index target function under Stochastic Gradient Descent (SGD), focusing on the challenging regime where many flat directions are present at initialization. It is well-established that in this scenario $n=O(d\\log{d})$ samples are typically needed. However, we provide precise results concerning the pre-factors in high-dimensional contexts and for varying widths. Notably, our findings suggest that overparameterization can only enhance convergence by a constant factor within this problem class. These insights are grounded in the reduction of SGD dynamics to a stochastic process in lower dimensions, where escaping mediocrity equates to calculating an exit time. Yet, we demonstrate that a deterministic approximation of this process adequately represents the escape time, implying that the role of stochasticity may be minimal in this scenario. ",
    "url": "https://arxiv.org/abs/2305.18502",
    "authors": [
      "Luca Arnaboldi",
      "Florent Krzakala",
      "Bruno Loureiro",
      "Ludovic Stephan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18506",
    "title": "Generalization Ability of Wide Residual Networks",
    "abstract": "In this paper, we study the generalization ability of the wide residual network on $\\mathbb{S}^{d-1}$ with the ReLU activation function. We first show that as the width $m\\rightarrow\\infty$, the residual network kernel (RNK) uniformly converges to the residual neural tangent kernel (RNTK). This uniform convergence further guarantees that the generalization error of the residual network converges to that of the kernel regression with respect to the RNTK. As direct corollaries, we then show $i)$ the wide residual network with the early stopping strategy can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space (RKHS) associated with the RNTK; $ii)$ the wide residual network can not generalize well if it is trained till overfitting the data. We finally illustrate some experiments to reconcile the contradiction between our theoretical result and the widely observed ``benign overfitting phenomenon'' ",
    "url": "https://arxiv.org/abs/2305.18506",
    "authors": [
      "Jianfa Lai",
      "Zixiong Yu",
      "Songtao Tian",
      "Qian Lin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18571",
    "title": "Quantum variational embedding for ground-state energy problems: sum of  squares and cluster selection",
    "abstract": "We introduce a sum-of-squares SDP hierarchy approximating the ground-state energy from below for quantum many-body problems, with a natural quantum embedding interpretation. We establish the connections between our approach and other variational methods for lower bounds, including the variational embedding, the RDM method in quantum chemistry, and the Anderson bounds. Additionally, inspired by the quantum information theory, we propose efficient strategies for optimizing cluster selection to tighten SDP relaxations while staying within a computational budget. Numerical experiments are presented to demonstrate the effectiveness of our strategy. As a byproduct of our investigation, we find that quantum entanglement has the potential to capture the underlying graph of the many-body Hamiltonian. ",
    "url": "https://arxiv.org/abs/2305.18571",
    "authors": [
      "Bowen Li",
      "Jianfeng Lu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2305.18592",
    "title": "Deep Neural Networks Generalization and Fine-Tuning for 12-lead ECG  Classification",
    "abstract": "Numerous studies are aimed at diagnosing heart diseases based on 12-lead electrocardiographic (ECG) records using deep learning methods. These studies usually use specific datasets that differ in size and parameters, such as patient metadata, number of doctors annotating ECGs, types of devices for ECG recording, data preprocessing techniques, etc. It is well-known that high-quality deep neural networks trained on one ECG dataset do not necessarily perform well on another dataset or clinical settings. In this paper, we propose a methodology to improve the quality of heart disease prediction regardless of the dataset by training neural networks on a variety of datasets with further fine-tuning for the specific dataset. To show its applicability, we train different neural networks on a large private dataset TIS containing various ECG records from multiple hospitals and on a relatively small public dataset PTB-XL. We demonstrate that training the networks on a large dataset and fine-tuning it on a small dataset from another source outperforms the networks trained only on one small dataset. We also show how the ability of a deep neural networks to generalize allows to improve classification quality of more diseases. ",
    "url": "https://arxiv.org/abs/2305.18592",
    "authors": [
      "Aram Avetisyan",
      "Shahane Tigranyan",
      "Ariana Asatryan",
      "Olga Mashkova",
      "Sergey Skorik",
      "Vladislav Ananev",
      "Yury Markin"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18702",
    "title": "Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the  Approximation of PDEs",
    "abstract": "Solving partial differential equations (PDEs) is a central task in scientific computing. Recently, neural network approximation of PDEs has received increasing attention due to its flexible meshless discretization and its potential for high-dimensional problems. One fundamental numerical difficulty is that random samples in the training set introduce statistical errors into the discretization of loss functional which may become the dominant error in the final approximation, and therefore overshadow the modeling capability of the neural network. In this work, we propose a new minmax formulation to optimize simultaneously the approximate solution, given by a neural network model, and the random samples in the training set, provided by a deep generative model. The key idea is to use a deep generative model to adjust random samples in the training set such that the residual induced by the approximate PDE solution can maintain a smooth profile when it is being minimized. Such an idea is achieved by implicitly embedding the Wasserstein distance between the residual-induced distribution and the uniform distribution into the loss, which is then minimized together with the residual. A nearly uniform residual profile means that its variance is small for any normalized weight function such that the Monte Carlo approximation error of the loss functional is reduced significantly for a certain sample size. The adversarial adaptive sampling (AAS) approach proposed in this work is the first attempt to formulate two essential components, minimizing the residual and seeking the optimal training set, into one minmax objective functional for the neural network approximation of PDEs. ",
    "url": "https://arxiv.org/abs/2305.18702",
    "authors": [
      "Kejun Tang",
      "Jiayu Zhai",
      "Xiaoliang Wan",
      "Chao Yang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2305.18753",
    "title": "Dual Transformer Decoder based Features Fusion Network for Automated  Audio Captioning",
    "abstract": "Automated audio captioning (AAC) which generates textual descriptions of audio content. Existing AAC models achieve good results but only use the high-dimensional representation of the encoder. There is always insufficient information learning of high-dimensional methods owing to high-dimensional representations having a large amount of information. In this paper, a new encoder-decoder model called the Low- and High-Dimensional Feature Fusion (LHDFF) is proposed. LHDFF uses a new PANNs encoder called Residual PANNs (RPANNs) to fuse low- and high-dimensional features. Low-dimensional features contain limited information about specific audio scenes. The fusion of low- and high-dimensional features can improve model performance by repeatedly emphasizing specific audio scene information. To fully exploit the fused features, LHDFF uses a dual transformer decoder structure to generate captions in parallel. Experimental results show that LHDFF outperforms existing audio captioning models. ",
    "url": "https://arxiv.org/abs/2305.18753",
    "authors": [
      "Jianyuan Sun",
      "Xubo Liu",
      "Xinhao Mei",
      "Volkan K\u0131l\u0131\u00e7",
      "Mark D. Plumbley",
      "Wenwu Wang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2305.18831",
    "title": "Convolutional Monge Mapping Normalization for learning on biosignals",
    "abstract": "In many machine learning applications on signals and biomedical data, especially electroencephalogram (EEG), one major challenge is the variability of the data across subjects, sessions, and hardware devices. In this work, we propose a new method called Convolutional Monge Mapping Normalization (CMMN), which consists in filtering the signals in order to adapt their power spectrum density (PSD) to a Wasserstein barycenter estimated on training data. CMMN relies on novel closed-form solutions for optimal transport mappings and barycenters and provides individual test time adaptation to new data without needing to retrain a prediction model. Numerical experiments on sleep EEG data show that CMMN leads to significant and consistent performance gains independent from the neural network architecture when adapting between subjects, sessions, and even datasets collected with different hardware. Notably our performance gain is on par with much more numerically intensive Domain Adaptation (DA) methods and can be used in conjunction with those for even better performances. ",
    "url": "https://arxiv.org/abs/2305.18831",
    "authors": [
      "Gnassounou Th\u00e9o",
      "Flamary R\u00e9mi",
      "Gramfort Alexandre"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18944",
    "title": "Fast Dynamic 1D Simulation of Divertor Plasmas with Neural PDE  Surrogates",
    "abstract": "Managing divertor plasmas is crucial for operating reactor scale tokamak devices due to heat and particle flux constraints on the divertor target. Simulation is an important tool to understand and control these plasmas, however, for real-time applications or exhaustive parameter scans only simple approximations are currently fast enough. We address this lack of fast simulators using neural PDE surrogates, data-driven neural network-based surrogate models trained using solutions generated with a classical numerical method. The surrogate approximates a time-stepping operator that evolves the full spatial solution of a reference physics-based model over time. We use DIV1D, a 1D dynamic model of the divertor plasma, as reference model to generate data. DIV1D's domain covers a 1D heat flux tube from the X-point (upstream) to the target. We simulate a realistic TCV divertor plasma with dynamics induced by upstream density ramps and provide an exploratory outlook towards fast transients. State-of-the-art neural PDE surrogates are evaluated in a common framework and extended for properties of the DIV1D data. We evaluate (1) the speed-accuracy trade-off; (2) recreating non-linear behavior; (3) data efficiency; and (4) parameter inter- and extrapolation. Once trained, neural PDE surrogates can faithfully approximate DIV1D's divertor plasma dynamics at sub real-time computation speeds: In the proposed configuration, 2ms of plasma dynamics can be computed in $\\approx$0.63ms of wall-clock time, several orders of magnitude faster than DIV1D. ",
    "url": "https://arxiv.org/abs/2305.18944",
    "authors": [
      "Yoeri Poels",
      "Gijs Derks",
      "Egbert Westerhof",
      "Koen Minartz",
      "Sven Wiesen",
      "Vlado Menkovski"
    ],
    "subjectives": [
      "Plasma Physics (physics.plasm-ph)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2305.18974",
    "title": "Asymptotic Characterisation of Robust Empirical Risk Minimisation  Performance in the Presence of Outliers",
    "abstract": "We study robust linear regression in high-dimension, when both the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\\alpha=n/d$, and study a data model that includes outliers. We provide exact asymptotics for the performances of the empirical risk minimisation (ERM) using $\\ell_2$-regularised $\\ell_2$, $\\ell_1$, and Huber loss, which are the standard approach to such problems. We focus on two metrics for the performance: the generalisation error to similar datasets with outliers, and the estimation error of the original, unpolluted function. Our results are compared with the information theoretic Bayes-optimal estimation bound. For the generalization error, we find that optimally-regularised ERM is asymptotically consistent in the large sample complexity limit if one perform a simple calibration, and compute the rates of convergence. For the estimation error however, we show that due to a norm calibration mismatch, the consistency of the estimator requires an oracle estimate of the optimal norm, or the presence of a cross-validation set not corrupted by the outliers. We examine in detail how performance depends on the loss function and on the degree of outlier corruption in the training set and identify a region of parameters where the optimal performance of the Huber loss is identical to that of the $\\ell_2$ loss, offering insights into the use cases of different loss functions. ",
    "url": "https://arxiv.org/abs/2305.18974",
    "authors": [
      "Matteo Vilucchio",
      "Emanuele Troiani",
      "Vittorio Erba",
      "Florent Krzakala"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.19004",
    "title": "Policy Gradient Algorithms for Robust MDPs with Non-Rectangular  Uncertainty Sets",
    "abstract": "We propose a policy gradient algorithm for robust infinite-horizon Markov Decision Processes (MDPs) with non-rectangular uncertainty sets, thereby addressing an open challenge in the robust MDP literature. Indeed, uncertainty sets that display statistical optimality properties and make optimal use of limited data often fail to be rectangular. Unfortunately, the corresponding robust MDPs cannot be solved with dynamic programming techniques and are in fact provably intractable. This prompts us to develop a projected Langevin dynamics algorithm tailored to the robust policy evaluation problem, which offers global optimality guarantees. We also propose a deterministic policy gradient method that solves the robust policy evaluation problem approximately, and we prove that the approximation error scales with a new measure of non-rectangularity of the uncertainty set. Numerical experiments showcase that our projected Langevin dynamics algorithm can escape local optima, while algorithms tailored to rectangular uncertainty fail to do so. ",
    "url": "https://arxiv.org/abs/2305.19004",
    "authors": [
      "Mengmeng Li",
      "Tobias Sutter",
      "Daniel Kuhn"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.19011",
    "title": "MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models",
    "abstract": "Self-supervised learning (SSL) is a popular research topic in speech processing. Successful SSL speech models must generalize well. SUPERB was proposed to evaluate the ability of SSL speech models across many speech tasks. However, due to the diversity of tasks, the evaluation process requires huge computational costs. We present MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL speech models with comparable results to SUPERB while greatly reducing the computational cost. We select representative tasks and sample datasets and extract model representation offline, achieving 0.954 and 0.982 Spearman's rank correlation with SUPERB Paper and SUPERB Challenge, respectively. In the meanwhile, the computational cost is reduced by 97% in regard to MACs (number of Multiply-ACcumulate operations) in the tasks we choose. To the best of our knowledge, this is the first study to examine not only the computational cost of a model itself but the cost of evaluating it on a benchmark. ",
    "url": "https://arxiv.org/abs/2305.19011",
    "authors": [
      "Yu-Hsiang Wang",
      "Huang-Yu Chen",
      "Kai-Wei Chang",
      "Winston Hsu",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.19079",
    "title": "Analyzing the Sample Complexity of Self-Supervised Image Reconstruction  Methods",
    "abstract": "Supervised training of deep neural networks on pairs of clean image and noisy measurement achieves state-of-the-art performance for many image reconstruction tasks, but such training pairs are usually difficult to collect. A variety of self-supervised methods enable training based on noisy measurements only, without clean images. In this work, we investigate the cost of self-supervised training by characterizing its sample complexity. We focus on a class of self-supervised methods that enable the computation of unbiased estimates of gradients of the supervised loss, including noise2noise methods. We first analytically show that a model trained with such self-supervised training is as good as the same model trained in a supervised fashion, but self-supervised training requires more examples than supervised training. We then study self-supervised denoising and accelerated MRI empirically and characterize the cost of self-supervised training in terms of the number of additional samples required, and find that the performance gap between self-supervised and supervised training vanishes as a function of the training examples, at a problem-dependent rate, as predicted by our theory. ",
    "url": "https://arxiv.org/abs/2305.19079",
    "authors": [
      "Tobit Klug",
      "Dogukan Atik",
      "Reinhard Heckel"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.19082",
    "title": "Embedding Inequalities for Barron-type Spaces",
    "abstract": "One of the fundamental problems in deep learning theory is understanding the approximation and generalization properties of two-layer neural networks in high dimensions. In order to tackle this issue, researchers have introduced the Barron space $\\mathcal{B}_s(\\Omega)$ and the spectral Barron space $\\mathcal{F}_s(\\Omega)$, where the index $s$ characterizes the smoothness of functions within these spaces and $\\Omega\\subset\\mathbb{R}^d$ represents the input domain. However, it is still not clear what is the relationship between the two types of Barron spaces. In this paper, we establish continuous embeddings between these spaces as implied by the following inequality: for any $\\delta\\in (0,1), s\\in \\mathbb{N}^{+}$ and $f: \\Omega \\mapsto\\mathbb{R}$, it holds that \\[ \\delta\\gamma^{\\delta-s}_{\\Omega}\\|f\\|_{\\mathcal{F}_{s-\\delta}(\\Omega)}\\lesssim_s \\|f\\|_{\\mathcal{B}_s(\\Omega)}\\lesssim_s \\|f\\|_{\\mathcal{F}_{s+1}(\\Omega)}, \\] where $\\gamma_{\\Omega}=\\sup_{\\|v\\|_2=1,x\\in\\Omega}|v^Tx|$ and notably, the hidden constants depend solely on the value of $s$. Furthermore, we provide examples to demonstrate that the lower bound is tight. ",
    "url": "https://arxiv.org/abs/2305.19082",
    "authors": [
      "Lei Wu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2305.19090",
    "title": "Prospective Validation of Motor-Based Intervention with Automated  Mispronunciation Detection of Rhotics in Residual Speech Sound Disorders",
    "abstract": "Because lab accuracy of clinical speech technology systems may be overoptimistic, clinical validation is vital to demonstrate system reproducibility - in this case, the ability of the PERCEPT-R Classifier to predict clinician judgment of American English /r/ during ChainingAI motor-based speech sound disorder intervention. All five participants experienced statistically-significant improvement in untreated words following 10 sessions of combined human-ChainingAI treatment. These gains, despite a wide range of PERCEPT-human and human-human (F1-score) agreement, raise questions about best measuring classification performance for clinical speech that may be perceptually ambiguous. ",
    "url": "https://arxiv.org/abs/2305.19090",
    "authors": [
      "Nina R Benway",
      "Jonathan L Preston"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2305.19184",
    "title": "Leveraging Semantic Information for Efficient Self-Supervised Emotion  Recognition with Audio-Textual Distilled Models",
    "abstract": "In large part due to their implicit semantic modeling, self-supervised learning (SSL) methods have significantly increased the performance of valence recognition in speech emotion recognition (SER) systems. Yet, their large size may often hinder practical implementations. In this work, we take HuBERT as an example of an SSL model and analyze the relevance of each of its layers for SER. We show that shallow layers are more important for arousal recognition while deeper layers are more important for valence. This observation motivates the importance of additional textual information for accurate valence recognition, as the distilled framework lacks the depth of its large-scale SSL teacher. Thus, we propose an audio-textual distilled SSL framework that, while having only ~20% of the trainable parameters of a large SSL model, achieves on par performance across the three emotion dimensions (arousal, valence, dominance) on the MSP-Podcast v1.10 dataset. ",
    "url": "https://arxiv.org/abs/2305.19184",
    "authors": [
      "Danilo de Oliveira",
      "Navin Raj Prabhu",
      "Timo Gerkmann"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2305.19243",
    "title": "Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural  Networks",
    "abstract": "It is widely recognized that the generalization ability of neural networks can be greatly enhanced through carefully designing the training procedure. The current state-of-the-art training approach involves utilizing stochastic gradient descent (SGD) or Adam optimization algorithms along with a combination of additional regularization techniques such as weight decay, dropout, or noise injection. Optimal generalization can only be achieved by tuning a multitude of hyperparameters through grid search, which can be time-consuming and necessitates additional validation datasets. To address this issue, we introduce a practical PAC-Bayes training framework that is nearly tuning-free and requires no additional regularization while achieving comparable testing performance to that of SGD/Adam after a complete grid search and with extra regularizations. Our proposed algorithm demonstrates the remarkable potential of PAC training to achieve state-of-the-art performance on deep neural networks with enhanced robustness and interpretability. ",
    "url": "https://arxiv.org/abs/2305.19243",
    "authors": [
      "Xitong Zhang",
      "Avrajit Ghosh",
      "Guangliang Liu",
      "Rongrong Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.19255",
    "title": "A Stutter Seldom Comes Alone -- Cross-Corpus Stuttering Detection as a  Multi-label Problem",
    "abstract": "Most stuttering detection and classification research has viewed stuttering as a multi-class classification problem or a binary detection task for each dysfluency type; however, this does not match the nature of stuttering, in which one dysfluency seldom comes alone but rather co-occurs with others. This paper explores multi-language and cross-corpus end-to-end stuttering detection as a multi-label problem using a modified wav2vec 2.0 system with an attention-based classification head and multi-task learning. We evaluate the method using combinations of three datasets containing English and German stuttered speech, one containing speech modified by fluency shaping. The experimental results and an error analysis show that multi-label stuttering detection systems trained on cross-corpus and multi-language data achieve competitive results but performance on samples with multiple labels stays below over-all detection results. ",
    "url": "https://arxiv.org/abs/2305.19255",
    "authors": [
      "Sebastian P. Bayerl",
      "Dominik Wagner",
      "Ilja Baumann",
      "Florian H\u00f6nig",
      "Tobias Bocklet",
      "Elmar N\u00f6th",
      "Korbinian Riedhammer"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2305.19269",
    "title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation",
    "abstract": "Various applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \"coarse-to-fine\" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io ",
    "url": "https://arxiv.org/abs/2305.19269",
    "authors": [
      "Rongjie Huang",
      "Chunlei Zhang",
      "Yongqi Wang",
      "Dongchao Yang",
      "Luping Liu",
      "Zhenhui Ye",
      "Ziyue Jiang",
      "Chao Weng",
      "Zhou Zhao",
      "Dong Yu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2008.08427",
    "title": "How Powerful are Shallow Neural Networks with Bandlimited Random  Weights?",
    "abstract": " Comments: Published as a conference paper at ICML 2023 ",
    "url": "https://arxiv.org/abs/2008.08427",
    "authors": [
      "Ming Li",
      "Sho Sonoda",
      "Feilong Cao",
      "Yu Guang Wang",
      "Jiye Liang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2105.12120",
    "title": "Sampling random graphs with specified degree sequences",
    "abstract": " Comments: Same as version v3 but with corrected white spaces between paragraphs ",
    "url": "https://arxiv.org/abs/2105.12120",
    "authors": [
      "Upasana Dutta",
      "Bailey K. Fosdick",
      "Aaron Clauset"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2106.12594",
    "title": "Real-time gravitational-wave science with neural posterior estimation",
    "abstract": " Comments: 7+12 pages, 4+11 figures. [v2]: Minor updates to match published version, code available at this https URL ",
    "url": "https://arxiv.org/abs/2106.12594",
    "authors": [
      "Maximilian Dax",
      "Stephen R. Green",
      "Jonathan Gair",
      "Jakob H. Macke",
      "Alessandra Buonanno",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "General Relativity and Quantum Cosmology (gr-qc)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.01876",
    "title": "Which Invariance Should We Transfer? A Causal Minimax Learning Approach",
    "abstract": " Comments: Accepted version of ICML-23 ",
    "url": "https://arxiv.org/abs/2107.01876",
    "authors": [
      "Mingzhou Liu",
      "Xiangyu Zheng",
      "Xinwei Sun",
      "Fang Fang",
      "Yizhou Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2109.02734",
    "title": "Detecting Inspiring Content on Social Media",
    "abstract": " Comments: accepted at ACII 2021 ",
    "url": "https://arxiv.org/abs/2109.02734",
    "authors": [
      "Oana Ignat",
      "Y-Lan Boureau",
      "Jane A. Yu",
      "Alon Halevy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2109.10319",
    "title": "Community detection for weighted bipartite networks",
    "abstract": " Comments: 27pages ",
    "url": "https://arxiv.org/abs/2109.10319",
    "authors": [
      "Huan Qing",
      "Jingli Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2111.01557",
    "title": "PointNu-Net: Keypoint-assisted Convolutional Neural Network for  Simultaneous Multi-tissue Histology Nuclei Segmentation and Classification",
    "abstract": " Comments: 12 pages,7 figures, journal ",
    "url": "https://arxiv.org/abs/2111.01557",
    "authors": [
      "Kai Yao",
      "Kaizhu Huang",
      "Jie Sun",
      "Amir Hussain"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2111.13139",
    "title": "Group equivariant neural posterior estimation",
    "abstract": " Comments: 13+11 pages, 5+8 figures. [v2]: Minor updates to match published version, code available at this https URL ",
    "url": "https://arxiv.org/abs/2111.13139",
    "authors": [
      "Maximilian Dax",
      "Stephen R. Green",
      "Jonathan Gair",
      "Michael Deistler",
      "Bernhard Sch\u00f6lkopf",
      "Jakob H. Macke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "General Relativity and Quantum Cosmology (gr-qc)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2204.08352",
    "title": "MHSCNet: A Multimodal Hierarchical Shot-aware Convolutional Network for  Video Summarization",
    "abstract": " Title: MHSCNet: A Multimodal Hierarchical Shot-aware Convolutional Network for  Video Summarization ",
    "url": "https://arxiv.org/abs/2204.08352",
    "authors": [
      "Wujiang Xu",
      "Runzhong Wang",
      "Xiaobo Guo",
      "Shaoshuai Li",
      "Qiongxu Ma",
      "Yunan Zhao",
      "Sheng Guo",
      "Zhenfeng Zhu",
      "Junchi Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2205.03612",
    "title": "BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with  Graph Information Bottleneck",
    "abstract": " Comments: 15 pages, 8 figures ",
    "url": "https://arxiv.org/abs/2205.03612",
    "authors": [
      "Kaizhong Zheng",
      "Shujian Yu",
      "Baojuan Li",
      "Robert Jenssen",
      "Badong Chen"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2207.03367",
    "title": "Joint Super-Resolution and Inverse Tone-Mapping: A Feature Decomposition  Aggregation Network and A New Benchmark",
    "abstract": " Comments: update authors info ",
    "url": "https://arxiv.org/abs/2207.03367",
    "authors": [
      "Gang Xu",
      "Yu-chen Yang",
      "Liang Wang",
      "Xian-Tong Zhen",
      "Jun Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2207.12718",
    "title": "XInsight: eXplainable Data Analysis Through The Lens of Causality",
    "abstract": " Title: XInsight: eXplainable Data Analysis Through The Lens of Causality ",
    "url": "https://arxiv.org/abs/2207.12718",
    "authors": [
      "Pingchuan Ma",
      "Rui Ding",
      "Shuai Wang",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2207.13176",
    "title": "Exploring the Unprecedented Privacy Risks of the Metaverse",
    "abstract": " Title: Exploring the Unprecedented Privacy Risks of the Metaverse ",
    "url": "https://arxiv.org/abs/2207.13176",
    "authors": [
      "Vivek Nair",
      "Gonzalo Munilla Garrido",
      "Dawn Song"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2207.13700",
    "title": "Remote Medication Status Prediction for Individuals with Parkinson's  Disease using Time-series Data from Smartphones",
    "abstract": " Comments: Accepted to ICDH-2023. Camera ready with supplementary material ",
    "url": "https://arxiv.org/abs/2207.13700",
    "authors": [
      "Weijian Li",
      "Wei Zhu",
      "E. Ray Dorsey",
      "Jiebo Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2208.00277",
    "title": "MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient  Neural Field Rendering on Mobile Architectures",
    "abstract": " Comments: CVPR 2023. Project page: this https URL, code: this https URL ",
    "url": "https://arxiv.org/abs/2208.00277",
    "authors": [
      "Zhiqin Chen",
      "Thomas Funkhouser",
      "Peter Hedman",
      "Andrea Tagliasacchi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2208.02693",
    "title": "Relict landslide detection using Deep-Learning architectures for image  segmentation in rainforest areas: A new framework",
    "abstract": " Title: Relict landslide detection using Deep-Learning architectures for image  segmentation in rainforest areas: A new framework ",
    "url": "https://arxiv.org/abs/2208.02693",
    "authors": [
      "Guilherme P.B. Garcia",
      "Carlos H. Grohmann",
      "Lucas P. Soares",
      "Mateus Espadoto"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2208.04514",
    "title": "A Novel Shortest Paths Algorithm on Unweighted Graphs",
    "abstract": " Title: A Novel Shortest Paths Algorithm on Unweighted Graphs ",
    "url": "https://arxiv.org/abs/2208.04514",
    "authors": [
      "Yelai Feng",
      "Huaixi Wang",
      "Yining Zhu",
      "Chao Chang",
      "Hongyi Lu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2208.06963",
    "title": "Privacy-Preserving Decentralized Inference with Graph Neural Networks in  Wireless Networks",
    "abstract": " Comments: This paper has been accepted by TWC ",
    "url": "https://arxiv.org/abs/2208.06963",
    "authors": [
      "Mengyuan Lee",
      "Guanding Yu",
      "Huaiyu Dai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2209.01470",
    "title": "Neural Sign Reenactor: Deep Photorealistic Sign Language Retargeting",
    "abstract": " Comments: Accepted at AI4CC Workshop at CVPR 2023 ",
    "url": "https://arxiv.org/abs/2209.01470",
    "authors": [
      "Christina O. Tze",
      "Panagiotis P. Filntisis",
      "Athanasia-Lida Dimou",
      "Anastasios Roussos",
      "Petros Maragos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2209.07348",
    "title": "Coupled Evolutionary Behavioral and Disease Dynamics under Reinfection  Risk",
    "abstract": " Comments: arXiv admin note: text overlap with arXiv:2203.10276 ",
    "url": "https://arxiv.org/abs/2209.07348",
    "authors": [
      "Abhisek Satapathi",
      "Narendra Kumar Dhar",
      "Ashish R. Hota",
      "Vaibhav Srivastava"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2210.00062",
    "title": "Learning Robust Kernel Ensembles with Kernel Average Pooling",
    "abstract": " Title: Learning Robust Kernel Ensembles with Kernel Average Pooling ",
    "url": "https://arxiv.org/abs/2210.00062",
    "authors": [
      "Pouya Bashivan",
      "Adam Ibrahim",
      "Amirozhan Dehghani",
      "Yifei Ren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2210.05686",
    "title": "Neural Importance Sampling for Rapid and Reliable Gravitational-Wave  Inference",
    "abstract": " Comments: 8+7 pages, 1+5 figures. [v2]: Minor updates to match published version, code available at this https URL ",
    "url": "https://arxiv.org/abs/2210.05686",
    "authors": [
      "Maximilian Dax",
      "Stephen R. Green",
      "Jonathan Gair",
      "Michael P\u00fcrrer",
      "Jonas Wildberger",
      "Jakob H. Macke",
      "Alessandra Buonanno",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "General Relativity and Quantum Cosmology (gr-qc)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2210.17312",
    "title": "Neural Network-based CUSUM for Online Change-point Detection",
    "abstract": " Title: Neural Network-based CUSUM for Online Change-point Detection ",
    "url": "https://arxiv.org/abs/2210.17312",
    "authors": [
      "Junghwan Lee",
      "Tingnan Gong",
      "Xiuyuan Cheng",
      "Yao Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2211.02809",
    "title": "LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition  and Translation Model Using Neural Transducers",
    "abstract": " Comments: INTERSPEECH 2023 ",
    "url": "https://arxiv.org/abs/2211.02809",
    "authors": [
      "Peidong Wang",
      "Eric Sun",
      "Jian Xue",
      "Yu Wu",
      "Long Zhou",
      "Yashesh Gaur",
      "Shujie Liu",
      "Jinyu Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2212.04655",
    "title": "MIMO Is All You Need : A Strong Multi-In-Multi-Out Baseline for Video  Prediction",
    "abstract": " Title: MIMO Is All You Need : A Strong Multi-In-Multi-Out Baseline for Video  Prediction ",
    "url": "https://arxiv.org/abs/2212.04655",
    "authors": [
      "Shuliang Ning",
      "Mengcheng Lan",
      "Yanran Li",
      "Chaofeng Chen",
      "Qian Chen",
      "Xunlai Chen",
      "Xiaoguang Han",
      "Shuguang Cui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2301.00122",
    "title": "Hair and Scalp Disease Detection using Machine Learning and Image  Processing",
    "abstract": " Title: Hair and Scalp Disease Detection using Machine Learning and Image  Processing ",
    "url": "https://arxiv.org/abs/2301.00122",
    "authors": [
      "Mrinmoy Roy",
      "Anica Tasnim Protity"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2301.07743",
    "title": "Leveraging generative adversarial networks to create realistic scanning  transmission electron microscopy images",
    "abstract": " Comments: 25 pages, 6 figures, 2 tables ",
    "url": "https://arxiv.org/abs/2301.07743",
    "authors": [
      "Abid Khan",
      "Chia-Hao Lee",
      "Pinshane Y. Huang",
      "Bryan K. Clark"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2302.01178",
    "title": "Convolutional Neural Operators for robust and accurate learning of PDEs",
    "abstract": " Title: Convolutional Neural Operators for robust and accurate learning of PDEs ",
    "url": "https://arxiv.org/abs/2302.01178",
    "authors": [
      "Bogdan Raoni\u0107",
      "Roberto Molinaro",
      "Tim De Ryck",
      "Tobias Rohner",
      "Francesca Bartolucci",
      "Rima Alaifari",
      "Siddhartha Mishra",
      "Emmanuel de B\u00e9zenac"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.03023",
    "title": "V1T: large-scale mouse V1 response prediction using a Vision Transformer",
    "abstract": " Comments: updated references and added link to code repository; add analysis on generalization and visualize aRFs ",
    "url": "https://arxiv.org/abs/2302.03023",
    "authors": [
      "Bryan M. Li",
      "Isabel M. Cornacchia",
      "Nathalie L. Rochefort",
      "Arno Onken"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2302.03271",
    "title": "IB-UQ: Information bottleneck based uncertainty quantification for  neural function regression and neural operator learning",
    "abstract": " Comments: 27 pages, 22figures ",
    "url": "https://arxiv.org/abs/2302.03271",
    "authors": [
      "Ling Guo",
      "Hao Wu",
      "Wenwen Zhou",
      "Yan Wang",
      "Tao Zhou"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.04441",
    "title": "Multi-task Representation Learning for Pure Exploration in Linear  Bandits",
    "abstract": " Title: Multi-task Representation Learning for Pure Exploration in Linear  Bandits ",
    "url": "https://arxiv.org/abs/2302.04441",
    "authors": [
      "Yihan Du",
      "Longbo Huang",
      "Wen Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.06381",
    "title": "Self-supervised phase unwrapping in fringe projection profilometry",
    "abstract": " Title: Self-supervised phase unwrapping in fringe projection profilometry ",
    "url": "https://arxiv.org/abs/2302.06381",
    "authors": [
      "Xiaomin Gao",
      "Wanzhong Song",
      "Chunqian Tan",
      "Junzhe Lei"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2302.07025",
    "title": "Optimal Transport for Change Detection on LiDAR Point Clouds",
    "abstract": " Comments: Accepted to IEEE International Geoscience and Remote Sensing Symposium 2023 (IGARSS 2023) @IEEE copyright ",
    "url": "https://arxiv.org/abs/2302.07025",
    "authors": [
      "Marco Fiorucci",
      "Peter Naylor",
      "Makoto Yamada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2302.07849",
    "title": "Zero-Shot Batch-Level Anomaly Detection",
    "abstract": " Title: Zero-Shot Batch-Level Anomaly Detection ",
    "url": "https://arxiv.org/abs/2302.07849",
    "authors": [
      "Aodong Li",
      "Chen Qiu",
      "Marius Kloft",
      "Padhraic Smyth",
      "Maja Rudolph",
      "Stephan Mandt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2302.09852",
    "title": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection",
    "abstract": " Title: Unsupervised Layer-wise Score Aggregation for Textual OOD Detection ",
    "url": "https://arxiv.org/abs/2302.09852",
    "authors": [
      "Maxime Darrin",
      "Guillaume Staerman",
      "Eduardo Dadalto C\u00e2mara Gomes",
      "Jackie CK Cheung",
      "Pablo Piantanida",
      "Pierre Colombo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2302.10886",
    "title": "Some Fundamental Aspects about Lipschitz Continuity of Neural Network  Functions",
    "abstract": " Title: Some Fundamental Aspects about Lipschitz Continuity of Neural Network  Functions ",
    "url": "https://arxiv.org/abs/2302.10886",
    "authors": [
      "Grigory Khromov",
      "Sidak Pal Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2302.12537",
    "title": "Why Target Networks Stabilise Temporal Difference Methods",
    "abstract": " Comments: Found a small error in Appendix (Proposition 1, Appendix B3, penultimate line) that affects results presented ",
    "url": "https://arxiv.org/abs/2302.12537",
    "authors": [
      "Mattie Fellows",
      "Matthew J. A. Smith",
      "Shimon Whiteson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2302.13221",
    "title": "Data-Centric AI: Deep Generative Differentiable Feature Selection via  Discrete Subsetting as Continuous Embedding Space Optimization",
    "abstract": " Comments: We found some errors in this paper, so we decide to revise this paper and submit it again in a better version ",
    "url": "https://arxiv.org/abs/2302.13221",
    "authors": [
      "Meng Xiao",
      "Dongjie Wang",
      "Min Wu",
      "Pengfei Wang",
      "Yuanchun Zhou",
      "Yanjie Fu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2303.04620",
    "title": "Followback Clusters, Satellite Audiences, and Bridge Nodes: Coengagement  Networks for the 2020 US Election",
    "abstract": " Comments: Accepted for publication at ICWSM '23 ",
    "url": "https://arxiv.org/abs/2303.04620",
    "authors": [
      "Andrew Beers",
      "Joseph S. Schafer",
      "Ian Kennedy",
      "Morgan Wack",
      "Emma S. Spiro",
      "Kate Starbird"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2303.10528",
    "title": "LNO: Laplace Neural Operator for Solving Differential Equations",
    "abstract": " Comments: 18 pages, 8 figures, 2 tables ",
    "url": "https://arxiv.org/abs/2303.10528",
    "authors": [
      "Qianying Cao",
      "Somdatta Goswami",
      "George Em Karniadakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.10770",
    "title": "RN-Net: Reservoir Nodes-Enabled Neuromorphic Vision Sensing Network",
    "abstract": " Comments: 12 pages, 5 figures, 4 tables ",
    "url": "https://arxiv.org/abs/2303.10770",
    "authors": [
      "Sangmin Yoo",
      "Eric Yeu-Jer Lee",
      "Ziyu Wang",
      "Xinxin Wang",
      "Wei D. Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2303.12147",
    "title": "Universal Approximation Property of Hamiltonian Deep Neural Networks",
    "abstract": " Title: Universal Approximation Property of Hamiltonian Deep Neural Networks ",
    "url": "https://arxiv.org/abs/2303.12147",
    "authors": [
      "Muhammad Zakwan",
      "Massimiliano d'Angelo",
      "Giancarlo Ferrari-Trecate"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2303.12711",
    "title": "Geometry-Aware Latent Representation Learning for Modeling Disease  Progression of Barrett's Esophagus",
    "abstract": " Title: Geometry-Aware Latent Representation Learning for Modeling Disease  Progression of Barrett's Esophagus ",
    "url": "https://arxiv.org/abs/2303.12711",
    "authors": [
      "Vivien van Veldhuizen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.15103",
    "title": "Contrastive Learning Is Spectral Clustering On Similarity Graph",
    "abstract": " Comments: We express our gratitude to the anonymous reviewers for their valuable feedback ",
    "url": "https://arxiv.org/abs/2303.15103",
    "authors": [
      "Zhiquan Tan",
      "Yifan Zhang",
      "Jingqin Yang",
      "Yang Yuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2303.15438",
    "title": "On the Stepwise Nature of Self-Supervised Learning",
    "abstract": " Comments: 9 pages (main text) + 14 pages (refs + appendices). ICML '23 ",
    "url": "https://arxiv.org/abs/2303.15438",
    "authors": [
      "James B. Simon",
      "Maksis Knutins",
      "Liu Ziyin",
      "Daniel Geisz",
      "Abraham J. Fetterman",
      "Joshua Albrecht"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.15771",
    "title": "TerrainNet: Visual Modeling of Complex Terrain for High-speed, Off-road  Navigation",
    "abstract": " Title: TerrainNet: Visual Modeling of Complex Terrain for High-speed, Off-road  Navigation ",
    "url": "https://arxiv.org/abs/2303.15771",
    "authors": [
      "Xiangyun Meng",
      "Nathan Hatch",
      "Alexander Lambert",
      "Anqi Li",
      "Nolan Wagener",
      "Matthew Schmittle",
      "JoonHo Lee",
      "Wentao Yuan",
      "Zoey Chen",
      "Samuel Deng",
      "Greg Okopal",
      "Dieter Fox",
      "Byron Boots",
      "Amirreza Shaban"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2304.00474",
    "title": "On the Optimal Recovery of Graph Signals",
    "abstract": " Comments: This paper has been accepted by 14th International conference on Sampling Theory and Applications (SampTA 2023) ",
    "url": "https://arxiv.org/abs/2304.00474",
    "authors": [
      "Simon Foucart",
      "Chunyang Liao",
      "Nate Veldt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2304.00902",
    "title": "FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction",
    "abstract": " Comments: Accepted by AAAI 2023. Code available at this https URL ",
    "url": "https://arxiv.org/abs/2304.00902",
    "authors": [
      "Kelong Mao",
      "Jieming Zhu",
      "Liangcai Su",
      "Guohao Cai",
      "Yuru Li",
      "Zhenhua Dong"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2304.09010",
    "title": "CauF-VAE: Causal Disentangled Representation Learning with VAE and  Causal Flows",
    "abstract": " Comments: 18 pages, 15 figures ",
    "url": "https://arxiv.org/abs/2304.09010",
    "authors": [
      "Di Fan",
      "Yannian Kou",
      "Chuanhou Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2304.10970",
    "title": "Can GPT-4 Perform Neural Architecture Search?",
    "abstract": " Title: Can GPT-4 Perform Neural Architecture Search? ",
    "url": "https://arxiv.org/abs/2304.10970",
    "authors": [
      "Mingkai Zheng",
      "Xiu Su",
      "Shan You",
      "Fei Wang",
      "Chen Qian",
      "Chang Xu",
      "Samuel Albanie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2304.12794",
    "title": "Expand-and-Cluster: Exact Parameter Recovery of Neural Networks",
    "abstract": " Comments: Preprint: 14 pages, 6 figures. Appendix: 8 pages, 7 figures ",
    "url": "https://arxiv.org/abs/2304.12794",
    "authors": [
      "Flavio Martinelli",
      "Berfin Simsek",
      "Johanni Brea",
      "Wulfram Gerstner"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2304.12824",
    "title": "Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling  in Offline Reinforcement Learning",
    "abstract": " Comments: Accepted at ICML 2023 ",
    "url": "https://arxiv.org/abs/2304.12824",
    "authors": [
      "Cheng Lu",
      "Huayu Chen",
      "Jianfei Chen",
      "Hang Su",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.00871",
    "title": "No One Size (PPM) Fits All: Towards Privacy in Stream Processing Systems",
    "abstract": " Comments: Vision paper accepted to DEBS 2023 ",
    "url": "https://arxiv.org/abs/2305.00871",
    "authors": [
      "Mikhail Fomichev",
      "Manisha Luthra",
      "Maik Benndorf",
      "Pratyush Agnihotri"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2305.00909",
    "title": "Outline, Then Details: Syntactically Guided Coarse-To-Fine Code  Generation",
    "abstract": " Comments: Accepted in ICML 2023 ",
    "url": "https://arxiv.org/abs/2305.00909",
    "authors": [
      "Wenqing Zheng",
      "S P Sharan",
      "Ajay Kumar Jaiswal",
      "Kevin Wang",
      "Yihan Xi",
      "Dejia Xu",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.02886",
    "title": "SlipCover: Near Zero-Overhead Code Coverage for Python",
    "abstract": " Comments: Accepted to ISSTA 2023 ",
    "url": "https://arxiv.org/abs/2305.02886",
    "authors": [
      "Juan Altmayer Pizzorno",
      "Emery D Berger"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2305.03602",
    "title": "A Dual Semantic-Aware Recurrent Global-Adaptive Network For  Vision-and-Language Navigation",
    "abstract": " Comments: Accepted by IJCAI 2023 ",
    "url": "https://arxiv.org/abs/2305.03602",
    "authors": [
      "Liuyi Wang",
      "Zongtao He",
      "Jiagui Tang",
      "Ronghao Dang",
      "Naijia Wang",
      "Chengju Liu",
      "Qijun Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.04111",
    "title": "Efficient and Degree-Guided Graph Generation via Discrete Diffusion  Modeling",
    "abstract": " Comments: ICML 2023, camera-ready revision ",
    "url": "https://arxiv.org/abs/2305.04111",
    "authors": [
      "Xiaohui Chen",
      "Jiaxing He",
      "Xu Han",
      "Li-Ping Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2305.04445",
    "title": "New metrics and search algorithms for weighted causal DAGs",
    "abstract": " Comments: Accepted into ICML 2023 ",
    "url": "https://arxiv.org/abs/2305.04445",
    "authors": [
      "Davin Choo",
      "Kirankumar Shiragur"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.05368",
    "title": "Deep Graph Neural Networks via Flexible Subgraph Aggregation",
    "abstract": " Title: Deep Graph Neural Networks via Flexible Subgraph Aggregation ",
    "url": "https://arxiv.org/abs/2305.05368",
    "authors": [
      "Jingbo Zhou",
      "Yixuan Du",
      "Ruqiong Zhang",
      "Di Jin",
      "Carl Yang",
      "Rui Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.05402",
    "title": "Consistent Text Categorization using Data Augmentation in e-Commerce",
    "abstract": " Title: Consistent Text Categorization using Data Augmentation in e-Commerce ",
    "url": "https://arxiv.org/abs/2305.05402",
    "authors": [
      "Guy Horowitz",
      "Stav Yanovsky Daye",
      "Noa Avigdor-Elgrabli",
      "Ariel Raviv"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2305.10036",
    "title": "Are You Copying My Model? Protecting the Copyright of Large Language  Models for EaaS via Backdoor Watermark",
    "abstract": " Comments: Accepted by ACL 2023 ",
    "url": "https://arxiv.org/abs/2305.10036",
    "authors": [
      "Wenjun Peng",
      "Jingwei Yi",
      "Fangzhao Wu",
      "Shangxi Wu",
      "Bin Zhu",
      "Lingjuan Lyu",
      "Binxing Jiao",
      "Tong Xu",
      "Guangzhong Sun",
      "Xing Xie"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2305.10259",
    "title": "Runtime Analyses of Multi-Objective Evolutionary Algorithms in the  Presence of Noise",
    "abstract": " Comments: Appears at IJCAI 2023 ",
    "url": "https://arxiv.org/abs/2305.10259",
    "authors": [
      "Matthieu Dinot",
      "Benjamin Doerr",
      "Ulysse Hennebelle",
      "Sebastian Will"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2305.11400",
    "title": "Few-Shot Continual Learning for Conditional Generative Adversarial  Networks",
    "abstract": " Title: Few-Shot Continual Learning for Conditional Generative Adversarial  Networks ",
    "url": "https://arxiv.org/abs/2305.11400",
    "authors": [
      "Cat P. Le",
      "Juncheng Dong",
      "Ahmed Aloui",
      "Vahid Tarokh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.14000",
    "title": "Node-wise Diffusion for Scalable Graph Learning",
    "abstract": " Title: Node-wise Diffusion for Scalable Graph Learning ",
    "url": "https://arxiv.org/abs/2305.14000",
    "authors": [
      "Keke Huang",
      "Jing Tang",
      "Juncheng Liu",
      "Renchi Yang",
      "Xiaokui Xiao"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2305.14343",
    "title": "Video Prediction Models as Rewards for Reinforcement Learning",
    "abstract": " Comments: 22 pages, 18 figures, 4 tables. under review ",
    "url": "https://arxiv.org/abs/2305.14343",
    "authors": [
      "Alejandro Escontrela",
      "Ademi Adeniji",
      "Wilson Yan",
      "Ajay Jain",
      "Xue Bin Peng",
      "Ken Goldberg",
      "Youngwoon Lee",
      "Danijar Hafner",
      "Pieter Abbeel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.14452",
    "title": "Fourier Neural Operators for Arbitrary Resolution Climate Data  Downscaling",
    "abstract": " Comments: Presented at the ICLR 2023 workshop on \"Tackling Climate Change with Machine Learning\" ",
    "url": "https://arxiv.org/abs/2305.14452",
    "authors": [
      "Qidong Yang",
      "Alex Hernandez-Garcia",
      "Paula Harder",
      "Venkatesh Ramesh",
      "Prasanna Sattegeri",
      "Daniela Szwarcman",
      "Campbell D. Watson",
      "David Rolnick"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2305.15642",
    "title": "Learning-Based Automatic Synthesis of Software Code and Configuration",
    "abstract": " Comments: arXiv admin note: text overlap with arXiv:2211.00828 ",
    "url": "https://arxiv.org/abs/2305.15642",
    "authors": [
      "Shantanu Mandal"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.16508",
    "title": "Most Neural Networks Are Almost Learnable",
    "abstract": " Comments: Fixing small typos ",
    "url": "https://arxiv.org/abs/2305.16508",
    "authors": [
      "Amit Daniely",
      "Nathan Srebro",
      "Gal Vardi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.16780",
    "title": "Graph Neural Convection-Diffusion with Heterophily",
    "abstract": " Comments: Proc. International Joint Conference on Artificial Intelligence (IJCAI), Macao, China, Aug. 2023 ",
    "url": "https://arxiv.org/abs/2305.16780",
    "authors": [
      "Kai Zhao",
      "Qiyu Kang",
      "Yang Song",
      "Rui She",
      "Sijie Wang",
      "Wee Peng Tay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2305.16967",
    "title": "Evaluating Open-Domain Dialogues in Latent Space with Next Sentence  Prediction and Mutual Information",
    "abstract": " Comments: Accepted at ACL2023 ",
    "url": "https://arxiv.org/abs/2305.16967",
    "authors": [
      "Kun Zhao",
      "Bohao Yang",
      "Chenghua Lin",
      "Wenge Rong",
      "Aline Villavicencio",
      "Xiaohui Cui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.17220",
    "title": "VoxDet: Voxel Learning for Novel Instance Detection",
    "abstract": " Comments: 17 pages, 10 figures ",
    "url": "https://arxiv.org/abs/2305.17220",
    "authors": [
      "Bowen Li",
      "Jiashun Wang",
      "Yaoyu Hu",
      "Chen Wang",
      "Sebastian Scherer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.17326",
    "title": "Kernel-SSL: Kernel KL Divergence for Self-Supervised Learning",
    "abstract": " Title: Kernel-SSL: Kernel KL Divergence for Self-Supervised Learning ",
    "url": "https://arxiv.org/abs/2305.17326",
    "authors": [
      "Yifan Zhang",
      "Zhiquan Tan",
      "Jingqin Yang",
      "Yang Yuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.17380",
    "title": "No-Regret Online Reinforcement Learning with Adversarial Losses and  Transitions",
    "abstract": " Comments: 66 pages ",
    "url": "https://arxiv.org/abs/2305.17380",
    "authors": [
      "Tiancheng Jin",
      "Junyan Liu",
      "Chlo\u00e9 Rouyer",
      "William Chang",
      "Chen-Yu Wei",
      "Haipeng Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.17480",
    "title": "A Match Made in Heaven: A Multi-task Framework for Hyperbole and  Metaphor Detection",
    "abstract": " Title: A Match Made in Heaven: A Multi-task Framework for Hyperbole and  Metaphor Detection ",
    "url": "https://arxiv.org/abs/2305.17480",
    "authors": [
      "Naveen Badathala",
      "Abisek Rajakumar Kalarani",
      "Tejpalsingh Siledar",
      "Pushpak Bhattacharyya"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.17701",
    "title": "KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large  Language Model Application",
    "abstract": " Comments: 17 pages, 8 figures, 12 tables, ACL 2023 ",
    "url": "https://arxiv.org/abs/2305.17701",
    "authors": [
      "Hwaran Lee",
      "Seokhee Hong",
      "Joonsuk Park",
      "Takyoung Kim",
      "Gunhee Kim",
      "Jung-Woo Ha"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.17860",
    "title": "speech and noise dual-stream spectrogram refine network with speech  distortion loss for robust speech recognition",
    "abstract": " Title: speech and noise dual-stream spectrogram refine network with speech  distortion loss for robust speech recognition ",
    "url": "https://arxiv.org/abs/2305.17860",
    "authors": [
      "Haoyu Lu",
      "Nan Li",
      "Tongtong Song",
      "Longbiao Wang",
      "Jianwu Dang",
      "Xiaobao Wang",
      "Shiliang Zhang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2305.18079",
    "title": "Towards a Robust Framework for NeRF Evaluation",
    "abstract": " Comments: 9 pages, 4 experiments ",
    "url": "https://arxiv.org/abs/2305.18079",
    "authors": [
      "Adrian Azzarelli",
      "Nantheera Anantrasirichai",
      "David R Bull"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2305.18158",
    "title": "Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning",
    "abstract": " Comments: Accpected by CVPR 2023 ",
    "url": "https://arxiv.org/abs/2305.18158",
    "authors": [
      "Yu Wang",
      "Pengchong Qiao",
      "Chang Liu",
      "Guoli Song",
      "Xiawu Zheng",
      "Jie Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  }
]