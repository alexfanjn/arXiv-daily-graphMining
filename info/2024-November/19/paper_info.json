[
  {
    "id": "arXiv:2411.10449",
    "title": "Love in Action: Gamifying Public Video Cameras for Fostering Social Relationships in Real World",
    "abstract": "           In this paper, we create \"Love in Action\" (LIA), a body language-based social game utilizing video cameras installed in public spaces to enhance social relationships in real-world. In the game, participants assume dual roles, i.e., requesters, who issue social requests, and performers, who respond social requests through performing specified body languages. To mediate the communication between participants, we build an AI-enhanced video analysis system incorporating multiple visual analysis modules like person detection, attribute recognition, and action recognition, to assess the performer's body language quality. A two-week field study involving 27 participants shows significant improvements in their social friendships, as indicated by self-reported questionnaires. Moreover, user experiences are investigated to highlight the potential of public video cameras as a novel communication medium for socializing in public spaces.         ",
    "url": "https://arxiv.org/abs/2411.10449",
    "authors": [
      "Zhang Zhang",
      "Da Li",
      "Geng Wu",
      "Yaoning Li",
      "Xiaobing Sun",
      "Liang Wang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.10455",
    "title": "LLM-itation is the Sincerest Form of Data: Generating Synthetic Buggy Code Submissions for Computing Education",
    "abstract": "           There is a great need for data in computing education research. Data is needed to understand how students behave, to train models of student behavior to optimally support students, and to develop and validate new assessment tools and learning analytics techniques. However, relatively few computing education datasets are shared openly, often due to privacy regulations and issues in making sure the data is anonymous. Large language models (LLMs) offer a promising approach to create large-scale, privacy-preserving synthetic data, which can be used to explore various aspects of student learning, develop and test educational technologies, and support research in areas where collecting real student data may be challenging or impractical. This work explores generating synthetic buggy code submissions for introductory programming exercises using GPT-4o. We compare the distribution of test case failures between synthetic and real student data from two courses to analyze the accuracy of the synthetic data in mimicking real student data. Our findings suggest that LLMs can be used to generate synthetic incorrect submissions that are not significantly different from real student data with regard to test case failure distributions. Our research contributes to the development of reliable synthetic datasets for computing education research and teaching, potentially accelerating progress in the field while preserving student privacy.         ",
    "url": "https://arxiv.org/abs/2411.10455",
    "authors": [
      "Juho Leinonen",
      "Paul Denny",
      "Olli Kiljunen",
      "Stephen MacNeil",
      "Sami Sarsa",
      "Arto Hellas"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2411.10459",
    "title": "Evolutionary Multi-agent Reinforcement Learning in Group Social Dilemmas",
    "abstract": "           Reinforcement learning (RL) is a powerful machine learning technique that has been successfully applied to a wide variety of problems. However, it can be unpredictable and produce suboptimal results in complicated learning environments. This is especially true when multiple agents learn simultaneously, which creates a complex system that is often analytically intractable. Our work considers the fundamental framework of Q-learning in Public Goods Games, where RL individuals must work together to achieve a common goal. This setting allows us to study the tragedy of the commons and free rider effects in AI cooperation, an emerging field with potential to resolve challenging obstacles to the wider application of artificial intelligence. While this social dilemma has been mainly investigated through traditional and evolutionary game theory, our approach bridges the gap between these two by studying agents with an intermediate level of intelligence. Specifically, we consider the influence of learning parameters on cooperation levels in simulations and a limiting system of differential equations, as well as the effect of evolutionary pressures on exploration rate in both of these models. We find selection for higher and lower levels of exploration, as well as attracting values, and a condition that separates these in a restricted class of games. Our work enhances the theoretical understanding of evolutionary Q-learning, and extends our knowledge of the evolution of machine behavior in social dilemmas.         ",
    "url": "https://arxiv.org/abs/2411.10459",
    "authors": [
      "Brian Mintz",
      "Feng Fu"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Populations and Evolution (q-bio.PE)"
    ]
  },
  {
    "id": "arXiv:2411.10480",
    "title": "Hateful Meme Detection through Context-Sensitive Prompting and Fine-Grained Labeling",
    "abstract": "           The prevalence of multi-modal content on social media complicates automated moderation strategies. This calls for an enhancement in multi-modal classification and a deeper understanding of understated meanings in images and memes. Although previous efforts have aimed at improving model performance through fine-tuning, few have explored an end-to-end optimization pipeline that accounts for modalities, prompting, labeling, and fine-tuning. In this study, we propose an end-to-end conceptual framework for model optimization in complex tasks. Experiments support the efficacy of this traditional yet novel framework, achieving the highest accuracy and AUROC. Ablation experiments demonstrate that isolated optimizations are not ineffective on their own.         ",
    "url": "https://arxiv.org/abs/2411.10480",
    "authors": [
      "Rongxin Ouyang",
      "Kokil Jaidka",
      "Subhayan Mukerjee",
      "Guangyu Cui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2411.10481",
    "title": "Boolean-aware Boolean Circuit Classification: A Comprehensive Study on Graph Neural Network",
    "abstract": "           Boolean circuit is a computational graph that consists of the dynamic directed graph structure and static functionality. The commonly used logic optimization and Boolean matching-based transformation can change the behavior of the Boolean circuit for its graph structure and functionality in logic synthesis. The graph structure-based Boolean circuit classification can be grouped into the graph classification task, however, the functionality-based Boolean circuit classification remains an open problem for further research. In this paper, we first define the proposed matching-equivalent class based on its ``Boolean-aware'' property. The Boolean circuits in the proposed class can be transformed into each other. Then, we present a commonly study framework based on graph neural network~(GNN) to analyze the key factors that can affect the Boolean-aware Boolean circuit classification. The empirical experiment results verify the proposed analysis, and it also shows the direction and opportunity to improve the proposed problem. The code and dataset will be released after acceptance.         ",
    "url": "https://arxiv.org/abs/2411.10481",
    "authors": [
      "Liwei Ni",
      "Xinquan Li",
      "Biwei Xie",
      "Huawei Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2411.10483",
    "title": "Physics-Informed Neural Networks for Electrical Circuit Analysis: Applications in Dielectric Material Modeling",
    "abstract": "           Scientific machine learning (SciML) represents a significant advancement in integrating machine learning (ML) with scientific methodologies. At the forefront of this development are Physics-Informed Neural Networks (PINNs), which offer a promising approach by incorporating physical laws directly into the learning process, thereby reducing the need for extensive datasets. However, when data is limited or the system becomes more complex, PINNs can face challenges, such as instability and difficulty in accurately fitting the training data. In this article, we explore the capabilities and limitations of the DeepXDE framework, a tool specifically designed for implementing PINNs, in addressing both forward and inverse problems related to dielectric properties. Using RC circuit models to represent dielectric materials in HVDC systems, we demonstrate the effectiveness of PINNs in analyzing and improving system performance. Additionally, we show that applying a logarithmic transformation to the current (ln(I)) significantly enhances the stability and accuracy of PINN predictions, especially in challenging scenarios with sparse data or complex models. In inverse mode, however, we faced challenges in estimating key system parameters, such as resistance and capacitance, in more complex scenarios with longer time domains. This highlights the potential for future work in improving PINNs through transformations or other methods to enhance performance in inverse problems. This article provides pedagogical insights for those looking to use PINNs in both forward and inverse modes, particularly within the DeepXDE framework.         ",
    "url": "https://arxiv.org/abs/2411.10483",
    "authors": [
      "Reyhaneh Taj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)"
    ]
  },
  {
    "id": "arXiv:2411.10486",
    "title": "Artificial Intelligence for Infectious Disease Prediction and Prevention: A Comprehensive Review",
    "abstract": "           Artificial Intelligence (AI) and infectious diseases prediction have recently experienced a common development and advancement. Machine learning (ML) apparition, along with deep learning (DL) emergence, extended many approaches against diseases apparition and their spread. And despite their outstanding results in predicting infectious diseases, conflicts appeared regarding the types of data used and how they can be studied, analyzed, and exploited using various emerging methods. This has led to some ongoing discussions in the field. This research aims not only to provide an overview of what has been accomplished, but also to highlight the difficulties related to the types of data used, and the learning methods applied for each research objective. It categorizes these contributions into three areas: predictions using Public Health Data to prevent the spread of a transmissible disease within a region; predictions using Patients' Medical Data to detect whether a person is infected by a transmissible disease; and predictions using both Public and patient medical data to estimate the extent of disease spread in a population. The paper also critically assesses the potential of AI and outlines its limitations in infectious disease management.         ",
    "url": "https://arxiv.org/abs/2411.10486",
    "authors": [
      "Selestine Melchane",
      "Youssef Elmir",
      "Farid Kacimi",
      "Larbi Boubchir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Populations and Evolution (q-bio.PE)"
    ]
  },
  {
    "id": "arXiv:2411.10497",
    "title": "Structure Tensor Representation for Robust Oriented Object Detection",
    "abstract": "           Oriented object detection predicts orientation in addition to object location and bounding box. Precisely predicting orientation remains challenging due to angular periodicity, which introduces boundary discontinuity issues and symmetry ambiguities. Inspired by classical works on edge and corner detection, this paper proposes to represent orientation in oriented bounding boxes as a structure tensor. This representation combines the strengths of Gaussian-based methods and angle-coder solutions, providing a simple yet efficient approach that is robust to angular periodicity issues without additional hyperparameters. Extensive evaluations across five datasets demonstrate that the proposed structure tensor representation outperforms previous methods in both fully-supervised and weakly supervised tasks, achieving high precision in angular prediction with minimal computational overhead. Thus, this work establishes structure tensors as a robust and modular alternative for encoding orientation in oriented object detection. We make our code publicly available, allowing for seamless integration into existing object detectors.         ",
    "url": "https://arxiv.org/abs/2411.10497",
    "authors": [
      "Xavier Bou",
      "Gabriele Facciolo",
      "Rafael Grompone von Gioi",
      "Jean-Michel Morel",
      "Thibaud Ehret"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10498",
    "title": "Prompt-Guided Environmentally Consistent Adversarial Patch",
    "abstract": "           Adversarial attacks in the physical world pose a significant threat to the security of vision-based systems, such as facial recognition and autonomous driving. Existing adversarial patch methods primarily focus on improving attack performance, but they often produce patches that are easily detectable by humans and struggle to achieve environmental consistency, i.e., blending patches into the environment. This paper introduces a novel approach for generating adversarial patches, which addresses both the visual naturalness and environmental consistency of the patches. We propose Prompt-Guided Environmentally Consistent Adversarial Patch (PG-ECAP), a method that aligns the patch with the environment to ensure seamless integration into the environment. The approach leverages diffusion models to generate patches that are both environmental consistency and effective in evading detection. To further enhance the naturalness and consistency, we introduce two alignment losses: Prompt Alignment Loss and Latent Space Alignment Loss, ensuring that the generated patch maintains its adversarial properties while fitting naturally within its environment. Extensive experiments in both digital and physical domains demonstrate that PG-ECAP outperforms existing methods in attack success rate and environmental consistency.         ",
    "url": "https://arxiv.org/abs/2411.10498",
    "authors": [
      "Chaoqun Li",
      "Huanqian Yan",
      "Lifeng Zhou",
      "Tairan Chen",
      "Zhuodong Liu",
      "Hang Su"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10500",
    "title": "Edge-Only Universal Adversarial Attacks in Distributed Learning",
    "abstract": "           Distributed learning frameworks, which partition neural network models across multiple computing nodes, enhance efficiency in collaborative edge-cloud systems but may also introduce new vulnerabilities. In this work, we explore the feasibility of generating universal adversarial attacks when an attacker has access to the edge part of the model only, which consists in the first network layers. Unlike traditional universal adversarial perturbations (UAPs) that require full model knowledge, our approach shows that adversaries can induce effective mispredictions in the unknown cloud part by leveraging key features on the edge side. Specifically, we train lightweight classifiers from intermediate features available at the edge, i.e., before the split point, and use them in a novel targeted optimization to craft effective UAPs. Our results on ImageNet demonstrate strong attack transferability to the unknown cloud part. Additionally, we analyze the capability of an attacker to achieve targeted adversarial effect with edge-only knowledge, revealing intriguing behaviors. By introducing the first adversarial attacks with edge-only knowledge in split inference, this work underscores the importance of addressing partial model access in adversarial robustness, encouraging further research in this area.         ",
    "url": "https://arxiv.org/abs/2411.10500",
    "authors": [
      "Giulio Rossolini",
      "Tommaso Baldi",
      "Alessandro Biondi",
      "Giorgio Buttazzo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10503",
    "title": "Everything is a Video: Unifying Modalities through Next-Frame Prediction",
    "abstract": "           Multimodal learning, which involves integrating information from various modalities such as text, images, audio, and video, is pivotal for numerous complex tasks like visual question answering, cross-modal retrieval, and caption generation. Traditional approaches rely on modality-specific encoders and late fusion techniques, which can hinder scalability and flexibility when adapting to new tasks or modalities. To address these limitations, we introduce a novel framework that extends the concept of task reformulation beyond natural language processing (NLP) to multimodal learning. We propose to reformulate diverse multimodal tasks into a unified next-frame prediction problem, allowing a single model to handle different modalities without modality-specific components. This method treats all inputs and outputs as sequential frames in a video, enabling seamless integration of modalities and effective knowledge transfer across tasks. Our approach is evaluated on a range of tasks, including text-to-text, image-to-text, video-to-video, video-to-text, and audio-to-text, demonstrating the model's ability to generalize across modalities with minimal adaptation. We show that task reformulation can significantly simplify multimodal model design across various tasks, laying the groundwork for more generalized multimodal foundation models.         ",
    "url": "https://arxiv.org/abs/2411.10503",
    "authors": [
      "G. Thomas Hudson",
      "Dean Slack",
      "Thomas Winterbottom",
      "Jamie Sterling",
      "Chenghao Xiao",
      "Junjie Shentu",
      "Noura Al Moubayed"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.10507",
    "title": "RedTest: Towards Measuring Redundancy in Deep Neural Networks Effectively",
    "abstract": "           Deep learning has revolutionized computing in many real-world applications, arguably due to its remarkable performance and extreme convenience as an end-to-end solution. However, deep learning models can be costly to train and to use, especially for those large-scale models, making it necessary to optimize the original overly complicated models into smaller ones in scenarios with limited resources such as mobile applications or simply for resource saving. The key question in such model optimization is, how can we effectively identify and measure the redundancy in a deep learning model structure. While several common metrics exist in the popular model optimization techniques to measure the performance of models after optimization, they are not able to quantitatively inform the degree of remaining redundancy. To address the problem, we present a novel testing approach, i.e., RedTest, which proposes a novel testing metric called Model Structural Redundancy Score (MSRS) to quantitatively measure the degree of redundancy in a deep learning model structure. We first show that MSRS is effective in both revealing and assessing the redundancy issues in many state-of-the-art models, which urgently calls for model optimization. Then, we utilize MSRS to assist deep learning model developers in two practical application scenarios: 1) in Neural Architecture Search, we design a novel redundancy-aware algorithm to guide the search for the optimal model structure and demonstrate its effectiveness by comparing it to existing standard NAS practice; 2) in the pruning of large-scale pre-trained models, we prune the redundant layers of pre-trained models with the guidance of layer similarity to derive less redundant ones of much smaller size. Extensive experimental results demonstrate that removing such redundancy has a negligible effect on the model utility.         ",
    "url": "https://arxiv.org/abs/2411.10507",
    "authors": [
      "Yao Lu",
      "Peixin Zhang",
      "Jingyi Wang",
      "Lei Ma",
      "Xiaoniu Yang",
      "Qi Xuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10508",
    "title": "DR-BFR: Degradation Representation with Diffusion Models for Blind Face Restoration",
    "abstract": "           Blind face restoration (BFR) is fundamentally challenged by the extensive range of degradation types and degrees that impact model generalization. Recent advancements in diffusion models have made considerable progress in this field. Nevertheless, a critical limitation is their lack of awareness of specific degradation, leading to potential issues such as unnatural details and inaccurate textures. In this paper, we equip diffusion models with the capability to decouple various degradation as a degradation prompt from low-quality (LQ) face images via unsupervised contrastive learning with reconstruction loss, and demonstrate that this capability significantly improves performance, particularly in terms of the naturalness of the restored images. Our novel restoration scheme, named DR-BFR, guides the denoising of Latent Diffusion Models (LDM) by incorporating Degradation Representation (DR) and content features from LQ images. DR-BFR comprises two modules: 1) Degradation Representation Module (DRM): This module extracts degradation representation with content-irrelevant features from LQ faces and estimates a reasonable distribution in the degradation space through contrastive learning and a specially designed LQ reconstruction. 2) Latent Diffusion Restoration Module (LDRM): This module perceives both degradation features and content features in the latent space, enabling the restoration of high-quality images from LQ inputs. Our experiments demonstrate that the proposed DR-BFR significantly outperforms state-of-the-art methods quantitatively and qualitatively across various datasets. The DR effectively distinguishes between various degradations in blind face inverse problems and provides a reasonably powerful prompt to LDM.         ",
    "url": "https://arxiv.org/abs/2411.10508",
    "authors": [
      "Xinmin Qiu",
      "Bonan Li",
      "Zicheng Zhang",
      "Congying Han",
      "Tiande Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10509",
    "title": "TESGNN: Temporal Equivariant Scene Graph Neural Networks for Efficient and Robust Multi-View 3D Scene Understanding",
    "abstract": "           Scene graphs have proven to be highly effective for various scene understanding tasks due to their compact and explicit representation of relational information. However, current methods often overlook the critical importance of preserving symmetry when generating scene graphs from 3D point clouds, which can lead to reduced accuracy and robustness, particularly when dealing with noisy, multi-view data. This work, to the best of our knowledge, presents the first implementation of an Equivariant Scene Graph Neural Network (ESGNN) to generate semantic scene graphs from 3D point clouds, specifically for enhanced scene understanding. Furthermore, a significant limitation of prior methods is the absence of temporal modeling to capture time-dependent relationships among dynamically evolving entities within a scene. To address this gap, we introduce a novel temporal layer that leverages the symmetry-preserving properties of ESGNN to fuse scene graphs across multiple sequences into a unified global representation by an approximate graph-matching algorithm. Our combined architecture, termed the Temporal Equivariant Scene Graph Neural Network (TESGNN), not only surpasses existing state-of-the-art methods in scene estimation accuracy but also achieves faster convergence. Importantly, TESGNN is computationally efficient and straightforward to implement using existing frameworks, making it well-suited for real-time applications in robotics and computer vision. This approach paves the way for more robust and scalable solutions to complex multi-view scene understanding challenges. Our source code is publicly available at: this https URL ",
    "url": "https://arxiv.org/abs/2411.10509",
    "authors": [
      "Quang P. M. Pham",
      "Khoi T. N. Nguyen",
      "Lan C. Ngo",
      "Dezhen Song",
      "Truong Do",
      "Truong Son Hy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.10512",
    "title": "On the Privacy Risk of In-context Learning",
    "abstract": "           Large language models (LLMs) are excellent few-shot learners. They can perform a wide variety of tasks purely based on natural language prompts provided to them. These prompts contain data of a specific downstream task -- often the private dataset of a party, e.g., a company that wants to leverage the LLM for their purposes. We show that deploying prompted models presents a significant privacy risk for the data used within the prompt by instantiating a highly effective membership inference attack. We also observe that the privacy risk of prompted models exceeds fine-tuned models at the same utility levels. After identifying the model's sensitivity to their prompts -- in the form of a significantly higher prediction confidence on the prompted data -- as a cause for the increased risk, we propose ensembling as a mitigation strategy. By aggregating over multiple different versions of a prompted model, membership inference risk can be decreased.         ",
    "url": "https://arxiv.org/abs/2411.10512",
    "authors": [
      "Haonan Duan",
      "Adam Dziedzic",
      "Mohammad Yaghini",
      "Nicolas Papernot",
      "Franziska Boenisch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.10513",
    "title": "Any2Any: Incomplete Multimodal Retrieval with Conformal Prediction",
    "abstract": "           Autonomous agents perceive and interpret their surroundings by integrating multimodal inputs, such as vision, audio, and LiDAR. These perceptual modalities support retrieval tasks, such as place recognition in robotics. However, current multimodal retrieval systems encounter difficulties when parts of the data are missing due to sensor failures or inaccessibility, such as silent videos or LiDAR scans lacking RGB information. We propose Any2Any-a novel retrieval framework that addresses scenarios where both query and reference instances have incomplete modalities. Unlike previous methods limited to the imputation of two modalities, Any2Any handles any number of modalities without training generative models. It calculates pairwise similarities with cross-modal encoders and employs a two-stage calibration process with conformal prediction to align the similarities. Any2Any enables effective retrieval across multimodal datasets, e.g., text-LiDAR and text-time series. It achieves a Recall@5 of 35% on the KITTI dataset, which is on par with baseline models with complete modalities.         ",
    "url": "https://arxiv.org/abs/2411.10513",
    "authors": [
      "Po-han Li",
      "Yunhao Yang",
      "Mohammad Omama",
      "Sandeep Chinchali",
      "Ufuk Topcu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2411.10524",
    "title": "Robust Communication Design in RIS-Assisted THz Channels",
    "abstract": "           Terahertz (THz) communication offers the necessary bandwidth to meet the high data rate demands of next-generation wireless systems. However, it faces significant challenges, including severe path loss, dynamic blockages, and beam misalignment, which jeopardize communication reliability. Given that many 6G use cases require both high data rates and strong reliability, robust transmission schemes that achieve high throughput under these challenging conditions are essential for the effective use of high-frequency bands. In this context, we propose a novel mixed-criticality superposition coding scheme for reconfigurable intelligent surface (RIS)-assisted THz systems. This scheme leverages both the strong but intermittent direct line-of-sight link and the more reliable, yet weaker, RIS path to ensure robust delivery of high-criticality data while maintaining high overall throughput. We model a mixed-criticality queuing system and optimize transmit power to meet reliability and queue stability constraints. Simulation results show that our approach significantly reduces queuing delays for critical data while sustaining high overall throughput, outperforming conventional time-sharing methods. Additionally, we examine the impact of blockage, beam misalignment, and beamwidth adaptation on system performance. These results demonstrate that our scheme effectively balances reliability and throughput under challenging conditions, while also underscoring the need for robust beamforming techniques to mitigate the impact of misalignment in RIS-assisted channels.         ",
    "url": "https://arxiv.org/abs/2411.10524",
    "authors": [
      "Yasemin Karacora",
      "Adam Umra",
      "Aydin Sezgin"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2411.10558",
    "title": "Multi-agent Path Finding for Timed Tasks using Evolutionary Games",
    "abstract": "           Autonomous multi-agent systems such as hospital robots and package delivery drones often operate in highly uncertain environments and are expected to achieve complex temporal task objectives while ensuring safety. While learning-based methods such as reinforcement learning are popular methods to train single and multi-agent autonomous systems under user-specified and state-based reward functions, applying these methods to satisfy trajectory-level task objectives is a challenging problem. Our first contribution is the use of weighted automata to specify trajectory-level objectives, such that, maximal paths induced in the weighted automaton correspond to desired trajectory-level behaviors. We show how weighted automata-based specifications go beyond timeliness properties focused on deadlines to performance properties such as expeditiousness. Our second contribution is the use of evolutionary game theory (EGT) principles to train homogeneous multi-agent teams targeting homogeneous task objectives. We show how shared experiences of agents and EGT-based policy updates allow us to outperform state-of-the-art reinforcement learning (RL) methods in minimizing path length by nearly 30\\% in large spaces. We also show that our algorithm is computationally faster than deep RL methods by at least an order of magnitude. Additionally our results indicate that it scales better with an increase in the number of agents as compared to other methods.         ",
    "url": "https://arxiv.org/abs/2411.10558",
    "authors": [
      "Sheryl Paul",
      "Anand Balakrishnan",
      "Xin Qin",
      "Jyotirmoy V. Deshmukh"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computer Science and Game Theory (cs.GT)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2411.10565",
    "title": "Comparing Robustness Against Adversarial Attacks in Code Generation: LLM-Generated vs. Human-Written",
    "abstract": "           Thanks to the widespread adoption of Large Language Models (LLMs) in software engineering research, the long-standing dream of automated code generation has become a reality on a large scale. Nowadays, LLMs such as GitHub Copilot and ChatGPT are extensively used in code generation for enterprise and open-source software development and maintenance. Despite their unprecedented successes in code generation, research indicates that codes generated by LLMs exhibit vulnerabilities and security issues. Several studies have been conducted to evaluate code generated by LLMs, considering various aspects such as security, vulnerability, code smells, and robustness. While some studies have compared the performance of LLMs with that of humans in various software engineering tasks, there's a notable gap in research: no studies have directly compared human-written and LLM-generated code for their robustness analysis. To fill this void, this paper introduces an empirical study to evaluate the adversarial robustness of Pre-trained Models of Code (PTMCs) fine-tuned on code written by humans and generated by LLMs against adversarial attacks for software clone detection. These attacks could potentially undermine software security and reliability. We consider two datasets, two state-of-the-art PTMCs, two robustness evaluation criteria, and three metrics to use in our experiments. Regarding effectiveness criteria, PTMCs fine-tuned on human-written code always demonstrate more robustness than those fine-tuned on LLMs-generated code. On the other hand, in terms of adversarial code quality, in 75% experimental combinations, PTMCs fine-tuned on the human-written code exhibit more robustness than the PTMCs fine-tuned on the LLMs-generated code.         ",
    "url": "https://arxiv.org/abs/2411.10565",
    "authors": [
      "Md Abdul Awal",
      "Mrigank Rochan",
      "Chanchal K. Roy"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2411.10581",
    "title": "On the Shortcut Learning in Multilingual Neural Machine Translation",
    "abstract": "           In this study, we revisit the commonly-cited off-target issue in multilingual neural machine translation (MNMT). By carefully designing experiments on different MNMT scenarios and models, we attribute the off-target issue to the overfitting of the shortcuts of (non-centric, centric) language mappings. Specifically, the learned shortcuts biases MNMT to mistakenly translate non-centric languages into the centric language instead of the expected non-centric language for zero-shot translation. Analyses on learning dynamics show that the shortcut learning generally occurs in the later stage of model training, and multilingual pretraining accelerates and aggravates the shortcut learning. Based on these observations, we propose a simple and effective training strategy to eliminate the shortcuts in MNMT models by leveraging the forgetting nature of model training. The only difference from the standard training is that we remove the training instances that may induce the shortcut learning in the later stage of model training. Without introducing any additional data and computational costs, our approach can consistently and significantly improve the zero-shot translation performance by alleviating the shortcut learning for different MNMT models and benchmarks.         ",
    "url": "https://arxiv.org/abs/2411.10581",
    "authors": [
      "Wenxuan Wang",
      "Wenxiang Jiao",
      "Jen-tse Huang",
      "Zhaopeng Tu",
      "Michael R. Lyu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.10583",
    "title": "Personalization of Code Readability Evaluation Based on LLM Using Collaborative Filtering",
    "abstract": "           Code readability is an important indicator of software maintenance as it can significantly impact maintenance efforts. Recently, LLM (large language models) have been utilized for code readability evaluation. However, readability evaluation differs among developers, so personalization of the evaluation by LLM is needed. This study proposes a method which calibrates the evaluation, using collaborative filtering. Our preliminary analysis suggested that the method effectively enhances the accuracy of the readability evaluation using LLMs.         ",
    "url": "https://arxiv.org/abs/2411.10583",
    "authors": [
      "Buntaro Hiraki",
      "Kensei Hamamoto",
      "Ami Kimura",
      "Masateru Tsunoda",
      "Amjed Tahir",
      "Kwabena Ebo Bennin",
      "Akito Monden",
      "Keitaro Nakasai"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2411.10599",
    "title": "Generating Energy-efficient code with LLMs",
    "abstract": "           The increasing electricity demands of personal computers, communication networks, and data centers contribute to higher atmospheric greenhouse gas emissions, which in turn lead to global warming and climate change. Therefore the energy consumption of code must be minimized. Code can be generated by large language models. We look at the influence of prompt modification on the energy consumption of the code generated. We use three different Python code problems of varying difficulty levels. Prompt modification is done by adding the sentence ``Give me an energy-optimized solution for this problem'' or by using two Python coding best practices. The large language models used are CodeLlama-70b, CodeLlama-70b-Instruct, CodeLlama-70b-Python, DeepSeek-Coder-33b-base, and DeepSeek-Coder-33b-instruct. We find a decrease in energy consumption for a specific combination of prompt optimization, LLM, and Python code problem. However, no single optimization prompt consistently decreases energy consumption for the same LLM across the different Python code problems.         ",
    "url": "https://arxiv.org/abs/2411.10599",
    "authors": [
      "Tom Cappendijk",
      "Pepijn de Reus",
      "Ana Oprescu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.10612",
    "title": "Contextualizing Security and Privacy of Software-Defined Vehicles: State of the Art and Industry Perspectives",
    "abstract": "           The growing reliance on software in vehicles has given rise to the concept of Software-Defined Vehicles (SDVs), fundamentally reshaping the vehicles and the automotive industry. This survey explores the cybersecurity and privacy challenges posed by SDVs, which increasingly integrate features like Over-the-Air (OTA) updates and Vehicle-to-Everything (V2X) communication. While these advancements enhance vehicle capabilities and flexibility, they also come with a flip side: increased exposure to security risks including API vulnerabilities, third-party software risks, and supply-chain threats. The transition to SDVs also raises significant privacy concerns, with vehicles collecting vast amounts of sensitive data, such as location and driver behavior, that could be exploited using inference attacks. This work aims to provide a detailed overview of security threats, mitigation strategies, and privacy risks in SDVs, primarily through a literature review, enriched with insights from a targeted questionnaire with industry experts. Key topics include defining SDVs, comparing them to Connected Vehicles (CVs) and Autonomous Vehicles (AVs), discussing the security challenges associated with OTA updates and the impact of SDV features on data privacy. Our findings highlight the need for robust security frameworks, standardized communication protocols, and privacy-preserving techniques to address the issues of SDVs. This work ultimately emphasizes the importance of a multi-layered defense strategy,integrating both in-vehicle and cloud-based security solutions, to safeguard future SDVs and increase user trust.         ",
    "url": "https://arxiv.org/abs/2411.10612",
    "authors": [
      "Marco De Vincenzi",
      "Mert D. Pes\u00e9",
      "Chiara Bodei",
      "Ilaria Matteucci",
      "Richard R. Brooks",
      "Monowar Hasan",
      "Andrea Saracino",
      "Mohammad Hamad",
      "Sebastian Steinhorst"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Operating Systems (cs.OS)"
    ]
  },
  {
    "id": "arXiv:2411.10622",
    "title": "KAT to KANs: A Review of Kolmogorov-Arnold Networks and the Neural Leap Forward",
    "abstract": "           The curse of dimensionality poses a significant challenge to modern multilayer perceptron-based architectures, often causing performance stagnation and scalability issues. Addressing this limitation typically requires vast amounts of data. In contrast, Kolmogorov-Arnold Networks have gained attention in the machine learning community for their bold claim of being unaffected by the curse of dimensionality. This paper explores the Kolmogorov-Arnold representation theorem and the mathematical principles underlying Kolmogorov-Arnold Networks, which enable their scalability and high performance in high-dimensional spaces. We begin with an introduction to foundational concepts necessary to understand Kolmogorov-Arnold Networks, including interpolation methods and Basis-splines, which form their mathematical backbone. This is followed by an overview of perceptron architectures and the Universal approximation theorem, a key principle guiding modern machine learning. This is followed by an overview of the Kolmogorov-Arnold representation theorem, including its mathematical formulation and implications for overcoming dimensionality challenges. Next, we review the architecture and error-scaling properties of Kolmogorov-Arnold Networks, demonstrating how these networks achieve true freedom from the curse of dimensionality. Finally, we discuss the practical viability of Kolmogorov-Arnold Networks, highlighting scenarios where their unique capabilities position them to excel in real-world applications. This review aims to offer insights into Kolmogorov-Arnold Networks' potential to redefine scalability and performance in high-dimensional learning tasks.         ",
    "url": "https://arxiv.org/abs/2411.10622",
    "authors": [
      "Divesh Basina",
      "Joseph Raj Vishal",
      "Aarya Choudhary",
      "Bharatesh Chakravarthi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.10629",
    "title": "Leveraging large language models for efficient representation learning for entity resolution",
    "abstract": "           In this paper, the authors propose TriBERTa, a supervised entity resolution system that utilizes a pre-trained large language model and a triplet loss function to learn representations for entity matching. The system consists of two steps: first, name entity records are fed into a Sentence Bidirectional Encoder Representations from Transformers (SBERT) model to generate vector representations, which are then fine-tuned using contrastive learning based on a triplet loss function. Fine-tuned representations are used as input for entity matching tasks, and the results show that the proposed approach outperforms state-of-the-art representations, including SBERT without fine-tuning and conventional Term Frequency-Inverse Document Frequency (TF-IDF), by a margin of 3 - 19%. Additionally, the representations generated by TriBERTa demonstrated increased robustness, maintaining consistently higher performance across a range of datasets. The authors also discussed the importance of entity resolution in today's data-driven landscape and the challenges that arise when identifying and reconciling duplicate data across different sources. They also described the ER process, which involves several crucial steps, including blocking, entity matching, and clustering.         ",
    "url": "https://arxiv.org/abs/2411.10629",
    "authors": [
      "Xiaowei Xu",
      "Bi T. Foua",
      "Xingqiao Wang",
      "Vivek Gunasekaran",
      "John R. Talburt"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.10632",
    "title": "Quantifying community evolves in temporal networks",
    "abstract": "           When we detect communities in temporal networks it is important to ask questions about how they change in time. Normalised Mutual Information (NMI) has been used to measure the similarity of communities when the nodes on a network do not change. We propose two extensions namely Union-Normalised Mutual Information (UNMI) and Intersection-Normalised Mutual Information (INMI). UNMI and INMI evaluate the similarity of community structure under the condition of node variation. Experiments show that these methods are effective in dealing with temporal networks with the changes in the set of nodes, and can capture the dynamic evolution of community structure in both synthetic and real temporal networks. This study not only provides a new similarity measurement method for network analysis but also helps to deepen the understanding of community change in complex temporal networks.         ",
    "url": "https://arxiv.org/abs/2411.10632",
    "authors": [
      "Peijie Zhong",
      "Cheick Ba",
      "Ra\u00fal Mondrag\u00f3n",
      "Richard Clegg"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2411.10656",
    "title": "Precision or Peril: Evaluating Code Quality from Quantized Large Language Models",
    "abstract": "           When scaled to hundreds of billions of parameters, Large Language Models (LLMs) such as GPT-4 and LLaMA-405b have demonstrated remarkable capabilities in tasks such as code generation, code completion, and writing test cases. However, scaling up model sizes results in exponentially higher computational cost and energy consumption, leaving a large carbon footprint and making these models difficult to use by academic researchers and small businesses. Quantization has emerged as a way to mitigate the memory overhead of LLMs, allowing them to run on smaller hardware for lower prices. Quantization, however, may have detrimental effects on a model's output and it's effects on LLM generated code quality remains understudied and requires constant evaluation as LLMs are improved. This study aims to evaluate the current code generation capabilities of smaller LLMs using various metrics, exploring the impact of quantization on code quality, and identifying prevalent quality issues in the generated code. Method: We conducted a comprehensive evaluation of four smaller open-source LLMs across two benchmarks and code similarity scores. The impact of 8-bit and 4-bit quantization was analyzed, and a static analysis tool was utilized to scrutinize the generated code's quality. Our findings reveal that while the tested LLMs exhibit potential, these smaller LLMs produce code with subpar performance on established benchmarks. The effects of quantization on code quality are inconsistent, and the generated code frequently exhibits recurring quality and maintainability issues. This study underscores the necessity for careful scrutiny and validation of LLM-generated code before its adoption in software projects. While smaller LLMs can generate code, their output requires careful monitoring and validation by practitioners before integration into software projects.         ",
    "url": "https://arxiv.org/abs/2411.10656",
    "authors": [
      "Eric L. Melin",
      "Adam J. Torek",
      "Nasir U. Eisty",
      "Casey Kennington"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2411.10661",
    "title": "Enhancing PTSD Outcome Prediction with Ensemble Models in Disaster Contexts",
    "abstract": "           Post-traumatic stress disorder (PTSD) is a significant mental health challenge that affects individuals exposed to traumatic events. Early detection and effective intervention for PTSD are crucial, as it can lead to long-term psychological distress if untreated. Accurate detection of PTSD is essential for timely and targeted mental health interventions, especially in disaster-affected populations. Existing research has explored machine learning approaches for classifying PTSD, but many face limitations in terms of model performance and generalizability. To address these issues, we implemented a comprehensive preprocessing pipeline. This included data cleaning, missing value treatment using the SimpleImputer, label encoding of categorical variables, data augmentation using SMOTE to balance the dataset, and feature scaling with StandardScaler. The dataset was split into 80\\% training and 20\\% testing. We developed an ensemble model using a majority voting technique among several classifiers, including Logistic Regression, Support Vector Machines (SVM), Random Forest, XGBoost, LightGBM, and a customized Artificial Neural Network (ANN). The ensemble model achieved an accuracy of 96.76\\% with a benchmark dataset, significantly outperforming individual models. The proposed method's advantages include improved robustness through the combination of multiple models, enhanced ability to generalize across diverse data points, and increased accuracy in detecting PTSD. Additionally, the use of SMOTE for data augmentation ensured better handling of imbalanced datasets, leading to more reliable predictions. The proposed approach offers valuable insights for policymakers and healthcare providers by leveraging predictive analytics to address mental health issues in vulnerable populations, particularly those affected by disasters.         ",
    "url": "https://arxiv.org/abs/2411.10661",
    "authors": [
      "Ayesha Siddiqua",
      "Atib Mohammad Oni",
      "Abu Saleh Musa Miah",
      "Jungpil Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10673",
    "title": "How to Defend Against Large-scale Model Poisoning Attacks in Federated Learning: A Vertical Solution",
    "abstract": "           Federated learning (FL) is vulnerable to model poisoning attacks due to its distributed nature. The current defenses start from all user gradients (model updates) in each communication round and solve for the optimal aggregation gradients (horizontal solution). This horizontal solution will completely fail when facing large-scale (>50%) model poisoning attacks. In this work, based on the key insight that the convergence process of the model is a highly predictable process, we break away from the traditional horizontal solution of defense and innovatively transform the problem of solving the optimal aggregation gradients into a vertical solution problem. We propose VERT, which uses global communication rounds as the vertical axis, trains a predictor using historical gradients information to predict user gradients, and compares the similarity with actual user gradients to precisely and efficiently select the optimal aggregation gradients. In order to reduce the computational complexity of VERT, we design a low dimensional vector projector to project the user gradients to a computationally acceptable length, and then perform subsequent predictor training and prediction tasks. Exhaustive experiments show that VERT is efficient and scalable, exhibiting excellent large-scale (>=80%) model poisoning defense effects under different FL scenarios. In addition, we can design projector with different structures for different model structures to adapt to aggregation servers with different computing power.         ",
    "url": "https://arxiv.org/abs/2411.10673",
    "authors": [
      "Jinbo Wang",
      "Ruijin Wang",
      "Fengli Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.10679",
    "title": "SPDFusion: An Infrared and Visible Image Fusion Network Based on a Non-Euclidean Representation of Riemannian Manifolds",
    "abstract": "           Euclidean representation learning methods have achieved commendable results in image fusion tasks, which can be attributed to their clear advantages in handling with linear space. However, data collected from a realistic scene usually have a non-Euclidean structure, where Euclidean metric might be limited in representing the true data relationships, degrading fusion performance. To address this issue, a novel SPD (symmetric positive definite) manifold learning framework is proposed for multi-modal image fusion, named SPDFusion, which extends the image fusion approach from the Euclidean space to the SPD manifolds. Specifically, we encode images according to the Riemannian geometry to exploit their intrinsic statistical correlations, thereby aligning with human visual perception. Actually, the SPD matrix underpins our network learning, with a cross-modal fusion strategy employed to harness modality-specific dependencies and augment complementary information. Subsequently, an attention module is designed to process the learned weight matrix, facilitating the weighting of spatial global correlation semantics via SPD matrix multiplication. Based on this, we design an end-to-end fusion network based on cross-modal manifold learning. Extensive experiments on public datasets demonstrate that our framework exhibits superior performance compared to the current state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2411.10679",
    "authors": [
      "Huan Kang",
      "Hui Li",
      "Tianyang Xu",
      "Rui Wang",
      "Xiao-Jun Wu",
      "Josef Kittler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10697",
    "title": "Language Model Evolutionary Algorithms for Recommender Systems: Benchmarks and Algorithm Comparisons",
    "abstract": "           In the evolutionary computing community, the remarkable language-handling capabilities and reasoning power of large language models (LLMs) have significantly enhanced the functionality of evolutionary algorithms (EAs), enabling them to tackle optimization problems involving structured language or program code. Although this field is still in its early stages, its impressive potential has led to the development of various LLM-based EAs. To effectively evaluate the performance and practical applicability of these LLM-based EAs, benchmarks with real-world relevance are essential. In this paper, we focus on LLM-based recommender systems (RSs) and introduce a benchmark problem set, named RSBench, specifically designed to assess the performance of LLM-based EAs in recommendation prompt optimization. RSBench emphasizes session-based recommendations, aiming to discover a set of Pareto optimal prompts that guide the recommendation process, providing accurate, diverse, and fair recommendations. We develop three LLM-based EAs based on established EA frameworks and experimentally evaluate their performance using RSBench. Our study offers valuable insights into the application of EAs in LLM-based RSs. Additionally, we explore key components that may influence the overall performance of the RS, providing meaningful guidance for future research on the development of LLM-based EAs in RSs.         ",
    "url": "https://arxiv.org/abs/2411.10697",
    "authors": [
      "Jiao Liu",
      "Zhu Sun",
      "Shanshan Feng",
      "Yew-Soon Ong"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2411.10701",
    "title": "Diffusion-based Layer-wise Semantic Reconstruction for Unsupervised Out-of-Distribution Detection",
    "abstract": "           Unsupervised out-of-distribution (OOD) detection aims to identify out-of-domain data by learning only from unlabeled In-Distribution (ID) training samples, which is crucial for developing a safe real-world machine learning system. Current reconstruction-based methods provide a good alternative approach by measuring the reconstruction error between the input and its corresponding generative counterpart in the pixel/feature space. However, such generative methods face a key dilemma: improving the reconstruction power of the generative model while keeping a compact representation of the ID data. To address this issue, we propose the diffusion-based layer-wise semantic reconstruction approach for unsupervised OOD detection. The innovation of our approach is that we leverage the diffusion model's intrinsic data reconstruction ability to distinguish ID samples from OOD samples in the latent feature space. Moreover, to set up a comprehensive and discriminative feature representation, we devise a multi-layer semantic feature extraction strategy. By distorting the extracted features with Gaussian noise and applying the diffusion model for feature reconstruction, the separation of ID and OOD samples is implemented according to the reconstruction errors. Extensive experimental results on multiple benchmarks built upon various datasets demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy and speed. Code is available at <this https URL.         ",
    "url": "https://arxiv.org/abs/2411.10701",
    "authors": [
      "Ying Yang",
      "De Cheng",
      "Chaowei Fang",
      "Yubiao Wang",
      "Changzhe Jiao",
      "Lechao Cheng",
      "Nannan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2411.10702",
    "title": "Wireless Resource Allocation with Collaborative Distributed and Centralized DRL under Control Channel Attacks",
    "abstract": "           In this paper, we consider a wireless resource allocation problem in a cyber-physical system (CPS) where the control channel, carrying resource allocation commands, is subjected to denial-of-service (DoS) attacks. We propose a novel concept of collaborative distributed and centralized (CDC) resource allocation to effectively mitigate the impact of these attacks. To optimize the CDC resource allocation policy, we develop a new CDC-deep reinforcement learning (DRL) algorithm, whereas existing DRL frameworks only formulate either centralized or distributed decision-making problems. Simulation results demonstrate that the CDC-DRL algorithm significantly outperforms state-of-the-art DRL benchmarks, showcasing its ability to address resource allocation problems in large-scale CPSs under control channel attacks.         ",
    "url": "https://arxiv.org/abs/2411.10702",
    "authors": [
      "Ke Wang",
      "Wanchun Liu",
      "Teng Joon Lim"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.10709",
    "title": "Diagnostic Text-guided Representation Learning in Hierarchical Classification for Pathological Whole Slide Image",
    "abstract": "           With the development of digital imaging in medical microscopy, artificial intelligent-based analysis of pathological whole slide images (WSIs) provides a powerful tool for cancer diagnosis. Limited by the expensive cost of pixel-level annotation, current research primarily focuses on representation learning with slide-level labels, showing success in various downstream tasks. However, given the diversity of lesion types and the complex relationships between each other, these techniques still deserve further exploration in addressing advanced pathology tasks. To this end, we introduce the concept of hierarchical pathological image classification and propose a representation learning called PathTree. PathTree considers the multi-classification of diseases as a binary tree structure. Each category is represented as a professional pathological text description, which messages information with a tree-like encoder. The interactive text features are then used to guide the aggregation of hierarchical multiple representations. PathTree uses slide-text similarity to obtain probability scores and introduces two extra tree specific losses to further constrain the association between texts and slides. Through extensive experiments on three challenging hierarchical classification datasets: in-house cryosectioned lung tissue lesion identification, public prostate cancer grade assessment, and public breast cancer subtyping, our proposed PathTree is consistently competitive compared to the state-of-the-art methods and provides a new perspective on the deep learning-assisted solution for more complex WSI classification.         ",
    "url": "https://arxiv.org/abs/2411.10709",
    "authors": [
      "Jiawen Li",
      "Qiehe Sun",
      "Renao Yan",
      "Yizhi Wang",
      "Yuqiu Fu",
      "Yani Wei",
      "Tian Guan",
      "Huijuan Shi",
      "Yonghonghe He",
      "Anjia Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10715",
    "title": "EVT: Efficient View Transformation for Multi-Modal 3D Object Detection",
    "abstract": "           Multi-modal sensor fusion in bird's-eye-view (BEV) representation has become the leading approach in 3D object detection. However, existing methods often rely on depth estimators or transformer encoders for view transformation, incurring substantial computational overhead. Furthermore, the lack of precise geometric correspondence between 2D and 3D spaces leads to spatial and ray-directional misalignments, restricting the effectiveness of BEV representations. To address these challenges, we propose a novel 3D object detector via efficient view transformation (EVT), which leverages a well-structured BEV representation to enhance accuracy and efficiency. EVT focuses on two main areas. First, it employs Adaptive Sampling and Adaptive Projection (ASAP), using LiDAR guidance to generate 3D sampling points and adaptive kernels. The generated points and kernels are then used to facilitate the transformation of image features into BEV space and refine the BEV features. Second, EVT includes an improved transformer-based detection framework, which contains a group-wise query initialization method and an enhanced query update framework. It is designed to effectively utilize the obtained multi-modal BEV features within the transformer decoder. By leveraging the geometric properties of object queries, this framework significantly enhances detection performance, especially in a multi-layer transformer decoder structure. EVT achieves state-of-the-art performance on the nuScenes test set with real-time inference speed.         ",
    "url": "https://arxiv.org/abs/2411.10715",
    "authors": [
      "Yongjin Lee",
      "Hyeon-Mun Jeong",
      "Yurim Jeon",
      "Sanghyun Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10716",
    "title": "FlowScope: Enhancing Decision Making by Time Series Forecasting based on Prediction Optimization using HybridFlow Forecast Framework",
    "abstract": "           Time series forecasting is crucial in several sectors, such as meteorology, retail, healthcare, and finance. Accurately forecasting future trends and patterns is crucial for strategic planning and making well-informed decisions. In this case, it is crucial to include many forecasting methodologies. The strengths of Auto-regressive Integrated Moving Average (ARIMA) for linear time series, Seasonal ARIMA models (SARIMA) for seasonal time series, Exponential Smoothing State Space Models (ETS) for handling errors and trends, and Long Short-Term Memory (LSTM) Neural Network model for complex pattern recognition have been combined to create a comprehensive framework called FlowScope. SARIMA excels in capturing seasonal variations, whereas ARIMA ensures effective handling of linear time series. ETS models excel in capturing trends and correcting errors, whereas LSTM networks excel in reflecting intricate temporal connections. By combining these methods from both machine learning and deep learning, we propose a deep-hybrid learning approach FlowScope which offers a versatile and robust platform for predicting time series data. This empowers enterprises to make informed decisions and optimize long-term strategies for maximum performance. Keywords: Time Series Forecasting, HybridFlow Forecast Framework, Deep-Hybrid Learning, Informed Decisions.         ",
    "url": "https://arxiv.org/abs/2411.10716",
    "authors": [
      "Nitin Sagar Boyeena",
      "Begari Susheel Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2411.10719",
    "title": "Computational Complexity of Envy-free and Exchange-stable Seat Arrangement Problems on Grid Graphs",
    "abstract": "           The Seat Arrangement Problem is a problem of finding a desirable seat arrangement for given preferences of agents and a seat graph that represents a configuration of seats. In this paper, we consider decision problems of determining if an envy-free arrangement exists and an exchange-stable arrangement exists, when a seat graph is an $\\ell \\times m$ grid graph. When $\\ell=1$, the seat graph is a path of length $m$ and both problems have been known to be NP-complete. In this paper, we extend it and show that both problems are NP-complete for any integer $\\ell \\geq 2$.         ",
    "url": "https://arxiv.org/abs/2411.10719",
    "authors": [
      "Sota Kawase",
      "Shuichi Miyazaki"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2411.10720",
    "title": "Multi Scale Graph Neural Network for Alzheimer's Disease",
    "abstract": "           Alzheimer's disease (AD) is a complex, progressive neurodegenerative disorder characterized by extracellular A\\b{eta} plaques, neurofibrillary tau tangles, glial activation, and neuronal degeneration, involving multiple cell types and pathways. Current models often overlook the cellular context of these pathways. To address this, we developed a multiscale graph neural network (GNN) model, ALZ PINNACLE, using brain omics data from donors spanning the entire aging to AD spectrum. ALZ PINNACLE is based on the PINNACLE GNN framework, which learns context-aware protein, cell type, and tissue representations within a unified latent space. ALZ PINNACLE was trained on 14,951 proteins, 206,850 protein interactions, 7 cell types, and 48 cell subtypes or states. After pretraining, we investigated the learned embedding of APOE, the largest genetic risk factor for AD, across different cell types. Notably, APOE embeddings showed high similarity in microglial, neuronal, and CD8 cells, suggesting a similar role of APOE in these cell types. Fine tuning the model on AD risk genes revealed cell type contexts predictive of the role of APOE in AD. Our results suggest that ALZ PINNACLE may provide a valuable framework for uncovering novel insights into AD neurobiology.         ",
    "url": "https://arxiv.org/abs/2411.10720",
    "authors": [
      "Anya Chauhan",
      "Ayush Noori",
      "Zhaozhi Li",
      "Yingnan He",
      "Michelle M Li",
      "Marinka Zitnik",
      "Sudeshna Das"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2411.10729",
    "title": "On-device Anomaly Detection in Conveyor Belt Operations",
    "abstract": "           Mining 4.0 leverages advancements in automation, digitalization, and interconnected technologies from Industry 4.0 to address the unique challenges of the mining sector, enhancing efficiency, safety, and sustainability. Conveyor belts are crucial in mining operations by enabling the continuous and efficient movement of bulk materials over long distances, which directly impacts productivity. While detecting anomalies in specific conveyor belt components, such as idlers, pulleys, and belt surfaces, has been widely studied, identifying the root causes of these failures remains critical due to factors like changing production conditions and operator errors. Continuous monitoring of mining conveyor belt work cycles for anomaly detection is still at an early stage and requires robust solutions. This study proposes two distinctive pattern recognition approaches for real-time anomaly detection in the operational cycles of mining conveyor belts, combining feature extraction, threshold-based cycle detection, and tiny machine-learning classification. Both approaches outperformed a state-of-the-art technique on two datasets for duty cycle classification in terms of F1-scores. The first approach, with 97.3% and 80.2% for normal and abnormal cycles, respectively, reaches the highest performance in the first dataset while the second approach excels on the second dataset, scoring 91.3% and 67.9%. Implemented on two low-power microcontrollers, the methods demonstrated efficient, real-time operation with energy consumption of 13.3 and 20.6 ${\\mu}$J during inference. These results offer valuable insights for detecting mechanical failure sources, supporting targeted preventive maintenance, and optimizing production cycles.         ",
    "url": "https://arxiv.org/abs/2411.10729",
    "authors": [
      "Luciano S. Martinez-Rau",
      "Yuxuan Zhang",
      "Bengt Oelmann",
      "Sebastian Bader"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2411.10730",
    "title": "Comparison of Multilingual and Bilingual Models for Satirical News Detection of Arabic and English",
    "abstract": "           Satirical news is real news combined with a humorous comment or exaggerated content, and it often mimics the format and style of real news. However, satirical news is often misunderstood as misinformation, especially by individuals from different cultural and social backgrounds. This research addresses the challenge of distinguishing satire from truthful news by leveraging multilingual satire detection methods in English and Arabic. We explore both zero-shot and chain-of-thought (CoT) prompting using two language models, Jais-chat(13B) and LLaMA-2-chat(7B). Our results show that CoT prompting offers a significant advantage for the Jais-chat model over the LLaMA-2-chat model. Specifically, Jais-chat achieved the best performance, with an F1-score of 80\\% in English when using CoT prompting. These results highlight the importance of structured reasoning in CoT, which enhances contextual understanding and is vital for complex tasks like satire detection.         ",
    "url": "https://arxiv.org/abs/2411.10730",
    "authors": [
      "Omar W. Abdalla",
      "Aditya Joshi",
      "Rahat Masood",
      "Salil S. Kanhere"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.10753",
    "title": "Chain-of-Programming (CoP) : Empowering Large Language Models for Geospatial Code Generation",
    "abstract": "           With the rapid growth of interdisciplinary demands for geospatial modeling and the rise of large language models (LLMs), geospatial code generation technology has seen significant advancements. However, existing LLMs often face challenges in the geospatial code generation process due to incomplete or unclear user requirements and insufficient knowledge of specific platform syntax rules, leading to the generation of non-executable code, a phenomenon known as \"code hallucination.\" To address this issue, this paper proposes a Chain of Programming (CoP) framework, which decomposes the code generation process into five steps: requirement analysis, algorithm design, code implementation, code debugging, and code annotation. The framework incorporates a shared information pool, knowledge base retrieval, and user feedback mechanisms, forming an end-to-end code generation flow from requirements to code without the need for model fine-tuning. Based on a geospatial problem classification framework and evaluation benchmarks, the CoP strategy significantly improves the logical clarity, syntactical correctness, and executability of the generated code, with improvements ranging from 3.0% to 48.8%. Comparative and ablation experiments further validate the superiority of the CoP strategy over other optimization approaches and confirm the rationality and necessity of its key components. Through case studies on building data visualization and fire data analysis, this paper demonstrates the application and effectiveness of CoP in various geospatial scenarios. The CoP framework offers a systematic, step-by-step approach to LLM-based geospatial code generation tasks, significantly enhancing code generation performance in geospatial tasks and providing valuable insights for code generation in other vertical domains.         ",
    "url": "https://arxiv.org/abs/2411.10753",
    "authors": [
      "Shuyang Hou",
      "Haoyue Jiao",
      "Zhangxiao Shen",
      "Jianyuan Liang",
      "Anqi Zhao",
      "Xiaopu Zhang",
      "Jianxun Wang",
      "Huayi Wu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2411.10764",
    "title": "ML$^2$Tuner: Efficient Code Tuning via Multi-Level Machine Learning Models",
    "abstract": "           The increasing complexity of deep learning models necessitates specialized hardware and software optimizations, particularly for deep learning accelerators. Existing autotuning methods often suffer from prolonged tuning times due to profiling invalid configurations, which can cause runtime errors. We introduce ML$^2$Tuner, a multi-level machine learning tuning technique that enhances autotuning efficiency by incorporating a validity prediction model to filter out invalid configurations and an advanced performance prediction model utilizing hidden features from the compilation process. Experimental results on an extended VTA accelerator demonstrate that ML$^2$Tuner achieves equivalent performance improvements using only 12.3% of the samples required with a similar approach as TVM and reduces invalid profiling attempts by an average of 60.8%, Highlighting its potential to enhance autotuning performance by filtering out invalid configurations         ",
    "url": "https://arxiv.org/abs/2411.10764",
    "authors": [
      "JooHyoung Cha",
      "Munyoung Lee",
      "Jinse Kwon",
      "Jubin Lee",
      "Jemin Lee",
      "Yongin Kwon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.10794",
    "title": "Going Beyond Conventional OOD Detection",
    "abstract": "           Out-of-distribution (OOD) detection is critical to ensure the safe deployment of deep learning models in critical applications. Deep learning models can often misidentify OOD samples as in-distribution (ID) samples. This vulnerability worsens in the presence of spurious correlation in the training set. Likewise, in fine-grained classification settings, detection of fine-grained OOD samples becomes inherently challenging due to their high similarity to ID samples. However, current research on OOD detection has largely ignored these challenging scenarios, focusing instead on relatively easier (conventional) cases. In this work, we present a unified Approach to Spurious, fine-grained, and Conventional OOD Detection (ASCOOD). First, we propose synthesizing virtual outliers from ID data by approximating the destruction of invariant features. We identify invariant features with the pixel attribution method using the model being learned. This approach eliminates the burden of curating external OOD datasets. Then, we simultaneously incentivize ID classification and predictive uncertainty towards the virtual outliers leveraging standardized feature representation. Our approach effectively mitigates the impact of spurious correlations and encourages capturing fine-grained attributes. Extensive experiments across six datasets demonstrate the merit of ASCOOD in spurious, fine-grained, and conventional settings. The code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2411.10794",
    "authors": [
      "Sudarshan Regmi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.10814",
    "title": "DEAL: Decoupled Classifier with Adaptive Linear Modulation for Group Robust Early Diagnosis of MCI to AD Conversion",
    "abstract": "           While deep learning-based Alzheimer's disease (AD) diagnosis has recently made significant advancements, particularly in predicting the conversion of mild cognitive impairment (MCI) to AD based on MRI images, there remains a critical gap in research regarding the group robustness of the diagnosis. Although numerous studies pointed out that deep learning-based classifiers may exhibit poor performance in certain groups by relying on unimportant attributes, this issue has been largely overlooked in the early diagnosis of MCI to AD conversion. In this paper, we present the first comprehensive investigation of the group robustness in the early diagnosis of MCI to AD conversion using MRI images, focusing on disparities in accuracy between groups, specifically sMCI and pMCI individuals divided by age. Our experiments reveal that standard classifiers consistently underperform for certain groups across different architectures, highlighting the need for more tailored approaches. To address this, we propose a novel method, dubbed DEAL (DEcoupled classifier with Adaptive Linear modulation), comprising two key components: (1) a linear modulation of features from the penultimate layer, incorporating easily obtainable age and cognitive indicative tabular features, and (2) a decoupled classifier that provides more tailored decision boundaries for each group, further improving performance. Through extensive experiments and evaluations across different architectures, we demonstrate the efficacy of DEAL in improving the group robustness of the MCI to AD conversion prediction.         ",
    "url": "https://arxiv.org/abs/2411.10814",
    "authors": [
      "Donggyu Lee",
      "Juhyeon Park",
      "Taesup Moon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10815",
    "title": "Collaborative UAVs Multi-task Video Processing Optimization Based on Enhanced Distributed Actor-Critic Networks",
    "abstract": "           With the rapid advancement of the Internet of Things (IoT) and Artificial Intelligence (AI), intelligent information services are being increasingly integrated across various sectors, including healthcare, industry, and transportation. Traditional solutions rely on centralized cloud processing, which encounters considerable challenges in fulfilling the Quality of Service (QoS) requirements of Computer Vision (CV) tasks generated in the resource-constrained infrastructure-less environments. In this paper, we introduce a distributed framework called CoUAV-Pro for multi-task video processing powered by Unmanned Aerial Vehicles (UAVs). This framework empowers multiple UAVs to meet the service demands of various computer vision (CV) tasks in infrastructure-less environments, thereby eliminating the need for centralized processing. Specifically, we develop a novel task allocation algorithm that leverages enhanced distributed actor-critic networks within CoUAV-Pro, aiming to optimize task processing efficiency while contending with constraints associated with UAV's energy, computational, and communication resources. Comprehensive experiments demonstrate that our proposed solution achieves satisfactory performance levels against those of centralized methods across key metrics including task acquisition rates, task latency, and energy consumption.         ",
    "url": "https://arxiv.org/abs/2411.10815",
    "authors": [
      "Ziqi Rong",
      "Qiushi Zheng",
      "Zhishu Shen",
      "Xiaolong Li",
      "Tiehua Zhang",
      "Zheng Lei",
      "Jiong Jin"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2411.10819",
    "title": "An Oversampling-enhanced Multi-class Imbalanced Classification Framework for Patient Health Status Prediction Using Patient-reported Outcomes",
    "abstract": "           Patient-reported outcomes (PROs) directly collected from cancer patients being treated with radiation therapy play a vital role in assisting clinicians in counseling patients regarding likely toxicities. Precise prediction and evaluation of symptoms or health status associated with PROs are fundamental to enhancing decision-making and planning for the required services and support as patients transition into survivorship. However, the raw PRO data collected from hospitals exhibits some intrinsic challenges such as incomplete item reports and imbalance patient toxicities. To the end, in this study, we explore various machine learning techniques to predict patient outcomes related to health status such as pain levels and sleep discomfort using PRO datasets from a cancer photon/proton therapy center. Specifically, we deploy six advanced machine learning classifiers -- Random Forest (RF), XGBoost, Gradient Boosting (GB), Support Vector Machine (SVM), Multi-Layer Perceptron with Bagging (MLP-Bagging), and Logistic Regression (LR) -- to tackle a multi-class imbalance classification problem across three prevalent cancer types: head and neck, prostate, and breast cancers. To address the class imbalance issue, we employ an oversampling strategy, adjusting the training set sample sizes through interpolations of in-class neighboring samples, thereby augmenting minority classes without deviating from the original skewed class distribution. Our experimental findings across multiple PRO datasets indicate that the RF and XGB methods achieve robust generalization performance, evidenced by weighted AUC and detailed confusion matrices, in categorizing outcomes as mild, intermediate, and severe post-radiation therapy. These results underscore the models' effectiveness and potential utility in clinical settings.         ",
    "url": "https://arxiv.org/abs/2411.10819",
    "authors": [
      "Yang Yan",
      "Zhong Chen",
      "Cai Xu",
      "Xinglei Shen",
      "Jay Shiao",
      "John Einck",
      "Ronald C Chen",
      "Hao Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.10842",
    "title": "CODECLEANER: Elevating Standards with A Robust Data Contamination Mitigation Toolkit",
    "abstract": "           Data contamination presents a critical barrier preventing widespread industrial adoption of advanced software engineering techniques that leverage code language models (CLMs). This phenomenon occurs when evaluation data inadvertently overlaps with the public code repositories used to train CLMs, severely undermining the credibility of performance evaluations. For software companies considering the integration of CLM-based techniques into their development pipeline, this uncertainty about true performance metrics poses an unacceptable business risk. Code refactoring, which comprises code restructuring and variable renaming, has emerged as a promising measure to mitigate data contamination. It provides a practical alternative to the resource-intensive process of building contamination-free evaluation datasets, which would require companies to collect, clean, and label code created after the CLMs' training cutoff dates. However, the lack of automated code refactoring tools and scientifically validated refactoring techniques has hampered widespread industrial implementation. To bridge the gap, this paper presents the first systematic study to examine the efficacy of code refactoring operators at multiple scales (method-level, class-level, and cross-class level) and in different programming languages. In particular, we develop an open-sourced toolkit, CODECLEANER, which includes 11 operators for Python, with nine method-level, one class-level, and one cross-class-level operator. A drop of 65% overlap ratio is found when applying all operators in CODECLEANER, demonstrating their effectiveness in addressing data contamination. Additionally, we migrate four operators to Java, showing their generalizability to another language. We make CODECLEANER online available to facilitate further studies on mitigating CLM data contamination.         ",
    "url": "https://arxiv.org/abs/2411.10842",
    "authors": [
      "Jialun Cao",
      "Songqiang Chen",
      "Wuqi Zhang",
      "Hau Ching Lo",
      "Shing-Chi Cheung"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.10861",
    "title": "See-Saw Generative Mechanism for Scalable Recursive Code Generation with Generative AI",
    "abstract": "           The generation of complex, large-scale code projects using generative AI models presents challenges due to token limitations, dependency management, and iterative refinement requirements. This paper introduces the See-Saw generative mechanism, a novel methodology for dynamic and recursive code generation. The proposed approach alternates between main code updates and dependency generation to ensure alignment and functionality. By dynamically optimizing token usage and incorporating key elements of the main code into the generation of dependencies, the method enables efficient and scalable code generation for projects requiring hundreds of interdependent files. The mechanism ensures that all code components are synchronized and functional, enabling scalable and efficient project generation. Experimental validation demonstrates the method's capability to manage dependencies effectively while maintaining coherence and minimizing computational overhead.         ",
    "url": "https://arxiv.org/abs/2411.10861",
    "authors": [
      "Ruslan Idelfonso Maga\u00f1a Vsevolodovna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2411.10868",
    "title": "Destabilizing a Social Network Model via Intrinsic Feedback Vulnerabilities",
    "abstract": "           Social influence plays an important role in shaping individual opinions and actions, particularly in our digitally connected world. AI-generated, personalized content has led to serious and well-founded concerns, including United States Supreme Court Cases regarding the potential for the radicalization of individuals based on social influence. Motivated by these developments, we present a case study investigating the effects of small but intentional perturbations on the integrity of a simple social network. We employ Taylor's classic model of social influence and use tools from robust control theory (most notably the Dynamic Structure Function (DSF)), to identify precisely the perturbations that are sufficient to qualitatively alter the system's equilibrium and also minimal in norm. In particular, we examine two scenarios: perturbations to an existing link and perturbations taking the form of the addition of a new link to the network. In each case, we identify destabilizing perturbations and simulate their effects. Remarkably, we find that even small alterations to network structure may cause sentiments to grow in magnitude without bound, indicating the potential for large-scale shifts in collective behavior to be triggered by minor adjustments in social influence. Our findings emphasize the imperative need for further investigation into vulnerabilities in real-world social networks, where such dynamics may already exist.         ",
    "url": "https://arxiv.org/abs/2411.10868",
    "authors": [
      "Lane H. Rogers",
      "Emma J. Reid",
      "Robert A. Bridges"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Optimization and Control (math.OC)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2411.10883",
    "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems via syncfs",
    "abstract": "           Operating Systems enforce logical isolation using abstractions such as processes, containers, and isolation technologies to protect a system from malicious or buggy code. In this paper, we show new types of side channels through the file system that break this logical isolation. The file system plays a critical role in the operating system, managing all I/O activities between the application layer and the physical storage device. We observe that the file system implementation is shared, leading to timing leakage when using common I/O system calls. Specifically, we found that modern operating systems take advantage of any flush operation (which saves cached blocks in memory to the SSD or disk) to flush all of the I/O buffers, even those used by other isolation domains. Thus, by measuring the delay of syncfs, the attacker can infer the I/O behavior of victim programs. We then demonstrate a syncfs covert channel attack on multiple file systems, including both Linux native file systems and the Windows file system, achieving a maximum bandwidth of 5 Kbps with an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on Windows. In addition, we construct three side-channel attacks targeting both Linux and Android devices. On Linux devices, we implement a website fingerprinting attack and a video fingerprinting attack by tracking the write patterns of temporary buffering files. On Android devices, we design an application fingerprinting attack that leaks application write patterns during boot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally, we demonstrate that these attacks can be exploited across containers implementing a container detection technique and a cross-container covert channel attack.         ",
    "url": "https://arxiv.org/abs/2411.10883",
    "authors": [
      "Cheng Gu",
      "Yicheng Zhang",
      "Nael Abu-Ghazaleh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.10887",
    "title": "Practitioner Paper: Decoding Intellectual Property: Acoustic and Magnetic Side-channel Attack on a 3D Printer",
    "abstract": "           The widespread accessibility and ease of use of additive manufacturing (AM), widely recognized as 3D printing, has put Intellectual Property (IP) at great risk of theft. As 3D printers emit acoustic and magnetic signals while printing, the signals can be captured and analyzed using a smartphone for the purpose of IP attack. This is an instance of physical-to-cyber exploitation, as there is no direct contact with the 3D printer. Although cyber vulnerabilities in 3D printers are becoming more apparent, the methods for protecting IPs are yet to be fully investigated. The threat scenarios in previous works have mainly rested on advanced recording devices for data collection and entailed placing the device very close to the 3D printer. However, our work demonstrates the feasibility of reconstructing G-codes by performing side-channel attacks on a 3D printer using a smartphone from greater distances. By training models using Gradient Boosted Decision Trees, our prediction results for each axial movement, stepper, nozzle, and rotor speed achieve high accuracy, with a mean of 98.80%, without any intrusiveness. We effectively deploy the model in a real-world examination, achieving a Mean Tendency Error (MTE) of 4.47% on a plain G-code design.         ",
    "url": "https://arxiv.org/abs/2411.10887",
    "authors": [
      "Amirhossein Jamarani",
      "Yazhou Tu",
      "Xiali Hei"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.10891",
    "title": "ChannelDropBack: Forward-Consistent Stochastic Regularization for Deep Networks",
    "abstract": "           Incorporating stochasticity into the training process of deep convolutional networks is a widely used technique to reduce overfitting and improve regularization. Existing techniques often require modifying the architecture of the network by adding specialized layers, are effective only to specific network topologies or types of layers - linear or convolutional, and result in a trained model that is different from the deployed one. We present ChannelDropBack, a simple stochastic regularization approach that introduces randomness only into the backward information flow, leaving the forward pass intact. ChannelDropBack randomly selects a subset of channels within the network during the backpropagation step and applies weight updates only to them. As a consequence, it allows for seamless integration into the training process of any model and layers without the need to change its architecture, making it applicable to various network topologies, and the exact same network is deployed during training and inference. Experimental evaluations validate the effectiveness of our approach, demonstrating improved accuracy on popular datasets and models, including ImageNet and ViT. Code is available at \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2411.10891",
    "authors": [
      "Evgeny Hershkovitch Neiterman",
      "Gil Ben-Artzi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10894",
    "title": "Deep BI-RADS Network for Improved Cancer Detection from Mammograms",
    "abstract": "           While state-of-the-art models for breast cancer detection leverage multi-view mammograms for enhanced diagnostic accuracy, they often focus solely on visual mammography data. However, radiologists document valuable lesion descriptors that contain additional information that can enhance mammography-based breast cancer screening. A key question is whether deep learning models can benefit from these expert-derived features. To address this question, we introduce a novel multi-modal approach that combines textual BI-RADS lesion descriptors with visual mammogram content. Our method employs iterative attention layers to effectively fuse these different modalities, significantly improving classification performance over image-only models. Experiments on the CBIS-DDSM dataset demonstrate substantial improvements across all metrics, demonstrating the contribution of handcrafted features to end-to-end.         ",
    "url": "https://arxiv.org/abs/2411.10894",
    "authors": [
      "Gil Ben-Artzi",
      "Feras Daragma",
      "Shahar Mahpod"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10895",
    "title": "Evolution of IVR building techniques: from code writing to AI-powered automation",
    "abstract": "           Interactive Voice Response (IVR) systems have undergone significant transformation in recent years, moving from traditional code-based development to more user-friendly approaches leveraging widgets and, most recently, harnessing the power of Artificial Intelligence (AI) for automated IVR flow creation. This paper explores the evolution of IVR building techniques, highlighting the industry's revolution and shaping the future of IVR systems. The authors delve into the historical context, current trends, and future prospects of IVR development, elucidating the impact of AI on simplifying IVR creation processes and enhancing customer experiences.         ",
    "url": "https://arxiv.org/abs/2411.10895",
    "authors": [
      "Khushbu Mehboob Shaikh",
      "Georgios Giannakopoulos"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.10902",
    "title": "Attention-based U-Net Method for Autonomous Lane Detection",
    "abstract": "           Lane detection involves identifying lanes on the road and accurately determining their location and shape. This is a crucial technique for modern assisted and autonomous driving systems. However, several unique properties of lanes pose challenges for detection methods. The lack of distinctive features can cause lane detection algorithms to be confused by other objects with similar appearances. Additionally, the varying number of lanes and the diversity in lane line patterns, such as solid, broken, single, double, merging, and splitting lines, further complicate the task. To address these challenges, Deep Learning (DL) approaches can be employed in various ways. Merging DL models with an attention mechanism has recently surfaced as a new approach. In this context, two deep learning-based lane recognition methods are proposed in this study. The first method employs the Feature Pyramid Network (FPN) model, delivering an impressive 87.59% accuracy in detecting road lanes. The second method, which incorporates attention layers into the U-Net model, significantly boosts the performance of semantic segmentation tasks. The advanced model, achieving an extraordinary 98.98% accuracy and far surpassing the basic U-Net model, clearly showcases its superiority over existing methods in a comparative analysis. The groundbreaking findings of this research pave the way for the development of more effective and reliable road lane detection methods, significantly advancing the capabilities of modern assisted and autonomous driving systems.         ",
    "url": "https://arxiv.org/abs/2411.10902",
    "authors": [
      "Mohammadhamed Tangestanizadeh",
      "Mohammad Dehghani Tezerjani",
      "Saba Yousefian Jazi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10918",
    "title": "LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection",
    "abstract": "           Modern industrial infrastructures rely heavily on Cyber-Physical Systems (CPS), but these are vulnerable to cyber-attacks with potentially catastrophic effects. To reduce these risks, anomaly detection methods based on physical invariants have been developed. However, these methods often require domain-specific expertise to manually define invariants, making them costly and difficult to scale. To address this limitation, we propose a novel approach to extract physical invariants from CPS testbeds for anomaly detection. Our insight is that CPS design documentation often contains semantically rich descriptions of physical procedures, which can profile inter-correlated dynamics among system components. Leveraging the built-in physics and engineering knowledge of recent generative AI models, we aim to automate this traditionally manual process, improving scalability and reducing costs. This work focuses on designing and optimizing a Retrieval-Augmented-Generation (RAG) workflow with a customized prompting system tailored for CPS documentation, enabling accurate extraction of semantic information and inference of physical invariants from complex, multimodal content. Then, rather than directly applying the inferred invariants for anomaly detection, we introduce an innovative statistics-based learning approach that integrates these invariants into the training dataset. This method addresses limitations such as hallucination and concept drift, enhancing the reliability of the model. We evaluate our approach on real-world public CPS security dataset which contains 86 data points and 58 attacking cases. The results show that our approach achieves a high precision of 0.923, accurately detecting anomalies while minimizing false alarms.         ",
    "url": "https://arxiv.org/abs/2411.10918",
    "authors": [
      "Danial Abshari",
      "Chenglong Fu",
      "Meera Sridhar"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.10919",
    "title": "Multi-Modal Self-Supervised Learning for Surgical Feedback Effectiveness Assessment",
    "abstract": "           During surgical training, real-time feedback from trainers to trainees is important for preventing errors and enhancing long-term skill acquisition. Accurately predicting the effectiveness of this feedback, specifically whether it leads to a change in trainee behavior, is crucial for developing methods for improving surgical training and education. However, relying on human annotations to assess feedback effectiveness is laborious and prone to biases, underscoring the need for an automated, scalable, and objective method. Creating such an automated system poses challenges, as it requires an understanding of both the verbal feedback delivered by the trainer and the visual context of the real-time surgical scene. To address this, we propose a method that integrates information from transcribed verbal feedback and corresponding surgical video to predict feedback effectiveness. Our findings show that both transcribed feedback and surgical video are individually predictive of trainee behavior changes, and their combination achieves an AUROC of 0.70+/-0.02, improving prediction accuracy by up to 6.6%. Additionally, we introduce self-supervised fine-tuning as a strategy for enhancing surgical video representation learning, which is scalable and further enhances prediction performance. Our results demonstrate the potential of multi-modal learning to advance the automated assessment of surgical feedback.         ",
    "url": "https://arxiv.org/abs/2411.10919",
    "authors": [
      "Arushi Gupta",
      "Rafal Kocielnik",
      "Jiayun Wang",
      "Firdavs Nasriddinov",
      "Cherine Yang",
      "Elyssa Wong",
      "Anima Anandkumar",
      "Andrew Hung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10921",
    "title": "Distributed solar generation forecasting using attention-based deep neural networks for cloud movement prediction",
    "abstract": "           Accurate forecasts of distributed solar generation are necessary to reduce negative impacts resulting from the increased uptake of distributed solar photovoltaic (PV) systems. However, the high variability of solar generation over short time intervals (seconds to minutes) caused by cloud movement makes this forecasting task difficult. To address this, using cloud images, which capture the second-to-second changes in cloud cover affecting solar generation, has shown promise. Recently, deep neural networks with \"attention\" that focus on important regions of an image have been applied with success in many computer vision applications. However, their use for forecasting cloud movement has not yet been extensively explored. In this work, we propose an attention-based convolutional long short-term memory network to forecast cloud movement and apply an existing self-attention-based method previously proposed for video prediction to forecast cloud movement. We investigate and discuss the impact of cloud forecasts from attention-based methods towards forecasting distributed solar generation, compared to cloud forecasts from non-attention-based methods. We further provide insights into the different solar forecast performances that can be achieved for high and low altitude clouds. We find that for clouds at high altitudes, the cloud predictions obtained using attention-based methods result in solar forecast skill score improvements of 5.86% or more compared to non-attention-based methods.         ",
    "url": "https://arxiv.org/abs/2411.10921",
    "authors": [
      "Maneesha Perera",
      "Julian De Hoog",
      "Kasun Bandara",
      "Saman Halgamuge"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10922",
    "title": "Exploiting VLM Localizability and Semantics for Open Vocabulary Action Detection",
    "abstract": "           Action detection aims to detect (recognize and localize) human actions spatially and temporally in videos. Existing approaches focus on the closed-set setting where an action detector is trained and tested on videos from a fixed set of action categories. However, this constrained setting is not viable in an open world where test videos inevitably come beyond the trained action categories. In this paper, we address the practical yet challenging Open-Vocabulary Action Detection (OVAD) problem. It aims to detect any action in test videos while training a model on a fixed set of action categories. To achieve such an open-vocabulary capability, we propose a novel method OpenMixer that exploits the inherent semantics and localizability of large vision-language models (VLM) within the family of query-based detection transformers (DETR). Specifically, the OpenMixer is developed by spatial and temporal OpenMixer blocks (S-OMB and T-OMB), and a dynamically fused alignment (DFA) module. The three components collectively enjoy the merits of strong generalization from pre-trained VLMs and end-to-end learning from DETR design. Moreover, we established OVAD benchmarks under various settings, and the experimental results show that the OpenMixer performs the best over baselines for detecting seen and unseen actions. We release the codes, models, and dataset splits at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.10922",
    "authors": [
      "Wentao Bao",
      "Kai Li",
      "Yuxiao Chen",
      "Deep Patel",
      "Martin Renqiang Min",
      "Yu Kong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10925",
    "title": "A Resilience Perspective on C-V2X Communication Networks under Imperfect CSI",
    "abstract": "           Cellular vehicle-to-everything (C-V2X) networks provide a promising solution to improve road safety and traffic efficiency. One key challenge in such systems lies in meeting different quality-of-service (QoS) requirements of coexisting vehicular communication links, particularly under imperfect channel state information (CSI) conditions caused by the highly dynamic environment. In this paper, a novel analytical framework for examining the resilience of C-V2X networks in face of imperfect CSI is proposed. In this framework, the adaptation phase of the C-V2X network is studied, in which an adaptation power scheme is employed and the probability distribution function (PDF) of the imperfect CSI is estimated. Then, the resilience of C-V2X networks is studied through two principal dimensions: remediation capability and adaptation performance, both of which are defined, quantified, and analyzed for the first time. Particularly, an upper bound on the estimation's mean square error (MSE) is explicitly derived to capture the C-V2X's remediation capability, and a novel metric named hazard rate (HR) is exploited to evaluate the C-V2X's adaptation performance. Afterwards, the impact of the adaptation power scheme on the C-V2X's resilience is examined, revealing a tradeoff between the C-V2X's remediation capability and adaptation performance. Simulation results validate the framework's superiority in capturing the interplay between adaptation and remediation, as well as the effectiveness of the two proposed metrics in guiding the design of the adaptation power scheme to enhance the system's resilience.         ",
    "url": "https://arxiv.org/abs/2411.10925",
    "authors": [
      "Tingyu Shui",
      "Walid Saad",
      "Mingzhe Chen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2411.10939",
    "title": "Evaluating Generative AI Systems is a Social Science Measurement Challenge",
    "abstract": "           Across academia, industry, and government, there is an increasing awareness that the measurement tasks involved in evaluating generative AI (GenAI) systems are especially difficult. We argue that these measurement tasks are highly reminiscent of measurement tasks found throughout the social sciences. With this in mind, we present a framework, grounded in measurement theory from the social sciences, for measuring concepts related to the capabilities, impacts, opportunities, and risks of GenAI systems. The framework distinguishes between four levels: the background concept, the systematized concept, the measurement instrument(s), and the instance-level measurements themselves. This four-level approach differs from the way measurement is typically done in ML, where researchers and practitioners appear to jump straight from background concepts to measurement instruments, with little to no explicit systematization in between. As well as surfacing assumptions, thereby making it easier to understand exactly what the resulting measurements do and do not mean, this framework has two important implications for evaluating evaluations: First, it can enable stakeholders from different worlds to participate in conceptual debates, broadening the expertise involved in evaluating GenAI systems. Second, it brings rigor to operational debates by offering a set of lenses for interrogating the validity of measurement instruments and their resulting measurements.         ",
    "url": "https://arxiv.org/abs/2411.10939",
    "authors": [
      "Hanna Wallach",
      "Meera Desai",
      "Nicholas Pangakis",
      "A. Feder Cooper",
      "Angelina Wang",
      "Solon Barocas",
      "Alexandra Chouldechova",
      "Chad Atalla",
      "Su Lin Blodgett",
      "Emily Corvi",
      "P. Alex Dow",
      "Jean Garcia-Gathright",
      "Alexandra Olteanu",
      "Stefanie Reed",
      "Emily Sheng",
      "Dan Vann",
      "Jennifer Wortman Vaughan",
      "Matthew Vogel",
      "Hannah Washington",
      "Abigail Z. Jacobs"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2411.10945",
    "title": "Anomaly Detection for People with Visual Impairments Using an Egocentric 360-Degree Camera",
    "abstract": "           Recent advancements in computer vision have led to a renewed interest in developing assistive technologies for individuals with visual impairments. Although extensive research has been conducted in the field of computer vision-based assistive technologies, most of the focus has been on understanding contexts in images, rather than addressing their physical safety and security concerns. To address this challenge, we propose the first step towards detecting anomalous situations for visually impaired people by observing their entire surroundings using an egocentric 360-degree camera. We first introduce a novel egocentric 360-degree video dataset called VIEW360 (Visually Impaired Equipped with Wearable 360-degree camera), which contains abnormal activities that visually impaired individuals may encounter, such as shoulder surfing and pickpocketing. Furthermore, we propose a new architecture called the FDPN (Frame and Direction Prediction Network), which facilitates frame-level prediction of abnormal events and identifying of their directions. Finally, we evaluate our approach on our VIEW360 dataset and the publicly available UCF-Crime and Shanghaitech datasets, demonstrating state-of-the-art performance.         ",
    "url": "https://arxiv.org/abs/2411.10945",
    "authors": [
      "Inpyo Song",
      "Sanghyeon Lee",
      "Minjun Joo",
      "Jangwon Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10951",
    "title": "TSFormer: A Robust Framework for Efficient UHD Image Restoration",
    "abstract": "           Ultra-high-definition (UHD) image restoration is vital for applications demanding exceptional visual fidelity, yet existing methods often face a trade-off between restoration quality and efficiency, limiting their practical deployment. In this paper, we propose TSFormer, an all-in-one framework that integrates \\textbf{T}rusted learning with \\textbf{S}parsification to boost both generalization capability and computational efficiency in UHD image restoration. The key is that only a small amount of token movement is allowed within the model. To efficiently filter tokens, we use Min-$p$ with random matrix theory to quantify the uncertainty of tokens, thereby improving the robustness of the model. Our model can run a 4K image in real time (40fps) with 3.38 M parameters. Extensive experiments demonstrate that TSFormer achieves state-of-the-art restoration quality while enhancing generalization and reducing computational demands. In addition, our token filtering method can be applied to other image restoration models to effectively accelerate inference and maintain performance.         ",
    "url": "https://arxiv.org/abs/2411.10951",
    "authors": [
      "Xin Su",
      "Chen Wu",
      "Zhuoran Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10957",
    "title": "IMPaCT GNN: Imposing invariance with Message Passing in Chronological split Temporal Graphs",
    "abstract": "           This paper addresses domain adaptation challenges in graph data resulting from chronological splits. In a transductive graph learning setting, where each node is associated with a timestamp, we focus on the task of Semi-Supervised Node Classification (SSNC), aiming to classify recent nodes using labels of past nodes. Temporal dependencies in node connections create domain shifts, causing significant performance degradation when applying models trained on historical data into recent data. Given the practical relevance of this scenario, addressing domain adaptation in chronological split data is crucial, yet underexplored. We propose Imposing invariance with Message Passing in Chronological split Temporal Graphs (IMPaCT), a method that imposes invariant properties based on realistic assumptions derived from temporal graph structures. Unlike traditional domain adaptation approaches which rely on unverifiable assumptions, IMPaCT explicitly accounts for the characteristics of chronological splits. The IMPaCT is further supported by rigorous mathematical analysis, including a derivation of an upper bound of the generalization error. Experimentally, IMPaCT achieves a 3.8% performance improvement over current SOTA method on the ogbn-mag graph dataset. Additionally, we introduce the Temporal Stochastic Block Model (TSBM), which replicates temporal graphs under varying conditions, demonstrating the applicability of our methods to general spatial GNNs.         ",
    "url": "https://arxiv.org/abs/2411.10957",
    "authors": [
      "Sejun Park",
      "Joo Young Park",
      "Hyunwoo Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.10960",
    "title": "Beamforming Design and Multi-User Scheduling in Transmissive RIS Enabled Distributed Cooperative ISAC Networks with RSMA",
    "abstract": "           In this paper, we propose a novel transmissive reconfigurable intelligent surface (TRIS) transceiver-empowered distributed cooperative integrated sensing and communication (ISAC) network to enhance coverage as well as to enhance wireless environment understanding. Based on the network requirements, the users are categorized into cooperative users (CUEs) and destination users (DUEs), and the CUEs utilize their own resources to serve the DUEs. To realize cooperation, we implement rate-splitting multiple access (RSMA) at the base station (BS), where the common stream is decoded and reencoded at the CUEs and forwarded to the DUEs, while the private stream satisfies the CUEs' own communication requirements. We construct an optimization problem with maximum minimum radar mutual information (RMI) as the objective function to optimize the BS beamforming matrix, the CUE beamforming matrices, the common stream rate vectors, and the user scheduling vectors. Due to the coupling of the optimization variables and non-convex operation, the proposed problem is a non-convex optimization problem that cannot be solved directly. To address the above challenges, we adopt a consensus alternating direction method of multipliers (ADMM) framework to decouple the optimization variables and solve it. Specifically, the problem is decoupled into multiple subproblems and solved by iterative optimization independently until overall convergence is achieved. Finally, numerical results validate the superiority of the proposed scheme in terms of improving communication sum-rate and RMI, and greatly reduce the algorithm complexity.         ",
    "url": "https://arxiv.org/abs/2411.10960",
    "authors": [
      "Ziwei Liu",
      "Wen Chen",
      "Qingqing Wu",
      "Zhendong Li",
      "Qiong Wu",
      "Nan Cheng",
      "Jun Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2411.10961",
    "title": "Map-Free Trajectory Prediction with Map Distillation and Hierarchical Encoding",
    "abstract": "           Reliable motion forecasting of surrounding agents is essential for ensuring the safe operation of autonomous vehicles. Many existing trajectory prediction methods rely heavily on high-definition (HD) maps as strong driving priors. However, the availability and accuracy of these priors are not guaranteed due to substantial costs to build, localization errors of vehicles, or ongoing road constructions. In this paper, we introduce MFTP, a Map-Free Trajectory Prediction method that offers several advantages. First, it eliminates the need for HD maps during inference while still benefiting from map priors during training via knowledge distillation. Second, we present a novel hierarchical encoder that effectively extracts spatial-temporal agent features and aggregates them into multiple trajectory queries. Additionally, we introduce an iterative decoder that sequentially decodes trajectory queries to generate the final predictions. Extensive experiments show that our approach achieves state-of-the-art performance on the Argoverse dataset under the map-free setting.         ",
    "url": "https://arxiv.org/abs/2411.10961",
    "authors": [
      "Xiaodong Liu",
      "Yucheng Xing",
      "Xin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10964",
    "title": "Exploring Device-Oriented Video Encryption for Hierarchical Privacy Protection in AR Content Sharing",
    "abstract": "           Content sharing across multiple Augmented Reality (AR) displays is becoming commonplace, enhancing team communication and collaboration through devices like smartphones and AR glasses. However, this practice raises significant privacy concerns, especially concerning the physical environment visible in AR, which may include sensitive personal details like facial features and identifiable information. Our research focuses on protecting privacy within AR environments, particularly the physical backgrounds visible during content sharing across three common AR display methods: projection, smartphone, and AR glasses. We analyze the potential privacy risks associated with each method and employ a Region Of Interest (ROI) video encryption system to hierarchically encrypt the physical backdrop based on its safety rating. This study pioneers the integration of ROI video encryption at the bitstream level within AR contexts, providing a more efficient solution than traditional pixel-level encryption by enhancing encryption speed and reducing the required space. Our adaptive system dynamically adjusts the encryption intensity based on the AR display method, ensuring tailored privacy protection.         ",
    "url": "https://arxiv.org/abs/2411.10964",
    "authors": [
      "Yongquan Hu",
      "Dongsheng Zheng",
      "Kexin Nie",
      "Junyan Zhang",
      "Wen Hu",
      "Aaron Quigley"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2411.11003",
    "title": "TeG: Temporal-Granularity Method for Anomaly Detection with Attention in Smart City Surveillance",
    "abstract": "           Anomaly detection in video surveillance has recently gained interest from the research community. Temporal duration of anomalies vary within video streams, leading to complications in learning the temporal dynamics of specific events. This paper presents a temporal-granularity method for an anomaly detection model (TeG) in real-world surveillance, combining spatio-temporal features at different time-scales. The TeG model employs multi-head cross-attention blocks and multi-head self-attention blocks for this purpose. Additionally, we extend the UCF-Crime dataset with new anomaly types relevant to Smart City research project. The TeG model is deployed and validated in a city surveillance system, achieving successful real-time results in industrial settings.         ",
    "url": "https://arxiv.org/abs/2411.11003",
    "authors": [
      "Erkut Akdag",
      "Egor Bondarev",
      "Peter H. N. De With"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11006",
    "title": "BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for Backdoor Defense Evaluation",
    "abstract": "           We introduce BackdoorMBTI, the first backdoor learning toolkit and benchmark designed for multimodal evaluation across three representative modalities from eleven commonly used datasets. BackdoorMBTI provides a systematic backdoor learning pipeline, encompassing data processing, data poisoning, backdoor training, and evaluation. The generated poison datasets and backdoor models enable detailed evaluation of backdoor defense methods. Given the diversity of modalities, BackdoorMBTI facilitates systematic evaluation across different data types. Furthermore, BackdoorMBTI offers a standardized approach to handling practical factors in backdoor learning, such as issues related to data quality and erroneous labels. We anticipate that BackdoorMBTI will expedite future research in backdoor defense methods within a multimodal context. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.11006",
    "authors": [
      "Haiyang Yu",
      "Tian Xie",
      "Jiaping Gui",
      "Pengyang Wang",
      "Ping Yi",
      "Yue Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11011",
    "title": "CCi-YOLOv8n: Enhanced Fire Detection with CARAFE and Context-Guided Modules",
    "abstract": "           Fire incidents in urban and forested areas pose serious threats,underscoring the need for more effective detection technologies. To address these challenges, we present CCi-YOLOv8n, an enhanced YOLOv8 model with targeted improvements for detecting small fires and smoke. The model integrates the CARAFE up-sampling operator and a context-guided module to reduce information loss during up-sampling and down-sampling, thereby retaining richer feature representations. Additionally, an inverted residual mobile block enhanced C2f module captures small targets and fine smoke patterns, a critical improvement over the original model's detection this http URL validation, we introduce Web-Fire, a dataset curated for fire and smoke detection across diverse real-world scenarios. Experimental results indicate that CCi-YOLOv8n outperforms YOLOv8n in detection precision, confirming its effectiveness for robust fire detection tasks.         ",
    "url": "https://arxiv.org/abs/2411.11011",
    "authors": [
      "Kunwei Lv"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11020",
    "title": "Training a Label-Noise-Resistant GNN with Reduced Complexity",
    "abstract": "           Graph Neural Networks (GNNs) have been widely employed for semi-supervised node classification tasks on graphs. However, the performance of GNNs is significantly affected by label noise, that is, a small amount of incorrectly labeled nodes can substantially misguide model training. Mainstream solutions define node classification with label noise (NCLN) as a reliable labeling task, often introducing node similarity with quadratic computational complexity to more accurately assess label reliability. To this end, in this paper, we introduce the Label Ensemble Graph Neural Network (LEGNN), a lower complexity method for robust GNNs training against label noise. LEGNN reframes NCLN as a label ensemble task, gathering informative multiple labels instead of constructing a single reliable label, avoiding high-complexity computations for reliability assessment. Specifically, LEGNN conducts a two-step process: bootstrapping neighboring contexts and robust learning with gathered multiple labels. In the former step, we apply random neighbor masks for each node and gather the predicted labels as a high-probability label set. This mitigates the impact of inaccurately labeled neighbors and diversifies the label set. In the latter step, we utilize a partial label learning based strategy to aggregate the high-probability label information for model training. Additionally, we symmetrically gather a low-probability label set to counteract potential noise from the bootstrapped high-probability label set. Extensive experiments on six datasets demonstrate that LEGNN achieves outstanding performance while ensuring efficiency. Moreover, it exhibits good scalability on dataset with over one hundred thousand nodes and one million edges.         ",
    "url": "https://arxiv.org/abs/2411.11020",
    "authors": [
      "Rui Zhao",
      "Bin Shi",
      "Zhiming Liang",
      "Jianfei Ruan",
      "Bo Dong",
      "Lu Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2411.11029",
    "title": "Wafer Map Defect Classification Using Autoencoder-Based Data Augmentation and Convolutional Neural Network",
    "abstract": "           In semiconductor manufacturing, wafer defect maps (WDMs) play a crucial role in diagnosing issues and enhancing process yields by revealing critical defect patterns. However, accurately categorizing WDM defects presents significant challenges due to noisy data, unbalanced defect classes, and the complexity of failure modes. To address these challenges, this study proposes a novel method combining a self-encoder-based data augmentation technique with a convolutional neural network (CNN). By introducing noise into the latent space, the self-encoder enhances data diversity and mitigates class imbalance, thereby improving the model's generalization capabilities. The augmented dataset is subsequently used to train the CNN, enabling it to deliver precise classification of both common and rare defect patterns. Experimental results on the WM-811K dataset demonstrate that the proposed method achieves a classification accuracy of 98.56%, surpassing Random Forest, SVM, and Logistic Regression by 19%, 21%, and 27%, respectively. These findings highlight the robustness and effectiveness of the proposed approach, offering a reliable solution for wafer defect detection and classification.         ",
    "url": "https://arxiv.org/abs/2411.11029",
    "authors": [
      "Yin-Yin Bao",
      "Er-Chao Li",
      "Hong-Qiang Yang",
      "Bin-Bin Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2411.11033",
    "title": "REACCEPT: Automated Co-evolution of Production and Test Code Based on Dynamic Validation and Large Language Models",
    "abstract": "           Synchronizing production and test code, known as PT co-evolution, is critical for software quality in the software development lifecycle. Existing methods for automatic PT co-evolution either utilize predefined heuristic rules or rely on simple application of machine learning techniques. Due to the limitations of underlying techniques, existing methods either only partially automate PT co-evolution (e.g., only automate obsolete test code identification) or result in low accuracy. In this paper, we propose REACCEPT, a novel approach that leverages large language models and dynamic validation to fully automate PT co-evolution (i.e., capable of both identifying and updating obsolete test cases). REACCEPT relies on experience-based prompt template generation, dynamic validation, and retrieval-augmented generation techniques to accomplish automated PT co-evolution. To evaluate REACCEPT's effectiveness, we extensive experiments with a dataset of 537 Java projects and compared REACCEPT's performance with several state-of-the-art methods. Results show that REACCEPT achieved an update accuracy of 60.16% on correctly identified obsolete test code, surpassing the state-of-the-art technique CEPROT by 90%. This confirms that REACCEPT can effectively assist developers in maintaining test code, improving overall software quality and reducing maintenance effort.         ",
    "url": "https://arxiv.org/abs/2411.11033",
    "authors": [
      "Jianlei Chi",
      "Xiaotian Wang",
      "Yuhan Huang",
      "Lechen Yu",
      "Di Cui",
      "Jianguo Sun",
      "Jun Sun"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2411.11034",
    "title": "Digital Twin for Advanced Network Planning: Tackling Interference",
    "abstract": "           Operational data in next-generation networks offers a valuable resource for Mobile Network Operators to autonomously manage their systems and predict potential network issues. Machine Learning and Digital Twin can be applied to gain important insights for intelligent decision-making. This paper proposes a framework for Radio Frequency planning and failure detection using Digital Twin reducing the level of manual intervention. In this study, we propose a methodology for analyzing Radio Frequency issues as external interference employing clustering techniques in operational networks, and later incorporating this in the planning process. Simulation results demonstrate that the architecture proposed can improve planning operations through a data-aided anomaly detection strategy.         ",
    "url": "https://arxiv.org/abs/2411.11034",
    "authors": [
      "Juan Carlos Estrada-Jimenez",
      "Valdemar Ramon Farre-Guijarro",
      "Diana Carolina Alvarez-Paredes",
      "Marie-Laure Watrinet"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2411.11044",
    "title": "Efficient Federated Unlearning with Adaptive Differential Privacy Preservation",
    "abstract": "           Federated unlearning (FU) offers a promising solution to effectively address the need to erase the impact of specific clients' data on the global model in federated learning (FL), thereby granting individuals the ``Right to be Forgotten\". The most straightforward approach to achieve unlearning is to train the model from scratch, excluding clients who request data removal, but it is resource-intensive. Current state-of-the-art FU methods extend traditional FL frameworks by leveraging stored historical updates, enabling more efficient unlearning than training from scratch. However, the use of stored updates introduces significant privacy risks. Adversaries with access to these updates can potentially reconstruct clients' local data, a well-known vulnerability in the privacy domain. While privacy-enhanced techniques exist, their applications to FU scenarios that balance unlearning efficiency with privacy protection remain underexplored. To address this gap, we propose FedADP, a method designed to achieve both efficiency and privacy preservation in FU. Our approach incorporates an adaptive differential privacy (DP) mechanism, carefully balancing privacy and unlearning performance through a novel budget allocation strategy tailored for FU. FedADP also employs a dual-layered selection process, focusing on global models with significant changes and client updates closely aligned with the global model, reducing storage and communication costs. Additionally, a novel calibration method is introduced to facilitate effective unlearning. Extensive experimental results demonstrate that FedADP effectively manages the trade-off between unlearning efficiency and privacy protection.         ",
    "url": "https://arxiv.org/abs/2411.11044",
    "authors": [
      "Yu Jiang",
      "Xindi Tong",
      "Ziyao Liu",
      "Huanyi Ye",
      "Chee Wei Tan",
      "Kwok-Yan Lam"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11048",
    "title": "Generating medical screening questionnaires through analysis of social media data",
    "abstract": "           Screening questionnaires are used in medicine as a diagnostic aid. Creating them is a long and expensive process, which could potentially be improved through analysis of social media posts related to symptoms and behaviors prior to diagnosis. Here we show a preliminary investigation into the feasibility of generating screening questionnaires for a given medical condition from social media postings. The method first identifies a cohort of relevant users through their posts in dedicated patient groups and a control group of users who reported similar symptoms but did not report being diagnosed with the condition of interest. Posts made prior to diagnosis are used to generate decision rules to differentiate between the different groups, by clustering symptoms mentioned by these users and training a decision tree to differentiate between the two groups. We validate the generated rules by correlating them with scores given by medical doctors to matching hypothetical cases. We demonstrate the proposed method by creating questionnaires for three conditions (endometriosis, lupus, and gout) using the data of several hundreds of users from Reddit. These questionnaires were then validated by medical doctors. The average Pearson's correlation between the latter's scores and the decision rules were 0.58 (endometriosis), 0.40 (lupus) and 0.27 (gout). Our results suggest that the process of questionnaire generation can be, at least partly, automated. These questionnaires are advantageous in that they are based on real-world experience but are currently lacking in their ability to capture the context, duration, and timing of symptoms.         ",
    "url": "https://arxiv.org/abs/2411.11048",
    "authors": [
      "Ortal Ashkenazi",
      "Elad Yom-Tov",
      "Liron Vardi David"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2411.11053",
    "title": "SRA-MCTS: Self-driven Reasoning Aurmentation with Monte Carlo Tree Search for Enhanced Code Generation",
    "abstract": "           Large language models demonstrate exceptional performance in simple code generation tasks but still face challenges in tackling complex problems. These challenges may stem from insufficient reasoning and problem decomposition capabilities. To address this issue, we propose a reasoning-augmented data generation process, SRA-MCTS, which guides the model to autonomously generate high-quality intermediate reasoning paths. This creates a positive feedback loop, enabling continuous improvement. Our method operates entirely through the model itself without requiring additional supervision. By synthesizing natural language reasoning paths and translating them into executable code, the approach ensures analytical accuracy and enhances the success rate in solving complex tasks. Experimental results show that, even without additional supervisory signals, our method achieves performance improvements across different model scales, demonstrating the significant potential of self-improvement in small models. Furthermore, the method remains robust when traditional Chain-of-Thought (CoT) approaches exhibit performance degradation, with notable improvements observed in diversity metrics such as pass@10. We encourage further exploration of reasoning processes within training data to enhance the ability of language models to address complex problems.         ",
    "url": "https://arxiv.org/abs/2411.11053",
    "authors": [
      "Bin Xu",
      "Yiguan Lin",
      "Yinghao Li",
      "YangGao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11074",
    "title": "Spectral Subspace Clustering for Attributed Graphs",
    "abstract": "           Subspace clustering seeks to identify subspaces that segment a set of n data points into k (k<<n) groups, which has emerged as a powerful tool for analyzing data from various domains, especially images and videos. Recently, several studies have demonstrated the great potential of subspace clustering models for partitioning vertices in attributed graphs, referred to as SCAG. However, these works either demand significant computational overhead for constructing the nxn self-expressive matrix, or fail to incorporate graph topology and attribute data into the subspace clustering framework effectively, and thus, compromise result quality. Motivated by this, this paper presents two effective and efficient algorithms, S2CAG and M-S2CAG, for SCAG computation. Particularly, S2CAG obtains superb performance through three major contributions. First, we formulate a new objective function for SCAG with a refined representation model for vertices and two non-trivial constraints. On top of that, an efficient linear-time optimization solver is developed based on our theoretically grounded problem transformation and well-thought-out adaptive strategy. We then conduct an in-depth analysis to disclose the theoretical connection of S2CAG to conductance minimization, which further inspires the design of M-S2CAG that maximizes the modularity. Our extensive experiments, comparing S2CAG and M-S2CAG against 17 competitors over 8 benchmark datasets, exhibit that our solutions outperform all baselines in terms of clustering quality measured against the ground truth while delivering high efficiency         ",
    "url": "https://arxiv.org/abs/2411.11074",
    "authors": [
      "Xiaoyang Lin",
      "Renchi Yang",
      "Haoran Zheng",
      "Xiangyu Ke"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11079",
    "title": "Electrostatic Force Regularization for Neural Structured Pruning",
    "abstract": "           The demand for deploying deep convolutional neural networks (DCNNs) on resource-constrained devices for real-time applications remains substantial. However, existing state-of-the-art structured pruning methods often involve intricate implementations, require modifications to the original network architectures, and necessitate an extensive fine-tuning phase. To overcome these challenges, we propose a novel method that, for the first time, incorporates the concepts of charge and electrostatic force from physics into the training process of DCNNs. The magnitude of this force is directly proportional to the product of the charges of the convolution filter and the source filter, and inversely proportional to the square of the distance between them. We applied this electrostatic-like force to the convolution filters, either attracting filters with opposite charges toward non-zero weights or repelling filters with like charges toward zero weights. Consequently, filters subject to repulsive forces have their weights reduced to zero, enabling their removal, while the attractive forces preserve filters with significant weights that retain information. Unlike conventional methods, our approach is straightforward to implement, does not require any architectural modifications, and simultaneously optimizes weights and ranks filter importance, all without the need for extensive fine-tuning. We validated the efficacy of our method on modern DCNN architectures using the MNIST, CIFAR, and ImageNet datasets, achieving competitive performance compared to existing structured pruning approaches.         ",
    "url": "https://arxiv.org/abs/2411.11079",
    "authors": [
      "Abdesselam Ferdi",
      "Abdelmalik Taleb-Ahmed",
      "Amir Nakib",
      "Youcef Ferdi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11081",
    "title": "The Promises and Pitfalls of LLM Annotations in Dataset Labeling: a Case Study on Media Bias Detection",
    "abstract": "           High annotation costs from hiring or crowdsourcing complicate the creation of large, high-quality datasets needed for training reliable text classifiers. Recent research suggests using Large Language Models (LLMs) to automate the annotation process, reducing these costs while maintaining data quality. LLMs have shown promising results in annotating downstream tasks like hate speech detection and political framing. Building on the success in these areas, this study investigates whether LLMs are viable for annotating the complex task of media bias detection and whether a downstream media bias classifier can be trained on such data. We create annolexical, the first large-scale dataset for media bias classification with over 48000 synthetically annotated examples. Our classifier, fine-tuned on this dataset, surpasses all of the annotator LLMs by 5-9 percent in Matthews Correlation Coefficient (MCC) and performs close to or outperforms the model trained on human-labeled data when evaluated on two media bias benchmark datasets (BABE and BASIL). This study demonstrates how our approach significantly reduces the cost of dataset creation in the media bias domain and, by extension, the development of classifiers, while our subsequent behavioral stress-testing reveals some of its current limitations and trade-offs.         ",
    "url": "https://arxiv.org/abs/2411.11081",
    "authors": [
      "Tomas Horych",
      "Christoph Mandl",
      "Terry Ruas",
      "Andre Greiner-Petter",
      "Bela Gipp",
      "Akiko Aizawa",
      "Timo Spinde"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2411.11082",
    "title": "STOP: Spatiotemporal Orthogonal Propagation for Weight-Threshold-Leakage Synergistic Training of Deep Spiking Neural Networks",
    "abstract": "           The prevailing of artificial intelligence-of-things calls for higher energy-efficient edge computing paradigms, such as neuromorphic agents leveraging brain-inspired spiking neural network (SNN) models based on spatiotemporally sparse binary activations. However, the lack of efficient and high-accuracy deep SNN learning algorithms prevents them from practical edge deployments with a strictly bounded cost. In this paper, we propose a spatiotemporal orthogonal propagation (STOP) algorithm to tack this challenge. Our algorithm enables fully synergistic learning of synaptic weights as well as firing thresholds and leakage factors in spiking neurons to improve SNN accuracy, while under a unified temporally-forward trace-based framework to mitigate the huge memory requirement for storing neural states of all time-steps in the forward pass. Characteristically, the spatially-backward neuronal errors and temporally-forward traces propagate orthogonally to and independently of each other, substantially reducing computational overhead. Our STOP algorithm obtained high recognition accuracies of 99.53%, 94.84%, 74.92%, 98.26% and 77.10% on the MNIST, CIFAR-10, CIFAR-100, DVS-Gesture and DVS-CIFAR10 datasets with adequate SNNs of intermediate scales from LeNet-5 to ResNet-18. Compared with other deep SNN training works, our method is more plausible for edge intelligent scenarios where resources are limited but high-accuracy in-situ learning is desired.         ",
    "url": "https://arxiv.org/abs/2411.11082",
    "authors": [
      "Haoran Gao",
      "Xichuan Zhou",
      "Yingcheng Lin",
      "Min Tian",
      "Liyuan Liu",
      "Cong Shi"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11087",
    "title": "D-Cube: Exploiting Hyper-Features of Diffusion Model for Robust Medical Classification",
    "abstract": "           The integration of deep learning technologies in medical imaging aims to enhance the efficiency and accuracy of cancer diagnosis, particularly for pancreatic and breast cancers, which present significant diagnostic challenges due to their high mortality rates and complex imaging characteristics. This paper introduces Diffusion-Driven Diagnosis (D-Cube), a novel approach that leverages hyper-features from a diffusion model combined with contrastive learning to improve cancer diagnosis. D-Cube employs advanced feature selection techniques that utilize the robust representational capabilities of diffusion models, enhancing classification performance on medical datasets under challenging conditions such as data imbalance and limited sample availability. The feature selection process optimizes the extraction of clinically relevant features, significantly improving classification accuracy and demonstrating resilience in imbalanced and limited data scenarios. Experimental results validate the effectiveness of D-Cube across multiple medical imaging modalities, including CT, MRI, and X-ray, showing superior performance compared to existing baseline models. D-Cube represents a new strategy in cancer detection, employing advanced deep learning techniques to achieve state-of-the-art diagnostic accuracy and efficiency.         ",
    "url": "https://arxiv.org/abs/2411.11087",
    "authors": [
      "Minhee Jang",
      "Juheon Son",
      "Thanaporn Viriyasaranon",
      "Junho Kim",
      "Jang-Hwan Choi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11090",
    "title": "ForPKG-1.0: A Framework for Constructing Forestry Policy Knowledge Graph and Application Analysis",
    "abstract": "           A policy knowledge graph can provide decision support for tasks such as project compliance, policy analysis, and intelligent question answering, and can also serve as an external knowledge base to assist the reasoning process of related large language models. Although there have been many related works on knowledge graphs, there is currently a lack of research on the construction methods of policy knowledge graphs. This paper, focusing on the forestry field, designs a complete policy knowledge graph construction framework, including: firstly, proposing a fine-grained forestry policy domain ontology; then, proposing an unsupervised policy information extraction method, and finally, constructing a complete forestry policy knowledge graph. The experimental results show that the proposed ontology has good expressiveness and extensibility, and the policy information extraction method proposed in this paper achieves better results than other unsupervised methods. Furthermore, by analyzing the application of the knowledge graph in the retrieval-augmented-generation task of the large language models, the practical application value of the knowledge graph in the era of large language models is confirmed. The knowledge graph resource will be released on an open-source platform and can serve as the basic knowledge base for forestry policy-related intelligent systems. It can also be used for academic research. In addition, this study can provide reference and guidance for the construction of policy knowledge graphs in other fields.         ",
    "url": "https://arxiv.org/abs/2411.11090",
    "authors": [
      "Jingyun Sun",
      "Zhongze Luo"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2411.11114",
    "title": "JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of Representation and Circuit",
    "abstract": "           Despite the outstanding performance of Large language models (LLMs) in diverse tasks, they are vulnerable to jailbreak attacks, wherein adversarial prompts are crafted to bypass their security mechanisms and elicit unexpected this http URL jailbreak attacks are prevalent, the understanding of their underlying mechanisms remains limited. Recent studies have explain typical jailbreaking behavior (e.g., the degree to which the model refuses to respond) of LLMs by analyzing the representation shifts in their latent space caused by jailbreak prompts or identifying key neurons that contribute to the success of these attacks. However, these studies neither explore diverse jailbreak patterns nor provide a fine-grained explanation from the failure of circuit to the changes of representational, leaving significant gaps in uncovering the jailbreak mechanism. In this paper, we propose JailbreakLens, an interpretation framework that analyzes jailbreak mechanisms from both representation (which reveals how jailbreaks alter the model's harmfulness perception) and circuit perspectives (which uncovers the causes of these deceptions by identifying key circuits contributing to the vulnerability), tracking their evolution throughout the entire response generation process. We then conduct an in-depth evaluation of jailbreak behavior on four mainstream LLMs under seven jailbreak strategies. Our evaluation finds that jailbreak prompts amplify components that reinforce affirmative responses while suppressing those that produce refusal. Although this manipulation shifts model representations toward safe clusters to deceive the LLM, leading it to provide detailed responses instead of refusals, it still produce abnormal activation which can be caught in the circuit analysis.         ",
    "url": "https://arxiv.org/abs/2411.11114",
    "authors": [
      "Zeqing He",
      "Zhibo Wang",
      "Zhixuan Chu",
      "Huiyu Xu",
      "Rui Zheng",
      "Kui Ren",
      "Chun Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.11119",
    "title": "Leveraging Bitcoin Mining Machines in Demand-Response Mechanisms to Mitigate Ramping-Induced Transients",
    "abstract": "           We propose an extended demand response program, based on ancillary service for supplying flexible electricity demand. In our proposed scheme, we suggest a broader management model to control the scheduling and power consumption of Bitcoin mining machines. The main aspect that we focus on is suppressing the power ramping and related transient effects. We extend previous works on the subject, that study the impact of incorporating cryptocurrency mining machines into existing power grid, and explore the potential profit of exploiting this flexible load in the Israeli electricity market. We analyze a trend based on historical data, of increasing electricity prices and ramping costs due to the increasing penetration of renewable energy sources. We suggest an extension to the unit commitment problem from which we obtain the scheduling scheme of the Bitcoin mining machines. We use simulation and the real-world data acquired from the \"Noga\" grid operator to verify the proposed ancillary service and test its practical limits for reducing the ramping costs, under changing ratio of energy production from renewable sources. Out results suggests that the machine price and ratio of production from renewable sources plays a significant role in determining the profitability of the proposed demand-response program.         ",
    "url": "https://arxiv.org/abs/2411.11119",
    "authors": [
      "Elinor Ginzburg-Ganz",
      "Ittay Eyal",
      "Ram Machlev",
      "Dmitry Baimel",
      "Leena Santosh",
      "Juri Belikov",
      "Yoash Levron"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.11137",
    "title": "Factors in Crowdsourcing for Evaluation of Complex Dialogue Systems",
    "abstract": "           In the last decade, crowdsourcing has become a popular method for conducting quantitative empirical studies in human-machine interaction. The remote work on a given task in crowdworking settings suits the character of typical speech/language-based interactive systems for instance with regard to argumentative conversations and information retrieval. Thus, crowdworking promises a valuable opportunity to study and evaluate the usability and user experience of real humans in interactions with such interactive systems. In contrast to physical attendance in laboratory studies, crowdsourcing studies offer much more flexible and easier access to large numbers of heterogeneous participants with a specific background, e.g., native speakers or domain expertise. On the other hand, the experimental and environmental conditions as well as the participant's compliance and reliability (at least better monitoring of the latter) are much better controllable in a laboratory. This paper seeks to present a (self-)critical examination of crowdsourcing-based studies in the context of complex (spoken) dialogue systems. It describes and discusses observed issues in crowdsourcing studies involving complex tasks and suggests solutions to improve and ensure the quality of the study results. Thereby, our work contributes to a better understanding and what needs to be considered when designing and evaluating studies with crowdworkers for complex dialogue systems.         ",
    "url": "https://arxiv.org/abs/2411.11137",
    "authors": [
      "Annalena Aicher",
      "Stefan Hillmann",
      "Isabel Feustel",
      "Thilo Michael",
      "Sebastian M\u00f6ller",
      "Wolfgang Minker"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2411.11144",
    "title": "CLMIA: Membership Inference Attacks via Unsupervised Contrastive Learning",
    "abstract": "           Since machine learning model is often trained on a limited data set, the model is trained multiple times on the same data sample, which causes the model to memorize most of the training set data. Membership Inference Attacks (MIAs) exploit this feature to determine whether a data sample is used for training a machine learning model. However, in realistic scenarios, it is difficult for the adversary to obtain enough qualified samples that mark accurate identity information, especially since most samples are non-members in real world applications. To address this limitation, in this paper, we propose a new attack method called CLMIA, which uses unsupervised contrastive learning to train an attack model without using extra membership status information. Meanwhile, in CLMIA, we require only a small amount of data with known membership status to fine-tune the attack model. Experimental results demonstrate that CLMIA performs better than existing attack methods for different datasets and model structures, especially with data with less marked identity information. In addition, we experimentally find that the attack performs differently for different proportions of labeled identity information for member and non-member data. More analysis proves that our attack method performs better with less labeled identity information, which applies to more realistic scenarios.         ",
    "url": "https://arxiv.org/abs/2411.11144",
    "authors": [
      "Depeng Chen",
      "Xiao Liu",
      "Jie Cui",
      "Hong Zhong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.11149",
    "title": "From Primes to Paths: Enabling Fast Multi-Relational Graph Analysis",
    "abstract": "           Multi-relational networks capture intricate relationships in data and have diverse applications across fields such as biomedical, financial, and social sciences. As networks derived from increasingly large datasets become more common, identifying efficient methods for representing and analyzing them becomes crucial. This work extends the Prime Adjacency Matrices (PAMs) framework, which employs prime numbers to represent distinct relations within a network uniquely. This enables a compact representation of a complete multi-relational graph using a single adjacency matrix, which, in turn, facilitates quick computation of multi-hop adjacency matrices. In this work, we enhance the framework by introducing a lossless algorithm for calculating the multi-hop matrices and propose the Bag of Paths (BoP) representation, a versatile feature extraction methodology for various graph analytics tasks, at the node, edge, and graph level. We demonstrate the efficiency of the framework across various tasks and datasets, showing that simple BoP-based models perform comparably to or better than commonly used neural models while offering improved speed and interpretability.         ",
    "url": "https://arxiv.org/abs/2411.11149",
    "authors": [
      "Konstantinos Bougiatiotis",
      "Georgios Paliouras"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2411.11161",
    "title": "MPLite: Multi-Aspect Pretraining for Mining Clinical Health Records",
    "abstract": "           The adoption of digital systems in healthcare has resulted in the accumulation of vast electronic health records (EHRs), offering valuable data for machine learning methods to predict patient health outcomes. However, single-visit records of patients are often neglected in the training process due to the lack of annotations of next-visit information, thereby limiting the predictive and expressive power of machine learning models. In this paper, we present a novel framework MPLite that utilizes Multi-aspect Pretraining with Lab results through a light-weight neural network to enhance medical concept representation and predict future health outcomes of individuals. By incorporating both structured medical data and additional information from lab results, our approach fully leverages patient admission records. We design a pretraining module that predicts medical codes based on lab results, ensuring robust prediction by fusing multiple aspects of features. Our experimental evaluation using both MIMIC-III and MIMIC-IV datasets demonstrates improvements over existing models in diagnosis prediction and heart failure prediction tasks, achieving a higher weighted-F1 and recall with MPLite. This work reveals the potential of integrating diverse aspects of data to advance predictive modeling in healthcare.         ",
    "url": "https://arxiv.org/abs/2411.11161",
    "authors": [
      "Eric Yang",
      "Pengfei Hu",
      "Xiaoxue Han",
      "Yue Ning"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11167",
    "title": "Feature Selection Approaches for Newborn Birthweight Prediction in Multiple Linear Regression Models",
    "abstract": "           This project is based on the dataset \"this http URL\", which contains a subcohort of 1301 mother-child pairs who were enrolled into the HELIX study during pregnancy. Several health outcomes were measured on the child at birth or at age 6-11 years, taking environmental exposures of interest and other covariates into account. This report outlines the process of obtaining the best MLR model with optimal predictive power. We first obtain three candidate models we obtained from the forward selection, backward elimination and stepwise selection, and select the optimal model using various comparison schemes including AIC, Adjusted R^2 and cross-validation for 8000 repetitions. The report ended with some additional findings revealed by the selected model, along with restrictions on the method we use in the model selection process.         ",
    "url": "https://arxiv.org/abs/2411.11167",
    "authors": [
      "Esther Liu",
      "Pei Xi Lin",
      "Qianqi Wang",
      "Karina Chen Feng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2411.11176",
    "title": "Infinite Width Limits of Self Supervised Neural Networks",
    "abstract": "           The NTK is a widely used tool in the theoretical analysis of deep learning, allowing us to look at supervised deep neural networks through the lenses of kernel regression. Recently, several works have investigated kernel models for self-supervised learning, hypothesizing that these also shed light on the behaviour of wide neural networks by virtue of the NTK. However, it remains an open question to what extent this connection is mathematically sound -- it is a commonly encountered misbelief that the kernel behaviour of wide neural networks emerges irrespective of the loss function it is trained on. In this paper, we bridge the gap between the NTK and self-supervised learning, focusing on two-layer neural networks trained under the Barlow Twins loss. We prove that the NTK of Barlow Twins indeed becomes constant as the width of the network approaches infinity. Our analysis technique is different from previous works on the NTK and may be of independent interest. Overall, our work provides a first rigorous justification for the use of classic kernel theory to understand self-supervised learning of wide neural networks. Building on this result, we derive generalization error bounds for kernelized Barlow Twins and connect them to neural networks of finite width.         ",
    "url": "https://arxiv.org/abs/2411.11176",
    "authors": [
      "Maximilian Fleissner",
      "Gautham Govind Anil",
      "Debarghya Ghoshdastidar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11180",
    "title": "Robust Defense Against Extreme Grid Events Using Dual-Policy Reinforcement Learning Agents",
    "abstract": "           Reinforcement learning (RL) agents are powerful tools for managing power grids. They use large amounts of data to inform their actions and receive rewards or penalties as feedback to learn favorable responses for the system. Once trained, these agents can efficiently make decisions that would be too computationally complex for a human operator. This ability is especially valuable in decarbonizing power networks, where the demand for RL agents is increasing. These agents are well suited to control grid actions since the action space is constantly growing due to uncertainties in renewable generation, microgrid integration, and cybersecurity threats. To assess the efficacy of RL agents in response to an adverse grid event, we use the Grid2Op platform for agent training. We employ a proximal policy optimization (PPO) algorithm in conjunction with graph neural networks (GNNs). By simulating agents' responses to grid events, we assess their performance in avoiding grid failure for as long as possible. The performance of an agent is expressed concisely through its reward function, which helps the agent learn the most optimal ways to reconfigure a grid's topology amidst certain events. To model multi-actor scenarios that threaten modern power networks, particularly those resulting from cyberattacks, we integrate an opponent that acts iteratively against a given agent. This interplay between the RL agent and opponent is utilized in N-k contingency screening, providing a novel alternative to the traditional security assessment.         ",
    "url": "https://arxiv.org/abs/2411.11180",
    "authors": [
      "Benjamin M. Peter",
      "Mert Korkali"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11185",
    "title": "Mixing Neural Networks and Exponential Moving Averages for Predicting Wireless Links Behavior",
    "abstract": "           Predicting the behavior of a wireless link in terms of, e.g., the frame delivery ratio, is a critical task for optimizing the performance of wireless industrial communication systems. This is because industrial applications are typically characterized by stringent dependability and end-to-end latency requirements, which are adversely affected by channel quality degradation. In this work, we studied two neural network models for Wi-Fi link quality prediction in dense indoor environments. Experimental results show that their accuracy outperforms conventional methods based on exponential moving averages, due to their ability to capture complex patterns about communications, including the effects of shadowing and multipath propagation, which are particularly pronounced in industrial scenarios. This highlights the potential of neural networks for predicting spectrum behavior in challenging operating conditions, and suggests that they can be exploited to improve determinism and dependability of wireless communications, fostering their adoption in the industry.         ",
    "url": "https://arxiv.org/abs/2411.11185",
    "authors": [
      "Gabriele Formis",
      "Stefano Scanzio",
      "Lukasz Wisniewski",
      "Gianluca Cena"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11197",
    "title": "Stealing Training Graphs from Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNNs) have shown promising results in modeling graphs in various tasks. The training of GNNs, especially on specialized tasks such as bioinformatics, demands extensive expert annotations, which are expensive and usually contain sensitive information of data providers. The trained GNN models are often shared for deployment in the real world. As neural networks can memorize the training samples, the model parameters of GNNs have a high risk of leaking private training data. Our theoretical analysis shows the strong connections between trained GNN parameters and the training graphs used, confirming the training graph leakage issue. However, explorations into training data leakage from trained GNNs are rather limited. Therefore, we investigate a novel problem of stealing graphs from trained GNNs. To obtain high-quality graphs that resemble the target training set, a graph diffusion model with diffusion noise optimization is deployed as a graph generator. Furthermore, we propose a selection method that effectively leverages GNN model parameters to identify training graphs from samples generated by the graph diffusion model. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed framework in stealing training graphs from the trained GNN.         ",
    "url": "https://arxiv.org/abs/2411.11197",
    "authors": [
      "Minhua Lin",
      "Enyan Dai",
      "Junjie Xu",
      "Jinyuan Jia",
      "Xiang Zhang",
      "Suhang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.11200",
    "title": "Countering Backdoor Attacks in Image Recognition: A Survey and Evaluation of Mitigation Strategies",
    "abstract": "           The widespread adoption of deep learning across various industries has introduced substantial challenges, particularly in terms of model explainability and security. The inherent complexity of deep learning models, while contributing to their effectiveness, also renders them susceptible to adversarial attacks. Among these, backdoor attacks are especially concerning, as they involve surreptitiously embedding specific triggers within training data, causing the model to exhibit aberrant behavior when presented with input containing the triggers. Such attacks often exploit vulnerabilities in outsourced processes, compromising model integrity without affecting performance on clean (trigger-free) input data. In this paper, we present a comprehensive review of existing mitigation strategies designed to counter backdoor attacks in image recognition. We provide an in-depth analysis of the theoretical foundations, practical efficacy, and limitations of these approaches. In addition, we conduct an extensive benchmarking of sixteen state-of-the-art approaches against eight distinct backdoor attacks, utilizing three datasets, four model architectures, and three poisoning ratios. Our results, derived from 122,236 individual experiments, indicate that while many approaches provide some level of protection, their performance can vary considerably. Furthermore, when compared to two seminal approaches, most newer approaches do not demonstrate substantial improvements in overall performance or consistency across diverse settings. Drawing from these findings, we propose potential directions for developing more effective and generalizable defensive mechanisms in the future.         ",
    "url": "https://arxiv.org/abs/2411.11200",
    "authors": [
      "Kealan Dunnett",
      "Reza Arablouei",
      "Dimity Miller",
      "Volkan Dedeoglu",
      "Raja Jurdak"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11202",
    "title": "Forecasting the risk of software choices: A model to foretell security vulnerabilities from library dependencies and source code evolution",
    "abstract": "           Software security mainly studies vulnerability detection: is my code vulnerable today? This hinders risk estimation, so new approaches are emerging to forecast the occurrence of future vulnerabilities. While useful, these approaches are coarse-grained and hard to employ for project-specific technical decisions. We introduce a model capable of vulnerability forecasting at library level. Formalising source-code evolution in time together with library dependency, our model can estimate the probability that a software project faces a CVE disclosure in a future time window. Our approach is white-box and lightweight, which we demonstrate via experiments involving 1255 CVEs and 768 Java libraries, made public as an open-source artifact. Besides probabilities estimation, e.g. to plan software updates, this formal model can be used to detect security-sensitive points in a project, or measure the health of a development ecosystem.         ",
    "url": "https://arxiv.org/abs/2411.11202",
    "authors": [
      "Carlos E. Budde",
      "Ranindya Paramitha",
      "Fabio Massacci"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2411.11213",
    "title": "Making Sigmoid-MSE Great Again: Output Reset Challenges Softmax Cross-Entropy in Neural Network Classification",
    "abstract": "           This study presents a comparative analysis of two objective functions, Mean Squared Error (MSE) and Softmax Cross-Entropy (SCE) for neural network classification tasks. While SCE combined with softmax activation is the conventional choice for transforming network outputs into class probabilities, we explore an alternative approach using MSE with sigmoid activation. We introduce the Output Reset algorithm, which reduces inconsistent errors and enhances classifier robustness. Through extensive experiments on benchmark datasets (MNIST, CIFAR-10, and Fashion-MNIST), we demonstrate that MSE with sigmoid activation achieves comparable accuracy and convergence rates to SCE, while exhibiting superior performance in scenarios with noisy data. Our findings indicate that MSE, despite its traditional association with regression tasks, serves as a viable alternative for classification problems, challenging conventional wisdom about neural network training strategies.         ",
    "url": "https://arxiv.org/abs/2411.11213",
    "authors": [
      "Kanishka Tyagi",
      "Chinmay Rane",
      "Ketaki Vaidya",
      "Jeshwanth Challgundla",
      "Soumitro Swapan Auddy",
      "Michael Manry"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.11232",
    "title": "SAMOS: A Neural MOS Prediction Model Leveraging Semantic Representations and Acoustic Features",
    "abstract": "           Assessing the naturalness of speech using mean opinion score (MOS) prediction models has positive implications for the automatic evaluation of speech synthesis systems. Early MOS prediction models took the raw waveform or amplitude spectrum of speech as input, whereas more advanced methods employed self-supervised-learning (SSL) based models to extract semantic representations from speech for MOS prediction. These methods utilized limited aspects of speech information for MOS prediction, resulting in restricted prediction accuracy. Therefore, in this paper, we propose SAMOS, a MOS prediction model that leverages both Semantic and Acoustic information of speech to be assessed. Specifically, the proposed SAMOS leverages a pretrained wav2vec2 to extract semantic representations and uses the feature extractor of a pretrained BiVocoder to extract acoustic features. These two types of features are then fed into the prediction network, which includes multi-task heads and an aggregation layer, to obtain the final MOS score. Experimental results demonstrate that the proposed SAMOS outperforms current state-of-the-art MOS prediction models on the BVCC dataset and performs comparable performance on the BC2019 dataset, according to the results of system-level evaluation metrics.         ",
    "url": "https://arxiv.org/abs/2411.11232",
    "authors": [
      "Yu-Fei Shi",
      "Yang Ai",
      "Ye-Xin Lu",
      "Hui-Peng Du",
      "Zhen-Hua Ling"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2411.11249",
    "title": "EXCON: Extreme Instance-based Contrastive Representation Learning of Severely Imbalanced Multivariate Time Series for Solar Flare Prediction",
    "abstract": "           In heliophysics research, predicting solar flares is crucial due to their potential to impact both space-based systems and Earth's infrastructure substantially. Magnetic field data from solar active regions, recorded by solar imaging observatories, are transformed into multivariate time series to enable solar flare prediction using temporal window-based analysis. In the realm of multivariate time series-driven solar flare prediction, addressing severe class imbalance with effective strategies for multivariate time series representation learning is key to developing robust predictive models. Traditional methods often struggle with overfitting to the majority class in prediction tasks where major solar flares are infrequent. This work presents EXCON, a contrastive representation learning framework designed to enhance classification performance amidst such imbalances. EXCON operates through four stages: obtaining core features from multivariate time series data; selecting distinctive contrastive representations for each class to maximize inter-class separation; training a temporal feature embedding module with a custom extreme reconstruction loss to minimize intra-class variation; and applying a classifier to the learned embeddings for robust classification. The proposed method leverages contrastive learning principles to map similar instances closer in the feature space while distancing dissimilar ones, a strategy not extensively explored in solar flare prediction tasks. This approach not only addresses class imbalance but also offers a versatile solution applicable to univariate and multivariate time series across binary and multiclass classification problems. Experimental results, including evaluations on the benchmark solar flare dataset and multiple time series archive datasets with binary and multiclass labels, demonstrate EXCON's efficacy in enhancing classification performance.         ",
    "url": "https://arxiv.org/abs/2411.11249",
    "authors": [
      "Onur Vural",
      "Shah Muhammad Hamdi",
      "Soukaina Filali Boubrahimi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11254",
    "title": "Semantic or Covariate? A Study on the Intractable Case of Out-of-Distribution Detection",
    "abstract": "           The primary goal of out-of-distribution (OOD) detection tasks is to identify inputs with semantic shifts, i.e., if samples from novel classes are absent in the in-distribution (ID) dataset used for training, we should reject these OOD samples rather than misclassifying them into existing ID classes. However, we find the current definition of \"semantic shift\" is ambiguous, which renders certain OOD testing protocols intractable for the post-hoc OOD detection methods based on a classifier trained on the ID dataset. In this paper, we offer a more precise definition of the Semantic Space and the Covariate Space for the ID distribution, allowing us to theoretically analyze which types of OOD distributions make the detection task intractable. To avoid the flaw in the existing OOD settings, we further define the \"Tractable OOD\" setting which ensures the distinguishability of OOD and ID distributions for the post-hoc OOD detection methods. Finally, we conduct several experiments to demonstrate the necessity of our definitions and validate the correctness of our theorems.         ",
    "url": "https://arxiv.org/abs/2411.11254",
    "authors": [
      "Xingming Long",
      "Jie Zhang",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11256",
    "title": "Progressive Generalization Risk Reduction for Data-Efficient Causal Effect Estimation",
    "abstract": "           Causal effect estimation (CEE) provides a crucial tool for predicting the unobserved counterfactual outcome for an entity. As CEE relaxes the requirement for ``perfect'' counterfactual samples (e.g., patients with identical attributes and only differ in treatments received) that are impractical to obtain and can instead operate on observational data, it is usually used in high-stake domains like medical treatment effect prediction. Nevertheless, in those high-stake domains, gathering a decently sized, fully labelled observational dataset remains challenging due to hurdles associated with costs, ethics, expertise and time needed, etc., of which medical treatment surveys are a typical example. Consequently, if the training dataset is small in scale, low generalization risks can hardly be achieved on any CEE algorithms. Unlike existing CEE methods that assume the constant availability of a dataset with abundant samples, in this paper, we study a more realistic CEE setting where the labelled data samples are scarce at the beginning, while more can be gradually acquired over the course of training -- assuredly under a limited budget considering their expensive nature. Then, the problem naturally comes down to actively selecting the best possible samples to be labelled, e.g., identifying the next subset of patients to conduct the treatment survey. However, acquiring quality data for reducing the CEE risk under limited labelling budgets remains under-explored until now. To fill the gap, we theoretically analyse the generalization risk from an intriguing perspective of progressively shrinking its upper bound, and develop a principled label acquisition pipeline exclusively for CEE tasks. With our analysis, we propose the Model Agnostic Causal Active Learning (MACAL) algorithm for batch-wise label acquisition, which aims to reduce both the CEE model's uncertainty and the post-acquisition ...         ",
    "url": "https://arxiv.org/abs/2411.11256",
    "authors": [
      "Hechuan Wen",
      "Tong Chen",
      "Guanhua Ye",
      "Li Kheng Chai",
      "Shazia Sadiq",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.11258",
    "title": "ESTVocoder: An Excitation-Spectral-Transformed Neural Vocoder Conditioned on Mel Spectrogram",
    "abstract": "           This paper proposes ESTVocoder, a novel excitation-spectral-transformed neural vocoder within the framework of source-filter theory. The ESTVocoder transforms the amplitude and phase spectra of the excitation into the corresponding speech amplitude and phase spectra using a neural filter whose backbone is ConvNeXt v2 blocks. Finally, the speech waveform is reconstructed through the inverse short-time Fourier transform (ISTFT). The excitation is constructed based on the F0: for voiced segments, it contains full harmonic information, while for unvoiced segments, it is represented by noise. The excitation provides the filter with prior knowledge of the amplitude and phase patterns, expecting to reduce the modeling difficulty compared to conventional neural vocoders. To ensure the fidelity of the synthesized speech, an adversarial training strategy is applied to ESTVocoder with multi-scale and multi-resolution discriminators. Analysis-synthesis and text-to-speech experiments both confirm that our proposed ESTVocoder outperforms or is comparable to other baseline neural vocoders, e.g., HiFi-GAN, SiFi-GAN, and Vocos, in terms of synthesized speech quality, with a reasonable model complexity and generation speed. Additional analysis experiments also demonstrate that the introduced excitation effectively accelerates the model's convergence process, thanks to the speech spectral prior information contained in the excitation.         ",
    "url": "https://arxiv.org/abs/2411.11258",
    "authors": [
      "Xiao-Hang Jiang",
      "Hui-Peng Du",
      "Yang Ai",
      "Ye-Xin Lu",
      "Zhen-Hua Ling"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2411.11259",
    "title": "Graph Retention Networks for Dynamic Graphs",
    "abstract": "           In this work, we propose Graph Retention Network as a unified architecture for deep learning on dynamic graphs. The GRN extends the core computational manner of retention to dynamic graph data as graph retention, which empowers the model with three key computational paradigms that enable training parallelism, $O(1)$ low-cost inference, and long-term batch training. This architecture achieves an optimal balance of effectiveness, efficiency, and scalability. Extensive experiments conducted on benchmark datasets present the superior performance of the GRN in both edge-level prediction and node-level classification tasks. Our architecture achieves cutting-edge results while maintaining lower training latency, reduced GPU memory consumption, and up to an 86.7x improvement in inference throughput compared to baseline models. The GRNs have demonstrated strong potential to become a widely adopted architecture for dynamic graph learning tasks. Code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.11259",
    "authors": [
      "Qian Chang",
      "Xia Li",
      "Xiufeng Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11283",
    "title": "Multi-Hyperbolic Space-based Heterogeneous Graph Attention Network",
    "abstract": "           To leverage the complex structures within heterogeneous graphs, recent studies on heterogeneous graph embedding use a hyperbolic space, characterized by a constant negative curvature and exponentially increasing space, which aligns with the structural properties of heterogeneous graphs. However, despite heterogeneous graphs inherently possessing diverse power-law structures, most hyperbolic heterogeneous graph embedding models use a single hyperbolic space for the entire heterogeneous graph, which may not effectively capture the diverse power-law structures within the heterogeneous graph. To address this limitation, we propose Multi-hyperbolic Space-based heterogeneous Graph Attention Network (MSGAT), which uses multiple hyperbolic spaces to effectively capture diverse power-law structures within heterogeneous graphs. We conduct comprehensive experiments to evaluate the effectiveness of MSGAT. The experimental results demonstrate that MSGAT outperforms state-of-the-art baselines in various graph machine learning tasks, effectively capturing the complex structures of heterogeneous graphs.         ",
    "url": "https://arxiv.org/abs/2411.11283",
    "authors": [
      "Jongmin Park",
      "Seunghoon Han",
      "Jong-Ryul Lee",
      "Sungsu Lim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11284",
    "title": "Dual-Frequency Filtering Self-aware Graph Neural Networks for Homophilic and Heterophilic Graphs",
    "abstract": "           Graph Neural Networks (GNNs) have excelled in handling graph-structured data, attracting significant research interest. However, two primary challenges have emerged: interference between topology and attributes distorting node representations, and the low-pass filtering nature of most GNNs leading to the oversight of valuable high-frequency information in graph signals. These issues are particularly pronounced in heterophilic graphs. To address these challenges, we propose Dual-Frequency Filtering Self-aware Graph Neural Networks (DFGNN). DFGNN integrates low-pass and high-pass filters to extract smooth and detailed topological features, using frequency-specific constraints to minimize noise and redundancy in the respective frequency bands. The model dynamically adjusts filtering ratios to accommodate both homophilic and heterophilic graphs. Furthermore, DFGNN mitigates interference by aligning topological and attribute representations through dynamic correspondences between their respective frequency bands, enhancing overall model performance and expressiveness. Extensive experiments conducted on benchmark datasets demonstrate that DFGNN outperforms state-of-the-art methods in classification performance, highlighting its effectiveness in handling both homophilic and heterophilic graphs.         ",
    "url": "https://arxiv.org/abs/2411.11284",
    "authors": [
      "Yachao Yang",
      "Yanfeng Sun",
      "Jipeng Guo",
      "Junbin Gao",
      "Shaofan Wang",
      "Fujiao Ju",
      "Baocai Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11293",
    "title": "SADDE: Semi-supervised Anomaly Detection with Dependable Explanations",
    "abstract": "           Semi-supervised learning holds a pivotal position in anomaly detection applications, yet identifying anomaly patterns with a limited number of labeled samples poses a significant challenge. Furthermore, the absence of interpretability poses major obstacles to the practical adoption of semi-supervised frameworks. The majority of existing interpretation techniques are tailored for supervised/unsupervised frameworks or non-security domains, falling short in providing dependable interpretations. In this research paper, we introduce SADDE, a general framework designed to accomplish two primary objectives: (1) to render the anomaly detection process interpretable and enhance the credibility of interpretation outcomes, and (2) to assign high-confidence pseudo labels to unlabeled samples, thereby boosting the performance of anomaly detection systems when supervised data is scarce. To achieve the first objective, we devise a cutting-edge interpretation method that utilizes both global and local interpreters to furnish trustworthy explanations. For the second objective, we conceptualize a novel two-stage semi-supervised learning framework tailored for network anomaly detection, ensuring that the model predictions of both stages align with specific constraints. We apply SADDE to two illustrative network anomaly detection tasks and conduct extensive evaluations in comparison with notable prior works. The experimental findings underscore that SADDE is capable of delivering precise detection results alongside dependable interpretations for semi-supervised network anomaly detection systems. The source code for SADDE is accessible at: this https URL.         ",
    "url": "https://arxiv.org/abs/2411.11293",
    "authors": [
      "Yachao Yuan",
      "Yu Huang",
      "Yali Yuan",
      "Jin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11303",
    "title": "Recurrent Stochastic Configuration Networks with Incremental Blocks",
    "abstract": "           Recurrent stochastic configuration networks (RSCNs) have shown promise in modelling nonlinear dynamic systems with order uncertainty due to their advantages of easy implementation, less human intervention, and strong approximation capability. This paper develops the original RSCNs with block increments, termed block RSCNs (BRSCNs), to further enhance the learning capacity and efficiency of the network. BRSCNs can simultaneously add multiple reservoir nodes (subreservoirs) during the construction. Each subreservoir is configured with a unique structure in the light of a supervisory mechanism, ensuring the universal approximation property. The reservoir feedback matrix is appropriately scaled to guarantee the echo state property of the network. Furthermore, the output weights are updated online using a projection algorithm, and the persistent excitation conditions that facilitate parameter convergence are also established. Numerical results over a time series prediction, a nonlinear system identification task, and two industrial data predictive analyses demonstrate that the proposed BRSCN performs favourably in terms of modelling efficiency, learning, and generalization performance, highlighting their significant potential for coping with complex dynamics.         ",
    "url": "https://arxiv.org/abs/2411.11303",
    "authors": [
      "Gang Dang",
      "Dainhui Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11317",
    "title": "Establishing Minimum Elements for Effective Vulnerability Management in AI Software",
    "abstract": "           In the rapidly evolving field of artificial intelligence (AI), the identification, documentation, and mitigation of vulnerabilities are paramount to ensuring robust and secure systems. This paper discusses the minimum elements for AI vulnerability management and the establishment of an Artificial Intelligence Vulnerability Database (AIVD). It presents standardized formats and protocols for disclosing, analyzing, cataloging, and documenting AI vulnerabilities. It discusses how such an AI incident database must extend beyond the traditional scope of vulnerabilities by focusing on the unique aspects of AI systems. Additionally, this paper highlights challenges and gaps in AI Vulnerability Management, including the need for new severity scores, weakness enumeration systems, and comprehensive mitigation strategies specifically designed to address the multifaceted nature of AI vulnerabilities.         ",
    "url": "https://arxiv.org/abs/2411.11317",
    "authors": [
      "Mohamad Fazelnia",
      "Sara Moshtari",
      "Mehdi Mirakhorli"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.11342",
    "title": "Multi-hop Differential Topology based Algorithms for Resilient Network of UAV Swarm",
    "abstract": "           Unmanned aerial vehicle (UAV) swarm networks face severe challenges of communication network split (CNS) issues caused by massive damage in hostile environments. In this paper, we propose a new paradigm to restore network connectivity by repositioning remaining UAVs based on damage information within local topologies. Particularly, the locations of destroyed UAVs distributed in gaps between disconnected sub-nets are considered for recovery trajectory planning. Specifically, we construct the multi-hop differential sub-graph (MDSG) to represent local damage-varying topologies. Based on this, we develop two distinct algorithms to address CNS issues. The first approach leverages an artificial potential field algorithm to calculate the recovery velocities via MDSG, enabling simple deployment on low-intelligence UAVs. In the second approach, we design an MDSG-based graph convolution framework to find the recovery topology for high-intelligence swarms. As per the unique topology of MDSG, we propose a novel bipartite graph convolution operation, enhanced with a batch-processing mechanism to improve graph convolution efficiency. Simulation results show that the proposed algorithms expedite the recovery with significant margin while improving the spatial coverage and topology degree uniformity after recovery.         ",
    "url": "https://arxiv.org/abs/2411.11342",
    "authors": [
      "Huan Lin",
      "Lianghui Ding"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2411.11351",
    "title": "Visual-Semantic Graph Matching Net for Zero-Shot Learning",
    "abstract": "           Zero-shot learning (ZSL) aims to leverage additional semantic information to recognize unseen classes. To transfer knowledge from seen to unseen classes, most ZSL methods often learn a shared embedding space by simply aligning visual embeddings with semantic prototypes. However, methods trained under this paradigm often struggle to learn robust embedding space because they align the two modalities in an isolated manner among classes, which ignore the crucial class relationship during the alignment process. To address the aforementioned challenges, this paper proposes a Visual-Semantic Graph Matching Net, termed as VSGMN, which leverages semantic relationships among classes to aid in visual-semantic embedding. VSGMN employs a Graph Build Network (GBN) and a Graph Matching Network (GMN) to achieve two-stage visual-semantic alignment. Specifically, GBN first utilizes an embedding-based approach to build visual and semantic graphs in the semantic space and align the embedding with its prototype for first-stage alignment. Additionally, to supplement unseen class relations in these graphs, GBN also build the unseen class nodes based on semantic relationships. In the second stage, GMN continuously integrates neighbor and cross-graph information into the constructed graph nodes, and aligns the node relationships between the two graphs under the class relationship constraint. Extensive experiments on three benchmark datasets demonstrate that VSGMN achieves superior performance in both conventional and generalized ZSL scenarios. The implementation of our VSGMN and experimental results are available at github: this https URL ",
    "url": "https://arxiv.org/abs/2411.11351",
    "authors": [
      "Bowen Duan",
      "Shiming Chen",
      "Yufei Guo",
      "Guo-Sen Xie",
      "Weiping Ding",
      "Yisong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11356",
    "title": "Superpixel-informed Implicit Neural Representation for Multi-Dimensional Data",
    "abstract": "           Recently, implicit neural representations (INRs) have attracted increasing attention for multi-dimensional data recovery. However, INRs simply map coordinates via a multi-layer perception (MLP) to corresponding values, ignoring the inherent semantic information of the data. To leverage semantic priors from the data, we propose a novel Superpixel-informed INR (S-INR). Specifically, we suggest utilizing generalized superpixel instead of pixel as an alternative basic unit of INR for multi-dimensional data (e.g., images and weather data). The coordinates of generalized superpixels are first fed into exclusive attention-based MLPs, and then the intermediate results interact with a shared dictionary matrix. The elaborately designed modules in S-INR allow us to ingenuously exploit the semantic information within and across generalized superpixels. Extensive experiments on various applications validate the effectiveness and efficacy of our S-INR compared to state-of-the-art INR methods.         ",
    "url": "https://arxiv.org/abs/2411.11356",
    "authors": [
      "Jiayi Li",
      "Xile Zhao",
      "Jianli Wang",
      "Chao Wang",
      "Min Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11375",
    "title": "Graph Neural Networks on Graph Databases",
    "abstract": "           Training graph neural networks on large datasets has long been a challenge. Traditional approaches include efficiently representing the whole graph in-memory, designing parameter efficient and sampling-based models, and graph partitioning in a distributed setup. Separately, graph databases with native graph storage and query engines have been developed, which enable time and resource efficient graph analytics workloads. We show how to directly train a GNN on a graph DB, by retrieving minimal data into memory and sampling using the query engine. Our experiments show resource advantages for single-machine and distributed training. Our approach opens up a new way of scaling GNNs as well as a new application area for graph DBs.         ",
    "url": "https://arxiv.org/abs/2411.11375",
    "authors": [
      "Dmytro Lopushanskyy",
      "Borun Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2411.11378",
    "title": "Next-generation optical networks to sustain connectivity of the future: All roads lead to optical-computing-enabled network?",
    "abstract": "           From an architectural perspective with the main goal of reducing the effective traffic load in the network and thus gaining more operational efficiency, optical networks have been essentially remained the same in the recent two decades since the year 2000s with the success and then dominance of optical-bypass mode. In the optical-bypass-enabled network, the add/drop and cross-connect functions constitute the fundamental operations in handling the traffic at the optical layer, whose the underlying principle lies in the fact that in cross-connecting in-transit lightpaths over an intermediate node, such lightpaths must be guarded from each other in a certain dimension, be it the time, frequency or spatial domain, to avoid interference, which is treated as destructive. In view of the rapid progresses in the realm of optical computing enabling the precisely controlled interference between optical channels for various computing capabilities, we envision a different perspective to turn the long-established wisdom in optical-bypass network around by putting the optical channel interference to a good use, resulting into the so-called optical-computing-enabled network. This paper presents two illustrative examples based on the optical aggregation and optical XOR operations which have been progressively maturing and thus, could be feasibly integrated into the current legacy infrastructure with possibly minimal disruptions. We then propose a detailed case study in formulating and solving the network coding-enabled optical networks, demonstrating the efficacy of the optical-computing-enabled network, and highlighting the unique challenges tied with greater complexities in network design problems, compared to optical-bypass counterpart         ",
    "url": "https://arxiv.org/abs/2411.11378",
    "authors": [
      "Dao Thanh Hai",
      "Isaac Woungang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2411.11389",
    "title": "Adapting to Cyber Threats: A Phishing Evolution Network (PEN) Framework for Phishing Generation and Analyzing Evolution Patterns using Large Language Models",
    "abstract": "           Phishing remains a pervasive cyber threat, as attackers craft deceptive emails to lure victims into revealing sensitive information. While Artificial Intelligence (AI), particularly deep learning, has become a key component in defending against phishing attacks, these approaches face critical limitations. The scarcity of publicly available, diverse, and updated data, largely due to privacy concerns, constrains their effectiveness. As phishing tactics evolve rapidly, models trained on limited, outdated data struggle to detect new, sophisticated deception strategies, leaving systems vulnerable to an ever-growing array of attacks. Addressing this gap is essential to strengthening defenses in an increasingly hostile cyber landscape. To address this gap, we propose the Phishing Evolution Network (PEN), a framework leveraging large language models (LLMs) and adversarial training mechanisms to continuously generate high quality and realistic diverse phishing samples, and analyze features of LLM-provided phishing to understand evolving phishing patterns. We evaluate the quality and diversity of phishing samples generated by PEN and find that it produces over 80% realistic phishing samples, effectively expanding phishing datasets across seven dominant types. These PEN-generated samples enhance the performance of current phishing detectors, leading to a 40% improvement in detection accuracy. Additionally, the use of PEN significantly boosts model robustness, reducing detectors' sensitivity to perturbations by up to 60%, thereby decreasing attack success rates under adversarial conditions. When we analyze the phishing patterns that are used in LLM-generated phishing, the cognitive complexity and the tone of time limitation are detected with statistically significant differences compared with existing phishing.         ",
    "url": "https://arxiv.org/abs/2411.11389",
    "authors": [
      "Fengchao Chen",
      "Tingmin Wu",
      "Van Nguyen",
      "Shuo Wang",
      "Hongsheng Hu",
      "Alsharif Abuadbba",
      "Carsten Rudolph"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.11391",
    "title": "The GECo algorithm for Graph Neural Networks Explanation",
    "abstract": "           Graph Neural Networks (GNNs) are powerful models that can manage complex data sources and their interconnection links. One of GNNs' main drawbacks is their lack of interpretability, which limits their application in sensitive fields. In this paper, we introduce a new methodology involving graph communities to address the interpretability of graph classification problems. The proposed method, called GECo, exploits the idea that if a community is a subset of graph nodes densely connected, this property should play a role in graph classification. This is reasonable, especially if we consider the message-passing mechanism, which is the basic mechanism of GNNs. GECo analyzes the contribution to the classification result of the communities in the graph, building a mask that highlights graph-relevant structures. GECo is tested for Graph Convolutional Networks on six artificial and four real-world graph datasets and is compared to the main explainability methods such as PGMExplainer, PGExplainer, GNNExplainer, and SubgraphX using four different metrics. The obtained results outperform the other methods for artificial graph datasets and most real-world datasets.         ",
    "url": "https://arxiv.org/abs/2411.11391",
    "authors": [
      "Salvatore Calderaro",
      "Domenico Amato",
      "Giosu\u00e8 Lo Bosco",
      "Riccardo Rizzo",
      "Filippo Vella"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11396",
    "title": "Stacking Brick by Brick: Aligned Feature Isolation for Incremental Face Forgery Detection",
    "abstract": "           The rapid advancement of face forgery techniques has introduced a growing variety of forgeries. Incremental Face Forgery Detection (IFFD), involving gradually adding new forgery data to fine-tune the previously trained model, has been introduced as a promising strategy to deal with evolving forgery methods. However, a naively trained IFFD model is prone to catastrophic forgetting when new forgeries are integrated, as treating all forgeries as a single ''Fake\" class in the Real/Fake classification can cause different forgery types overriding one another, thereby resulting in the forgetting of unique characteristics from earlier tasks and limiting the model's effectiveness in learning forgery specificity and generality. In this paper, we propose to stack the latent feature distributions of previous and new tasks brick by brick, $\\textit{i.e.}$, achieving $\\textbf{aligned feature isolation}$. In this manner, we aim to preserve learned forgery information and accumulate new knowledge by minimizing distribution overriding, thereby mitigating catastrophic forgetting. To achieve this, we first introduce Sparse Uniform Replay (SUR) to obtain the representative subsets that could be treated as the uniformly sparse versions of the previous global distributions. We then propose a Latent-space Incremental Detector (LID) that leverages SUR data to isolate and align distributions. For evaluation, we construct a more advanced and comprehensive benchmark tailored for IFFD. The leading experimental results validate the superiority of our method.         ",
    "url": "https://arxiv.org/abs/2411.11396",
    "authors": [
      "Jikang Cheng",
      "Zhiyuan Yan",
      "Ying Zhang",
      "Li Hao",
      "Jiaxin Ai",
      "Qin Zou",
      "Chen Li",
      "Zhongyuan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11401",
    "title": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged Sword?",
    "abstract": "           Several techniques have been proposed to automate code review. Early support consisted in recommending the most suited reviewer for a given change or in prioritizing the review tasks. With the advent of deep learning in software engineering, the level of automation has been pushed to new heights, with approaches able to provide feedback on source code in natural language as a human reviewer would do. Also, recent work documented open source projects adopting Large Language Models (LLMs) as co-reviewers. Although the research in this field is very active, little is known about the actual impact of including automatically generated code reviews in the code review process. While there are many aspects worth investigating, in this work we focus on three of them: (i) review quality, i.e., the reviewer's ability to identify issues in the code; (ii) review cost, i.e., the time spent reviewing the code; and (iii) reviewer's confidence, i.e., how confident is the reviewer about the provided feedback. We run a controlled experiment with 29 experts who reviewed different programs with/without the support of an automatically generated code review. During the experiment we monitored the reviewers' activities, for over 50 hours of recorded code reviews. We show that reviewers consider valid most of the issues automatically identified by the LLM and that the availability of an automated review as a starting point strongly influences their behavior: Reviewers tend to focus on the code locations indicated by the LLM rather than searching for additional issues in other parts of the code. The reviewers who started from an automated review identified a higher number of low-severity issues while, however, not identifying more high-severity issues as compared to a completely manual process. Finally, the automated support did not result in saved time and did not increase the reviewers' confidence.         ",
    "url": "https://arxiv.org/abs/2411.11401",
    "authors": [
      "Rosalia Tufano",
      "Alberto Martin-Lopez",
      "Ahmad Tayeb",
      "Ozren Dabi\u0107",
      "Sonia Haiduc",
      "Gabriele Bavota"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2411.11405",
    "title": "Extended Neural Contractive Dynamical Systems: On Multiple Tasks and Riemannian Safety Regions",
    "abstract": "           Stability guarantees are crucial when ensuring that a fully autonomous robot does not take undesirable or potentially harmful actions. We recently proposed the Neural Contractive Dynamical Systems (NCDS), which is a neural network architecture that guarantees contractive stability. With this, learning-from-demonstrations approaches can trivially provide stability guarantees. However, our early work left several unanswered questions, which we here address. Beyond providing an in-depth explanation of NCDS, this paper extends the framework with more careful regularization, a conditional variant of the framework for handling multiple tasks, and an uncertainty-driven approach to latent obstacle avoidance. Experiments verify that the developed system has the flexibility of ordinary neural networks while providing the stability guarantees needed for autonomous robotics.         ",
    "url": "https://arxiv.org/abs/2411.11405",
    "authors": [
      "Hadi Beik Mohammadi",
      "S\u00f8ren Hauberg",
      "Georgios Arvanitidis",
      "Gerhard Neumann",
      "Leonel Rozo"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11407",
    "title": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on Large Language Models",
    "abstract": "           The widespread deployment of large language models (LLMs) across various domains has showcased their immense potential while exposing significant safety vulnerabilities. A major concern is ensuring that LLM-generated content aligns with human values. Existing jailbreak techniques reveal how this alignment can be compromised through specific prompts or adversarial suffixes. In this study, we introduce a new threat: LLMs' bias toward authority. While this inherent bias can improve the quality of outputs generated by LLMs, it also introduces a potential vulnerability, increasing the risk of producing harmful content. Notably, the biases in LLMs is the varying levels of trust given to different types of authoritative information in harmful queries. For example, malware development often favors trust GitHub. To better reveal the risks with LLM, we propose DarkCite, an adaptive authority citation matcher and generator designed for a black-box setting. DarkCite matches optimal citation types to specific risk types and generates authoritative citations relevant to harmful instructions, enabling more effective jailbreak attacks on aligned this http URL experiments show that DarkCite achieves a higher attack success rate (e.g., LLama-2 at 76% versus 68%) than previous methods. To counter this risk, we propose an authenticity and harm verification defense strategy, raising the average defense pass rate (DPR) from 11% to 74%. More importantly, the ability to link citations to the content they encompass has become a foundational function in LLMs, amplifying the influence of LLMs' bias toward authority.         ",
    "url": "https://arxiv.org/abs/2411.11407",
    "authors": [
      "Xikang Yang",
      "Xuehai Tang",
      "Jizhong Han",
      "Songlin Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11424",
    "title": "Membership Inference Attack against Long-Context Large Language Models",
    "abstract": "           Recent advances in Large Language Models (LLMs) have enabled them to overcome their context window limitations, and demonstrate exceptional retrieval and reasoning capacities on longer context. Quesion-answering systems augmented with Long-Context Language Models (LCLMs) can automatically search massive external data and incorporate it into their contexts, enabling faithful predictions and reducing issues such as hallucinations and knowledge staleness. Existing studies targeting LCLMs mainly concentrate on addressing the so-called lost-in-the-middle problem or improving the inference effiencicy, leaving their privacy risks largely unexplored. In this paper, we aim to bridge this gap and argue that integrating all information into the long context makes it a repository of sensitive information, which often contains private data such as medical records or personal identities. We further investigate the membership privacy within LCLMs external context, with the aim of determining whether a given document or sequence is included in the LCLMs context. Our basic idea is that if a document lies in the context, it will exhibit a low generation loss or a high degree of semantic similarity to the contents generated by LCLMs. We for the first time propose six membership inference attack (MIA) strategies tailored for LCLMs and conduct extensive experiments on various popular models. Empirical results demonstrate that our attacks can accurately infer membership status in most cases, e.g., 90.66% attack F1-score on Multi-document QA datasets with LongChat-7b-v1.5-32k, highlighting significant risks of membership leakage within LCLMs input contexts. Furthermore, we examine the underlying reasons why LCLMs are susceptible to revealing such membership information.         ",
    "url": "https://arxiv.org/abs/2411.11424",
    "authors": [
      "Zixiong Wang",
      "Gaoyang Liu",
      "Yang Yang",
      "Chen Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2411.11426",
    "title": "SpiderDAN: Matching Augmentation in Demand-Aware Networks",
    "abstract": "           Graph augmentation is a fundamental and well-studied problem that arises in network optimization. We consider a new variant of this model motivated by reconfigurable communication networks. In this variant, we consider a given physical network and the measured communication demands between the nodes. Our goal is to augment the given physical network with a matching, so that the shortest path lengths in the augmented network, weighted with the demands, are this http URL prove that this problem is NP-hard, even if the physical network is a cycle. We then use results from demand-aware network design to provide a constant-factor approximation algorithm for adding a matching in case that only a few nodes in the network cause almost all the communication. For general real-world communication patterns, we design and evaluate a series of heuristics that can deal with arbitrary graphs as the underlying network structure. Our algorithms are validated experimentally using real-world traces (from e.g., Facebook) of data centers.         ",
    "url": "https://arxiv.org/abs/2411.11426",
    "authors": [
      "Aleksander Figiel",
      "Darya Melnyk",
      "Andr\u00e9 Nichterlein",
      "Arash Pourdamghani",
      "Stefan Schmid"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2411.11437",
    "title": "Causal Effect of Group Diversity on Redundancy and Coverage in Peer-Reviewing",
    "abstract": "           A large host of scientific journals and conferences solicit peer reviews from multiple reviewers for the same submission, aiming to gather a broader range of perspectives and mitigate individual biases. In this work, we reflect on the role of diversity in the slate of reviewers assigned to evaluate a submitted paper as a factor in diversifying perspectives and improving the utility of the peer-review process. We propose two measures for assessing review utility: review coverage -- reviews should cover most contents of the paper -- and review redundancy -- reviews should add information not already present in other reviews. We hypothesize that reviews from diverse reviewers will exhibit high coverage and low redundancy. We conduct a causal study of different measures of reviewer diversity on review coverage and redundancy using observational data from a peer-reviewed conference with approximately 5,000 submitted papers. Our study reveals disparate effects of different diversity measures on review coverage and redundancy. Our study finds that assigning a group of reviewers that are topically diverse, have different seniority levels, or have distinct publication networks leads to broader coverage of the paper or review criteria, but we find no evidence of an increase in coverage for reviewer slates with reviewers from diverse organizations or geographical locations. Reviewers from different organizations, seniority levels, topics, or publications networks (all except geographical diversity) lead to a decrease in redundancy in reviews. Furthermore, publication network-based diversity alone also helps bring in varying perspectives (that is, low redundancy), even within specific review criteria. Our study adopts a group decision-making perspective for reviewer assignments in peer review and suggests dimensions of diversity that can help guide the reviewer assignment process.         ",
    "url": "https://arxiv.org/abs/2411.11437",
    "authors": [
      "Navita Goyal",
      "Ivan Stelmakh",
      "Nihar Shah",
      "Hal Daum\u00e9 III"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computation and Language (cs.CL)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2411.11448",
    "title": "Unveiling the Inflexibility of Adaptive Embedding in Traffic Forecasting",
    "abstract": "           Spatiotemporal Graph Neural Networks (ST-GNNs) and Transformers have shown significant promise in traffic forecasting by effectively modeling temporal and spatial correlations. However, rapid urbanization in recent years has led to dynamic shifts in traffic patterns and travel demand, posing major challenges for accurate long-term traffic prediction. The generalization capability of ST-GNNs in extended temporal scenarios and cross-city applications remains largely unexplored. In this study, we evaluate state-of-the-art models on an extended traffic benchmark and observe substantial performance degradation in existing ST-GNNs over time, which we attribute to their limited inductive capabilities. Our analysis reveals that this degradation stems from an inability to adapt to evolving spatial relationships within urban environments. To address this limitation, we reconsider the design of adaptive embeddings and propose a Principal Component Analysis (PCA) embedding approach that enables models to adapt to new scenarios without retraining. We incorporate PCA embeddings into existing ST-GNN and Transformer architectures, achieving marked improvements in performance. Notably, PCA embeddings allow for flexibility in graph structures between training and testing, enabling models trained on one city to perform zero-shot predictions on other cities. This adaptability demonstrates the potential of PCA embeddings in enhancing the robustness and generalization of spatiotemporal models.         ",
    "url": "https://arxiv.org/abs/2411.11448",
    "authors": [
      "Hongjun Wang",
      "Jiyuan Chen",
      "Lingyu Zhang",
      "Renhe Jiang",
      "Xuan Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11451",
    "title": "Robust Markov Decision Processes: A Place Where AI and Formal Methods Meet",
    "abstract": "           Markov decision processes (MDPs) are a standard model for sequential decision-making problems and are widely used across many scientific areas, including formal methods and artificial intelligence (AI). MDPs do, however, come with the restrictive assumption that the transition probabilities need to be precisely known. Robust MDPs (RMDPs) overcome this assumption by instead defining the transition probabilities to belong to some uncertainty set. We present a gentle survey on RMDPs, providing a tutorial covering their fundamentals. In particular, we discuss RMDP semantics and how to solve them by extending standard MDP methods such as value iteration and policy iteration. We also discuss how RMDPs relate to other models and how they are used in several contexts, including reinforcement learning and abstraction techniques. We conclude with some challenges for future work on RMDPs.         ",
    "url": "https://arxiv.org/abs/2411.11451",
    "authors": [
      "Marnix Suilen",
      "Thom Badings",
      "Eline M. Bovy",
      "David Parker",
      "Nils Jansen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2411.11454",
    "title": "Relevance-guided Audio Visual Fusion for Video Saliency Prediction",
    "abstract": "           Audio data, often synchronized with video frames, plays a crucial role in guiding the audience's visual attention. Incorporating audio information into video saliency prediction tasks can enhance the prediction of human visual behavior. However, existing audio-visual saliency prediction methods often directly fuse audio and visual features, which ignore the possibility of inconsistency between the two modalities, such as when the audio serves as background music. To address this issue, we propose a novel relevance-guided audio-visual saliency prediction network dubbed AVRSP. Specifically, the Relevance-guided Audio-Visual feature Fusion module (RAVF) dynamically adjusts the retention of audio features based on the semantic relevance between audio and visual elements, thereby refining the integration process with visual features. Furthermore, the Multi-scale feature Synergy (MS) module integrates visual features from different encoding stages, enhancing the network's ability to represent objects at various scales. The Multi-scale Regulator Gate (MRG) could transfer crucial fusion information to visual features, thus optimizing the utilization of multi-scale visual features. Extensive experiments on six audio-visual eye movement datasets have demonstrated that our AVRSP network achieves competitive performance in audio-visual saliency prediction.         ",
    "url": "https://arxiv.org/abs/2411.11454",
    "authors": [
      "Li Yu",
      "Xuanzhe Sun",
      "Pan Gao",
      "Moncef Gabbouj"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11467",
    "title": "Physics meets Topology: Physics-informed topological neural networks for learning rigid body dynamics",
    "abstract": "           Rigid body interactions are fundamental to numerous scientific disciplines, but remain challenging to simulate due to their abrupt nonlinear nature and sensitivity to complex, often unknown environmental factors. These challenges call for adaptable learning-based methods capable of capturing complex interactions beyond explicit physical models and simulations. While graph neural networks can handle simple scenarios, they struggle with complex scenes and long-term predictions. We introduce a novel framework for modeling rigid body dynamics and learning collision interactions, addressing key limitations of existing graph-based methods. Our approach extends the traditional representation of meshes by incorporating higher-order topology complexes, offering a physically consistent representation. Additionally, we propose a physics-informed message-passing neural architecture, embedding physical laws directly in the model. Our method demonstrates superior accuracy, even during long rollouts, and exhibits strong generalization to unseen scenarios. Importantly, this work addresses the challenge of multi-entity dynamic interactions, with applications spanning diverse scientific and engineering domains.         ",
    "url": "https://arxiv.org/abs/2411.11467",
    "authors": [
      "Amaury Wei",
      "Olga Fink"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11468",
    "title": "$\\nu$-LPA: Fast GPU-based Label Propagation Algorithm (LPA) for Community Detection",
    "abstract": "           Community detection is the problem of identifying natural divisions in networks. Efficient parallel algorithms for identifying such divisions are critical in a number of applications. This report presents an optimized implementation of the Label Propagation Algorithm (LPA) for community detection, featuring an asynchronous LPA with a Pick-Less (PL) method every 4 iterations to handle community swaps, ideal for SIMT hardware like GPUs. It also introduces a novel per-vertex hashtable with hybrid quadratic-double probing for collision resolution. On an NVIDIA A100 GPU, our implementation, $\\nu$-LPA, outperforms FLPA, NetworKit LPA, and GVE-LPA by 364x, 62x, and 2.6x, respectively, on a server with dual 16-core Intel Xeon Gold 6226R processors - processing 3.0B edges/s on a 2.2B edge graph - and achieves 4.7% higher modularity than FLPA, but 6.1% and 2.2% lower than NetworKit LPA and GVE-LPA.         ",
    "url": "https://arxiv.org/abs/2411.11468",
    "authors": [
      "Subhajit Sahu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2411.11474",
    "title": "Graph Artificial Intelligence for Quantifying Compatibility Mechanisms in Traditional Chinese Medicine",
    "abstract": "           Traditional Chinese Medicine (TCM) involves complex compatibility mechanisms characterized by multi-component and multi-target interactions, which are challenging to quantify. To address this challenge, we applied graph artificial intelligence to develop a TCM multi-dimensional knowledge graph that bridges traditional TCM theory and modern biomedical science (this https URL ). Using feature engineering and embedding, we processed key TCM terminology and Chinese herbal pieces (CHP), introducing medicinal properties as virtual nodes and employing graph neural networks with attention mechanisms to model and analyze 6,080 Chinese herbal formulas (CHF). Our method quantitatively assessed the roles of CHP within CHF and was validated using 215 CHF designed for COVID-19 management. With interpretable models, open-source data, and code (this https URL ), this study provides robust tools for advancing TCM theory and drug discovery.         ",
    "url": "https://arxiv.org/abs/2411.11474",
    "authors": [
      "Jingqi Zeng",
      "Xiaobin Jia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2411.11477",
    "title": "SL-YOLO: A Stronger and Lighter Drone Target Detection Model",
    "abstract": "           Detecting small objects in complex scenes, such as those captured by drones, is a daunting challenge due to the difficulty in capturing the complex features of small targets. While the YOLO family has achieved great success in large target detection, its performance is less than satisfactory when faced with small targets. Because of this, this paper proposes a revolutionary model SL-YOLO (Stronger and Lighter YOLO) that aims to break the bottleneck of small target detection. We propose the Hierarchical Extended Path Aggregation Network (HEPAN), a pioneering cross-scale feature fusion method that can ensure unparalleled detection accuracy even in the most challenging environments. At the same time, without sacrificing detection capabilities, we design the C2fDCB lightweight module and add the SCDown downsampling module to greatly reduce the model's parameters and computational complexity. Our experimental results on the VisDrone2019 dataset reveal a significant improvement in performance, with mAP@0.5 jumping from 43.0% to 46.9% and mAP@0.5:0.95 increasing from 26.0% to 28.9%. At the same time, the model parameters are reduced from 11.1M to 9.6M, and the FPS can reach 132, making it an ideal solution for real-time small object detection in resource-constrained environments.         ",
    "url": "https://arxiv.org/abs/2411.11477",
    "authors": [
      "Defan Chen",
      "Luchan Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11479",
    "title": "Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts",
    "abstract": "           The rapid advancement of Vision-Language Models (VLMs) has expanded multimodal applications, yet evaluations often focus on basic tasks like object recognition, overlooking abstract aspects such as personalities and values. To address this gap, we introduce Value-Spectrum, a visual question-answering benchmark aimed at assessing VLMs based on Schwartz's value dimensions, which capture core values guiding people's beliefs and actions across cultures. We constructed a vectorized database of over 50,000 short videos sourced from TikTok, YouTube Shorts, and Instagram Reels, covering multiple months and a wide array of topics such as family, health, hobbies, society, and technology. We also developed a VLM agent pipeline to automate video browsing and analysis. Benchmarking representative VLMs on Value-Spectrum reveals significant differences in their responses to value-oriented content, with most models exhibiting a preference for hedonistic topics. Beyond identifying natural preferences, we explored the ability of VLM agents to adopt specific personas when explicitly prompted, revealing insights into the models' adaptability in role-playing scenarios. These findings highlight the potential of Value-Spectrum as a comprehensive evaluation set for tracking VLM advancements in value-based tasks and for developing more sophisticated role-playing AI agents.         ",
    "url": "https://arxiv.org/abs/2411.11479",
    "authors": [
      "Jingxuan Li",
      "Yuning Yang",
      "Shengqi Yang",
      "Yizhou Zhao",
      "Ying Nian Wu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2411.11483",
    "title": "Robust State Estimation for Legged Robots with Dual Beta Kalman Filter",
    "abstract": "           Existing state estimation algorithms for legged robots that rely on proprioceptive sensors often overlook foot slippage and leg deformation in the physical world, leading to large estimation errors. To address this limitation, we propose a comprehensive measurement model that accounts for both foot slippage and variable leg length by analyzing the relative motion between foot contact points and the robot's body center. We show that leg length is an observable quantity, meaning that its value can be explicitly inferred by designing an auxiliary filter. To this end, we introduce a dual estimation framework that iteratively employs a parameter filter to estimate the leg length parameters and a state filter to estimate the robot's state. To prevent error accumulation in this iterative framework, we construct a partial measurement model for the parameter filter using the leg static equation. This approach ensures that leg length estimation relies solely on joint torques and foot contact forces, avoiding the influence of state estimation errors on the parameter estimation. Unlike leg length which can be directly estimated, foot slippage cannot be measured directly with the current sensor configuration. However, since foot slippage occurs at a low frequency, it can be treated as outliers in the measurement data. To mitigate the impact of these outliers, we propose the beta Kalman filter (beta KF), which redefines the estimation loss in canonical Kalman filtering using beta divergence. This divergence can assign low weights to outliers in an adaptive manner, thereby enhancing the robustness of the estimation algorithm. These techniques together form the dual beta-Kalman filter (Dual beta KF), a novel algorithm for robust state estimation in legged robots. Experimental results on the Unitree GO2 robot demonstrate that the Dual beta KF significantly outperforms state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2411.11483",
    "authors": [
      "Tianyi Zhang",
      "Wenhan Cao",
      "Chang Liu",
      "Tao Zhang",
      "Jiangtao Li",
      "Shengbo Eben Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2411.11487",
    "title": "Look a Group at Once: Multi-Slide Modeling for Survival Prediction",
    "abstract": "           Survival prediction is a critical task in pathology. In clinical practice, pathologists often examine multiple cases, leveraging a broader spectrum of cancer phenotypes to enhance pathological assessment. Despite significant advancements in deep learning, current solutions typically model each slide as a sample, struggling to effectively capture comparable and slide-agnostic pathological features. In this paper, we introduce GroupMIL, a novel framework inspired by the clinical practice of collective analysis, which models multiple slides as a single sample and organizes groups of patches and slides sequentially to capture cross-slide prognostic features. We also present GPAMamba, a model designed to facilitate intra- and inter-slide feature interactions, effectively capturing local micro-environmental characteristics within slide-level graphs while uncovering essential prognostic patterns across an extended patch sequence within the group framework. Furthermore, we develop a dual-head predictor that delivers comprehensive survival risk and probability assessments for each patient. Extensive empirical evaluations demonstrate that our model significantly outperforms state-of-the-art approaches across five datasets from The Cancer Genome Atlas.         ",
    "url": "https://arxiv.org/abs/2411.11487",
    "authors": [
      "Xinyang Li",
      "Yi Zhang",
      "Yi Xie",
      "Jianfei Yang",
      "Xi Wang",
      "Hao Chen",
      "Haixian Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11497",
    "title": "Physics Encoded Blocks in Residual Neural Network Architectures for Digital Twin Models",
    "abstract": "           Physics Informed Machine Learning has emerged as a popular approach in modelling and simulation for digital twins to generate accurate models of processes and behaviours of real-world systems. However, despite their success in generating accurate and reliable models, the existing methods either use simple regularizations in loss functions to offer limited physics integration or are too specific in architectural definitions to be generalized to a wide variety of physical systems. This paper presents a generic approach based on a novel physics-encoded residual neural network architecture to combine data-driven and physics-based analytical models to address these limitations. Our method combines physics blocks as mathematical operators from physics-based models with learning blocks comprising feed-forward layers. Intermediate residual blocks are incorporated for stable gradient flow as they train on physical system observation data. This way, the model learns to comply with the geometric and kinematic aspects of the physical system. Compared to conventional neural network-based methods, our method improves generalizability with substantially low data requirements and model complexity in terms of parameters, especially in scenarios where prior physics knowledge is either elementary or incomplete. We investigate our approach in two application domains. The first is a basic robotic motion model using Euler Lagrangian equations of motion as physics prior. The second application is a complex scenario of a steering model for a self-driving vehicle in a simulation. In both applications, our method outperforms both conventional neural network based approaches as-well as state-of-the-art Physics Informed Machine Learning methods.         ",
    "url": "https://arxiv.org/abs/2411.11497",
    "authors": [
      "Muhammad Saad Zia",
      "Ashiq Anjum",
      "Lu Liu",
      "Anthony Conway",
      "Anasol Pena Rios"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2411.11502",
    "title": "All-domain Moveline Evolution Network for Click-Through Rate Prediction",
    "abstract": "           E-commerce app users exhibit behaviors that are inherently logically consistent. A series of multi-scenario user behaviors interconnect to form the scene-level all-domain user moveline, which ultimately reveals the user's true intention. Traditional CTR prediction methods typically focus on the item-level interaction between the target item and the historically interacted items. However, the scene-level interaction between the target item and the user moveline remains underexplored. There are two challenges when modeling the interaction with preceding all-domain user moveline: (i) Heterogeneity between items and scenes: Unlike traditional user behavior sequences that utilize items as carriers, the user moveline utilizes scenes as carriers. The heterogeneity between items and scenes complicates the process of aligning interactions within a unified representation space. (ii) Temporal misalignment of linked scene-level and item-level behaviors: In the preceding user moveline with a fixed sampling length, certain critical scene-level behaviors are closely linked to subsequent item-level behaviors. However, it is impossible to establish a complete temporal alignment that clearly identifies which specific scene-level behaviors correspond to which item-level behaviors. To address these challenges and pioneer modeling user intent from the perspective of the all-domain moveline, we propose All-domain Moveline Evolution Network (AMEN). AMEN not only transfers interactions between items and scenes to homogeneous representation spaces, but also introduces a Temporal Sequential Pairwise (TSP) mechanism to understand the nuanced associations between scene-level and item-level behaviors, ensuring that the all-domain user moveline differentially influences CTR predictions for user's favored and unfavored items. Online A/B testing demonstrates that our method achieves a +11.6% increase in CTCVR.         ",
    "url": "https://arxiv.org/abs/2411.11502",
    "authors": [
      "Chen Gao",
      "Zixin Zhao",
      "Lv Shao",
      "Tong Liu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2411.11508",
    "title": "Collaborative Contrastive Network for Click-Through Rate Prediction",
    "abstract": "           E-commerce platforms provide entrances for customers to enter mini-apps to meet their specific shopping needs. At the entrance of a mini-app, a trigger item recommended based on customers' historical preferences, is displayed to attract customers to enter the mini-app. Existing Click-Through Rate (CTR) prediction approaches have two significant weaknesses: (i) A portion of customer entries is driven by their interest in the mini-app itself rather than the trigger item. In such cases, approaches highly hinging on the trigger item tend to recommend similar items, thus misunderstanding the customers' real intention; (ii) Approaches that consider customers' intention toward mini-apps, require the regular existence of mini-apps for customers to cultivate routine shopping habits, making such approaches less robust for mini-apps that are available for only short periods (1 or 3 days) in Explosive Promotional Scenarios (EPS), such as the Black Friday and China's Double 11 Shopping Carnival. To address the above-mentioned issues, we introduce a more general and robust CTR prediction approach, dubbed Collaborative Contrastive Network (CCN). Given a user, CCN learns to identify two item clusters that can represent the user's interests and disinterests, via leveraging the collaborative relationship of co-click/co-non-click or the non-collaborative relationship of mono-click as the supervision signal for contrastive learning. This paradigm does not need to explicitly estimate user's binary entry intention and avoids amplifying the impact of the trigger item. Online A/B testing on large-scale real-world data demonstrates that CCN sets a new state-of-the-art performance on Taobao, boosting CTR by 12.3% and order volume by 12.7%.         ",
    "url": "https://arxiv.org/abs/2411.11508",
    "authors": [
      "Chen Gao",
      "Zixin Zhao",
      "Sihao Hu",
      "Lv Shao",
      "Tong Liu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2411.11514",
    "title": "Learning a Neural Association Network for Self-supervised Multi-Object Tracking",
    "abstract": "           This paper introduces a novel framework to learn data association for multi-object tracking in a self-supervised manner. Fully-supervised learning methods are known to achieve excellent tracking performances, but acquiring identity-level annotations is tedious and time-consuming. Motivated by the fact that in real-world scenarios object motion can be usually represented by a Markov process, we present a novel expectation maximization (EM) algorithm that trains a neural network to associate detections for tracking, without requiring prior knowledge of their temporal correspondences. At the core of our method lies a neural Kalman filter, with an observation model conditioned on associations of detections parameterized by a neural network. Given a batch of frames as input, data associations between detections from adjacent frames are predicted by a neural network followed by a Sinkhorn normalization that determines the assignment probabilities of detections to states. Kalman smoothing is then used to obtain the marginal probability of observations given the inferred states, producing a training objective to maximize this marginal probability using gradient descent. The proposed framework is fully differentiable, allowing the underlying neural model to be trained end-to-end. We evaluate our approach on the challenging MOT17 and MOT20 datasets and achieve state-of-the-art results in comparison to self-supervised trackers using public detections. We furthermore demonstrate the capability of the learned model to generalize across datasets.         ",
    "url": "https://arxiv.org/abs/2411.11514",
    "authors": [
      "Shuai Li",
      "Michael Burke",
      "Subramanian Ramamoorthy",
      "Juergen Gall"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11525",
    "title": "Reliable Poisoned Sample Detection against Backdoor Attacks Enhanced by Sharpness Aware Minimization",
    "abstract": "           Backdoor attack has been considered as a serious security threat to deep neural networks (DNNs). Poisoned sample detection (PSD) that aims at filtering out poisoned samples from an untrustworthy training dataset has shown very promising performance for defending against data poisoning based backdoor attacks. However, we observe that the detection performance of many advanced methods is likely to be unstable when facing weak backdoor attacks, such as low poisoning ratio or weak trigger strength. To further verify this observation, we make a statistical investigation among various backdoor attacks and poisoned sample detections, showing a positive correlation between backdoor effect and detection performance. It inspires us to strengthen the backdoor effect to enhance detection performance. Since we cannot achieve that goal via directly manipulating poisoning ratio or trigger strength, we propose to train one model using the Sharpness-Aware Minimization (SAM) algorithm, rather than the vanilla training algorithm. We also provide both empirical and theoretical analysis about how SAM training strengthens the backdoor effect. Then, this SAM trained model can be seamlessly integrated with any off-the-shelf PSD method that extracts discriminative features from the trained model for detection, called SAM-enhanced PSD. Extensive experiments on several benchmark datasets show the reliable detection performance of the proposed method against both weak and strong backdoor attacks, with significant improvements against various attacks ($+34.38\\%$ TPR on average), over the conventional PSD methods (i.e., without SAM enhancement). Overall, this work provides new insights about PSD and proposes a novel approach that can complement existing detection methods, which may inspire more in-depth explorations in this field.         ",
    "url": "https://arxiv.org/abs/2411.11525",
    "authors": [
      "Mingda Zhang",
      "Mingli Zhu",
      "Zihao Zhu",
      "Baoyuan Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11531",
    "title": "Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality",
    "abstract": "           In this paper we present an approach to reduce hallucinations in Large Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional modality. Our method involves transforming input text into a set of KG embeddings and using an adapter to integrate these embeddings into the language model space, without relying on external retrieval processes. To facilitate this, we created WikiEntities, a dataset containing over 3 million Wikipedia texts annotated with entities from Wikidata and their corresponding embeddings from PyTorch-BigGraph. This dataset serves as a valuable resource for training Entity Linking models and adapting the described method to various LLMs using specialized adapters. Our method does not require fine-tuning of the language models themselves; instead, we only train the adapter. This ensures that the model's performance on other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA 2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and demonstrated that our approach improves performance on the HaluEval, True-False benchmarks and FEVER dataset. The results indicate that incorporating KGs as a new modality can effectively reduce hallucinations and improve the factual accuracy of language models, all without the need for external retrieval.         ",
    "url": "https://arxiv.org/abs/2411.11531",
    "authors": [
      "Viktoriia Chekalina",
      "Anton Razzigaev",
      "Elizaveta Goncharova",
      "Andrey Kuznetsov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11532",
    "title": "A Code Knowledge Graph-Enhanced System for LLM-Based Fuzz Driver Generation",
    "abstract": "           The rapid development of large language models (LLMs) with advanced programming capabilities has paved the way for innovative approaches in software testing. Fuzz testing, a cornerstone for improving software reliability and detecting vulnerabilities, often relies on manually written fuzz drivers, limiting scalability and efficiency. To address this challenge, we propose CodeGraphGPT, a novel system that integrates code knowledge graphs with an LLM-powered intelligent agent to automate the fuzz driver generation process. By framing fuzz driver creation as a code generation task, CodeGraphGPT leverages program analysis to construct a knowledge graph of code repositories, where nodes represent code entities, such as functions or files, and edges capture their relationships. This enables the system to generate tailored fuzz drivers and input seeds, resolve compilation errors, and analyze crash reports, all while adapting to specific API usage scenarios. Additionally, querying the knowledge graph helps identify precise testing targets and contextualize the purpose of each fuzz driver within the fuzzing loop. We evaluated CodeGraphGPT on eight open-source software projects, achieving an average improvement of 8.73\\% in code coverage compared to state-of-the-art methods. Moreover, it reduced the manual workload in crash case analysis by 84.4\\% and identified 11 real-world bugs, including nine previously unreported ones. This work highlights how integrating LLMs with code knowledge graphs enhances fuzz driver generation, offering an efficient solution for vulnerability detection and software quality improvement.         ",
    "url": "https://arxiv.org/abs/2411.11532",
    "authors": [
      "Hanxiang Xu",
      "Wei Ma",
      "Ting Zhou",
      "Yanjie Zhao",
      "Kai Chen",
      "Qiang Hu",
      "Yang Liu",
      "Haoyu Wang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.11536",
    "title": "Hierarchical-Graph-Structured Edge Partition Models for Learning Evolving Community Structure",
    "abstract": "           We propose a novel dynamic network model to capture evolving latent communities within temporal networks. To achieve this, we decompose each observed dynamic edge between vertices using a Poisson-gamma edge partition model, assigning each vertex to one or more latent communities through \\emph{nonnegative} vertex-community memberships. Specifically, hierarchical transition kernels are employed to model the interactions between these latent communities in the observed temporal network. A hierarchical graph prior is placed on the transition structure of the latent communities, allowing us to model how they evolve and interact over time. Consequently, our dynamic network enables the inferred community structure to merge, split, and interact with one another, providing a comprehensive understanding of complex network dynamics. Experiments on various real-world network datasets demonstrate that the proposed model not only effectively uncovers interpretable latent structures but also surpasses other state-of-the art dynamic network models in the tasks of link prediction and community detection.         ",
    "url": "https://arxiv.org/abs/2411.11536",
    "authors": [
      "Xincan Yu",
      "Sikun Yang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11567",
    "title": "GNN-Based Code Annotation Logic for Establishing Security Boundaries in C Code",
    "abstract": "           Securing sensitive operations in today's interconnected software landscape is crucial yet challenging. Modern platforms rely on Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, to isolate security sensitive code from the main system, reducing the Trusted Computing Base (TCB) and providing stronger assurances. However, identifying which code should reside in TEEs is complex and requires specialized expertise, which is not supported by current automated tools. Existing solutions often migrate entire applications to TEEs, leading to suboptimal use and an increased TCB. To address this gap, we propose Code Annotation Logic (CAL), a pioneering tool that automatically identifies security sensitive components for TEE isolation. CAL analyzes codebases, leveraging a graph-based approach with novel feature construction and employing a custom graph neural network model to accurately determine which parts of the code should be isolated. CAL effectively optimizes TCB, reducing the burden of manual analysis and enhancing overall security. Our contributions include the definition of security sensitive code, the construction and labeling of a comprehensive dataset of source files, a feature rich graph based data preparation pipeline, and the CAL model for TEE integration. Evaluation results demonstrate CAL's efficacy in identifying sensitive code with a recall of 86.05%, an F1 score of 81.56%, and an identification rate of 91.59% for security sensitive functions. By enabling efficient code isolation, CAL advances the secure development of applications using TEEs, offering a practical solution for developers to reduce attack vectors.         ",
    "url": "https://arxiv.org/abs/2411.11567",
    "authors": [
      "Varun Gadey",
      "Raphael Goetz",
      "Christoph Sendner",
      "Sampo Sovio",
      "Alexandra Dmitrienko"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11574",
    "title": "Reduced Network Cumulative Constraint Violation for Distributed Bandit Convex Optimization under Slater Condition",
    "abstract": "           This paper studies the distributed bandit convex optimization problem with time-varying inequality constraints, where the goal is to minimize network regret and cumulative constraint violation. To calculate network cumulative constraint violation, existing distributed bandit online algorithms solving this problem directly use the clipped constraint function to replace its original constraint function. However, the use of the clipping operation renders Slater condition (i.e, there exists a point that strictly satisfies the inequality constraints at all iterations) ineffective to achieve reduced network cumulative constraint violation. To tackle this challenge, we propose a new distributed bandit online primal-dual algorithm. If local loss functions are convex, we show that the proposed algorithm establishes sublinear network regret and cumulative constraint violation bounds. When Slater condition holds, the network cumulative constraint violation bound is reduced. In addition, if local loss functions are strongly convex, for the case where strongly convex parameters are unknown, the network regret bound is reduced. For the case where strongly convex parameters are known, the network regret and cumulative constraint violation bounds are further reduced. To the best of our knowledge, this paper is among the first to establish reduced (network) cumulative constraint violation bounds for (distributed) bandit convex optimization with time-varying constraints under Slater condition. Finally, a numerical example is provided to verify the theoretical results.         ",
    "url": "https://arxiv.org/abs/2411.11574",
    "authors": [
      "Kunpeng Zhang",
      "Xinlei Yi",
      "Jinliang Ding",
      "Ming Cao",
      "Karl H. Johansson",
      "Tao Yang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.11581",
    "title": "OASIS: Open Agents Social Interaction Simulations on One Million Agents",
    "abstract": "           There has been a growing interest in enhancing rule-based agent-based models (ABMs) for social media platforms (\\emph{i.e.}, X, Reddit) with more realistic large language model (LLM) agents, thereby allowing for a more nuanced study of complex systems. As a result, several LLM-based ABMs have been proposed in the past year. While they hold promise, each simulator is specifically designed to study a particular scenario, making it time-consuming and resource-intensive to explore other phenomena using the same ABM. Additionally, these models simulate only a limited number of agents, whereas real-world social media platforms involve millions of users. To this end, we propose OASIS, a generalizable and scalable social media simulator. OASIS is designed based on real-world social media platforms, incorporating dynamically updated environments (\\emph{i.e.}, dynamic social networks and post information), diverse action spaces (\\emph{i.e.}, following, commenting), and recommendation systems (\\emph{i.e.}, interest-based and hot-score-based). Additionally, OASIS supports large-scale user simulations, capable of modeling up to one million users. With these features, OASIS can be easily extended to different social media platforms to study large-scale group phenomena and behaviors. We replicate various social phenomena, including information spreading, group polarization, and herd effects across X and Reddit platforms. Moreover, we provide observations of social phenomena at different agent group scales. We observe that the larger agent group scale leads to more enhanced group dynamics and more diverse and helpful agents' opinions. These findings demonstrate OASIS's potential as a powerful tool for studying complex systems in digital environments.         ",
    "url": "https://arxiv.org/abs/2411.11581",
    "authors": [
      "Ziyi Yang",
      "Zaibin Zhang",
      "Zirui Zheng",
      "Yuxian Jiang",
      "Ziyue Gan",
      "Zhiyu Wang",
      "Zijian Ling",
      "Jinsong Chen",
      "Martz Ma",
      "Bowen Dong",
      "Prateek Gupta",
      "Shuyue Hu",
      "Zhenfei Yin",
      "Guohao Li",
      "Xu Jia",
      "Lijun Wang",
      "Bernard Ghanem",
      "Huchuan Lu",
      "Wanli Ouyang",
      "Yu Qiao",
      "Philip Torr",
      "Jing Shao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2411.11603",
    "title": "Feature Selection for Network Intrusion Detection",
    "abstract": "           Network Intrusion Detection (NID) remains a key area of research within the information security community, while also being relevant to Machine Learning (ML) practitioners. The latter generally aim to detect attacks using network features, which have been extracted from raw network data typically using dimensionality reduction methods, such as principal component analysis (PCA). However, PCA is not able to assess the relevance of features for the task at hand. Consequently, the features available are of varying quality, with some being entirely non-informative. From this, two major drawbacks arise. Firstly, trained and deployed models have to process large amounts of unnecessary data, therefore draining potentially costly resources. Secondly, the noise caused by the presence of irrelevant features can, in some cases, impede a model's ability to detect an attack. In order to deal with these challenges, we present Feature Selection for Network Intrusion Detection (FSNID) a novel information-theoretic method that facilitates the exclusion of non-informative features when detecting network intrusions. The proposed method is based on function approximation using a neural network, which enables a version of our approach that incorporates a recurrent layer. Consequently, this version uniquely enables the integration of temporal dependencies. Through an extensive set of experiments, we demonstrate that the proposed method selects a significantly reduced feature set, while maintaining NID performance. Code will be made available upon publication.         ",
    "url": "https://arxiv.org/abs/2411.11603",
    "authors": [
      "Charles Westphal",
      "Stephen Hailes",
      "Mirco Musolesi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.11616",
    "title": "Signaling and Social Learning in Swarms of Robots",
    "abstract": "           This paper investigates the role of communication in improving coordination within robot swarms, focusing on a paradigm where learning and execution occur simultaneously in a decentralized manner. We highlight the role communication can play in addressing the credit assignment problem (individual contribution to the overall performance), and how it can be influenced by it. We propose a taxonomy of existing and future works on communication, focusing on information selection and physical abstraction as principal axes for classification: from low-level lossless compression with raw signal extraction and processing to high-level lossy compression with structured communication models. The paper reviews current research from evolutionary robotics, multi-agent (deep) reinforcement learning, language models, and biophysics models to outline the challenges and opportunities of communication in a collective of robots that continuously learn from one another through local message exchanges, illustrating a form of social learning.         ",
    "url": "https://arxiv.org/abs/2411.11616",
    "authors": [
      "Leo Cazenille",
      "Maxime Toquebiau",
      "Nicolas Lobato-Dauzier",
      "Alessia Loi",
      "Loona Macabre",
      "Nathanael Aubert-Kato",
      "Anthony Genot",
      "Nicolas Bredeche"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2411.11641",
    "title": "TSINR: Capturing Temporal Continuity via Implicit Neural Representations for Time Series Anomaly Detection",
    "abstract": "           Time series anomaly detection aims to identify unusual patterns in data or deviations from systems' expected behavior. The reconstruction-based methods are the mainstream in this task, which learn point-wise representation via unsupervised learning. However, the unlabeled anomaly points in training data may cause these reconstruction-based methods to learn and reconstruct anomalous data, resulting in the challenge of capturing normal patterns. In this paper, we propose a time series anomaly detection method based on implicit neural representation (INR) reconstruction, named TSINR, to address this challenge. Due to the property of spectral bias, TSINR enables prioritizing low-frequency signals and exhibiting poorer performance on high-frequency abnormal data. Specifically, we adopt INR to parameterize time series data as a continuous function and employ a transformer-based architecture to predict the INR of given data. As a result, the proposed TSINR method achieves the advantage of capturing the temporal continuity and thus is more sensitive to discontinuous anomaly data. In addition, we further design a novel form of INR continuous function to learn inter- and intra-channel information, and leverage a pre-trained large language model to amplify the intense fluctuations in anomalies. Extensive experiments demonstrate that TSINR achieves superior overall performance on both univariate and multivariate time series anomaly detection benchmarks compared to other state-of-the-art reconstruction-based methods. Our codes are available.         ",
    "url": "https://arxiv.org/abs/2411.11641",
    "authors": [
      "Mengxuan Li",
      "Ke Liu",
      "Hongyang Chen",
      "Jiajun Bu",
      "Hongwei Wang",
      "Haishuai Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11659",
    "title": "Improving Data Curation of Software Vulnerability Patches through Uncertainty Quantification",
    "abstract": "           The changesets (or patches) that fix open source software vulnerabilities form critical datasets for various machine learning security-enhancing applications, such as automated vulnerability patching and silent fix detection. These patch datasets are derived from extensive collections of historical vulnerability fixes, maintained in databases like the Common Vulnerabilities and Exposures list and the National Vulnerability Database. However, since these databases focus on rapid notification to the security community, they contain significant inaccuracies and omissions that have a negative impact on downstream software security quality assurance tasks. In this paper, we propose an approach employing Uncertainty Quantification (UQ) to curate datasets of publicly-available software vulnerability patches. Our methodology leverages machine learning models that incorporate UQ to differentiate between patches based on their potential utility. We begin by evaluating a number of popular UQ techniques, including Vanilla, Monte Carlo Dropout, and Model Ensemble, as well as homoscedastic and heteroscedastic models of noise. Our findings indicate that Model Ensemble and heteroscedastic models are the best choices for vulnerability patch datasets. Based on these UQ modeling choices, we propose a heuristic that uses UQ to filter out lower quality instances and select instances with high utility value from the vulnerability dataset. Using our approach, we observe an improvement in predictive performance and significant reduction of model training time (i.e., energy consumption) for a state-of-the-art vulnerability prediction model.         ",
    "url": "https://arxiv.org/abs/2411.11659",
    "authors": [
      "Hui Chen",
      "Yunhua Zhao",
      "Kostadin Damevski"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2411.11668",
    "title": "Efficient and Robust Continual Graph Learning for Graph Classification in Biology",
    "abstract": "           Graph classification is essential for understanding complex biological systems, where molecular structures and interactions are naturally represented as graphs. Traditional graph neural networks (GNNs) perform well on static tasks but struggle in dynamic settings due to catastrophic forgetting. We present Perturbed and Sparsified Continual Graph Learning (PSCGL), a robust and efficient continual graph learning framework for graph data classification, specifically targeting biological datasets. We introduce a perturbed sampling strategy to identify critical data points that contribute to model learning and a motif-based graph sparsification technique to reduce storage needs while maintaining performance. Additionally, our PSCGL framework inherently defends against graph backdoor attacks, which is crucial for applications in sensitive biological contexts. Extensive experiments on biological datasets demonstrate that PSCGL not only retains knowledge across tasks but also enhances the efficiency and robustness of graph classification models in biology.         ",
    "url": "https://arxiv.org/abs/2411.11668",
    "authors": [
      "Ding Zhang",
      "Jane Downer",
      "Can Chen",
      "Ren Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2411.11677",
    "title": "Few-shot Model Extraction Attacks against Sequential Recommender Systems",
    "abstract": "           Among adversarial attacks against sequential recommender systems, model extraction attacks represent a method to attack sequential recommendation models without prior knowledge. Existing research has primarily concentrated on the adversary's execution of black-box attacks through data-free model extraction. However, a significant gap remains in the literature concerning the development of surrogate models by adversaries with access to few-shot raw data (10\\% even less). That is, the challenge of how to construct a surrogate model with high functional similarity within the context of few-shot data scenarios remains an issue that requires this http URL study addresses this gap by introducing a novel few-shot model extraction framework against sequential recommenders, which is designed to construct a superior surrogate model with the utilization of few-shot data. The proposed few-shot model extraction framework is comprised of two components: an autoregressive augmentation generation strategy and a bidirectional repair loss-facilitated model distillation procedure. Specifically, to generate synthetic data that closely approximate the distribution of raw data, autoregressive augmentation generation strategy integrates a probabilistic interaction sampler to extract inherent dependencies and a synthesis determinant signal module to characterize user behavioral patterns. Subsequently, bidirectional repair loss, which target the discrepancies between the recommendation lists, is designed as auxiliary loss to rectify erroneous predictions from surrogate models, transferring knowledge from the victim model to the surrogate model effectively. Experiments on three datasets show that the proposed few-shot model extraction framework yields superior surrogate models.         ",
    "url": "https://arxiv.org/abs/2411.11677",
    "authors": [
      "Hui Zhang",
      "Fu Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2411.11683",
    "title": "TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the Physical World",
    "abstract": "           Robotic manipulation refers to the autonomous handling and interaction of robots with objects using advanced techniques in robotics and artificial intelligence. The advent of powerful tools such as large language models (LLMs) and large vision-language models (LVLMs) has significantly enhanced the capabilities of these robots in environmental perception and decision-making. However, the introduction of these intelligent agents has led to security threats such as jailbreak attacks and adversarial attacks. In this research, we take a further step by proposing a backdoor attack specifically targeting robotic manipulation and, for the first time, implementing backdoor attack in the physical world. By embedding a backdoor visual language model into the visual perception module within the robotic system, we successfully mislead the robotic arm's operation in the physical world, given the presence of common items as triggers. Experimental evaluations in the physical world demonstrate the effectiveness of the proposed backdoor attack.         ",
    "url": "https://arxiv.org/abs/2411.11683",
    "authors": [
      "Xianlong Wang",
      "Hewen Pan",
      "Hangtao Zhang",
      "Minghui Li",
      "Shengshan Hu",
      "Ziqi Zhou",
      "Lulu Xue",
      "Peijin Guo",
      "Yichen Wang",
      "Wei Wan",
      "Aishan Liu",
      "Leo Yu Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11697",
    "title": "Robust Reinforcement Learning under Diffusion Models for Data with Jumps",
    "abstract": "           Reinforcement Learning (RL) has proven effective in solving complex decision-making tasks across various domains, but challenges remain in continuous-time settings, particularly when state dynamics are governed by stochastic differential equations (SDEs) with jump components. In this paper, we address this challenge by introducing the Mean-Square Bipower Variation Error (MSBVE) algorithm, which enhances robustness and convergence in scenarios involving significant stochastic noise and jumps. We first revisit the Mean-Square TD Error (MSTDE) algorithm, commonly used in continuous-time RL, and highlight its limitations in handling jumps in state dynamics. The proposed MSBVE algorithm minimizes the mean-square quadratic variation error, offering improved performance over MSTDE in environments characterized by SDEs with jumps. Simulations and formal proofs demonstrate that the MSBVE algorithm reliably estimates the value function in complex settings, surpassing MSTDE's performance when faced with jump processes. These findings underscore the importance of alternative error metrics to improve the resilience and effectiveness of RL algorithms in continuous-time frameworks.         ",
    "url": "https://arxiv.org/abs/2411.11697",
    "authors": [
      "Chenyang Jiang",
      "Donggyu Kim",
      "Alejandra Quintos",
      "Yazhen Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.11702",
    "title": "Bitcoin Under Volatile Block Rewards: How Mempool Statistics Can Influence Bitcoin Mining",
    "abstract": "           As Bitcoin experiences more halving events, the protocol reward converges to zero, making transaction fees the primary source of miner rewards. This shift in Bitcoin's incentivization mechanism, which introduces volatility into block rewards, could lead to the emergence of new security threats or intensify existing ones. Previous security analyses of Bitcoin have either considered a fixed block reward model or a highly simplified volatile model, overlooking the complexities of Bitcoin's mempool behavior. In this paper, we present a reinforcement learning-based tool designed to analyze mining strategies under a more realistic volatile model. Our tool uses the Asynchronous Advantage Actor-Critic (A3C) algorithm to derive near-optimal mining strategies while interacting with an environment that models the complexity of the Bitcoin mempool. This tool enables the analysis of adversarial mining strategies, such as selfish mining and undercutting, both before and after difficulty adjustments, providing insights into the effects of mining attacks in both the short and long term. Our analysis reveals that Bitcoin users' trend of offering higher fees to speed up the inclusion of their transactions in the chain can incentivize payoff-maximizing miners to deviate from the honest strategy. In the fixed reward model, a disincentive for the selfish mining attack is the initial loss period of at least two weeks, during which the attack is not profitable. However, our analysis shows that once the protocol reward diminishes to zero in the future, or even currently on days when transaction fees are comparable to the protocol reward, mining pools might be incentivized to abandon honest mining to gain an immediate profit.         ",
    "url": "https://arxiv.org/abs/2411.11702",
    "authors": [
      "Roozbeh Sarenche",
      "Alireza Aghabagherloo",
      "Svetla Nikova",
      "Bart Preneel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.11714",
    "title": "Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via Skill Library and Tactile Representation",
    "abstract": "           Deploying robots in open-world environments involves complex tasks characterized by long sequences and rich interactions, necessitating efficient transfer of robotic skills across diverse and complex scenarios. To address this challenge, we propose a skill library framework based on knowledge graphs, which endows robots with high-level skill awareness and spatial semantic understanding. The framework hierarchically organizes operational knowledge by constructing a \"task graph\" and a \"scene graph\" to represent task and scene semantic information, respectively. We introduce a \"state graph\" to facilitate interaction between high-level task planning and low-level scene information. Furthermore, we propose a hierarchical transfer framework for operational skills. At the task level, the framework integrates contextual learning and chain-of-thought prompting within a four-stage prompt paradigm, leveraging large language models' (LLMs) reasoning and generalization capabilities to achieve task-level subtask sequence transfer. At the motion level, an adaptive trajectory transfer method is developed using the A* algorithm and the skill library, enabling motion-level adaptive trajectory transfer. At the physical level, we introduce an adaptive contour extraction and posture perception method based on tactile perception. This method dynamically obtains high-precision contour and posture information from visual-tactile texture data and adjusts transferred skills, such as contact positions and postures, to ensure effectiveness in new environments. Experimental results validate the effectiveness of the proposed methods. Project website:this https URL ",
    "url": "https://arxiv.org/abs/2411.11714",
    "authors": [
      "Mingchao Qi",
      "Yuanjin Li",
      "Xing Liu",
      "Zhengxiong Liu",
      "Panfeng Huang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11718",
    "title": "Distributed Maximum Flow in Planar Graphs",
    "abstract": "           The dual of a planar graph $G$ is a planar graph $G^*$ that has a vertex for each face of $G$ and an edge for each pair of adjacent faces of $G$. The profound relationship between a planar graph and its dual has been the algorithmic basis for solving numerous (centralized) classical problems on planar graphs. In the distributed setting however, the only use of planar duality is for finding a recursive decomposition of $G$ [DISC 2017, STOC 2019]. We extend the distributed algorithmic toolkit to work on the dual graph $G^*$. These tools can then facilitate various algorithms on $G$ by solving a suitable dual problem on $G^*$. Given a directed planar graph $G$ with positive and negative edge-lengths and hop-diameter $D$, our key result is an $\\tilde{O}(D^2)$-round algorithm for Single Source Shortest Paths on $G^*$, which then implies an $\\tilde{O}(D^2)$-round algorithm for Maximum $st$-Flow on $G$. Prior to our work, no $\\tilde{O}(\\text{poly}(D))$-round algorithm was known for Maximum $st$-Flow. We further obtain a $D\\cdot n^{o(1)}$-rounds $(1-\\epsilon)$-approximation algorithm for Maximum $st$-Flow on $G$ when $G$ is undirected and $st$-planar. Finally, we give a near optimal $\\tilde O(D)$-round algorithm for computing the weighted girth of $G$. The main challenges in our work are that $G^*$ is not the communication graph (e.g., a vertex of $G$ is mapped to multiple vertices of $G^*$), and that the diameter of $G^*$ can be much larger than $D$ (i.e., possibly by a linear factor). We overcome these challenges by carefully defining and maintaining subgraphs of the dual graph $G^*$ while applying the recursive decomposition on the primal graph $G$. The main technical difficulty, is that along the recursive decomposition, a face of $G$ gets shattered into (disconnected) components yet we still need to treat it as a dual node.         ",
    "url": "https://arxiv.org/abs/2411.11718",
    "authors": [
      "Yaseen Abd-Elhaleem",
      "Michal Dory",
      "Merav Parter",
      "Oren Weimann"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2411.11730",
    "title": "Lifted Model Construction without Normalisation: A Vectorised Approach to Exploit Symmetries in Factor Graphs",
    "abstract": "           Lifted probabilistic inference exploits symmetries in a probabilistic model to allow for tractable probabilistic inference with respect to domain sizes of logical variables. We found that the current state-of-the-art algorithm to construct a lifted representation in form of a parametric factor graph misses symmetries between factors that are exchangeable but scaled differently, thereby leading to a less compact representation. In this paper, we propose a generalisation of the advanced colour passing (ACP) algorithm, which is the state of the art to construct a parametric factor graph. Our proposed algorithm allows for potentials of factors to be scaled arbitrarily and efficiently detects more symmetries than the original ACP algorithm. By detecting strictly more symmetries than ACP, our algorithm significantly reduces online query times for probabilistic inference when the resulting model is applied, which we also confirm in our experiments.         ",
    "url": "https://arxiv.org/abs/2411.11730",
    "authors": [
      "Malte Luttermann",
      "Ralf M\u00f6ller",
      "Marcel Gehrke"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11736",
    "title": "Advacheck at GenAI Detection Task 1: AI Detection Powered by Domain-Aware Multi-Tasking",
    "abstract": "           The paper describes a system designed by Advacheck team to recognise machine-generated and human-written texts in the monolingual subtask of GenAI Detection Task 1 competition. Our developed system is a multi-task architecture with shared Transformer Encoder between several classification heads. One head is responsible for binary classification between human-written and machine-generated texts, while the other heads are auxiliary multiclass classifiers for texts of different domains from particular datasets. As multiclass heads were trained to distinguish the domains presented in the data, they provide a better understanding of the samples. This approach led us to achieve the first place in the official ranking with 83.07% macro F1-score on the test set and bypass the baseline by 10%. We further study obtained system through ablation, error and representation analyses, finding that multi-task learning outperforms single-task mode and simultaneous tasks form a cluster structure in embeddings space.         ",
    "url": "https://arxiv.org/abs/2411.11736",
    "authors": [
      "German Gritsai",
      "Anastasia Voznyuk",
      "Ildar Khabutdinov",
      "Andrey Grabovoy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2411.11738",
    "title": "WoodYOLO: A Novel Object Detector for Wood Species Detection in Microscopic Images",
    "abstract": "           Wood species identification plays a crucial role in various industries, from ensuring the legality of timber products to advancing ecological conservation efforts. This paper introduces WoodYOLO, a novel object detection algorithm specifically designed for microscopic wood fiber analysis. Our approach adapts the YOLO architecture to address the challenges posed by large, high-resolution microscopy images and the need for high recall in localization of the cell type of interest (vessel elements). Our results show that WoodYOLO significantly outperforms state-of-the-art models, achieving performance gains of 12.9% and 6.5% in F2 score over YOLOv10 and YOLOv7, respectively. This improvement in automated wood cell type localization capabilities contributes to enhancing regulatory compliance, supporting sustainable forestry practices, and promoting biodiversity conservation efforts globally.         ",
    "url": "https://arxiv.org/abs/2411.11738",
    "authors": [
      "Lars Nieradzik",
      "Henrike Stephani",
      "J\u00f6rdis Sieburg-Rockel",
      "Stephanie Helmling",
      "Andrea Olbrich",
      "Stephanie Wrage",
      "Janis Keuper"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11764",
    "title": "Freezing of Gait Detection Using Gramian Angular Fields and Federated Learning from Wearable Sensors",
    "abstract": "           Freezing of gait (FOG) is a debilitating symptom of Parkinson's disease (PD) that impairs mobility and safety. Traditional detection methods face challenges due to intra and inter-patient variability, and most systems are tested in controlled settings, limiting their real-world applicability. Addressing these gaps, we present FOGSense, a novel FOG detection system designed for uncontrolled, free-living conditions. It uses Gramian Angular Field (GAF) transformations and federated deep learning to capture temporal and spatial gait patterns missed by traditional methods. We evaluated our FOGSense system using a public PD dataset, 'tdcsfog'. FOGSense improves accuracy by 10.4% over a single-axis accelerometer, reduces failure points compared to multi-sensor systems, and demonstrates robustness to missing values. The federated architecture allows personalized model adaptation and efficient smartphone synchronization during off-peak hours, making it effective for long-term monitoring as symptoms evolve. Overall, FOGSense achieves a 22.2% improvement in F1-score compared to state-of-the-art methods, along with enhanced sensitivity for FOG episode detection. Code is available: this https URL.         ",
    "url": "https://arxiv.org/abs/2411.11764",
    "authors": [
      "Shovito Barua Soumma",
      "S M Raihanul Alam",
      "Rudmila Rahman",
      "Umme Niraj Mahi",
      "Sayyed Mostafa Mostafavi",
      "Hassan Ghasemzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2411.11789",
    "title": "Resonance: Transaction Fees for Heterogeneous Computation",
    "abstract": "           Blockchain networks are facing increasingly heterogeneous computational demands, and in response, protocol designers have started building specialized infrastructure to supply that demand. This paper introduces Resoonance: a new kind of transaction fee mechanism that operates in a general two-sided marketplace setting with extreme preference heterogeneity on both sides of the market. We allow users submitting transactions to have arbitrary valuations for inclusion, nodes responsible for executing transactions to incur arbitrary net costs for executing any bundle, and further allow for arbitrary constraints in allocation validity. These constraints, for example, may range from representing an individual node's specialized hardware constraints to denoting the fact that transactions may not be executed in parallel across different nodes if they utilize the same part of the network's state. Transactions may even require multiple nodes for execution. Resonance's design utilizes competition among sophisticated brokers to find idiosyncratic prices. We show that at pure Nash equilibria, Resonance finds an efficient outcome and minimizes the need for strategization by users and nodes. It is also budget-balanced, individually rational for all parties, and computationally tractable.         ",
    "url": "https://arxiv.org/abs/2411.11789",
    "authors": [
      "Maryam Bahrani",
      "Naveen Durvasula"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2411.11791",
    "title": "Machine Learning-Assisted Distribution System Network Reconfiguration Problem",
    "abstract": "           High penetration from volatile renewable energy resources in the grid and the varying nature of loads raise the need for frequent line switching to ensure the efficient operation of electrical distribution networks. Operators must ensure maximum load delivery, reduced losses, and the operation between voltage limits. However, computations to decide the optimal feeder configuration are often computationally expensive and intractable, making it unfavorable for real-time operations. This is mainly due to the existence of binary variables in the network reconfiguration optimization problem. To tackle this issue, we have devised an approach that leverages machine learning techniques to reshape distribution networks featuring multiple substations. This involves predicting the substation responsible for serving each part of the network. Hence, it leaves simple and more tractable Optimal Power Flow problems to be solved. This method can produce accurate results in a significantly faster time, as demonstrated using the IEEE 37-bus distribution feeder. Compared to the traditional optimization-based approaches, a feasible solution is achieved approximately ten times faster for all the tested scenarios.         ",
    "url": "https://arxiv.org/abs/2411.11791",
    "authors": [
      "Richard Asiamah",
      "Yuqi Zhou",
      "Ahmed S. Zamzam"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.11826",
    "title": "LightFFDNets: Lightweight Convolutional Neural Networks for Rapid Facial Forgery Detection",
    "abstract": "           Accurate and fast recognition of forgeries is an issue of great importance in the fields of artificial intelligence, image processing and object detection. Recognition of forgeries of facial imagery is the process of classifying and defining the faces in it by analyzing real-world facial images. This process is usually accomplished by extracting features from an image, using classifier algorithms, and correctly interpreting the results. Recognizing forgeries of facial imagery correctly can encounter many different challenges. For example, factors such as changing lighting conditions, viewing faces from different angles can affect recognition performance, and background complexity and perspective changes in facial images can make accurate recognition difficult. Despite these difficulties, significant progress has been made in the field of forgery detection. Deep learning algorithms, especially Convolutional Neural Networks (CNNs), have significantly improved forgery detection performance. This study focuses on image processing-based forgery detection using Fake-Vs-Real-Faces (Hard) [10] and 140k Real and Fake Faces [61] data sets. Both data sets consist of two classes containing real and fake facial images. In our study, two lightweight deep learning models are proposed to conduct forgery detection using these images. Additionally, 8 different pretrained CNN architectures were tested on both data sets and the results were compared with newly developed lightweight CNN models. It's shown that the proposed lightweight deep learning models have minimum number of layers. It's also shown that the proposed lightweight deep learning models detect forgeries of facial imagery accurately, and computationally efficiently. Although the data set consists only of face images, the developed models can also be used in other two-class object recognition problems.         ",
    "url": "https://arxiv.org/abs/2411.11826",
    "authors": [
      "G\u00fcnel Jabbarl\u0131",
      "Murat Kurt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.11829",
    "title": "Tackling prediction tasks in relational databases with LLMs",
    "abstract": "           Though large language models (LLMs) have demonstrated exceptional performance across numerous problems, their application to predictive tasks in relational databases remains largely unexplored. In this work, we address the notion that LLMs cannot yield satisfactory results on relational databases due to their interconnected tables, complex relationships, and heterogeneous data types. Using the recently introduced RelBench benchmark, we demonstrate that even a straightforward application of LLMs achieves competitive performance on these tasks. These findings establish LLMs as a promising new baseline for ML on relational databases and encourage further research in this direction.         ",
    "url": "https://arxiv.org/abs/2411.11829",
    "authors": [
      "Marek Wydmuch",
      "\u0141ukasz Borchmann",
      "Filip Grali\u0144ski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2411.10458",
    "title": "Neural decoding from stereotactic EEG: accounting for electrode variability across subjects",
    "abstract": "           Deep learning based neural decoding from stereotactic electroencephalography (sEEG) would likely benefit from scaling up both dataset and model size. To achieve this, combining data across multiple subjects is crucial. However, in sEEG cohorts, each subject has a variable number of electrodes placed at distinct locations in their brain, solely based on clinical needs. Such heterogeneity in electrode number/placement poses a significant challenge for data integration, since there is no clear correspondence of the neural activity recorded at distinct sites between individuals. Here we introduce seegnificant: a training framework and architecture that can be used to decode behavior across subjects using sEEG data. We tokenize the neural activity within electrodes using convolutions and extract long-term temporal dependencies between tokens using self-attention in the time dimension. The 3D location of each electrode is then mixed with the tokens, followed by another self-attention in the electrode dimension to extract effective spatiotemporal neural representations. Subject-specific heads are then used for downstream decoding tasks. Using this approach, we construct a multi-subject model trained on the combined data from 21 subjects performing a behavioral task. We demonstrate that our model is able to decode the trial-wise response time of the subjects during the behavioral task solely from neural data. We also show that the neural representations learned by pretraining our model across individuals can be transferred in a few-shot manner to new subjects. This work introduces a scalable approach towards sEEG data integration for multi-subject model training, paving the way for cross-subject generalization for sEEG decoding.         ",
    "url": "https://arxiv.org/abs/2411.10458",
    "authors": [
      "Georgios Mentzelopoulos",
      "Evangelos Chatzipantazis",
      "Ashwin G. Ramayya",
      "Michelle J. Hedlund",
      "Vivek P. Buch",
      "Kostas Daniilidis",
      "Konrad P. Kording",
      "Flavia Vitale"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2411.10596",
    "title": "A minimalistic representation model for head direction system",
    "abstract": "           We present a minimalistic representation model for the head direction (HD) system, aiming to learn a high-dimensional representation of head direction that captures essential properties of HD cells. Our model is a representation of rotation group $U(1)$, and we study both the fully connected version and convolutional version. We demonstrate the emergence of Gaussian-like tuning profiles and a 2D circle geometry in both versions of the model. We also demonstrate that the learned model is capable of accurate path integration.         ",
    "url": "https://arxiv.org/abs/2411.10596",
    "authors": [
      "Minglu Zhao",
      "Dehong Xu",
      "Deqian Kong",
      "Wen-Hao Zhang",
      "Ying Nian Wu"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.10831",
    "title": "Neighboring Slice Noise2Noise: Self-Supervised Medical Image Denoising from Single Noisy Image Volume",
    "abstract": "           In the last few years, with the rapid development of deep learning technologies, supervised methods based on convolutional neural networks have greatly enhanced the performance of medical image denoising. However, these methods require large quantities of noisy-clean image pairs for training, which greatly limits their practicality. Although some researchers have attempted to train denoising networks using only single noisy images, existing self-supervised methods, including blind-spot-based and data-splitting-based methods, heavily rely on the assumption that noise is pixel-wise independent. However, this assumption often does not hold in real-world medical images. Therefore, in the field of medical imaging, there remains a lack of simple and practical denoising methods that can achieve high-quality denoising performance using only single noisy images. In this paper, we propose a novel self-supervised medical image denoising method, Neighboring Slice Noise2Noise (NS-N2N). The proposed method utilizes neighboring slices within a single noisy image volume to construct weighted training data, and then trains the denoising network using a self-supervised scheme with regional consistency loss and inter-slice continuity loss. NS-N2N only requires a single noisy image volume obtained from one medical imaging procedure to achieve high-quality denoising of the image volume itself. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art self-supervised denoising methods in both denoising performance and processing efficiency. Furthermore, since NS-N2N operates solely in the image domain, it is free from device-specific issues such as reconstruction geometry, making it easier to apply in various clinical practices.         ",
    "url": "https://arxiv.org/abs/2411.10831",
    "authors": [
      "Langrui Zhou",
      "Ziteng Zhou",
      "Xinyu Huang",
      "Xiangyu Zhang",
      "Huiru Wang",
      "Guang Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10843",
    "title": "A Novel Adaptive Hybrid Focal-Entropy Loss for Enhancing Diabetic Retinopathy Detection Using Convolutional Neural Networks",
    "abstract": "           Diabetic retinopathy is a leading cause of blindness around the world and demands precise AI-based diagnostic tools. Traditional loss functions in multi-class classification, such as Categorical Cross-Entropy (CCE), are very common but break down with class imbalance, especially in cases with inherently challenging or overlapping classes, which leads to biased and less sensitive models. Since a heavy imbalance exists in the number of examples for higher severity stage 4 diabetic retinopathy, etc., classes compared to those very early stages like class 0, achieving class balance is key. For this purpose, we propose the Adaptive Hybrid Focal-Entropy Loss which combines the ideas of focal loss and entropy loss with adaptive weighting in order to focus on minority classes and highlight the challenging samples. The state-of-the art models applied for diabetic retinopathy detection with AHFE revealed good performance improvements, indicating the top performances of ResNet50 at 99.79%, DenseNet121 at 98.86%, Xception at 98.92%, MobileNetV2 at 97.84%, and InceptionV3 at 93.62% accuracy. This sheds light into how AHFE promotes enhancement in AI-driven diagnostics for complex and imbalanced medical datasets.         ",
    "url": "https://arxiv.org/abs/2411.10843",
    "authors": [
      "Pandiyaraju V",
      "Santhosh Malarvannan",
      "Shravan Venkatraman",
      "Abeshek A",
      "Priyadarshini B",
      "Kannan A"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.10908",
    "title": "The Conflict Graph Design: Estimating Causal Effects under Arbitrary Neighborhood Interference",
    "abstract": "           A fundamental problem in network experiments is selecting an appropriate experimental design in order to precisely estimate a given causal effect of interest. In fact, optimal rates of estimation remain unknown for essentially all causal effects in network experiments. In this work, we propose a general approach for constructing experiment designs under network interference with the goal of precisely estimating a pre-specified causal effect. A central aspect of our approach is the notion of a conflict graph, which captures the fundamental unobservability associated with the casual effect and the underlying network. We refer to our experimental design as the Conflict Graph Design. In order to estimate effects, we propose a modified Horvitz--Thompson estimator. We show that its variance under the Conflict Graph Design is bounded as $O(\\lambda(H) / n )$, where $\\lambda(H)$ is the largest eigenvalue of the adjacency matrix of the conflict graph. These rates depend on both the underlying network and the particular causal effect under investigation. Not only does this yield the best known rates of estimation for several well-studied causal effects (e.g. the global and direct effects) but it also provides new methods for effects which have received less attention from the perspective of experiment design (e.g. spill-over effects). Our results corroborate two implicitly understood points in the literature: (1) that in order to increase precision, experiment designs should be tailored to specific causal effects of interest and (2) that \"more local\" effects are easier to estimate than \"more global\" effects. In addition to point estimation, we construct conservative variance estimators which facilitate the construction of asymptotically valid confidence intervals for the casual effect of interest.         ",
    "url": "https://arxiv.org/abs/2411.10908",
    "authors": [
      "Vardis Kandiros",
      "Charilaos Pipis",
      "Constantinos Daskalakis",
      "Christopher Harshaw"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Data Structures and Algorithms (cs.DS)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2411.10941",
    "title": "Efficient Estimation of Relaxed Model Parameters for Robust UAV Trajectory Optimization",
    "abstract": "           Online trajectory optimization and optimal control methods are crucial for enabling sustainable unmanned aerial vehicle (UAV) services, such as agriculture, environmental monitoring, and transportation, where available actuation and energy are limited. However, optimal controllers are highly sensitive to model mismatch, which can occur due to loaded equipment, packages to be delivered, or pre-existing variability in fundamental structural and thrust-related parameters. To circumvent this problem, optimal controllers can be paired with parameter estimators to improve their trajectory planning performance and perform adaptive control. However, UAV platforms are limited in terms of onboard processing power, oftentimes making nonlinear parameter estimation too computationally expensive to consider. To address these issues, we propose a relaxed, affine-in-parameters multirotor model along with an efficient optimal parameter estimator. We convexify the nominal Moving Horizon Parameter Estimation (MHPE) problem into a linear-quadratic form (LQ-MHPE) via an affine-in-parameter relaxation on the nonlinear dynamics, resulting in fast quadratic programs (QPs) that facilitate adaptive Model Predictve Control (MPC) in real time. We compare this approach to the equivalent nonlinear estimator in Monte Carlo simulations, demonstrating a decrease in average solve time and trajectory optimality cost by 98.2% and 23.9-56.2%, respectively.         ",
    "url": "https://arxiv.org/abs/2411.10941",
    "authors": [
      "D. Fan",
      "D. A. Copp"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.11116",
    "title": "DBF-Net: A Dual-Branch Network with Feature Fusion for Ultrasound Image Segmentation",
    "abstract": "           Accurately segmenting lesions in ultrasound images is challenging due to the difficulty in distinguishing boundaries between lesions and surrounding tissues. While deep learning has improved segmentation accuracy, there is limited focus on boundary quality and its relationship with body structures. To address this, we introduce UBBS-Net, a dual-branch deep neural network that learns the relationship between body and boundary for improved segmentation. We also propose a feature fusion module to integrate body and boundary information. Evaluated on three public datasets, UBBS-Net outperforms existing methods, achieving Dice Similarity Coefficients of 81.05% for breast cancer, 76.41% for brachial plexus nerves, and 87.75% for infantile hemangioma segmentation. Our results demonstrate the effectiveness of UBBS-Net for ultrasound image segmentation. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.11116",
    "authors": [
      "Guoping Xu",
      "Ximing Wu",
      "Wentao Liao",
      "Xinglong Wu",
      "Qing Huang",
      "Chang Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11132",
    "title": "Variational Bayesian Bow tie Neural Networks with Shrinkage",
    "abstract": "           Despite the dominant role of deep models in machine learning, limitations persist, including overconfident predictions, susceptibility to adversarial attacks, and underestimation of variability in predictions. The Bayesian paradigm provides a natural framework to overcome such issues and has become the gold standard for uncertainty estimation with deep models, also providing improved accuracy and a framework for tuning critical hyperparameters. However, exact Bayesian inference is challenging, typically involving variational algorithms that impose strong independence and distributional assumptions. Moreover, existing methods are sensitive to the architectural choice of the network. We address these issues by constructing a relaxed version of the standard feed-forward rectified neural network, and employing Polya-Gamma data augmentation tricks to render a conditionally linear and Gaussian model. Additionally, we use sparsity-promoting priors on the weights of the neural network for data-driven architectural design. To approximate the posterior, we derive a variational inference algorithm that avoids distributional assumptions and independence across layers and is a faster alternative to the usual Markov Chain Monte Carlo schemes.         ",
    "url": "https://arxiv.org/abs/2411.11132",
    "authors": [
      "Alisa Sheinkman",
      "Sara Wade"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2411.11191",
    "title": "Accelerating Quantum Emitter Characterization with Latent Neural Ordinary Differential Equations",
    "abstract": "           Deep neural network models can be used to learn complex dynamics from data and reconstruct sparse or noisy signals, thereby accelerating and augmenting experimental measurements. Evaluating the quantum optical properties of solid-state single-photon emitters is a time-consuming task that typically requires interferometric photon correlation experiments, such as Photon correlation Fourier spectroscopy (PCFS) which measures time-resolved single emitter lineshapes. Here, we demonstrate a latent neural ordinary differential equation model that can forecast a complete and noise-free PCFS experiment from a small subset of noisy correlation functions. By encoding measured photon correlations into an initial value problem, the NODE can be propagated to an arbitrary number of interferometer delay times. We demonstrate this with 10 noisy photon correlation functions that are used to extrapolate an entire de-noised interferograms of up to 200 stage positions, enabling up to a 20-fold speedup in experimental acquisition time from $\\sim$3 hours to 10 minutes. Our work presents a new approach to greatly accelerate the experimental characterization of novel quantum emitter materials using deep learning.         ",
    "url": "https://arxiv.org/abs/2411.11191",
    "authors": [
      "Andrew H. Proppe",
      "Kin Long Kelvin Lee",
      "Weiwei Sun",
      "Chantalle J. Krajewska",
      "Oliver Tye",
      "Moungi G. Bawendi"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11282",
    "title": "Continuous K-space Recovery Network with Image Guidance for Fast MRI Reconstruction",
    "abstract": "           Magnetic resonance imaging (MRI) is a crucial tool for clinical diagnosis while facing the challenge of long scanning time. To reduce the acquisition time, fast MRI reconstruction aims to restore high-quality images from the undersampled k-space. Existing methods typically train deep learning models to map the undersampled data to artifact-free MRI images. However, these studies often overlook the unique properties of k-space and directly apply general networks designed for image processing to k-space recovery, leaving the precise learning of k-space largely underexplored. In this work, we propose a continuous k-space recovery network from a new perspective of implicit neural representation with image domain guidance, which boosts the performance of MRI reconstruction. Specifically, (1) an implicit neural representation based encoder-decoder structure is customized to continuously query unsampled k-values. (2) an image guidance module is designed to mine the semantic information from the low-quality MRI images to further guide the k-space recovery. (3) a multi-stage training strategy is proposed to recover dense k-space progressively. Extensive experiments conducted on CC359, fastMRI, and IXI datasets demonstrate the effectiveness of our method and its superiority over other competitors.         ",
    "url": "https://arxiv.org/abs/2411.11282",
    "authors": [
      "Yucong Meng",
      "Zhiwei Yang",
      "Minghong Duan",
      "Yonghong Shi",
      "Zhijian Song"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11348",
    "title": "Modeling Multivariable High-resolution 3D Urban Microclimate Using Localized Fourier Neural Operator",
    "abstract": "           Accurate urban microclimate analysis with wind velocity and temperature is vital for energy-efficient urban planning, supporting carbon reduction, enhancing public health and comfort, and advancing the low-altitude economy. However, traditional computational fluid dynamics (CFD) simulations that couple velocity and temperature are computationally expensive. Recent machine learning advancements offer promising alternatives for accelerating urban microclimate simulations. The Fourier neural operator (FNO) has shown efficiency and accuracy in predicting single-variable velocity magnitudes in urban wind fields. Yet, for multivariable high-resolution 3D urban microclimate prediction, FNO faces three key limitations: blurry output quality, high GPU memory demand, and substantial data requirements. To address these issues, we propose a novel localized Fourier neural operator (Local-FNO) model that employs local training, geometry encoding, and patch overlapping. Local-FNO provides accurate predictions for rapidly changing turbulence in urban microclimate over 60 seconds, four times the average turbulence integral time scale, with an average error of 0.35 m/s in velocity and 0.30 \u00b0C in temperature. It also accurately captures turbulent heat flux represented by the velocity-temperature correlation. In a 2 km by 2 km domain, Local-FNO resolves turbulence patterns down to a 10 m resolution. It provides high-resolution predictions with 150 million feature dimensions on a single 32 GB GPU at nearly 50 times the speed of a CFD solver. Compared to FNO, Local-FNO achieves a 23.9% reduction in prediction error and a 47.3% improvement in turbulent fluctuation correlation.         ",
    "url": "https://arxiv.org/abs/2411.11348",
    "authors": [
      "Shaoxiang Qin",
      "Dongxue Zhan",
      "Dingyang Geng",
      "Wenhui Peng",
      "Geng Tian",
      "Yurong Shi",
      "Naiping Gao",
      "Xue Liu",
      "Liangzhu Leon Wang"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11376",
    "title": "Lung Disease Detection with Vision Transformers: A Comparative Study of Machine Learning Methods",
    "abstract": "           Recent advancements in medical image analysis have predominantly relied on Convolutional Neural Networks (CNNs), achieving impressive performance in chest X-ray classification tasks, such as the 92% AUC reported by AutoThorax-Net and the 88% AUC achieved by ChexNet in classifcation tasks. However, in the medical field, even small improvements in accuracy can have significant clinical implications. This study explores the application of Vision Transformers (ViT), a state-of-the-art architecture in machine learning, to chest X-ray analysis, aiming to push the boundaries of diagnostic accuracy. I present a comparative analysis of two ViT-based approaches: one utilizing full chest X-ray images and another focusing on segmented lung regions. Experiments demonstrate that both methods surpass the performance of traditional CNN-based models, with the full-image ViT achieving up to 97.83% accuracy and the lung-segmented ViT reaching 96.58% accuracy in classifcation of diseases on three label and AUC of 94.54% when label numbers are increased to eight. Notably, the full-image approach showed superior performance across all metrics, including precision, recall, F1 score, and AUC-ROC. These findings suggest that Vision Transformers can effectively capture relevant features from chest X-rays without the need for explicit lung segmentation, potentially simplifying the preprocessing pipeline while maintaining high accuracy. This research contributes to the growing body of evidence supporting the efficacy of transformer-based architectures in medical image analysis and highlights their potential to enhance diagnostic precision in clinical settings.         ",
    "url": "https://arxiv.org/abs/2411.11376",
    "authors": [
      "Baljinnyam Dayan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11464",
    "title": "PALMS: Parallel Adaptive Lasso with Multi-directional Signals for Latent Networks Reconstruction",
    "abstract": "           Large-scale networks exist in many field and play an important role in real-world dynamics. However, the networks are usually latent and expensive to detect, which becomes the main challenging for many applications and empirical analysis. Several statistical methods were proposed to infer the edges, but the complexity of algorithms make them hard to be applied for large-scale networks. In this paper, we proposed a general distributed and parallel computing framework for network reconstruction methods via compressive sensing technical, to make them feasible for inferring the super large networks in practice. Combining with the CALMS, we proposed for those estimators enjoy additional theoretical properties, such as the consistency and asymptotic normality, we prove that the approximate estimation utilizing the distributed algorithm can keep the theoretical results.         ",
    "url": "https://arxiv.org/abs/2411.11464",
    "authors": [
      "Zhaoyu Xing",
      "Wei Zhong"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.11542",
    "title": "Data-Driven Structured Robust Control of Linear Systems",
    "abstract": "           Static structured control refers to the task of designing a state-feedback controller such that the control gain satisfies a subspace constraint. Structured control has applications in control of communication-inhibited dynamical systems, such as systems in networked environments. This work performs $H_2$-suboptimal regulation under a common structured state-feedback controller for a class of data-consistent plants. The certification of $H_2$-performance is attained through a combination of standard $H_2$ LMIs, convex sufficient conditions for structured control, and a matrix S-lemma for set-membership. The resulting convex optimization problems are linear matrix inequalities whose size scales independently of the number of data samples collected. Data-driven structured $H_2$-regulation control is demonstrated on example systems.         ",
    "url": "https://arxiv.org/abs/2411.11542",
    "authors": [
      "Jared Miller",
      "Jaap Eising",
      "Florian D\u00f6rfler",
      "Roy S. Smith"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.11576",
    "title": "Hybrid Data-Driven SSM for Interpretable and Label-Free mmWave Channel Prediction",
    "abstract": "           Accurate prediction of mmWave time-varying channels is essential for mitigating the issue of channel aging in complex scenarios owing to high user mobility. Existing channel prediction methods have limitations: classical model-based methods often struggle to track highly nonlinear channel dynamics due to limited expert knowledge, while emerging data-driven methods typically require substantial labeled data for effective training and often lack interpretability. To address these issues, this paper proposes a novel hybrid method that integrates a data-driven neural network into a conventional model-based workflow based on a state-space model (SSM), implicitly tracking complex channel dynamics from data without requiring precise expert knowledge. Additionally, a novel unsupervised learning strategy is developed to train the embedded neural network solely with unlabeled data. Theoretical analyses and ablation studies are conducted to interpret the enhanced benefits gained from the hybrid integration. Numerical simulations based on the 3GPP mmWave channel model corroborate the superior prediction accuracy of the proposed method, compared to state-of-the-art methods that are either purely model-based or data-driven. Furthermore, extensive experiments validate its robustness against various challenging factors, including among others severe channel variations and high noise levels.         ",
    "url": "https://arxiv.org/abs/2411.11576",
    "authors": [
      "Yiyong Sun",
      "Jiajun He",
      "Zhidi Lin",
      "Wenqiang Pu",
      "Feng Yin",
      "Hing Cheung So"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11590",
    "title": "Robust Causal Analysis of Linear Cyclic Systems With Hidden Confounders",
    "abstract": "           We live in a world full of complex systems which we need to improve our understanding of. To accomplish this, purely probabilistic investigations are often not enough. They are only the first step and must be followed by learning the system's underlying mechanisms. This is what the discipline of causality is concerned with. Many of those complex systems contain feedback loops which means that our methods have to allow for cyclic causal relations. Furthermore, systems are rarely sufficiently isolated, which means that there are usually hidden confounders, i.e., unmeasured variables that each causally affects more than one measured variable. Finally, data is often distorted by contaminating processes, and we need to apply methods that are robust against such distortions. That's why we consider the robustness of LLC, see \\cite{llc}, one of the few causal analysis methods that can deal with cyclic models with hidden confounders. Following a theoretical analysis of LLC's robustness properties, we also provide robust extensions of LLC. To facilitate reproducibility and further research in this field, we make the source code publicly available.         ",
    "url": "https://arxiv.org/abs/2411.11590",
    "authors": [
      "Boris Lorbeer"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11598",
    "title": "Carleman-Fourier Linearization of Complex Dynamical Systems: Convergence and Explicit Error Bounds",
    "abstract": "           This paper presents a Carleman-Fourier linearization method for nonlinear dynamical systems with periodic vector fields involving multiple fundamental frequencies. By employing Fourier basis functions, the nonlinear dynamical system is transformed into a linear model on an infinite-dimensional space. The proposed approach yields accurate approximations over extended regions around equilibria and for longer time horizons, compared to traditional Carleman linearization with monomials. Additionally, we develop a finite-section approximation for the resulting infinite-dimensional system and provide explicit error bounds that demonstrate exponential convergence to the original system's solution as the truncation length increases. For specific classes of dynamical systems, exponential convergence is achieved across the entire time horizon. The practical significance of these results lies in guiding the selection of suitable truncation lengths for applications such as model predictive control, safety verification through reachability analysis, and efficient quantum computing algorithms. The theoretical findings are validated through illustrative simulations.         ",
    "url": "https://arxiv.org/abs/2411.11598",
    "authors": [
      "Panpan Chen",
      "Nader Motee",
      "Qiyu Sun"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.11682",
    "title": "Learning Differentiable Surrogate Losses for Structured Prediction",
    "abstract": "           Structured prediction involves learning to predict complex structures rather than simple scalar values. The main challenge arises from the non-Euclidean nature of the output space, which generally requires relaxing the problem formulation. Surrogate methods build on kernel-induced losses or more generally, loss functions admitting an Implicit Loss Embedding, and convert the original problem into a regression task followed by a decoding step. However, designing effective losses for objects with complex structures presents significant challenges and often requires domain-specific expertise. In this work, we introduce a novel framework in which a structured loss function, parameterized by neural networks, is learned directly from output training data through Contrastive Learning, prior to addressing the supervised surrogate regression problem. As a result, the differentiable loss not only enables the learning of neural networks due to the finite dimension of the surrogate space but also allows for the prediction of new structures of the output data via a decoding strategy based on gradient descent. Numerical experiments on supervised graph prediction problems show that our approach achieves similar or even better performance than methods based on a pre-defined kernel.         ",
    "url": "https://arxiv.org/abs/2411.11682",
    "authors": [
      "Junjie Yang",
      "Matthieu Labeau",
      "Florence d'Alch\u00e9-Buc"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11786",
    "title": "Parallelly Tempered Generative Adversarial Networks",
    "abstract": "           A generative adversarial network (GAN) has been a representative backbone model in generative artificial intelligence (AI) because of its powerful performance in capturing intricate data-generating processes. However, the GAN training is well-known for its notorious training instability, usually characterized by the occurrence of mode collapse. Through the lens of gradients' variance, this work particularly analyzes the training instability and inefficiency in the presence of mode collapse by linking it to multimodality in the target distribution. To ease the raised training issues from severe multimodality, we introduce a novel GAN training framework that leverages a series of tempered distributions produced via convex interpolation. With our newly developed GAN objective function, the generator can learn all the tempered distributions simultaneously, conceptually resonating with the parallel tempering in Statistics. Our simulation studies demonstrate the superiority of our approach over existing popular training strategies in both image and tabular data synthesis. We theoretically analyze that such significant improvement can arise from reducing the variance of gradient estimates by using the tempered distributions. Finally, we further develop a variant of the proposed framework aimed at generating fair synthetic data which is one of the growing interests in the field of trustworthy AI.         ",
    "url": "https://arxiv.org/abs/2411.11786",
    "authors": [
      "Jinwon Sohn",
      "Qifan Song"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11795",
    "title": "Exploring adversarial robustness of JPEG AI: methodology, comparison and new methods",
    "abstract": "           Adversarial robustness of neural networks is an increasingly important area of research, combining studies on computer vision models, large language models (LLMs), and others. With the release of JPEG AI - the first standard for end-to-end neural image compression (NIC) methods - the question of its robustness has become critically significant. JPEG AI is among the first international, real-world applications of neural-network-based models to be embedded in consumer devices. However, research on NIC robustness has been limited to open-source codecs and a narrow range of attacks. This paper proposes a new methodology for measuring NIC robustness to adversarial attacks. We present the first large-scale evaluation of JPEG AI's robustness, comparing it with other NIC models. Our evaluation results and code are publicly available online (link is hidden for a blind review).         ",
    "url": "https://arxiv.org/abs/2411.11795",
    "authors": [
      "Egor Kovalev",
      "Georgii Bychkov",
      "Khaled Abud",
      "Aleksandr Gushchin",
      "Anna Chistyakova",
      "Sergey Lavrushkin",
      "Dmitriy Vatolin",
      "Anastasia Antsiferova"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11799",
    "title": "Edge-Enhanced Dilated Residual Attention Network for Multimodal Medical Image Fusion",
    "abstract": "           Multimodal medical image fusion is a crucial task that combines complementary information from different imaging modalities into a unified representation, thereby enhancing diagnostic accuracy and treatment planning. While deep learning methods, particularly Convolutional Neural Networks (CNNs) and Transformers, have significantly advanced fusion performance, some of the existing CNN-based methods fall short in capturing fine-grained multiscale and edge features, leading to suboptimal feature integration. Transformer-based models, on the other hand, are computationally intensive in both the training and fusion stages, making them impractical for real-time clinical use. Moreover, the clinical application of fused images remains unexplored. In this paper, we propose a novel CNN-based architecture that addresses these limitations by introducing a Dilated Residual Attention Network Module for effective multiscale feature extraction, coupled with a gradient operator to enhance edge detail learning. To ensure fast and efficient fusion, we present a parameter-free fusion strategy based on the weighted nuclear norm of softmax, which requires no additional computations during training or inference. Extensive experiments, including a downstream brain tumor classification task, demonstrate that our approach outperforms various baseline methods in terms of visual quality, texture preservation, and fusion speed, making it a possible practical solution for real-world clinical applications. The code will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.11799",
    "authors": [
      "Meng Zhou",
      "Yuxuan Zhang",
      "Xiaolan Xu",
      "Jiayi Wang",
      "Farzad Khalvati"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11805",
    "title": "On the hardness of cloning and connections to representation theory",
    "abstract": "           The states accepted by a quantum circuit are known as the witnesses for the quantum circuit's satisfiability. The assumption BQP does not equal QMA implies that no efficient algorithm exists for constructing a witness for a quantum circuit from the circuit's classical description. However, a similar complexity-theoretic lower bound on the computational hardness of cloning a witness is not known. In this note, we derive a conjecture about cloning algorithms for maximally entangled states over hidden subspaces which would imply that no efficient algorithm exists for cloning witnesses (assuming BQP does not contain NP). The conjecture and result follow from connections between quantum computation and representation theory; specifically, the relationship between quantum state complexity and the complexity of computing Kronecker coefficients.         ",
    "url": "https://arxiv.org/abs/2411.11805",
    "authors": [
      "Vojt\u011bch Havl\u00ed\u010dek",
      "Chinmay Nirkhe"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2411.11819",
    "title": "Equivariant spatio-hemispherical networks for diffusion MRI deconvolution",
    "abstract": "           Each voxel in a diffusion MRI (dMRI) image contains a spherical signal corresponding to the direction and strength of water diffusion in the brain. This paper advances the analysis of such spatio-spherical data by developing convolutional network layers that are equivariant to the $\\mathbf{E(3) \\times SO(3)}$ group and account for the physical symmetries of dMRI including rotations, translations, and reflections of space alongside voxel-wise rotations. Further, neuronal fibers are typically antipodally symmetric, a fact we leverage to construct highly efficient spatio-hemispherical graph convolutions to accelerate the analysis of high-dimensional dMRI data. In the context of sparse spherical fiber deconvolution to recover white matter microstructure, our proposed equivariant network layers yield substantial performance and efficiency gains, leading to better and more practical resolution of crossing neuronal fibers and fiber tractography. These gains are experimentally consistent across both simulation and in vivo human datasets.         ",
    "url": "https://arxiv.org/abs/2411.11819",
    "authors": [
      "Axel Elaldi",
      "Guido Gerig",
      "Neel Dey"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:1403.3369",
    "title": "Controlling Recurrent Neural Networks by Conceptors",
    "abstract": "           The human brain is a dynamical system whose extremely complex sensor-driven neural processes give rise to conceptual, logical cognition. Understanding the interplay between nonlinear neural dynamics and concept-level cognition remains a major scientific challenge. Here I propose a mechanism of neurodynamical organization, called conceptors, which unites nonlinear dynamics with basic principles of conceptual abstraction and logic. It becomes possible to learn, store, abstract, focus, morph, generalize, de-noise and recognize a large number of dynamical patterns within a single neural system; novel patterns can be added without interfering with previously acquired ones; neural noise is automatically filtered. Conceptors help explaining how conceptual-level information processing emerges naturally and robustly in neural systems, and remove a number of roadblocks in the theory and applications of recurrent neural networks.         ",
    "url": "https://arxiv.org/abs/1403.3369",
    "authors": [
      "Herbert Jaeger"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2002.04990",
    "title": "Periodicity Pitch Detection in Complex Harmonies on EEG Timeline Data",
    "abstract": "           An acoustic stimulus, e.g., a musical harmony, is transformed in a highly non-linear way during the hearing process in ear and brain. We study this by comparing the frequency spectrum of an input stimulus and its response spectrum in the auditory processing stream using the frequency following response (FFR). Using electroencephalography (EEG), we investigate whether the periodicity pitches of complex harmonies (which are related to their missing fundamentals) are added in the auditory brainstem by analyzing the FFR. While other experiments focus on common musical harmonies like the major and the minor triad and dyads, we also consider the suspended chord. The suspended chord causes tension foreign to the common triads and therefore holds a special role among the triads. While watching a muted nature documentary, the participants hear synthesized classic piano triads and single tones with a duration of 300ms for the stimulus and 100ms interstimulus interval. We acquired EEG data of 64 electrodes with a sampling rate of 5kHz to get a detailed enough resolution of the perception process in the human brain. Applying a fast Fourier transformation (FFT) on the EEG response, starting 50ms after stimulus onset, the evaluation of the frequency spectra shows that the periodicity pitch frequencies calculated beforehand +/-3Hz occur with some accuracy. However, jitter turned out as a problem here. Note that the sought-for periodicity pitch frequencies do not physically exist in the frequency spectra of the stimuli.         ",
    "url": "https://arxiv.org/abs/2002.04990",
    "authors": [
      "Maria Heinze",
      "Lars Hausfeld",
      "Rainer Goebel",
      "Frieder Stolzenburg"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2205.10952",
    "title": "Generalization ability and Vulnerabilities to adversarial perturbations: Two sides of the same coin",
    "abstract": "           Deep neural networks (DNNs), the agents of deep learning (DL), require a massive number of parallel/sequential operations, which makes it difficult to comprehend them and impedes proper diagnosis. Without better knowledge of DNNs' internal process, deploying DNNs in high-stakes domains may lead to catastrophic failures. Therefore, to build more reliable DNNs/DL, it is imperative that we gain insights into their underlying decision-making process. Here, we use the self-organizing map (SOM) to analyze DL models' internal codes associated with DNNs' decision-making. Our analyses suggest that shallow layers close to the input layer map onto homogeneous codes and that deep layers close to the output layer transform these homogeneous codes in shallow layers to diverse codes. We also found evidence indicating that homogeneous codes may underlie DNNs' vulnerabilities to adversarial perturbations.         ",
    "url": "https://arxiv.org/abs/2205.10952",
    "authors": [
      "Jung Hoon Lee",
      "Sujith Vijayan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.05276",
    "title": "Game-Theoretic Neyman-Pearson Detection to Combat Strategic Evasion",
    "abstract": "           The security in networked systems depends greatly on recognizing and identifying adversarial behaviors. Traditional detection methods focus on specific categories of attacks and have become inadequate for increasingly stealthy and deceptive attacks that are designed to bypass detection strategically. This work aims to develop a holistic theory to countermeasure such evasive attacks. We focus on extending a fundamental class of statistical-based detection methods based on Neyman-Pearson's (NP) hypothesis testing formulation. We propose game-theoretic frameworks to capture the conflicting relationship between a strategic evasive attacker and an evasion-aware NP detector. By analyzing both the equilibrium behaviors of the attacker and the NP detector, we characterize their performance using Equilibrium Receiver-Operational-Characteristic (EROC) curves. We show that the evasion-aware NP detectors outperform the passive ones in the way that the former can act strategically against the attacker's behavior and adaptively modify their decision rules based on the received messages. In addition, we extend our framework to a sequential setting where the user sends out identically distributed messages. We corroborate the analytical results with a case study of anomaly detection.         ",
    "url": "https://arxiv.org/abs/2206.05276",
    "authors": [
      "Yinan Hu",
      "Quanyan Zhu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Science and Game Theory (cs.GT)",
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2211.10285",
    "title": "A Fair Loss Function for Network Pruning",
    "abstract": "           Model pruning can enable the deployment of neural networks in environments with resource constraints. While pruning may have a small effect on the overall performance of the model, it can exacerbate existing biases into the model such that subsets of samples see significantly degraded performance. In this paper, we introduce the performance weighted loss function, a simple modified cross-entropy loss function that can be used to limit the introduction of biases during pruning. Experiments using the CelebA, Fitzpatrick17k and CIFAR-10 datasets demonstrate that the proposed method is a simple and effective tool that can enable existing pruning methods to be used in fairness sensitive contexts. Code used to produce all experiments contained in this paper can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2211.10285",
    "authors": [
      "Robbie Meyer",
      "Alexander Wong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2212.14309",
    "title": "Learning to mask: Towards generalized face forgery detection",
    "abstract": "           Generalizability to unseen forgery types is crucial for face forgery detectors. Recent works have made significant progress in terms of generalization by synthetic forgery data augmentation. In this work, we explore another path for improving the generalization. Our goal is to reduce the features that are easy to learn in the training phase, so as to reduce the risk of overfitting on specific forgery types. Specifically, in our method, a teacher network takes as input the face images and generates an attention map of the deep features by a diverse multihead attention ViT. The attention map is used to guide a student network to focus on the low-attended features by reducing the highly-attended deep features. A deep feature mixup strategy is also proposed to synthesize forgeries in the feature domain. Experiments demonstrate that, without data augmentation, our method is able to achieve promising performances on unseen forgeries and highly compressed data.         ",
    "url": "https://arxiv.org/abs/2212.14309",
    "authors": [
      "Jianwei Fei",
      "Yunshu Dai",
      "Huaming Wang",
      "Zhihua Xia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2304.03907",
    "title": "Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding",
    "abstract": "           This paper presents an approach, Spectral Dynamics Embedding Control (SDEC), to optimal control for nonlinear stochastic systems. This method leverages an infinite-dimensional feature to linearly represent the state-action value function and exploits finite-dimensional truncation approximation for practical implementation. To characterize the effectiveness of these finite dimensional approximations, we provide an in-depth theoretical analysis to characterize the approximation error induced by the finite-dimension truncation and statistical error induced by finite-sample approximation in both policy evaluation and policy optimization. Our analysis includes two prominent kernel approximation methods: truncations onto random features and Nystrom features. We also empirically test the algorithm and compare the performance with Koopman-based, iLQR, and energy-based methods on a few benchmark problems.         ",
    "url": "https://arxiv.org/abs/2304.03907",
    "authors": [
      "Zhaolin Ren",
      "Tongzheng Ren",
      "Haitong Ma",
      "Na Li",
      "Bo Dai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2306.09471",
    "title": "Privacy Guarantees for Personal Mobility Data in Humanitarian Response",
    "abstract": "           Personal mobility data from mobile phones and other sensors are increasingly used to inform policymaking during pandemics, natural disasters, and other humanitarian crises. However, even aggregated mobility traces can reveal private information about individual movements to potentially malicious actors. This paper develops and tests an approach for releasing private mobility data, which provides formal guarantees over the privacy of the underlying subjects. Specifically, we (1) introduce an algorithm for constructing differentially private mobility matrices, and derive privacy and accuracy bounds on this algorithm; (2) use real-world data from mobile phone operators in Afghanistan and Rwanda to show how this algorithm can enable the use of private mobility data in two high-stakes policy decisions: pandemic response and the distribution of humanitarian aid; and (3) discuss practical decisions that need to be made when implementing this approach, such as how to optimally balance privacy and accuracy. Taken together, these results can help enable the responsible use of private mobility data in humanitarian response.         ",
    "url": "https://arxiv.org/abs/2306.09471",
    "authors": [
      "Nitin Kohli",
      "Emily Aiken",
      "Joshua Blumenstock"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2307.03992",
    "title": "Stimulating Diffusion Model for Image Denoising via Adaptive Embedding and Ensembling",
    "abstract": "           Image denoising is a fundamental problem in computational photography, where achieving high perception with low distortion is highly demanding. Current methods either struggle with perceptual quality or suffer from significant distortion. Recently, the emerging diffusion model has achieved state-of-the-art performance in various tasks and demonstrates great potential for image denoising. However, stimulating diffusion models for image denoising is not straightforward and requires solving several critical problems. For one thing, the input inconsistency hinders the connection between diffusion models and image denoising. For another, the content inconsistency between the generated image and the desired denoised image introduces distortion. To tackle these problems, we present a novel strategy called the Diffusion Model for Image Denoising (DMID) by understanding and rethinking the diffusion model from a denoising perspective. Our DMID strategy includes an adaptive embedding method that embeds the noisy image into a pre-trained unconditional diffusion model and an adaptive ensembling method that reduces distortion in the denoised image. Our DMID strategy achieves state-of-the-art performance on both distortion-based and perception-based metrics, for both Gaussian and real-world image this http URL code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2307.03992",
    "authors": [
      "Tong Li",
      "Hansen Feng",
      "Lizhi Wang",
      "Zhiwei Xiong",
      "Hua Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.07279",
    "title": "A Robust Image Forensic Framework Utilizing Multi-Colorspace Enriched Vision Transformer for Distinguishing Natural and Computer-Generated Images",
    "abstract": "           The digital image forensics based research works in literature classifying natural and computer generated images primarily focuses on binary tasks. These tasks typically involve the classification of natural images versus computer graphics images only or natural images versus GAN generated images only, but not natural images versus both types of generated images simultaneously. Furthermore, despite the support of advanced convolutional neural networks and transformer based architectures that can achieve impressive classification accuracies for this forensic classification task of distinguishing natural and computer generated images, these models are seen to fail over the images that have undergone post-processing operations intended to deceive forensic algorithms, such as JPEG compression, Gaussian noise addition, etc. In this digital image forensic based work to distinguish between natural and computer-generated images encompassing both computer graphics and GAN generated images, we propose a robust forensic classifier framework leveraging enriched vision transformers. By employing a fusion approach for the networks operating in RGB and YCbCr color spaces, we achieve higher classification accuracy and robustness against the post-processing operations of JPEG compression and addition of Gaussian noise. Our approach outperforms baselines, demonstrating 94.25% test accuracy with significant performance gains in individual class accuracies. Visualizations of feature representations and attention maps reveal improved separability as well as improved information capture relevant to the forensic task. This work advances the state-of-the-art in image forensics by providing a generalized and resilient solution to distinguish between natural and generated images.         ",
    "url": "https://arxiv.org/abs/2308.07279",
    "authors": [
      "Manjary P. Gangan",
      "Anoop Kadan",
      "Lajish V L"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02164",
    "title": "A Survey of Graph Unlearning",
    "abstract": "           Graph unlearning emerges as a crucial advancement in the pursuit of responsible AI, providing the means to remove sensitive data traces from trained models, thereby upholding the right to be forgotten. It is evident that graph machine learning exhibits sensitivity to data privacy and adversarial attacks, necessitating the application of graph unlearning techniques to address these concerns effectively. In this comprehensive survey paper, we present the first systematic review of graph unlearning approaches, encompassing a diverse array of methodologies and offering a detailed taxonomy and up-to-date literature overview to facilitate the understanding of researchers new to this field. To ensure clarity, we provide lucid explanations of the fundamental concepts and evaluation measures used in graph unlearning, catering to a broader audience with varying levels of expertise. Delving into potential applications, we explore the versatility of graph unlearning across various domains, including but not limited to social networks, adversarial settings, recommender systems, and resource-constrained environments like the Internet of Things, illustrating its potential impact in safeguarding data privacy and enhancing AI systems' robustness. Finally, we shed light on promising research directions, encouraging further progress and innovation within the domain of graph unlearning. By laying a solid foundation and fostering continued progress, this survey seeks to inspire researchers to further advance the field of graph unlearning, thereby instilling confidence in the ethical growth of AI systems and reinforcing the responsible application of machine learning techniques in various domains.         ",
    "url": "https://arxiv.org/abs/2310.02164",
    "authors": [
      "Anwar Said",
      "Yuying Zhao",
      "Tyler Derr",
      "Mudassir Shabbir",
      "Waseem Abbas",
      "Xenofon Koutsoukos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.12076",
    "title": "Towards Exploring Fairness in Visual Transformer based Natural and GAN Image Detection Systems",
    "abstract": "           Image forensics research has recently witnessed a lot of advancements towards developing computational models capable of accurately detecting natural images captured by cameras and GAN generated images. However, it is also important to ensure whether these computational models are fair enough and do not produce biased outcomes that could eventually harm certain societal groups or cause serious security threats. Exploring fairness in image forensic algorithms is an initial step towards mitigating these biases. This study explores bias in visual transformer based image forensic algorithms that classify natural and GAN images, since visual transformers are recently being widely used in image classification based tasks, including in the area of image forensics. The proposed study procures bias evaluation corpora to analyze bias in gender, racial, affective, and intersectional domains using a wide set of individual and pairwise bias evaluation measures. Since the robustness of the algorithms against image compression is an important factor to be considered in forensic tasks, this study also analyzes the impact of image compression on model bias. Hence to study the impact of image compression on model bias, a two-phase evaluation setting is followed, where the experiments are carried out in uncompressed and compressed evaluation settings. The study could identify bias existences in the visual transformer based models distinguishing natural and GAN images, and also observes that image compression impacts model biases, predominantly amplifying the presence of biases in class GAN predictions.         ",
    "url": "https://arxiv.org/abs/2310.12076",
    "authors": [
      "Manjary P. Gangan",
      "Anoop Kadan",
      "Lajish V L"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.17807",
    "title": "Clover: Closed-Loop Verifiable Code Generation",
    "abstract": "           The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to undesirable outcomes. In this paper, we introduce a new approach for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which uses consistency checking to provide a strong filter for incorrect code. Clover performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its performance on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results show that for this dataset: (i) LLMs are reasonably successful at automatically generating formal specifications; and (ii) our consistency checker achieves a promising acceptance rate (up to 87%) for correct instances while maintaining zero tolerance for adversarial incorrect ones (no false positives). Clover also discovered 6 incorrect programs in the existing human-written dataset MBPP-DFY-50.         ",
    "url": "https://arxiv.org/abs/2310.17807",
    "authors": [
      "Chuyue Sun",
      "Ying Sheng",
      "Oded Padon",
      "Clark Barrett"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2311.07105",
    "title": "Collaborative Goal Tracking of Multiple Mobile Robots Based on Geometric Graph Neural Network",
    "abstract": "           Multiple mobile robots play a significant role in various spatially distributed this http URL unfamiliar and non-repetitive scenarios, reconstructing the global map is time-inefficient and sometimes unrealistic. Hence, research has focused on achieving real-time collaborative planning by utilizing sensor data from multiple robots located at different positions, all without relying on a global this http URL paper introduces a Multi-Robot collaborative Path Planning method based on Geometric Graph Neural Network (MRPP-GeoGNN). We extract the features of each neighboring robot's sensory data and integrate the relative positions of neighboring robots into each interaction layer to incorporate obstacle information along with location details using geometric feature encoders. After that, a MLP layer is used to map the amalgamated local features to multiple forward directions for the robot's actual movement. We generated expert data in ROS to train the network and carried out both simulations and physical experiments to validate the effectiveness of the proposed method. Simulation results demonstrate an approximate 5% improvement in accuracy compared to the model based solely on CNN on expert datasets. The success rate is enhanced by about 4% compared to CNN, and the flowtime increase is reduced by approximately 18% in the ROS test, surpassing other GNN models. Besides, the proposed method is able to leverage neighbor's information and greatly improves path efficiency in real-world scenarios.         ",
    "url": "https://arxiv.org/abs/2311.07105",
    "authors": [
      "Weining Lu",
      "Qingquan Lin",
      "Litong Meng",
      "Chenxi Li",
      "Bin Liang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2311.07929",
    "title": "Variational Graph Autoencoder for Heterogeneous Information Networks with Missing and Inaccurate Attributes",
    "abstract": "           Heterogeneous Information Networks (HINs), which consist of various types of nodes and edges, have recently demonstrated excellent performance in graph mining. However, most existing heterogeneous graph neural networks (HGNNs) ignore the problems of missing attributes, inaccurate attributes and scarce labels for nodes, which limits their expressiveness. In this paper, we propose a generative self-supervised model GraMI to address these issues simultaneously. Specifically, GraMI first initializes all the nodes in the graph with a low-dimensional representation matrix. After that, based on the variational graph autoencoder framework, GraMI learns both node-level and attribute-level embeddings in the encoder, which can provide fine-grained semantic information to construct node attributes. In the decoder, GraMI reconstructs both links and attributes. Instead of directly reconstructing raw features for attributed nodes, GraMI generates the initial low-dimensional representation matrix for all the nodes, based on which raw features of attributed nodes are further reconstructed to leverage accurate attributes. In this way, GraMI can not only complete informative features for non-attributed nodes, but rectify inaccurate ones for attributed nodes. Finally, we conduct extensive experiments to show the superiority of GraMI in tackling HINs with missing and inaccurate attributes.         ",
    "url": "https://arxiv.org/abs/2311.07929",
    "authors": [
      "Yige Zhao",
      "Jianxiang Yu",
      "Yao Cheng",
      "Chengcheng Yu",
      "Yiding Liu",
      "Xiang Li",
      "Shuaiqiang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2311.18252",
    "title": "Privacy and Copyright Protection in Generative AI: A Lifecycle Perspective",
    "abstract": "           The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.         ",
    "url": "https://arxiv.org/abs/2311.18252",
    "authors": [
      "Dawen Zhang",
      "Boming Xia",
      "Yue Liu",
      "Xiwei Xu",
      "Thong Hoang",
      "Zhenchang Xing",
      "Mark Staples",
      "Qinghua Lu",
      "Liming Zhu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2312.02491",
    "title": "Pseudo Replay-based Class Continual Learning for Online New Category Anomaly Detection in Advanced Manufacturing",
    "abstract": "           The incorporation of advanced sensors and machine learning techniques has enabled modern manufacturing enterprises to perform data-driven classification-based anomaly detection based on the sensor data collected in manufacturing processes. However, one critical challenge is that newly presented defect category may manifest as the manufacturing process continues, resulting in monitoring performance deterioration of previously trained machine learning models. Hence, there is an increasing need for empowering machine learning models to learn continually. Among all continual learning methods, memory-based continual learning has the best performance but faces the constraints of data storage capacity. To address this issue, this paper develops a novel pseudo replay-based continual learning framework by integrating class incremental learning and oversampling-based data generation. Without storing all the data, the developed framework could generate high-quality data representing previous classes to train machine learning model incrementally when new category anomaly occurs. In addition, it could even enhance the monitoring performance since it also effectively improves the data quality. The effectiveness of the proposed framework is validated in three cases studies, which leverages supervised classification problem for anomaly detection. The experimental results show that the developed method is very promising in detecting novel anomaly while maintaining a good performance on the previous task and brings up more flexibility in model architecture.         ",
    "url": "https://arxiv.org/abs/2312.02491",
    "authors": [
      "Yuxuan Li",
      "Tianxin Xie",
      "Chenang Liu",
      "Zhangyue Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2401.05845",
    "title": "Graph Reconstruction via MIS Queries",
    "abstract": "           In the Graph Reconstruction (GR) problem, a player initially only knows the vertex set $V$ of an input graph $G=(V, E)$ and is required to learn its set of edges $E$. To this end, the player submits queries to an oracle and must deduce $E$ from the oracle's answers. In this paper, we initiate the study of GR via Maximal Independent Set (MIS) queries, a more powerful variant of Independent Set (IS) queries. Given a query $U \\subseteq V$, the oracle responds with any, potentially adversarially chosen, maximal independent set $I \\subseteq U$ in the induced subgraph $G[U]$. We show that, for GR, MIS queries are strictly more powerful than IS queries when parametrized by the maximum degree $\\Delta$ of the input graph. We give tight (up to poly-logarithmic factors) upper and lower bounds for this problem: 1. We observe that the simple strategy of taking uniform independent random samples of $V$ and submitting those to the oracle yields a non-adaptive randomized algorithm that executes $O(\\Delta^2 \\cdot \\log n)$ queries and succeeds with high probability. Furthermore, combining the strategy of taking uniform random samples of $V$ with the probabilistic method, we show the existence of a deterministic non-adaptive algorithm that executes $O(\\Delta^3 \\cdot \\log(\\frac{n}{\\Delta}))$ queries. 2. Regarding lower bounds, we prove that the additional $\\Delta$ factor when going from randomized non-adaptive algorithms to deterministic non-adaptive algorithms is necessary. We show that every non-adaptive deterministic algorithm requires $\\Omega(\\Delta^3 / \\log^2 \\Delta)$ queries. For arbitrary randomized adaptive algorithms, we show that $\\Omega(\\Delta^2)$ queries are necessary in graphs of maximum degree $\\Delta$, and that $\\Omega(\\log n)$ queries are necessary, even when the input graph is an $n$-vertex cycle.         ",
    "url": "https://arxiv.org/abs/2401.05845",
    "authors": [
      "Christian Konrad",
      "Conor O'Sullivan",
      "Victor Traistaru"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2401.09596",
    "title": "Efficient generative adversarial networks using linear additive-attention Transformers",
    "abstract": "           Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present a novel GAN architecture which we call LadaGAN. This architecture is based on a linear attention Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms existing convolutional and Transformer GANs on benchmark datasets at different resolutions while being significantly more efficient. Moreover, LadaGAN shows competitive performance compared to state-of-the-art multi-step generative models (e.g. DMs) using orders of magnitude less computational resources.         ",
    "url": "https://arxiv.org/abs/2401.09596",
    "authors": [
      "Emilio Morales-Juarez",
      "Gibran Fuentes-Pineda"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2401.15299",
    "title": "SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graph-like in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problems using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of factory issues. By utilizing this dataset, researchers can employ GNNs to address numerous supply chain problems, thereby advancing the field of supply chain analytics and planning. Source: this https URL ",
    "url": "https://arxiv.org/abs/2401.15299",
    "authors": [
      "Azmine Toushik Wasi",
      "MD Shafikul Islam",
      "Adipto Raihan Akib"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Systems and Control (eess.SY)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2402.02164",
    "title": "Hierarchical Structure Enhances the Convergence and Generalizability of Linear Molecular Representation",
    "abstract": "           Language models demonstrate fundamental abilities in syntax, semantics, and reasoning, though their performance often depends significantly on the inputs they process. This study introduces TSIS (Simplified TSID) and its variants:TSISD (TSIS with Depth-First Search), TSISO (TSIS in Order), and TSISR (TSIS in Random), as integral components of the t-SMILES framework. These additions complete the framework's design, providing diverse approaches to molecular representation. Through comprehensive analysis and experiments employing deep generative models, including GPT, diffusion models, and reinforcement learning, the findings reveal that the hierarchical structure of t-SMILES is more straightforward to parse than initially anticipated. Furthermore, t-SMILES consistently outperforms other linear representations such as SMILES, SELFIES, and SAFE, demonstrating superior convergence speed and enhanced generalization capabilities.         ",
    "url": "https://arxiv.org/abs/2402.02164",
    "authors": [
      "Juan-Ni Wu",
      "Tong Wang",
      "Li-Juan Tang",
      "Hai-Long Wu",
      "Ru-Qin Yu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2403.05100",
    "title": "Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume",
    "abstract": "           The escalating threat of adversarial attacks on deep learning models, particularly in security-critical fields, has underscored the need for robust deep learning systems. Conventional robustness evaluations have relied on adversarial accuracy, which measures a model's performance under a specific perturbation intensity. However, this singular metric does not fully encapsulate the overall resilience of a model against varying degrees of perturbation. To address this gap, we propose a new metric termed adversarial hypervolume, assessing the robustness of deep learning models comprehensively over a range of perturbation intensities from a multi-objective optimization standpoint. This metric allows for an in-depth comparison of defense mechanisms and recognizes the trivial improvements in robustness afforded by less potent defensive strategies. Additionally, we adopt a novel training algorithm that enhances adversarial robustness uniformly across various perturbation intensities, in contrast to methods narrowly focused on optimizing adversarial accuracy. Our extensive empirical studies validate the effectiveness of the adversarial hypervolume metric, demonstrating its ability to reveal subtle differences in robustness that adversarial accuracy overlooks. This research contributes a new measure of robustness and establishes a standard for assessing and benchmarking the resilience of current and future defensive models against adversarial threats.         ",
    "url": "https://arxiv.org/abs/2403.05100",
    "authors": [
      "Ping Guo",
      "Cheng Gong",
      "Xi Lin",
      "Zhiyuan Yang",
      "Qingfu Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.08499",
    "title": "Improved YOLOv5 Based on Attention Mechanism and FasterNet for Foreign Object Detection on Railway and Airway tracks",
    "abstract": "           In recent years, there have been frequent incidents of foreign objects intruding into railway and Airport runways. These objects can include pedestrians, vehicles, animals, and debris. This paper introduces an improved YOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance the detection of foreign objects on railways and Airport runways. This study proposes a new dataset, AARFOD (Aero and Rail Foreign Object Detection), which combines two public datasets for detecting foreign objects in aviation and railway this http URL dataset aims to improve the recognition capabilities of foreign object targets. Experimental results on this large dataset have demonstrated significant performance improvements of the proposed model over the baseline YOLOv5 model, reducing computational this http URL YOLO model shows a significant improvement in precision by 1.2%, recall rate by 1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters were reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%. In the ablation experiment, it is found that the FasterNet module can significantly reduce the number of parameters of the model, and the reference of the attention mechanism can slow down the performance loss caused by lightweight.         ",
    "url": "https://arxiv.org/abs/2403.08499",
    "authors": [
      "Zongqing Qi",
      "Danqing Ma",
      "Jingyu Xu",
      "Ao Xiang",
      "Hedi Qu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.08511",
    "title": "A Multimodal Fusion Network For Student Emotion Recognition Based on Transformer and Tensor Product",
    "abstract": "           This paper introduces a new multi-modal model based on the Transformer architecture and tensor product fusion strategy, combining BERT's text vectors and ViT's image vectors to classify students' psychological conditions, with an accuracy of 93.65%. The purpose of the study is to accurately analyze the mental health status of students from various data sources. This paper discusses modal fusion methods, including early, late and intermediate fusion, to overcome the challenges of integrating multi-modal information. Ablation studies compare the performance of different models and fusion techniques, showing that the proposed model outperforms existing methods such as CLIP and ViLBERT in terms of accuracy and inference speed. Conclusions indicate that while this model has significant advantages in emotion recognition, its potential to incorporate other data modalities provides areas for future research.         ",
    "url": "https://arxiv.org/abs/2403.08511",
    "authors": [
      "Ao Xiang",
      "Zongqing Qi",
      "Han Wang",
      "Qin Yang",
      "Danqing Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.09901",
    "title": "Robust Subgraph Learning by Monitoring Early Training Representations",
    "abstract": "           Graph neural networks (GNNs) have attracted significant attention for their outstanding performance in graph learning and node classification tasks. However, their vulnerability to adversarial attacks, particularly through susceptible nodes, poses a challenge in decision-making. The need for robust graph summarization is evident in adversarial challenges resulting from the propagation of attacks throughout the entire graph. In this paper, we address both performance and adversarial robustness in graph input by introducing the novel technique SHERD (Subgraph Learning Hale through Early Training Representation Distances). SHERD leverages information from layers of a partially trained graph convolutional network (GCN) to detect susceptible nodes during adversarial attacks using standard distance metrics. The method identifies \"vulnerable (bad)\" nodes and removes such nodes to form a robust subgraph while maintaining node classification performance. Through our experiments, we demonstrate the increased performance of SHERD in enhancing robustness by comparing the network's performance on original and subgraph inputs against various baselines alongside existing adversarial attacks. Our experiments across multiple datasets, including citation datasets such as Cora, Citeseer, and Pubmed, as well as microanatomical tissue structures of cell graphs in the placenta, highlight that SHERD not only achieves substantial improvement in robust performance but also outperforms several baselines in terms of node classification accuracy and computational complexity.         ",
    "url": "https://arxiv.org/abs/2403.09901",
    "authors": [
      "Sepideh Neshatfar",
      "Salimeh Yasaei Sekeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2403.10751",
    "title": "LightCode: Light Analytical and Neural Codes for Channels with Feedback",
    "abstract": "           The design of reliable and efficient codes for channels with feedback remains a longstanding challenge in communication theory. While significant improvements have been achieved by leveraging deep learning techniques, neural codes often suffer from high computational costs, a lack of interpretability, and limited practicality in resource-constrained settings. We focus on designing low-complexity coding schemes that are interpretable and more suitable for communication systems. We advance both analytical and neural codes. First, we demonstrate that PowerBlast, an analytical coding scheme inspired by Schalkwijk-Kailath (SK) and Gallager-Nakibo\u011flu (GN) schemes, achieves notable reliability improvements over both SK and GN schemes, outperforming neural codes in high signal-to-noise ratio (SNR) regions. Next, to enhance reliability in low-SNR regions, we propose LightCode, a lightweight neural code that achieves state-of-the-art reliability while using a fraction of memory and compute compared to existing deeplearning-based codes. Finally, we systematically analyze the learned codes, establishing connections between LightCode and PowerBlast, identifying components crucial for performance, and providing interpretation aided by linear regression analysis.         ",
    "url": "https://arxiv.org/abs/2403.10751",
    "authors": [
      "Sravan Kumar Ankireddy",
      "Krishna Narayanan",
      "Hyeji Kim"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.01768",
    "title": "Stereotype Detection in LLMs: A Multiclass, Explainable, and Benchmark-Driven Approach",
    "abstract": "           Stereotype detection is a challenging and subjective task, as certain statements, such as \"Black people like to play basketball,\" may not appear overtly toxic but still reinforce racial stereotypes. With the increasing prevalence of large language models (LLMs) in human-facing artificial intelligence (AI) applications, detecting these types of biases is essential. However, LLMs risk perpetuating and amplifying stereotypical outputs derived from their training data. A reliable stereotype detector is crucial for benchmarking bias, monitoring model input and output, filtering training data, and ensuring fairer model behavior in downstream applications. This paper introduces the Multi-Grain Stereotype (MGS) dataset, consisting of 51,867 instances across gender, race, profession, religion, and other stereotypes, curated from multiple existing datasets. We evaluate various machine learning approaches to establish baselines and fine-tune language models of different architectures and sizes, presenting a suite of stereotype multiclass classifiers trained on the MGS dataset. Given the subjectivity of stereotypes, explainability is essential to align model learning with human understanding of stereotypes. We employ explainable AI (XAI) tools, including SHAP, LIME, and BertViz, to assess whether the model's learned patterns align with human intuitions about this http URL, we develop stereotype elicitation prompts and benchmark the presence of stereotypes in text generation tasks using popular LLMs, employing the best-performing stereotype classifiers.         ",
    "url": "https://arxiv.org/abs/2404.01768",
    "authors": [
      "Zekun Wu",
      "Sahan Bulathwela",
      "Maria Perez-Ortiz",
      "Adriano Soares Koshiyama"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.04254",
    "title": "Watermark-based Detection and Attribution of AI-Generated Content",
    "abstract": "           Several companies have deployed watermark-based detection to identify AI-generated content. However, attribution--the ability to trace back to the user of a generative AI (GenAI) service who created a given piece of AI-generated content--remains largely unexplored despite its growing importance. In this work, we aim to bridge this gap by conducting the first systematic study on watermark-based, user-level attribution of AI-generated content. Our key idea is to assign a unique watermark to each user of the GenAI service and embed this watermark into the AI-generated content created by that user. Attribution is then performed by identifying the user whose watermark best matches the one extracted from the given content. This approach, however, faces a key challenge: How should watermarks be selected for users to maximize attribution performance? To address the challenge, we first theoretically derive lower bounds on detection and attribution performance through rigorous probabilistic analysis for any given set of user watermarks. Then, we select watermarks for users to maximize these lower bounds, thereby optimizing detection and attribution performance. Our theoretical and empirical results show that watermark-based attribution inherits both the accuracy and (non-)robustness properties of the underlying watermark. Specifically, attribution remains highly accurate when the watermarked AI-generated content is either not post-processed or subjected to common post-processing such as JPEG compression, as well as black-box adversarial post-processing with limited query budgets.         ",
    "url": "https://arxiv.org/abs/2404.04254",
    "authors": [
      "Zhengyuan Jiang",
      "Moyang Guo",
      "Yuepeng Hu",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.12634",
    "title": "Transformer-Based Classification Outcome Prediction for Multimodal Stroke Treatment",
    "abstract": "           This study proposes a multi-modal fusion framework Multitrans based on the Transformer architecture and self-attention mechanism. This architecture combines the study of non-contrast computed tomography (NCCT) images and discharge diagnosis reports of patients undergoing stroke treatment, using a variety of methods based on Transformer architecture approach to predicting functional outcomes of stroke treatment. The results show that the performance of single-modal text classification is significantly better than single-modal image classification, but the effect of multi-modal combination is better than any single modality. Although the Transformer model only performs worse on imaging data, when combined with clinical meta-diagnostic information, both can learn better complementary information and make good contributions to accurately predicting stroke treatment effects..         ",
    "url": "https://arxiv.org/abs/2404.12634",
    "authors": [
      "Danqing Ma",
      "Meng Wang",
      "Ao Xiang",
      "Zongqing Qi",
      "Qin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.14964",
    "title": "Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks",
    "abstract": "           Training spiking neural networks to approximate universal functions is essential for studying information processing in the brain and for neuromorphic computing. Yet the binary nature of spikes poses a challenge for direct gradient-based training. Surrogate gradients have been empirically successful in circumventing this problem, but their theoretical foundation remains elusive. Here, we investigate the relation of surrogate gradients to two theoretically well-founded approaches. On the one hand, we consider smoothed probabilistic models, which, due to the lack of support for automatic differentiation, are impractical for training multi-layer spiking neural networks but provide derivatives equivalent to surrogate gradients for single neurons. On the other hand, we investigate stochastic automatic differentiation, which is compatible with discrete randomness but has not yet been used to train spiking neural networks. We find that the latter gives surrogate gradients a theoretical basis in stochastic spiking neural networks, where the surrogate derivative matches the derivative of the neuronal escape noise function. This finding supports the effectiveness of surrogate gradients in practice and suggests their suitability for stochastic spiking neural networks. However, surrogate gradients are generally not gradients of a surrogate loss despite their relation to stochastic automatic differentiation. Nevertheless, we empirically confirm the effectiveness of surrogate gradients in stochastic multi-layer spiking neural networks and discuss their relation to deterministic networks as a special case. Our work gives theoretical support to surrogate gradients and the choice of a suitable surrogate derivative in stochastic spiking neural networks.         ",
    "url": "https://arxiv.org/abs/2404.14964",
    "authors": [
      "Julia Gygax",
      "Friedemann Zenke"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2404.18459",
    "title": "Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in the Wild",
    "abstract": "           Large language models have evolved data-efficient generalists, benefiting from the universal language interface and large-scale pre-training. However, constructing a data-efficient generalist for dense visual prediction presents a distinct challenge due to the variation in label structures across different tasks. Consequently, generalization to unseen dense prediction tasks in the low-data regime is not straightforward and has received less attention from previous vision generalists. In this study, we explore a universal model that can flexibly adapt to unseen dense label structures with a few examples, enabling it to serve as a data-efficient vision generalist in diverse real-world scenarios. To this end, we base our method on a powerful meta-learning framework and explore several axes to improve its performance and versatility for real-world problems, such as flexible adaptation mechanisms and scalability. We evaluate our model across a spectrum of unseen real-world scenarios where low-shot learning is desirable, including video, 3D, medical, biological, and user-interactive tasks. Equipped with a generic architecture and an effective adaptation mechanism, our model flexibly adapts to all of these tasks with at most 50 labeled images, showcasing a significant advancement over existing data-efficient generalist approaches. Codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2404.18459",
    "authors": [
      "Donggyun Kim",
      "Seongwoong Cho",
      "Semin Kim",
      "Chong Luo",
      "Seunghoon Hong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2405.03911",
    "title": "Federated Graph Condensation with Information Bottleneck Principles",
    "abstract": "           Graph condensation, which reduces the size of a large-scale graph by synthesizing a small-scale condensed graph as its substitution, has immediately benefited various graph learning tasks. However, existing graph condensation methods rely on centralized data storage, which is unfeasible for real-world decentralized data distribution, and overlook data holders' privacy-preserving requirements. To bridge the gap, we propose and study the novel problem of federated graph condensation for graph neural networks (GNNs). Specifically, we first propose a general framework for federated graph condensation, in which we decouple the typical gradient matching process for graph condensation into client-side gradient calculation and server-side gradient matching. In this way, the burdensome computation cost in client-side is largely alleviated. Besides, our empirical studies show that under the federated setting, the condensed graph will consistently leak data membership privacy, i.e., the condensed graph during the federated training can be utilized to steal the training data under the membership inference attacks (MIA). To tackle this issue, we innovatively incorporate information bottleneck principles into the federated graph condensation, which only needs to extract partial node features in one local pre-training step and utilize the features during federated training. Extensive experiments on real-world datasets demonstrate that our framework can consistently protect membership privacy during training. Meanwhile, it also achieves comparable and even superior performance against existing centralized graph condensation and federated graph learning methods.         ",
    "url": "https://arxiv.org/abs/2405.03911",
    "authors": [
      "Bo Yan",
      "Sihao He",
      "Cheng Yang",
      "Shang Liu",
      "Yang Cao",
      "Chuan Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2405.05574",
    "title": "Vision-Language Modeling with Regularized Spatial Transformer Networks for All Weather Crosswind Landing of Aircraft",
    "abstract": "           The intrinsic capability of the Human Vision System (HVS) to perceive depth of field and failure of Instrument Landing Systems (ILS) stimulates a pilot to perform a vision-based manual landing over an autoland approach. However, harsh weather creates challenges, and a pilot must have a clear view of runway elements before the minimum decision altitude. To aid in manual landing, a vision-based system trained to clear weather-induced visual degradations requires a robust landing dataset under various climatic conditions. Nevertheless, to acquire a dataset, flying an aircraft in dangerous weather impacts safety. Also, this system fails to generate reliable warnings, as localization of runway elements suffers from projective distortion while landing at crosswind. To combat, we propose to synthesize harsh weather landing images by training a prompt-based climatic diffusion network. Also, we optimize a weather distillation model using a novel diffusion-distillation loss to learn to clear these visual degradations. Precisely, the distillation model learns an inverse relationship with the diffusion network. Inference time, pre-trained distillation network directly clears weather-impacted onboard camera images, which can be further projected to display devices for improved this http URL, to tackle crosswind landing, a novel Regularized Spatial Transformer Networks (RuSTaN) module accurately warps landing images. It minimizes the localization error of runway object detector and helps generate reliable internal software warnings. Finally, we curated an aircraft landing dataset (AIRLAD) by simulating a landing scenario under various weather degradations and experimentally validated our contributions.         ",
    "url": "https://arxiv.org/abs/2405.05574",
    "authors": [
      "Debabrata Pal",
      "Anvita Singh",
      "Saumya Saumya",
      "Shouvik Das"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2405.14210",
    "title": "Eidos: Efficient, Imperceptible Adversarial 3D Point Clouds",
    "abstract": "           Classification of 3D point clouds is a challenging machine learning (ML) task with important real-world applications in a spectrum from autonomous driving and robot-assisted surgery to earth observation from low orbit. As with other ML tasks, classification models are notoriously brittle in the presence of adversarial attacks. These are rooted in imperceptible changes to inputs with the effect that a seemingly well-trained model ends up misclassifying the input. This paper adds to the understanding of adversarial attacks by presenting Eidos, a framework providing Efficient Imperceptible aDversarial attacks on 3D pOint cloudS. Eidos supports a diverse set of imperceptibility metrics. It employs an iterative, two-step procedure to identify optimal adversarial examples, thereby enabling a runtime-imperceptibility trade-off. We provide empirical evidence relative to several popular 3D point cloud classification models and several established 3D attack methods, showing Eidos' superiority with respect to efficiency as well as imperceptibility.         ",
    "url": "https://arxiv.org/abs/2405.14210",
    "authors": [
      "Hanwei Zhang",
      "Luo Cheng",
      "Qisong He",
      "Wei Huang",
      "Renjue Li",
      "Ronan Sicre",
      "Xiaowei Huang",
      "Holger Hermanns",
      "Lijun Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2405.15866",
    "title": "Governing the Commons: Code Ownership and Code-Clones in Large-Scale Software Development",
    "abstract": "           Context: In software development organizations employing weak or collective ownership, different teams are allowed and expected to autonomously perform changes in various components. This creates diversity both in the knowledge of, and in the responsibility for, individual components. Objective: Our objective is to understand how and why different teams introduce technical debt in the form of code clones as they change different components. Method: We collected data about change size and clone introductions made by ten teams in eight components which was part of a large industrial software system. We then designed a Multi-Level Generalized Linear Model (MLGLM), to illustrate the teams' differing behavior. Finally, we discussed the results with three development teams, plus line manager and the architect team, evaluating whether the model inferences aligned with what they expected. Responses were recorded and thematically coded. Results: The results show that teams do behave differently in different components, and the feedback from the teams indicates that this method of illustrating team behavior can be useful as a complement to traditional summary statistics of ownership. Conclusions: We find that our model-based approach produces useful visualizations of team introductions of code clones as they change different components. Practitioners stated that the visualizations gave them insights that were useful, and by comparing with an average team, inter-team comparisons can be avoided. Thus, this has the potential to be a useful feedback tool for teams in software development organizations that employ weak or collective ownership.         ",
    "url": "https://arxiv.org/abs/2405.15866",
    "authors": [
      "Anders Sundelin",
      "Javier Gonzalez-Huerta",
      "Richard Torkar",
      "Krzysztof Wnuk"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2405.15994",
    "title": "Verified Safe Reinforcement Learning for Neural Network Dynamic Models",
    "abstract": "           Learning reliably safe autonomous control is one of the core problems in trustworthy autonomy. However, training a controller that can be formally verified to be safe remains a major challenge. We introduce a novel approach for learning verified safe control policies in nonlinear neural dynamical systems while maximizing overall performance. Our approach aims to achieve safety in the sense of finite-horizon reachability proofs, and is comprised of three key parts. The first is a novel curriculum learning scheme that iteratively increases the verified safe horizon. The second leverages the iterative nature of gradient-based learning to leverage incremental verification, reusing information from prior verification runs. Finally, we learn multiple verified initial-state-dependent controllers, an idea that is especially valuable for more complex domains where learning a single universal verified safe controller is extremely challenging. Our experiments on five safe control problems demonstrate that our trained controllers can achieve verified safety over horizons that are as much as an order of magnitude longer than state-of-the-art baselines, while maintaining high reward, as well as a perfect safety record over entire episodes. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2405.15994",
    "authors": [
      "Junlin Wu",
      "Huan Zhang",
      "Yevgeniy Vorobeychik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2405.16606",
    "title": "Link Prediction on Textual Edge Graphs",
    "abstract": "           Textual-edge Graphs (TEGs), characterized by rich text annotations on edges, are increasingly significant in network science due to their ability to capture rich contextual information among entities. Existing works have proposed various edge-aware graph neural networks (GNNs) or let language models directly make predictions. However, they often fall short of fully capturing the contextualized semantics on edges and graph topology, respectively. This inadequacy is particularly evident in link prediction tasks that require a comprehensive understanding of graph topology and semantics between nodes. In this paper, we present a novel framework - Link2Doc, designed especially for link prediction on textual-edge graphs. Specifically, we propose to summarize neighborhood information between node pairs as a human-written document to preserve both semantic and topology information. A self-supervised learning model is then utilized to enhance GNN's text-understanding ability from language models. Empirical evaluations, including link prediction, edge classification, parameter analysis, runtime comparison, and ablation studies, on four real-world datasets demonstrate that Link2Doc achieves generally better performance against existing edge-aware GNNs and pre-trained language models in predicting links on TEGs.         ",
    "url": "https://arxiv.org/abs/2405.16606",
    "authors": [
      "Chen Ling",
      "Zhuofeng Li",
      "Yuntong Hu",
      "Zheng Zhang",
      "Zhongyuan Liu",
      "Shuang Zheng",
      "Jian Pei",
      "Liang Zhao"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2405.18458",
    "title": "Asymmetrical estimator for training encapsulated deep photonic neural networks",
    "abstract": "           Photonic neural networks (PNNs) are fast in-propagation and high bandwidth paradigms that aim to popularize reproducible NN acceleration with higher efficiency and lower cost. However, the training of PNN is known to be a challenge, where the device-to-device and system-to-system variations create imperfect knowledge of the PNN. Despite backpropagation (BP)-based training algorithms often being the industry standard for their robustness, generality, and fast gradient convergence for digital training, existing PNN-BP methods rely heavily on the accurate intermediate state extraction for a deep PNN (DPNN). These information accesses truncate the photonic signal propagation, bottlenecking DPNN's operation speed and increasing the system construction cost. Here, we introduce the asymmetrical training (AT) method, tailored for encapsulated DPNNs, where the signal is preserved in the analogue photonic domain for the entire structure. AT's minimum information readout for training bypasses analogue-digital interfaces wherever possible for fast operation and minimum system footprint. AT's error tolerance and generality aim to promote PNN acceleration in a widened operational scenario despite the fabrication variations and imperfect controls. We demonstrated AT for encapsulated DPNN with integrated photonic chips, repeatably enhancing the performance from in-silico BP for different network structures and datasets.         ",
    "url": "https://arxiv.org/abs/2405.18458",
    "authors": [
      "Yizhi Wang",
      "Minjia Chen",
      "Chunhui Yao",
      "Jie Ma",
      "Ting Yan",
      "Richard Penty",
      "Qixiang Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2405.18942",
    "title": "Verifiably Robust Conformal Prediction",
    "abstract": "           Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable. In such a case, CP's prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability. Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage. Recently, several approaches have been put forward to recover CP guarantees in this setting. These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations. They are, however, limited in that they only support $\\ell^2$-bounded perturbations and classification tasks. This paper introduces VRCP (Verifiably Robust Conformal Prediction), a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks. Our VRCP method is the first to support perturbations bounded by arbitrary norms including $\\ell^1$, $\\ell^2$, and $\\ell^\\infty$, as well as regression tasks. We evaluate and compare our approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks for deep reinforcement learning environments. In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA.         ",
    "url": "https://arxiv.org/abs/2405.18942",
    "authors": [
      "Linus Jeary",
      "Tom Kuipers",
      "Mehran Hosseini",
      "Nicola Paoletti"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06541",
    "title": "Global and Local Attention-based Inception U-Net for Static IR Drop Prediction",
    "abstract": "           Static IR drop analysis is a fundamental and critical task in chip design since the IR drop will significantly affect the design's functionality, performance, and reliability. However, the process of IR drop analysis can be time-consuming, potentially taking several hours. Therefore, a fast and accurate IR drop prediction is paramount for reducing the overall time invested in chip design. In this paper, we propose a global and local attention-based Inception U-Net for static IR drop prediction. Our U-Net incorporates the Transformer, CBAM, and Inception architectures to enhance its feature capture capability at different scales and improve the accuracy of predicted IR drop. Moreover, we propose 4 new features, which enhance our model with richer information. Finally, to balance the sampling probabilities across different regions in one design, we propose a series of novel data spatial adjustment techniques, with each batch randomly selecting one of them during training. Experimental results demonstrate that our proposed algorithm can achieve the best results among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.         ",
    "url": "https://arxiv.org/abs/2406.06541",
    "authors": [
      "Yilu Chen",
      "Zhijie Cai",
      "Min Wei",
      "Zhifeng Lin",
      "Jianli Chen"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2406.06818",
    "title": "Conformal Prediction for Class-wise Coverage via Augmented Label Rank Calibration",
    "abstract": "           Conformal prediction (CP) is an emerging uncertainty quantification framework that allows us to construct a prediction set to cover the true label with a pre-specified marginal or conditional probability. Although the valid coverage guarantee has been extensively studied for classification problems, CP often produces large prediction sets which may not be practically useful. This issue is exacerbated for the setting of class-conditional coverage on imbalanced classification tasks with many and/or imbalanced classes. This paper proposes the Rank Calibrated Class-conditional CP (RC3P) algorithm to reduce the prediction set sizes to achieve class-conditional coverage, where the valid coverage holds for each class. In contrast to the standard class-conditional CP (CCP) method that uniformly thresholds the class-wise conformity score for each class, the augmented label rank calibration step allows RC3P to selectively iterate this class-wise thresholding subroutine only for a subset of classes whose class-wise top-k error is small. We prove that agnostic to the classifier and data distribution, RC3P achieves class-wise coverage. We also show that RC3P reduces the size of prediction sets compared to the CCP method. Comprehensive experiments on multiple real-world datasets demonstrate that RC3P achieves class-wise coverage and 26.25% reduction in prediction set sizes on average.         ",
    "url": "https://arxiv.org/abs/2406.06818",
    "authors": [
      "Yuanjie Shi",
      "Subhankar Ghosh",
      "Taha Belkhouja",
      "Janardhan Rao Doppa",
      "Yan Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07640",
    "title": "When is an Embedding Model More Promising than Another?",
    "abstract": "           Embedders play a central role in machine learning, projecting any object into numerical representations that can, in turn, be leveraged to perform various downstream tasks. The evaluation of embedding models typically depends on domain-specific empirical approaches utilizing downstream tasks, primarily because of the lack of a standardized framework for comparison. However, acquiring adequately large and representative datasets for conducting these assessments is not always viable and can prove to be prohibitively expensive and time-consuming. In this paper, we present a unified approach to evaluate embedders. First, we establish theoretical foundations for comparing embedding models, drawing upon the concepts of sufficiency and informativeness. We then leverage these concepts to devise a tractable comparison criterion (information sufficiency), leading to a task-agnostic and self-supervised ranking procedure. We demonstrate experimentally that our approach aligns closely with the capability of embedding models to facilitate various downstream tasks in both natural language processing and molecular biology. This effectively offers practitioners a valuable tool for prioritizing model trials.         ",
    "url": "https://arxiv.org/abs/2406.07640",
    "authors": [
      "Maxime Darrin",
      "Philippe Formont",
      "Ismail Ben Ayed",
      "Jackie CK Cheung",
      "Pablo Piantanida"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.10965",
    "title": "DocNet: Semantic Structure in Inductive Bias Detection Models",
    "abstract": "           News will have biases so long as people have opinions. It is increasingly important for informed citizens to be able to identify bias as social media becomes the primary entry point for news and partisan differences increase. If people know the biases of the news they are consuming, they will be able to take action to avoid polarizing echo chambers. In this paper, we explore an often overlooked aspect of bias detection in documents: the semantic structure of news articles. We present DocNet, a novel, inductive, and low-resource document embedding and bias detection model that outperforms large language models. We also demonstrate that the semantic structure of news articles from opposing partisan sides, as represented in document-level graph embeddings, have significant similarities. These results can be used to advance bias detection in low-resource environments. Our code, data, and the corresponding datasheet are made available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2406.10965",
    "authors": [
      "Jessica Zhu",
      "Iain Cruickshank",
      "Michel Cukier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2406.11109",
    "title": "Investigating Annotator Bias in Large Language Models for Hate Speech Detection",
    "abstract": "           Data annotation, the practice of assigning descriptive labels to raw data, is pivotal in optimizing the performance of machine learning models. However, it is a resource-intensive process susceptible to biases introduced by annotators. The emergence of sophisticated Large Language Models (LLMs) presents a unique opportunity to modernize and streamline this complex procedure. While existing research extensively evaluates the efficacy of LLMs, as annotators, this paper delves into the biases present in LLMs when annotating hate speech data. Our research contributes to understanding biases in four key categories: gender, race, religion, and disability with four LLMs: GPT-3.5, GPT-4o, Llama-3.1 and Gemma-2. Specifically targeting highly vulnerable groups within these categories, we analyze annotator biases. Furthermore, we conduct a comprehensive examination of potential factors contributing to these biases by scrutinizing the annotated data. We introduce our custom hate speech detection dataset, HateBiasNet, to conduct this research. Additionally, we perform the same experiments on the ETHOS (Mollas et al. 2022) dataset also for comparative analysis. This paper serves as a crucial resource, guiding researchers and practitioners in harnessing the potential of LLMs for data annotation, thereby fostering advancements in this critical field.         ",
    "url": "https://arxiv.org/abs/2406.11109",
    "authors": [
      "Amit Das",
      "Zheng Zhang",
      "Najib Hasan",
      "Souvika Sarkar",
      "Fatemeh Jamshidi",
      "Tathagata Bhattacharya",
      "Mostafa Rahgouy",
      "Nilanjana Raychawdhary",
      "Dongji Feng",
      "Vinija Jain",
      "Aman Chadha",
      "Mary Sandage",
      "Lauramarie Pope",
      "Gerry Dozier",
      "Cheryl Seals"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.12952",
    "title": "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents",
    "abstract": "           Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents to formalize user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth bug-fixes, and golden tests. We find that LLMs generally perform surprisingly well at generating relevant test cases, with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-Agent. We release all data and code at this https URL ",
    "url": "https://arxiv.org/abs/2406.12952",
    "authors": [
      "Niels M\u00fcndler",
      "Mark Niklas M\u00fcller",
      "Jingxuan He",
      "Martin Vechev"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.01295",
    "title": "Formal Verification of Deep Neural Networks for Object Detection",
    "abstract": "           Deep neural networks (DNNs) are widely used in real-world applications, yet they remain vulnerable to errors and adversarial attacks. Formal verification offers a systematic approach to identify and mitigate these vulnerabilities, enhancing model robustness and reliability. While most existing verification methods focus on image classification models, this work extends formal verification to the more complex domain of emph{object detection} models. We propose a formulation for verifying the robustness of such models and demonstrate how state-of-the-art verification tools, originally developed for classification, can be adapted for this purpose. Our experiments, conducted on various datasets and networks, highlight the ability of formal verification to uncover vulnerabilities in object detection models, underscoring the need to extend verification efforts to this domain. This work lays the foundation for further research into formal verification across a broader range of computer vision applications.         ",
    "url": "https://arxiv.org/abs/2407.01295",
    "authors": [
      "Yizhak Y. Elboher",
      "Avraham Raviv",
      "Yael Leibovich Weiss",
      "Omer Cohen",
      "Roy Assa",
      "Guy Katz",
      "Hillel Kugler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.03854",
    "title": "Risk Bounds on MDL Estimators for Linear Regression Models with Application to Simple ReLU Neural Networks",
    "abstract": "           To investigate the theoretical foundations of deep learning from the viewpoint of the minimum description length (MDL) principle, we analyse risk bounds of MDL estimators based on two-stage codes for simple two-layers neural networks (NNs) with ReLU activation. For that purpose, we propose a method to design two-stage codes for linear regression models and establish an upper bound on the risk of the corresponding MDL estimators based on the theory of MDL estimators originated by Barron and Cover (1991). Then, we apply this result to the simple two-layers NNs with ReLU activation which consist of $d$ nodes in the input layer, $m$ nodes in the hidden layer and one output node. Since the object of estimation is only the $m$ weights from the hidden layer to the output node in our setting, this is an example of linear regression models. As a result, we show that the redundancy of the obtained two-stage codes is small owing to the fact that the eigenvalue distribution of the Fisher information matrix of the NNs is strongly biased, which was shown by Takeishi et al. (2023) and has been refined in this paper. That is, we establish a tight upper bound on the risk of our MDL estimators. Note that our risk bound for the simple ReLU networks, of which the leading term is $O(d^2 \\log n /n)$, is independent of the number of parameters $m$.         ",
    "url": "https://arxiv.org/abs/2407.03854",
    "authors": [
      "Yoshinari Takeishi",
      "Jun'ichi Takeuchi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2407.07128",
    "title": "Modularity aided consistent attributed graph clustering via coarsening",
    "abstract": "           Graph clustering is an important unsupervised learning technique for partitioning graphs with attributes and detecting communities. However, current methods struggle to accurately capture true community structures and intra-cluster relations, be computationally efficient, and identify smaller communities. We address these challenges by integrating coarsening and modularity maximization, effectively leveraging both adjacency and node features to enhance clustering accuracy. We propose a loss function incorporating log-determinant, smoothness, and modularity components using a block majorization-minimization technique, resulting in superior clustering outcomes. The method is theoretically consistent under the Degree-Corrected Stochastic Block Model (DC-SBM), ensuring asymptotic error-free performance and complete label recovery. Our provably convergent and time-efficient algorithm seamlessly integrates with graph neural networks (GNNs) and variational graph autoencoders (VGAEs) to learn enhanced node features and deliver exceptional clustering performance. Extensive experiments on benchmark datasets demonstrate its superiority over existing state-of-the-art methods for both attributed and non-attributed graphs.         ",
    "url": "https://arxiv.org/abs/2407.07128",
    "authors": [
      "Samarth Bhatia",
      "Yukti Makhija",
      "Manoj Kumar",
      "Sandeep Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.14971",
    "title": "Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and Semantically-Rich Vision-Language Models",
    "abstract": "           Vision-language models (VLMs) have achieved significant strides in recent times specially in multimodal tasks, yet they remain susceptible to adversarial attacks on their vision components. To address this, we propose Sim-CLIP, an unsupervised adversarial fine-tuning method that enhances the robustness of the widely-used CLIP vision encoder against such attacks while maintaining semantic richness and specificity. By employing a Siamese architecture with cosine similarity loss, Sim-CLIP learns semantically meaningful and attack-resilient visual representations without requiring large batch sizes or momentum encoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned CLIP encoder exhibit significantly enhanced robustness against adversarial attacks, while preserving semantic meaning of the perturbed images. Notably, Sim-CLIP does not require additional training or fine-tuning of the VLM itself; replacing the original vision encoder with our fine-tuned Sim-CLIP suffices to provide robustness. This work underscores the significance of reinforcing foundational models like CLIP to safeguard the reliability of downstream VLM applications, paving the way for more secure and effective multimodal systems.         ",
    "url": "https://arxiv.org/abs/2407.14971",
    "authors": [
      "Md Zarif Hossain",
      "Ahmed Imteaj"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.18715",
    "title": "BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation",
    "abstract": "           Scene Graph Generation (SGG) remains a challenging task due to its compositional property. Previous approaches improve prediction efficiency through end-to-end learning. However, these methods exhibit limited performance as they assume unidirectional conditioning between entities and predicates, which restricts effective information interaction. To address this limitation, we propose a novel bidirectional conditioning factorization in a semantic-aligned space for SGG, enabling efficient and generalizable interaction between entities and predicates. Specifically, we introduce an end-to-end scene graph generation model, the Bidirectional Conditioning Transformer (BCTR), to implement this factorization. BCTR consists of two key modules. First, the Bidirectional Conditioning Generator (BCG) performs multi-stage interactive feature augmentation between entities and predicates, enabling mutual enhancement between these predictions. Second, Random Feature Alignment (RFA) is present to regularize feature space by distilling multi-modal knowledge from pre-trained models. Within this regularized feature space, BCG is feasible to capture interaction patterns across diverse relationships during training, and the learned interaction patterns can generalize to unseen but semantically related relationships during inference. Extensive experiments on Visual Genome and Open Image V6 show that BCTR achieves state-of-the-art performance on both benchmarks.         ",
    "url": "https://arxiv.org/abs/2407.18715",
    "authors": [
      "Peng Hao",
      "Xiaobing Wang",
      "Yingying Jiang",
      "Hanchao Jia",
      "Xiaoshuai Hao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.21063",
    "title": "Network and Sentiment Analysis of Enron Emails",
    "abstract": "           The objective of the research was to analyze e-mails exchanged at Enron, a power company that declared bankruptcy in 2001 following an investigation into unethical operations regarding their financials. Like other researchers, we identify the most important employees and detect communities using network science methods. We find that the importance of a person depends on the centrality measure used; while the communities we detected resembled the formal organizational structure of the company. In addition, because previous work required that 10 e-mails be sent and received for an e-mail relationship to exist, we analyzed the effect of different thresholds on the results and found that results were very dependent on the threshold used. We also performed sentiment analyses on the e-mails to evaluate whether sentiment changed over time and found that the sentiments of the e-mails do not give insight into the financial wellbeing of Enron. Our results provide insight into how information flowed through Enron, who the key employees were, and e-mail sentiment before and after the crisis         ",
    "url": "https://arxiv.org/abs/2407.21063",
    "authors": [
      "Natnael Belay"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2408.01271",
    "title": "HRFT: Mining High-Frequency Risk Factor Collections End-to-End via Transformer",
    "abstract": "           In quantitative trading, transforming historical stock data into interpretable, formulaic risk factors enhances the identification of market volatility and risk. Despite recent advancements in neural networks for extracting latent risk factors, these models remain limited to feature extraction and lack explicit, formulaic risk factor designs. By viewing symbolic mathematics as a language where valid mathematical expressions serve as meaningful \"sentences\" we propose framing the task of mining formulaic risk factors as a language modeling problem. In this paper, we introduce an end to end methodology, Intraday Risk Factor Transformer (IRFT), to directly generate complete formulaic risk factors, including constants. We use a hybrid symbolic numeric vocabulary where symbolic tokens represent operators and stock features, and numeric tokens represent constants. We train a Transformer model on high frequency trading (HFT) datasets to generate risk factors without relying on a predefined skeleton of operators. It determines the general form of the stock volatility law, including constants. We refine the predicted constants using the Broyden Fletcher Goldfarb Shanno (BFGS) algorithm to mitigate non linear issues. Compared to the ten approaches in SRBench, an active benchmark for symbolic regression (SR), IRFT achieves a 30% higher investment return on the HS300 and SP500 datasets, while achieving inference times that are orders of magnitude faster than existing methods in HF risk factor mining tasks.         ",
    "url": "https://arxiv.org/abs/2408.01271",
    "authors": [
      "Wenyan Xu",
      "Rundong Wang",
      "Chen Li",
      "Yonghong Hu",
      "Zhonghua Lu"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2408.02697",
    "title": "Why Rectified Power Unit Networks Fail and How to Improve It: An Effective Theory Perspective",
    "abstract": "           The Rectified Power Unit (RePU) activation functions, unlike the Rectified Linear Unit (ReLU), have the advantage of being a differentiable function when constructing neural networks. However, it can be experimentally observed when deep layers are stacked, neural networks constructed with RePU encounter critical issues. These issues include the values exploding or vanishing and failure of training. And these happen regardless of the hyperparameter initialization. From the perspective of effective theory, we aim to identify the causes of this phenomenon and propose a new activation function that retains the advantages of RePU while overcoming its drawbacks.         ",
    "url": "https://arxiv.org/abs/2408.02697",
    "authors": [
      "Taeyoung Kim",
      "Myungjoo Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.03077",
    "title": "Backdoor defense, learnability and obfuscation",
    "abstract": "           We introduce a formal notion of defendability against backdoors using a game between an attacker and a defender. In this game, the attacker modifies a function to behave differently on a particular input known as the \"trigger\", while behaving the same almost everywhere else. The defender then attempts to detect the trigger at evaluation time. If the defender succeeds with high enough probability, then the function class is said to be defendable. The key constraint on the attacker that makes defense possible is that the attacker's strategy must work for a randomly-chosen trigger. Our definition is simple and does not explicitly mention learning, yet we demonstrate that it is closely connected to learnability. In the computationally unbounded setting, we use a voting algorithm of Hanneke et al. (2022) to show that defendability is essentially determined by the VC dimension of the function class, in much the same way as PAC learnability. In the computationally bounded setting, we use a similar argument to show that efficient PAC learnability implies efficient defendability, but not conversely. On the other hand, we use indistinguishability obfuscation to show that the class of polynomial size circuits is not efficiently defendable. Finally, we present polynomial size decision trees as a natural example for which defense is strictly easier than learning. Thus, we identify efficient defendability as a notable intermediate concept in between efficient learnability and obfuscation.         ",
    "url": "https://arxiv.org/abs/2409.03077",
    "authors": [
      "Paul Christiano",
      "Jacob Hilton",
      "Victor Lecomte",
      "Mark Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.08411",
    "title": "Social Equity Based Optimal Power Flow Framework to Hedge Against Price Events",
    "abstract": "           With the increasing frequency of high impact low probability events, electricity markets are experiencing significant price spikes more often. This paper proposes a novel social equity driven optimal power flow framework to mitigate the adverse effects of price events that lead to such price spikes. The framework integrates social welfare optimization with socioeconomic considerations by including a socioeconomic score that quantifies the energy burden and socioeconomic status of consumers. By incorporating both supply cost and consumer satisfaction, the model aims to achieve a balanced and fair distribution of resources during price events, while considering resource scarcity and possible load curtailment. The proposed framework is tested for convergence on modified versions of the PJM 5-bus system and IEEE 24-bus reliability test system, discussing its potential effectiveness in enhancing social equity and optimizing power flow under system security constraints. Sensitivity analysis further highlights the impact of socioeconomic score on social welfare, providing insights for future improvements.         ",
    "url": "https://arxiv.org/abs/2409.08411",
    "authors": [
      "Sachinth Viththarachchige",
      "Demy Alexander",
      "Sarangan Rajendran",
      "Visvakumar Aravinthan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.10071",
    "title": "Towards Physically-Realizable Adversarial Attacks in Embodied Vision Navigation",
    "abstract": "           The deployment of embodied navigation agents in safety-critical environments raises concerns about their vulnerability to adversarial attacks on deep neural networks. However, current attack methods often lack practicality due to challenges in transitioning from the digital to the physical world, while existing physical attacks for object detection fail to achieve both multi-view effectiveness and naturalness. To address this, we propose a practical attack method for embodied navigation by attaching adversarial patches with learnable textures and opacity to objects. Specifically, to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling, which uses feedback from the navigation model to optimize the patch's texture. To make the patch inconspicuous to human observers, we introduce a two-stage opacity optimization mechanism, where opacity is refined after texture optimization. Experimental results show our adversarial patches reduce navigation success rates by about 40%, outperforming previous methods in practicality, effectiveness, and naturalness. Code is available at: [this https URL].         ",
    "url": "https://arxiv.org/abs/2409.10071",
    "authors": [
      "Meng Chen",
      "Jiawei Tu",
      "Chao Qi",
      "Yonghao Dang",
      "Feng Zhou",
      "Wei Wei",
      "Jianqin Yin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.10549",
    "title": "Confronting Project Conflicts into Success: a Complex Systems Design Approach to Resolving Stalemates",
    "abstract": "           In today's complex projects development, stakeholders are often involved too late. There is also in many cases a one-sided technical focus that only focuses on the system's behaviour and does not integrate the individual stakeholder preferences. This locks stakeholders into a 'technical' conflict instead of being able to emerge from it 'socially'. Moreover, stakeholders are often involved a-posteriori in a multi-faceted development process which is untransparent, leading to stalemates or even artefacts that nobody ever wants. There is thus a need for a purely associative and a-priori design-supported approach that integrates both system's reality and stakeholder's interests within a joint agreement and technical framework. The state-of-the-art Preferendus, the computer-aided design engine embedded within the proven Open Design Systems (Odesys) methodology, is a neutral tool in confronting complexity into success. The Preferendus is deployed to co-creatively generate a best-fit-for-common-purpose solution for a number of wind farm related degrees of freedom, project constraints and given a number of stakeholder objective functions. Since, the Preferendus design potential for a stalemate depends strongly on stakeholder interest, importance and trust, in this paper an structured stakeholder judgement approach is introduced to transparently arrive at individual stakeholder weights using a choice-based conjoint analysis (CBCA) method. This method also allows for obtaining an initial estimate for the individual stakeholder preference functions. By modelling disputable exogenous factors as endogenous design parameters, it is also shown for which factors the stalemate problem is indeed both technically and socially (un)solvable, while interests and reality are conjoined.         ",
    "url": "https://arxiv.org/abs/2409.10549",
    "authors": [
      "L.G. Teuber",
      "A.R.M. Wolfert"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2409.13222",
    "title": "3D-GSW: 3D Gaussian Splatting for Robust Watermarking",
    "abstract": "           As 3D Gaussian Splatting (3D-GS) gains significant attention and its commercial usage increases, the need for watermarking technologies to prevent unauthorized use of the 3D-GS models and rendered images has become increasingly important. In this paper, we introduce a robust watermarking method for 3D-GS that secures ownership of both the model and its rendered images. Our proposed method remains robust against distortions in rendered images and model attacks while maintaining high rendering quality. To achieve these objectives, we present Frequency-Guided Densification (FGD), which removes 3D Gaussians based on their contribution to rendering quality, enhancing real-time rendering and the robustness of the message. FGD utilizes Discrete Fourier Transform to split 3D Gaussians in high-frequency areas, improving rendering quality. Furthermore, we employ a gradient mask for 3D Gaussians and design a wavelet-subband loss to enhance rendering quality. Our experiments show that our method embeds the message in the rendered images invisibly and robustly against various attacks, including model distortion. Our method achieves state-of-the-art performance.         ",
    "url": "https://arxiv.org/abs/2409.13222",
    "authors": [
      "Youngdong Jang",
      "Hyunje Park",
      "Feng Yang",
      "Heeju Ko",
      "Euijin Choo",
      "Sangpil Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.13870",
    "title": "Instruct-Tuning Pretrained Causal Language Models for Ancient Greek Papyrology and Epigraphy",
    "abstract": "           This article presents an experiment in fine-tuning a pretrained causal language model (Meta's Llama 3.1 8B Instruct) to assist with restoring missing or illegible characters in ancient Greek inscriptions and documentary papyri. Utilizing a straightforward instruction-based approach and a 95%/5% train/test split, the papyrus restoration model achieved a character error rate (CER) of 14.9%, a top-1 accuracy of 73.5%, and a top-20 accuracy of 86.0% for sequences up to 10 characters. A model was also fine-tuned for geographic attribution, reaching a top-1 accuracy of 66.4% and a top-3 accuracy of 79.9%. In chronological attribution, it demonstrated an average deviation of 21.7 years from the actual terminus post/ante quem, with a median deviation of 0 years. For inscriptions, the restoration model achieved a CER of 20.5%, a top-1 accuracy of 63.7%, and a top-20 accuracy of 83.0% for sequences up to 10 characters. In geographic attribution, it attained a top-1 accuracy of 75.0% and a top-3 accuracy of 83.7%, while in dating, it had an average deviation of 37.1 years and a median deviation of 3 years from the actual date range. Benchmarked against the state-of-the-art model (Ithaca) on a shared test set and on recently edited inscriptions, the instruction-tuned models excelled in text restoration, while also offering the practical advantage of ignoring spaces during reconstruction, which aligns with the scriptio continua of ancient textual artifacts. However, their performance in geographic and chronological attribution was lower than Ithaca's. To evaluate the approach in a more even setup, the instruction model was retrained with an 80%/10%/10% train-validation-test split, and still outperformed Ithaca in text restoration. The results suggest that fine-tuning larger pretrained causal language models using instruction templates for emendations and conjectures to ancient texts holds promise.         ",
    "url": "https://arxiv.org/abs/2409.13870",
    "authors": [
      "Eric Cullhed"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.16794",
    "title": "Optimal Denial-of-Service Attacks Against Partially-Observable Real-Time Monitoring Systems",
    "abstract": "           In this paper, we investigate the impact of denial-of-service attacks on the status updating of a cyber-physical system with one or more sensors connected to a remote monitor via unreliable channels. We approach the problem from the perspective of an adversary that can strategically jam a subset of the channels. The sources are modeled as Markov chains, and the performance of status updating is measured based on the age of incorrect information at the monitor. Our objective is to derive jamming policies that strike a balance between the degradation of the system's performance and the conservation of the adversary's energy. For a single-source scenario, we formulate the problem as a partially-observable Markov decision process, and rigorously prove that the optimal jamming policy is of a threshold form. We then extend the problem to a multi-source scenario. We formulate this problem as a restless multi-armed bandit, and provide a jamming policy based on the Whittle's index. Our numerical results highlight the performance of our policies compared to baseline policies.         ",
    "url": "https://arxiv.org/abs/2409.16794",
    "authors": [
      "Saad Kriouile",
      "Mohamad Assaad",
      "Amira Alloum",
      "Touraj Soleymani"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2409.17994",
    "title": "CRoP: Context-wise Robust Static Human-Sensing Personalization",
    "abstract": "           The advancement in deep learning and internet-of-things have led to diverse human sensing applications. However, distinct patterns in human sensing, influenced by various factors or contexts, challenge the generic neural network model's performance due to natural distribution shifts. To address this, personalization tailors models to individual users. Yet most personalization studies overlook intra-user heterogeneity across contexts in sensory data, limiting intra-user generalizability. This limitation is especially critical in clinical applications, where limited data availability hampers both generalizability and personalization. Notably, intra-user sensing attributes are expected to change due to external factors such as treatment progression, further complicating the challenges. To address the intra-user generalization challenge, this work introduces CRoP, a novel static personalization approach. CRoP leverages off-the-shelf pre-trained models as generic starting points and captures user-specific traits through adaptive pruning on a minimal sub-network while preserving generic knowledge in the remaining parameters. CRoP demonstrates superior personalization effectiveness and intra-user robustness across four human-sensing datasets, including two from real-world health domains, underscoring its practical and social impact. Additionally, to support CRoP's generalization ability and design choices, we provide empirical justification through gradient inner product analysis, ablation studies, and comparisons against state-of-the-art baselines.         ",
    "url": "https://arxiv.org/abs/2409.17994",
    "authors": [
      "Sawinder Kaur",
      "Avery Gump",
      "Jingyu Xin",
      "Yi Xiao",
      "Harshit Sharma",
      "Nina R Benway",
      "Jonathan L Preston",
      "Asif Salekin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.01010",
    "title": "Code Interviews: Design and Evaluation of a More Authentic Assessment for Introductory Programming Assignments",
    "abstract": "           Generative artificial intelligence poses new challenges around assessment, increasingly driving introductory programming educators to employ invigilated exams. But exams do not afford more authentic programming experiences that involve planning, implementing, and debugging programs with computer interaction. In this experience report, we describe code interviews: a more authentic assessment method for take-home programming assignments. Through action research, we experimented with varying the number and type of questions as well as whether interviews were conducted individually or with groups of students. To scale the program, we converted most of our weekly teaching assistant (TA) sections to conduct code interviews on 5 major weekly take-home programming assignments. By triangulating data from 5 sources, we identified 4 themes. Code interviews (1) pushed students to discuss their work, motivating more nuanced but sometimes repetitive insights; (2) enabled peer learning, reducing stress in some ways but increasing stress in other ways; (3) scaled with TA-led sections, replacing familiar practice with an unfamiliar assessment; (4) focused on student contributions, limiting opportunities for TAs to give guidance and feedback. We conclude by discussing the different decisions about the design of code interviews with implications for student experience, academic integrity, and teaching workload.         ",
    "url": "https://arxiv.org/abs/2410.01010",
    "authors": [
      "Suhas Kannam",
      "Yuri Yang",
      "Aarya Dharm",
      "Kevin Lin"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2410.06304",
    "title": "FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning",
    "abstract": "           Hallucinations in large language models (LLMs) pose significant challenges in tasks requiring complex multi-step reasoning, such as mathematical problem-solving. Existing approaches primarily detect the presence of hallucinations but lack a nuanced understanding of their types and manifestations. In this paper, we first introduce a comprehensive taxonomy that categorizes the common hallucinations in mathematical reasoning task into six types: fabrication, factual inconsistency, context inconsistency, instruction inconsistency, logical inconsistency, and logical error. We then propose FG-PRM (Fine-Grained Process Reward Model), an augmented model designed to detect and mitigate hallucinations in a fine-grained, step-level manner. To address the limitations of manually labeling training data, we propose an automated method for generating fine-grained hallucination data using LLMs. By injecting hallucinations into reasoning steps of correct solutions, we create a diverse and balanced synthetic dataset for training FG-PRM, which consists of six specialized Process Reward Models (PRMs), each tailored to detect a specific hallucination type. Our FG-PRM demonstrates superior performance across two key tasks: 1) Fine-grained hallucination detection: classifying hallucination types for each reasoning step; and 2) Verification: ranking multiple LLM-generated outputs to select the most accurate solution, mitigating reasoning hallucinations. Our experiments show that FG-PRM outperforms ChatGPT-3.5 and Claude-3 on fine-grained hallucination detection and substantially boosts the performance of LLMs on GSM8K and MATH benchmarks.         ",
    "url": "https://arxiv.org/abs/2410.06304",
    "authors": [
      "Ruosen Li",
      "Ziming Luo",
      "Xinya Du"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2410.08900",
    "title": "A Benchmark for Cross-Domain Argumentative Stance Classification on Social Media",
    "abstract": "           Argumentative stance classification plays a key role in identifying authors' viewpoints on specific topics. However, generating diverse pairs of argumentative sentences across various domains is challenging. Existing benchmarks often come from a single domain or focus on a limited set of topics. Additionally, manual annotation for accurate labeling is time-consuming and labor-intensive. To address these challenges, we propose leveraging platform rules, readily available expert-curated content, and large language models to bypass the need for human annotation. Our approach produces a multidomain benchmark comprising 4,498 topical claims and 30,961 arguments from three sources, spanning 21 domains. We benchmark the dataset in fully supervised, zero-shot, and few-shot settings, shedding light on the strengths and limitations of different methodologies. We release the dataset and code in this study at hidden for anonymity.         ",
    "url": "https://arxiv.org/abs/2410.08900",
    "authors": [
      "Jiaqing Yuan",
      "Ruijie Xi",
      "Munindar P. Singh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.09797",
    "title": "Task Adaptive Feature Distribution Based Network for Few-shot Fine-grained Target Classification",
    "abstract": "           Metric-based few-shot fine-grained classification has shown promise due to its simplicity and efficiency. However, existing methods often overlook task-level special cases and struggle with accurate category description and irrelevant sample information. To tackle these, we propose TAFD-Net: a task adaptive feature distribution network. It features a task-adaptive component for embedding to capture task-level nuances, an asymmetric metric for calculating feature distribution similarities between query samples and support categories, and a contrastive measure strategy to boost performance. Extensive experiments have been conducted on three datasets and the experimental results show that our proposed algorithm outperforms recent incremental learning algorithms.         ",
    "url": "https://arxiv.org/abs/2410.09797",
    "authors": [
      "Ping Li",
      "Hongbo Wang",
      "Lei Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.14148",
    "title": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment",
    "abstract": "           The recent advancements in large language models (LLMs) and pre-trained vision models have accelerated the development of vision-language large models (VLLMs), enhancing the interaction between visual and linguistic modalities. Despite their notable success across various domains, VLLMs face challenges in modality alignment, which can lead to issues like hallucinations and unsafe content generation. Current alignment techniques often rely on coarse feedback and external datasets, limiting scalability and performance. In this paper, we propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel self-alignment method that utilizes the model's own visual encoder as a fine-grained verifier to improve vision-language alignment without the need for additional data. By leveraging token-level feedback from the vision encoder, FiSAO significantly improves vision-language alignment, even surpassing traditional preference tuning methods that require additional data. Through both theoretical analysis and experimental validation, we demonstrate that FiSAO effectively addresses the misalignment problem in VLLMs, marking the first instance of token-level rewards being applied to such models.         ",
    "url": "https://arxiv.org/abs/2410.14148",
    "authors": [
      "Chenhang Cui",
      "An Zhang",
      "Yiyang Zhou",
      "Zhaorun Chen",
      "Gelei Deng",
      "Huaxiu Yao",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2410.14970",
    "title": "Taming the Long Tail in Human Mobility Prediction",
    "abstract": "           With the popularity of location-based services, human mobility prediction plays a key role in enhancing personalized navigation, optimizing recommendation systems, and facilitating urban mobility and planning. This involves predicting a user's next POI (point-of-interest) visit using their past visit history. However, the uneven distribution of visitations over time and space, namely the long-tail problem in spatial distribution, makes it difficult for AI models to predict those POIs that are less visited by humans. In light of this issue, we propose the Long-Tail Adjusted Next POI Prediction (LoTNext) framework for mobility prediction, combining a Long-Tailed Graph Adjustment module to reduce the impact of the long-tailed nodes in the user-POI interaction graph and a novel Long-Tailed Loss Adjustment module to adjust loss by logit score and sample weight adjustment strategy. Also, we employ the auxiliary prediction task to enhance generalization and accuracy. Our experiments with two real-world trajectory datasets demonstrate that LoTNext significantly surpasses existing state-of-the-art works. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.14970",
    "authors": [
      "Xiaohang Xu",
      "Renhe Jiang",
      "Chuang Yang",
      "Zipei Fan",
      "Kaoru Sezaki"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.16527",
    "title": "Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A Comparative Analysis",
    "abstract": "           This report presents a comparative analysis of open-source vulnerability scanners for conversational large language models (LLMs). As LLMs become integral to various applications, they also present potential attack surfaces, exposed to security risks such as information leakage and jailbreak attacks. Our study evaluates prominent scanners - Garak, Giskard, PyRIT, and CyberSecEval - that adapt red-teaming practices to expose these vulnerabilities. We detail the distinctive features and practical use of these scanners, outline unifying principles of their design and perform quantitative evaluations to compare them. These evaluations uncover significant reliability issues in detecting successful attacks, highlighting a fundamental gap for future development. Additionally, we contribute a preliminary labelled dataset, which serves as an initial step to bridge this gap. Based on the above, we provide strategic recommendations to assist organizations choose the most suitable scanner for their red-teaming needs, accounting for customizability, test suite comprehensiveness, and industry-specific use cases.         ",
    "url": "https://arxiv.org/abs/2410.16527",
    "authors": [
      "Jonathan Brokman",
      "Omer Hofman",
      "Oren Rachmil",
      "Inderjeet Singh",
      "Vikas Pahuja",
      "Rathina Sabapathy Aishvariya Priya",
      "Amit Giloni",
      "Roman Vainshtein",
      "Hisashi Kojima"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.17221",
    "title": "Scalable spectral representations for multi-agent reinforcement learning in network MDPs",
    "abstract": "           Network Markov Decision Processes (MDPs), a popular model for multi-agent control, pose a significant challenge to efficient learning due to the exponential growth of the global state-action space with the number of agents. In this work, utilizing the exponential decay property of network dynamics, we first derive scalable spectral local representations for network MDPs, which induces a network linear subspace for the local $Q$-function of each agent. Building on these local spectral representations, we design a scalable algorithmic framework for continuous state-action network MDPs, and provide end-to-end guarantees for the convergence of our algorithm. Empirically, we validate the effectiveness of our scalable representation-based approach on two benchmark problems, and demonstrate the advantages of our approach over generic function approximation approaches to representing the local $Q$-functions.         ",
    "url": "https://arxiv.org/abs/2410.17221",
    "authors": [
      "Zhaolin Ren",
      "Runyu Zhang",
      "Bo Dai",
      "Na Li"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2410.18583",
    "title": "Benchmarking Graph Learning for Drug-Drug Interaction Prediction",
    "abstract": "           Predicting drug-drug interaction (DDI) plays an important role in pharmacology and healthcare for identifying potential adverse interactions and beneficial combination therapies between drug pairs. Recently, a flurry of graph learning methods have been introduced to predict drug-drug interactions. However, evaluating existing methods has several limitations, such as the absence of a unified comparison framework for DDI prediction methods, lack of assessments in meaningful real-world scenarios, and insufficient exploration of side information usage. In order to address these unresolved limitations in the literature, we propose a DDI prediction benchmark on graph learning. We first conduct unified evaluation comparison among existing methods. To meet realistic scenarios, we further evaluate the performance of different methods in settings with new drugs involved and examine the performance across different DDI types. Component analysis is conducted on the biomedical network to better utilize side information. Through this work, we hope to provide more insights for the problem of DDI prediction. Our implementation and data is open-sourced at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.18583",
    "authors": [
      "Zhenqian Shen",
      "Mingyang Zhou",
      "Yongqi Zhang",
      "Quanming Yao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.20911",
    "title": "Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks",
    "abstract": "           Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker's LLM to disrupt their own operations (passive defense) or even compromise the attacker's machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker's LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: this https URL ",
    "url": "https://arxiv.org/abs/2410.20911",
    "authors": [
      "Dario Pasquini",
      "Evgenios M. Kornaropoulos",
      "Giuseppe Ateniese"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.22339",
    "title": "DAWN: Designing Distributed Agents in a Worldwide Network",
    "abstract": "           The rapid evolution of Large Language Models (LLMs) has transformed them from basic conversational tools into sophisticated entities capable of complex reasoning and decision-making. These advancements have led to the development of specialized LLM-based agents designed for diverse tasks such as coding and web browsing. As these agents become more capable, the need for a robust framework that facilitates global communication and collaboration among them towards advanced objectives has become increasingly critical. Distributed Agents in a Worldwide Network (DAWN) addresses this need by offering a versatile framework that integrates LLM-based agents with traditional software systems, enabling the creation of agentic applications suited for a wide range of use cases. DAWN enables distributed agents worldwide to register and be easily discovered through Gateway Agents. Collaborations among these agents are coordinated by a Principal Agent equipped with reasoning strategies. DAWN offers three operational modes: No-LLM Mode for deterministic tasks, Copilot for augmented decision-making, and LLM Agent for autonomous operations. Additionally, DAWN ensures the safety and security of agent collaborations globally through a dedicated safety, security, and compliance layer, protecting the network against attackers and adhering to stringent security and compliance standards. These features make DAWN a robust network for deploying agent-based applications across various industries.         ",
    "url": "https://arxiv.org/abs/2410.22339",
    "authors": [
      "Zahra Aminiranjbar",
      "Jianan Tang",
      "Qiudan Wang",
      "Shubha Pant",
      "Mahesh Viswanathan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2410.22883",
    "title": "Dataset Awareness is not Enough: Implementing Sample-level Tail Encouragement in Long-tailed Self-supervised Learning",
    "abstract": "           Self-supervised learning (SSL) has shown remarkable data representation capabilities across a wide range of datasets. However, when applied to real-world datasets with long-tailed distributions, performance on multiple downstream tasks degrades significantly. Recently, the community has begun to focus more on self-supervised long-tailed learning. Some works attempt to transfer temperature mechanisms to self-supervised learning or use category-space uniformity constraints to balance the representation of different categories in the embedding space to fight against long-tail distributions. However, most of these approaches focus on the joint optimization of all samples in the dataset or on constraining the category distribution, with little attention given to whether each individual sample is optimally guided during training. To address this issue, we propose Temperature Auxiliary Sample-level Encouragement (TASE). We introduce pseudo-labels into self-supervised long-tailed learning, utilizing pseudo-label information to drive a dynamic temperature and re-weighting strategy. Specifically, We assign an optimal temperature parameter to each sample. Additionally, we analyze the lack of quantity awareness in the temperature parameter and use re-weighting to compensate for this deficiency, thereby achieving optimal training patterns at the sample level. Comprehensive experimental results on six benchmarks across three datasets demonstrate that our method achieves outstanding performance in improving long-tail recognition, while also exhibiting high robustness.         ",
    "url": "https://arxiv.org/abs/2410.22883",
    "authors": [
      "Haowen Xiao",
      "Guanghui Liu",
      "Xinyi Gao",
      "Yang Li",
      "Fengmao Lv",
      "Jielei Chu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.23091",
    "title": "CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense",
    "abstract": "           Despite ongoing efforts to defend neural classifiers from adversarial attacks, they remain vulnerable, especially to unseen attacks. In contrast, humans are difficult to be cheated by subtle manipulations, since we make judgments only based on essential factors. Inspired by this observation, we attempt to model label generation with essential label-causative factors and incorporate label-non-causative factors to assist data generation. For an adversarial example, we aim to discriminate the perturbations as non-causative factors and make predictions only based on the label-causative factors. Concretely, we propose a casual diffusion model (CausalDiff) that adapts diffusion models for conditional data generation and disentangles the two types of casual factors by learning towards a novel casual information bottleneck objective. Empirically, CausalDiff has significantly outperformed state-of-the-art defense methods on various unseen attacks, achieving an average robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on CIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition Benchmark). The code is available at \\href{this https URL}{this https URL}         ",
    "url": "https://arxiv.org/abs/2410.23091",
    "authors": [
      "Mingkun Zhang",
      "Keping Bi",
      "Wei Chen",
      "Quanrun Chen",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.00299",
    "title": "RadFlag: A Black-Box Hallucination Detection Method for Medical Vision Language Models",
    "abstract": "           Generating accurate radiology reports from medical images is a clinically important but challenging task. While current Vision Language Models (VLMs) show promise, they are prone to generating hallucinations, potentially compromising patient care. We introduce RadFlag, a black-box method to enhance the accuracy of radiology report generation. Our method uses a sampling-based flagging technique to find hallucinatory generations that should be removed. We first sample multiple reports at varying temperatures and then use a Large Language Model (LLM) to identify claims that are not consistently supported across samples, indicating that the model has low confidence in those claims. Using a calibrated threshold, we flag a fraction of these claims as likely hallucinations, which should undergo extra review or be automatically rejected. Our method achieves high precision when identifying both individual hallucinatory sentences and reports that contain hallucinations. As an easy-to-use, black-box system that only requires access to a model's temperature parameter, RadFlag is compatible with a wide range of radiology report generation models and has the potential to broadly improve the quality of automated radiology reporting.         ",
    "url": "https://arxiv.org/abs/2411.00299",
    "authors": [
      "Serena Zhang",
      "Sraavya Sambara",
      "Oishi Banerjee",
      "Julian Acosta",
      "L. John Fahrner",
      "Pranav Rajpurkar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.00660",
    "title": "Physics in Next-token Prediction",
    "abstract": "           We discovered the underlying physics in Next-token Prediction (NTP). We identified the law of information conservation within NTP and proposed the First Law of Information Capacity (IC-1), demonstrating that the essence of intelligence emergence in auto-regressive models is fundamentally a process of information transfer. We also introduced Landauer's Principle into NTP, formulating the Second Law of Information Capacity (IC-2), which establishes the relationship between auto-regressive model training and energy consumption. Additionally, we presented several corollaries, which hold practical significance for production practices. Finally, we demonstrate the consistency between the Law of Information Capacity and the Scaling Law for Neural Language Models, the Knowledge Capacity Scaling Laws, and the Scaling Laws for Precision.         ",
    "url": "https://arxiv.org/abs/2411.00660",
    "authors": [
      "Hongjun An",
      "Yiliang Song",
      "Xuelong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.01045",
    "title": "Towards Robust Text Classification: Mitigating Spurious Correlations with Causal Learning",
    "abstract": "           In text classification tasks, models often rely on spurious correlations for predictions, incorrectly associating irrelevant features with the target labels. This issue limits the robustness and generalization of models, especially when faced with out-of-distribution data where such spurious correlations no longer hold. To address this challenge, we propose the Causally Calibrated Robust Classifier (CCR), which aims to reduce models' reliance on spurious correlations and improve model robustness. Our approach integrates a causal feature selection method based on counterfactual reasoning, along with an unbiased inverse propensity weighting (IPW) loss function. By focusing on selecting causal features, we ensure that the model relies less on spurious features during prediction. We theoretically justify our approach and empirically show that CCR achieves state-of-the-art performance among methods without group labels, and in some cases, it can compete with the models that utilize group labels.         ",
    "url": "https://arxiv.org/abs/2411.01045",
    "authors": [
      "Yuqing Zhou",
      "Ziwei Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2411.01904",
    "title": "FPPL: An Efficient and Non-IID Robust Federated Continual Learning Framework",
    "abstract": "           Federated continual learning (FCL) aims to learn from sequential data stream in the decentralized federated learning setting, while simultaneously mitigating the catastrophic forgetting issue in classical continual learning. Existing FCL methods usually employ typical rehearsal mechanisms, which could result in privacy violations or additional onerous storage and computational burdens. In this work, an efficient and non-IID robust federated continual learning framework, called Federated Prototype-Augmented Prompt Learning (FPPL), is proposed. The FPPL can collaboratively learn lightweight prompts augmented by prototypes without rehearsal. On the client side, a fusion function is employed to fully leverage the knowledge contained in task-specific prompts for alleviating catastrophic forgetting. Additionally, global prototypes aggregated from the server are used to obtain unified representation through contrastive learning, mitigating the impact of non-IID-derived data heterogeneity. On the server side, locally uploaded prototypes are utilized to perform debiasing on the classifier, further alleviating the performance degradation caused by both non-IID and catastrophic forgetting. Empirical evaluations demonstrate the effectiveness of FPPL, achieving notable performance with an efficient design while remaining robust to diverse non-IID degrees. Code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2411.01904",
    "authors": [
      "Yuchen He",
      "Chuyun Shen",
      "Xiangfeng Wang",
      "Bo Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.06392",
    "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with Multi-Level CSR",
    "abstract": "           The growing volume of graph data may exhaust the main memory. It is crucial to design a disk-based graph storage system to ingest updates and analyze graphs efficiently. However, existing dynamic graph storage systems suffer from read or write amplification and face the challenge of optimizing both read and write performance simultaneously. To address this challenge, we propose LSMGraph, a novel dynamic graph storage system that combines the write-friendly LSM-tree and the read-friendly CSR. It leverages the multi-level structure of LSM-trees to optimize write performance while utilizing the compact CSR structures embedded in the LSM-trees to boost read performance. LSMGraph uses a new memory structure, MemGraph, to efficiently cache graph updates and uses a multi-level index to speed up reads within the multi-level structure. Furthermore, LSMGraph incorporates a vertex-grained version control mechanism to mitigate the impact of LSM-tree compaction on read performance and ensure the correctness of concurrent read and write operations. Our evaluation shows that LSMGraph significantly outperforms state-of-the-art (graph) storage systems on both graph update and graph analytical workloads.         ",
    "url": "https://arxiv.org/abs/2411.06392",
    "authors": [
      "Song Yu",
      "Shufeng Gong",
      "Qian Tao",
      "Sijie Shen",
      "Yanfeng Zhang",
      "Wenyuan Yu",
      "Pengxi Liu",
      "Zhixin Zhang",
      "Hongfu Li",
      "Xiaojian Luo",
      "Ge Yu",
      "Jingren Zhou"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2411.07168",
    "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network",
    "abstract": "           Mining machinery operating in variable environments faces high wear and unpredictable stress, challenging Predictive Maintenance (PdM). This paper introduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a hierarchical inference framework across edge devices, gateways, and cloud services for real-time condition monitoring. The system dynamically adjusts inference locations--on-device, on-gateway, or on-cloud--based on trade-offs among accuracy, latency, and battery life, leveraging Tiny Machine Learning (TinyML) techniques for model optimization on resource-constrained devices. Performance evaluations showed that on-sensor and on-gateway inference modes achieved over 90\\% classification accuracy, while cloud-based inference reached 99\\%. On-sensor inference reduced power consumption by approximately 44\\%, enabling up to 104 hours of operation. Latency was lowest for on-device inference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or cloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution for reliable anomaly detection and PdM, crucial for maintaining machinery uptime in remote environments. By balancing accuracy, latency, and energy consumption, this approach advances PdM frameworks for industrial applications.         ",
    "url": "https://arxiv.org/abs/2411.07168",
    "authors": [
      "Ra\u00fal de la Fuente",
      "Luciano Radrigan",
      "Anibal S Morales"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2411.07475",
    "title": "Degree Matrix Comparison for Graph Alignment",
    "abstract": "           Graph alignment considers the optimal node correspondence across networks. To advance unsupervised graph alignment algorithms on plain graphs, we propose Degree Matrix Comparison (DMC). Through extensive experiments and mathematical motivations, we demonstrate the potential of this method. Remarkably, DMC achieves up to 99% correct node alignment for 90%-overlap graphs and 100% accuracy for isomorphic graphs. Additionally, we propose a reduced version of DMC (Greedy DMC) that provides a solution to the graph alignment problem with lower time complexity. DMC could significantly impact graph alignment, offering a reliable solution for the task.         ",
    "url": "https://arxiv.org/abs/2411.07475",
    "authors": [
      "Ashley Wang",
      "Peter Chin"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2411.07612",
    "title": "A Simple Multi-agent Joint Prediction Method for Autonomous Driving",
    "abstract": "           Predicting future motions of road participants is an important task for driving autonomously. Most existing models excel at predicting the marginal trajectory of a single agent, but predicting joint trajectories for multiple agents that are consistent within a scene remains a challenge. Previous research has often focused on marginal predictions, but the importance of joint predictions has become increasingly apparent. Joint prediction aims to generate trajectories that are consistent across the entire scene. Our research builds upon the SIMPL baseline to explore methods for generating scene-consistent trajectories. We tested our algorithm on the Argoverse 2 dataset, and experimental results demonstrate that our approach can generate scene-consistent trajectories. Compared to the SIMPL baseline, our method significantly reduces the collision rate of joint trajectories within the scene.         ",
    "url": "https://arxiv.org/abs/2411.07612",
    "authors": [
      "Mingyi Wang",
      "Hongqun Zou",
      "Yifan Liu",
      "You Wang",
      "Guang Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2411.07699",
    "title": "RINO: Accurate, Robust Radar-Inertial Odometry with Non-Iterative Estimation",
    "abstract": "           Precise localization and mapping are critical for achieving autonomous navigation in self-driving vehicles. However, ego-motion estimation still faces significant challenges, particularly when GNSS failures occur or under extreme weather conditions (e.g., fog, rain, and snow). In recent years, scanning radar has emerged as an effective solution due to its strong penetration capabilities. Nevertheless, scanning radar data inherently contains high levels of noise, necessitating hundreds to thousands of iterations of optimization to estimate a reliable transformation from the noisy data. Such iterative solving is time-consuming, unstable, and prone to failure. To address these challenges, we propose an accurate and robust Radar-Inertial Odometry system, RINO, which employs a non-iterative solving approach. Our method decouples rotation and translation estimation and applies an adaptive voting scheme for 2D rotation estimation, enhancing efficiency while ensuring consistent solving time. Additionally, the approach implements a loosely coupled system between the scanning radar and an inertial measurement unit (IMU), leveraging Error-State Kalman Filtering (ESKF). Notably, we successfully estimated the uncertainty of the pose estimation from the scanning radar, incorporating this into the filter's Maximum A Posteriori estimation, a consideration that has been previously overlooked. Validation on publicly available datasets demonstrates that RINO outperforms state-of-the-art methods and baselines in both accuracy and robustness. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.07699",
    "authors": [
      "Shuocheng Yang",
      "Yueming Cao",
      "Shengbo Eben Li",
      "Jianqiang Wang",
      "Shaobing Xu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2411.08402",
    "title": "V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with Denoising Diffusion",
    "abstract": "           Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D object detection using LiDAR and camera data. However, these methods suffer from performance degradation in adverse weather conditions. The weatherrobust 4D radar provides Doppler and additional geometric information, raising the possibility of addressing this challenge. To this end, we present V2X-R, the first simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-R contains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point clouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes. Subsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for 3D object detection and implement it with various fusion strategies. To achieve weather-robust detection, we additionally propose a Multi-modal Denoising Diffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D radar feature as a condition to prompt the diffusion model to denoise noisy LiDAR features. Experiments show that our LiDAR-4D radar fusion pipeline demonstrates superior performance in the V2X-R dataset. Over and above this, our MDD module further improved the performance of basic fusion model by up to 5.73%/6.70% in foggy/snowy conditions with barely disrupting normal performance. The dataset and code will be publicly available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2411.08402",
    "authors": [
      "Xun Huang",
      "Jinlong Wang",
      "Qiming Xia",
      "Siheng Chen",
      "Bisheng Yang",
      "Xin Li",
      "Cheng Wang",
      "Chenglu Wen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.08449",
    "title": "Towards Evaluating Large Language Models for Graph Query Generation",
    "abstract": "           Large Language Models (LLMs) are revolutionizing the landscape of Generative Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging rapidly. However, when applied to database technologies, specifically query generation for graph databases and Knowledge Graphs (KGs), LLMs still face significant challenges. While research on LLM-driven query generation for Structured Query Language (SQL) exists, similar systems for graph databases remain underdeveloped. This paper presents a comparative study addressing the challenge of generating Cypher queries a powerful language for interacting with graph databases using open-access LLMs. We rigorously evaluate several LLM agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT) reasoning. Our empirical analysis of query generation accuracy reveals that Claude Sonnet 3.5 outperforms its counterparts in this specific domain. Further, we highlight promising future research directions to address the identified limitations and advance LLM-driven query generation for graph databases.         ",
    "url": "https://arxiv.org/abs/2411.08449",
    "authors": [
      "Siraj Munir",
      "Alessandro Aldini"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2411.08521",
    "title": "SAD-TIME: a Spatiotemporal-fused network for depression detection with Automated multi-scale Depth-wise and TIME-interval-related common feature extractor",
    "abstract": "           Background and Objective: Depression is a severe mental disorder, and accurate diagnosis is pivotal to the cure and rehabilitation of people with depression. However, the current questionnaire-based diagnostic methods could bring subjective biases and may be denied by subjects. In search of a more objective means of diagnosis, researchers have begun to experiment with deep learning-based methods for identifying depressive disorders in recent years. Methods: In this study, a novel Spatiotemporal-fused network with Automated multi-scale Depth-wise and TIME-interval-related common feature extractor (SAD-TIME) is proposed. SAD-TIME incorporates an automated nodes' common features extractor (CFE), a spatial sector (SpS), a modified temporal sector (TeS), and a domain adversarial learner (DAL). The CFE includes a multi-scale depth-wise 1D-convolutional neural network and a time-interval embedding generator, where the unique information of each channel is preserved. The SpS fuses the functional connectivity with the distance-based connectivity containing spatial position of EEG electrodes. A multi-head-attention graph convolutional network is also applied in the SpS to fuse the features from different EEG channels. The TeS is based on long short-term memory and graph transformer networks, where the temporal information of different time-windows is fused. Moreover, the DAL is used after the SpS to obtain the domain-invariant feature. Results: Experimental results under tenfold cross-validation show that the proposed SAD-TIME method achieves 92.00% and 94.00% depression classification accuracies on two datasets, respectively, in cross-subject mode. Conclusion: SAD-TIME is a robust depression detection model, where the automatedly-generated features, the SpS and the TeS assist the classification performance with the fusion of the innate spatiotemporal information in the EEG signals.         ",
    "url": "https://arxiv.org/abs/2411.08521",
    "authors": [
      "Han-Guang Wang",
      "Hui-Rang Hou",
      "Li-Cheng Jin",
      "Chen-Yang Xu",
      "Zhong-Yi Zhang",
      "Qing-Hao Meng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.08735",
    "title": "New advances in universal approximation with neural networks of minimal width",
    "abstract": "           Deep neural networks have achieved remarkable success in diverse applications, prompting the need for a solid theoretical foundation. Recent research has identified the minimal width $\\max\\{2,d_x,d_y\\}$ required for neural networks with input dimensions $d_x$ and output dimension $d_y$ that use leaky ReLU activations to universally approximate $L^p(\\mathbb{R}^{d_x},\\mathbb{R}^{d_y})$ on compacta. Here, we present an alternative proof for the minimal width of such neural networks, by directly constructing approximating networks using a coding scheme that leverages the properties of leaky ReLUs and standard $L^p$ results. The obtained construction has a minimal interior dimension of $1$, independent of input and output dimensions, which allows us to show that autoencoders with leaky ReLU activations are universal approximators of $L^p$ functions. Furthermore, we demonstrate that the normalizing flow LU-Net serves as a distributional universal approximator. We broaden our results to show that smooth invertible neural networks can approximate $L^p(\\mathbb{R}^{d},\\mathbb{R}^{d})$ on compacta when the dimension $d\\geq 2$, which provides a constructive proof of a classical theorem of Brenier and Gangbo. In addition, we use a topological argument to establish that for FNNs with monotone Lipschitz continuous activations, $d_x+1$ is a lower bound on the minimal width required for the uniform universal approximation of continuous functions $C^0(\\mathbb{R}^{d_x},\\mathbb{R}^{d_y})$ on compacta when $d_x\\geq d_y$.         ",
    "url": "https://arxiv.org/abs/2411.08735",
    "authors": [
      "Dennis Rochau",
      "Robin Chan",
      "Hanno Gottschalk"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Functional Analysis (math.FA)"
    ]
  },
  {
    "id": "arXiv:2411.09178",
    "title": "SAFES: Sequential Privacy and Fairness Enhancing Data Synthesis for Responsible AI",
    "abstract": "           As data-driven and AI-based decision making gains widespread adoption in most disciplines, it is crucial that both data privacy and decision fairness are appropriately addressed. While differential privacy (DP) provides a robust framework for guaranteeing privacy and several widely accepted methods have been proposed for improving fairness, the vast majority of existing literature treats the two concerns independently. For methods that do consider privacy and fairness simultaneously, they often only apply to a specific machine learning task, limiting their generalizability. In response, we introduce SAFES, a Sequential PrivAcy and Fairness Enhancing data Synthesis procedure that sequentially combines DP data synthesis with a fairness-aware data transformation. SAFES allows full control over the privacy-fairness-utility trade-off via tunable privacy and fairness parameters. We illustrate SAFES by combining AIM, a graphical model-based DP data synthesizer, with a popular fairness-aware data pre-processing transformation. Empirical evaluations on the Adult and COMPAS datasets demonstrate that for reasonable privacy loss, SAFES-generated synthetic data achieve significantly improved fairness metrics with relatively low utility loss.         ",
    "url": "https://arxiv.org/abs/2411.09178",
    "authors": [
      "Spencer Giddens",
      "Fang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.09463",
    "title": "Teaching Program Decomposition in CS1: A Conceptual Framework for Improved Code Quality",
    "abstract": "           Program decomposition is essential for developing maintainable and efficient software, yet it remains a challenging skill to teach and learn in introductory programming courses. What does program decomposition for procedural CS1 programs entail? How can CS1 students improve the decomposition of their programs? What scaffolded exercises can instructors use to teach program decomposition skills? We aim to answer all these questions by presenting a conceptual framework that (1) is grounded in the established code style principles, (2) provides a systematic approach that can be taught to students as an actionable strategy to improve the program decomposition of their programs, and (3) includes scaffolded exercises to be used in classroom activities. In addition, this systematic approach is automatable and can further be used to implement visualizers, automated feedback generators and digital tutors.         ",
    "url": "https://arxiv.org/abs/2411.09463",
    "authors": [
      "Georgiana Haldeman",
      "Judah Robbins Bernal",
      "Alec Wydra",
      "Paul Denny"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2411.09553",
    "title": "OOD-SEG: Out-Of-Distribution detection for image SEGmentation with sparse multi-class positive-only annotations",
    "abstract": "           Despite significant advancements, segmentation based on deep neural networks in medical and surgical imaging faces several challenges, two of which we aim to address in this work. First, acquiring complete pixel-level segmentation labels for medical images is time-consuming and requires domain expertise. Second, typical segmentation pipelines cannot detect out-of-distribution (OOD) pixels, leaving them prone to spurious outputs during deployment. In this work, we propose a novel segmentation approach exploiting OOD detection that learns only from sparsely annotated pixels from multiple positive-only classes. These multi-class positive annotations naturally fall within the in-distribution (ID) set. Unlabelled pixels may contain positive classes but also negative ones, including what is typically referred to as \\emph{background} in standard segmentation formulations. Here, we forgo the need for background annotation and consider these together with any other unseen classes as part of the OOD set. Our framework can integrate, at a pixel-level, any OOD detection approaches designed for classification tasks. To address the lack of existing OOD datasets and established evaluation metric for medical image segmentation, we propose a cross-validation strategy that treats held-out labelled classes as OOD. Extensive experiments on both multi-class hyperspectral and RGB surgical imaging datasets demonstrate the robustness and generalisation capability of our proposed framework.         ",
    "url": "https://arxiv.org/abs/2411.09553",
    "authors": [
      "Junwen Wang",
      "Zhonghao Wang",
      "Oscar MacCormac",
      "Jonathan Shapey",
      "Tom Vercauteren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2003.13648",
    "title": "Weakly-supervised land classification for coastal zone based on deep convolutional neural networks by incorporating dual-polarimetric characteristics into training dataset",
    "abstract": "           In this work we explore the performance of DCNNs on semantic segmentation using spaceborne polarimetric synthetic aperture radar (PolSAR) datasets. The semantic segmentation task using PolSAR data can be categorized as weakly supervised learning when the characteristics of SAR data and data annotating procedures are factored in. Datasets are initially analyzed for selecting feasible pre-training images. Then the differences between spaceborne and airborne datasets are examined in terms of spatial resolution and viewing geometry. In this study we used two dual-polarimetric images acquired by TerraSAR-X DLR. A novel method to produce training dataset with more supervised information is developed. Specifically, a series of typical classified images as well as intensity images serve as training datasets. A field survey is conducted for an area of about 20 square kilometers to obtain a ground truth dataset used for accuracy evaluation. Several transfer learning strategies are made for aforementioned training datasets which will be combined in a practicable order. Three DCNN models, including SegNet, U-Net, and LinkNet, are implemented next.         ",
    "url": "https://arxiv.org/abs/2003.13648",
    "authors": [
      "Sheng Sun",
      "Armando Marino",
      "Wenze Shui",
      "Zhongwen Hu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2103.10300",
    "title": "Asymptotics of Proximity Operator for Squared Loss and Performance Prediction of Nonconvex Sparse Signal Recovery",
    "abstract": "           Proximal splitting-based convex optimization is a promising approach to linear inverse problems because we can use some prior knowledge of the unknown variables explicitly. An understanding of the behavior of the optimization algorithms would be important for the tuning of the parameters and the development of new algorithms. In this paper, we first analyze the asymptotic property of the proximity operator for the squared loss function, which appears in the update equations of some proximal splitting methods for linear inverse problems. Our analysis shows that the output of the proximity operator can be characterized with a scalar random variable in the large system limit. Moreover, we apply the asymptotic result to the prediction of optimization algorithms for compressed sensing. Simulation results demonstrate that the MSE performance of the Douglas-Rachford algorithm can be well predicted in compressed sensing with the $\\ell_{1}$ optimization. We also examine the behavior of the prediction for the case with nonconvex smoothly clipped absolute deviation (SCAD) regularization.         ",
    "url": "https://arxiv.org/abs/2103.10300",
    "authors": [
      "Ryo Hayakawa"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2204.01645",
    "title": "3D microstructural generation from 2D images of cement paste using generative adversarial networks",
    "abstract": "           Establishing a realistic three-dimensional (3D) microstructure is a crucial step for studying microstructure development of hardened cement pastes. However, acquiring 3D microstructural images for cement often involves high costs and quality compromises. This paper proposes a generative adversarial networks-based method for generating 3D microstructures from a single two-dimensional (2D) image, capable of producing high-quality and realistic 3D images at low cost. In the method, a framework (CEM3DMG) is designed to synthesize 3D images by learning microstructural information from a 2D cross-sectional image. Experimental results show that CEM3DMG can generate realistic 3D images of large size. Visual observation confirms that the generated 3D images exhibit similar microstructural features to the 2D images, including similar pore distribution and particle morphology. Furthermore, quantitative analysis reveals that reconstructed 3D microstructures closely match the real 2D microstructure in terms of gray level histogram, phase proportions, and pore size distribution. The source code for CEM3DMG is available in the GitHub repository at: this https URL.         ",
    "url": "https://arxiv.org/abs/2204.01645",
    "authors": [
      "Xin Zhao",
      "Lin Wang",
      "Qinfei Li",
      "Heng Chen",
      "Shuangrong Liu",
      "Pengkun Hou",
      "Jiayuan Ye",
      "Yan Pei",
      "Xu Wu",
      "Jianfeng Yuan",
      "Haozhong Gao",
      "Bo Yang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2303.10116",
    "title": "Stack and Queue Numbers of Graphs Revisited",
    "abstract": "           A long-standing question of the mutual relation between the stack and queue numbers of a graph, explicitly emphasized by Dujmovi\u0107 and Wood in 2005, was \"half-answered\" by Dujmovi\u0107, Eppstein, Hickingbotham, Morin and Wood in 2022; they proved the existence of a graph family with the queue number at most 4 but unbounded stack number. We give an alternative very short, and still elementary, proof of the same fact.         ",
    "url": "https://arxiv.org/abs/2303.10116",
    "authors": [
      "Petr Hlin\u011bn\u00fd",
      "Adam Straka"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2306.03741",
    "title": "Pre-training Tensor-Train Networks Facilitates Machine Learning with Variational Quantum Circuits",
    "abstract": "           Variational quantum circuits (VQCs) hold promise for quantum machine learning on noisy intermediate-scale quantum (NISQ) devices. While tensor-train networks (TTNs) can enhance VQC representation and generalization, the resulting hybrid model, TTN-VQC, faces optimization challenges due to the Polyak-Lojasiewicz (PL) condition. To mitigate this challenge, we introduce Pre+TTN-VQC, a pre-trained TTN model combined with a VQC. Our theoretical analysis, grounded in two-stage empirical risk minimization, provides an upper bound on the transfer learning risk. It demonstrates the approach's advantages in overcoming the optimization challenge while maintaining TTN-VQC's generalization capability. We validate our findings through experiments on quantum dot and handwritten digit classification using simulated and actual NISQ environments.         ",
    "url": "https://arxiv.org/abs/2306.03741",
    "authors": [
      "Jun Qi",
      "Chao-Han Huck Yang",
      "Pin-Yu Chen",
      "Min-Hsiu Hsieh"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.02340",
    "title": "Generative Priors for MRI Reconstruction Trained from Magnitude-Only Images Using Phase Augmentation",
    "abstract": "           Purpose: In this work, we present a workflow to construct generic and robust generative image priors from magnitude-only images. The priors can then be used for regularization in reconstruction to improve image quality. Methods: The workflow begins with the preparation of training datasets from magnitude-only MR images. This dataset is then augmented with phase information and used to train generative priors of complex images. Finally, trained priors are evaluated using both linear and nonlinear reconstruction for compressed sensing parallel imaging with various undersampling schemes. Results: The results of our experiments demonstrate that priors trained on complex images outperform priors trained only on magnitude images. Additionally, a prior trained on a larger dataset exhibits higher robustness. Finally, we show that the generative priors are superior to L1 -wavelet regularization for compressed sensing parallel imaging with high undersampling. Conclusion: These findings stress the importance of incorporating phase information and leveraging large datasets to raise the performance and reliability of the generative priors for MRI reconstruction. Phase augmentation makes it possible to use existing image databases for training.         ",
    "url": "https://arxiv.org/abs/2308.02340",
    "authors": [
      "Guanxiong Luo",
      "Xiaoqing Wang",
      "Mortiz Blumenthal",
      "Martin Schilling",
      "Erik Hans Ulrich Rauf",
      "Raviteja Kotikalapudi",
      "Niels Focke",
      "Martin Uecker"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2309.13459",
    "title": "A Model-Agnostic Graph Neural Network for Integrating Local and Global Information",
    "abstract": "           Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, however, existing GNNs suffer from two significant limitations: a lack of interpretability in their results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to effectively integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and demonstrate its power to represent layer-wise neighborhood mixing. We conduct comprehensive numerical studies using simulated data to demonstrate the superior performance of MaGNet in comparison to several state-of-the-art alternatives. Furthermore, we apply MaGNet to a real-world case study aimed at extracting task-critical information from brain activity data, thereby highlighting its effectiveness in advancing scientific research.         ",
    "url": "https://arxiv.org/abs/2309.13459",
    "authors": [
      "Wenzhuo Zhou",
      "Annie Qu",
      "Keiland W. Cooper",
      "Norbert Fortin",
      "Babak Shahbaba"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05271",
    "title": "Feature learning as alignment: a structural property of gradient descent in non-linear neural networks",
    "abstract": "           Understanding the mechanisms through which neural networks extract statistics from input-label pairs through feature learning is one of the most important unsolved problems in supervised learning. Prior works demonstrated that the gram matrices of the weights (the neural feature matrices, NFM) and the average gradient outer products (AGOP) become correlated during training, in a statement known as the neural feature ansatz (NFA). Through the NFA, the authors introduce mapping with the AGOP as a general mechanism for neural feature learning. However, these works do not provide a theoretical explanation for this correlation or its origins. In this work, we further clarify the nature of this correlation, and explain its emergence. We show that this correlation is equivalent to alignment between the left singular structure of the weight matrices and the newly defined pre-activation tangent features at each layer. We further establish that the alignment is driven by the interaction of weight changes induced by SGD with the pre-activation features, and analyze the resulting dynamics analytically at early times in terms of simple statistics of the inputs and labels. We prove the derivative alignment occurs almost surely in specific high dimensional settings. Finally, we introduce a simple optimization rule motivated by our analysis of the centered correlation which dramatically increases the NFA correlations at any given layer and improves the quality of features learned.         ",
    "url": "https://arxiv.org/abs/2402.05271",
    "authors": [
      "Daniel Beaglehole",
      "Ioannis Mitliagkas",
      "Atish Agarwala"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.03276",
    "title": "ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures",
    "abstract": "           Electroencephalography (EEG) is a widely used tool for diagnosing brain disorders due to its high temporal resolution, non-invasive nature, and affordability. Manual analysis of EEG is labor-intensive and requires expertise, making automatic EEG interpretation crucial for reducing workload and accurately assessing seizures. In epilepsy diagnosis, prolonged EEG monitoring generates extensive data, often spanning hours, days, or even weeks. While machine learning techniques for automatic EEG interpretation have advanced significantly in recent decades, there remains a gap in its ability to efficiently analyze large datasets with a balance of accuracy and computational efficiency. To address the challenges mentioned above, an Attention Recurrent Neural Network (ARNN) is proposed that can process a large amount of data efficiently and accurately. This ARNN cell recurrently applies attention layers along a sequence and has linear complexity with the sequence length and leverages parallel computation by processing multi-channel EEG signals rather than single-channel signals. In this architecture, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. This framework is inspired in part by the attention layer and long short-term memory (LSTM) cells, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. The model's effectiveness is evaluated through extensive experiments with heterogeneous datasets, including the CHB-MIT and UPenn and Mayo's Clinic datasets.         ",
    "url": "https://arxiv.org/abs/2403.03276",
    "authors": [
      "Salim Rukhsar",
      "Anil Kumar Tiwari"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.17177",
    "title": "Deep models for stroke segmentation: do complex architectures always perform better?",
    "abstract": "           Stroke segmentation plays a crucial role in the diagnosis and treatment of stroke patients by providing spatial information about affected brain regions and the extent of damage. Segmenting stroke lesions accurately is a challenging task, given that conventional manual techniques are time consuming and prone to errors. Recently, advanced deep models have been introduced for general medical image segmentation, demonstrating promising results that surpass many state of the art networks when evaluated on specific datasets. With the advent of the vision Transformers, several models have been introduced based on them, while others have aimed to design better modules based on traditional convolutional layers to extract long-range dependencies like Transformers. The question of whether such high-level designs are necessary for all segmentation cases to achieve the best results remains unanswered. In this study, we selected four types of deep models that were recently proposed and evaluated their performance for stroke segmentation: a pure Transformer-based architecture (DAE-Former), two advanced CNN-based models (LKA and DLKA) with attention mechanisms in their design, an advanced hybrid model that incorporates CNNs with Transformers (FCT), and the well-known self-adaptive nnUNet framework with its configuration based on given data. We examined their performance on two publicly available datasets, and found that the nnUNet achieved the best results with the simplest design among all. Revealing the robustness issue of Transformers to such variabilities serves as a potential reason for their weaker performance. Furthermore, nnUNet's success underscores the significant impact of preprocessing and postprocessing techniques in enhancing segmentation results, surpassing the focus solely on architectural designs         ",
    "url": "https://arxiv.org/abs/2403.17177",
    "authors": [
      "Yalda Zafari-Ghadim",
      "Ahmed Soliman",
      "Yousif Yousif",
      "Ahmed Ibrahim",
      "Essam A. Rashed",
      "Mohamed Mabrok"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.15074",
    "title": "4+3 Phases of Compute-Optimal Neural Scaling Laws",
    "abstract": "           We consider the solvable neural scaling model with three parameters: data complexity, target complexity, and model-parameter-count. We use this neural scaling model to derive new predictions about the compute-limited, infinite-data scaling law regime. To train the neural scaling model, we run one-pass stochastic gradient descent on a mean-squared loss. We derive a representation of the loss curves which holds over all iteration counts and improves in accuracy as the model parameter count grows. We then analyze the compute-optimal model-parameter-count, and identify 4 phases (+3 subphases) in the data-complexity/target-complexity phase-plane. The phase boundaries are determined by the relative importance of model capacity, optimizer noise, and embedding of the features. We furthermore derive, with mathematical proof and extensive numerical evidence, the scaling-law exponents in all of these phases, in particular computing the optimal model-parameter-count as a function of floating point operation budget.         ",
    "url": "https://arxiv.org/abs/2405.15074",
    "authors": [
      "Elliot Paquette",
      "Courtney Paquette",
      "Lechao Xiao",
      "Jeffrey Pennington"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2405.16503",
    "title": "Integrating GNN and Neural ODEs for Estimating Non-Reciprocal Two-Body Interactions in Mixed-Species Collective Motion",
    "abstract": "           Analyzing the motion of multiple biological agents, be it cells or individual animals, is pivotal for the understanding of complex collective behaviors. With the advent of advanced microscopy, detailed images of complex tissue formations involving multiple cell types have become more accessible in recent years. However, deciphering the underlying rules that govern cell movements is far from trivial. Here, we present a novel deep learning framework for estimating the underlying equations of motion from observed trajectories, a pivotal step in decoding such complex dynamics. Our framework integrates graph neural networks with neural differential equations, enabling effective prediction of two-body interactions based on the states of the interacting entities. We demonstrate the efficacy of our approach through two numerical experiments. First, we used simulated data from a toy model to tune the hyperparameters. Based on the obtained hyperparameters, we then applied this approach to a more complex model with non-reciprocal forces that mimic the collective dynamics of the cells of slime molds. Our results show that the proposed method can accurately estimate the functional forms of two-body interactions -- even when they are nonreciprocal -- thereby precisely replicating both individual and collective behaviors within these systems.         ",
    "url": "https://arxiv.org/abs/2405.16503",
    "authors": [
      "Masahito Uwamichi",
      "Simon K. Schnyder",
      "Tetsuya J. Kobayashi",
      "Satoshi Sawai"
    ],
    "subjectives": [
      "Biological Physics (physics.bio-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07005",
    "title": "DecoR: Deconfounding Time Series with Robust Regression",
    "abstract": "           Causal inference on time series data is a challenging problem, especially in the presence of unobserved confounders. This work focuses on estimating the causal effect between two time series that are confounded by a third, unobserved time series. Assuming spectral sparsity of the confounder, we show how in the frequency domain this problem can be framed as an adversarial outlier problem. We introduce Deconfounding by Robust regression (DecoR), a novel approach that estimates the causal effect using robust linear regression in the frequency domain. Considering two different robust regression techniques, we first improve existing bounds on the estimation error for such techniques. Crucially, our results do not require distributional assumptions on the covariates. We can therefore use them in time series settings. Applying these results to DecoR, we prove, under suitable assumptions, upper bounds for the estimation error of DecoR that imply consistency. We demonstrate DecoR's effectiveness through experiments on both synthetic and real-world data from Earth system science. The simulation experiments furthermore suggest that DecoR is robust with respect to model misspecification.         ",
    "url": "https://arxiv.org/abs/2406.07005",
    "authors": [
      "Felix Schur",
      "Jonas Peters"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.08913",
    "title": "Maximizing the Maximum Degree in Ordered Nearest Neighbor Graphs",
    "abstract": "           For an ordered point set in a Euclidean space or, more generally, in an abstract metric space, the ordered Nearest Neighbor Graph is obtained by connecting each of the points to its closest predecessor by a directed edge. We show that for every set of $n$ points in $\\mathbb{R}^d$, there exists an order such that the corresponding ordered Nearest Neighbor Graph has maximum degree at least $\\log{n}/(4d)$. Apart from the $1/(4d)$ factor, this bound is the best possible. As for the abstract setting, we show that for every $n$-element metric space, there exists an order such that the corresponding ordered Nearest Neighbor Graph has maximum degree $\\Omega(\\sqrt{\\log{n}/\\log\\log{n}})$.         ",
    "url": "https://arxiv.org/abs/2406.08913",
    "authors": [
      "P\u00e9ter \u00c1goston",
      "Adrian Dumitrescu",
      "Arsenii Sagdeev",
      "Karamjeet Singh",
      "Ji Zeng"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Geometry (cs.CG)",
      "Metric Geometry (math.MG)"
    ]
  },
  {
    "id": "arXiv:2406.12112",
    "title": "Thermodynamic Transferability in Coarse-Grained Force Fields using Graph Neural Networks",
    "abstract": "           Coarse-graining is a molecular modeling technique in which an atomistic system is represented in a simplified fashion that retains the most significant system features that contribute to a target output, while removing the degrees of freedom that are less relevant. This reduction in model complexity allows coarse-grained molecular simulations to reach increased spatial and temporal scales compared to corresponding all-atom models. A core challenge in coarse-graining is to construct a force field that represents the interactions in the new representation in a way that preserves the atomistic-level properties. Many approaches to building coarse-grained force fields have limited transferability between different thermodynamic conditions as a result of averaging over internal fluctuations at a specific thermodynamic state point. Here, we use a graph-convolutional neural network architecture, the Hierarchically Interacting Particle Neural Network with Tensor Sensitivity (HIP-NN-TS), to develop a highly automated training pipeline for coarse grained force fields which allows for studying the transferability of coarse-grained models based on the force-matching approach. We show that this approach not only yields highly accurate force fields, but also that these force fields are more transferable through a variety of thermodynamic conditions. These results illustrate the potential of machine learning techniques such as graph neural networks to improve the construction of transferable coarse-grained force fields.         ",
    "url": "https://arxiv.org/abs/2406.12112",
    "authors": [
      "Emily Shinkle",
      "Aleksandra Pachalieva",
      "Riti Bahl",
      "Sakib Matin",
      "Brendan Gifford",
      "Galen T. Craven",
      "Nicholas Lubbers"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.10921",
    "title": "Leveraging Bi-Focal Perspectives and Granular Feature Integration for Accurate Reliable Early Alzheimer's Detection",
    "abstract": "           Alzheimer's disease (AD) is the most common neurodegeneration, annually diagnosed in millions of patients. The present medicine scenario still finds challenges in the exact diagnosis and classification of AD through neuroimaging data. Traditional CNNs can extract a good amount of low-level information in an image but fail to extract high-level minuscule particles, which is a significant challenge in detecting AD from MRI scans. To overcome this, we propose a novel Granular Feature Integration method to combine information extraction at different scales combined with an efficient information flow, enabling the model to capture both broad and fine-grained features simultaneously. We also propose a Bi-Focal Perspective mechanism to highlight the subtle neurofibrillary tangles and amyloid plaques in the MRI scans, ensuring that critical pathological markers are accurately identified. Our model achieved an F1-Score of 99.31%, precision of 99.24%, and recall of 99.51%. These scores prove that our model is significantly better than the state-of-the-art (SOTA) CNNs in existence.         ",
    "url": "https://arxiv.org/abs/2407.10921",
    "authors": [
      "Pandiyaraju V",
      "Shravan Venkatraman",
      "Abeshek A",
      "Pavan Kumar S",
      "Aravintakshan S A",
      "Kannan A"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.06220",
    "title": "CerviXpert: A Multi-Structural Convolutional Neural Network for Predicting Cervix Type and Cervical Cell Abnormalities",
    "abstract": "           Cervical cancer is a major cause of cancer-related mortality among women worldwide, and its survival rate improves significantly with early detection. Traditional diagnostic methods such as Pap smears and cervical biopsies rely heavily on cytologist expertise, making the process prone to human error. This study introduces CerviXpert, a multi-structural convolutional neural network model designed to efficiently classify cervix types and detect cervical cell abnormalities. CerviXpert is built as a computationally efficient model that classifies cervical cancer using images from the publicly available SiPaKMeD dataset. The model architecture emphasizes simplicity, using a limited number of convolutional layers followed by max pooling and dense layers, trained from scratch. We assessed the performance of CerviXpert against other state of the art convolutional neural network models including ResNet50, VGG16, MobileNetV2, and InceptionV3, evaluating them on accuracy, computational efficiency, and robustness using five fold cross validation. CerviXpert achieved an accuracy of 98.04 percent in classifying cervical cell abnormalities into three classes and 98.60 percent for five class cervix type classification, outperforming MobileNetV2 and InceptionV3 in both accuracy and computational requirements. It showed comparable results to ResNet50 and VGG16 while reducing computational complexity and resource needs. CerviXpert provides an effective solution for cervical cancer screening and diagnosis, balancing accuracy with computational efficiency. Its streamlined design enables deployment in resource constrained environments, potentially enhancing early detection and management of cervical cancer.         ",
    "url": "https://arxiv.org/abs/2409.06220",
    "authors": [
      "Rashik Shahriar Akash",
      "Radiful Islam",
      "S.M. Saiful Islam Badhon",
      "K. S. M. Tozammel Hossain"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.09733",
    "title": "Self-supervised Multimodal Speech Representations for the Assessment of Schizophrenia Symptoms",
    "abstract": "           Multimodal schizophrenia assessment systems have gained traction over the last few years. This work introduces a schizophrenia assessment system to discern between prominent symptom classes of schizophrenia and predict an overall schizophrenia severity score. We develop a Vector Quantized Variational Auto-Encoder (VQ-VAE) based Multimodal Representation Learning (MRL) model to produce task-agnostic speech representations from vocal Tract Variables (TVs) and Facial Action Units (FAUs). These representations are then used in a Multi-Task Learning (MTL) based downstream prediction model to obtain class labels and an overall severity score. The proposed framework outperforms the previous works on the multi-class classification task across all evaluation metrics (Weighted F1 score, AUC-ROC score, and Weighted Accuracy). Additionally, it estimates the schizophrenia severity score, a task not addressed by earlier approaches.         ",
    "url": "https://arxiv.org/abs/2409.09733",
    "authors": [
      "Gowtham Premananth",
      "Carol Espy-Wilson"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2410.11064",
    "title": "Parsing altered brain connectivity in neurodevelopmental disorders by integrating graph-based normative modeling and deep generative networks",
    "abstract": "           Divergent brain connectivity is thought to underlie the behavioral and cognitive symptoms observed in many neurodevelopmental disorders. Quantifying divergence from neurotypical connectivity patterns offers a promising pathway to inform diagnosis and therapeutic interventions. While advanced neuroimaging techniques, such as diffusion MRI (dMRI), have facilitated the mapping of brain's structural connectome, the challenge lies in accurately modeling developmental trajectories within these complex networked structures to create robust neurodivergence markers. In this work, we present the Brain Representation via Individualized Deep Generative Embedding (BRIDGE) framework, which integrates normative modeling with a bio-inspired deep generative model to create a reference trajectory of connectivity transformation as part of neurotypical development. This will enable the assessment of neurodivergence by comparing individuals to the established neurotypical trajectory. BRIDGE provides a global neurodivergence score based on the difference between connectivity-based brain age and chronological age, along with region-wise neurodivergence maps that highlight localized connectivity differences. Application of BRIDGE to a large cohort of children with autism spectrum disorder demonstrates that the global neurodivergence score correlates with clinical assessments in autism, and the regional map offers insights into the heterogeneity at the individual level in neurodevelopmental disorders. Together, the neurodivergence score and map form powerful tools for quantifying developmental divergence in connectivity patterns, advancing the development of imaging markers for personalized diagnosis and intervention in various clinical contexts.         ",
    "url": "https://arxiv.org/abs/2410.11064",
    "authors": [
      "Rui Sherry Shen",
      "Yusuf Osmanl\u0131o\u011flu",
      "Drew Parker",
      "Darien Aunapu",
      "Benjamin E. Yerys",
      "Birkan Tun\u00e7",
      "Ragini Verma"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2410.11181",
    "title": "DARNet: Dual Attention Refinement Network with Spatiotemporal Construction for Auditory Attention Detection",
    "abstract": "           At a cocktail party, humans exhibit an impressive ability to direct their attention. The auditory attention detection (AAD) approach seeks to identify the attended speaker by analyzing brain signals, such as EEG signals. However, current AAD algorithms overlook the spatial distribution information within EEG signals and lack the ability to capture long-range latent dependencies, limiting the model's ability to decode brain activity. To address these issues, this paper proposes a dual attention refinement network with spatiotemporal construction for AAD, named DARNet, which consists of the spatiotemporal construction module, dual attention refinement module, and feature fusion \\& classifier module. Specifically, the spatiotemporal construction module aims to construct more expressive spatiotemporal feature representations, by capturing the spatial distribution characteristics of EEG signals. The dual attention refinement module aims to extract different levels of temporal patterns in EEG signals and enhance the model's ability to capture long-range latent dependencies. The feature fusion \\& classifier module aims to aggregate temporal patterns and dependencies from different levels and obtain the final classification results. The experimental results indicate that compared to the state-of-the-art models, DARNet achieves an average classification accuracy improvement of 5.9\\% for 0.1s, 4.6\\% for 1s, and 3.9\\% for 2s on the DTU dataset. While maintaining excellent classification performance, DARNet significantly reduces the number of required parameters. Compared to the state-of-the-art models, DARNet reduces the parameter count by 91\\%. Code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2410.11181",
    "authors": [
      "Sheng Yan",
      "Cunhang fan",
      "Hongyu Zhang",
      "Xiaoke Yang",
      "Jianhua Tao",
      "Zhao Lv"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2410.20039",
    "title": "Crack opening calculation in phase-field modeling of fluid-filled fracture: A robust and efficient strain-based method",
    "abstract": "           The phase-field method has become popular for the numerical modeling of fluid-filled fractures, thanks to its ability to represent complex fracture geometry without algorithms. However, the algorithm-free representation of fracture geometry poses a significant challenge in calculating the crack opening (aperture) of phase-field fracture, which governs the fracture permeability and hence the overall hydromechanical behavior. Although several approaches have been devised to compute the crack opening of phase-field fracture, they require a sophisticated algorithm for post-processing the phase-field values or an additional parameter sensitive to the element size and alignment. Here, we develop a novel method for calculating the crack opening of fluid-filled phase-field fracture, which enables one to obtain the crack opening without additional algorithms or parameters. We transform the displacement-jump-based kinematics of a fracture into a continuous strain-based version, insert it into a force balance equation on the fracture, and apply the phase-field approximation. Through this procedure, we obtain a simple equation for the crack opening which can be calculated with quantities at individual material points. We verify the proposed method with analytical and numerical solutions obtained based on discrete representations of fractures, demonstrating its capability to calculate the crack opening regardless of the element size or alignment.         ",
    "url": "https://arxiv.org/abs/2410.20039",
    "authors": [
      "Fan Fei",
      "Jinhyun Choo"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2411.05815",
    "title": "Graph Neural Networks for Financial Fraud Detection: A Review",
    "abstract": "           The landscape of financial transactions has grown increasingly complex due to the expansion of global economic integration and advancements in information technology. This complexity poses greater challenges in detecting and managing financial fraud. This review explores the role of Graph Neural Networks (GNNs) in addressing these challenges by proposing a unified framework that categorizes existing GNN methodologies applied to financial fraud detection. Specifically, by examining a series of detailed research questions, this review delves into the suitability of GNNs for financial fraud detection, their deployment in real-world scenarios, and the design considerations that enhance their effectiveness. This review reveals that GNNs are exceptionally adept at capturing complex relational patterns and dynamics within financial networks, significantly outperforming traditional fraud detection methods. Unlike previous surveys that often overlook the specific potentials of GNNs or address them only superficially, our review provides a comprehensive, structured analysis, distinctly focusing on the multifaceted applications and deployments of GNNs in financial fraud detection. This review not only highlights the potential of GNNs to improve fraud detection mechanisms but also identifies current gaps and outlines future research directions to enhance their deployment in financial systems. Through a structured review of over 100 studies, this review paper contributes to the understanding of GNN applications in financial fraud detection, offering insights into their adaptability and potential integration strategies.         ",
    "url": "https://arxiv.org/abs/2411.05815",
    "authors": [
      "Dawei Cheng",
      "Yao Zou",
      "Sheng Xiang",
      "Changjun Jiang"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Machine Learning (cs.LG)"
    ]
  }
]