[
  {
    "id": "arXiv:2306.00001",
    "title": "TinyissimoYOLO: A Quantized, Low-Memory Footprint, TinyML Object  Detection Network for Low Power Microcontrollers",
    "abstract": "This paper introduces a highly flexible, quantized, memory-efficient, and ultra-lightweight object detection network, called TinyissimoYOLO. It aims to enable object detection on microcontrollers in the power domain of milliwatts, with less than 0.5MB memory available for storing convolutional neural network (CNN) weights. The proposed quantized network architecture with 422k parameters, enables real-time object detection on embedded microcontrollers, and it has been evaluated to exploit CNN accelerators. In particular, the proposed network has been deployed on the MAX78000 microcontroller achieving high frame-rate of up to 180fps and an ultra-low energy consumption of only 196{\\mu}J per inference with an inference efficiency of more than 106 MAC/Cycle. TinyissimoYOLO can be trained for any multi-object detection. However, considering the small network size, adding object detection classes will increase the size and memory consumption of the network, thus object detection with up to 3 classes is demonstrated. Furthermore, the network is trained using quantization-aware training and deployed with 8-bit quantization on different microcontrollers, such as STM32H7A3, STM32L4R9, Apollo4b and on the MAX78000's CNN accelerator. Performance evaluations are presented in this paper. ",
    "url": "https://arxiv.org/abs/2306.00001",
    "authors": [
      "Julian Moosmann",
      "Marco Giordano",
      "Christian Vogt",
      "Michele Magno"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Hardware Architecture (cs.AR)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2306.00006",
    "title": "Truncated Affinity Maximization: One-class Homophily Modeling for Graph  Anomaly Detection",
    "abstract": "One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be biased by non-homophily edges (i.e., edges connecting normal and abnormal nodes). Thus, TAM is instead optimized on truncated graphs where non-homophily edges are removed iteratively to mitigate this bias. The learned representations result in significantly stronger local affinity for normal nodes than abnormal nodes. Extensive empirical results on six real-world GAD datasets show that TAM substantially outperforms seven competing models, achieving over 10% increase in AUROC/AUPRC compared to the best contenders on challenging datasets. Our code will be made available at https: //github.com/mala-lab/TAM-master/. ",
    "url": "https://arxiv.org/abs/2306.00006",
    "authors": [
      "Qiao Hezhe",
      "Pang Guansong"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00009",
    "title": "Graph Exploration Matters: Improving both individual-level and  system-level diversity in WeChat Feed Recommender",
    "abstract": "There are roughly three stages in real industrial recommendation systems, candidates generation (retrieval), ranking and reranking. Individual-level diversity and system-level diversity are both important for industrial recommender systems. The former focus on each single user's experience, while the latter focus on the difference among users. Graph-based retrieval strategies are inevitably hijacked by heavy users and popular items, leading to the convergence of candidates for users and the lack of system-level diversity. Meanwhile, in the reranking phase, Determinantal Point Process (DPP) is deployed to increase individual-level diverisity. Heavily relying on the semantic information of items, DPP suffers from clickbait and inaccurate attributes. Besides, most studies only focus on one of the two levels of diversity, and ignore the mutual influence among different stages in real recommender systems. We argue that individual-level diversity and system-level diversity should be viewed as an integrated problem, and we provide an efficient and deployable solution for web-scale recommenders. Generally, we propose to employ the retrieval graph information in diversity-based reranking, by which to weaken the hidden similarity of items exposed to users, and consequently gain more graph explorations to improve the system-level diveristy. Besides, we argue that users' propensity for diversity changes over time in content feed recommendation. Therefore, with the explored graph, we also propose to capture the user's real-time personalized propensity to the diversity. We implement and deploy the combined system in WeChat App's Top Stories used by hundreds of millions of users. Offline simulations and online A/B tests show our solution can effectively improve both user engagement and system revenue. ",
    "url": "https://arxiv.org/abs/2306.00009",
    "authors": [
      "Shuai Yang",
      "Lixin Zhang",
      "Feng Xia",
      "Leyu Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00010",
    "title": "Explainability in Simplicial Map Neural Networks",
    "abstract": "Simplicial map neural networks (SMNNs) are topology-based neural networks with interesting properties such as universal approximation capability and robustness to adversarial examples under appropriate conditions. However, SMNNs present some bottlenecks for their possible application in high dimensions. First, no SMNN training process has been defined so far. Second, SMNNs require the construction of a convex polytope surrounding the input dataset. In this paper, we propose a SMNN training procedure based on a support subset of the given dataset and a method based on projection to a hypersphere as a replacement for the convex polytope construction. In addition, the explainability capacity of SMNNs is also introduced for the first time in this paper. ",
    "url": "https://arxiv.org/abs/2306.00010",
    "authors": [
      "Eduardo Paluzo-Hidalgo",
      "Miguel A. Guti\u00e9rrez-Naranjo",
      "Rocio Gonzalez-Diaz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Algebraic Topology (math.AT)"
    ]
  },
  {
    "id": "arXiv:2306.00011",
    "title": "A Self-Supervised Approach for Cluster Assessment of High-Dimensional  Data",
    "abstract": "Estimating the number of clusters and underlying cluster structure in a dataset is a crucial task. Real-world data are often unlabeled, complex and high-dimensional, which makes it difficult for traditional clustering algorithms to perform well. In recent years, a matrix reordering based algorithm, called \"visual assessment of tendency\" (VAT), and its variants have attracted many researchers from various domains to estimate the number of clusters and inherent cluster structure present in the data. However, these algorithms fail when applied to high-dimensional data due to the curse of dimensionality, as they rely heavily on the notions of closeness and farness between data points. To address this issue, we propose a deep-learning based framework for cluster structure assessment in complex, image datasets. First, our framework generates representative embeddings for complex data using a self-supervised deep neural network, and then, these low-dimensional embeddings are fed to VAT/iVAT algorithms to estimate the underlying cluster structure. In this process, we ensured not to use any prior knowledge for the number of clusters (i.e k). We present our results on four real-life image datasets, and our findings indicate that our framework outperforms state-of-the-art VAT/iVAT algorithms in terms of clustering accuracy and normalized mutual information (NMI). ",
    "url": "https://arxiv.org/abs/2306.00011",
    "authors": [
      "Alokendu Mazumder",
      "Pagadala Krishna Murthy",
      "Punit Rathore"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00012",
    "title": "Graph Neural Network for spatiotemporal data: methods and applications",
    "abstract": "In the era of big data, there has been a surge in the availability of data containing rich spatial and temporal information, offering valuable insights into dynamic systems and processes for applications such as weather forecasting, natural disaster management, intelligent transport systems, and precision agriculture. Graph neural networks (GNNs) have emerged as a powerful tool for modeling and understanding data with dependencies to each other such as spatial and temporal dependencies. There is a large amount of existing work that focuses on addressing the complex spatial and temporal dependencies in spatiotemporal data using GNNs. However, the strong interdisciplinary nature of spatiotemporal data has created numerous GNNs variants specifically designed for distinct application domains. Although the techniques are generally applicable across various domains, cross-referencing these methods remains essential yet challenging due to the absence of a comprehensive literature review on GNNs for spatiotemporal data. This article aims to provide a systematic and comprehensive overview of the technologies and applications of GNNs in the spatiotemporal domain. First, the ways of constructing graphs from spatiotemporal data are summarized to help domain experts understand how to generate graphs from various types of spatiotemporal data. Then, a systematic categorization and summary of existing spatiotemporal GNNs are presented to enable domain experts to identify suitable techniques and to support model developers in advancing their research. Moreover, a comprehensive overview of significant applications in the spatiotemporal domain is offered to introduce a broader range of applications to model developers and domain experts, assisting them in exploring potential research topics and enhancing the impact of their work. Finally, open challenges and future directions are discussed. ",
    "url": "https://arxiv.org/abs/2306.00012",
    "authors": [
      "Yun Li",
      "Dazhou Yu",
      "Zhenke Liu",
      "Minxing Zhang",
      "Xiaoyun Gong",
      "Liang Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.00015",
    "title": "GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning  Benchmarks",
    "abstract": "Label errors have been found to be prevalent in popular text, vision, and audio datasets, which heavily influence the safe development and evaluation of machine learning algorithms. Despite increasing efforts towards improving the quality of generic data types, such as images and texts, the problem of mislabel detection in graph data remains underexplored. To bridge the gap, we explore mislabelling issues in popular real-world graph datasets and propose GraphCleaner, a post-hoc method to detect and correct these mislabelled nodes in graph datasets. GraphCleaner combines the novel ideas of 1) Synthetic Mislabel Dataset Generation, which seeks to generate realistic mislabels; and 2) Neighborhood-Aware Mislabel Detection, where neighborhood dependency is exploited in both labels and base classifier predictions. Empirical evaluations on 6 datasets and 6 experimental settings demonstrate that GraphCleaner outperforms the closest baseline, with an average improvement of 0.14 in F1 score, and 0.16 in MCC. On real-data case studies, GraphCleaner detects real and previously unknown mislabels in popular graph benchmarks: PubMed, Cora, CiteSeer and OGB-arxiv; we find that at least 6.91% of PubMed data is mislabelled or ambiguous, and simply removing these mislabelled data can boost evaluation performance from 86.71% to 89.11%. ",
    "url": "https://arxiv.org/abs/2306.00015",
    "authors": [
      "Yuwen Li",
      "Miao Xiong",
      "Bryan Hooi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.00016",
    "title": "Incorporating Domain Knowledge in Deep Neural Networks for Discrete  Choice Models",
    "abstract": "Discrete choice models (DCM) are widely employed in travel demand analysis as a powerful theoretical econometric framework for understanding and predicting choice behaviors. DCMs are formed as random utility models (RUM), with their key advantage of interpretability. However, a core requirement for the estimation of these models is a priori specification of the associated utility functions, making them sensitive to modelers' subjective beliefs. Recently, machine learning (ML) approaches have emerged as a promising avenue for learning unobserved non-linear relationships in DCMs. However, ML models are considered \"black box\" and may not correspond with expected relationships. This paper proposes a framework that expands the potential of data-driven approaches for DCM by supporting the development of interpretable models that incorporate domain knowledge and prior beliefs through constraints. The proposed framework includes pseudo data samples that represent required relationships and a loss function that measures their fulfillment, along with observed data, for model training. The developed framework aims to improve model interpretability by combining ML's specification flexibility with econometrics and interpretable behavioral analysis. A case study demonstrates the potential of this framework for discrete choice analysis. ",
    "url": "https://arxiv.org/abs/2306.00016",
    "authors": [
      "Shadi Haj-Yahia",
      "Omar Mansour",
      "Tomer Toledo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Econometrics (econ.EM)"
    ]
  },
  {
    "id": "arXiv:2306.00029",
    "title": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLM",
    "abstract": "Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners. ",
    "url": "https://arxiv.org/abs/2306.00029",
    "authors": [
      "Nghi D. Q. Bui",
      "Hung Le",
      "Yue Wang",
      "Junnan Li",
      "Akhilesh Deepak Gotmare",
      "Steven C. H. Hoi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.00032",
    "title": "A Multi-Factorial Analysis of Polarization on Social Media",
    "abstract": "Polarization is an increasingly worrying phenomenon within social media. Recent work has made it possible to detect and even quantify polarization. Nevertheless, the few existing metrics, although defined in a continuous space, often lead to a unimodal distribution of data once applied to users' interactions, making the distinction between polarized and non-polarized users difficult to draw. Furthermore, each metric relies on a single factor and does not reflect the overall user behavior. Modeling polarization in a single form runs the risk of obscuring inter-individual differences. In this paper, we propose to have a deeper look at polarized online behaviors and to compare individual metrics. We collected about 300K retweets from 1K French users between January and July 2022 on Twitter. Each retweet is related to the highly controversial vaccine debate. Results show that a multi-factorial analysis leads to the identification of distinct and potentially explainable behavioral classes. This finer understanding of behaviors is an essential step to adapt news recommendation strategies so that no user gets locked into an echo chamber or filter bubble. ",
    "url": "https://arxiv.org/abs/2306.00032",
    "authors": [
      "Celina Treuillier",
      "Sylvain Castagnos",
      "Armelle Brun"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2306.00037",
    "title": "BotArtist: Twitter bot detection Machine Learning model based on Twitter  suspension",
    "abstract": "Twitter as one of the most popular social networks, offers a means for communication and online discourse, which unfortunately has been the target of bots and fake accounts, leading to the manipulation and spreading of false information. Towards this end, we gather a challenging, multilingual dataset of social discourse on Twitter, originating from 9M users regarding the recent Russo-Ukrainian war, in order to detect the bot accounts and the conversation involving them. We collect the ground truth for our dataset through the Twitter API suspended accounts collection, containing approximately 343K of bot accounts and 8M of normal users. Additionally, we use a dataset provided by Botometer-V3 with 1,777 Varol, 483 German accounts, and 1,321 US accounts. Besides the publicly available datasets, we also manage to collect 2 independent datasets around popular discussion topics of the 2022 energy crisis and the 2022 conspiracy discussions. Both of the datasets were labeled according to the Twitter suspension mechanism. We build a novel ML model for bot detection using the state-of-the-art XGBoost model. We combine the model with a high volume of labeled tweets according to the Twitter suspension mechanism ground truth. This requires a limited set of profile features allowing labeling of the dataset in different time periods from the collection, as it is independent of the Twitter API. In comparison with Botometer our methodology achieves an average 11% higher ROC-AUC score over two real-case scenario datasets. ",
    "url": "https://arxiv.org/abs/2306.00037",
    "authors": [
      "Alex Shevtsov",
      "Despoina Antonakaki",
      "Ioannis Lamprou",
      "Polyvios Pratikakis",
      "Sotiris Ioannidis"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00038",
    "title": "FedCSD: A Federated Learning Based Approach for Code-Smell Detection",
    "abstract": "This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\\%) where fewer smells exist in the training dataset, which has a noticeable reflection (technical debt) on the model's performance. Finally, the last and third experiments evaluate our approach by splitting the dataset into 10 companies. The ML model was trained on the company's site, then all model-updated weights were transferred to the server. Ultimately, an accuracy of 98.34% was achieved by the global model that has been trained using 10 companies for 100 training rounds. The results reveal a slight difference in the global model's accuracy compared to the highest accuracy of the centralized model, which can be ignored in favour of the global model's comprehensive knowledge, lower training cost, preservation of data privacy, and avoidance of the technical debt problem. ",
    "url": "https://arxiv.org/abs/2306.00038",
    "authors": [
      "Sadi Alawadi",
      "Khalid Alkharabsheh",
      "Fahed Alkhabbas",
      "Victor Kebande",
      "Feras M. Awaysheh",
      "Fabio Palomba"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00042",
    "title": "Graph-based methods coupled with specific distributional distances for  adversarial attack detection",
    "abstract": "Artificial neural networks are prone to being fooled by carefully perturbed inputs which cause an egregious misclassification. These \\textit{adversarial} attacks have been the focus of extensive research. Likewise, there has been an abundance of research in ways to detect and defend against them. We introduce a novel approach of detection and interpretation of adversarial attacks from a graph perspective. For an image, benign or adversarial, we study how a neural network's architecture can induce an associated graph. We study this graph and introduce specific measures used to predict and interpret adversarial attacks. We show that graphs-based approaches help to investigate the inner workings of adversarial attacks. ",
    "url": "https://arxiv.org/abs/2306.00042",
    "authors": [
      "Dwight Nwaigwe",
      "Lucrezia Carboni",
      "Martial Mermillod",
      "Sophie Achard",
      "Michel Dojat"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00045",
    "title": "Lottery Tickets in Evolutionary Optimization: On Sparse  Backpropagation-Free Trainability",
    "abstract": "Is the lottery ticket phenomenon an idiosyncrasy of gradient-based training or does it generalize to evolutionary optimization? In this paper we establish the existence of highly sparse trainable initializations for evolution strategies (ES) and characterize qualitative differences compared to gradient descent (GD)-based sparse training. We introduce a novel signal-to-noise iterative pruning procedure, which incorporates loss curvature information into the network pruning step. This can enable the discovery of even sparser trainable network initializations when using black-box evolution as compared to GD-based optimization. Furthermore, we find that these initializations encode an inductive bias, which transfers across different ES, related tasks and even to GD-based training. Finally, we compare the local optima resulting from the different optimization paradigms and sparsity levels. In contrast to GD, ES explore diverse and flat local optima and do not preserve linear mode connectivity across sparsity levels and independent runs. The results highlight qualitative differences between evolution and gradient-based learning dynamics, which can be uncovered by the study of iterative pruning procedures. ",
    "url": "https://arxiv.org/abs/2306.00045",
    "authors": [
      "Robert Tjarko Lange",
      "Henning Sprekeler"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00087",
    "title": "Adaptive Coordination in Social Embodied Rearrangement",
    "abstract": "We present the task of \"Social Rearrangement\", consisting of cooperative everyday tasks like setting up the dinner table, tidying a house or unpacking groceries in a simulated multi-agent environment. In Social Rearrangement, two robots coordinate to complete a long-horizon task, using onboard sensing and egocentric observations, and no privileged information about the environment. We study zero-shot coordination (ZSC) in this task, where an agent collaborates with a new partner, emulating a scenario where a robot collaborates with a new human partner. Prior ZSC approaches struggle to generalize in our complex and visually rich setting, and on further analysis, we find that they fail to generate diverse coordination behaviors at training time. To counter this, we propose Behavior Diversity Play (BDP), a novel ZSC approach that encourages diversity through a discriminability objective. Our results demonstrate that BDP learns adaptive agents that can tackle visual coordination, and zero-shot generalize to new partners in unseen environments, achieving 35% higher success and 32% higher efficiency compared to baselines. ",
    "url": "https://arxiv.org/abs/2306.00087",
    "authors": [
      "Andrew Szot",
      "Unnat Jain",
      "Dhruv Batra",
      "Zsolt Kira",
      "Ruta Desai",
      "Akshara Rai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2306.00095",
    "title": "Side-Channel VoIP Profiling Attack against Customer Service Automated  Phone System",
    "abstract": "In many VoIP systems, Voice Activity Detection (VAD) is often used on VoIP traffic to suppress packets of silence in order to reduce the bandwidth consumption of phone calls. Unfortunately, although VoIP traffic is fully encrypted and secured, traffic analysis of this suppression can reveal identifying information about calls made to customer service automated phone systems. Because different customer service phone systems have distinct, but fixed (pre-recorded) automated voice messages sent to customers, VAD silence suppression used in VoIP will enable an eavesdropper to profile and identify these automated voice messages. In this paper, we will use a popular enterprise VoIP system (Cisco CallManager), running the default Session Initiation Protocol (SIP) protocol, to demonstrate that an attacker can reliably use the silence suppression to profile calls to such VoIP systems. Our real-world experiments demonstrate that this side-channel profiling attack can be used to accurately identify not only what customer service phone number a customer calls, but also what following options are subsequently chosen by the caller in the phone conversation. ",
    "url": "https://arxiv.org/abs/2306.00095",
    "authors": [
      "Roy Laurens",
      "Edo Christianto",
      "Bruce Caulkins",
      "Cliff C. Zou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2306.00100",
    "title": "MetaXLR -- Mixed Language Meta Representation Transformation for  Low-resource Cross-lingual Learning based on Multi-Armed Bandit",
    "abstract": "Transfer learning for extremely low resource languages is a challenging task as there is no large scale monolingual corpora for pre training or sufficient annotated data for fine tuning. We follow the work of MetaXL which suggests using meta learning for transfer learning from a single source language to an extremely low resource one. We propose an enhanced approach which uses multiple source languages chosen in a data driven manner. In addition, we introduce a sample selection strategy for utilizing the languages in training by using a multi armed bandit algorithm. Using both of these improvements we managed to achieve state of the art results on the NER task for the extremely low resource languages while using the same amount of data, making the representations better generalized. Also, due to the method ability to use multiple languages it allows the framework to use much larger amounts of data, while still having superior results over the former MetaXL method even with the same amounts of data. ",
    "url": "https://arxiv.org/abs/2306.00100",
    "authors": [
      "Liat Bezalel",
      "Eyal Orgad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2306.00103",
    "title": "ManagerTower: Aggregating the Insights of Uni-Modal Experts for  Vision-Language Representation Learning",
    "abstract": "Two-Tower Vision-Language (VL) models have shown promising improvements on various downstream VL tasks. Although the most advanced work improves performance by building bridges between encoders, it suffers from ineffective layer-by-layer utilization of uni-modal representations and cannot flexibly exploit different levels of uni-modal semantic knowledge. In this work, we propose ManagerTower, a novel VL model architecture that gathers and combines the insights of pre-trained uni-modal experts at different levels. The managers introduced in each cross-modal layer can adaptively aggregate uni-modal semantic knowledge to facilitate more comprehensive cross-modal alignment and fusion. ManagerTower outperforms previous strong baselines both with and without Vision-Language Pre-training (VLP). With only 4M VLP data, ManagerTower achieves superior performances on various downstream VL tasks, especially 79.15% accuracy on VQAv2 Test-Std, 86.56% IR@1 and 95.64% TR@1 on Flickr30K. Code and checkpoints are available at https://github.com/LooperXX/ManagerTower. ",
    "url": "https://arxiv.org/abs/2306.00103",
    "authors": [
      "Xiao Xu",
      "Bei Li",
      "Chenfei Wu",
      "Shao-Yen Tseng",
      "Anahita Bhiwandiwalla",
      "Shachar Rosenman",
      "Vasudev Lal",
      "Wanxiang Che",
      "Nan Duan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00107",
    "title": "MERT: Acoustic Music Understanding Model with Large-Scale  Self-supervised Training",
    "abstract": "Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). These teachers effectively guide our student model, a BERT-style transformer encoder, to better model music audio. In addition, we introduce an in-batch noise mixture augmentation to enhance the representation robustness. Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attains state-of-the-art (SOTA) overall scores. The code and models are online: https://github.com/yizhilll/MERT. ",
    "url": "https://arxiv.org/abs/2306.00107",
    "authors": [
      "Yizhi Li",
      "Ruibin Yuan",
      "Ge Zhang",
      "Yinghao Ma",
      "Xingran Chen",
      "Hanzhi Yin",
      "Chenghua Lin",
      "Anton Ragni",
      "Emmanouil Benetos",
      "Norbert Gyenge",
      "Roger Dannenberg",
      "Ruibo Liu",
      "Wenhu Chen",
      "Gus Xia",
      "Yemin Shi",
      "Wenhao Huang",
      "Yike Guo",
      "Jie Fu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2306.00112",
    "title": "Additional Positive Enables Better Representation Learning for Medical  Images",
    "abstract": "This paper presents a new way to identify additional positive pairs for BYOL, a state-of-the-art (SOTA) self-supervised learning framework, to improve its representation learning ability. Unlike conventional BYOL which relies on only one positive pair generated by two augmented views of the same image, we argue that information from different images with the same label can bring more diversity and variations to the target features, thus benefiting representation learning. To identify such pairs without any label, we investigate TracIn, an instance-based and computationally efficient influence function, for BYOL training. Specifically, TracIn is a gradient-based method that reveals the impact of a training sample on a test sample in supervised learning. We extend it to the self-supervised learning setting and propose an efficient batch-wise per-sample gradient computation method to estimate the pairwise TracIn to represent the similarity of samples in the mini-batch during training. For each image, we select the most similar sample from other images as the additional positive and pull their features together with BYOL loss. Experimental results on two public medical datasets (i.e., ISIC 2019 and ChestX-ray) demonstrate that the proposed method can improve the classification performance compared to other competitive baselines in both semi-supervised and transfer learning settings. ",
    "url": "https://arxiv.org/abs/2306.00112",
    "authors": [
      "Dewen Zeng",
      "Yawen Wu",
      "Xinrong Hu",
      "Xiaowei Xu",
      "Jingtong Hu",
      "Yiyu Shi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00118",
    "title": "Neural Textured Deformable Meshes for Robust Analysis-by-Synthesis",
    "abstract": "Human vision demonstrates higher robustness than current AI algorithms under out-of-distribution scenarios. It has been conjectured such robustness benefits from performing analysis-by-synthesis. Our paper formulates triple vision tasks in a consistent manner using approximate analysis-by-synthesis by render-and-compare algorithms on neural features. In this work, we introduce Neural Textured Deformable Meshes, which involve the object model with deformable geometry that allows optimization on both camera parameters and object geometries. The deformable mesh is parameterized as a neural field, and covered by whole-surface neural texture maps, which are trained to have spatial discriminability. During inference, we extract the feature map of the test image and subsequently optimize the 3D pose and shape parameters of our model using differentiable rendering to best reconstruct the target feature map. We show that our analysis-by-synthesis is much more robust than conventional neural networks when evaluated on real-world images and even in challenging out-of-distribution scenarios, such as occlusion and domain shift. Our algorithms are competitive with standard algorithms when tested on conventional performance measures. ",
    "url": "https://arxiv.org/abs/2306.00118",
    "authors": [
      "Angtian Wang",
      "Wufei Ma",
      "Alan Yuille",
      "Adam Kortylewski"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00119",
    "title": "Optimal Sets and Solution Paths of ReLU Networks",
    "abstract": "We develop an analytical framework to characterize the set of optimal ReLU neural networks by reformulating the non-convex training problem as a convex program. We show that the global optima of the convex parameterization are given by a polyhedral set and then extend this characterization to the optimal set of the non-convex training objective. Since all stationary points of the ReLU training problem can be represented as optima of sub-sampled convex programs, our work provides a general expression for all critical points of the non-convex objective. We then leverage our results to provide an optimal pruning algorithm for computing minimal networks, establish conditions for the regularization path of ReLU networks to be continuous, and develop sensitivity results for minimal ReLU networks. ",
    "url": "https://arxiv.org/abs/2306.00119",
    "authors": [
      "Aaron Mishkin",
      "Mert Pilanci"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00120",
    "title": "VMap: An Interactive Rectangular Space-filling Visualization for  Map-like Vertex-centric Graph Exploration",
    "abstract": "We present VMap, a map-like rectangular space-filling visualization, to perform vertex-centric graph exploration. Existing visualizations have limited support for quality optimization among rectangular aspect ratios, vertex-edge intersection, and data encoding accuracy. To tackle this problem, VMap integrates three novel components: (1) a desired-aspect-ratio (DAR) rectangular partitioning algorithm, (2) a two-stage rectangle adjustment algorithm, and (3) a simulated annealing based heuristic optimizer. First, to generate a rectangular space-filling layout of an input graph, we subdivide the 2D embedding of the graph into rectangles with optimization of rectangles' aspect ratios toward a desired aspect ratio. Second, to route graph edges between rectangles without vertex-edge occlusion, we devise a two-stage algorithm to adjust a rectangular layout to insert border space between rectangles. Third, to produce and arrange rectangles by considering multiple visual criteria, we design a simulated annealing based heuristic optimization to adjust vertices' 2D embedding to support trade-offs among aspect ratio quality and the encoding accuracy of vertices' weights and adjacency. We evaluated the effectiveness of VMap on both synthetic and application datasets. The resulting rectangular layout has better aspect ratio quality on synthetic data compared with the existing method for the rectangular partitioning of 2D points. On three real-world datasets, VMap achieved better encoding accuracy and attained faster generation speed compared with existing methods on graphs' rectangular layout generation. We further illustrate the usefulness of VMap for vertex-centric graph exploration through three case studies on visualizing social networks, representing academic communities, and displaying geographic information. ",
    "url": "https://arxiv.org/abs/2306.00120",
    "authors": [
      "Jiayi Xu",
      "Han-Wei Shen"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Discrete Mathematics (cs.DM)",
      "Human-Computer Interaction (cs.HC)",
      "Information Retrieval (cs.IR)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2306.00121",
    "title": "Multilingual Multi-Figurative Language Detection",
    "abstract": "Figures of speech help people express abstract concepts and evoke stronger emotions than literal expressions, thereby making texts more creative and engaging. Due to its pervasive and fundamental character, figurative language understanding has been addressed in Natural Language Processing, but it's highly understudied in a multilingual setting and when considering more than one figure of speech at the same time. To bridge this gap, we introduce multilingual multi-figurative language modelling, and provide a benchmark for sentence-level figurative language detection, covering three common figures of speech and seven languages. Specifically, we develop a framework for figurative language detection based on template-based prompt learning. In so doing, we unify multiple detection tasks that are interrelated across multiple figures of speech and languages, without requiring task- or language-specific modules. Experimental results show that our framework outperforms several strong baselines and may serve as a blueprint for the joint modelling of other interrelated tasks. ",
    "url": "https://arxiv.org/abs/2306.00121",
    "authors": [
      "Huiyuan Lai",
      "Antonio Toral",
      "Malvina Nissim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2306.00125",
    "title": "Graph Colouring is Hard for Algorithms Based on Hilbert's  Nullstellensatz and Gr\u00f6bner Bases",
    "abstract": "We consider the graph $k$-colouring problem encoded as a set of polynomial equations in the standard way over $0/1$-valued variables. We prove that there are bounded-degree graphs that do not have legal $k$-colourings but for which the polynomial calculus proof system defined in [Clegg et al '96, Alekhnovich et al '02] requires linear degree, and hence exponential size, to establish this fact. This implies a linear degree lower bound for any algorithms based on Gr\\\"{o}bner bases solving graph $k$-colouring using this encoding. The same bound applies also for the algorithm studied in a sequence of papers [De Loera et al '08,'09,'11,'15] based on Hilbert's Nullstellensatz proofs for a slightly different encoding, thus resolving an open problem mentioned in [De Loera et al '08,'09,'11] and [Li '16]. We obtain our results by combining the polynomial calculus degree lower bound for functional pigeonhole principle (FPHP) formulas over bounded-degree bipartite graphs in [Mik\\v{s}a and Nordstr\\\"{o}m '15] with a reduction from FPHP to $k$-colouring derivable by polynomial calculus in constant degree. ",
    "url": "https://arxiv.org/abs/2306.00125",
    "authors": [
      "Massimo Lauria",
      "Jakob Nordstr\u00f6m"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2306.00127",
    "title": "Surrogate Model Extension (SME): A Fast and Accurate Weight Update  Attack on Federated Learning",
    "abstract": "In Federated Learning (FL) and many other distributed training frameworks, collaborators can hold their private data locally and only share the network weights trained with the local data after multiple iterations. Gradient inversion is a family of privacy attacks that recovers data from its generated gradients. Seemingly, FL can provide a degree of protection against gradient inversion attacks on weight updates, since the gradient of a single step is concealed by the accumulation of gradients over multiple local iterations. In this work, we propose a principled way to extend gradient inversion attacks to weight updates in FL, thereby better exposing weaknesses in the presumed privacy protection inherent in FL. In particular, we propose a surrogate model method based on the characteristic of two-dimensional gradient flow and low-rank property of local updates. Our method largely boosts the ability of gradient inversion attacks on weight updates containing many iterations and achieves state-of-the-art (SOTA) performance. Additionally, our method runs up to $100\\times$ faster than the SOTA baseline in the common FL scenario. Our work re-evaluates and highlights the privacy risk of sharing network weights. Our code is available at https://github.com/JunyiZhu-AI/surrogate_model_extension. ",
    "url": "https://arxiv.org/abs/2306.00127",
    "authors": [
      "Junyi Zhu",
      "Ruicong Yao",
      "Matthew B. Blaschko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2306.00129",
    "title": "Self-supervised Vision Transformers for 3D Pose Estimation of Novel  Objects",
    "abstract": "Object pose estimation is important for object manipulation and scene understanding. In order to improve the general applicability of pose estimators, recent research focuses on providing estimates for novel objects, that is objects unseen during training. Such works use deep template matching strategies to retrieve the closest template connected to a query image. This template retrieval implicitly provides object class and pose. Despite the recent success and improvements of Vision Transformers over CNNs for many vision tasks, the state of the art uses CNN-based approaches for novel object pose estimation. This work evaluates and demonstrates the differences between self-supervised CNNs and Vision Transformers for deep template matching. In detail, both types of approaches are trained using contrastive learning to match training images against rendered templates of isolated objects. At test time, such templates are matched against query images of known and novel objects under challenging settings, such as clutter, occlusion and object symmetries, using masked cosine similarity. The presented results not only demonstrate that Vision Transformers improve in matching accuracy over CNNs, but also that for some cases pre-trained Vision Transformers do not need fine-tuning to do so. Furthermore, we highlight the differences in optimization and network architecture when comparing these two types of network for deep template matching. ",
    "url": "https://arxiv.org/abs/2306.00129",
    "authors": [
      "Stefan Thalhammer",
      "Jean-Baptiste Weibel",
      "Markus Vincze",
      "Jose Garcia-Rodriguez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00152",
    "title": "Learning the Right Layers: a Data-Driven Layer-Aggregation Strategy for  Semi-Supervised Learning on Multilayer Graphs",
    "abstract": "Clustering (or community detection) on multilayer graphs poses several additional complications with respect to standard graphs as different layers may be characterized by different structures and types of information. One of the major challenges is to establish the extent to which each layer contributes to the cluster assignment in order to effectively take advantage of the multilayer structure and improve upon the classification obtained using the individual layers or their union. However, making an informed a-priori assessment about the clustering information content of the layers can be very complicated. In this work, we assume a semi-supervised learning setting, where the class of a small percentage of nodes is initially provided, and we propose a parameter-free Laplacian-regularized model that learns an optimal nonlinear combination of the different layers from the available input labels. The learning algorithm is based on a Frank-Wolfe optimization scheme with inexact gradient, combined with a modified Label Propagation iteration. We provide a detailed convergence analysis of the algorithm and extensive experiments on synthetic and real-world datasets, showing that the proposed method compares favourably with a variety of baselines and outperforms each individual layer when used in isolation. ",
    "url": "https://arxiv.org/abs/2306.00152",
    "authors": [
      "Sara Venturini",
      "Andrea Cristofari",
      "Francesco Rinaldi",
      "Francesco Tudisco"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00168",
    "title": "Measuring the Robustness of Natural Language Processing Models to Domain  Shifts",
    "abstract": "Large Language Models have shown promising performance on various tasks, including fine-tuning, few-shot learning, and zero-shot learning. However, their performance on domains without labeled data still lags behind those with labeled data, which we refer as the Domain Robustness (DR) challenge. Existing research on DR suffers from disparate setups, lack of evaluation task variety, and reliance on challenge sets. In this paper, we explore the DR challenge of both fine-tuned and few-shot learning models in natural domain shift settings. We introduce a DR benchmark comprising diverse NLP tasks, including sentence and token-level classification, QA, and generation, each task consists of several domains. We propose two views of the DR challenge: Source Drop (SD) and Target Drop (TD), which alternate between the source and target in-domain performance as reference points. We find that in significant proportions of domain shifts, either SD or TD is positive, but not both, emphasizing the importance of considering both measures as diagnostic tools. Our experimental results demonstrate the persistent existence of the DR challenge in both fine-tuning and few-shot learning models, though it is less pronounced in the latter. We also find that increasing the fine-tuned model size improves performance, particularly in classification. ",
    "url": "https://arxiv.org/abs/2306.00168",
    "authors": [
      "Nitay Calderon",
      "Naveh Porat",
      "Eyal Ben-David",
      "Zorik Gekhman",
      "Nadav Oved",
      "Roi Reichart"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2306.00169",
    "title": "Inconsistency, Instability, and Generalization Gap of Deep Neural  Network Training",
    "abstract": "As deep neural networks are highly expressive, it is important to find solutions with small generalization gap (the difference between the performance on the training data and unseen data). Focusing on the stochastic nature of training, we first present a theoretical analysis in which the bound of generalization gap depends on what we call inconsistency and instability of model outputs, which can be estimated on unlabeled data. Our empirical study based on this analysis shows that instability and inconsistency are strongly predictive of generalization gap in various settings. In particular, our finding indicates that inconsistency is a more reliable indicator of generalization gap than the sharpness of the loss landscape. Furthermore, we show that algorithmic reduction of inconsistency leads to superior performance. The results also provide a theoretical basis for existing methods such as co-distillation and ensemble. ",
    "url": "https://arxiv.org/abs/2306.00169",
    "authors": [
      "Rie Johnson",
      "Tong Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00172",
    "title": "Learning for Edge-Weighted Online Bipartite Matching with Robustness  Guarantees",
    "abstract": "Many problems, such as online ad display, can be formulated as online bipartite matching. The crucial challenge lies in the nature of sequentially-revealed online item information, based on which we make irreversible matching decisions at each step. While numerous expert online algorithms have been proposed with bounded worst-case competitive ratios, they may not offer satisfactory performance in average cases. On the other hand, reinforcement learning (RL) has been applied to improve the average performance, but it lacks robustness and can perform arbitrarily poorly. In this paper, we propose a novel RL-based approach to edge-weighted online bipartite matching with robustness guarantees (LOMAR), achieving both good average-case and worst-case performance. The key novelty of LOMAR is a new online switching operation which, based on a judicious condition to hedge against future uncertainties, decides whether to follow the expert's decision or the RL decision for each online item. We prove that for any $\\rho\\in[0,1]$, LOMAR is $\\rho$-competitive against any given expert online algorithm. To improve the average performance, we train the RL policy by explicitly considering the online switching operation. Finally, we run empirical experiments to demonstrate the advantages of LOMAR compared to existing baselines. Our code is available at: https://github.com/Ren-Research/LOMAR ",
    "url": "https://arxiv.org/abs/2306.00172",
    "authors": [
      "Pengfei Li",
      "Jianyi Yang",
      "Shaolei Ren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00177",
    "title": "Contrastive Hierarchical Discourse Graph for Scientific Document  Summarization",
    "abstract": "The extended structural context has made scientific paper summarization a challenging task. This paper proposes CHANGES, a contrastive hierarchical graph neural network for extractive scientific paper summarization. CHANGES represents a scientific paper with a hierarchical discourse graph and learns effective sentence representations with dedicated designed hierarchical graph information aggregation. We also propose a graph contrastive learning module to learn global theme-aware sentence representations. Extensive experiments on the PubMed and arXiv benchmark datasets prove the effectiveness of CHANGES and the importance of capturing hierarchical structure information in modeling scientific papers. ",
    "url": "https://arxiv.org/abs/2306.00177",
    "authors": [
      "Haopeng Zhang",
      "Xiao Liu",
      "Jiawei Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2306.00197",
    "title": "SSL-CPCD: Self-supervised learning with composite pretext-class  discrimination for improved generalisability in endoscopic image analysis",
    "abstract": "Data-driven methods have shown tremendous progress in medical image analysis. In this context, deep learning-based supervised methods are widely popular. However, they require a large amount of training data and face issues in generalisability to unseen datasets that hinder clinical translation. Endoscopic imaging data incorporates large inter- and intra-patient variability that makes these models more challenging to learn representative features for downstream tasks. Thus, despite the publicly available datasets and datasets that can be generated within hospitals, most supervised models still underperform. While self-supervised learning has addressed this problem to some extent in natural scene data, there is a considerable performance gap in the medical image domain. In this paper, we propose to explore patch-level instance-group discrimination and penalisation of inter-class variation using additive angular margin within the cosine similarity metrics. Our novel approach enables models to learn to cluster similar representative patches, thereby improving their ability to provide better separation between different classes. Our results demonstrate significant improvement on all metrics over the state-of-the-art (SOTA) methods on the test set from the same and diverse datasets. We evaluated our approach for classification, detection, and segmentation. SSL-CPCD achieves 79.77% on Top 1 accuracy for ulcerative colitis classification, 88.62% on mAP for polyp detection, and 82.32% on dice similarity coefficient for segmentation tasks are nearly over 4%, 2%, and 3%, respectively, compared to the baseline architectures. We also demonstrate that our method generalises better than all SOTA methods to unseen datasets, reporting nearly 7% improvement in our generalisability assessment. ",
    "url": "https://arxiv.org/abs/2306.00197",
    "authors": [
      "Ziang Xu",
      "Jens Rittscher",
      "Sharib Ali"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00202",
    "title": "Building Manufacturing Deep Learning Models with Minimal and Imbalanced  Training Data Using Domain Adaptation and Data Augmentation",
    "abstract": "Deep learning (DL) techniques are highly effective for defect detection from images. Training DL classification models, however, requires vast amounts of labeled data which is often expensive to collect. In many cases, not only the available training data is limited but may also imbalanced. In this paper, we propose a novel domain adaptation (DA) approach to address the problem of labeled training data scarcity for a target learning task by transferring knowledge gained from an existing source dataset used for a similar learning task. Our approach works for scenarios where the source dataset and the dataset available for the target learning task have same or different feature spaces. We combine our DA approach with an autoencoder-based data augmentation approach to address the problem of imbalanced target datasets. We evaluate our combined approach using image data for wafer defect prediction. The experiments show its superior performance against other algorithms when the number of labeled samples in the target dataset is significantly small and the target dataset is imbalanced. ",
    "url": "https://arxiv.org/abs/2306.00202",
    "authors": [
      "Adrian Shuai Li",
      "Elisa Bertino",
      "Rih-Teng Wu",
      "Ting-Yan Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00206",
    "title": "Representation Reliability and Its Impact on Downstream Tasks",
    "abstract": "Self-supervised pre-trained models extract general-purpose representations from data, and quantifying how reliable they are is crucial because many downstream models use these representations as input for their own tasks. To this end, we first introduce a formal definition of representation reliability: the representation for a given test input is considered to be reliable if the downstream models built on top of that representation can consistently generate accurate predictions for that test point. It is desired to estimate the representation reliability without knowing the downstream tasks a priori. We provide a negative result showing that existing frameworks for uncertainty quantification in supervised learning are not suitable for this purpose. As an alternative, we propose an ensemble-based method for quantifying representation reliability, based on the concept of neighborhood consistency in the representation spaces across various pre-trained models. More specifically, the key insight is to use shared neighboring points as anchors to align different representation spaces. We demonstrate through comprehensive numerical experiments that our method is capable of predicting representation reliability with high accuracy. ",
    "url": "https://arxiv.org/abs/2306.00206",
    "authors": [
      "Young-Jin Park",
      "Hao Wang",
      "Shervin Ardeshir",
      "Navid Azizan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.00210",
    "title": "PERFOGRAPH: A Numerical Aware Program Graph Representation for  Performance Optimization and Program Analysis",
    "abstract": "The remarkable growth and significant success of machine learning have expanded its applications into programming languages and program analysis. However, a key challenge in adopting the latest machine learning methods is the representation of programming languages, which directly impacts the ability of machine learning methods to reason about programs. The absence of numerical awareness, composite data structure information, and improper way of presenting variables in previous representation works have limited their performances. To overcome the limitations and challenges of current program representations, we propose a novel graph-based program representation called PERFOGRAPH. PERFOGRAPH can capture numerical information and the composite data structure by introducing new nodes and edges. Furthermore, we propose an adapted embedding method to incorporate numerical awareness. These enhancements make PERFOGRAPH a highly flexible and scalable representation that can effectively capture program intricate dependencies and semantics. Consequently, it serves as a powerful tool for various applications such as program analysis, performance optimization, and parallelism discovery. Our experimental results demonstrate that PERFOGRAPH outperforms existing representations and sets new state-of-the-art results by reducing the error rate by 7.4% (AMD dataset) and 10% (NVIDIA dataset) in the well-known Device Mapping challenge. It also sets new state-of-the-art results in various performance optimization tasks like Parallelism Discovery and Numa and Prefetchers Configuration prediction. ",
    "url": "https://arxiv.org/abs/2306.00210",
    "authors": [
      "Ali TehraniJamsaz",
      "Quazi Ishtiaque Mahmud",
      "Le Chen",
      "Nasreen K. Ahmed",
      "Ali Jannesari"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00230",
    "title": "Predictive Limitations of Physics-Informed Neural Networks in Vortex  Shedding",
    "abstract": "The recent surge of interest in physics-informed neural network (PINN) methods has led to a wave of studies that attest to their potential for solving partial differential equations (PDEs) and predicting the dynamics of physical systems. However, the predictive limitations of PINNs have not been thoroughly investigated. We look at the flow around a 2D cylinder and find that data-free PINNs are unable to predict vortex shedding. Data-driven PINN exhibits vortex shedding only while the training data (from a traditional CFD solver) is available, but reverts to the steady state solution when the data flow stops. We conducted dynamic mode decomposition and analyze the Koopman modes in the solutions obtained with PINNs versus a traditional fluid solver (PetIBM). The distribution of the Koopman eigenvalues on the complex plane suggests that PINN is numerically dispersive and diffusive. The PINN method reverts to the steady solution possibly as a consequence of spectral bias. This case study reaises concerns about the ability of PINNs to predict flows with instabilities, specifically vortex shedding. Our computational study supports the need for more theoretical work to analyze the numerical properties of PINN methods. The results in this paper are transparent and reproducible, with all data and code available in public repositories and persistent archives; links are provided in the paper repository at \\url{https://github.com/barbagroup/jcs_paper_pinn}, and a Reproducibility Statement within the paper. ",
    "url": "https://arxiv.org/abs/2306.00230",
    "authors": [
      "Pi-Yueh Chuang",
      "Lorena A. Barba"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00234",
    "title": "Implementing Man-in-the-Middle Attack to Investigate Network  Vulnerabilities in Smart Grid Test-bed",
    "abstract": "The smart-grid introduces several new data-gathering, communication, and information-sharing capabilities into the electrical system, as well as additional privacy threats, vulnerabilities, and cyber-attacks. In this study, Modbus is regarded as one of the most prevalent interfaces for control systems in power plants. Modern control interfaces are vulnerable to cyber-attacks, posing a risk to the entire energy infrastructure. In order to strengthen resistance to cyber-attacks, this study introduces a test bed for cyber-physical systems that operate in real-time. To investigate the network vulnerabilities of smart power grids, Modbus protocol has been examined combining a real-time power system simulator with a communication system simulator and the effects of the system presented and analyzed. The goal is to detect the vulnerability in Modbus protocol and perform the Man-in-the-middle attack with its impact on the system. This proposed testbed can be evaluated as a research model for vulnerability assessment as well as a tool for evaluating cyber-attacks and enquire into any detection mechanism for safeguarding and defending smart grid systems from a variety of cyberattacks. We present here the preliminary findings on using the testbed to identify a particular MiTM attack and the effects on system performance. Finally, we suggest a cyber security strategy as a solution to address such network vulnerabilities and deploy appropriate countermeasures. ",
    "url": "https://arxiv.org/abs/2306.00234",
    "authors": [
      "Shampa Banik",
      "Trapa Banik",
      "S.M. Mostaq Hossain",
      "Sohag Kumar Saha"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2306.00240",
    "title": "Trusting code in the wild: A social network-based centrality rating for  developers in the Rust ecosystem",
    "abstract": "As modern software extensively uses open source packages, developers regularly pull in new upstream code through frequent updates. While a manual review of all upstream changes may not be practical, developers may rely on the authors' and reviewers' identities, among other factors, to decide what level of review the new code may require. The goal of this study is to help downstream project developers prioritize review efforts for upstream code by providing a social network-based centrality rating for the authors and reviewers of that code. To that end, we build a social network of 6,949 developers across the collaboration activity from 1,644 Rust packages. Further, we survey the developers in the network to evaluate if code coming from a developer with a higher centrality rating is likely to be accepted with lesser scrutiny by the downstream projects and, therefore, is perceived to be more trusted. Our results show that 97.7\\% of the developers from the studied packages are interconnected via collaboration, with each developer separated from another via only four other developers in the network. The interconnection among developers from different Rust packages establishes the ground for identifying the central developers in the ecosystem. Our survey responses ($N=206$) show that the respondents are more likely to not differentiate between developers in deciding how to review upstream changes (60.2\\% of the time). However, when they do differentiate, our statistical analysis showed a significant correlation between developers' centrality ratings and the level of scrutiny their code might face from the downstream projects, as indicated by the respondents. ",
    "url": "https://arxiv.org/abs/2306.00240",
    "authors": [
      "Nasif Imtiaz",
      "Preya Shabrina",
      "Laurie Williams"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2306.00265",
    "title": "Doubly Robust Self-Training",
    "abstract": "Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline. ",
    "url": "https://arxiv.org/abs/2306.00265",
    "authors": [
      "Banghua Zhu",
      "Mingyu Ding",
      "Philip Jacobson",
      "Ming Wu",
      "Wei Zhan",
      "Michael Jordan",
      "Jiantao Jiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00266",
    "title": "A polynomial-time iterative algorithm for random graph matching with  non-vanishing correlation",
    "abstract": "We propose an efficient algorithm for matching two correlated Erd\\H{o}s--R\\'enyi graphs with $n$ vertices whose edges are correlated through a latent vertex correspondence. When the edge density $q= n^{- \\alpha+o(1)}$ for a constant $\\alpha \\in [0,1)$, we show that our algorithm has polynomial running time and succeeds to recover the latent matching as long as the edge correlation is non-vanishing. This is closely related to our previous work on a polynomial-time algorithm that matches two Gaussian Wigner matrices with non-vanishing correlation, and provides the first polynomial-time random graph matching algorithm (regardless of the regime of $q$) when the edge correlation is below the square root of the Otter's constant (which is $\\approx 0.338$). ",
    "url": "https://arxiv.org/abs/2306.00266",
    "authors": [
      "Jian Ding",
      "Zhangsong Li"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00283",
    "title": "Autism Disease Detection Using Transfer Learning Techniques: Performance  Comparison Between Central Processing Unit vs Graphics Processing Unit  Functions for Neural Networks",
    "abstract": "Neural network approaches are machine learning methods that are widely used in various domains, such as healthcare and cybersecurity. Neural networks are especially renowned for their ability to deal with image datasets. During the training process with images, various fundamental mathematical operations are performed in the neural network. These operations include several algebraic and mathematical functions, such as derivatives, convolutions, and matrix inversions and transpositions. Such operations demand higher processing power than what is typically required for regular computer usage. Since CPUs are built with serial processing, they are not appropriate for handling large image datasets. On the other hand, GPUs have parallel processing capabilities and can provide higher speed. This paper utilizes advanced neural network techniques, such as VGG16, Resnet50, Densenet, Inceptionv3, Xception, Mobilenet, XGBOOST VGG16, and our proposed models, to compare CPU and GPU resources. We implemented a system for classifying Autism disease using face images of autistic and non-autistic children to compare performance during testing. We used evaluation matrices such as Accuracy, F1 score, Precision, Recall, and Execution time. It was observed that GPU outperformed CPU in all tests conducted. Moreover, the performance of the neural network models in terms of accuracy increased on GPU compared to CPU. ",
    "url": "https://arxiv.org/abs/2306.00283",
    "authors": [
      "Mst Shapna Akter",
      "Hossain Shahriar",
      "Alfredo Cuzzocrea"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00285",
    "title": "Linear codes with arbitrary dimensional hull and pure LCD code",
    "abstract": "In this paper, we introduce a general construction of linear codes with small dimension hull from any non LCD codes. Furthermore, we show that for any linear code $\\Co$ over $\\F_q$ ($q > 3$) with $dim(Hull(\\Co))=h$ there exist an equivalent codes $\\Co_j$ with $dim(Hull(\\Co_j))=j$ for any integer $0\\leq j \\leq h$. We also introduce the notion of pure LCD code; an LCD code and all its equivalent are LCD; and construct an infinite family of pure LCD codes. In addition, we introduce a general construction of linear codes with one dimension hull. ",
    "url": "https://arxiv.org/abs/2306.00285",
    "authors": [
      "Maouche Youcef"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2306.00286",
    "title": "Efficient Deep Learning of Robust Policies from MPC using Imitation and  Tube-Guided Data Augmentation",
    "abstract": "Imitation Learning (IL) has been increasingly employed to generate computationally efficient policies from task-relevant demonstrations provided by Model Predictive Control (MPC). However, commonly employed IL methods are often data- and computationally-inefficient, as they require a large number of MPC demonstrations, resulting in long training times, and they produce policies with limited robustness to disturbances not experienced during training. In this work, we propose an IL strategy to efficiently compress a computationally expensive MPC into a Deep Neural Network (DNN) policy that is robust to previously unseen disturbances. By using a robust variant of the MPC, called Robust Tube MPC (RTMPC), and leveraging properties from the controller, we introduce a computationally-efficient Data Aggregation (DA) method that enables a significant reduction of the number of MPC demonstrations and training time required to generate a robust policy. Our approach opens the possibility of zero-shot transfer of a policy trained from a single MPC demonstration collected in a nominal domain, such as a simulation or a robot in a lab/controlled environment, to a new domain with previously-unseen bounded model errors/perturbations. Numerical and experimental evaluations performed using linear and nonlinear MPC for agile flight on a multirotor show that our method outperforms strategies commonly employed in IL (such as DAgger and DR) in terms of demonstration-efficiency, training time, and robustness to perturbations unseen during training. ",
    "url": "https://arxiv.org/abs/2306.00286",
    "authors": [
      "Andrea Tagliabue",
      "Jonathan P. How"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2306.00288",
    "title": "Training-free Neural Architecture Search for RNNs and Transformers",
    "abstract": "Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS metrics for recurrent neural network (RNN) and BERT-based transformer architectures, targeted towards language modeling tasks. First, we develop a new training-free metric, named hidden covariance, that predicts the trained performance of an RNN architecture and significantly outperforms existing training-free metrics. We experimentally evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP benchmark. Second, we find that the current search space paradigm for transformer architectures is not optimized for training-free neural architecture search. Instead, a simple qualitative analysis can effectively shrink the search space to the best performing architectures. This conclusion is based on our investigation of existing training-free metrics and new metrics developed from recent transformer pruning literature, evaluated on our own benchmark of trained BERT architectures. Ultimately, our analysis shows that the architecture search space and the training-free metric must be developed together in order to achieve effective results. ",
    "url": "https://arxiv.org/abs/2306.00288",
    "authors": [
      "Aaron Serianni",
      "Jugal Kalita"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2306.00294",
    "title": "Affinity-based Attention in Self-supervised Transformers Predicts  Dynamics of Object Grouping in Humans",
    "abstract": "The spreading of attention has been proposed as a mechanism for how humans group features to segment objects. However, such a mechanism has not yet been implemented and tested in naturalistic images. Here, we leverage the feature maps from self-supervised vision Transformers and propose a model of human object-based attention spreading and segmentation. Attention spreads within an object through the feature affinity signal between different patches of the image. We also collected behavioral data on people grouping objects in natural images by judging whether two dots are on the same object or on two different objects. We found that our models of affinity spread that were built on feature maps from the self-supervised Transformers showed significant improvement over baseline and CNN based models on predicting reaction time patterns of humans, despite not being trained on the task or with any other object labels. Our work provides new benchmarks for evaluating models of visual representation learning including Transformers. ",
    "url": "https://arxiv.org/abs/2306.00294",
    "authors": [
      "Hossein Adeli",
      "Seoyoung Ahn",
      "Nikolaus Kriegeskorte",
      "Gregory Zelinsky"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2306.00299",
    "title": "Robust Estimation of Surface Curvature Information from Point Cloud Data",
    "abstract": "This paper surveys and evaluates some popular state of the art methods for algorithmic curvature and normal estimation. In addition to surveying existing methods we also propose a new method for robust curvature estimation and evaluate it against existing methods thus demonstrating its superiority to existing methods in the case of significant data noise. Throughout this paper we are concerned with computation in low dimensional spaces (N < 10) and primarily focus on the computation of the Weingarten map and quantities that may be derived from this; however, the algorithms discussed are theoretically applicable in any dimension. One thing that is common to all these methods is their basis in an estimated graph structure. For any of these methods to work the local geometry of the manifold must be exploited; however, in the case of point cloud data it is often difficult to discover a robust manifold structure underlying the data, even in simple cases, which can greatly influence the results of these algorithms. We hope that in pushing these algorithms to their limits we are able to discover, and perhaps resolve, many major pitfalls that may affect potential users and future researchers hoping to improve these methods ",
    "url": "https://arxiv.org/abs/2306.00299",
    "authors": [
      "Jared Spang"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Differential Geometry (math.DG)"
    ]
  },
  {
    "id": "arXiv:2306.00315",
    "title": "Explicit Feature Interaction-aware Uplift Network for Online Marketing",
    "abstract": "As a key component in online marketing, uplift modeling aims to accurately capture the degree to which different treatments motivate different users, such as coupons or discounts, also known as the estimation of individual treatment effect (ITE). In an actual business scenario, the options for treatment may be numerous and complex, and there may be correlations between different treatments. In addition, each marketing instance may also have rich user and contextual features. However, existing methods still fall short in both fully exploiting treatment information and mining features that are sensitive to a particular treatment. In this paper, we propose an explicit feature interaction-aware uplift network (EFIN) to address these two problems. Our EFIN includes four customized modules: 1) a feature encoding module encodes not only the user and contextual features, but also the treatment features; 2) a self-interaction module aims to accurately model the user's natural response with all but the treatment features; 3) a treatment-aware interaction module accurately models the degree to which a particular treatment motivates a user through interactions between the treatment features and other features, i.e., ITE; and 4) an intervention constraint module is used to balance the ITE distribution of users between the control and treatment groups so that the model would still achieve a accurate uplift ranking on data collected from a non-random intervention marketing scenario. We conduct extensive experiments on two public datasets and one product dataset to verify the effectiveness of our EFIN. In addition, our EFIN has been deployed in a credit card bill payment scenario of a large online financial platform with a significant improvement. ",
    "url": "https://arxiv.org/abs/2306.00315",
    "authors": [
      "Dugang Liu",
      "Xing Tang",
      "Han Gao",
      "Fuyuan Lyu",
      "Xiuqiang He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2306.00316",
    "title": "Using Genetic Programming to Build Self-Adaptivity into Software-Defined  Networks",
    "abstract": "Self-adaptation solutions need to periodically monitor, reason about, and adapt a running system. The adaptation step involves generating an adaptation strategy and applying it to the running system whenever an anomaly arises. In this article, we argue that, rather than generating individual adaptation strategies, the goal should be to adapt the control logic of the running system in such a way that the system itself would learn how to steer clear of future anomalies, without triggering self-adaptation too frequently. While the need for adaptation is never eliminated, especially noting the uncertain and evolving environment of complex systems, reducing the frequency of adaptation interventions is advantageous for various reasons, e.g., to increase performance and to make a running system more robust. We instantiate and empirically examine the above idea for software-defined networking -- a key enabling technology for modern data centres and Internet of Things applications. Using genetic programming,(GP), we propose a self-adaptation solution that continuously learns and updates the control constructs in the data-forwarding logic of a software-defined network. Our evaluation, performed using open-source synthetic and industrial data, indicates that, compared to a baseline adaptation technique that attempts to generate individual adaptations, our GP-based approach is more effective in resolving network congestion, and further, reduces the frequency of adaptation interventions over time. In addition, we show that, for networks with the same topology, reusing over larger networks the knowledge that is learned on smaller networks leads to significant improvements in the performance of our GP-based adaptation approach. Finally, we compare our approach against a standard data-forwarding algorithm from the network literature, demonstrating that our approach significantly reduces packet loss. ",
    "url": "https://arxiv.org/abs/2306.00316",
    "authors": [
      "Jia Li",
      "Shiva Nejati",
      "Mehrdad Sabetzadeh"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2306.00342",
    "title": "Combining Explicit and Implicit Regularization for Efficient Learning in  Deep Networks",
    "abstract": "Works on implicit regularization have studied gradient trajectories during the optimization process to explain why deep networks favor certain kinds of solutions over others. In deep linear networks, it has been shown that gradient descent implicitly regularizes toward low-rank solutions on matrix completion/factorization tasks. Adding depth not only improves performance on these tasks but also acts as an accelerative pre-conditioning that further enhances this bias towards low-rankedness. Inspired by this, we propose an explicit penalty to mirror this implicit bias which only takes effect with certain adaptive gradient optimizers (e.g. Adam). This combination can enable a degenerate single-layer network to achieve low-rank approximations with generalization error comparable to deep linear networks, making depth no longer necessary for learning. The single-layer network also performs competitively or out-performs various approaches for matrix completion over a range of parameter and data regimes despite its simplicity. Together with an optimizer's inductive bias, our findings suggest that explicit regularization can play a role in designing different, desirable forms of regularization and that a more nuanced understanding of this interplay may be necessary. ",
    "url": "https://arxiv.org/abs/2306.00342",
    "authors": [
      "Dan Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00346",
    "title": "CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for  Mitigating Class Imbalance in Causal Claim Identification",
    "abstract": "The class imbalance problem can cause machine learning models to produce an undesirable performance on the minority class as well as the whole dataset. Using data augmentation techniques to increase the number of samples is one way to tackle this problem. We introduce a novel counterfactual data augmentation by verb replacement for the identification of medical claims. In addition, we investigate the impact of this method and compare it with 3 other data augmentation techniques, showing that the proposed method can result in a significant (relative) improvement in the minority class. ",
    "url": "https://arxiv.org/abs/2306.00346",
    "authors": [
      "Akbar Karimi",
      "Lucie Flek"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00349",
    "title": "CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV  Perception",
    "abstract": "Perception is crucial in the realm of autonomous driving systems, where bird's eye view (BEV)-based architectures have recently reached state-of-the-art performance. The desirability of self-supervised representation learning stems from the expensive and laborious process of annotating 2D and 3D data. Although previous research has investigated pretraining methods for both LiDAR and camera-based 3D object detection, a unified pretraining framework for multimodal BEV perception is missing. In this study, we introduce CALICO, a novel framework that applies contrastive objectives to both LiDAR and camera backbones. Specifically, CALICO incorporates two stages: point-region contrast (PRC) and region-aware distillation (RAD). PRC better balances the region- and scene-level representation learning on the LiDAR modality and offers significant performance improvement compared to existing methods. RAD effectively achieves contrastive distillation on our self-trained teacher model. CALICO's efficacy is substantiated by extensive evaluations on 3D object detection and BEV map segmentation tasks, where it delivers significant performance improvements. Notably, CALICO outperforms the baseline method by 10.5% and 8.6% on NDS and mAP. Moreover, CALICO boosts the robustness of multimodal 3D object detection against adversarial attacks and corruption. Additionally, our framework can be tailored to different backbones and heads, positioning it as a promising approach for multimodal BEV perception. ",
    "url": "https://arxiv.org/abs/2306.00349",
    "authors": [
      "Jiachen Sun",
      "Haizhong Zheng",
      "Qingzhao Zhang",
      "Atul Prakash",
      "Z. Morley Mao",
      "Chaowei Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00370",
    "title": "Graph Switching Dynamical Systems",
    "abstract": "Dynamical systems with complex behaviours, e.g. immune system cells interacting with a pathogen, are commonly modelled by splitting the behaviour into different regimes, or modes, each with simpler dynamics, and then learning the switching behaviour from one mode to another. Switching Dynamical Systems (SDS) are a powerful tool that automatically discovers these modes and mode-switching behaviour from time series data. While effective, these methods focus on independent objects, where the modes of one object are independent of the modes of the other objects. In this paper, we focus on the more general interacting object setting for switching dynamical systems, where the per-object dynamics also depends on an unknown and dynamically changing subset of other objects and their modes. To this end, we propose a novel graph-based approach for switching dynamical systems, GRAph Switching dynamical Systems (GRASS), in which we use a dynamic graph to characterize interactions between objects and learn both intra-object and inter-object mode-switching behaviour. We introduce two new datasets for this setting, a synthesized ODE-driven particles dataset and a real-world Salsa Couple Dancing dataset. Experiments show that GRASS can consistently outperforms previous state-of-the-art methods. ",
    "url": "https://arxiv.org/abs/2306.00370",
    "authors": [
      "Yongtuo Liu",
      "Sara Magliacane",
      "Miltiadis Kofinas",
      "Efstratios Gavves"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2306.00381",
    "title": "Better Context Makes Better Code Language Models: A Case Study on  Function Call Argument Completion",
    "abstract": "Pretrained code language models have enabled great progress towards program synthesis. However, common approaches only consider in-file local context and thus miss information and constraints imposed by other parts of the codebase and its external dependencies. Existing code completion benchmarks also lack such context. To resolve these restrictions we curate a new dataset of permissively licensed Python packages that includes full projects and their dependencies and provide tools to extract non-local information with the help of program analyzers. We then focus on the task of function call argument completion which requires predicting the arguments to function calls. We show that existing code completion models do not yield good results on our completion task. To better solve this task, we query a program analyzer for information relevant to a given function call, and consider ways to provide the analyzer results to different code completion models during inference and training. Our experiments show that providing access to the function implementation and function usages greatly improves the argument completion performance. Our ablation study provides further insights on how different types of information available from the program analyzer and different ways of incorporating the information affect the model performance. ",
    "url": "https://arxiv.org/abs/2306.00381",
    "authors": [
      "Hengzhi Pei",
      "Jinman Zhao",
      "Leonard Lausen",
      "Sheng Zha",
      "George Karypis"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00406",
    "title": "Faster Robust Tensor Power Method for Arbitrary Order",
    "abstract": "Tensor decomposition is a fundamental method used in various areas to deal with high-dimensional data. \\emph{Tensor power method} (TPM) is one of the widely-used techniques in the decomposition of tensors. This paper presents a novel tensor power method for decomposing arbitrary order tensors, which overcomes limitations of existing approaches that are often restricted to lower-order (less than $3$) tensors or require strong assumptions about the underlying data structure. We apply sketching method, and we are able to achieve the running time of $\\widetilde{O}(n^{p-1})$, on the power $p$ and dimension $n$ tensor. We provide a detailed analysis for any $p$-th order tensor, which is never given in previous works. ",
    "url": "https://arxiv.org/abs/2306.00406",
    "authors": [
      "Yichuan Deng",
      "Zhao Song",
      "Junze Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2306.00410",
    "title": "Towards hate speech detection in low-resource languages: Comparing ASR  to acoustic word embeddings on Wolof and Swahili",
    "abstract": "We consider hate speech detection through keyword spotting on radio broadcasts. One approach is to build an automatic speech recognition (ASR) system for the target low-resource language. We compare this to using acoustic word embedding (AWE) models that map speech segments to a space where matching words have similar vectors. We specifically use a multilingual AWE model trained on labelled data from well-resourced languages to spot keywords in data in the unseen target language. In contrast to ASR, the AWE approach only requires a few keyword exemplars. In controlled experiments on Wolof and Swahili where training and test data are from the same domain, an ASR model trained on just five minutes of data outperforms the AWE approach. But in an in-the-wild test on Swahili radio broadcasts with actual hate speech keywords, the AWE model (using one minute of template data) is more robust, giving similar performance to an ASR system trained on 30 hours of labelled data. ",
    "url": "https://arxiv.org/abs/2306.00410",
    "authors": [
      "Christiaan Jacobs",
      "Nathana\u00ebl Carraz Rakotonirina",
      "Everlyn Asiko Chimoto",
      "Bruce A. Bassett",
      "Herman Kamper"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2306.00412",
    "title": "Beamforming Design for IRS-and-UAV-aided Two-way Amplify-and-Forward  Relay Networks",
    "abstract": "As a promising solution to improve communication quality, unmanned aerial vehicle (UAV) has been widely integrated into wireless networks. In this paper, for the sake of enhancing the message exchange rate between User1 (U1) and User2 (U2), an intelligent reflective surface (IRS)-and-UAV- assisted two-way amplify-and-forward (AF) relay wireless system is proposed, where U1 and U2 can communicate each other via a UAV-mounted IRS and an AF relay. Besides, an optimization problem of maximizing minimum rate is casted, where the variables, namely AF relay beamforming matrix and IRS phase shifts of two time slots, need to be optimized. To achieve a maximum rate, a low-complexity alternately iterative (AI) scheme based on zero forcing and successive convex approximation (LC-ZF-SCA) algorithm is put forward, where the expression of AF relay beamforming matrix can be derived in semi-closed form by ZF method, and IRS phase shift vectors of two time slots can be respectively optimized by utilizing SCA algorithm. To obtain a significant rate enhancement, a high-performance AI method based on one step, semidefinite programming and penalty SCA (ONS-SDP-PSCA) is proposed, where the beamforming matrix at AF relay can be firstly solved by singular value decomposition and ONS method, IRS phase shift matrices of two time slots are optimized by SDP and PSCA algorithms. Simulation results present that the rate performance of the proposed LC-ZF-SCA and ONS-SDP-PSCA methods surpass those of random phase and only AF relay. In particular, when total transmit power is equal to 30dBm, the proposed two methods can harvest more than 68.5% rate gain compared to random phase and only AF relay. Meanwhile, the rate performance of ONS-SDP-PSCA method at cost of extremely high complexity is superior to that of LC-ZF-SCA method. ",
    "url": "https://arxiv.org/abs/2306.00412",
    "authors": [
      "Xuehui Wang",
      "Feng Shu",
      "Yuanyuan Wu",
      "Shihao Yan",
      "Yifan Zhao",
      "Qiankun Cheng",
      "Jiangzhou Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2306.00418",
    "title": "Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect  Sentiment Quad Prediction",
    "abstract": "Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works only focus on what to generate but ignore what not to generate. We argue that considering the negative samples also leads to potential benefits. In this work, we propose a template-agnostic method to control the token-level generation, which boosts original learning and reduces mistakes simultaneously. Specifically, we introduce Monte Carlo dropout to understand the built-in uncertainty of pre-trained language models, acquiring the noises and errors. We further propose marginalized unlikelihood learning to suppress the uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to balance the effects of marginalized unlikelihood learning. Extensive experiments on four public datasets demonstrate the effectiveness of our approach on various generation templates1. ",
    "url": "https://arxiv.org/abs/2306.00418",
    "authors": [
      "Mengting Hu",
      "Yinhao Bai",
      "Yike Wu",
      "Zhen Zhang",
      "Liqi Zhang",
      "Hang Gao",
      "Shiwan Zhao",
      "Minlie Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.00419",
    "title": "Challenges and Remedies to Privacy and Security in AIGC: Exploring the  Potential of Privacy Computing, Blockchain, and Beyond",
    "abstract": "Artificial Intelligence Generated Content (AIGC) is one of the latest achievements in AI development. The content generated by related applications, such as text, images and audio, has sparked a heated discussion. Various derived AIGC applications are also gradually entering all walks of life, bringing unimaginable impact to people's daily lives. However, the rapid development of such generative tools has also raised concerns about privacy and security issues, and even copyright issues in AIGC. We note that advanced technologies such as blockchain and privacy computing can be combined with AIGC tools, but no work has yet been done to investigate their relevance and prospect in a systematic and detailed way. Therefore it is necessary to investigate how they can be used to protect the privacy and security of data in AIGC by fully exploring the aforementioned technologies. In this paper, we first systematically review the concept, classification and underlying technologies of AIGC. Then, we discuss the privacy and security challenges faced by AIGC from multiple perspectives and purposefully list the countermeasures that currently exist. We hope our survey will help researchers and industry to build a more secure and robust AIGC system. ",
    "url": "https://arxiv.org/abs/2306.00419",
    "authors": [
      "Chuan Chen",
      "Zhenpeng Wu",
      "Yanyi Lai",
      "Wenlin Ou",
      "Tianchi Liao",
      "Zibin Zheng"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.00427",
    "title": "Out-of-distribution forgetting: vulnerability of continual learning to  intra-class distribution shift",
    "abstract": "Continual learning (CL) is an important technique to allow artificial neural networks to work in open environments. CL enables a system to learn new tasks without severe interference to its performance on old tasks, i.e., overcome the problems of catastrophic forgetting. In joint learning, it is well known that the out-of-distribution (OOD) problem caused by intentional attacks or environmental perturbations will severely impair the ability of networks to generalize. In this work, we reported a special form of catastrophic forgetting raised by the OOD problem in continual learning settings, and we named it out-of-distribution forgetting (OODF). In continual image classification tasks, we found that for a given category, introducing an intra-class distribution shift significantly impaired the recognition accuracy of CL methods for that category during subsequent learning. Interestingly, this phenomenon is special for CL as the same level of distribution shift had only negligible effects in the joint learning scenario. We verified that CL methods without dedicating subnetworks for individual tasks are all vulnerable to OODF. Moreover, OODF does not depend on any specific way of shifting the distribution, suggesting it is a risk for CL in a wide range of circumstances. Taken together, our work identified an under-attended risk during CL, highlighting the importance of developing approaches that can overcome OODF. ",
    "url": "https://arxiv.org/abs/2306.00427",
    "authors": [
      "Liangxuan Guo",
      "Yang Chen",
      "Shan Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00440",
    "title": "Edge-guided Representation Learning for Underwater Object Detection",
    "abstract": "Underwater object detection (UOD) is crucial for marine economic development, environmental protection, and the planet's sustainable development. The main challenges of this task arise from low-contrast, small objects, and mimicry of aquatic organisms. The key to addressing these challenges is to focus the model on obtaining more discriminative information. We observe that the edges of underwater objects are highly unique and can be distinguished from low-contrast or mimicry environments based on their edges. Motivated by this observation, we propose an Edge-guided Representation Learning Network, termed ERL-Net, that aims to achieve discriminative representation learning and aggregation under the guidance of edge cues. Firstly, we introduce an edge-guided attention module to model the explicit boundary information, which generates more discriminative features. Secondly, a feature aggregation module is proposed to aggregate the multi-scale discriminative features by regrouping them into three levels, effectively aggregating global and local information for locating and recognizing underwater objects. Finally, we propose a wide and asymmetric receptive field block to enable features to have a wider receptive field, allowing the model to focus on more small object information. Comprehensive experiments on three challenging underwater datasets show that our method achieves superior performance on the UOD task. ",
    "url": "https://arxiv.org/abs/2306.00440",
    "authors": [
      "Linhui Dai",
      "Hong Liu",
      "Pinhao Song",
      "Hao Tang",
      "Runwei Ding",
      "Shengquan Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00445",
    "title": "A big data approach towards sarcasm detection in Russian",
    "abstract": "We present a set of deterministic algorithms for Russian inflection and automated text synthesis. These algorithms are implemented in a publicly available web-service www.passare.ru. This service provides functions for inflection of single words, word matching and synthesis of grammatically correct Russian text. Selected code and datasets are available at https://github.com/passare-ru/PassareFunctions/ Performance of the inflectional functions has been tested against the annotated corpus of Russian language OpenCorpora, compared with that of other solutions, and used for estimating the morphological variability and complexity of different parts of speech in Russian. ",
    "url": "https://arxiv.org/abs/2306.00445",
    "authors": [
      "A.A. Gurin",
      "T.M. Sadykov",
      "T.A. Zhukov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.00483",
    "title": "Overcoming Language Bias in Remote Sensing Visual Question Answering via  Adversarial Training",
    "abstract": "The Visual Question Answering (VQA) system offers a user-friendly interface and enables human-computer interaction. However, VQA models commonly face the challenge of language bias, resulting from the learned superficial correlation between questions and answers. To address this issue, in this study, we present a novel framework to reduce the language bias of the VQA for remote sensing data (RSVQA). Specifically, we add an adversarial branch to the original VQA framework. Based on the adversarial branch, we introduce two regularizers to constrain the training process against language bias. Furthermore, to evaluate the performance in terms of language bias, we propose a new metric that combines standard accuracy with the performance drop when incorporating question and random image information. Experimental results demonstrate the effectiveness of our method. We believe that our method can shed light on future work for reducing language bias on the RSVQA task. ",
    "url": "https://arxiv.org/abs/2306.00483",
    "authors": [
      "Zhenghang Yuan",
      "Lichao Mou",
      "Xiao Xiang Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00488",
    "title": "Reconstructing Graph Diffusion History from a Single Snapshot",
    "abstract": "Diffusion on graphs is ubiquitous with numerous high-impact applications. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot} (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) estimation error of diffusion parameters is unavoidable due to NP-hardness of diffusion parameter estimation, and (b) the MLE formulation is sensitive to estimation error of diffusion parameters. To overcome the inherent limitation of the MLE formulation, we propose a novel barycenter formulation: finding the barycenter of the posterior distribution of histories, which is provably stable against the estimation error of diffusion parameters. We further develop an effective solver named DIffusion hiTting Times with Optimal proposal (DITTO) by reducing the problem to estimating posterior expected hitting times via the Metropolis--Hastings Markov chain Monte Carlo method (M--H MCMC) and employing an unsupervised graph neural network to learn an optimal proposal to accelerate the convergence of M--H MCMC. We conduct extensive experiments to demonstrate the efficacy of the proposed method. ",
    "url": "https://arxiv.org/abs/2306.00488",
    "authors": [
      "Ruizhong Qiu",
      "Dingsu Wang",
      "Lei Ying",
      "H. Vincent Poor",
      "Yifang Zhang",
      "Hanghang Tong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2306.00522",
    "title": "A New PHO-rmula for Improved Performance of Semi-Structured Networks",
    "abstract": "Recent advances to combine structured regression models and deep neural networks for better interpretability, more expressiveness, and statistically valid uncertainty quantification demonstrate the versatility of semi-structured neural networks (SSNs). We show that techniques to properly identify the contributions of the different model components in SSNs, however, lead to suboptimal network estimation, slower convergence, and degenerated or erroneous predictions. In order to solve these problems while preserving favorable model properties, we propose a non-invasive post-hoc orthogonalization (PHO) that guarantees identifiability of model components and provides better estimation and prediction quality. Our theoretical findings are supported by numerical experiments, a benchmark comparison as well as a real-world application to COVID-19 infections. ",
    "url": "https://arxiv.org/abs/2306.00522",
    "authors": [
      "David R\u00fcgamer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00543",
    "title": "A Novel Driver Distraction Behavior Detection Based on Self-Supervised  Learning Framework with Masked Image Modeling",
    "abstract": "Driver distraction causes a significant number of traffic accidents every year, resulting in economic losses and casualties. Currently, the level of automation in commercial vehicles is far from completely unmanned, and drivers still play an important role in operating and controlling the vehicle. Therefore, driver distraction behavior detection is crucial for road safety. At present, driver distraction detection primarily relies on traditional Convolutional Neural Networks (CNN) and supervised learning methods. However, there are still challenges such as the high cost of labeled datasets, limited ability to capture high-level semantic information, and weak generalization performance. In order to solve these problems, this paper proposes a new self-supervised learning method based on masked image modeling for driver distraction behavior detection. Firstly, a self-supervised learning framework for masked image modeling (MIM) is introduced to solve the serious human and material consumption issues caused by dataset labeling. Secondly, the Swin Transformer is employed as an encoder. Performance is enhanced by reconfiguring the Swin Transformer block and adjusting the distribution of the number of window multi-head self-attention (W-MSA) and shifted window multi-head self-attention (SW-MSA) detection heads across all stages, which leads to model more lightening. Finally, various data augmentation strategies are used along with the best random masking strategy to strengthen the model's recognition and generalization ability. Test results on a large-scale driver distraction behavior dataset show that the self-supervised learning method proposed in this paper achieves an accuracy of 99.60%, approximating the excellent performance of advanced supervised learning methods. ",
    "url": "https://arxiv.org/abs/2306.00543",
    "authors": [
      "Yingzhi Zhang",
      "Taiguo Li",
      "Chao Li",
      "Xinghong Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00544",
    "title": "Codebook Configuration for 1-bit RIS-aided Systems Based on Implicit  Neural Representations",
    "abstract": "Reconfigurable intelligent surfaces (RISs) have become one of the key technologies in 6G wireless communications. By configuring the reflection beamforming codebooks, RIS focuses signals on target receivers. In this paper, we investigate the codebook configuration for 1-bit RIS-aided systems. We propose a novel learning-based method built upon the advanced methodology of implicit neural representations. The proposed model learns a continuous and differentiable coordinate-to-codebook representation from samplings. Our method only requires the information of the user's coordinate and avoids the assumption of channel models. Moreover, we propose an encoding-decoding strategy to reduce the dimension of codebooks, and thus improve the learning efficiency of the proposed method. Experimental results on simulation and measured data demonstrated the remarkable advantages of the proposed method. ",
    "url": "https://arxiv.org/abs/2306.00544",
    "authors": [
      "Yao Xiao",
      "Zhijie Fan",
      "Zenan Ling",
      "Rujing Xiong",
      "Tiebin Mi",
      "Robert Caiming Qiu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2306.00578",
    "title": "Does Black-box Attribute Inference Attacks on Graph Neural Networks  Constitute Privacy Risk?",
    "abstract": "Graph neural networks (GNNs) have shown promising results on real-life datasets and applications, including healthcare, finance, and education. However, recent studies have shown that GNNs are highly vulnerable to attacks such as membership inference attack and link reconstruction attack. Surprisingly, attribute inference attacks has received little attention. In this paper, we initiate the first investigation into attribute inference attack where an attacker aims to infer the sensitive user attributes based on her public or non-sensitive attributes. We ask the question whether black-box attribute inference attack constitutes a significant privacy risk for graph-structured data and their corresponding GNN model. We take a systematic approach to launch the attacks by varying the adversarial knowledge and assumptions. Our findings reveal that when an attacker has black-box access to the target model, GNNs generally do not reveal significantly more information compared to missing value estimation techniques. Code is available. ",
    "url": "https://arxiv.org/abs/2306.00578",
    "authors": [
      "Iyiola E. Olatunji",
      "Anmar Hizber",
      "Oliver Sihlovec",
      "Megha Khosla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2306.00579",
    "title": "FMapping: Factorized Efficient Neural Field Mapping for Real-Time Dense  RGB SLAM",
    "abstract": "In this paper, we introduce FMapping, an efficient neural field mapping framework that facilitates the continuous estimation of a colorized point cloud map in real-time dense RGB SLAM. To achieve this challenging goal without depth, a hurdle is how to improve efficiency and reduce the mapping uncertainty of the RGB SLAM system. To this end, we first build up a theoretical analysis by decomposing the SLAM system into tracking and mapping parts, and the mapping uncertainty is explicitly defined within the frame of neural representations. Based on the analysis, we then propose an effective factorization scheme for scene representation and introduce a sliding window strategy to reduce the uncertainty for scene reconstruction. Specifically, we leverage the factorized neural field to decompose uncertainty into a lower-dimensional space, which enhances robustness to noise and improves training efficiency. We then propose the sliding window sampler to reduce uncertainty by incorporating coherent geometric cues from observed frames during map initialization to enhance convergence. Our factorized neural mapping approach enjoys some advantages, such as low memory consumption, more efficient computation, and fast convergence during map initialization. Experiments on two benchmark datasets show that our method can update the map of high-fidelity colorized point clouds around 2 seconds in real time while requiring no customized CUDA kernels. Additionally, it utilizes x20 fewer parameters than the most concise neural implicit mapping of prior methods for SLAM, e.g., iMAP [ 31] and around x1000 fewer parameters than the state-of-the-art approach, e.g., NICE-SLAM [ 42]. For more details, please refer to our project homepage: https://vlis2022.github.io/fmap/. ",
    "url": "https://arxiv.org/abs/2306.00579",
    "authors": [
      "Tongyan Hua",
      "Haotian Bai",
      "Zidong Cao",
      "Lin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00582",
    "title": "Anomaly Detection with Variance Stabilized Density Estimation",
    "abstract": "Density estimation based anomaly detection schemes typically model anomalies as examples that reside in low-density regions. We propose a modified density estimation problem and demonstrate its effectiveness for anomaly detection. Specifically, we assume the density function of normal samples is uniform in some compact domain. This assumption implies the density function is more stable (with lower variance) around normal samples than anomalies. We first corroborate this assumption empirically using a wide range of real-world data. Then, we design a variance stabilized density estimation problem for maximizing the likelihood of the observed samples while minimizing the variance of the density around normal samples. We introduce an ensemble of autoregressive models to learn the variance stabilized distribution. Finally, we perform an extensive benchmark with 52 datasets demonstrating that our method leads to state-of-the-art results while alleviating the need for data-specific hyperparameter tuning. ",
    "url": "https://arxiv.org/abs/2306.00582",
    "authors": [
      "Amit Rozner",
      "Barak Battash",
      "Henry Li",
      "Lior Wolf",
      "Ofir Lindenbaum"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.00584",
    "title": "Representation Theorems Obtained by Miningacross Web Sources for Hints",
    "abstract": "A representation theorem relates different mathematical structures by providing an isomorphism between them: that is, a one-to-one correspondence preserving their original properties. Establishing that the two structures substantially behave in the same way, representation theorems typically provide insight and generate powerful techniques to study the involved structures, by cross-fertilising between the methodologies existing for each of the respective branches of mathematics. When the related structures have no obvious a priori connection, however, such results can be, by their own nature, elusive. Here, we show how data-mining across distinct web sources (including the Online Encyclopedia of Integer Sequences, OEIS), was crucial in the discovery of two original representation theorems relating event structures (mathematical structures commonly used to represent concurrent discrete systems) to families of sets (endowed with elementary disjointness and subset relations) and to full graphs, respectively. The latter originally emerged in the apparently unrelated field of bioinformatics. As expected, our representation theorems are powerful, allowing to capitalise on existing theorems about full graphs to immediately conclude new facts about event structures. Our contribution is twofold: on one hand, we illustrate our novel method to mine the web, resulting in thousands of candidate connections between distinct mathematical realms; on the other hand, we explore one of these connections to obtain our new representation theorems. We hope this paper can encourage people with relevant expertise to scrutinize these candidate connections. We anticipate that, building on the ideas presented here, further connections can be unearthed, by refining the mining techniques and by extending the mined repositories. ",
    "url": "https://arxiv.org/abs/2306.00584",
    "authors": [
      "Marco B. Caminati",
      "Juliana K. F. Bowles"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2306.00585",
    "title": "Causal Imitability Under Context-Specific Independence Relations",
    "abstract": "Drawbacks of ignoring the causal mechanisms when performing imitation learning have recently been acknowledged. Several approaches both to assess the feasibility of imitation and to circumvent causal confounding and causal misspecifications have been proposed in the literature. However, the potential benefits of the incorporation of additional information about the underlying causal structure are left unexplored. An example of such overlooked information is context-specific independence (CSI), i.e., independence that holds only in certain contexts. We consider the problem of causal imitation learning when CSI relations are known. We prove that the decision problem pertaining to the feasibility of imitation in this setting is NP-hard. Further, we provide a necessary graphical criterion for imitation learning under CSI and show that under a structural assumption, this criterion is also sufficient. Finally, we propose a sound algorithmic approach for causal imitation learning which takes both CSI relations and data into account. ",
    "url": "https://arxiv.org/abs/2306.00585",
    "authors": [
      "Fateme Jamshidi",
      "Sina Akbari",
      "Negar Kiyavash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.00597",
    "title": "Analysis of ChatGPT on Source Code",
    "abstract": "This paper explores the use of Large Language Models (LLMs) and in particular ChatGPT in programming, source code analysis, and code generation. LLMs and ChatGPT are built using machine learning and artificial intelligence techniques, and they offer several benefits to developers and programmers. While these models can save time and provide highly accurate results, they are not yet advanced enough to replace human programmers entirely. The paper investigates the potential applications of LLMs and ChatGPT in various areas, such as code creation, code documentation, bug detection, refactoring, and more. The paper also suggests that the usage of LLMs and ChatGPT is expected to increase in the future as they offer unparalleled benefits to the programming community. ",
    "url": "https://arxiv.org/abs/2306.00597",
    "authors": [
      "Ahmed Sadik",
      "Antonello Ceravola",
      "Frank Joublin",
      "Jibesh Patra"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2306.00605",
    "title": "Stay on Track: A Frenet Wrapper to Overcome Off-road Trajectories in  Vehicle Motion Prediction",
    "abstract": "Predicting the future motion of observed vehicles is a crucial enabler for safe autonomous driving. The field of motion prediction has seen large progress recently with State-of-the-Art (SotA) models achieving impressive results on large-scale public benchmarks. However, recent work revealed that learning-based methods are prone to predict off-road trajectories in challenging scenarios. These can be created by perturbing existing scenarios with additional turns in front of the target vehicle while the motion history is left unchanged. We argue that this indicates that SotA models do not consider the map information sufficiently and demonstrate how this can be solved, by representing model inputs and outputs in a Frenet frame defined by lane centreline sequences. To this end, we present a general wrapper that leverages a Frenet representation of the scene and that can be applied to SotA models without changing their architecture. We demonstrate the effectiveness of this approach in a comprehensive benchmark using two SotA motion prediction models. Our experiments show that this reduces the off-road rate on challenging scenarios by more than 90\\%, without sacrificing average performance. ",
    "url": "https://arxiv.org/abs/2306.00605",
    "authors": [
      "Marcel Hallgarten",
      "Ismail Kisa",
      "Martin Stoll",
      "Andreas Zell"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2306.00607",
    "title": "FACT: Federated Adversarial Cross Training",
    "abstract": "Federated Learning (FL) facilitates distributed model development to aggregate multiple confidential data sources. The information transfer among clients can be compromised by distributional differences, i.e., by non-i.i.d. data. A particularly challenging scenario is the federated model adaptation to a target client without access to annotated data. We propose Federated Adversarial Cross Training (FACT), which uses the implicit domain differences between source clients to identify domain shifts in the target domain. In each round of FL, FACT cross initializes a pair of source clients to generate domain specialized representations which are then used as a direct adversary to learn a domain invariant data representation. We empirically show that FACT outperforms state-of-the-art federated, non-federated and source-free domain adaptation models on three popular multi-source-single-target benchmarks, and state-of-the-art Unsupervised Domain Adaptation (UDA) models on single-source-single-target experiments. We further study FACT's behavior with respect to communication restrictions and the number of participating clients. ",
    "url": "https://arxiv.org/abs/2306.00607",
    "authors": [
      "Stefan Schrod",
      "Jonas Lippl",
      "Andreas Sch\u00e4fer",
      "Michael Altenbuchinger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00616",
    "title": "Progressive Learning for Physics-informed Neural Motion Planning",
    "abstract": "Motion planning (MP) is one of the core robotics problems requiring fast methods for finding a collision-free robot motion path connecting the given start and goal states. Neural motion planners (NMPs) demonstrate fast computational speed in finding path solutions but require a huge amount of expert trajectories for learning, thus adding a significant training computational load. In contrast, recent advancements have also led to a physics-informed NMP approach that directly solves the Eikonal equation for motion planning and does not require expert demonstrations for learning. However, experiments show that the physics-informed NMP approach performs poorly in complex environments and lacks scalability in multiple scenarios and high-dimensional real robot settings. To overcome these limitations, this paper presents a novel and tractable Eikonal equation formulation and introduces a new progressive learning strategy to train neural networks without expert data in complex, cluttered, multiple high-dimensional robot motion planning scenarios. The results demonstrate that our method outperforms state-of-the-art traditional MP, data-driven NMP, and physics-informed NMP methods by a significant margin in terms of computational planning speed, path quality, and success rates. We also show that our approach scales to multiple complex, cluttered scenarios and the real robot set up in a narrow passage environment. The proposed method's videos and code implementations are available at https://github.com/ruiqini/P-NTFields. ",
    "url": "https://arxiv.org/abs/2306.00616",
    "authors": [
      "Ruiqi Ni",
      "Ahmed H. Qureshi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00623",
    "title": "Physical Attacks on the Railway System",
    "abstract": "Recent attacks encouraged public interest in physical security for railways. Knowing about and learning from previous attacks is necessary to secure against them. This paper presents a structured data set of physical attacks against railways. We analyze the data regarding the used means, the railway system's target component, the attacker type, and the geographical distribution of attacks. The results indicate a growing heterogeneity of observed attacks in the recent decade compared to the previous decades and centuries, making protecting railways more complex. ",
    "url": "https://arxiv.org/abs/2306.00623",
    "authors": [
      "Lukas Iffl\u00e4nder",
      "Thomas Buder",
      "Teresa Loreth",
      "Marina Alonso Villota",
      "Walter Schmitz",
      "Karl Adolf Neubecker",
      "Stefan Pickl"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2306.00624",
    "title": "From Temporal to Contemporaneous Iterative Causal Discovery in the  Presence of Latent Confounders",
    "abstract": "We present a constraint-based algorithm for learning causal structures from observational time-series data, in the presence of latent confounders. We assume a discrete-time, stationary structural vector autoregressive process, with both temporal and contemporaneous causal relations. One may ask if temporal and contemporaneous relations should be treated differently. The presented algorithm gradually refines a causal graph by learning long-term temporal relations before short-term ones, where contemporaneous relations are learned last. This ordering of causal relations to be learnt leads to a reduction in the required number of statistical tests. We validate this reduction empirically and demonstrate that it leads to higher accuracy for synthetic data and more plausible causal graphs for real-world data compared to state-of-the-art algorithms. ",
    "url": "https://arxiv.org/abs/2306.00624",
    "authors": [
      "Raanan Y. Rohekar",
      "Shami Nisimov",
      "Yaniv Gurwicz",
      "Gal Novik"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00651",
    "title": "Learning Prescriptive ReLU Networks",
    "abstract": "We study the problem of learning optimal policy from a set of discrete treatment options using observational data. We propose a piecewise linear neural network model that can balance strong prescriptive performance and interpretability, which we refer to as the prescriptive ReLU network, or P-ReLU. We show analytically that this model (i) partitions the input space into disjoint polyhedra, where all instances that belong to the same partition receive the same treatment, and (ii) can be converted into an equivalent prescriptive tree with hyperplane splits for interpretability. We demonstrate the flexibility of the P-ReLU network as constraints can be easily incorporated with minor modifications to the architecture. Through experiments, we validate the superior prescriptive accuracy of P-ReLU against competing benchmarks. Lastly, we present examples of interpretable prescriptive trees extracted from trained P-ReLUs using a real-world dataset, for both the unconstrained and constrained scenarios. ",
    "url": "https://arxiv.org/abs/2306.00651",
    "authors": [
      "Wei Sun",
      "Asterios Tsiourvas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00652",
    "title": "Explanation Graph Generation via Generative Pre-training over Synthetic  Graphs",
    "abstract": "The generation of explanation graphs is a significant task that aims to produce explanation graphs in response to user input, revealing the internal reasoning process. This task is challenging due to the significant discrepancy between unstructured user queries and structured explanation graphs. Current research commonly fine-tunes a text-based pre-trained language model on a small downstream dataset that is annotated with labeled graphs. However, due to the limited scale of available datasets, this approach may prove to be insufficient in bridging the gap between natural language text and structured graphs. In this paper, to alleviate the above limitations, we propose a novel pre-trained framework EG3P(for Explanation Graph Generation via Generative Pre-training over synthetic graphs) for the explanation graph generation task. Specifically, we first propose a text-to-graph generative task to pre-train the model with the goal of bridging the text-graph gap. Additionally, we propose an automatic corpus synthesis strategy for synthesizing a large scale of high-quality corpus, reducing the reliance on costly manual annotation methods. Experimental results on ExplaGraphs show the effectiveness of EG3P that our model surpasses all baseline systems with remarkable margins. Besides, further analysis demonstrates that EG3P is able to generate better explanation graphs on actual reasoning tasks such as CommonsenseQA and OpenbookQA. ",
    "url": "https://arxiv.org/abs/2306.00652",
    "authors": [
      "Han Cui",
      "Shangzhan Li",
      "Yu Zhang",
      "Qi Shi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.00658",
    "title": "NeuroGF: A Neural Representation for Fast Geodesic Distance and Path  Queries",
    "abstract": "Geodesics are essential in many geometry processing applications. However, traditional algorithms for computing geodesic distances and paths on 3D mesh models are often inefficient and slow. This makes them impractical for scenarios that require extensive querying of arbitrary point-to-point geodesics. Although neural implicit representations have emerged as a popular way of representing 3D shape geometries, there is still no research on representing geodesics with deep implicit functions. To bridge this gap, this paper presents the first attempt to represent geodesics on 3D mesh models using neural implicit functions. Specifically, we introduce neural geodesic fields (NeuroGFs), which are learned to represent the all-pairs geodesics of a given mesh. By using NeuroGFs, we can efficiently and accurately answer queries of arbitrary point-to-point geodesic distances and paths, overcoming the limitations of traditional algorithms. Evaluations on common 3D models show that NeuroGFs exhibit exceptional performance in solving the single-source all-destination (SSAD) and point-to-point geodesics, and achieve high accuracy consistently. Moreover, NeuroGFs offer the unique advantage of encoding both 3D geometry and geodesics in a unified representation. Code is made available at https://github.com/keeganhk/NeuroGF/tree/master. ",
    "url": "https://arxiv.org/abs/2306.00658",
    "authors": [
      "Qijian Zhang",
      "Junhui Hou",
      "Yohanes Yudhi Adikusuma",
      "Wenping Wang",
      "Ying He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00659",
    "title": "Do not Interfere but Cooperate: A Fully Learnable Code Design for  Multi-Access Channels with Feedback",
    "abstract": "Data-driven deep learning based code designs, including low-complexity neural decoders for existing codes, or end-to-end trainable auto-encoders have exhibited impressive results, particularly in scenarios for which we do not have high-performing structured code designs. However, the vast majority of existing data-driven solutions for channel coding focus on a point-to-point scenario. In this work, we consider a multiple access channel (MAC) with feedback and try to understand whether deep learning-based designs are capable of enabling coordination and cooperation among the encoders as well as allowing error correction. Simulation results show that the proposed multi-access block attention feedback (MBAF) code improves the upper bound of the achievable rate of MAC without feedback in finite block length regime. ",
    "url": "https://arxiv.org/abs/2306.00659",
    "authors": [
      "Emre Ozfatura",
      "Chenghong Bian",
      "Deniz Gunduz"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2306.00660",
    "title": "Improving Polish to English Neural Machine Translation with Transfer  Learning: Effects of Data Volume and Language Similarity",
    "abstract": "This paper investigates the impact of data volume and the use of similar languages on transfer learning in a machine translation task. We find out that having more data generally leads to better performance, as it allows the model to learn more patterns and generalizations from the data. However, related languages can also be particularly effective when there is limited data available for a specific language pair, as the model can leverage the similarities between the languages to improve performance. To demonstrate, we fine-tune mBART model for a Polish-English translation task using the OPUS-100 dataset. We evaluate the performance of the model under various transfer learning configurations, including different transfer source languages and different shot levels for Polish, and report the results. Our experiments show that a combination of related languages and larger amounts of data outperforms the model trained on related languages or larger amounts of data alone. Additionally, we show the importance of related languages in zero-shot and few-shot configurations. ",
    "url": "https://arxiv.org/abs/2306.00660",
    "authors": [
      "Juuso Eronen",
      "Michal Ptaszynski",
      "Karol Nowakowski",
      "Zheng Lin Chia",
      "Fumito Masui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2306.00668",
    "title": "Evaluating Stability in Massive Social Networks: Efficient Streaming  Algorithms for Structural Balance",
    "abstract": "Structural balance theory studies stability in networks. Given a $n$-vertex complete graph $G=(V,E)$ whose edges are labeled positive or negative, the graph is considered \\emph{balanced} if every triangle either consists of three positive edges (three mutual ``friends''), or one positive edge and two negative edges (two ``friends'' with a common ``enemy''). From a computational perspective, structural balance turns out to be a special case of correlation clustering with the number of clusters at most two. The two main algorithmic problems of interest are: $(i)$ detecting whether a given graph is balanced, or $(ii)$ finding a partition that approximates the \\emph{frustration index}, i.e., the minimum number of edge flips that turn the graph balanced. We study these problems in the streaming model where edges are given one by one and focus on \\emph{memory efficiency}. We provide randomized single-pass algorithms for: $(i)$ determining whether an input graph is balanced with $O(\\log{n})$ memory, and $(ii)$ finding a partition that induces a $(1 + \\varepsilon)$-approximation to the frustration index with $O(n \\cdot \\text{polylog}(n))$ memory. We further provide several new lower bounds, complementing different aspects of our algorithms such as the need for randomization or approximation. To obtain our main results, we develop a method using pseudorandom generators (PRGs) to sample edges between independently-chosen \\emph{vertices} in graph streaming. Furthermore, our algorithm that approximates the frustration index improves the running time of the state-of-the-art correlation clustering with two clusters (Giotis-Guruswami algorithm [SODA 2006]) from $n^{O(1/\\varepsilon^2)}$ to $O(n^2\\log^3{n}/\\varepsilon^2 + n\\log n \\cdot (1/\\varepsilon)^{O(1/\\varepsilon^4)})$ time for $(1+\\varepsilon)$-approximation. These results may be of independent interest. ",
    "url": "https://arxiv.org/abs/2306.00668",
    "authors": [
      "Vikrant Ashvinkumar",
      "Sepehr Assadi",
      "Chengyuan Deng",
      "Jie Gao",
      "Chen Wang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2306.00676",
    "title": "Hyperspectral Target Detection Based on Low-Rank Background Subspace  Learning and Graph Laplacian Regularization",
    "abstract": "Hyperspectral target detection is good at finding dim and small objects based on spectral characteristics. However, existing representation-based methods are hindered by the problem of the unknown background dictionary and insufficient utilization of spatial information. To address these issues, this paper proposes an efficient optimizing approach based on low-rank representation (LRR) and graph Laplacian regularization (GLR). Firstly, to obtain a complete and pure background dictionary, we propose a LRR-based background subspace learning method by jointly mining the low-dimensional structure of all pixels. Secondly, to fully exploit local spatial relationships and capture the underlying geometric structure, a local region-based GLR is employed to estimate the coefficients. Finally, the desired detection map is generated by computing the ratio of representation errors from binary hypothesis testing. The experiments conducted on two benchmark datasets validate the effectiveness and superiority of the approach. For reproduction, the accompanying code is available at https://github.com/shendb2022/LRBSL-GLR. ",
    "url": "https://arxiv.org/abs/2306.00676",
    "authors": [
      "Dunbin Shen",
      "Xiaorui Ma",
      "Wenfeng Kong",
      "Jiacheng Tian",
      "Hongyu Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00680",
    "title": "Encoder-decoder multimodal speaker change detection",
    "abstract": "The task of speaker change detection (SCD), which detects points where speakers change in an input, is essential for several applications. Several studies solved the SCD task using audio inputs only and have shown limited performance. Recently, multimodal SCD (MMSCD) models, which utilise text modality in addition to audio, have shown improved performance. In this study, the proposed model are built upon two main proposals, a novel mechanism for modality fusion and the adoption of a encoder-decoder architecture. Different to previous MMSCD works that extract speaker embeddings from extremely short audio segments, aligned to a single word, we use a speaker embedding extracted from 1.5s. A transformer decoder layer further improves the performance of an encoder-only MMSCD model. The proposed model achieves state-of-the-art results among studies that report SCD performance and is also on par with recent work that combines SCD with automatic speech recognition via human transcription. ",
    "url": "https://arxiv.org/abs/2306.00680",
    "authors": [
      "Jee-weon Jung",
      "Soonshin Seo",
      "Hee-Soo Heo",
      "Geonmin Kim",
      "You Jin Kim",
      "Young-ki Kwon",
      "Minjae Lee",
      "Bong-Jin Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2306.00681",
    "title": "Green Segment Routing for Improved Sustainability of Backbone Networks",
    "abstract": "Improving the energy efficiency of Internet Service Provider (ISP) backbone networks is an important objective for ISP operators. In these networks, the overall traffic load throughout the day can vary drastically, resulting in many backbone networks being highly overprovisioned during periods of lower traffic volume. In this paper, we propose a new Segment Routing (SR)-based optimization algorithm that aims at reducing the energy consumption of networks during such low-traffic periods. It uses the traffic steering capabilities of SR to remove traffic from as many links as possible to allow the respective hardware components to be switched off. Furthermore, it simultaneously ensures that solutions comply to additional operator requirements regarding the overall Maximum Link Utilization in the network. Based on data from a Tier-1 ISP and a public available dataset, we show that our approach allows for up to 70 % of the overall linecards to be switched off, corresponding to an around 56% reduction of the overall energy consumption of the network in times of low traffic demands. ",
    "url": "https://arxiv.org/abs/2306.00681",
    "authors": [
      "Daniel Otten",
      "Alexander Brundiers",
      "Timmy Sch\u00fcller",
      "Nils Aschenbruck"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2306.00687",
    "title": "Adversarial Robustness in Unsupervised Machine Learning: A Systematic  Review",
    "abstract": "As the adoption of machine learning models increases, ensuring robust models against adversarial attacks is increasingly important. With unsupervised machine learning gaining more attention, ensuring it is robust against attacks is vital. This paper conducts a systematic literature review on the robustness of unsupervised learning, collecting 86 papers. Our results show that most research focuses on privacy attacks, which have effective defenses; however, many attacks lack effective and general defensive measures. Based on the results, we formulate a model on the properties of an attack on unsupervised learning, contributing to future research by providing a model to use. ",
    "url": "https://arxiv.org/abs/2306.00687",
    "authors": [
      "Mathias Lundteigen Mohus",
      "Jinyue Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2306.00689",
    "title": "Stuttering Detection Using Speaker Representations and Self-supervised  Contextual Embeddings",
    "abstract": "The adoption of advanced deep learning architectures in stuttering detection (SD) tasks is challenging due to the limited size of the available datasets. To this end, this work introduces the application of speech embeddings extracted from pre-trained deep learning models trained on large audio datasets for different tasks. In particular, we explore audio representations obtained using emphasized channel attention, propagation, and aggregation time delay neural network (ECAPA-TDNN) and Wav2Vec2.0 models trained on VoxCeleb and LibriSpeech datasets respectively. After extracting the embeddings, we benchmark with several traditional classifiers, such as the K-nearest neighbour (KNN), Gaussian naive Bayes, and neural network, for the SD tasks. In comparison to the standard SD systems trained only on the limited SEP-28k dataset, we obtain a relative improvement of 12.08%, 28.71%, 37.9% in terms of unweighted average recall (UAR) over the baselines. Finally, we have shown that combining two embeddings and concatenating multiple layers of Wav2Vec2.0 can further improve the UAR by up to 2.60% and 6.32% respectively. ",
    "url": "https://arxiv.org/abs/2306.00689",
    "authors": [
      "Shakeel A. Sheikh",
      "Md Sahidullah",
      "Fabrice Hirsch",
      "Slim Ouni"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2306.00696",
    "title": "Analyzing the Internals of Neural Radiance Fields",
    "abstract": "Modern Neural Radiance Fields (NeRFs) learn a mapping from position to volumetric density via proposal network samplers. In contrast to the coarse-to-fine sampling approach with two NeRFs, this offers significant potential for speedups using lower network capacity as the task of mapping spatial coordinates to volumetric density involves no view-dependent effects and is thus much easier to learn. Given that most of the network capacity is utilized to estimate radiance, NeRFs could store valuable density information in their parameters or their deep features. To this end, we take one step back and analyze large, trained ReLU-MLPs used in coarse-to-fine sampling. We find that trained NeRFs, Mip-NeRFs and proposal network samplers map samples with high density to local minima along a ray in activation feature space. We show how these large MLPs can be accelerated by transforming the intermediate activations to a weight estimate, without any modifications to the parameters post-optimization. With our approach, we can reduce the computational requirements of trained NeRFs by up to 50% with only a slight hit in rendering quality and no changes to the training protocol or architecture. We evaluate our approach on a variety of architectures and datasets, showing that our proposition holds in various settings. ",
    "url": "https://arxiv.org/abs/2306.00696",
    "authors": [
      "Lukas Radl",
      "Andreas Kurz",
      "Markus Steinberger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2306.00698",
    "title": "Prediction of Post-Operative Renal and Pulmonary Complication Using  Transformers",
    "abstract": "Postoperative complications pose a significant challenge in the healthcare industry, resulting in elevated healthcare expenses and prolonged hospital stays, and in rare instances, patient mortality. To improve patient outcomes and reduce healthcare costs, healthcare providers rely on various perioperative risk scores to guide clinical decisions and prioritize care. In recent years, machine learning techniques have shown promise in predicting postoperative complications and fatality, with deep learning models achieving remarkable success in healthcare applications. However, research on the application of deep learning models to intra-operative anesthesia management data is limited. In this paper, we evaluate the performance of transformer-based models in predicting postoperative acute renal failure, postoperative pulmonary complications, and postoperative in-hospital mortality. We compare our method's performance with state-of-the-art tabular data prediction models, including gradient boosting trees and sequential attention models, on a clinical dataset. Our results demonstrate that transformer-based models can achieve superior performance in predicting postoperative complications and outperform traditional machine learning models. This work highlights the potential of deep learning techniques, specifically transformer-based models, in revolutionizing the healthcare industry's approach to postoperative care. ",
    "url": "https://arxiv.org/abs/2306.00698",
    "authors": [
      "Reza Shirkavand",
      "Fei Zhang",
      "Heng Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00704",
    "title": "DAM-Net: Global Flood Detection from SAR Imagery Using Differential  Attention Metric-Based Vision Transformers",
    "abstract": "The detection of flooded areas using high-resolution synthetic aperture radar (SAR) imagery is a critical task with applications in crisis and disaster management, as well as environmental resource planning. However, the complex nature of SAR images presents a challenge that often leads to an overestimation of the flood extent. To address this issue, we propose a novel differential attention metric-based network (DAM-Net) in this study. The DAM-Net comprises two key components: a weight-sharing Siamese backbone to obtain multi-scale change features of multi-temporal images and tokens containing high-level semantic information of water-body changes, and a temporal differential fusion (TDF) module that integrates semantic tokens and change features to generate flood maps with reduced speckle noise. Specifically, the backbone is split into multiple stages. In each stage, we design three modules, namely, temporal-wise feature extraction (TWFE), cross-temporal change attention (CTCA), and temporal-aware change enhancement (TACE), to effectively extract the change features. In TACE of the last stage, we introduce a class token to record high-level semantic information of water-body changes via the attention mechanism. Another challenge faced by data-driven deep learning algorithms is the limited availability of flood detection datasets. To overcome this, we have created the S1GFloods open-source dataset, a global-scale high-resolution Sentinel-1 SAR image pairs dataset covering 46 global flood events between 2015 and 2022. The experiments on the S1GFloods dataset using the proposed DAM-Net showed top results compared to state-of-the-art methods in terms of overall accuracy, F1-score, and IoU, which reached 97.8%, 96.5%, and 93.2%, respectively. Our dataset and code will be available online at https://github.com/Tamer-Saleh/S1GFlood-Detection. ",
    "url": "https://arxiv.org/abs/2306.00704",
    "authors": [
      "Tamer Saleh",
      "Xingxing Weng",
      "Shimaa Holail",
      "Chen Hao",
      "Gui-Song Xia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00707",
    "title": "Renormalized Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have become essential for studying complex data, particularly when represented as graphs. Their value is underpinned by their ability to reflect the intricacies of numerous areas, ranging from social to biological networks. GNNs can grapple with non-linear behaviors, emerging patterns, and complex connections; these are also typical characteristics of complex systems. The renormalization group (RG) theory has emerged as the language for studying complex systems. It is recognized as the preferred lens through which to study complex systems, offering a framework that can untangle their intricate dynamics. Despite the clear benefits of integrating RG theory with GNNs, no existing methods have ventured into this promising territory. This paper proposes a new approach that applies RG theory to devise a novel graph rewiring to improve GNNs' performance on graph-related tasks. We support our proposal with extensive experiments on standard benchmarks and baselines. The results demonstrate the effectiveness of our method and its potential to remedy the current limitations of GNNs. Finally, this paper marks the beginning of a new research direction. This path combines the theoretical foundations of RG, the magnifying glass of complex systems, with the structural capabilities of GNNs. By doing so, we aim to enhance the potential of GNNs in modeling and unraveling the complexities inherent in diverse systems. ",
    "url": "https://arxiv.org/abs/2306.00707",
    "authors": [
      "Francesco Caso",
      "Giovanni Trappolini",
      "Andrea Bacciu",
      "Pietro Li\u00f2",
      "Fabrizio Silvestri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2306.00709",
    "title": "Understanding the Social Context of Eating with Multimodal Smartphone  Sensing: The Role of Country Diversity",
    "abstract": "Understanding the social context of eating is crucial for promoting healthy eating behaviors by providing timely interventions. Multimodal smartphone sensing data has the potential to provide valuable insights into eating behavior, particularly in mobile food diaries and mobile health applications. However, research on the social context of eating with smartphone sensor data is limited, despite extensive study in nutrition and behavioral science. Moreover, the impact of country differences on the social context of eating, as measured by multimodal phone sensor data and self-reports, remains under-explored. To address this research gap, we present a study using a smartphone sensing dataset from eight countries (China, Denmark, India, Italy, Mexico, Mongolia, Paraguay, and the UK). Our study focuses on a set of approximately 24K self-reports on eating events provided by 678 college students to investigate the country diversity that emerges from smartphone sensors during eating events for different social contexts (alone or with others). Our analysis revealed that while some smartphone usage features during eating events were similar across countries, others exhibited unique behaviors in each country. We further studied how user and country-specific factors impact social context inference by developing machine learning models with population-level (non-personalized) and hybrid (partially personalized) experimental setups. We showed that models based on the hybrid approach achieve AUC scores up to 0.75 with XGBoost models. These findings have implications for future research on mobile food diaries and mobile health sensing systems, emphasizing the importance of considering country differences in building and deploying machine learning models to minimize biases and improve generalization across different populations. ",
    "url": "https://arxiv.org/abs/2306.00709",
    "authors": [
      "Nathan Kammoun",
      "Lakmal Meegahapola",
      "Daniel Gatica-Perez"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2306.00717",
    "title": "Graph Neural Networks-Based User Pairing in Wireless Communication  Systems",
    "abstract": "Recently, deep neural networks have emerged as a solution to solve NP-hard wireless resource allocation problems in real-time. However, multi-layer perceptron (MLP) and convolutional neural network (CNN) structures, which are inherited from image processing tasks, are not optimized for wireless network problems. As network size increases, these methods get harder to train and generalize. User pairing is one such essential NP-hard optimization problem in wireless communication systems that entails selecting users to be scheduled together while minimizing interference and maximizing throughput. In this paper, we propose an unsupervised graph neural network (GNN) approach to efficiently solve the user pairing problem. Our proposed method utilizes the Erdos goes neural pipeline to significantly outperform other scheduling methods such as k-means and semi-orthogonal user scheduling (SUS). At 20 dB SNR, our proposed approach achieves a 49% better sum rate than k-means and a staggering 95% better sum rate than SUS while consuming minimal time and resources. The scalability of the proposed method is also explored as our model can handle dynamic changes in network size without experiencing a substantial decrease in performance. Moreover, our model can accomplish this without being explicitly trained for larger or smaller networks facilitating a dynamic functionality that cannot be achieved using CNNs or MLPs. ",
    "url": "https://arxiv.org/abs/2306.00717",
    "authors": [
      "Sharan Mourya",
      "Pavan Reddy",
      "SaiDhiraj Amuru",
      "Kiran Kumar Kuchi"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2306.00720",
    "title": "Neural Bee Colony Optimization: A Case Study in Public Transit Network  Design",
    "abstract": "In this work we explore the combination of metaheuristics and learned neural network solvers for combinatorial optimization. We do this in the context of the transit network design problem, a uniquely challenging combinatorial optimization problem with real-world importance. We train a neural network policy to perform single-shot planning of individual transit routes, and then incorporate it as one of several sub-heuristics in a modified Bee Colony Optimization (BCO) metaheuristic algorithm. Our experimental results demonstrate that this hybrid algorithm outperforms the learned policy alone by up to 20% and the original BCO algorithm by up to 53% on realistic problem instances. We perform a set of ablations to study the impact of each component of the modified algorithm. ",
    "url": "https://arxiv.org/abs/2306.00720",
    "authors": [
      "Andrew Holliday",
      "Gregory Dudek"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00753",
    "title": "Robust T-Loss for Medical Image Segmentation",
    "abstract": "This paper presents a new robust loss function, the T-Loss, for medical image segmentation. The proposed loss is based on the negative log-likelihood of the Student-t distribution and can effectively handle outliers in the data by controlling its sensitivity with a single parameter. This parameter is updated during the backpropagation process, eliminating the need for additional computation or prior information about the level and spread of noisy labels. Our experiments show that the T-Loss outperforms traditional loss functions in terms of dice scores on two public medical datasets for skin lesion and lung segmentation. We also demonstrate the ability of T-Loss to handle different types of simulated label noise, resembling human error. Our results provide strong evidence that the T-Loss is a promising alternative for medical image segmentation where high levels of noise or outliers in the dataset are a typical phenomenon in practice. The project website can be found at https://robust-tloss.github.io ",
    "url": "https://arxiv.org/abs/2306.00753",
    "authors": [
      "Alvaro Gonzalez-Jimenez",
      "Simone Lionetti",
      "Philippe Gottfrois",
      "Fabian Gr\u00f6ger",
      "Marc Pouly",
      "Alexander Navarini"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00757",
    "title": "AI Chain on Large Language Model for Unsupervised Control Flow Graph  Generation for Statically-Typed Partial Code",
    "abstract": "Control Flow Graphs (CFGs) are essential for visualizing, understanding and analyzing program behavior. For statically-typed programming language like Java, developers obtain CFGs by using bytecode-based methods for compilable code and Abstract Syntax Tree (AST)-based methods for partially uncompilable code. However, explicit syntax errors during AST construction and implicit semantic errors caused by bad coding practices can lead to behavioral loss and deviation of CFGs.To address the issue, we propose a novel approach that leverages the error-tolerant and understanding ability of pre-trained Large Language Models (LLMs) to generate CFGs. Our approach involves a Chain of Thought (CoT) with four steps: structure hierarchy extraction, nested code block extraction, CFG generation of nested code blocks, and fusion of all nested code blocks' CFGs. To address the limitations of the original CoT's single-prompt approach (i.e., completing all steps in a single generative pass), which can result in an ``epic'' prompt with hard-to-control behavior and error accumulation, we break down the CoT into an AI chain with explicit sub-steps. Each sub-step corresponds to a separate AI-unit, with an effective prompt assigned to each unit for interacting with LLMs to accomplish a specific purpose.Our experiments confirmed that our method outperforms existing CFG tools in terms of node and edge coverage, especially for incomplete or erroneous code. We also conducted an ablation experiment and confirmed the effectiveness of AI chain design principles: Hierarchical Task Breakdown, Unit Composition, and Mix of AI Units and Non-AI Units.Our work opens up new possibilities for building foundational software engineering tools based on LLMs, as opposed to traditional program analysis methods. ",
    "url": "https://arxiv.org/abs/2306.00757",
    "authors": [
      "Qing Huang",
      "Zhou Zou",
      "Zhenchang Xing",
      "Zhenkang Zuo",
      "Xiwei Xu",
      "Qinghua Lu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2306.00765",
    "title": "Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection",
    "abstract": "Stance Detection is concerned with identifying the attitudes expressed by an author towards a target of interest. This task spans a variety of domains ranging from social media opinion identification to detecting the stance for a legal claim. However, the framing of the task varies within these domains, in terms of the data collection protocol, the label dictionary and the number of available annotations. Furthermore, these stance annotations are significantly imbalanced on a per-topic and inter-topic basis. These make multi-domain stance detection a challenging task, requiring standardization and domain adaptation. To overcome this challenge, we propose $\\textbf{T}$opic $\\textbf{E}$fficient $\\textbf{St}$anc$\\textbf{E}$ $\\textbf{D}$etection (TESTED), consisting of a topic-guided diversity sampling technique and a contrastive objective that is used for fine-tuning a stance classifier. We evaluate the method on an existing benchmark of $16$ datasets with in-domain, i.e. all topics seen and out-of-domain, i.e. unseen topics, experiments. The results show that our method outperforms the state-of-the-art with an average of $3.5$ F1 points increase in-domain, and is more generalizable with an averaged increase of $10.2$ F1 on out-of-domain evaluation while using $\\leq10\\%$ of the training data. We show that our sampling technique mitigates both inter- and per-topic class imbalances. Finally, our analysis demonstrates that the contrastive learning objective allows the model a more pronounced segmentation of samples with varying labels. ",
    "url": "https://arxiv.org/abs/2306.00765",
    "authors": [
      "Erik Arakelyan",
      "Arnav Arora",
      "Isabelle Augenstein"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00766",
    "title": "Impact of using a privacy model on smart buildings data for CO2  prediction",
    "abstract": "There is a constant trade-off between the utility of the data collected and processed by the many systems forming the Internet of Things (IoT) revolution and the privacy concerns of the users living in the spaces hosting these sensors. Privacy models, such as the SITA (Spatial, Identity, Temporal, and Activity) model, can help address this trade-off. In this paper, we focus on the problem of $CO_2$ prediction, which is crucial for health monitoring but can be used to monitor occupancy, which might reveal some private information. We apply a number of transformations on a real dataset from a Smart Building to simulate different SITA configurations on the collected data. We use the transformed data with multiple Machine Learning (ML) techniques to analyse the performance of the models to predict $CO_{2}$ levels. Our results show that, for different algorithms, different SITA configurations do not make one algorithm perform better or worse than others, compared to the baseline data; also, in our experiments, the temporal dimension was particularly sensitive, with scores decreasing up to $18.9\\%$ between the original and the transformed data. The results can be useful to show the effect of different levels of data privacy on the data utility of IoT applications, and can also help to identify which parameters are more relevant for those systems so that higher privacy settings can be adopted while data utility is still preserved. ",
    "url": "https://arxiv.org/abs/2306.00766",
    "authors": [
      "Marlon P. da Silva",
      "Henry C. Nunes",
      "Charles V. Neu",
      "Luana T. Thomas",
      "Avelino F. Zorzo",
      "Charles Morisset"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2306.00788",
    "title": "Understanding Augmentation-based Self-Supervised Representation Learning  via RKHS Approximation",
    "abstract": "Good data augmentation is one of the key factors that lead to the empirical success of self-supervised representation learning such as contrastive learning and masked language modeling, yet theoretical understanding of its role in learning good representations remains limited. Recent work has built the connection between self-supervised learning and approximating the top eigenspace of a graph Laplacian operator. Learning a linear probe on top of such features can naturally be connected to RKHS regression. In this work, we use this insight to perform a statistical analysis of augmentation-based pretraining. We start from the isometry property, a key geometric characterization of the target function given by the augmentation. Our first main theorem provides, for an arbitrary encoder, near tight bounds for both the estimation error incurred by fitting the linear probe on top of the encoder, and the approximation error entailed by the fitness of the RKHS the encoder learns. Our second main theorem specifically addresses the case where the encoder extracts the top-d eigenspace of a Monte-Carlo approximation of the underlying kernel with the finite pretraining samples. Our analysis completely disentangles the effects of the model and the augmentation. A key ingredient in our analysis is the augmentation complexity, which we use to quantitatively compare different augmentations and analyze their impact on downstream performance on synthetic and real datasets. ",
    "url": "https://arxiv.org/abs/2306.00788",
    "authors": [
      "Runtian Zhai",
      "Bingbin Liu",
      "Andrej Risteski",
      "Zico Kolter",
      "Pradeep Ravikumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00794",
    "title": "SlothSpeech: Denial-of-service Attack Against Speech Recognition Models",
    "abstract": "Deep Learning (DL) models have been popular nowadays to execute different speech-related tasks, including automatic speech recognition (ASR). As ASR is being used in different real-time scenarios, it is important that the ASR model remains efficient against minor perturbations to the input. Hence, evaluating efficiency robustness of the ASR model is the need of the hour. We show that popular ASR models like Speech2Text model and Whisper model have dynamic computation based on different inputs, causing dynamic efficiency. In this work, we propose SlothSpeech, a denial-of-service attack against ASR models, which exploits the dynamic behaviour of the model. SlothSpeech uses the probability distribution of the output text tokens to generate perturbations to the audio such that efficiency of the ASR model is decreased. We find that SlothSpeech generated inputs can increase the latency up to 40X times the latency induced by benign input. ",
    "url": "https://arxiv.org/abs/2306.00794",
    "authors": [
      "Mirazul Haque",
      "Rutvij Shah",
      "Simin Chen",
      "Berrak \u015ei\u015fman",
      "Cong Liu",
      "Wei Yang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2306.00809",
    "title": "Initial Guessing Bias: How Untrained Networks Favor Some Classes",
    "abstract": "The initial state of neural networks plays a central role in conditioning the subsequent training dynamics. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a neural network can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We show that the presence of this phenomenon, which we call \"Initial Guessing Bias\" (IGB), depends on architectural choices such as activation functions, max-pooling layers, and network depth. Our analysis of IGB has practical consequences, in that it guides architecture selection and initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging, the validity of some mean-field approximations, and the non-trivial differences arising with depth. ",
    "url": "https://arxiv.org/abs/2306.00809",
    "authors": [
      "Emanuele Francazi",
      "Aurelien Lucchi",
      "Marco Baity-Jesi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00814",
    "title": "Vocos: Closing the gap between time-domain and Fourier-based neural  vocoders for high-quality audio synthesis",
    "abstract": "Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that addresses the key challenges of modeling spectral coefficients. Vocos demonstrates improved computational efficiency, achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches. As shown by objective evaluation, Vocos not only matches state-of-the-art audio quality, but thanks to frequency-aware generator, also effectively mitigates the periodicity issues frequently associated with time-domain GANs. The source code and model weights have been open-sourced at https://github.com/charactr-platform/vocos. ",
    "url": "https://arxiv.org/abs/2306.00814",
    "authors": [
      "Hubert Siuzdak"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2306.00816",
    "title": "Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and  Compatible Triggers",
    "abstract": "Deep neural networks (DNNs) can be manipulated to exhibit specific behaviors when exposed to specific trigger patterns, without affecting their performance on normal samples. This type of attack is known as a backdoor attack. Recent research has focused on designing invisible triggers for backdoor attacks to ensure visual stealthiness. These triggers have demonstrated strong attack performance even under backdoor defense, which aims to eliminate or suppress the backdoor effect in the model. However, through experimental observations, we have noticed that these carefully designed invisible triggers are often susceptible to visual distortion during inference, such as Gaussian blurring or environmental variations in real-world scenarios. This phenomenon significantly undermines the effectiveness of attacks in practical applications. Unfortunately, this issue has not received sufficient attention and has not been thoroughly investigated. To address this limitation, we propose a novel approach called the Visible, Semantic, Sample-Specific, and Compatible trigger (VSSC-trigger), which leverages a recent powerful image method known as the stable diffusion model. In this approach, a text trigger is utilized as a prompt and combined with a benign image. The resulting combination is then processed by a pre-trained stable diffusion model, generating a corresponding semantic object. This object is seamlessly integrated with the original image, resulting in a new realistic image, referred to as the poisoned image. Extensive experimental results and analysis validate the effectiveness and robustness of our proposed attack method, even in the presence of visual distortion. We believe that the new trigger proposed in this work, along with the proposed idea to address the aforementioned issues, will have significant prospective implications for further advancements in this direction. ",
    "url": "https://arxiv.org/abs/2306.00816",
    "authors": [
      "Ruotong Wang",
      "Hongrui Chen",
      "Zihao Zhu",
      "Li Liu",
      "Yong Zhang",
      "Yanbo Fan",
      "Baoyuan Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2306.00826",
    "title": "In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation",
    "abstract": "Out-of-distribution (OOD) detection is the problem of identifying inputs which are unrelated to the in-distribution task. The OOD detection performance when the in-distribution (ID) is ImageNet-1K is commonly being tested on a small range of test OOD datasets. We find that most of the currently used test OOD datasets, including datasets from the open set recognition (OSR) literature, have severe issues: In some cases more than 50$\\%$ of the dataset contains objects belonging to one of the ID classes. These erroneous samples heavily distort the evaluation of OOD detectors. As a solution, we introduce with NINCO a novel test OOD dataset, each sample checked to be ID free, which with its fine-grained range of OOD classes allows for a detailed analysis of an OOD detector's strengths and failure modes, particularly when paired with a number of synthetic \"OOD unit-tests\". We provide detailed evaluations across a large set of architectures and OOD detection methods on NINCO and the unit-tests, revealing new insights about model weaknesses and the effects of pretraining on OOD detection performance. We provide code and data at https://github.com/j-cb/NINCO. ",
    "url": "https://arxiv.org/abs/2306.00826",
    "authors": [
      "Julian Bitterwolf",
      "Maximilian M\u00fcller",
      "Matthias Hein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00833",
    "title": "When Does Bottom-up Beat Top-down in Hierarchical Community Detection?",
    "abstract": "Hierarchical clustering of networks consists in finding a tree of communities, such that lower levels of the hierarchy reveal finer-grained community structures. There are two main classes of algorithms tackling this problem. Divisive ($\\textit{top-down}$) algorithms recursively partition the nodes into two communities, until a stopping rule indicates that no further split is needed. In contrast, agglomerative ($\\textit{bottom-up}$) algorithms first identify the smallest community structure and then repeatedly merge the communities using a $\\textit{linkage}$ method. In this article, we establish theoretical guarantees for the recovery of the hierarchical tree and community structure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We also establish that this bottom-up algorithm attains the information-theoretic threshold for exact recovery at intermediate levels of the hierarchy. Notably, these recovery conditions are less restrictive compared to those existing for top-down algorithms. This shows that bottom-up algorithms extend the feasible region for achieving exact recovery at intermediate levels. Numerical experiments on both synthetic and real data sets confirm the superiority of bottom-up algorithms over top-down algorithms. We also observe that top-down algorithms can produce dendrograms with inversions. These findings contribute to a better understanding of hierarchical clustering techniques and their applications in network analysis. ",
    "url": "https://arxiv.org/abs/2306.00833",
    "authors": [
      "Maximilien Dreveton",
      "Daichi Kuroda",
      "Matthias Grossglauser",
      "Patrick Thiran"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00834",
    "title": "Deformable Convolutions and LSTM-based Flexible Event Frame Fusion  Network for Motion Deblurring",
    "abstract": "Event cameras differ from conventional RGB cameras in that they produce asynchronous data sequences. While RGB cameras capture every frame at a fixed rate, event cameras only capture changes in the scene, resulting in sparse and asynchronous data output. Despite the fact that event data carries useful information that can be utilized in motion deblurring of RGB cameras, integrating event and image information remains a challenge. Recent state-of-the-art CNN-based deblurring solutions produce multiple 2-D event frames based on the accumulation of event data over a time period. In most of these techniques, however, the number of event frames is fixed and predefined, which reduces temporal resolution drastically, particularly for scenarios when fast-moving objects are present or when longer exposure times are required. It is also important to note that recent modern cameras (e.g., cameras in mobile phones) dynamically set the exposure time of the image, which presents an additional problem for networks developed for a fixed number of event frames. A Long Short-Term Memory (LSTM)-based event feature extraction module has been developed for addressing these challenges, which enables us to use a dynamically varying number of event frames. Using these modules, we constructed a state-of-the-art deblurring network, Deformable Convolutions and LSTM-based Flexible Event Frame Fusion Network (DLEFNet). It is particularly useful for scenarios in which exposure times vary depending on factors such as lighting conditions or the presence of fast-moving objects in the scene. It has been demonstrated through evaluation results that the proposed method can outperform the existing state-of-the-art networks for deblurring task in synthetic and real-world data sets. ",
    "url": "https://arxiv.org/abs/2306.00834",
    "authors": [
      "Dan Yang",
      "Mehmet Yamac"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00858",
    "title": "Adversarial learning of neural user simulators for dialogue policy  optimisation",
    "abstract": "Reinforcement learning based dialogue policies are typically trained in interaction with a user simulator. To obtain an effective and robust policy, this simulator should generate user behaviour that is both realistic and varied. Current data-driven simulators are trained to accurately model the user behaviour in a dialogue corpus. We propose an alternative method using adversarial learning, with the aim to simulate realistic user behaviour with more variation. We train and evaluate several simulators on a corpus of restaurant search dialogues, and then use them to train dialogue system policies. In policy cross-evaluation experiments we demonstrate that an adversarially trained simulator produces policies with 8.3% higher success rate than those trained with a maximum likelihood simulator. Subjective results from a crowd-sourced dialogue system user evaluation confirm the effectiveness of adversarially training user simulators. ",
    "url": "https://arxiv.org/abs/2306.00858",
    "authors": [
      "Simon Keizer",
      "Caroline Dockes",
      "Norbert Braunschweiler",
      "Svetlana Stoyanchev",
      "Rama Doddipatla"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2306.00863",
    "title": "DeepFake-Adapter: Dual-Level Adapter for DeepFake Detection",
    "abstract": "Existing deepfake detection methods fail to generalize well to unseen or degraded samples, which can be attributed to the over-fitting of low-level forgery patterns. Here we argue that high-level semantics are also indispensable recipes for generalizable forgery detection. Recently, large pre-trained Vision Transformers (ViTs) have shown promising generalization capability. In this paper, we propose the first parameter-efficient tuning approach for deepfake detection, namely DeepFake-Adapter, to effectively and efficiently adapt the generalizable high-level semantics from large pre-trained ViTs to aid deepfake detection. Given large pre-trained models but limited deepfake data, DeepFake-Adapter introduces lightweight yet dedicated dual-level adapter modules to a ViT while keeping the model backbone frozen. Specifically, to guide the adaptation process to be aware of both global and local forgery cues of deepfake data, 1) we not only insert Globally-aware Bottleneck Adapters in parallel to MLP layers of ViT, 2) but also actively cross-attend Locally-aware Spatial Adapters with features from ViT. Unlike existing deepfake detection methods merely focusing on low-level forgery patterns, the forgery detection process of our model can be regularized by generalizable high-level semantics from a pre-trained ViT and adapted by global and local low-level forgeries of deepfake data. Extensive experiments on several standard deepfake detection benchmarks validate the effectiveness of our approach. Notably, DeepFake-Adapter demonstrates a convincing advantage under cross-dataset and cross-manipulation settings. The source code is released at https://github.com/rshaojimmy/DeepFake-Adapter ",
    "url": "https://arxiv.org/abs/2306.00863",
    "authors": [
      "Rui Shao",
      "Tianxing Wu",
      "Liqiang Nie",
      "Ziwei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00876",
    "title": "Quantifying Deep Learning Model Uncertainty in Conformal Prediction",
    "abstract": "Precise estimation of predictive uncertainty in deep neural networks is a critical requirement for reliable decision-making in machine learning and statistical modeling, particularly in the context of medical AI. Conformal Prediction (CP) has emerged as a promising framework for representing the model uncertainty by providing well-calibrated confidence levels for individual predictions. However, the quantification of model uncertainty in conformal prediction remains an active research area, yet to be fully addressed. In this paper, we explore state-of-the-art CP methodologies and their theoretical foundations. We propose a probabilistic approach in quantifying the model uncertainty derived from the produced prediction sets in conformal prediction and provide certified boundaries for the computed uncertainty. By doing so, we allow model uncertainty measured by CP to be compared by other uncertainty quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and Evidential approaches. ",
    "url": "https://arxiv.org/abs/2306.00876",
    "authors": [
      "Hamed Karimi",
      "Reza Samavi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.00893",
    "title": "Efficient Temporal Butterfly Counting and Enumeration on Temporal  Bipartite Graphs",
    "abstract": "Bipartite graphs model relationships between two different sets of entities, like actor-movie, user-item, and author-paper. The butterfly, a 4-vertices 4-edges $2\\times 2$ bi-clique, is the simplest cohesive motif in a bipartite graph and is the fundamental component of higher-order substructures. Counting and enumerating the butterflies offer significant benefits across various applications, including fraud detection, graph embedding, and community search. While the corresponding motif, the triangle, in the unipartite graphs has been widely studied in both static and temporal settings, the extension of butterfly to temporal bipartite graphs remains unexplored. In this paper, we investigate the temporal butterfly counting and enumeration problem: count and enumerate the butterflies whose edges establish following a certain order within a given duration. Towards efficient computation, we devise a non-trivial baseline rooted in the state-of-the-art butterfly counting algorithm on static graphs, further, explore the intrinsic property of the temporal butterfly, and optimize the process with a compact data structure and smart pruning strategies. The time complexity is proved to be significantly reduced without compromising on space efficiency. In addition, we generalize our algorithms to practical streaming settings and multi-core computing architectures. Our extensive experiments on 11 large-scale real-world datasets demonstrate the efficiency and scalability of our solutions. ",
    "url": "https://arxiv.org/abs/2306.00893",
    "authors": [
      "Xinwei Cai",
      "Xiangyu Ke",
      "Kai Wang",
      "Lu Chen",
      "Tianming Zhang",
      "Qing Liu",
      "Yunjun Gao"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2306.00899",
    "title": "SpotTarget: Rethinking the Effect of Target Edges for Link Prediction in  Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated promising outcomes across various tasks, including node classification and link prediction. Despite their remarkable success in various high-impact applications, we have identified three common pitfalls in message passing for link prediction. Particularly, in prevalent GNN frameworks (e.g., DGL and PyTorch-Geometric), the target edges (i.e., the edges being predicted) consistently exist as message passing edges in the graph during training. Consequently, this results in overfitting and distribution shift, both of which adversely impact the generalizability to test the target edges. Additionally, during test time, the failure to exclude the test target edges leads to implicit test leakage caused by neighborhood aggregation. In this paper, we analyze these three pitfalls and investigate the impact of including or excluding target edges on the performance of nodes with varying degrees during training and test phases. Our theoretical and empirical analysis demonstrates that low-degree nodes are more susceptible to these pitfalls. These pitfalls can have detrimental consequences when GNNs are implemented in production systems. To systematically address these pitfalls, we propose SpotTarget, an effective and efficient GNN training framework. During training, SpotTarget leverages our insight regarding low-degree nodes and excludes train target edges connected to at least one low-degree node. During test time, it emulates real-world scenarios of GNN usage in production and excludes all test target edges. Our experiments conducted on diverse real-world datasets, demonstrate that SpotTarget significantly enhances GNNs, achieving up to a 15x increase in accuracy in sparse graphs. Furthermore, SpotTarget consistently and dramatically improves the performance for low-degree nodes in dense graphs. ",
    "url": "https://arxiv.org/abs/2306.00899",
    "authors": [
      "Jing Zhu",
      "Yuhang Zhou",
      "Vassilis N. Ioannidis",
      "Shengyi Qian",
      "Wei Ai",
      "Xiang Song",
      "Danai Koutra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2306.00919",
    "title": "Understanding Social Context from Smartphone Sensing: Generalization  Across Countries and Daily Life Moments",
    "abstract": "Understanding and longitudinally tracking the social context of people help in understanding their behavior and mental well-being better. Hence, instead of burdensome questionnaires, some studies used passive smartphone sensors to infer social context with machine learning models. However, the few studies that have been done up to date have focused on unique, situated contexts (i.e., when eating or drinking) in one or two countries, hence limiting the understanding of the inference in terms of generalization to (i) everyday life occasions and (ii) different countries. In this paper, we used a novel, large-scale, and multimodal smartphone sensing dataset with over 216K self-reports collected from over 580 participants in five countries (Mongolia, Italy, Denmark, UK, Paraguay), first to understand whether social context inference (i.e., alone or not) is feasible with sensor data, and then, to know how behavioral and country-level diversity affects the inference. We found that (i) sensor features from modalities such as activity, location, app usage, Bluetooth, and WiFi could be informative of social context; (ii) partially personalized multi-country models (trained and tested with data from all countries) and country-specific models (trained and tested within countries) achieved similar accuracies in the range of 80%-90%; and (iii) models do not generalize well to unseen countries regardless of geographic similarity. ",
    "url": "https://arxiv.org/abs/2306.00919",
    "authors": [
      "Aurel Ruben Mader",
      "Lakmal Meegahapola",
      "Daniel Gatica-Perez"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2306.00928",
    "title": "ACLM: A Selective-Denoising based Generative Data Augmentation Approach  for Low-Resource Complex NER",
    "abstract": "Complex Named Entity Recognition (NER) is the task of detecting linguistically complex named entities in low-context text. In this paper, we present ACLM Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach based on conditional generation to address the data scarcity problem in low-resource complex NER. ACLM alleviates the context-entity mismatch issue, a problem existing NER data augmentation techniques suffer from and often generates incoherent augmentations by placing complex named entities in the wrong context. ACLM builds on BART and is optimized on a novel text reconstruction or denoising task - we use selective masking (aided by attention maps) to retain the named entities and certain keywords in the input sentence that provide contextually relevant additional knowledge or hints about the named entities. Compared with other data augmentation strategies, ACLM can generate more diverse and coherent augmentations preserving the true word sense of complex entities in the sentence. We demonstrate the effectiveness of ACLM both qualitatively and quantitatively on monolingual, cross-lingual, and multilingual complex NER across various low-resource settings. ACLM outperforms all our neural baselines by a significant margin (1%-36%). In addition, we demonstrate the application of ACLM to other domains that suffer from data scarcity (e.g., biomedical). In practice, ACLM generates more effective and factual augmentations for these domains than prior methods. Code: https://github.com/Sreyan88/ACLM ",
    "url": "https://arxiv.org/abs/2306.00928",
    "authors": [
      "Sreyan Ghosh",
      "Utkarsh Tyagi",
      "Manan Suri",
      "Sonal Kumar",
      "S Ramaneswaran",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2306.00934",
    "title": "Interpreting GNN-based IDS Detections Using Provenance Graph Structural  Features",
    "abstract": "The black-box nature of complex Neural Network (NN)-based models has hindered their widespread adoption in security domains due to the lack of logical explanations and actionable follow-ups for their predictions. To enhance the transparency and accountability of Graph Neural Network (GNN) security models used in system provenance analysis, we propose PROVEXPLAINER, a framework for projecting abstract GNN decision boundaries onto interpretable feature spaces. We first replicate the decision-making process of GNNbased security models using simpler and explainable models such as Decision Trees (DTs). To maximize the accuracy and fidelity of the surrogate models, we propose novel graph structural features founded on classical graph theory and enhanced by extensive data study with security domain knowledge. Our graph structural features are closely tied to problem-space actions in the system provenance domain, which allows the detection results to be explained in descriptive, human language. PROVEXPLAINER allowed simple DT models to achieve 95% fidelity to the GNN on program classification tasks with general graph structural features, and 99% fidelity on malware detection tasks with a task-specific feature package tailored for direct interpretation. The explanations for malware classification are demonstrated with case studies of five real-world malware samples across three malware families. ",
    "url": "https://arxiv.org/abs/2306.00934",
    "authors": [
      "Kunal Mukherjee",
      "Joshua Wiedemeier",
      "Tianhao Wang",
      "Muhyun Kim",
      "Feng Chen",
      "Murat Kantarcioglu",
      "Kangkook Jee"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00936",
    "title": "AMR4NLI: Interpretable and robust NLI measures from semantic graphs",
    "abstract": "The task of natural language inference (NLI) asks whether a given premise (expressed in NL) entails a given NL hypothesis. NLI benchmarks contain human ratings of entailment, but the meaning relationships driving these ratings are not formalized. Can the underlying sentence pair relationships be made more explicit in an interpretable yet robust fashion? We compare semantic structures to represent premise and hypothesis, including sets of contextualized embeddings and semantic graphs (Abstract Meaning Representations), and measure whether the hypothesis is a semantic substructure of the premise, utilizing interpretable metrics. Our evaluation on three English benchmarks finds value in both contextualized embeddings and semantic graphs; moreover, they provide complementary signals, and can be leveraged together in a hybrid model. ",
    "url": "https://arxiv.org/abs/2306.00936",
    "authors": [
      "Juri Opitz",
      "Shira Wein",
      "Julius Steen",
      "Anette Frank",
      "Nathan Schneider"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2306.00956",
    "title": "The ObjectFolder Benchmark: Multisensory Learning with Neural and Real  Objects",
    "abstract": "We introduce the ObjectFolder Benchmark, a benchmark suite of 10 tasks for multisensory object-centric learning, centered around object recognition, reconstruction, and manipulation with sight, sound, and touch. We also introduce the ObjectFolder Real dataset, including the multisensory measurements for 100 real-world household objects, building upon a newly designed pipeline for collecting the 3D meshes, videos, impact sounds, and tactile readings of real-world objects. We conduct systematic benchmarking on both the 1,000 multisensory neural objects from ObjectFolder, and the real multisensory data from ObjectFolder Real. Our results demonstrate the importance of multisensory perception and reveal the respective roles of vision, audio, and touch for different object-centric learning tasks. By publicly releasing our dataset and benchmark suite, we hope to catalyze and enable new research in multisensory object-centric learning in computer vision, robotics, and beyond. Project page: https://objectfolder.stanford.edu ",
    "url": "https://arxiv.org/abs/2306.00956",
    "authors": [
      "Ruohan Gao",
      "Yiming Dou",
      "Hao Li",
      "Tanmay Agarwal",
      "Jeannette Bohg",
      "Yunzhu Li",
      "Li Fei-Fei",
      "Jiajun Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2306.00984",
    "title": "StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual  Representation Learners",
    "abstract": "We investigate the potential of learning visual representations using synthetic images generated by text-to-image models. This is a natural question in the light of the excellent performance of such models in generating high-quality images. We consider specifically the Stable Diffusion, one of the leading open source text-to-image models. We show that (1) when the generative model is configured with proper classifier-free guidance scale, training self-supervised methods on synthetic images can match or beat the real image counterpart; (2) by treating the multiple images generated from the same text prompt as positives for each other, we develop a multi-positive contrastive learning method, which we call StableRep. With solely synthetic images, the representations learned by StableRep surpass the performance of representations learned by SimCLR and CLIP using the same set of text prompts and corresponding real images, on large scale datasets. When we further add language supervision, StableRep trained with 20M synthetic images achieves better accuracy than CLIP trained with 50M real images. ",
    "url": "https://arxiv.org/abs/2306.00984",
    "authors": [
      "Yonglong Tian",
      "Lijie Fan",
      "Phillip Isola",
      "Huiwen Chang",
      "Dilip Krishnan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:1902.07987",
    "title": "Learning representations of irregular particle-detector geometry with  distance-weighted graph networks",
    "abstract": "We explore the use of graph networks to deal with irregular-geometry detectors in the context of particle reconstruction. Thanks to their representation-learning capabilities, graph networks can exploit the full detector granularity, while natively managing the event sparsity and arbitrarily complex detector geometries. We introduce two distance-weighted graph network architectures, dubbed GarNet and GravNet layers, and apply them to a typical particle reconstruction task. The performance of the new architectures is evaluated on a data set of simulated particle interactions on a toy model of a highly granular calorimeter, loosely inspired by the endcap calorimeter to be installed in the CMS detector for the High-Luminosity LHC phase. We study the clustering of energy depositions, which is the basis for calorimetric particle reconstruction, and provide a quantitative comparison to alternative approaches. The proposed algorithms provide an interesting alternative to existing methods, offering equally performing or less resource-demanding solutions with less underlying assumptions on the detector geometry and, consequently, the possibility to generalize to other detectors. ",
    "url": "https://arxiv.org/abs/1902.07987",
    "authors": [
      "Shah Rukh Qasim",
      "Jan Kieseler",
      "Yutaro Iiyama",
      "Maurizio Pierini"
    ],
    "subjectives": [
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00041",
    "title": "Research And Implementation Of Drug Target Interaction Confidence  Measurement Method Based On Causal Intervention",
    "abstract": "The identification and discovery of drug-target Interaction (DTI) is an important step in the field of Drug research and development, which can help scientists discover new drugs and accelerate the development process. KnowledgeGraph and the related knowledge graph Embedding (KGE) model develop rapidly and show good performance in the field of drug discovery in recent years. In the task of drug target identification, the lack of authenticity and accuracy of the model will lead to the increase of misjudgment rate and the low efficiency of drug development. To solve the above problems, this study focused on the problem of drug target link prediction with knowledge mapping as the core technology, and adopted the confidence measurement method based on causal intervention to measure the triplet score, so as to improve the accuracy of drug target interaction prediction model. By comparing with the traditional Softmax and Sigmod confidence measurement methods on different KGE models, the results show that the confidence measurement method based on causal intervention can effectively improve the accuracy of DTI link prediction, especially for high-precision models. The predicted results are more conducive to guiding the design and development of followup experiments of drug development, so as to improve the efficiency of drug development. ",
    "url": "https://arxiv.org/abs/2306.00041",
    "authors": [
      "Wenting Ye",
      "Bowen Wang",
      "Yang Xie",
      "Debo Cheng",
      "Zaiwen Feng"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00091",
    "title": "A General Framework for Equivariant Neural Networks on Reductive Lie  Groups",
    "abstract": "Reductive Lie Groups, such as the orthogonal groups, the Lorentz group, or the unitary groups, play essential roles across scientific fields as diverse as high energy physics, quantum mechanics, quantum chromodynamics, molecular dynamics, computer vision, and imaging. In this paper, we present a general Equivariant Neural Network architecture capable of respecting the symmetries of the finite-dimensional representations of any reductive Lie Group G. Our approach generalizes the successful ACE and MACE architectures for atomistic point clouds to any data equivariant to a reductive Lie group action. We also introduce the lie-nn software library, which provides all the necessary tools to develop and implement such general G-equivariant neural networks. It implements routines for the reduction of generic tensor products of representations into irreducible representations, making it easy to apply our architecture to a wide range of problems and groups. The generality and performance of our approach are demonstrated by applying it to the tasks of top quark decay tagging (Lorentz group) and shape recognition (orthogonal group). ",
    "url": "https://arxiv.org/abs/2306.00091",
    "authors": [
      "Ilyes Batatia",
      "Mario Geiger",
      "Jose Munoz",
      "Tess Smidt",
      "Lior Silberman",
      "Christoph Ortner"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Theory (hep-th)"
    ]
  },
  {
    "id": "arXiv:2306.00145",
    "title": "On the Expressive Power of Neural Networks",
    "abstract": "In 1989 George Cybenko proved in a landmark paper that wide shallow neural networks can approximate arbitrary continuous functions on a compact set. This universal approximation theorem sparked a lot of follow-up research. Shen, Yang and Zhang determined optimal approximation rates for ReLU-networks in $L^p$-norms with $p \\in [1,\\infty)$. Kidger and Lyons proved a universal approximation theorem for deep narrow ReLU-networks. Telgarsky gave an example of a deep narrow ReLU-network that cannot be approximated by a wide shallow ReLU-network unless it has exponentially many neurons. However, there are even more questions that still remain unresolved. Are there any wide shallow ReLU-networks that cannot be approximated well by deep narrow ReLU-networks? Is the universal approximation theorem still true for other norms like the Sobolev norm $W^{1,1}$? Do these results hold for activation functions other than ReLU? We will answer all of those questions and more with a framework of two expressive powers. The first one is well-known and counts the maximal number of linear regions of a function calculated by a ReLU-network. We will improve the best known bounds for this expressive power. The second one is entirely new. ",
    "url": "https://arxiv.org/abs/2306.00145",
    "authors": [
      "Jan Holstermann"
    ],
    "subjectives": [
      "Classical Analysis and ODEs (math.CA)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.00149",
    "title": "Distributed Online Convex Optimization with Adversarial Constraints:  Reduced Cumulative Constraint Violation Bounds under Slater's Condition",
    "abstract": "This paper considers distributed online convex optimization with adversarial constraints. In this setting, a network of agents makes decisions at each round, and then only a portion of the loss function and a coordinate block of the constraint function are privately revealed to each agent. The loss and constraint functions are convex and can vary arbitrarily across rounds. The agents collaborate to minimize network regret and cumulative constraint violation. A novel distributed online algorithm is proposed and it achieves an $\\mathcal{O}(T^{\\max\\{c,1-c\\}})$ network regret bound and an $\\mathcal{O}(T^{1-c/2})$ network cumulative constraint violation bound, where $T$ is the number of rounds and $c\\in(0,1)$ is a user-defined trade-off parameter. When Slater's condition holds (i.e, there is a point that strictly satisfies the inequality constraints), the network cumulative constraint violation bound is reduced to $\\mathcal{O}(T^{1-c})$. Moreover, if the loss functions are strongly convex, then the network regret bound is reduced to $\\mathcal{O}(\\log(T))$, and the network cumulative constraint violation bound is reduced to $\\mathcal{O}(\\sqrt{\\log(T)T})$ and $\\mathcal{O}(\\log(T))$ without and with Slater's condition, respectively. To the best of our knowledge, this paper is the first to achieve reduced (network) cumulative constraint violation bounds for (distributed) online convex optimization with adversarial constraints under Slater's condition. Finally, the theoretical results are verified through numerical simulations. ",
    "url": "https://arxiv.org/abs/2306.00149",
    "authors": [
      "Xinlei Yi",
      "Xiuxian Li",
      "Tao Yang",
      "Lihua Xie",
      "Yiguang Hong",
      "Tianyou Chai",
      "Karl H. Johansson"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00242",
    "title": "Combinatorial Neural Bandits",
    "abstract": "We consider a contextual combinatorial bandit problem where in each round a learning agent selects a subset of arms and receives feedback on the selected arms according to their scores. The score of an arm is an unknown function of the arm's feature. Approximating this unknown score function with deep neural networks, we propose algorithms: Combinatorial Neural UCB ($\\texttt{CN-UCB}$) and Combinatorial Neural Thompson Sampling ($\\texttt{CN-TS}$). We prove that $\\texttt{CN-UCB}$ achieves $\\tilde{\\mathcal{O}}(\\tilde{d} \\sqrt{T})$ or $\\tilde{\\mathcal{O}}(\\sqrt{\\tilde{d} T K})$ regret, where $\\tilde{d}$ is the effective dimension of a neural tangent kernel matrix, $K$ is the size of a subset of arms, and $T$ is the time horizon. For $\\texttt{CN-TS}$, we adapt an optimistic sampling technique to ensure the optimism of the sampled combinatorial action, achieving a worst-case (frequentist) regret of $\\tilde{\\mathcal{O}}(\\tilde{d} \\sqrt{TK})$. To the best of our knowledge, these are the first combinatorial neural bandit algorithms with regret performance guarantees. In particular, $\\texttt{CN-TS}$ is the first Thompson sampling algorithm with the worst-case regret guarantees for the general contextual combinatorial bandit problem. The numerical experiments demonstrate the superior performances of our proposed algorithms. ",
    "url": "https://arxiv.org/abs/2306.00242",
    "authors": [
      "Taehyun Hwang",
      "Kyuwook Chai",
      "Min-hwan Oh"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00274",
    "title": "Optimal Rate-Matrix Pruning For Large-Scale Heterogeneous Systems",
    "abstract": "We present an analysis of large-scale load balancing systems, where the processing time distribution of tasks depends on both the task and server types. Our study focuses on the asymptotic regime, where the number of servers and task types tend to infinity in proportion. In heterogeneous environments, commonly used load balancing policies such as Join Fastest Idle Queue and Join Fastest Shortest Queue exhibit poor performance and even shrink the stability region. Interestingly, prior to this work, finding a scalable policy with a provable performance guarantee in this setup remained an open question. To address this gap, we propose and analyze two asymptotically delay-optimal dynamic load balancing policies. The first policy efficiently reserves the processing capacity of each server for ``good\" tasks and routes tasks using the vanilla Join Idle Queue policy. The second policy, called the speed-priority policy, significantly increases the likelihood of assigning tasks to the respective ``good\" servers capable of processing them at high speeds. By leveraging a framework inspired by the graphon literature and employing the mean-field method and stochastic coupling arguments, we demonstrate that both policies achieve asymptotic zero queuing. Specifically, as the system scales, the probability of a typical task being assigned to an idle server approaches 1. ",
    "url": "https://arxiv.org/abs/2306.00274",
    "authors": [
      "Zhisheng Zhao",
      "Debankur Mukherjee"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2306.00353",
    "title": "Constructing Semantics-Aware Adversarial Examples with Probabilistic  Perspective",
    "abstract": "In this study, we introduce a novel, probabilistic viewpoint on adversarial examples, achieved through box-constrained Langevin Monte Carlo (LMC). Proceeding from this perspective, we develop an innovative approach for generating semantics-aware adversarial examples in a principled manner. This methodology transcends the restriction imposed by geometric distance, instead opting for semantic constraints. Our approach empowers individuals to incorporate their personal comprehension of semantics into the model. Through human evaluation, we validate that our semantics-aware adversarial examples maintain their inherent meaning. Experimental findings on the MNIST and SVHN datasets demonstrate that our semantics-aware adversarial examples can effectively circumvent robust adversarial training methods tailored for traditional adversarial attacks. ",
    "url": "https://arxiv.org/abs/2306.00353",
    "authors": [
      "Andi Zhang",
      "Damon Wischik"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00357",
    "title": "Efficient and Robust Bayesian Selection of Hyperparameters in Dimension  Reduction for Visualization",
    "abstract": "We introduce an efficient and robust auto-tuning framework for hyperparameter selection in dimension reduction (DR) algorithms, focusing on large-scale datasets and arbitrary performance metrics. By leveraging Bayesian optimization (BO) with a surrogate model, our approach enables efficient hyperparameter selection with multi-objective trade-offs and allows us to perform data-driven sensitivity analysis. By incorporating normalization and subsampling, the proposed framework demonstrates versatility and efficiency, as shown in applications to visualization techniques such as t-SNE and UMAP. We evaluate our results on various synthetic and real-world datasets using multiple quality metrics, providing a robust and efficient solution for hyperparameter selection in DR algorithms. ",
    "url": "https://arxiv.org/abs/2306.00357",
    "authors": [
      "Yin-Ting Liao",
      "Hengrui Luo",
      "Anna Ma"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2306.00382",
    "title": "Calibrated Propensity Scores for Causal Effect Estimation",
    "abstract": "Propensity scores are commonly used to balance observed covariates while estimating treatment effects. Estimates obtained through propensity score weighing can be biased when the propensity score model cannot learn the true treatment assignment mechanism. We argue that the probabilistic output of a learned propensity score model should be calibrated, i.e. a predictive treatment probability of 90% should correspond to 90% of individuals being assigned the treatment group. We propose simple recalibration techniques to ensure this property. We investigate the theoretical properties of a calibrated propensity score model and its role in unbiased treatment effect estimation. We demonstrate improved causal effect estimation with calibrated propensity scores in several tasks including high-dimensional genome-wide association studies, where we also show reduced computational requirements when calibration is applied to simpler propensity score models. ",
    "url": "https://arxiv.org/abs/2306.00382",
    "authors": [
      "Shachi Deshpande",
      "Volodymyr Kuleshov"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00426",
    "title": "Speaker verification using attentive multi-scale convolutional recurrent  network",
    "abstract": "In this paper, we propose a speaker verification method by an Attentive Multi-scale Convolutional Recurrent Network (AMCRN). The proposed AMCRN can acquire both local spatial information and global sequential information from the input speech recordings. In the proposed method, logarithm Mel spectrum is extracted from each speech recording and then fed to the proposed AMCRN for learning speaker embedding. Afterwards, the learned speaker embedding is fed to the back-end classifier (such as cosine similarity metric) for scoring in the testing stage. The proposed method is compared with state-of-the-art methods for speaker verification. Experimental data are three public datasets that are selected from two large-scale speech corpora (VoxCeleb1 and VoxCeleb2). Experimental results show that our method exceeds baseline methods in terms of equal error rate and minimal detection cost function, and has advantages over most of baseline methods in terms of computational complexity and memory requirement. In addition, our method generalizes well across truncated speech segments with different durations, and the speaker embedding learned by the proposed AMCRN has stronger generalization ability across two back-end classifiers. ",
    "url": "https://arxiv.org/abs/2306.00426",
    "authors": [
      "Yanxiong Li",
      "Zhongjie Jiang",
      "Wenchang Cao",
      "Qisheng Huang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2306.00452",
    "title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it  Right?",
    "abstract": "Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance on speech tasks using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models. ",
    "url": "https://arxiv.org/abs/2306.00452",
    "authors": [
      "Salah Zaiem",
      "Youcef Kemiche",
      "Titouan Parcollet",
      "Slim Essid",
      "Mirco Ravanelli"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00481",
    "title": "Automatic Data Augmentation for Domain Adapted Fine-Tuning of  Self-Supervised Speech Representations",
    "abstract": "Self-Supervised Learning (SSL) has allowed leveraging large amounts of unlabeled speech data to improve the performance of speech recognition models even with small annotated datasets. Despite this, speech SSL representations may fail while facing an acoustic mismatch between the pretraining and target datasets. To address this issue, we propose a novel supervised domain adaptation method, designed for cases exhibiting such a mismatch in acoustic domains. It consists in applying properly calibrated data augmentations on a large clean dataset, bringing it closer to the target domain, and using it as part of an initial fine-tuning stage. Augmentations are automatically selected through the minimization of a conditional-dependence estimator, based on the target dataset. The approach is validated during an oracle experiment with controlled distortions and on two amateur-collected low-resource domains, reaching better performances compared to the baselines in both cases. ",
    "url": "https://arxiv.org/abs/2306.00481",
    "authors": [
      "Salah Zaiem",
      "Titouan Parcollet",
      "Slim Essid"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00485",
    "title": "Causal Estimation of User Learning in Personalized Systems",
    "abstract": "In online platforms, the impact of a treatment on an observed outcome may change over time as 1) users learn about the intervention, and 2) the system personalization, such as individualized recommendations, change over time. We introduce a non-parametric causal model of user actions in a personalized system. We show that the Cookie-Cookie-Day (CCD) experiment, designed for the measurement of the user learning effect, is biased when there is personalization. We derive new experimental designs that intervene in the personalization system to generate the variation necessary to separately identify the causal effect mediated through user learning and personalization. Making parametric assumptions allows for the estimation of long-term causal effects based on medium-term experiments. In simulations, we show that our new designs successfully recover the dynamic causal effects of interest. ",
    "url": "https://arxiv.org/abs/2306.00485",
    "authors": [
      "Evan Munro",
      "David Jones",
      "Jennifer Brennan",
      "Roland Nelet",
      "Vahab Mirrokni",
      "Jean Pouget-Abadie"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)"
    ]
  },
  {
    "id": "arXiv:2306.00542",
    "title": "Nonparametric Identifiability of Causal Representations from Unknown  Interventions",
    "abstract": "We study causal representation learning, the task of inferring latent causal variables and their causal relations from high-dimensional functions (\"mixtures\") of the variables. Prior work relies on weak supervision, in the form of counterfactual pre- and post-intervention views or temporal structure; places restrictive assumptions, such as linearity, on the mixing function or latent causal model; or requires partial knowledge of the generative process, such as the causal graph or the intervention targets. We instead consider the general setting in which both the causal model and the mixing function are nonparametric. The learning signal takes the form of multiple datasets, or environments, arising from unknown interventions in the underlying causal model. Our goal is to identify both the ground truth latents and their causal graph up to a set of ambiguities which we show to be irresolvable from interventional data. We study the fundamental setting of two causal variables and prove that the observational distribution and one perfect intervention per node suffice for identifiability, subject to a genericity condition. This condition rules out spurious solutions that involve fine-tuning of the intervened and observational distributions, mirroring similar conditions for nonlinear cause-effect inference. For an arbitrary number of variables, we show that two distinct paired perfect interventions per node guarantee identifiability. Further, we demonstrate that the strengths of causal influences among the latent variables are preserved by all equivalent solutions, rendering the inferred representation appropriate for drawing causal conclusions from new data. Our study provides the first identifiability results for the general nonparametric setting with unknown interventions, and elucidates what is possible and impossible for causal representation learning without more direct supervision. ",
    "url": "https://arxiv.org/abs/2306.00542",
    "authors": [
      "Julius von K\u00fcgelgen",
      "Michel Besserve",
      "Wendong Liang",
      "Luigi Gresele",
      "Armin Keki\u0107",
      "Elias Bareinboim",
      "David M. Blei",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.00571",
    "title": "Robust Exponential Stability and Invariance Guarantees with General  Dynamic O'Shea-Zames-Falb Multipliers",
    "abstract": "We propose novel time-domain dynamic integral quadratic constraints with a terminal cost for exponentially weighted slope-restricted gradients of not necessarily convex functions. This extends recent results for subdifferentials of convex function and their link to so-called O'Shea-Zames-Falb multipliers. The benefit of merging time-domain and frequency-domain techniques is demonstrated for linear saturated systems. ",
    "url": "https://arxiv.org/abs/2306.00571",
    "authors": [
      "Carsten W. Scherer"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2306.00952",
    "title": "Speaker-specific Thresholding for Robust Imposter Identification in  Unseen Speaker Recognition",
    "abstract": "Speaker identification systems are deployed in diverse environments, often different from the lab conditions on which they are trained and tested. In this paper, first, we show the problem of generalization using fixed thresholds computed using the equal error rate metric. Secondly, we introduce a novel and generalizable speaker-specific thresholding technique for robust imposter identification in unseen speaker identification. We propose a speaker-specific adaptive threshold, which can be computed using the enrollment audio samples, for identifying imposters in unseen speaker identification. Furthermore, we show the efficacy of the proposed technique on VoxCeleb1, VCTK and the FFSVC 2022 datasets, beating the baseline fixed thresholding by up to 25%. Finally, we exhibit that the proposed algorithm is also generalizable, demonstrating its performance on ResNet50, ECAPA-TDNN and RawNet3 speaker encoders. ",
    "url": "https://arxiv.org/abs/2306.00952",
    "authors": [
      "Ashutosh Chaubey",
      "Sparsh Sinha",
      "Susmita Ghose"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2006.16904",
    "title": "Graph Clustering with Graph Neural Networks",
    "abstract": " Comments: JMLR 24(127) 1-21 2023 ",
    "url": "https://arxiv.org/abs/2006.16904",
    "authors": [
      "Anton Tsitsulin",
      "John Palowitch",
      "Bryan Perozzi",
      "Emmanuel M\u00fcller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2008.09312",
    "title": "Near Optimal Adversarial Attack on UCB Bandits",
    "abstract": " Title: Near Optimal Adversarial Attack on UCB Bandits ",
    "url": "https://arxiv.org/abs/2008.09312",
    "authors": [
      "Shiliang Zuo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2010.05779",
    "title": "Sparse universal graphs for planarity",
    "abstract": " Comments: v4: minor change. v3: revised following referee's comments. v2: added new result about induced-universal graphs ",
    "url": "https://arxiv.org/abs/2010.05779",
    "authors": [
      "Louis Esperet",
      "Gwena\u00ebl Joret",
      "Pat Morin"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2108.11684",
    "title": "Disentangled Generative Models for Robust Prediction of System Dynamics",
    "abstract": " Title: Disentangled Generative Models for Robust Prediction of System Dynamics ",
    "url": "https://arxiv.org/abs/2108.11684",
    "authors": [
      "Stathi Fotiadis",
      "Mario Lino",
      "Shunlong Hu",
      "Stef Garasto",
      "Chris D Cantwell",
      "Anil Anthony Bharath"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2110.00267",
    "title": "Inductive Representation Learning in Temporal Networks via Mining  Neighborhood and Community Influences",
    "abstract": " Title: Inductive Representation Learning in Temporal Networks via Mining  Neighborhood and Community Influences ",
    "url": "https://arxiv.org/abs/2110.00267",
    "authors": [
      "Meng Liu",
      "Yong Liu"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2111.01075",
    "title": "Tight Exponential Analysis for Smoothing the Max-Relative Entropy and  for Quantum Privacy Amplification",
    "abstract": " Comments: V3: close to published version ",
    "url": "https://arxiv.org/abs/2111.01075",
    "authors": [
      "Ke Li",
      "Yongsheng Yao",
      "Masahito Hayashi"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)",
      "Mathematical Physics (math-ph)"
    ]
  },
  {
    "id": "arXiv:2111.08851",
    "title": "Deep Neural Networks for Rank-Consistent Ordinal Regression Based On  Conditional Probabilities",
    "abstract": " Comments: Accepted for publication in Pattern Analysis and Applications ",
    "url": "https://arxiv.org/abs/2111.08851",
    "authors": [
      "Xintong Shi",
      "Wenzhi Cao",
      "Sebastian Raschka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2111.13936",
    "title": "Is Causal Reasoning Harder than Probabilistic Reasoning?",
    "abstract": " Title: Is Causal Reasoning Harder than Probabilistic Reasoning? ",
    "url": "https://arxiv.org/abs/2111.13936",
    "authors": [
      "Milan Moss\u00e9",
      "Duligur Ibeling",
      "Thomas Icard"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2112.02052",
    "title": "TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs",
    "abstract": " Comments: Paper is accepted to USENIX ATC'23 ",
    "url": "https://arxiv.org/abs/2112.02052",
    "authors": [
      "Yuke Wang",
      "Boyuan Feng",
      "Zheng Wang",
      "Guyue Huang",
      "Yufei Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2112.10028",
    "title": "Attack of the Knights: A Non Uniform Cache Side-Channel Attack",
    "abstract": " Title: Attack of the Knights: A Non Uniform Cache Side-Channel Attack ",
    "url": "https://arxiv.org/abs/2112.10028",
    "authors": [
      "Farabi Mahmud",
      "Sungkeun Kim",
      "Harpreet Singh Chawla",
      "Chia-Che Tsai",
      "Eun Jung Kim",
      "Abdullah Muzahid"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2112.11397",
    "title": "NN2Poly: A polynomial representation for deep feed-forward artificial  neural networks",
    "abstract": " Title: NN2Poly: A polynomial representation for deep feed-forward artificial  neural networks ",
    "url": "https://arxiv.org/abs/2112.11397",
    "authors": [
      "Pablo Morala",
      "Jenny Alexandra Cifuentes",
      "Rosa E. Lillo",
      "I\u00f1aki Ucar"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.05397",
    "title": "Neural Architecture Search for Energy Efficient Always-on Audio Models",
    "abstract": " Title: Neural Architecture Search for Energy Efficient Always-on Audio Models ",
    "url": "https://arxiv.org/abs/2202.05397",
    "authors": [
      "Daniel T. Speckhard",
      "Karolis Misiunas",
      "Sagi Perel",
      "Tenghui Zhu",
      "Simon Carlile",
      "Malcolm Slaney"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2202.11592",
    "title": "A Law of Robustness beyond Isoperimetry",
    "abstract": " Comments: To appear in ICML 2023 ",
    "url": "https://arxiv.org/abs/2202.11592",
    "authors": [
      "Yihan Wu",
      "Heng Huang",
      "Hongyang Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2203.13226",
    "title": "SMARAGD: Learning SMatch for Accurate and Rapid Approximate Graph  Distance",
    "abstract": " Comments: to appear at 15th International Conference on Computational Semantics (IWCS 2023) ",
    "url": "https://arxiv.org/abs/2203.13226",
    "authors": [
      "Juri Opitz",
      "Philipp Meier",
      "Anette Frank"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2205.11775",
    "title": "Constrained Monotonic Neural Networks",
    "abstract": " Title: Constrained Monotonic Neural Networks ",
    "url": "https://arxiv.org/abs/2205.11775",
    "authors": [
      "Davor Runje",
      "Sharath M. Shankaranarayana"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.03353",
    "title": "Improving Adversarial Robustness by Putting More Regularizations on Less  Robust Samples",
    "abstract": " Comments: Accepted in ICML 2023 ",
    "url": "https://arxiv.org/abs/2206.03353",
    "authors": [
      "Dongyoon Yang",
      "Insung Kong",
      "Yongdai Kim"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.05239",
    "title": "StructCoder: Structure-Aware Transformer for Code Generation",
    "abstract": " Comments: Revised and added new experiments, edited writing ",
    "url": "https://arxiv.org/abs/2206.05239",
    "authors": [
      "Sindhu Tipirneni",
      "Ming Zhu",
      "Chandan K. Reddy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2206.13374",
    "title": "Stability Verification of Neural Network Controllers using Mixed-Integer  Programming",
    "abstract": " Title: Stability Verification of Neural Network Controllers using Mixed-Integer  Programming ",
    "url": "https://arxiv.org/abs/2206.13374",
    "authors": [
      "Roland Schwan",
      "Colin N. Jones",
      "Daniel Kuhn"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2207.11678",
    "title": "Quad-Net: Quad-domain Network for CT Metal Artifact Reduction",
    "abstract": " Title: Quad-Net: Quad-domain Network for CT Metal Artifact Reduction ",
    "url": "https://arxiv.org/abs/2207.11678",
    "authors": [
      "Zilong Li",
      "Qi Gao",
      "Yaping Wu",
      "Chuang Niu",
      "Junping Zhang",
      "Meiyun Wang",
      "Ge Wang",
      "Hongming Shan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2207.14116",
    "title": "Claim-Dissector: An Interpretable Fact-Checking System with Joint  Re-ranking and Veracity Prediction",
    "abstract": " Comments: updated acknowledgement ",
    "url": "https://arxiv.org/abs/2207.14116",
    "authors": [
      "Martin Fajcik",
      "Petr Motlicek",
      "Pavel Smrz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2208.12141",
    "title": "A Systematic Literature Review on the Impact of Formatting Elements on  Code Legibility",
    "abstract": " Comments: Journal of Systems and Software ",
    "url": "https://arxiv.org/abs/2208.12141",
    "authors": [
      "Delano Oliveira",
      "Reydne Santos",
      "Fernanda Madeiral",
      "Hidehiko Masuhara",
      "Fernando Castor"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2208.12700",
    "title": "Epistemic Parity: Reproducibility as an Evaluation Metric for  Differential Privacy",
    "abstract": " Comments: Preprint. 14 pages ",
    "url": "https://arxiv.org/abs/2208.12700",
    "authors": [
      "Lucas Rosenblatt",
      "Bernease Herman",
      "Anastasia Holovenko",
      "Wonkwon Lee",
      "Joshua Loftus",
      "Elizabeth McKinnie",
      "Taras Rumezhak",
      "Andrii Stadnik",
      "Bill Howe",
      "Julia Stoyanovich"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2209.10663",
    "title": "Convolutional Bayesian Kernel Inference for 3D Semantic Mapping",
    "abstract": " Title: Convolutional Bayesian Kernel Inference for 3D Semantic Mapping ",
    "url": "https://arxiv.org/abs/2209.10663",
    "authors": [
      "Joey Wilson",
      "Yuewei Fu",
      "Arthur Zhang",
      "Jingyu Song",
      "Andrew Capodieci",
      "Paramsothy Jayakumar",
      "Kira Barton",
      "Maani Ghaffari"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2210.06069",
    "title": "E3Bind: An End-to-End Equivariant Network for Protein-Ligand Docking",
    "abstract": " Comments: International Conference on Learning Representations (ICLR 2023) ",
    "url": "https://arxiv.org/abs/2210.06069",
    "authors": [
      "Yangtian Zhang",
      "Huiyu Cai",
      "Chence Shi",
      "Bozitao Zhong",
      "Jian Tang"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2210.09054",
    "title": "On the Identifiability and Estimation of Causal Location-Scale Noise  Models",
    "abstract": " Comments: ICML 2023 ",
    "url": "https://arxiv.org/abs/2210.09054",
    "authors": [
      "Alexander Immer",
      "Christoph Schultheiss",
      "Julia E. Vogt",
      "Bernhard Sch\u00f6lkopf",
      "Peter B\u00fchlmann",
      "Alexander Marx"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2210.10750",
    "title": "Canary in a Coalmine: Better Membership Inference with Ensembled  Adversarial Queries",
    "abstract": " Comments: Code is available at this https URL, published as a conference paper at ICLR 2023 ",
    "url": "https://arxiv.org/abs/2210.10750",
    "authors": [
      "Yuxin Wen",
      "Arpit Bansal",
      "Hamid Kazemi",
      "Eitan Borgnia",
      "Micah Goldblum",
      "Jonas Geiping",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2210.16478",
    "title": "GPA-Net:No-Reference Point Cloud Quality Assessment with Multi-task  Graph Convolutional Network",
    "abstract": " Title: GPA-Net:No-Reference Point Cloud Quality Assessment with Multi-task  Graph Convolutional Network ",
    "url": "https://arxiv.org/abs/2210.16478",
    "authors": [
      "Ziyu Shan",
      "Qi Yang",
      "Rui Ye",
      "Yujie Zhang",
      "Yiling Xu",
      "Xiaozhong Xu",
      "Shan Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2210.17051",
    "title": "Real-time high-resolution CO$_2$ geological storage prediction using  nested Fourier neural operators",
    "abstract": " Title: Real-time high-resolution CO$_2$ geological storage prediction using  nested Fourier neural operators ",
    "url": "https://arxiv.org/abs/2210.17051",
    "authors": [
      "Gege Wen",
      "Zongyi Li",
      "Qirui Long",
      "Kamyar Azizzadenesheli",
      "Anima Anandkumar",
      "Sally M. Benson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2210.17456",
    "title": "Audio-Visual Speech Enhancement and Separation by Utilizing Multi-Modal  Self-Supervised Embeddings",
    "abstract": " Comments: ICASSP AMHAT 2023 ",
    "url": "https://arxiv.org/abs/2210.17456",
    "authors": [
      "I-Chun Chern",
      "Kuo-Hsuan Hung",
      "Yi-Ting Chen",
      "Tassadaq Hussain",
      "Mandar Gogate",
      "Amir Hussain",
      "Yu Tsao",
      "Jen-Cheng Hou"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2211.00912",
    "title": "Bipartite Mixed Membership Distribution-Free Model. A novel model for  community detection in overlapping bipartite weighted networks",
    "abstract": " Comments: 31 pages, 15 figures, 7 tables ",
    "url": "https://arxiv.org/abs/2211.00912",
    "authors": [
      "Huan Qing",
      "Jingli Wang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2211.11406",
    "title": "Structural Optimization of Factor Graphs for Symbol Detection via  Continuous Clustering and Machine Learning",
    "abstract": " Comments: Accepted at ICASSP 2023 ",
    "url": "https://arxiv.org/abs/2211.11406",
    "authors": [
      "Lukas Rapp",
      "Luca Schmid",
      "Andrej Rode",
      "Laurent Schmalen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2211.13533",
    "title": "Prosody-controllable spontaneous TTS with neural HMMs",
    "abstract": " Comments: 5 pages, 3 figures, Published at ICASSP 2023 ",
    "url": "https://arxiv.org/abs/2211.13533",
    "authors": [
      "Harm Lameris",
      "Shivam Mehta",
      "Gustav Eje Henter",
      "Joakim Gustafson",
      "\u00c9va Sz\u00e9kely"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2211.17180",
    "title": "Nonlinear Advantage: Trained Networks Might Not Be As Complex as You  Think",
    "abstract": " Title: Nonlinear Advantage: Trained Networks Might Not Be As Complex as You  Think ",
    "url": "https://arxiv.org/abs/2211.17180",
    "authors": [
      "Christian H.X. Ali Mehmeti-G\u00f6pel",
      "Jan Disselhoff"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.00259",
    "title": "Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual  Reasoning",
    "abstract": " Comments: Published in CVPR 2023 as Highlight. Data and code are released at this https URL ",
    "url": "https://arxiv.org/abs/2212.00259",
    "authors": [
      "Zhuowan Li",
      "Xingrui Wang",
      "Elias Stengel-Eskin",
      "Adam Kortylewski",
      "Wufei Ma",
      "Benjamin Van Durme",
      "Alan Yuille"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.04592",
    "title": "Time-Synchronized State Estimation Using Graph Neural Networks in  Presence of Topology Changes",
    "abstract": " Comments: 6 pages, 2 figures ",
    "url": "https://arxiv.org/abs/2212.04592",
    "authors": [
      "Shiva Moshtagh",
      "Anwarul Islam Sifat",
      "Behrouz Azimian",
      "Anamitra Pal"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2212.08018",
    "title": "Privately Estimating a Gaussian: Efficient, Robust and Optimal",
    "abstract": " Title: Privately Estimating a Gaussian: Efficient, Robust and Optimal ",
    "url": "https://arxiv.org/abs/2212.08018",
    "authors": [
      "Daniel Alabi",
      "Pravesh K. Kothari",
      "Pranay Tankala",
      "Prayaag Venkat",
      "Fred Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2301.00785",
    "title": "CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection",
    "abstract": " Comments: Rank first in Medical Segmentation Decathlon (MSD) Competition ",
    "url": "https://arxiv.org/abs/2301.00785",
    "authors": [
      "Jie Liu",
      "Yixiao Zhang",
      "Jie-Neng Chen",
      "Junfei Xiao",
      "Yongyi Lu",
      "Bennett A. Landman",
      "Yixuan Yuan",
      "Alan Yuille",
      "Yucheng Tang",
      "Zongwei Zhou"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2301.07773",
    "title": "Temporal Logic Motion Planning with Convex Optimization via Graphs of  Convex Sets",
    "abstract": " Title: Temporal Logic Motion Planning with Convex Optimization via Graphs of  Convex Sets ",
    "url": "https://arxiv.org/abs/2301.07773",
    "authors": [
      "Vince Kurtz",
      "Hai Lin"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2301.10808",
    "title": "Graph Neural Tangent Kernel: Convergence on Large Graphs",
    "abstract": " Comments: Accepted in the 40th International Conference on Machine Learning (ICML), Honolulu, Hawaii ",
    "url": "https://arxiv.org/abs/2301.10808",
    "authors": [
      "Sanjukta Krishnagopal",
      "Luana Ruiz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Applications (stat.AP)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2301.12148",
    "title": "Quality Indicators for Preference-based Evolutionary Multi-objective  Optimization Using a Reference Point: A Review and Analysis",
    "abstract": " Title: Quality Indicators for Preference-based Evolutionary Multi-objective  Optimization Using a Reference Point: A Review and Analysis ",
    "url": "https://arxiv.org/abs/2301.12148",
    "authors": [
      "Ryoji Tanabe",
      "Ke Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2301.13083",
    "title": "Communication Drives the Emergence of Language Universals in Neural  Agents: Evidence from the Word-order/Case-marking Trade-off",
    "abstract": " Comments: Accepted to TACL, pre-MIT Press publication version ",
    "url": "https://arxiv.org/abs/2301.13083",
    "authors": [
      "Yuchen Lian",
      "Arianna Bisazza",
      "Tessa Verhoef"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2301.13755",
    "title": "Retrosynthetic Planning with Dual Value Networks",
    "abstract": " Comments: Accepted to ICML 2023 ",
    "url": "https://arxiv.org/abs/2301.13755",
    "authors": [
      "Guoqing Liu",
      "Di Xue",
      "Shufang Xie",
      "Yingce Xia",
      "Austin Tripp",
      "Krzysztof Maziarz",
      "Marwin Segler",
      "Tao Qin",
      "Zongzhang Zhang",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.01068",
    "title": "Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with  Record Level Differential Privacy",
    "abstract": " Title: Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with  Record Level Differential Privacy ",
    "url": "https://arxiv.org/abs/2302.01068",
    "authors": [
      "Hui-Po Wang",
      "Dingfan Chen",
      "Raouf Kerkouche",
      "Mario Fritz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.01503",
    "title": "LazyGNN: Large-Scale Graph Neural Networks via Lazy Propagation",
    "abstract": " Title: LazyGNN: Large-Scale Graph Neural Networks via Lazy Propagation ",
    "url": "https://arxiv.org/abs/2302.01503",
    "authors": [
      "Rui Xue",
      "Haoyu Han",
      "MohamadAli Torkamani",
      "Jian Pei",
      "Xiaorui Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.04638",
    "title": "Better Diffusion Models Further Improve Adversarial Training",
    "abstract": " Comments: ICML 2023 ",
    "url": "https://arxiv.org/abs/2302.04638",
    "authors": [
      "Zekai Wang",
      "Tianyu Pang",
      "Chao Du",
      "Min Lin",
      "Weiwei Liu",
      "Shuicheng Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.05209",
    "title": "A Survey on Causal Reinforcement Learning",
    "abstract": " Comments: 29 pages, 20 figures ",
    "url": "https://arxiv.org/abs/2302.05209",
    "authors": [
      "Yan Zeng",
      "Ruichu Cai",
      "Fuchun Sun",
      "Libo Huang",
      "Zhifeng Hao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.05637",
    "title": "Dual Relation Knowledge Distillation for Object Detection",
    "abstract": " Comments: Accepted by IJCAI-2023 ",
    "url": "https://arxiv.org/abs/2302.05637",
    "authors": [
      "Zhenliang Ni",
      "Fukui Yang",
      "Shengzhao Wen",
      "Gang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2302.05783",
    "title": "ConCerNet: A Contrastive Learning Based Framework for Automated  Conservation Law Discovery and Trustworthy Dynamical System Prediction",
    "abstract": " Comments: 22 pages, 7 figures ",
    "url": "https://arxiv.org/abs/2302.05783",
    "authors": [
      "Wang Zhang",
      "Tsui-Wei Weng",
      "Subhro Das",
      "Alexandre Megretski",
      "Luca Daniel",
      "Lam M. Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.07109",
    "title": "Reachability-Based Confidence-Aware Probabilistic Collision Detection in  Highway Driving",
    "abstract": " Comments: Under review at Engineering. arXiv admin note: text overlap with arXiv:2205.01357 ",
    "url": "https://arxiv.org/abs/2302.07109",
    "authors": [
      "Xinwei Wang",
      "Zirui Li",
      "Javier Alonso-Mora",
      "Meng Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2302.09450",
    "title": "Robust and Versatile Bipedal Jumping Control through Reinforcement  Learning",
    "abstract": " Comments: Accepted in Robotics: Science and Systems 2023 (RSS 2023). The accompanying video is at this https URL ",
    "url": "https://arxiv.org/abs/2302.09450",
    "authors": [
      "Zhongyu Li",
      "Xue Bin Peng",
      "Pieter Abbeel",
      "Sergey Levine",
      "Glen Berseth",
      "Koushil Sreenath"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2302.12057",
    "title": "ProsAudit, a prosodic benchmark for self-supervised speech models",
    "abstract": " Comments: Accepted at Interspeech 2023. 4 pages + references, 1 figure ",
    "url": "https://arxiv.org/abs/2302.12057",
    "authors": [
      "Maureen de Seyssel",
      "Marvin Lavechin",
      "Hadrien Titeux",
      "Arthur Thomas",
      "Gwendal Virlet",
      "Andrea Santos Revilla",
      "Guillaume Wisniewski",
      "Bogdan Ludusan",
      "Emmanuel Dupoux"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2302.12196",
    "title": "Adversarial Calibrated Regression for Online Decision Making",
    "abstract": " Comments: arXiv admin note: text overlap with arXiv:1607.03594 ",
    "url": "https://arxiv.org/abs/2302.12196",
    "authors": [
      "Volodymyr Kuleshov",
      "Shachi Deshpande"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.02243",
    "title": "Neural Operator Learning for Long-Time Integration in Dynamical Systems  with Recurrent Neural Networks",
    "abstract": " Comments: 12 pages, 5 figures ",
    "url": "https://arxiv.org/abs/2303.02243",
    "authors": [
      "Katarzyna Micha\u0142owska",
      "Somdatta Goswami",
      "George Em Karniadakis",
      "Signe Riemer-S\u00f8rensen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.03027",
    "title": "Critical Points and Convergence Analysis of Generative Deep Linear  Networks Trained with Bures-Wasserstein Loss",
    "abstract": " Comments: 35 pages, 1 figure, accepted at ICML 2023 ",
    "url": "https://arxiv.org/abs/2303.03027",
    "authors": [
      "Pierre Br\u00e9chet",
      "Katerina Papagiannouli",
      "Jing An",
      "Guido Mont\u00fafar"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.04485",
    "title": "Onsets and Velocities: Affordable Real-Time Piano Transcription Using  Convolutional Neural Networks",
    "abstract": " Comments: Accepted at EUSIPCO 2023 ",
    "url": "https://arxiv.org/abs/2303.04485",
    "authors": [
      "Andres Fernandez"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2303.17001",
    "title": "The G-invariant graph Laplacian",
    "abstract": " Title: The G-invariant graph Laplacian ",
    "url": "https://arxiv.org/abs/2303.17001",
    "authors": [
      "Eitan Rosen",
      "Paulina Hoyos",
      "Xiuyuan Cheng",
      "Joe Kileel",
      "Yoel Shkolnisky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2304.04697",
    "title": "Brain-Inspired Spiking Neural Network for Online Unsupervised Time  Series Prediction",
    "abstract": " Comments: Manuscript accepted to be published in IJCNN 2023 ",
    "url": "https://arxiv.org/abs/2304.04697",
    "authors": [
      "Biswadeep Chakraborty",
      "Saibal Mukhopadhyay"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2304.06461",
    "title": "Multi-Mode Online Knowledge Distillation for Self-Supervised Visual  Representation Learning",
    "abstract": " Comments: Accepted by CVPR 2023 ",
    "url": "https://arxiv.org/abs/2304.06461",
    "authors": [
      "Kaiyou Song",
      "Jin Xie",
      "Shan Zhang",
      "Zimeng Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2304.06631",
    "title": "Modeling Cell Size Distribution with Heterogeneous Flux Balance Analysis",
    "abstract": " Comments: 6 pages, 1 figure, Accepted for publication in IEEE Control Systems Letters ",
    "url": "https://arxiv.org/abs/2304.06631",
    "authors": [
      "Michiel Busschaert",
      "Florence H. Vermeire",
      "Steffen Waldherr"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2304.09367",
    "title": "Graph Neural Network-Based Anomaly Detection for River Network Systems",
    "abstract": " Title: Graph Neural Network-Based Anomaly Detection for River Network Systems ",
    "url": "https://arxiv.org/abs/2304.09367",
    "authors": [
      "Katie Buchhorn",
      "Edgar Santos-Fernandez",
      "Kerrie Mengersen",
      "Robert Salomone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2304.12217",
    "title": "Impact-Oriented Contextual Scholar Profiling using Self-Citation Graphs",
    "abstract": " Comments: to appear in KDD 2023 ",
    "url": "https://arxiv.org/abs/2304.12217",
    "authors": [
      "Yuankai Luo",
      "Lei Shi",
      "Mufan Xu",
      "Yuwen Ji",
      "Fengli Xiao",
      "Chunming Hu",
      "Zhiguang Shan"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.04560",
    "title": "Building Neural Networks on Matrix Manifolds: A Gyrovector Space  Approach",
    "abstract": " Title: Building Neural Networks on Matrix Manifolds: A Gyrovector Space  Approach ",
    "url": "https://arxiv.org/abs/2305.04560",
    "authors": [
      "Xuan Son Nguyen",
      "Shuo Yang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.04619",
    "title": "Graph Masked Autoencoder for Sequential Recommendation",
    "abstract": " Comments: This paper has been published as a full paper at SIGIR 2023 ",
    "url": "https://arxiv.org/abs/2305.04619",
    "authors": [
      "Yaowen Ye",
      "Lianghao Xia",
      "Chao Huang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.07437",
    "title": "Continual Vision-Language Representation Learning with Off-Diagonal  Information",
    "abstract": " Title: Continual Vision-Language Representation Learning with Off-Diagonal  Information ",
    "url": "https://arxiv.org/abs/2305.07437",
    "authors": [
      "Zixuan Ni",
      "Longhui Wei",
      "Siliang Tang",
      "Yueting Zhuang",
      "Qi Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.09948",
    "title": "HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic  Generalization Performance of Human-Object Interaction Detection Models",
    "abstract": " Comments: 19 pages, 3 figures, 4 tables ",
    "url": "https://arxiv.org/abs/2305.09948",
    "authors": [
      "Kentaro Takemoto",
      "Moyuru Yamada",
      "Tomotake Sasaki",
      "Hisanao Akima"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.10319",
    "title": "Automatic Photo Orientation Detection with Convolutional Neural Networks",
    "abstract": " Title: Automatic Photo Orientation Detection with Convolutional Neural Networks ",
    "url": "https://arxiv.org/abs/2305.10319",
    "authors": [
      "Ujash Joshi",
      "Michael Guerzhoy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.10638",
    "title": "Disentangled Causal Graph Learning forOnline Unsupervised Root Cause  Analysis",
    "abstract": " Title: Disentangled Causal Graph Learning forOnline Unsupervised Root Cause  Analysis ",
    "url": "https://arxiv.org/abs/2305.10638",
    "authors": [
      "Dongjie Wang",
      "Zhengzhang Chen",
      "Yanjie Fu",
      "Yanchi Liu",
      "Haifeng Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.10704",
    "title": "Attention-based Encoder-Decoder Network for End-to-End Neural Speaker  Diarization with Target Speaker Attractor",
    "abstract": " Comments: Accepted by InterSpeech 2023 ",
    "url": "https://arxiv.org/abs/2305.10704",
    "authors": [
      "Zhengyang Chen",
      "Bing Han",
      "Shuai Wang",
      "Yanmin Qian"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2305.10930",
    "title": "On the Off-Target Problem of Zero-Shot Multilingual Neural Machine  Translation",
    "abstract": " Comments: Findings of ACL 2023 ",
    "url": "https://arxiv.org/abs/2305.10930",
    "authors": [
      "Liang Chen",
      "Shuming Ma",
      "Dongdong Zhang",
      "Furu Wei",
      "Baobao Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.12530",
    "title": "Towards Robust Family-Infant Audio Analysis Based on Unsupervised  Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio",
    "abstract": " Comments: Accepted to Interspeech 2023 ",
    "url": "https://arxiv.org/abs/2305.12530",
    "authors": [
      "Jialu Li",
      "Mark Hasegawa-Johnson",
      "Nancy L. McElwain"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2305.16259",
    "title": "Neural Natural Language Processing for Long Texts: A Survey of the  State-of-the-Art",
    "abstract": " Comments: 54 pages, 2 figures, 173 citations ",
    "url": "https://arxiv.org/abs/2305.16259",
    "authors": [
      "Dimitrios Tsirmpas",
      "Ioannis Gkionis",
      "Ioannis Mademlis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.16283",
    "title": "CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs",
    "abstract": " Title: CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs ",
    "url": "https://arxiv.org/abs/2305.16283",
    "authors": [
      "Guangyao Zhai",
      "Evin P\u0131nar \u00d6rnek",
      "Shun-Cheng Wu",
      "Yan Di",
      "Federico Tombari",
      "Nassir Navab",
      "Benjamin Busam"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.16430",
    "title": "Too Few Bug Reports? Exploring Data Augmentation for Improved  Changeset-based Bug Localization",
    "abstract": " Title: Too Few Bug Reports? Exploring Data Augmentation for Improved  Changeset-based Bug Localization ",
    "url": "https://arxiv.org/abs/2305.16430",
    "authors": [
      "Agnieszka Ciborowska",
      "Kostadin Damevski"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2305.16966",
    "title": "Hybrid Energy Based Model in the Feature Space for Out-of-Distribution  Detection",
    "abstract": " Title: Hybrid Energy Based Model in the Feature Space for Out-of-Distribution  Detection ",
    "url": "https://arxiv.org/abs/2305.16966",
    "authors": [
      "Marc Lafon",
      "Elias Ramzi",
      "Cl\u00e9ment Rambour",
      "Nicolas Thome"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.17147",
    "title": "Heterogeneous Value Evaluation for Large Language Models",
    "abstract": " Comments: Our full prompts are released in the repo: this https URL ",
    "url": "https://arxiv.org/abs/2305.17147",
    "authors": [
      "Zhaowei Zhang",
      "Nian Liu",
      "Siyuan Qi",
      "Ceyao Zhang",
      "Ziqi Rong",
      "Song-Chun Zhu",
      "Shuguang Cui",
      "Yaodong Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.17375",
    "title": "Attention Schema in Neural Agents",
    "abstract": " Title: Attention Schema in Neural Agents ",
    "url": "https://arxiv.org/abs/2305.17375",
    "authors": [
      "Dianbo Liu",
      "Samuele Bolotta",
      "He Zhu",
      "Yoshua Bengio",
      "Guillaume Dumas"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.17497",
    "title": "FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph  Parsing",
    "abstract": " Comments: 9 pages, ACL 2023 (findings) ",
    "url": "https://arxiv.org/abs/2305.17497",
    "authors": [
      "Zhuang Li",
      "Yuyang Chai",
      "Terry Yue Zhuo",
      "Lizhen Qu",
      "Gholamreza Haffari",
      "Fei Li",
      "Donghong Ji",
      "Quan Hung Tran"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18079",
    "title": "Towards a Robust Framework for NeRF Evaluation",
    "abstract": " Comments: 9 pages, 2 main experiments, 2 additional experiments ",
    "url": "https://arxiv.org/abs/2305.18079",
    "authors": [
      "Adrian Azzarelli",
      "Nantheera Anantrasirichai",
      "David R Bull"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2305.18256",
    "title": "Representation Learning on Hyper-Relational and Numeric Knowledge Graphs  with Transformers",
    "abstract": " Comments: 11 pages, 5 figures, 12 tables. 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2023) ",
    "url": "https://arxiv.org/abs/2305.18256",
    "authors": [
      "Chanyoung Chung",
      "Jaejun Lee",
      "Joyce Jiyoung Whang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18342",
    "title": "Neural Task Synthesis for Visual Programming",
    "abstract": " Title: Neural Task Synthesis for Visual Programming ",
    "url": "https://arxiv.org/abs/2305.18342",
    "authors": [
      "Victor-Alexandru P\u0103durean",
      "Georgios Tzannetos",
      "Adish Singla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2305.18404",
    "title": "Conformal Prediction with Large Language Models for Multi-Choice  Question Answering",
    "abstract": " Comments: Added additional references ",
    "url": "https://arxiv.org/abs/2305.18404",
    "authors": [
      "Bhawesh Kumar",
      "Charlie Lu",
      "Gauri Gupta",
      "Anil Palepu",
      "David Bellamy",
      "Ramesh Raskar",
      "Andrew Beam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.18657",
    "title": "Representation Of Lexical Stylistic Features In Language Models'  Embedding Space",
    "abstract": " Comments: Accepted at *SEM 2023 ",
    "url": "https://arxiv.org/abs/2305.18657",
    "authors": [
      "Qing Lyu",
      "Marianna Apidianaki",
      "Chris Callison-Burch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2305.18687",
    "title": "Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic  Forecasting",
    "abstract": " Comments: Published in Transactions on Machine Learning Research, 2023 ",
    "url": "https://arxiv.org/abs/2305.18687",
    "authors": [
      "Zibo Liu",
      "Parshin Shojaee",
      "Chandan K Reddy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18758",
    "title": "Task-Equivariant Graph Few-shot Learning",
    "abstract": " Comments: KDD 2023 ",
    "url": "https://arxiv.org/abs/2305.18758",
    "authors": [
      "Sungwon Kim",
      "Junseok Lee",
      "Namkyeong Lee",
      "Wonjoong Kim",
      "Seungyoon Choi",
      "Chanyoung Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.18885",
    "title": "Criteria Tell You More than Ratings: Criteria Preference-Aware Light  Graph Convolution for Effective Multi-Criteria Recommendation",
    "abstract": " Comments: 12 pages, 10 figures, 5 tables; 29th ACM SIGKDD Conference on Knowledge Discovery & Data (KDD 2023) (to appear) (Please cite our conference version.) ",
    "url": "https://arxiv.org/abs/2305.18885",
    "authors": [
      "Jin-Duk Park",
      "Siqing Li",
      "Xin Cao",
      "Won-Yong Shin"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.18888",
    "title": "Contrastive Shapelet Learning for Unsupervised Multivariate Time Series  Representation Learning",
    "abstract": " Title: Contrastive Shapelet Learning for Unsupervised Multivariate Time Series  Representation Learning ",
    "url": "https://arxiv.org/abs/2305.18888",
    "authors": [
      "Zhiyu Liang",
      "Jianfeng Zhang",
      "Chen Liang",
      "Hongzhi Wang",
      "Zheng Liang",
      "Lujia Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.19004",
    "title": "Policy Gradient Algorithms for Robust MDPs with Non-Rectangular  Uncertainty Sets",
    "abstract": " Comments: 20 pages, 2 figures ",
    "url": "https://arxiv.org/abs/2305.19004",
    "authors": [
      "Mengmeng Li",
      "Tobias Sutter",
      "Daniel Kuhn"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.19125",
    "title": "Hierarchical Graph Generation with $K^2$-trees",
    "abstract": " Comments: 22 pages (10 appendices) ",
    "url": "https://arxiv.org/abs/2305.19125",
    "authors": [
      "Yunhui Jang",
      "Dongwoo Kim",
      "Sungsoo Ahn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2305.19470",
    "title": "Label Embedding by Johnson-Lindenstrauss Matrices",
    "abstract": " Title: Label Embedding by Johnson-Lindenstrauss Matrices ",
    "url": "https://arxiv.org/abs/2305.19470",
    "authors": [
      "Jianxin Zhang",
      "Clayton Scott"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.19567",
    "title": "DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code  Collaborated with Mixer",
    "abstract": " Comments: Accepted at Interspeech 2023 ",
    "url": "https://arxiv.org/abs/2305.19567",
    "authors": [
      "Yerin Choi",
      "Myoung-Wan Koo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2305.19623",
    "title": "Point-GCC: Universal Self-supervised 3D Scene Pre-training via  Geometry-Color Contrast",
    "abstract": " Title: Point-GCC: Universal Self-supervised 3D Scene Pre-training via  Geometry-Color Contrast ",
    "url": "https://arxiv.org/abs/2305.19623",
    "authors": [
      "Guofan Fan",
      "Zekun Qi",
      "Wenkai Shi",
      "Kaisheng Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.19725",
    "title": "Direct Learning-Based Deep Spiking Neural Networks: A Review",
    "abstract": " Comments: Accepted by Frontiers in Neuroscience ",
    "url": "https://arxiv.org/abs/2305.19725",
    "authors": [
      "Yufei Guo",
      "Xuhui Huang",
      "Zhe Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.19987",
    "title": "InGram: Inductive Knowledge Graph Embedding via Relation Graphs",
    "abstract": " Comments: 14 pages, 4 figures, 6 tables, 40th International Conference on Machine Learning (ICML 2023) ",
    "url": "https://arxiv.org/abs/2305.19987",
    "authors": [
      "Jaejun Lee",
      "Chanyoung Chung",
      "Joyce Jiyoung Whang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.20030",
    "title": "Tree-Ring Watermarks: Fingerprints for Diffusion Images that are  Invisible and Robust",
    "abstract": " Comments: 16 pages, 8 figures, code is available at this https URL, fixed the repo link ",
    "url": "https://arxiv.org/abs/2305.20030",
    "authors": [
      "Yuxin Wen",
      "John Kirchenbauer",
      "Jonas Geiping",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  }
]