[
  {
    "id": "arXiv:2308.12305",
    "title": "FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal  Heterogeneous Federated Learning",
    "abstract": "Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning framework tailored to heterogeneous multi-modal FL, called Federated Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the client local updates and applying Mutual Knowledge Distillation (MKD) for an efficient knowledge transfer. FedDAT is the first approach that enables an efficient distributed finetuning of foundation models for a variety of heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we conduct extensive experiments on four multi-modality FL benchmarks with different types of data heterogeneity, where FedDAT substantially outperforms the existing centralized PEFT methods adapted for FL. ",
    "url": "https://arxiv.org/abs/2308.12305",
    "authors": [
      "Haokun Chen",
      "Yao Zhang",
      "Denis Krompass",
      "Jindong Gu",
      "Volker Tresp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2308.12315",
    "title": "Trustworthy Representation Learning Across Domains",
    "abstract": "As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions. ",
    "url": "https://arxiv.org/abs/2308.12315",
    "authors": [
      "Ronghang Zhu",
      "Dongliang Guo",
      "Daiqing Qi",
      "Zhixuan Chu",
      "Xiang Yu",
      "Sheng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12316",
    "title": "Graph Neural Stochastic Differential Equations",
    "abstract": "We present a novel model Graph Neural Stochastic Differential Equations (Graph Neural SDEs). This technique enhances the Graph Neural Ordinary Differential Equations (Graph Neural ODEs) by embedding randomness into data representation using Brownian motion. This inclusion allows for the assessment of prediction uncertainty, a crucial aspect frequently missed in current models. In our framework, we spotlight the \\textit{Latent Graph Neural SDE} variant, demonstrating its effectiveness. Through empirical studies, we find that Latent Graph Neural SDEs surpass conventional models like Graph Convolutional Networks and Graph Neural ODEs, especially in confidence prediction, making them superior in handling out-of-distribution detection across both static and spatio-temporal contexts. ",
    "url": "https://arxiv.org/abs/2308.12316",
    "authors": [
      "Richard Bergna",
      "Felix Opolka",
      "Pietro Li\u00f2",
      "Jose Miguel Hernandez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12319",
    "title": "RemovalNet: DNN Fingerprint Removal Attacks",
    "abstract": "With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this paper, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named RemovalNet, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model's general semantic knowledge to maintain the surrogate model's performance. We conduct extensive experiments to evaluate the fidelity, effectiveness, and efficiency of the RemovalNet against four advanced defense methods on six metrics. The empirical results demonstrate that (1) the RemovalNet is effective. After our DNN fingerprint removal attack, the model distance between the target and surrogate models is x100 times higher than that of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2% (400 samples) of the substitute dataset and 1,000 iterations to conduct our attack. Besides, compared with advanced model stealing attacks, the RemovalNet saves nearly 85% of computational resources at most, (3) the RemovalNet achieves high fidelity that the created surrogate model maintains high accuracy after the DNN fingerprint removal process. Our code is available at: https://github.com/grasses/RemovalNet. ",
    "url": "https://arxiv.org/abs/2308.12319",
    "authors": [
      "Hongwei Yao",
      "Zheng Li",
      "Kunzhe Huang",
      "Jian Lou",
      "Zhan Qin",
      "Kui Ren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12322",
    "title": "Fine-grained Spatio-Temporal Distribution Prediction of Mobile Content  Delivery in 5G Ultra-Dense Networks",
    "abstract": "The 5G networks have extensively promoted the growth of mobile users and novel applications, and with the skyrocketing user requests for a large amount of popular content, the consequent content delivery services (CDSs) have been bringing a heavy load to mobile service providers. As a key mission in intelligent networks management, understanding and predicting the distribution of CDSs benefits many tasks of modern network services such as resource provisioning and proactive content caching for content delivery networks. However, the revolutions in novel ubiquitous network architectures led by ultra-dense networks (UDNs) make the task extremely challenging. Specifically, conventional methods face the challenges of insufficient spatio precision, lacking generalizability, and complex multi-feature dependencies of user requests, making their effectiveness unreliable in CDSs prediction under 5G UDNs. In this paper, we propose to adopt a series of encoding and sampling methods to model CDSs of known and unknown areas at a tailored fine-grained level. Moreover, we design a spatio-temporal-social multi-feature extraction framework for CDSs hotspots prediction, in which a novel edge-enhanced graph convolution block is proposed to encode dynamic CDSs networks based on the social relationships and the spatio features. Besides, we introduce the Long-Short Term Memory (LSTM) to further capture the temporal dependency. Extensive performance evaluations with real-world measurement data collected in two mobile content applications demonstrate the effectiveness of our proposed solution, which can improve the prediction area under the curve (AUC) by 40.5% compared to the state-of-the-art proposals at a spatio granularity of 76m, with up to 80% of the unknown areas. ",
    "url": "https://arxiv.org/abs/2308.12322",
    "authors": [
      "Shaoyuan Huang",
      "Heng Zhang",
      "Xiaofei Wang",
      "Min Chen",
      "Jianxin Li",
      "Victor C. M. Leung"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2308.12371",
    "title": "Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and  Feature Augmentation",
    "abstract": "Open-set face recognition refers to a scenario in which biometric systems have incomplete knowledge of all existing subjects. Therefore, they are expected to prevent face samples of unregistered subjects from being identified as previously enrolled identities. This watchlist context adds an arduous requirement that calls for the dismissal of irrelevant faces by focusing mainly on subjects of interest. As a response, this work introduces a novel method that associates an ensemble of compact neural networks with a margin-based cost function that explores additional samples. Supplementary negative samples can be obtained from external databases or synthetically built at the representation level in training time with a new mix-up feature augmentation approach. Deep neural networks pre-trained on large face datasets serve as the preliminary feature extraction module. We carry out experiments on well-known LFW and IJB-C datasets where results show that the approach is able to boost closed and open-set identification rates. ",
    "url": "https://arxiv.org/abs/2308.12371",
    "authors": [
      "Rafael Henrique Vareto",
      "Manuel G\u00fcnther",
      "William Robson Schwartz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12380",
    "title": "FG-Net: Facial Action Unit Detection with Generalizable Pyramidal  Features",
    "abstract": "Automatic detection of facial Action Units (AUs) allows for objective facial expression analysis. Due to the high cost of AU labeling and the limited size of existing benchmarks, previous AU detection methods tend to overfit the dataset, resulting in a significant performance loss when evaluated across corpora. To address this problem, we propose FG-Net for generalizable facial action unit detection. Specifically, FG-Net extracts feature maps from a StyleGAN2 model pre-trained on a large and diverse face image dataset. Then, these features are used to detect AUs with a Pyramid CNN Interpreter, making the training efficient and capturing essential local features. The proposed FG-Net achieves a strong generalization ability for heatmap-based AU detection thanks to the generalizable and semantic-rich features extracted from the pre-trained generative model. Extensive experiments are conducted to evaluate within- and cross-corpus AU detection with the widely-used DISFA and BP4D datasets. Compared with the state-of-the-art, the proposed method achieves superior cross-domain performance while maintaining competitive within-domain performance. In addition, FG-Net is data-efficient and achieves competitive performance even when trained on 1000 samples. Our code will be released at \\url{https://github.com/ihp-lab/FG-Net} ",
    "url": "https://arxiv.org/abs/2308.12380",
    "authors": [
      "Yufeng Yin",
      "Di Chang",
      "Guoxian Song",
      "Shen Sang",
      "Tiancheng Zhi",
      "Jing Liu",
      "Linjie Luo",
      "Mohammad Soleymani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12383",
    "title": "With a Little Help from your own Past: Prototypical Memory Networks for  Image Captioning",
    "abstract": "Image captioning, like many tasks involving vision and language, currently relies on Transformer-based architectures for extracting the semantics in an image and translating it into linguistically coherent descriptions. Although successful, the attention operator only considers a weighted summation of projections of the current input sample, therefore ignoring the relevant semantic information which can come from the joint observation of other samples. In this paper, we devise a network which can perform attention over activations obtained while processing other training samples, through a prototypical memory model. Our memory models the distribution of past keys and values through the definition of prototype vectors which are both discriminative and compact. Experimentally, we assess the performance of the proposed model on the COCO dataset, in comparison with carefully designed baselines and state-of-the-art approaches, and by investigating the role of each of the proposed components. We demonstrate that our proposal can increase the performance of an encoder-decoder Transformer by 3.7 CIDEr points both when training in cross-entropy only and when fine-tuning with self-critical sequence training. Source code and trained models are available at: https://github.com/aimagelab/PMA-Net. ",
    "url": "https://arxiv.org/abs/2308.12383",
    "authors": [
      "Manuele Barraco",
      "Sara Sarto",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2308.12394",
    "title": "Self-Supervised Learning for Endoscopic Video Analysis",
    "abstract": "Self-supervised learning (SSL) has led to important breakthroughs in computer vision by allowing learning from large amounts of unlabeled data. As such, it might have a pivotal role to play in biomedicine where annotating data requires a highly specialized expertise. Yet, there are many healthcare domains for which SSL has not been extensively explored. One such domain is endoscopy, minimally invasive procedures which are commonly used to detect and treat infections, chronic inflammatory diseases or cancer. In this work, we study the use of a leading SSL framework, namely Masked Siamese Networks (MSNs), for endoscopic video analysis such as colonoscopy and laparoscopy. To fully exploit the power of SSL, we create sizable unlabeled endoscopic video datasets for training MSNs. These strong image representations serve as a foundation for secondary training with limited annotated datasets, resulting in state-of-the-art performance in endoscopic benchmarks like surgical phase recognition during laparoscopy and colonoscopic polyp characterization. Additionally, we achieve a 50% reduction in annotated data size without sacrificing performance. Thus, our work provides evidence that SSL can dramatically reduce the need of annotated data in endoscopy. ",
    "url": "https://arxiv.org/abs/2308.12394",
    "authors": [
      "Roy Hirsch",
      "Mathilde Caron",
      "Regev Cohen",
      "Amir Livne",
      "Ron Shapiro",
      "Tomer Golany",
      "Roman Goldenberg",
      "Daniel Freedman",
      "Ehud Rivlin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12415",
    "title": "Benchmarking Causal Study to Interpret Large Language Models for Source  Code",
    "abstract": "One of the most common solutions adopted by software researchers to address code generation is by training Large Language Models (LLMs) on massive amounts of source code. Although a number of studies have shown that LLMs have been effectively evaluated on popular accuracy metrics (e.g., BLEU, CodeBleu), previous research has largely overlooked the role of Causal Inference as a fundamental component of the interpretability of LLMs' performance. Existing benchmarks and datasets are meant to highlight the difference between the expected and the generated outcome, but do not take into account confounding variables (e.g., lines of code, prompt size) that equally influence the accuracy metrics. The fact remains that, when dealing with generative software tasks by LLMs, no benchmark is available to tell researchers how to quantify neither the causal effect of SE-based treatments nor the correlation of confounders to the model's performance. In an effort to bring statistical rigor to the evaluation of LLMs, this paper introduces a benchmarking strategy named Galeras comprised of curated testbeds for three SE tasks (i.e., code completion, code summarization, and commit generation) to help aid the interpretation of LLMs' performance. We illustrate the insights of our benchmarking strategy by conducting a case study on the performance of ChatGPT under distinct prompt engineering methods. The results of the case study demonstrate the positive causal influence of prompt semantics on ChatGPT's generative performance by an average treatment effect of $\\approx 3\\%$. Moreover, it was found that confounders such as prompt size are highly correlated with accuracy metrics ($\\approx 0.412\\%$). The end result of our case study is to showcase causal inference evaluations, in practice, to reduce confounding bias. By reducing the bias, we offer an interpretable solution for the accuracy metric under analysis. ",
    "url": "https://arxiv.org/abs/2308.12415",
    "authors": [
      "Daniel Rodriguez-Cardenas",
      "David N. Palacio",
      "Dipin Khati",
      "Henry Burke",
      "Denys Poshyvanyk"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12435",
    "title": "Characterising representation dynamics in recurrent neural networks for  object recognition",
    "abstract": "Recurrent neural networks (RNNs) have yielded promising results for both recognizing objects in challenging conditions and modeling aspects of primate vision. However, the representational dynamics of recurrent computations remain poorly understood, especially in large-scale visual models. Here, we studied such dynamics in RNNs trained for object classification on MiniEcoset, a novel subset of ecoset. We report two main insights. First, upon inference, representations continued to evolve after correct classification, suggesting a lack of the notion of being ``done with classification''. Second, focusing on ``readout zones'' as a way to characterize the activation trajectories, we observe that misclassified representations exhibit activation patterns with lower L2 norm, and are positioned more peripherally in the readout zones. Such arrangements help the misclassified representations move into the correct zones as time progresses. Our findings generalize to networks with lateral and top-down connections, and include both additive and multiplicative interactions with the bottom-up sweep. The results therefore contribute to a general understanding of RNN dynamics in naturalistic tasks. We hope that the analysis framework will aid future investigations of other types of RNNs, including understanding of representational dynamics in primate vision. ",
    "url": "https://arxiv.org/abs/2308.12435",
    "authors": [
      "Sushrut Thorat",
      "Adrien Doerig",
      "Tim C. Kietzmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2308.12439",
    "title": "BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input  Detection",
    "abstract": "We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by an ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert (Backdoor Input Detection with Backdoor Expert), effectively mitigates 16 SOTA backdoor attacks while minimally impacting clean utility. The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet) across various model architectures (ResNet, VGG, MobileNetV2 and Vision Transformer). ",
    "url": "https://arxiv.org/abs/2308.12439",
    "authors": [
      "Tinghao Xie",
      "Xiangyu Qi",
      "Ping He",
      "Yiming Li",
      "Jiachen T. Wang",
      "Prateek Mittal"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12478",
    "title": "Attention-Based Acoustic Feature Fusion Network for Depression Detection",
    "abstract": "Depression, a common mental disorder, significantly influences individuals and imposes considerable societal impacts. The complexity and heterogeneity of the disorder necessitate prompt and effective detection, which nonetheless, poses a difficult challenge. This situation highlights an urgent requirement for improved detection methods. Exploiting auditory data through advanced machine learning paradigms presents promising research directions. Yet, existing techniques mainly rely on single-dimensional feature models, potentially neglecting the abundance of information hidden in various speech characteristics. To rectify this, we present the novel Attention-Based Acoustic Feature Fusion Network (ABAFnet) for depression detection. ABAFnet combines four different acoustic features into a comprehensive deep learning model, thereby effectively integrating and blending multi-tiered features. We present a novel weight adjustment module for late fusion that boosts performance by efficaciously synthesizing these features. The effectiveness of our approach is confirmed via extensive validation on two clinical speech databases, CNRAC and CS-NRAC, thereby outperforming previous methods in depression detection and subtype classification. Further in-depth analysis confirms the key role of each feature and highlights the importance of MFCCrelated features in speech-based depression detection. ",
    "url": "https://arxiv.org/abs/2308.12478",
    "authors": [
      "Xiao Xu",
      "Yang Wang",
      "Xinru Wei",
      "Fei Wang",
      "Xizhe Zhang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2308.12492",
    "title": "Optimizing Neural Network Scale for ECG Classification",
    "abstract": "We study scaling convolutional neural networks (CNNs), specifically targeting Residual neural networks (ResNet), for analyzing electrocardiograms (ECGs). Although ECG signals are time-series data, CNN-based models have been shown to outperform other neural networks with different architectures in ECG analysis. However, most previous studies in ECG analysis have overlooked the importance of network scaling optimization, which significantly improves performance. We explored and demonstrated an efficient approach to scale ResNet by examining the effects of crucial parameters, including layer depth, the number of channels, and the convolution kernel size. Through extensive experiments, we found that a shallower network, a larger number of channels, and smaller kernel sizes result in better performance for ECG classifications. The optimal network scale might differ depending on the target task, but our findings provide insight into obtaining more efficient and accurate models with fewer computing resources or less time. In practice, we demonstrate that a narrower search space based on our findings leads to higher performance. ",
    "url": "https://arxiv.org/abs/2308.12492",
    "authors": [
      "Byeong Tak Lee",
      "Yong-Yeon Jo",
      "Joon-Myoung Kwon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2308.12497",
    "title": "False Information, Bots and Malicious Campaigns: Demystifying Elements  of Social Media Manipulations",
    "abstract": "The rapid spread of false information and persistent manipulation attacks on online social networks (OSNs), often for political, ideological, or financial gain, has affected the openness of OSNs. While researchers from various disciplines have investigated different manipulation-triggering elements of OSNs (such as understanding information diffusion on OSNs or detecting automated behavior of accounts), these works have not been consolidated to present a comprehensive overview of the interconnections among these elements. Notably, user psychology, the prevalence of bots, and their tactics in relation to false information detection have been overlooked in previous research. To address this research gap, this paper synthesizes insights from various disciplines to provide a comprehensive analysis of the manipulation landscape. By integrating the primary elements of social media manipulation (SMM), including false information, bots, and malicious campaigns, we extensively examine each SMM element. Through a systematic investigation of prior research, we identify commonalities, highlight existing gaps, and extract valuable insights in the field. Our findings underscore the urgent need for interdisciplinary research to effectively combat social media manipulations, and our systematization can guide future research efforts and assist OSN providers in ensuring the safety and integrity of their platforms. ",
    "url": "https://arxiv.org/abs/2308.12497",
    "authors": [
      "Mohammad Majid Akhtar",
      "Rahat Masood",
      "Muhammad Ikram",
      "Salil S. Kanhere"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12501",
    "title": "DD-GCN: Directed Diffusion Graph Convolutional Network for  Skeleton-based Human Action Recognition",
    "abstract": "Graph Convolutional Networks (GCNs) have been widely used in skeleton-based human action recognition. In GCN-based methods, the spatio-temporal graph is fundamental for capturing motion patterns. However, existing approaches ignore the physical dependency and synchronized spatio-temporal correlations between joints, which limits the representation capability of GCNs. To solve these problems, we construct the directed diffusion graph for action modeling and introduce the activity partition strategy to optimize the weight sharing mechanism of graph convolution kernels. In addition, we present the spatio-temporal synchronization encoder to embed synchronized spatio-temporal semantics. Finally, we propose Directed Diffusion Graph Convolutional Network (DD-GCN) for action recognition, and the experiments on three public datasets: NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-art performance of our method. ",
    "url": "https://arxiv.org/abs/2308.12501",
    "authors": [
      "Chang Li",
      "Qian Huang",
      "Yingchi Mao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12512",
    "title": "I3DOD: Towards Incremental 3D Object Detection via Prompting",
    "abstract": "3D object detection has achieved significant performance in many fields, e.g., robotics system, autonomous driving, and augmented reality. However, most existing methods could cause catastrophic forgetting of old classes when performing on the class-incremental scenarios. Meanwhile, the current class-incremental 3D object detection methods neglect the relationships between the object localization information and category semantic information and assume all the knowledge of old model is reliable. To address the above challenge, we present a novel Incremental 3D Object Detection framework with the guidance of prompting, i.e., I3DOD. Specifically, we propose a task-shared prompts mechanism to learn the matching relationships between the object localization information and category semantic information. After training on the current task, these prompts will be stored in our prompt pool, and perform the relationship of old classes in the next task. Moreover, we design a reliable distillation strategy to transfer knowledge from two aspects: a reliable dynamic distillation is developed to filter out the negative knowledge and transfer the reliable 3D knowledge to new detection model; the relation feature is proposed to capture the responses relation in feature space and protect plasticity of the model when learning novel 3D classes. To the end, we conduct comprehensive experiments on two benchmark datasets and our method outperforms the state-of-the-art object detection methods by 0.6% - 2.7% in terms of mAP@0.25. ",
    "url": "https://arxiv.org/abs/2308.12512",
    "authors": [
      "Wenqi Liang",
      "Gan Sun",
      "Chenxi Liu",
      "Jiahua Dong",
      "Kangru Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12514",
    "title": "Privacy engineering through obfuscation",
    "abstract": "Obfuscation in privacy engineering denotes a diverse set of data operations aimed at reducing the privacy loss that users incur in by participating in digital systems. Obfuscation's domain of application is vast: privacy-preserving database analysis, location-based privacy, private web search or privacy-friendly recommender systems are but a few examples of the contexts in which privacy engineers have resorted to obfuscation. Yet an understanding of the role that obfuscation, in general, plays in the engineering of privacy has so far proved elusive. Similarly, we lack a cohesive view of the wide array of privacy measures that assist the evaluation of obfuscation technologies. This paper contributes to closing these research gaps. First, we provide a general analysis framework that brings together a multiplicity of obfuscation methods under the same analytical umbrella. Second, we distinguish between mechanism-centred and attack-centred evaluation, making explicit a hierarchy of assumptions behind privacy measures that assists and demystifies obfuscation tools' evaluation. Finally, we examine the role that obfuscation technology plays in privacy engineering by introducing the concepts of personal and public utility and distinguishing between utility-degrading and utility-preserving obfuscation. We observe that public utility requirements require us to resort to utility-degrading obfuscation to arbitrarily reduce privacy loss. Conversely, personal utility requirements do not, in theory, impose such a privacy-utility trade-off, and we illustrate how to perform utility-preserving obfuscation through chaff. ",
    "url": "https://arxiv.org/abs/2308.12514",
    "authors": [
      "Ero Balsa"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2308.12529",
    "title": "Privacy-Preserving Discretized Spiking Neural Networks",
    "abstract": "The rapid development of artificial intelligence has brought considerable convenience, yet also introduces significant security risks. One of the research hotspots is to balance data privacy and utility in the real world of artificial intelligence. The present second-generation artificial neural networks have made tremendous advances, but some big models could have really high computational costs. The third-generation neural network, SNN (Spiking Neural Network), mimics real neurons by using discrete spike signals, whose sequences exhibit strong sparsity, providing advantages such as low energy consumption and high efficiency. In this paper, we construct a framework to evaluate the homomorphic computation of SNN named FHE-DiSNN that enables SNN to achieve good prediction performance on encrypted data. First, benefitting from the discrete nature of spike signals, our proposed model avoids the errors introduced by discretizing activation functions. Second, by applying bootstrapping, we design new private preserving functions FHE-Fire and FHE-Reset, through which noise can be refreshed, allowing us to evaluate SNN for an arbitrary number of operations. Furthermore, We improve the computational efficiency of FHE-DiSNN while maintaining a high level of accuracy. Finally, we evaluate our model on the MNIST dataset. The experiments show that FHE-DiSNN with 30 neurons in the hidden layer achieves a minimum prediction accuracy of 94.4%. Under optimal parameters, it achieves a 95.1% accuracy, with only a 0.6% decrease compared to the original SNN (95.7%). These results demonstrate the superiority of SNN over second-generation neural networks for homomorphic evaluation. ",
    "url": "https://arxiv.org/abs/2308.12529",
    "authors": [
      "Pengbo Li",
      "Ting Gao",
      "Huifang Huang",
      "Jiani Cheng",
      "Shuhong Gao",
      "Zhigang Zeng",
      "Jinqiao Duan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2308.12530",
    "title": "SieveNet: Selecting Point-Based Features for Mesh Networks",
    "abstract": "Meshes are widely used in 3D computer vision and graphics, but their irregular topology poses challenges in applying them to existing neural network architectures. Recent advances in mesh neural networks turn to remeshing and push the boundary of pioneer methods that solely take the raw meshes as input. Although the remeshing offers a regular topology that significantly facilitates the design of mesh network architectures, features extracted from such remeshed proxies may struggle to retain the underlying geometry faithfully, limiting the subsequent neural network's capacity. To address this issue, we propose SieveNet, a novel paradigm that takes into account both the regular topology and the exact geometry. Specifically, this method utilizes structured mesh topology from remeshing and accurate geometric information from distortion-aware point sampling on the surface of the original mesh. Furthermore, our method eliminates the need for hand-crafted feature engineering and can leverage off-the-shelf network architectures such as the vision transformer. Comprehensive experimental results on classification and segmentation tasks well demonstrate the effectiveness and superiority of our method. ",
    "url": "https://arxiv.org/abs/2308.12530",
    "authors": [
      "Shengchao Yuan",
      "Yishun Dou",
      "Rui Shi",
      "Bingbing Ni",
      "Zhong Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12531",
    "title": "CARE: Co-Attention Network for Joint Entity and Relation Extraction",
    "abstract": "Joint entity and relation extraction is the fundamental task of information extraction, consisting of two subtasks: named entity recognition and relation extraction. Most existing joint extraction methods suffer from issues of feature confusion or inadequate interaction between two subtasks. In this work, we propose a Co-Attention network for joint entity and Relation Extraction (CARE). Our approach involves learning separate representations for each subtask, aiming to avoid feature overlap. At the core of our approach is the co-attention module that captures two-way interaction between two subtasks, allowing the model to leverage entity information for relation prediction and vice versa, thus promoting mutual enhancement. Extensive experiments on three joint entity-relation extraction benchmark datasets (NYT, WebNLG and SciERC) show that our proposed model achieves superior performance, surpassing existing baseline models. ",
    "url": "https://arxiv.org/abs/2308.12531",
    "authors": [
      "Wenjun Kong",
      "Yamei Xia"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2308.12533",
    "title": "Pre-trained Model-based Automated Software Vulnerability Repair: How Far  are We?",
    "abstract": "Various approaches are proposed to help under-resourced security researchers to detect and analyze software vulnerabilities. It is still incredibly time-consuming and labor-intensive for security researchers to fix vulnerabilities. The time lag between reporting and fixing a vulnerability causes software systems to suffer from significant exposure to possible attacks. Recently, some techniques have proposed applying pre-trained models to fix security vulnerabilities and have proved their success in improving repair accuracy. However, the effectiveness of existing pre-trained models has not been systematically analyzed, and little is known about their advantages and disadvantages. To bridge this gap, we perform the first extensive study on applying various pre-trained models to vulnerability repair. The results show that studied pre-trained models consistently outperform the state-of-the-art technique VRepair with a prediction accuracy of 32.94%~44.96%. We also investigate the impact of major phases in the vulnerability repair workflow. Surprisingly, a simplistic approach adopting transfer learning improves the prediction accuracy of pre-trained models by 9.40% on average. Besides, we provide additional discussion to illustrate the capacity and limitations of pre-trained models. Finally, we further pinpoint various practical guidelines for advancing pre-trained model-based vulnerability repair. Our study highlights the promising future of adopting pre-trained models to patch real-world vulnerabilities. ",
    "url": "https://arxiv.org/abs/2308.12533",
    "authors": [
      "Quanjun Zhang",
      "Chunrong Fang",
      "Bowen Yu",
      "Weisong Sun",
      "Tongke Zhang",
      "Zhenyu Chen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2308.12534",
    "title": "Channel and Spatial Relation-Propagation Network for RGB-Thermal  Semantic Segmentation",
    "abstract": "RGB-Thermal (RGB-T) semantic segmentation has shown great potential in handling low-light conditions where RGB-based segmentation is hindered by poor RGB imaging quality. The key to RGB-T semantic segmentation is to effectively leverage the complementarity nature of RGB and thermal images. Most existing algorithms fuse RGB and thermal information in feature space via concatenation, element-wise summation, or attention operations in either unidirectional enhancement or bidirectional aggregation manners. However, they usually overlook the modality gap between RGB and thermal images during feature fusion, resulting in modality-specific information from one modality contaminating the other. In this paper, we propose a Channel and Spatial Relation-Propagation Network (CSRPNet) for RGB-T semantic segmentation, which propagates only modality-shared information across different modalities and alleviates the modality-specific information contamination issue. Our CSRPNet first performs relation-propagation in channel and spatial dimensions to capture the modality-shared features from the RGB and thermal features. CSRPNet then aggregates the modality-shared features captured from one modality with the input feature from the other modality to enhance the input feature without the contamination issue. While being fused together, the enhanced RGB and thermal features will be also fed into the subsequent RGB or thermal feature extraction layers for interactive feature fusion, respectively. We also introduce a dual-path cascaded feature refinement module that aggregates multi-layer features to produce two refined features for semantic and boundary prediction. Extensive experimental results demonstrate that CSRPNet performs favorably against state-of-the-art algorithms. ",
    "url": "https://arxiv.org/abs/2308.12534",
    "authors": [
      "Zikun Zhou",
      "Shukun Wu",
      "Guoqing Zhu",
      "Hongpeng Wang",
      "Zhenyu He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12538",
    "title": "Mutual-Guided Dynamic Network for Image Fusion",
    "abstract": "Image fusion aims to generate a high-quality image from multiple images captured under varying conditions. The key problem of this task is to preserve complementary information while filtering out irrelevant information for the fused result. However, existing methods address this problem by leveraging static convolutional neural networks (CNNs), suffering two inherent limitations during feature extraction, i.e., being unable to handle spatial-variant contents and lacking guidance from multiple inputs. In this paper, we propose a novel mutual-guided dynamic network (MGDN) for image fusion, which allows for effective information utilization across different locations and inputs. Specifically, we design a mutual-guided dynamic filter (MGDF) for adaptive feature extraction, composed of a mutual-guided cross-attention (MGCA) module and a dynamic filter predictor, where the former incorporates additional guidance from different inputs and the latter generates spatial-variant kernels for different locations. In addition, we introduce a parallel feature fusion (PFF) module to effectively fuse local and global information of the extracted features. To further reduce the redundancy among the extracted features while simultaneously preserving their shared structural information, we devise a novel loss function that combines the minimization of normalized mutual information (NMI) with an estimated gradient mask. Experimental results on five benchmark datasets demonstrate that our proposed method outperforms existing methods on four image fusion tasks. The code and model are publicly available at: https://github.com/Guanys-dar/MGDN. ",
    "url": "https://arxiv.org/abs/2308.12538",
    "authors": [
      "Yuanshen Guan",
      "Ruikang Xu",
      "Mingde Yao",
      "Lizhi Wang",
      "Zhiwei Xiong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12550",
    "title": "Optimal and Robust Fault Tolerant Control of Wind Turbines Working under  Sensor, Actuator, and System Faults",
    "abstract": "Motivated by the increasing concerns over environmental challenges such as global warming and the exhaustion of fossil-fuel reserves, the renewable energy industry has become the most demanded electrical energy production source worldwide. In this context, wind energy conversion systems (WECSs) are the most dominant and fastest-growing alternative energy production technologies, playing an increasingly vital role in renewable power generation. To tackle the pitch control problem of WECSs, this thesis proposes an optimal fault-tolerant fractional-order pitch control strategy for pitch angle regulation of WT blades subjected to sensor, actuator, and system faults. To showcast the effects of faults, changes in the system parameters are considered as a result of sensor, actuator and system faults with various levels of severity.} Furthermore, taking advantage of the favourable merits of higher-order SMCs and fractional calculus, this thesis develops a fault-tolerant fractional-calculus-based higher-order sliding mode controller for optimum rotor speed tracking and power production maximization of WECSs. \\textcolor{black}{The partial loss of the generator output torque is considered as an actuator failure, leading to loss of partial actuation power. Moreover, active fault-tolerant fractional-order higher-order SMC strategies are developed for rotor current regulation and speed trajectory tracking of doubly-fed induction generator (DFIG) -driven WECSs subjected to model uncertainties and rotor current sensor faults. The developed controllers are augmented with two state observers, an algebraic state estimator and a sliding mode observer, to estimate the rotor current dynamics during sensors' faults. ",
    "url": "https://arxiv.org/abs/2308.12550",
    "authors": [
      "Yashar Mousavi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2308.12560",
    "title": "NOVA: NOvel View Augmentation for Neural Composition of Dynamic Objects",
    "abstract": "We propose a novel-view augmentation (NOVA) strategy to train NeRFs for photo-realistic 3D composition of dynamic objects in a static scene. Compared to prior work, our framework significantly reduces blending artifacts when inserting multiple dynamic objects into a 3D scene at novel views and times; achieves comparable PSNR without the need for additional ground truth modalities like optical flow; and overall provides ease, flexibility, and scalability in neural composition. Our codebase is on GitHub. ",
    "url": "https://arxiv.org/abs/2308.12560",
    "authors": [
      "Dakshit Agrawal",
      "Jiajie Xu",
      "Siva Karthik Mustikovela",
      "Ioannis Gkioulekas",
      "Ashish Shrivastava",
      "Yuning Chai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12563",
    "title": "Multivariate Time-Series Anomaly Detection with Contaminated Data:  Application to Physiological Signals",
    "abstract": "Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data are contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities (aka noise) present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data that can be considered as a surrogate of the pure normal data, and an Anomaly Scoring module to detect anomalies. Our extensive experiments conducted on three widely used physiological datasets conclusively demonstrate that our approach surpasses existing methodologies, thus establishing a new state-of-the-art performance in the field. ",
    "url": "https://arxiv.org/abs/2308.12563",
    "authors": [
      "Thi Kieu Khanh Ho",
      "Narges Armanfard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2308.12570",
    "title": "StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map  Construction",
    "abstract": "High-Definition (HD) maps are essential for the safety of autonomous driving systems. While existing techniques employ camera images and onboard sensors to generate vectorized high-precision maps, they are constrained by their reliance on single-frame input. This approach limits their stability and performance in complex scenarios such as occlusions, largely due to the absence of temporal information. Moreover, their performance diminishes when applied to broader perception ranges. In this paper, we present StreamMapNet, a novel online mapping pipeline adept at long-sequence temporal modeling of videos. StreamMapNet employs multi-point attention and temporal information which empowers the construction of large-range local HD maps with high stability and further addresses the limitations of existing methods. Furthermore, we critically examine widely used online HD Map construction benchmark and datasets, Argoverse2 and nuScenes, revealing significant bias in the existing evaluation protocols. We propose to resplit the benchmarks according to geographical spans, promoting fair and precise evaluations. Experimental results validate that StreamMapNet significantly outperforms existing methods across all settings while maintaining an online inference speed of $14.2$ FPS. ",
    "url": "https://arxiv.org/abs/2308.12570",
    "authors": [
      "Tianyuan Yuan",
      "Yicheng Liu",
      "Yue Wang",
      "Yilun Wang",
      "Hang Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12575",
    "title": "Hypergraph Convolutional Networks for Fine-grained ICU Patient  Similarity Analysis and Risk Prediction",
    "abstract": "The Intensive Care Unit (ICU) is one of the most important parts of a hospital, which admits critically ill patients and provides continuous monitoring and treatment. Various patient outcome prediction methods have been attempted to assist healthcare professionals in clinical decision-making. Existing methods focus on measuring the similarity between patients using deep neural networks to capture the hidden feature structures. However, the higher-order relationships are ignored, such as patient characteristics (e.g., diagnosis codes) and their causal effects on downstream clinical predictions. In this paper, we propose a novel Hypergraph Convolutional Network that allows the representation of non-pairwise relationships among diagnosis codes in a hypergraph to capture the hidden feature structures so that fine-grained patient similarity can be calculated for personalized mortality risk prediction. Evaluation using a publicly available eICU Collaborative Research Database indicates that our method achieves superior performance over the state-of-the-art models on mortality risk prediction. Moreover, the results of several case studies demonstrated the effectiveness of constructing graph networks in providing good transparency and robustness in decision-making. ",
    "url": "https://arxiv.org/abs/2308.12575",
    "authors": [
      "Yuxi Liu",
      "Zhenhao Zhang",
      "Shaowen Qin",
      "Flora D. Salim",
      "Antonio Jimeno Yepes",
      "Jun Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12577",
    "title": "REB: Reducing Biases in Representation for Industrial Anomaly Detection",
    "abstract": "Existing K-nearest neighbor (KNN) retrieval-based methods usually conduct industrial anomaly detection in two stages: obtain feature representations with a pre-trained CNN model and perform distance measures for defect detection. However, the features are not fully exploited as they ignore domain bias and the difference of local density in feature space, which limits the detection performance. In this paper, we propose Reducing Biases (REB) in representation by considering the domain bias of the pre-trained model and building a self-supervised learning task for better domain adaption with a defect generation strategy (DefectMaker) imitating the natural defects. Additionally, we propose a local density KNN (LDKNN) to reduce the local density bias and obtain effective anomaly detection. We achieve a promising result of 99.5\\% AUROC on the widely used MVTec AD benchmark. We also achieve 88.0\\% AUROC on the challenging MVTec LOCO AD dataset and bring an improvement of 4.7\\% AUROC to the state-of-the-art result. All results are obtained with smaller backbone networks such as Vgg11 and Resnet18, which indicates the effectiveness and efficiency of REB for practical industrial applications. ",
    "url": "https://arxiv.org/abs/2308.12577",
    "authors": [
      "Shuai Lyu",
      "Dongmei Mo",
      "Waikeung Wong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12578",
    "title": "Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in  Large Language Models",
    "abstract": "Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs. This paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. It posits that individuals' explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. We propose a two-stage approach and discover a parallel phenomenon in LLMs known as \"re-judge inconsistency\" in social bias. In the initial stage, the LLM is tasked with automatically completing statements, potentially incorporating implicit social bias. However, in the subsequent stage, the same LLM re-judges the biased statement generated by itself but contradicts it. We propose that this re-judge inconsistency can be similar to the inconsistency between human's unaware implicit social bias and their aware explicit social bias. Experimental investigations on ChatGPT and GPT-4 concerning common gender biases examined in psychology corroborate the highly stable nature of the re-judge inconsistency. This finding may suggest that diverse cognitive constructs emerge as LLMs' capabilities strengthen. Consequently, leveraging psychological theories can provide enhanced insights into the underlying mechanisms governing the expressions of explicit and implicit constructs in LLMs. ",
    "url": "https://arxiv.org/abs/2308.12578",
    "authors": [
      "Yachao Zhao",
      "Bo Wang",
      "Dongming Zhao",
      "Kun Huang",
      "Yan Wang",
      "Ruifang He",
      "Yuexian Hou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12581",
    "title": "A Huber Loss Minimization Approach to Byzantine Robust Federated  Learning",
    "abstract": "Federated learning systems are susceptible to adversarial attacks. To combat this, we introduce a novel aggregator based on Huber loss minimization, and provide a comprehensive theoretical analysis. Under independent and identically distributed (i.i.d) assumption, our approach has several advantages compared to existing methods. Firstly, it has optimal dependence on $\\epsilon$, which stands for the ratio of attacked clients. Secondly, our approach does not need precise knowledge of $\\epsilon$. Thirdly, it allows different clients to have unequal data sizes. We then broaden our analysis to include non-i.i.d data, such that clients have slightly different distributions. ",
    "url": "https://arxiv.org/abs/2308.12581",
    "authors": [
      "Puning Zhao",
      "Fei Yu",
      "Zhiguo Wan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12590",
    "title": "Self-supervised Learning of Implicit Shape Representation with Dense  Correspondence for Deformable Objects",
    "abstract": "Learning 3D shape representation with dense correspondence for deformable objects is a fundamental problem in computer vision. Existing approaches often need additional annotations of specific semantic domain, e.g., skeleton poses for human bodies or animals, which require extra annotation effort and suffer from error accumulation, and they are limited to specific domain. In this paper, we propose a novel self-supervised approach to learn neural implicit shape representation for deformable objects, which can represent shapes with a template shape and dense correspondence in 3D. Our method does not require the priors of skeleton and skinning weight, and only requires a collection of shapes represented in signed distance fields. To handle the large deformation, we constrain the learned template shape in the same latent space with the training shapes, design a new formulation of local rigid constraint that enforces rigid transformation in local region and addresses local reflection issue, and present a new hierarchical rigid constraint to reduce the ambiguity due to the joint learning of template shape and correspondences. Extensive experiments show that our model can represent shapes with large deformations. We also show that our shape representation can support two typical applications, such as texture transfer and shape editing, with competitive performance. The code and models are available at https://iscas3dv.github.io/deformshape ",
    "url": "https://arxiv.org/abs/2308.12590",
    "authors": [
      "Baowen Zhang",
      "Jiahe Li",
      "Xiaoming Deng",
      "Yinda Zhang",
      "Cuixia Ma",
      "Hongan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12600",
    "title": "PoseSync: Robust pose based video synchronization",
    "abstract": "Pose based video sychronization can have applications in multiple domains such as gameplay performance evaluation, choreography or guiding athletes. The subject's actions could be compared and evaluated against those performed by professionals side by side. In this paper, we propose an end to end pipeline for synchronizing videos based on pose. The first step crops the region where the person present in the image followed by pose detection on the cropped image. This is followed by application of Dynamic Time Warping(DTW) on angle/ distance measures between the pose keypoints leading to a scale and shift invariant pose matching pipeline. ",
    "url": "https://arxiv.org/abs/2308.12600",
    "authors": [
      "Rishit Javia",
      "Falak Shah",
      "Shivam Dave"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12605",
    "title": "APLA: Additional Perturbation for Latent Noise with Adversarial Training  Enables Consistency",
    "abstract": "Diffusion models have exhibited promising progress in video generation. However, they often struggle to retain consistent details within local regions across frames. One underlying cause is that traditional diffusion models approximate Gaussian noise distribution by utilizing predictive noise, without fully accounting for the impact of inherent information within the input itself. Additionally, these models emphasize the distinction between predictions and references, neglecting information intrinsic to the videos. To address this limitation, inspired by the self-attention mechanism, we propose a novel text-to-video (T2V) generation network structure based on diffusion models, dubbed Additional Perturbation for Latent noise with Adversarial training (APLA). Our approach only necessitates a single video as input and builds upon pre-trained stable diffusion networks. Notably, we introduce an additional compact network, known as the Video Generation Transformer (VGT). This auxiliary component is designed to extract perturbations from the inherent information contained within the input, thereby refining inconsistent pixels during temporal predictions. We leverage a hybrid architecture of transformers and convolutions to compensate for temporal intricacies, enhancing consistency between different frames within the video. Experiments demonstrate a noticeable improvement in the consistency of the generated videos both qualitatively and quantitatively. ",
    "url": "https://arxiv.org/abs/2308.12605",
    "authors": [
      "Yupu Yao",
      "Shangqi Deng",
      "Zihan Cao",
      "Harry Zhang",
      "Liang-Jian Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12612",
    "title": "Try with Simpler -- An Evaluation of Improved Principal Component  Analysis in Log-based Anomaly Detection",
    "abstract": "The rapid growth of deep learning (DL) has spurred interest in enhancing log-based anomaly detection. This approach aims to extract meaning from log events (log message templates) and develop advanced DL models for anomaly detection. However, these DL methods face challenges like heavy reliance on training data, labels, and computational resources due to model complexity. In contrast, traditional machine learning and data mining techniques are less data-dependent and more efficient but less effective than DL. To make log-based anomaly detection more practical, the goal is to enhance traditional techniques to match DL's effectiveness. Previous research in a different domain (linking questions on Stack Overflow) suggests that optimized traditional techniques can rival state-of-the-art DL methods. Drawing inspiration from this concept, we conducted an empirical study. We optimized the unsupervised PCA (Principal Component Analysis), a traditional technique, by incorporating lightweight semantic-based log representation. This addresses the issue of unseen log events in training data, enhancing log representation. Our study compared seven log-based anomaly detection methods, including four DL-based, two traditional, and the optimized PCA technique, using public and industrial datasets. Results indicate that the optimized unsupervised PCA technique achieves similar effectiveness to advanced supervised/semi-supervised DL methods while being more stable with limited training data and resource-efficient. This demonstrates the adaptability and strength of traditional techniques through small yet impactful adaptations. ",
    "url": "https://arxiv.org/abs/2308.12612",
    "authors": [
      "Lin Yang",
      "Junjie Chen",
      "Zhihao Gong",
      "Shutao Gao",
      "Hongyu Zhang",
      "Yue Kang",
      "Huaan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2308.12614",
    "title": "Obstruction characterization of co-TT graphs",
    "abstract": "Threshold tolerance graphs and their complement graphs ( known as co-TT graphs) were introduced by Monma, Reed and Trotter[24]. Introducing the concept of negative interval Hell et al.[19] defined signed-interval bigraphs/digraphs and have shown that they are equivalent to several seemingly different classes of bigraphs/digraphs. They have also shown that co-TT graphs are equivalent to symmetric signed-interval digraphs. In this paper we characterize signed-interval bigraphs and signed-interval graphs respectively in terms of their biadjacency matrices and adjacency matrices. Finally, based on the geometric representation of signed-interval graphs we have setteled the open problem of forbidden induced subgraph characterization of co-TT graphs posed by Monma, Reed and Trotter in the same paper. ",
    "url": "https://arxiv.org/abs/2308.12614",
    "authors": [
      "Ashok Kumar Das",
      "Indrajit Paul"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2308.12621",
    "title": "Hydrogen jet diffusion modeling by using physics-informed graph neural  network and sparsely-distributed sensor data",
    "abstract": "Efficient modeling of jet diffusion during accidental release is critical for operation and maintenance management of hydrogen facilities. Deep learning has proven effective for concentration prediction in gas jet diffusion scenarios. Nonetheless, its reliance on extensive simulations as training data and its potential disregard for physical laws limit its applicability to unseen accidental scenarios. Recently, physics-informed neural networks (PINNs) have emerged to reconstruct spatial information by using data from sparsely-distributed sensors which are easily collected in real-world applications. However, prevailing approaches use the fully-connected neural network as the backbone without considering the spatial dependency of sensor data, which reduces the accuracy of concentration prediction. This study introduces the physics-informed graph deep learning approach (Physic_GNN) for efficient and accurate hydrogen jet diffusion prediction by using sparsely-distributed sensor data. Graph neural network (GNN) is used to model the spatial dependency of such sensor data by using graph nodes at which governing equations describing the physical law of hydrogen jet diffusion are immediately solved. The computed residuals are then applied to constrain the training process. Public experimental data of hydrogen jet is used to compare the accuracy and efficiency between our proposed approach Physic_GNN and state-of-the-art PINN. The results demonstrate our Physic_GNN exhibits higher accuracy and physical consistency of centerline concentration prediction given sparse concentration compared to PINN and more efficient compared to OpenFOAM. The proposed approach enables accurate and robust real-time spatial consequence reconstruction and underlying physical mechanisms analysis by using sparse sensor data. ",
    "url": "https://arxiv.org/abs/2308.12621",
    "authors": [
      "Xinqi Zhang",
      "Jihao Shi",
      "Junjie Li",
      "Guoming Chen"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2308.12627",
    "title": "Introducing a New Alert Data Set for Multi-Step Attack Analysis",
    "abstract": "Intrusion detection systems (IDS) reinforce cyber defense by autonomously monitoring various data sources for traces of attacks. However, IDSs are also infamous for frequently raising false positives and alerts that are difficult to interpret without context. This results in high workloads on security operators who need to manually verify all reported alerts, often leading to fatigue and incorrect decisions. To generate more meaningful alerts and alleviate these issues, the research domain focused on multi-step attack analysis proposes approaches for filtering, clustering, and correlating IDS alerts, as well as generation of attack graphs. Unfortunately, existing data sets are outdated, unreliable, narrowly focused, or only suitable for IDS evaluation. Since hardly any suitable benchmark data sets are publicly available, researchers often resort to private data sets that prevent reproducibility of evaluations. We therefore generate a new alert data set that we publish alongside this paper. The data set contains alerts from three distinct IDSs monitoring eight executions of a multi-step attack as well as simulations of normal user behavior. To illustrate the potential of our data set, we experiment with alert prioritization as well as two open-source tools for meta-alert generation and attack graph extraction. ",
    "url": "https://arxiv.org/abs/2308.12627",
    "authors": [
      "Max Landauer",
      "Florian Skopik",
      "Markus Wurzenberger"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2308.12628",
    "title": "TimeLighting: Guidance-enhanced Exploration of 2D Projections of  Temporal Graphs",
    "abstract": "In temporal (or event-based) networks, time is a continuous axis, with real-valued time coordinates for each node and edge. Computing a layout for such graphs means embedding the node trajectories and edge surfaces over time in a 2D + t space, known as the space-time cube. Currently, these space-time cube layouts are visualized through animation or by slicing the cube at regular intervals. However, both techniques present problems ranging from sub-par performance on some tasks to loss of precision. In this paper, we present TimeLighting, a novel visual analytics approach to visualize and explore temporal graphs embedded in the space-time cube. Our interactive approach highlights the node trajectories and their mobility over time, visualizes node \"aging\", and provides guidance to support users during exploration. We evaluate our approach through two case studies, showing the system's efficacy in identifying temporal patterns and the role of the guidance features in the exploration process. ",
    "url": "https://arxiv.org/abs/2308.12628",
    "authors": [
      "Velitchko Filipov",
      "Davide Ceneda",
      "Daniel Archambault",
      "Alessio Arleo"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2308.12636",
    "title": "Exploring Transferability of Multimodal Adversarial Samples for  Vision-Language Pre-training Models with Contrastive Learning",
    "abstract": "Vision-language pre-training models (VLP) are vulnerable, especially to multimodal adversarial samples, which can be crafted by adding imperceptible perturbations on both original images and texts. However, under the black-box setting, there have been no works to explore the transferability of multimodal adversarial attacks against the VLP models. In this work, we take CLIP as the surrogate model and propose a gradient-based multimodal attack method to generate transferable adversarial samples against the VLP models. By applying the gradient to optimize the adversarial images and adversarial texts simultaneously, our method can better search for and attack the vulnerable images and text information pairs. To improve the transferability of the attack, we utilize contrastive learning including image-text contrastive learning and intra-modal contrastive learning to have a more generalized understanding of the underlying data distribution and mitigate the overfitting of the surrogate model so that the generated multimodal adversarial samples have a higher transferability for VLP models. Extensive experiments validate the effectiveness of the proposed method. ",
    "url": "https://arxiv.org/abs/2308.12636",
    "authors": [
      "Youze Wang",
      "Wenbo Hu",
      "Yinpeng Dong",
      "Richang Hong"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2308.12644",
    "title": "Evolutionary Dynamic Optimization Laboratory: A MATLAB Optimization  Platform for Education and Experimentation in Dynamic Environments",
    "abstract": "Many real-world optimization problems possess dynamic characteristics. Evolutionary dynamic optimization algorithms (EDOAs) aim to tackle the challenges associated with dynamic optimization problems. Looking at the existing works, the results reported for a given EDOA can sometimes be considerably different. This issue occurs because the source codes of many EDOAs, which are usually very complex algorithms, have not been made publicly available. Indeed, the complexity of components and mechanisms used in many EDOAs makes their re-implementation error-prone. In this paper, to assist researchers in performing experiments and comparing their algorithms against several EDOAs, we develop an open-source MATLAB platform for EDOAs, called Evolutionary Dynamic Optimization LABoratory (EDOLAB). This platform also contains an education module that can be used for educational purposes. In the education module, the user can observe a) a 2-dimensional problem space and how its morphology changes after each environmental change, b) the behaviors of individuals over time, and c) how the EDOA reacts to environmental changes and tries to track the moving optimum. In addition to being useful for research and education purposes, EDOLAB can also be used by practitioners to solve their real-world problems. The current version of EDOLAB includes 25 EDOAs and three fully-parametric benchmark generators. The MATLAB source code for EDOLAB is publicly available and can be accessed from [https://github.com/EDOLAB-platform/EDOLAB-MATLAB]. ",
    "url": "https://arxiv.org/abs/2308.12644",
    "authors": [
      "Mai Peng",
      "Zeneng She",
      "Delaram Yazdani",
      "Danial Yazdani",
      "Wenjian Luo",
      "Changhe Li",
      "Juergen Branke",
      "Trung Thanh Nguyen",
      "Amir H. Gandomi",
      "Yaochu Jin",
      "Xin Yao"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Mathematical Software (cs.MS)"
    ]
  },
  {
    "id": "arXiv:2308.12647",
    "title": "Multitasking Evolutionary Algorithm Based on Adaptive Seed Transfer for  Combinatorial Problem",
    "abstract": "Evolutionary computing (EC) is widely used in dealing with combinatorial optimization problems (COP). Traditional EC methods can only solve a single task in a single run, while real-life scenarios often need to solve multiple COPs simultaneously. In recent years, evolutionary multitasking optimization (EMTO) has become an emerging topic in the EC community. And many methods have been designed to deal with multiple COPs concurrently through exchanging knowledge. However, many-task optimization, cross-domain knowledge transfer, and negative transfer are still significant challenges in this field. A new evolutionary multitasking algorithm based on adaptive seed transfer (MTEA-AST) is developed for multitasking COPs in this work. First, a dimension unification strategy is proposed to unify the dimensions of different tasks. And then, an adaptive task selection strategy is designed to capture the similarity between the target task and other online optimization tasks. The calculated similarity is exploited to select suitable source tasks for the target one and determine the transfer strength. Next, a task transfer strategy is established to select seeds from source tasks and correct unsuitable knowledge in seeds to suppress negative transfer. Finally, the experimental results indicate that MTEA-AST can adaptively transfer knowledge in both same-domain and cross-domain many-task environments. And the proposed method shows competitive performance compared to other state-of-the-art EMTOs in experiments consisting of four COPs. ",
    "url": "https://arxiv.org/abs/2308.12647",
    "authors": [
      "Haoyuan Lv",
      "Ruochen Liu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2308.12651",
    "title": "Sink Location Problems in Dynamic Flow Grid Networks",
    "abstract": "A dynamic flow network consists of a directed graph, where nodes called sources represent locations of evacuees, and nodes called sinks represent locations of evacuation facilities. Each source and each sink are given supply representing the number of evacuees and demand representing the maximum number of acceptable evacuees, respectively. Each edge is given capacity and transit time. Here, the capacity of an edge bounds the rate at which evacuees can enter the edge per unit time, and the transit time represents the time which evacuees take to travel across the edge. The evacuation completion time is the minimum time at which each evacuees can arrive at one of the evacuation facilities. Given a dynamic flow network without sinks, once sinks are located on some nodes or edges, the evacuation completion time for this sink location is determined. We then consider the problem of locating sinks to minimize the evacuation completion time, called the sink location problem. The problems have been given polynomial-time algorithms only for limited networks such as paths, cycles, and trees, but no polynomial-time algorithms are known for more complex network classes. In this paper, we prove that the 1-sink location problem can be solved in polynomial-time when an input network is a grid with uniform edge capacity and transit time. ",
    "url": "https://arxiv.org/abs/2308.12651",
    "authors": [
      "Yuya Higashikawa",
      "Ayano Nishii",
      "Junichi Teruyama",
      "Yuki Tokuni"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2308.12653",
    "title": "Shortest Odd Paths in Undirected Graphs with Conservative Weight  Functions",
    "abstract": "We consider the Shortest Odd Path problem, where given an undirected graph $G$, a weight function on its edges, and two vertices $s$ and $t$ in $G$, the aim is to find an $(s,t)$-path with odd length and, among all such paths, of minimum weight. For the case when the weight function is conservative, i.e., when every cycle has non-negative total weight, the complexity of the Shortest Odd Path problem had been open for 20 years, and was recently shown to be NP-hard. We give a polynomial-time algorithm for the special case when the weight function is conservative and the set $E^-$ of negative-weight edges forms a single tree. Our algorithm exploits the strong connection between Shortest Odd Path and the problem of finding two internally vertex-disjoint paths between two terminals in an undirected edge-weighted graph. It also relies on solving an intermediary problem variant called Shortest Parity-Constrained Odd Path where for certain edges we have parity constraints on their position along the path. Also, we exhibit two FPT algorithms for solving Shortest Odd Path in graphs with conservative weight functions. The first FPT algorithm is parameterized by $|E^-|$, the number of negative edges, or more generally, by the maximum size of a matching in the subgraph of $G$ spanned by $E^-$. Our second FPT algorithm is parameterized by the treewidth of $G$. ",
    "url": "https://arxiv.org/abs/2308.12653",
    "authors": [
      "Alp\u00e1r J\u00fcttner",
      "Csaba Kir\u00e1ly",
      "Lydia Mirabel Mendoza-Cadena",
      "Gyula Pap",
      "Ildik\u00f3 Schlotter",
      "Yutaro Yamaguchi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2308.12659",
    "title": "kTrans: Knowledge-Aware Transformer for Binary Code Embedding",
    "abstract": "Binary Code Embedding (BCE) has important applications in various reverse engineering tasks such as binary code similarity detection, type recovery, control-flow recovery and data-flow analysis. Recent studies have shown that the Transformer model can comprehend the semantics of binary code to support downstream tasks. However, existing models overlooked the prior knowledge of assembly language. In this paper, we propose a novel Transformer-based approach, namely kTrans, to generate knowledge-aware binary code embedding. By feeding explicit knowledge as additional inputs to the Transformer, and fusing implicit knowledge with a novel pre-training task, kTrans provides a new perspective to incorporating domain knowledge into a Transformer framework. We inspect the generated embeddings with outlier detection and visualization, and also apply kTrans to 3 downstream tasks: Binary Code Similarity Detection (BCSD), Function Type Recovery (FTR) and Indirect Call Recognition (ICR). Evaluation results show that kTrans can generate high-quality binary code embeddings, and outperforms state-of-the-art (SOTA) approaches on downstream tasks by 5.2%, 6.8%, and 12.6% respectively. kTrans is publicly available at: https://github.com/Learner0x5a/kTrans-release ",
    "url": "https://arxiv.org/abs/2308.12659",
    "authors": [
      "Wenyu Zhu",
      "Hao Wang",
      "Yuchen Zhou",
      "Jiaming Wang",
      "Zihan Sha",
      "Zeyu Gao",
      "Chao Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2308.12661",
    "title": "Don't Look into the Sun: Adversarial Solarization Attacks on Image  Classifiers",
    "abstract": "Assessing the robustness of deep neural networks against out-of-distribution inputs is crucial, especially in safety-critical domains like autonomous driving, but also in safety systems where malicious actors can digitally alter inputs to circumvent safety guards. However, designing effective out-of-distribution tests that encompass all possible scenarios while preserving accurate label information is a challenging task. Existing methodologies often entail a compromise between variety and constraint levels for attacks and sometimes even both. In a first step towards a more holistic robustness evaluation of image classification models, we introduce an attack method based on image solarization that is conceptually straightforward yet avoids jeopardizing the global structure of natural images independent of the intensity. Through comprehensive evaluations of multiple ImageNet models, we demonstrate the attack's capacity to degrade accuracy significantly, provided it is not integrated into the training augmentations. Interestingly, even then, no full immunity to accuracy deterioration is achieved. In other settings, the attack can often be simplified into a black-box attack with model-independent parameters. Defenses against other corruptions do not consistently extend to be effective against our specific attack. Project website: https://github.com/paulgavrikov/adversarial_solarization ",
    "url": "https://arxiv.org/abs/2308.12661",
    "authors": [
      "Paul Gavrikov",
      "Janis Keuper"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12673",
    "title": "Masked Feature Modelling: Feature Masking for the Unsupervised  Pre-training of a Graph Attention Network Block for Bottom-up Video Event  Recognition",
    "abstract": "In this paper, we introduce Masked Feature Modelling (MFM), a novel approach for the unsupervised pre-training of a Graph Attention Network (GAT) block. MFM utilizes a pretrained Visual Tokenizer to reconstruct masked features of objects within a video, leveraging the MiniKinetics dataset. We then incorporate the pre-trained GAT block into a state-of-the-art bottom-up supervised video-event recognition architecture, ViGAT, to improve the model's starting point and overall accuracy. Experimental evaluations on the YLI-MED dataset demonstrate the effectiveness of MFM in improving event recognition performance. ",
    "url": "https://arxiv.org/abs/2308.12673",
    "authors": [
      "Dimitrios Daskalakis",
      "Nikolaos Gkalelis",
      "Vasileios Mezaris"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2308.12697",
    "title": "Prompt-Enhanced Software Vulnerability Detection Using ChatGPT",
    "abstract": "With the increase in software vulnerabilities that cause significant economic and social losses, automatic vulnerability detection has become essential in software development and maintenance. Recently, large language models (LLMs) like GPT have received considerable attention due to their stunning intelligence, and some studies consider using ChatGPT for vulnerability detection. However, they do not fully consider the characteristics of LLMs, since their designed questions to ChatGPT are simple without a specific prompt design tailored for vulnerability detection. This paper launches a study on the performance of software vulnerability detection using ChatGPT with different prompt designs. Firstly, we complement previous work by applying various improvements to the basic prompt. Moreover, we incorporate structural and sequential auxiliary information to improve the prompt design. Besides, we leverage ChatGPT's ability of memorizing multi-round dialogue to design suitable prompts for vulnerability detection. We conduct extensive experiments on two vulnerability datasets to demonstrate the effectiveness of prompt-enhanced vulnerability detection using ChatGPT. We also analyze the merit and demerit of using ChatGPT for vulnerability detection. ",
    "url": "https://arxiv.org/abs/2308.12697",
    "authors": [
      "Chenyuan Zhang",
      "Hao Liu",
      "Jiutian Zeng",
      "Kejing Yang",
      "Yuhong Li",
      "Hui Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2308.12698",
    "title": "Potato: A Data-Oriented Programming 3D Simulator for Large-Scale  Heterogeneous Swarm Robotics",
    "abstract": "Large-scale simulation with realistic nonlinear dynamic models is crucial for algorithms development for swarm robotics. However, existing platforms are mainly developed based on Object-Oriented Programming (OOP) and either use simple kinematic models to pursue a large number of simulating nodes or implement realistic dynamic models with limited simulating nodes. In this paper, we develop a simulator based on Data-Oriented Programming (DOP) that utilizes GPU parallel computing to achieve large-scale swarm robotic simulations. Specifically, we use a multi-process approach to simulate heterogeneous agents and leverage PyTorch with GPU to simulate homogeneous agents with a large number. We test our approach using a nonlinear quadrotor model and demonstrate that this DOP approach can maintain almost the same computational speed when quadrotors are less than 5,000. We also provide two examples to present the functionality of the platform. ",
    "url": "https://arxiv.org/abs/2308.12698",
    "authors": [
      "Jinjie Li",
      "Liang Han",
      "Haoyang Yu",
      "Zhaotian Wang",
      "Pengzhi Yang",
      "Ziwei Yan",
      "Zhang Ren"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2308.12701",
    "title": "How are We Detecting Inconsistent Method Names? An Empirical Study from  Code Review Perspective",
    "abstract": "Proper naming of methods can make program code easier to understand, and thus enhance software maintainability. Yet, developers may use inconsistent names due to poor communication or a lack of familiarity with conventions within the software development lifecycle. To address this issue, much research effort has been invested into building automatic tools that can check for method name inconsistency and recommend consistent names. However, existing datasets generally do not provide precise details about why a method name was deemed improper and required to be changed. Such information can give useful hints on how to improve the recommendation of adequate method names. Accordingly, we construct a sample method-naming benchmark, ReName4J, by matching name changes with code reviews. We then present an empirical study on how state-of-the-art techniques perform in detecting or recommending consistent and inconsistent method names based on ReName4J. The main purpose of the study is to reveal a different perspective based on reviewed names rather than proposing a complete benchmark. We find that the existing techniques underperform on our review-driven benchmark, both in inconsistent checking and the recommendation. We further identify potential biases in the evaluation of existing techniques, which future research should consider thoroughly. ",
    "url": "https://arxiv.org/abs/2308.12701",
    "authors": [
      "Kisub Kim",
      "Xin Zhou",
      "Dongsun Kim",
      "Julia Lawall",
      "Kui Liu",
      "Tegawend\u00e9 F. Bissyand\u00e9",
      "Jacques Klein",
      "Jaekwon Lee",
      "David Lo"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2308.12716",
    "title": "Solving Forward and Inverse Problems of Contact Mechanics using  Physics-Informed Neural Networks",
    "abstract": "This paper explores the ability of physics-informed neural networks (PINNs) to solve forward and inverse problems of contact mechanics for small deformation elasticity. We deploy PINNs in a mixed-variable formulation enhanced by output transformation to enforce Dirichlet and Neumann boundary conditions as hard constraints. Inequality constraints of contact problems, namely Karush-Kuhn-Tucker (KKT) type conditions, are enforced as soft constraints by incorporating them into the loss function during network training. To formulate the loss function contribution of KKT constraints, existing approaches applied to elastoplasticity problems are investigated and we explore a nonlinear complementarity problem (NCP) function, namely Fischer-Burmeister, which possesses advantageous characteristics in terms of optimization. Based on the Hertzian contact problem, we show that PINNs can serve as pure partial differential equation (PDE) solver, as data-enhanced forward model, as inverse solver for parameter identification, and as fast-to-evaluate surrogate model. Furthermore, we demonstrate the importance of choosing proper hyperparameters, e.g. loss weights, and a combination of Adam and L-BFGS-B optimizers aiming for better results in terms of accuracy and training time. ",
    "url": "https://arxiv.org/abs/2308.12716",
    "authors": [
      "T. Sahin",
      "M. von Danwitz",
      "A. Popp"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12729",
    "title": "Out of the Box Thinking: Improving Customer Lifetime Value Modelling via  Expert Routing and Game Whale Detection",
    "abstract": "Customer lifetime value (LTV) prediction is essential for mobile game publishers trying to optimize the advertising investment for each user acquisition based on the estimated worth. In mobile games, deploying microtransactions is a simple yet effective monetization strategy, which attracts a tiny group of game whales who splurge on in-game purchases. The presence of such game whales may impede the practicality of existing LTV prediction models, since game whales' purchase behaviours always exhibit varied distribution from general users. Consequently, identifying game whales can open up new opportunities to improve the accuracy of LTV prediction models. However, little attention has been paid to applying game whale detection in LTV prediction, and existing works are mainly specialized for the long-term LTV prediction with the assumption that the high-quality user features are available, which is not applicable in the UA stage. In this paper, we propose ExpLTV, a novel multi-task framework to perform LTV prediction and game whale detection in a unified way. In ExpLTV, we first innovatively design a deep neural network-based game whale detector that can not only infer the intrinsic order in accordance with monetary value, but also precisely identify high spenders (i.e., game whales) and low spenders. Then, by treating the game whale detector as a gating network to decide the different mixture patterns of LTV experts assembling, we can thoroughly leverage the shared information and scenario-specific information (i.e., game whales modelling and low spenders modelling). Finally, instead of separately designing a purchase rate estimator for two tasks, we design a shared estimator that can preserve the inner task relationships. The superiority of ExpLTV is further validated via extensive experiments on three industrial datasets. ",
    "url": "https://arxiv.org/abs/2308.12729",
    "authors": [
      "Shijie Zhang",
      "Xin Yan",
      "Xuejiao Yang",
      "Binfeng Jia",
      "Shuangyang Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12734",
    "title": "Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion",
    "abstract": "There are growing implications surrounding generative AI in the speech domain that enable voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation, thus there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. To address the above emerging issues, the DEEP-VOICE dataset is generated in this study, comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion. Presenting as a binary classification problem of whether the speech is real or AI-generated, statistical analysis of temporal audio features through t-testing reveals that there are significantly different distributions. Hyperparameter optimisation is implemented for machine learning models to identify the source of speech. Following the training of 208 individual machine learning models over 10-fold cross validation, it is found that the Extreme Gradient Boosting model can achieve an average classification accuracy of 99.3% and can classify speech in real-time, at around 0.004 milliseconds given one second of speech. All data generated for this study is released publicly for future research on AI speech detection. ",
    "url": "https://arxiv.org/abs/2308.12734",
    "authors": [
      "Jordan J. Bird",
      "Ahmad Lotfi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2308.12737",
    "title": "Asymmetric Co-Training with Explainable Cell Graph Ensembling for  Histopathological Image Classification",
    "abstract": "Convolutional neural networks excel in histopathological image classification, yet their pixel-level focus hampers explainability. Conversely, emerging graph convolutional networks spotlight cell-level features and medical implications. However, limited by their shallowness and suboptimal use of high-dimensional pixel data, GCNs underperform in multi-class histopathological image classification. To make full use of pixel-level and cell-level features dynamically, we propose an asymmetric co-training framework combining a deep graph convolutional network and a convolutional neural network for multi-class histopathological image classification. To improve the explainability of the entire framework by embedding morphological and topological distribution of cells, we build a 14-layer deep graph convolutional network to handle cell graph data. For the further utilization and dynamic interactions between pixel-level and cell-level information, we also design a co-training strategy to integrate the two asymmetric branches. Notably, we collect a private clinically acquired dataset termed LUAD7C, including seven subtypes of lung adenocarcinoma, which is rare and more challenging. We evaluated our approach on the private LUAD7C and public colorectal cancer datasets, showcasing its superior performance, explainability, and generalizability in multi-class histopathological image classification. ",
    "url": "https://arxiv.org/abs/2308.12737",
    "authors": [
      "Ziqi Yang",
      "Zhongyu Li",
      "Chen Liu",
      "Xiangde Luo",
      "Xingguang Wang",
      "Dou Xu",
      "Chaoqun Li",
      "Xiaoying Qin",
      "Meng Yang",
      "Long Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12738",
    "title": "Learning Heavily-Degraded Prior for Underwater Object Detection",
    "abstract": "Underwater object detection suffers from low detection performance because the distance and wavelength dependent imaging process yield evident image quality degradations such as haze-like effects, low visibility, and color distortions. Therefore, we commit to resolving the issue of underwater object detection with compounded environmental degradations. Typical approaches attempt to develop sophisticated deep architecture to generate high-quality images or features. However, these methods are only work for limited ranges because imaging factors are either unstable, too sensitive, or compounded. Unlike these approaches catering for high-quality images or features, this paper seeks transferable prior knowledge from detector-friendly images. The prior guides detectors removing degradations that interfere with detection. It is based on statistical observations that, the heavily degraded regions of detector-friendly (DFUI) and underwater images have evident feature distribution gaps while the lightly degraded regions of them overlap each other. Therefore, we propose a residual feature transference module (RFTM) to learn a mapping between deep representations of the heavily degraded patches of DFUI- and underwater- images, and make the mapping as a heavily degraded prior (HDP) for underwater detection. Since the statistical properties are independent to image content, HDP can be learned without the supervision of semantic labels and plugged into popular CNNbased feature extraction networks to improve their performance on underwater object detection. Without bells and whistles, evaluations on URPC2020 and UODD show that our methods outperform CNN-based detectors by a large margin. Our method with higher speeds and less parameters still performs better than transformer-based detectors. Our code and DFUI dataset can be found in https://github.com/xiaoDetection/Learning-Heavily-Degraed-Prior. ",
    "url": "https://arxiv.org/abs/2308.12738",
    "authors": [
      "Chenping Fu",
      "Xin Fan",
      "Jiewen Xiao",
      "Wanqi Yuan",
      "Risheng Liu",
      "Zhongxuan Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12740",
    "title": "Human Comprehensible Active Learning of Genome-Scale Metabolic Networks",
    "abstract": "An important application of Synthetic Biology is the engineering of the host cell system to yield useful products. However, an increase in the scale of the host system leads to huge design space and requires a large number of validation trials with high experimental costs. A comprehensible machine learning approach that efficiently explores the hypothesis space and guides experimental design is urgently needed for the Design-Build-Test-Learn (DBTL) cycle of the host cell system. We introduce a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. In contrast to numerical models, ILP-iML1515 is built on comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that reduce the experimental cost of learning gene functions in comparison to randomly selected experiments. ",
    "url": "https://arxiv.org/abs/2308.12740",
    "authors": [
      "Lun Ai",
      "Shi-Shun Liang",
      "Wang-Zhou Dai",
      "Liam Hallett",
      "Stephen H. Muggleton",
      "Geoff S. Baldwin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Molecular Networks (q-bio.MN)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2308.12743",
    "title": "Video Recommendation Using Social Network Analysis and User Viewing  Patterns",
    "abstract": "With the meteoric rise of video-on-demand (VOD) platforms, users face the challenge of sifting through an expansive sea of content to uncover shows that closely match their preferences. To address this information overload dilemma, VOD services have increasingly incorporated recommender systems powered by algorithms that analyze user behavior and suggest personalized content. However, a majority of existing recommender systems depend on explicit user feedback in the form of ratings and reviews, which can be difficult and time-consuming to collect at scale. This presents a key research gap, as leveraging users' implicit feedback patterns could provide an alternative avenue for building effective video recommendation models, circumventing the need for explicit ratings. However, prior literature lacks sufficient exploration into implicit feedback-based recommender systems, especially in the context of modeling video viewing behavior. Therefore, this paper aims to bridge this research gap by proposing a novel video recommendation technique that relies solely on users' implicit feedback in the form of their content viewing percentages. ",
    "url": "https://arxiv.org/abs/2308.12743",
    "authors": [
      "Mehrdad Maghsoudi",
      "MohammdHossein Valikhan",
      "MohammdHossein Zohdi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2308.12755",
    "title": "Acquiring Qualitative Explainable Graphs for Automated Driving Scene  Interpretation",
    "abstract": "The future of automated driving (AD) is rooted in the development of robust, fair and explainable artificial intelligence methods. Upon request, automated vehicles must be able to explain their decisions to the driver and the car passengers, to the pedestrians and other vulnerable road users and potentially to external auditors in case of accidents. However, nowadays, most explainable methods still rely on quantitative analysis of the AD scene representations captured by multiple sensors. This paper proposes a novel representation of AD scenes, called Qualitative eXplainable Graph (QXG), dedicated to qualitative spatiotemporal reasoning of long-term scenes. The construction of this graph exploits the recent Qualitative Constraint Acquisition paradigm. Our experimental results on NuScenes, an open real-world multi-modal dataset, show that the qualitative eXplainable graph of an AD scene composed of 40 frames can be computed in real-time and light in space storage which makes it a potentially interesting tool for improved and more trustworthy perception and control processes in AD. ",
    "url": "https://arxiv.org/abs/2308.12755",
    "authors": [
      "Nassim Belmecheri",
      "Arnaud Gotlieb",
      "Nadjib Lazaar",
      "Helge Spieker"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2308.12762",
    "title": "Reinforcement learning informed evolutionary search for autonomous  systems testing",
    "abstract": "Evolutionary search-based techniques are commonly used for testing autonomous robotic systems. However, these approaches often rely on computationally expensive simulator-based models for test scenario evaluation. To improve the computational efficiency of the search-based testing, we propose augmenting the evolutionary search (ES) with a reinforcement learning (RL) agent trained using surrogate rewards derived from domain knowledge. In our approach, known as RIGAA (Reinforcement learning Informed Genetic Algorithm for Autonomous systems testing), we first train an RL agent to learn useful constraints of the problem and then use it to produce a certain part of the initial population of the search algorithm. By incorporating an RL agent into the search process, we aim to guide the algorithm towards promising regions of the search space from the start, enabling more efficient exploration of the solution space. We evaluate RIGAA on two case studies: maze generation for an autonomous ant robot and road topology generation for an autonomous vehicle lane keeping assist system. In both case studies, RIGAA converges faster to fitter solutions and produces a better test suite (in terms of average test scenario fitness and diversity). RIGAA also outperforms the state-of-the-art tools for vehicle lane keeping assist system testing, such as AmbieGen and Frenetic. ",
    "url": "https://arxiv.org/abs/2308.12762",
    "authors": [
      "Dmytro Humeniuk",
      "Foutse Khomh",
      "Giuliano Antoniol"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2308.12773",
    "title": "Pre-training Code Representation with Semantic Flow Graph for Effective  Bug Localization",
    "abstract": "Enlightened by the big success of pre-training in natural language processing, pre-trained models for programming languages have been widely used to promote code intelligence in recent years. In particular, BERT has been used for bug localization tasks and impressive results have been obtained. However, these BERT-based bug localization techniques suffer from two issues. First, the pre-trained BERT model on source code does not adequately capture the deep semantics of program code. Second, the overall bug localization models neglect the necessity of large-scale negative samples in contrastive learning for representations of changesets and ignore the lexical similarity between bug reports and changesets during similarity estimation. We address these two issues by 1) proposing a novel directed, multiple-label code graph representation named Semantic Flow Graph (SFG), which compactly and adequately captures code semantics, 2) designing and training SemanticCodeBERT based on SFG, and 3) designing a novel Hierarchical Momentum Contrastive Bug Localization technique (HMCBL). Evaluation results show that our method achieves state-of-the-art performance in bug localization. ",
    "url": "https://arxiv.org/abs/2308.12773",
    "authors": [
      "Yali Du",
      "Zhongxing Yu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2308.12779",
    "title": "On Offline Evaluation of 3D Object Detection for Autonomous Driving",
    "abstract": "Prior work in 3D object detection evaluates models using offline metrics like average precision since closed-loop online evaluation on the downstream driving task is costly. However, it is unclear how indicative offline results are of driving performance. In this work, we perform the first empirical evaluation measuring how predictive different detection metrics are of driving performance when detectors are integrated into a full self-driving stack. We conduct extensive experiments on urban driving in the CARLA simulator using 16 object detection models. We find that the nuScenes Detection Score has a higher correlation to driving performance than the widely used average precision metric. In addition, our results call for caution on the exclusive reliance on the emerging class of `planner-centric' metrics. ",
    "url": "https://arxiv.org/abs/2308.12779",
    "authors": [
      "Tim Schreier",
      "Katrin Renz",
      "Andreas Geiger",
      "Kashyap Chitta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2308.12785",
    "title": "Single-shot Bayesian approximation for neural networks",
    "abstract": "Deep neural networks (NNs) are known for their high-prediction performances. However, NNs are prone to yield unreliable predictions when encountering completely new situations without indicating their uncertainty. Bayesian variants of NNs (BNNs), such as Monte Carlo (MC) dropout BNNs, do provide uncertainty measures and simultaneously increase the prediction performance. The only disadvantage of BNNs is their higher computation time during test time because they rely on a sampling approach. Here we present a single-shot MC dropout approximation that preserves the advantages of BNNs while being as fast as NNs. Our approach is based on moment propagation (MP) and allows to analytically approximate the expected value and the variance of the MC dropout signal for commonly used layers in NNs, i.e. convolution, max pooling, dense, softmax, and dropout layers. The MP approach can convert an NN into a BNN without re-training given the NN has been trained with standard dropout. We evaluate our approach on different benchmark datasets and a simulated toy example in a classification and regression setting. We demonstrate that our single-shot MC dropout approximation resembles the point estimate and the uncertainty estimate of the predictive distribution that is achieved with an MC approach, while being fast enough for real-time deployments of BNNs. We show that using part of the saved time to combine our MP approach with deep ensemble techniques does further improve the uncertainty measures. ",
    "url": "https://arxiv.org/abs/2308.12785",
    "authors": [
      "Kai Brach",
      "Beate Sick",
      "Oliver D\u00fcrr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2308.12789",
    "title": "Robotic Scene Segmentation with Memory Network for Runtime Surgical  Context Inference",
    "abstract": "Surgical context inference has recently garnered significant attention in robot-assisted surgery as it can facilitate workflow analysis, skill assessment, and error detection. However, runtime context inference is challenging since it requires timely and accurate detection of the interactions among the tools and objects in the surgical scene based on the segmentation of video data. On the other hand, existing state-of-the-art video segmentation methods are often biased against infrequent classes and fail to provide temporal consistency for segmented masks. This can negatively impact the context inference and accurate detection of critical states. In this study, we propose a solution to these challenges using a Space Time Correspondence Network (STCN). STCN is a memory network that performs binary segmentation and minimizes the effects of class imbalance. The use of a memory bank in STCN allows for the utilization of past image and segmentation information, thereby ensuring consistency of the masks. Our experiments using the publicly available JIGSAWS dataset demonstrate that STCN achieves superior segmentation performance for objects that are difficult to segment, such as needle and thread, and improves context inference compared to the state-of-the-art. We also demonstrate that segmentation and context inference can be performed at runtime without compromising performance. ",
    "url": "https://arxiv.org/abs/2308.12789",
    "authors": [
      "Zongyu Li",
      "Ian Reyes",
      "Homa Alemzadeh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12800",
    "title": "ICU Mortality Prediction Using Long Short-Term Memory Networks",
    "abstract": "Extensive bedside monitoring in Intensive Care Units (ICUs) has resulted in complex temporal data regarding patient physiology, which presents an upscale context for clinical data analysis. In the other hand, identifying the time-series patterns within these data may provide a high aptitude to predict clinical events. Hence, we investigate, during this work, the implementation of an automatic data-driven system, which analyzes large amounts of multivariate temporal data derived from Electronic Health Records (EHRs), and extracts high-level information so as to predict in-hospital mortality and Length of Stay (LOS) early. Practically, we investigate the applicability of LSTM network by reducing the time-frame to 6-hour so as to enhance clinical tasks. The experimental results highlight the efficiency of LSTM model with rigorous multivariate time-series measurements for building real-world prediction engines. ",
    "url": "https://arxiv.org/abs/2308.12800",
    "authors": [
      "Manel Mili",
      "Asma Kerkeni",
      "Asma Ben Abdallah",
      "Mohamed Hedi Bedoui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12817",
    "title": "MixNet: Toward Accurate Detection of Challenging Scene Text in the Wild",
    "abstract": "Detecting small scene text instances in the wild is particularly challenging, where the influence of irregular positions and nonideal lighting often leads to detection errors. We present MixNet, a hybrid architecture that combines the strengths of CNNs and Transformers, capable of accurately detecting small text from challenging natural scenes, regardless of the orientations, styles, and lighting conditions. MixNet incorporates two key modules: (1) the Feature Shuffle Network (FSNet) to serve as the backbone and (2) the Central Transformer Block (CTBlock) to exploit the 1D manifold constraint of the scene text. We first introduce a novel feature shuffling strategy in FSNet to facilitate the exchange of features across multiple scales, generating high-resolution features superior to popular ResNet and HRNet. The FSNet backbone has achieved significant improvements over many existing text detection methods, including PAN, DB, and FAST. Then we design a complementary CTBlock to leverage center line based features similar to the medial axis of text regions and show that it can outperform contour-based approaches in challenging cases when small scene texts appear closely. Extensive experimental results show that MixNet, which mixes FSNet with CTBlock, achieves state-of-the-art results on multiple scene text detection datasets. ",
    "url": "https://arxiv.org/abs/2308.12817",
    "authors": [
      "Yu-Xiang Zeng",
      "Jun-Wei Hsieh",
      "Xin Li",
      "Ming-Ching Chang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12820",
    "title": "Prediction without Preclusion: Recourse Verification with Reachable Sets",
    "abstract": "Machine learning models are often used to decide who will receive a loan, a job interview, or a public benefit. Standard techniques to build these models use features about people but overlook their actionability. In turn, models can assign predictions that are fixed, meaning that consumers who are denied loans, interviews, or benefits may be permanently locked out from access to credit, employment, or assistance. In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar access, and we provide tools to design algorithms that account for actionability when developing models. ",
    "url": "https://arxiv.org/abs/2308.12820",
    "authors": [
      "Avni Kothari",
      "Bogdan Kulynych",
      "Tsui-Wei Weng",
      "Berk Ustun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2308.12828",
    "title": "Short Run Transit Route Planning Decision Support System Using a Deep  Learning-Based Weighted Graph",
    "abstract": "Public transport routing plays a crucial role in transit network design, ensuring a satisfactory level of service for passengers. However, current routing solutions rely on traditional operational research heuristics, which can be time-consuming to implement and lack the ability to provide quick solutions. Here, we propose a novel deep learning-based methodology for a decision support system that enables public transport (PT) planners to identify short-term route improvements rapidly. By seamlessly adjusting specific sections of routes between two stops during specific times of the day, our method effectively reduces times and enhances PT services. Leveraging diverse data sources such as GTFS and smart card data, we extract features and model the transportation network as a directed graph. Using self-supervision, we train a deep learning model for predicting lateness values for road segments. These lateness values are then utilized as edge weights in the transportation graph, enabling efficient path searching. Through evaluating the method on Tel Aviv, we are able to reduce times on more than 9\\% of the routes. The improved routes included both intraurban and suburban routes showcasing a fact highlighting the model's versatility. The findings emphasize the potential of our data-driven decision support system to enhance public transport and city logistics, promoting greater efficiency and reliability in PT services. ",
    "url": "https://arxiv.org/abs/2308.12828",
    "authors": [
      "Nadav Shalit",
      "Michael Fire",
      "Dima Kagan",
      "Eran Ben-Elia"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12845",
    "title": "Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle  Avoidance",
    "abstract": "Robust obstacle avoidance is one of the critical steps for successful goal-driven indoor navigation tasks.Due to the obstacle missing in the visual image and the possible missed detection issue, visual image-based obstacle avoidance techniques still suffer from unsatisfactory robustness. To mitigate it, in this paper, we propose a novel implicit obstacle map-driven indoor navigation framework for robust obstacle avoidance, where an implicit obstacle map is learned based on the historical trial-and-error experience rather than the visual image. In order to further improve the navigation efficiency, a non-local target memory aggregation module is designed to leverage a non-local network to model the intrinsic relationship between the target semantic and the target orientation clues during the navigation process so as to mine the most target-correlated object clues for the navigation decision. Extensive experimental results on AI2-Thor and RoboTHOR benchmarks verify the excellent obstacle avoidance and navigation efficiency of our proposed method. The core source code is available at https://github.com/xwaiyy123/object-navigation. ",
    "url": "https://arxiv.org/abs/2308.12845",
    "authors": [
      "Wei Xie",
      "Haobo Jiang",
      "Shuo Gu",
      "Jin Xie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12857",
    "title": "Fast Adversarial Training with Smooth Convergence",
    "abstract": "Fast adversarial training (FAT) is beneficial for improving the adversarial robustness of neural networks. However, previous FAT work has encountered a significant issue known as catastrophic overfitting when dealing with large perturbation budgets, \\ie the adversarial robustness of models declines to near zero during training. To address this, we analyze the training process of prior FAT work and observe that catastrophic overfitting is accompanied by the appearance of loss convergence outliers. Therefore, we argue a moderately smooth loss convergence process will be a stable FAT process that solves catastrophic overfitting. To obtain a smooth loss convergence process, we propose a novel oscillatory constraint (dubbed ConvergeSmooth) to limit the loss difference between adjacent epochs. The convergence stride of ConvergeSmooth is introduced to balance convergence and smoothing. Likewise, we design weight centralization without introducing additional hyperparameters other than the loss balance coefficient. Our proposed methods are attack-agnostic and thus can improve the training stability of various FAT techniques. Extensive experiments on popular datasets show that the proposed methods efficiently avoid catastrophic overfitting and outperform all previous FAT methods. Code is available at \\url{https://github.com/FAT-CS/ConvergeSmooth}. ",
    "url": "https://arxiv.org/abs/2308.12857",
    "authors": [
      "Mengnan Zhao",
      "Lihe Zhang",
      "Yuqiu Kong",
      "Baocai Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12863",
    "title": "SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection",
    "abstract": "Multi-modal fusion is increasingly being used for autonomous driving tasks, as images from different modalities provide unique information for feature extraction. However, the existing two-stream networks are only fused at a specific network layer, which requires a lot of manual attempts to set up. As the CNN goes deeper, the two modal features become more and more advanced and abstract, and the fusion occurs at the feature level with a large gap, which can easily hurt the performance. In this study, we propose a novel fusion architecture called skip-cross networks (SkipcrossNets), which combines adaptively LiDAR point clouds and camera images without being bound to a certain fusion epoch. Specifically, skip-cross connects each layer to each layer in a feed-forward manner, and for each layer, the feature maps of all previous layers are used as input and its own feature maps are used as input to all subsequent layers for the other modality, enhancing feature propagation and multi-modal features fusion. This strategy facilitates selection of the most similar feature layers from two data pipelines, providing a complementary effect for sparse point cloud features during fusion processes. The network is also divided into several blocks to reduce the complexity of feature fusion and the number of model parameters. The advantages of skip-cross fusion were demonstrated through application to the KITTI and A2D2 datasets, achieving a MaxF score of 96.85% on KITTI and an F1 score of 84.84% on A2D2. The model parameters required only 2.33 MB of memory at a speed of 68.24 FPS, which could be viable for mobile terminals and embedded devices. ",
    "url": "https://arxiv.org/abs/2308.12863",
    "authors": [
      "Xinyu Zhang",
      "Yan Gong",
      "Zhiwei Li",
      "Xin Gao",
      "Dafeng Jin",
      "Jun Li",
      "Huaping Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12864",
    "title": "Auto-weighted Bayesian Physics-Informed Neural Networks and robust  estimations for multitask inverse problems in pore-scale imaging of  dissolution",
    "abstract": "In this article, we present a novel data assimilation strategy in pore-scale imaging and demonstrate that this makes it possible to robustly address reactive inverse problems incorporating Uncertainty Quantification (UQ). Pore-scale modeling of reactive flow offers a valuable opportunity to investigate the evolution of macro-scale properties subject to dynamic processes. Yet, they suffer from imaging limitations arising from the associated X-ray microtomography (X-ray microCT) process, which induces discrepancies in the properties estimates. Assessment of the kinetic parameters also raises challenges, as reactive coefficients are critical parameters that can cover a wide range of values. We account for these two issues and ensure reliable calibration of pore-scale modeling, based on dynamical microCT images, by integrating uncertainty quantification in the workflow. The present method is based on a multitasking formulation of reactive inverse problems combining data-driven and physics-informed techniques in calcite dissolution. This allows quantifying morphological uncertainties on the porosity field and estimating reactive parameter ranges through prescribed PDE models with a latent concentration field and dynamical microCT. The data assimilation strategy relies on sequential reinforcement incorporating successively additional PDE constraints. We guarantee robust and unbiased uncertainty quantification by straightforward adaptive weighting of Bayesian Physics-Informed Neural Networks (BPINNs), ensuring reliable micro-porosity changes during geochemical transformations. We demonstrate successful Bayesian Inference in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT images with meaningful posterior distribution on the reactive parameters and dimensionless numbers. ",
    "url": "https://arxiv.org/abs/2308.12864",
    "authors": [
      "Sarah Perez",
      "Philippe Poncet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2308.12882",
    "title": "LCANets++: Robust Audio Classification using Multi-layer Neural Networks  with Lateral Competition",
    "abstract": "Audio classification aims at recognizing audio signals, including speech commands or sound events. However, current audio classifiers are susceptible to perturbations and adversarial attacks. In addition, real-world audio classification tasks often suffer from limited labeled data. To help bridge these gaps, previous work developed neuro-inspired convolutional neural networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA) in the first layer (i.e., LCANets) for computer vision. LCANets learn in a combination of supervised and unsupervised learning, reducing dependency on labeled samples. Motivated by the fact that auditory cortex is also sparse, we extend LCANets to audio recognition tasks and introduce LCANets++, which are CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that LCANets++ are more robust than standard CNNs and LCANets against perturbations, e.g., background noise, as well as black-box and white-box attacks, e.g., evasion and fast gradient sign (FGSM) attacks. ",
    "url": "https://arxiv.org/abs/2308.12882",
    "authors": [
      "Sayanton V. Dibbo",
      "Juston S. Moore",
      "Garrett T. Kenyon",
      "Michael A. Teti"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2308.12888",
    "title": "Inducing Causal Structure for Abstractive Text Summarization",
    "abstract": "The mainstream of data-driven abstractive summarization models tends to explore the correlations rather than the causal relationships. Among such correlations, there can be spurious ones which suffer from the language prior learned from the training corpus and therefore undermine the overall effectiveness of the learned model. To tackle this issue, we introduce a Structural Causal Model (SCM) to induce the underlying causal structure of the summarization data. We assume several latent causal factors and non-causal factors, representing the content and style of the document and summary. Theoretically, we prove that the latent factors in our SCM can be identified by fitting the observed training data under certain conditions. On the basis of this, we propose a Causality Inspired Sequence-to-Sequence model (CI-Seq2Seq) to learn the causal representations that can mimic the causal factors, guiding us to pursue causal information for summary generation. The key idea is to reformulate the Variational Auto-encoder (VAE) to fit the joint distribution of the document and summary variables from the training corpus. Experimental results on two widely used text summarization datasets demonstrate the advantages of our approach. ",
    "url": "https://arxiv.org/abs/2308.12888",
    "authors": [
      "Lu Chen",
      "Ruqing Zhang",
      "Wei Huang",
      "Wei Chen",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12899",
    "title": "Unified Data Management and Comprehensive Performance Evaluation for  Urban Spatial-Temporal Prediction [Experiment, Analysis & Benchmark]",
    "abstract": "The field of urban spatial-temporal prediction is advancing rapidly with the development of deep learning techniques and the availability of large-scale datasets. However, challenges persist in accessing and utilizing diverse urban spatial-temporal datasets from different sources and stored in different formats, as well as determining effective model structures and components with the proliferation of deep learning models. This work addresses these challenges and provides three significant contributions. Firstly, we introduce \"atomic files\", a unified storage format designed for urban spatial-temporal big data, and validate its effectiveness on 40 diverse datasets, simplifying data management. Secondly, we present a comprehensive overview of technological advances in urban spatial-temporal prediction models, guiding the development of robust models. Thirdly, we conduct extensive experiments using diverse models and datasets, establishing a performance leaderboard and identifying promising research directions. Overall, this work effectively manages urban spatial-temporal data, guides future efforts, and facilitates the development of accurate and efficient urban spatial-temporal prediction models. It can potentially make long-term contributions to urban spatial-temporal data management and prediction, ultimately leading to improved urban living standards. ",
    "url": "https://arxiv.org/abs/2308.12899",
    "authors": [
      "Jiawei Jiang",
      "Chengkai Han",
      "Wayne Xin Zhao",
      "Jingyuan Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12902",
    "title": "CDAN: Convolutional Dense Attention-guided Network for Low-light Image  Enhancement",
    "abstract": "Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs remarkably on benchmark datasets, effectively mitigating under-exposure and proficiently restoring textures and colors in diverse low-light scenarios. This achievement underscores CDAN's potential for diverse computer vision tasks, notably enabling robust object detection and recognition in challenging low-light conditions. ",
    "url": "https://arxiv.org/abs/2308.12902",
    "authors": [
      "Hossein Shakibania",
      "Sina Raoufi",
      "Hassan Khotanlou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12910",
    "title": "SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data",
    "abstract": "We propose Subject-Conditional Relation Detection SCoRD, where conditioned on an input subject, the goal is to predict all its relations to other objects in a scene along with their locations. Based on the Open Images dataset, we propose a challenging OIv6-SCoRD benchmark such that the training and testing splits have a distribution shift in terms of the occurrence statistics of $\\langle$subject, relation, object$\\rangle$ triplets. To solve this problem, we propose an auto-regressive model that given a subject, it predicts its relations, objects, and object locations by casting this output as a sequence of tokens. First, we show that previous scene-graph prediction methods fail to produce as exhaustive an enumeration of relation-object pairs when conditioned on a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for our relation-object predictions compared to the 49.75% obtained by a recent scene graph detector. Then, we show improved generalization on both relation-object and object-box predictions by leveraging during training relation-object pairs obtained automatically from textual captions and for which no object-box annotations are available. Particularly, for $\\langle$subject, relation, object$\\rangle$ triplets for which no object locations are available during training, we are able to obtain a recall@3 of 42.59% for relation-object pairs and 32.27% for their box locations. ",
    "url": "https://arxiv.org/abs/2308.12910",
    "authors": [
      "Ziyan Yang",
      "Kushal Kafle",
      "Zhe Lin",
      "Scott Cohen",
      "Zhihong Ding",
      "Vicente Ordonez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12918",
    "title": "Evaluating the Vulnerabilities in ML systems in terms of adversarial  attacks",
    "abstract": "There have been recent adversarial attacks that are difficult to find. These new adversarial attacks methods may pose challenges to current deep learning cyber defense systems and could influence the future defense of cyberattacks. The authors focus on this domain in this research paper. They explore the consequences of vulnerabilities in AI systems. This includes discussing how they might arise, differences between randomized and adversarial examples and also potential ethical implications of vulnerabilities. Moreover, it is important to train the AI systems appropriately when they are in testing phase and getting them ready for broader use. ",
    "url": "https://arxiv.org/abs/2308.12918",
    "authors": [
      "John Harshith",
      "Mantej Singh Gill",
      "Madhan Jothimani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12921",
    "title": "An Efficient Distributed Multi-Agent Reinforcement Learning for EV  Charging Network Control",
    "abstract": "The increasing trend in adopting electric vehicles (EVs) will significantly impact the residential electricity demand, which results in an increased risk of transformer overload in the distribution grid. To mitigate such risks, there are urgent needs to develop effective EV charging controllers. Currently, the majority of the EV charge controllers are based on a centralized approach for managing individual EVs or a group of EVs. In this paper, we introduce a decentralized Multi-agent Reinforcement Learning (MARL) charging framework that prioritizes the preservation of privacy for EV owners. We employ the Centralized Training Decentralized Execution-Deep Deterministic Policy Gradient (CTDE-DDPG) scheme, which provides valuable information to users during training while maintaining privacy during execution. Our results demonstrate that the CTDE framework improves the performance of the charging network by reducing the network costs. Moreover, we show that the Peak-to-Average Ratio (PAR) of the total demand is reduced, which, in turn, reduces the risk of transformer overload during the peak hours. ",
    "url": "https://arxiv.org/abs/2308.12921",
    "authors": [
      "Amin Shojaeighadikolaei",
      "Morteza Hashemi"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2308.12925",
    "title": "Low-count Time Series Anomaly Detection",
    "abstract": "Low-count time series describe sparse or intermittent events, which are prevalent in large-scale online platforms that capture and monitor diverse data types. Several distinct challenges surface when modelling low-count time series, particularly low signal-to-noise ratios (when anomaly signatures are provably undetectable), and non-uniform performance (when average metrics are not representative of local behaviour). The time series anomaly detection community currently lacks explicit tooling and processes to model and reliably detect anomalies in these settings. We address this gap by introducing a novel generative procedure for creating benchmark datasets comprising of low-count time series with anomalous segments. Via a mixture of theoretical and empirical analysis, our work explains how widely-used algorithms struggle with the distribution overlap between normal and anomalous segments. In order to mitigate this shortcoming, we then leverage our findings to demonstrate how anomaly score smoothing consistently improves performance. The practical utility of our analysis and recommendation is validated on a real-world dataset containing sales data for retail stores. ",
    "url": "https://arxiv.org/abs/2308.12925",
    "authors": [
      "Philipp Renz",
      "Kurt Cutajar",
      "Niall Twomey",
      "Gavin K. C. Cheung",
      "Hanting Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2308.12938",
    "title": "Perspective-aware Convolution for Monocular 3D Object Detection",
    "abstract": "Monocular 3D object detection is a crucial and challenging task for autonomous driving vehicle, while it uses only a single camera image to infer 3D objects in the scene. To address the difficulty of predicting depth using only pictorial clue, we propose a novel perspective-aware convolutional layer that captures long-range dependencies in images. By enforcing convolutional kernels to extract features along the depth axis of every image pixel, we incorporates perspective information into network architecture. We integrate our perspective-aware convolutional layer into a 3D object detector and demonstrate improved performance on the KITTI3D dataset, achieving a 23.9\\% average precision in the easy benchmark. These results underscore the importance of modeling scene clues for accurate depth inference and highlight the benefits of incorporating scene structure in network design. Our perspective-aware convolutional layer has the potential to enhance object detection accuracy by providing more precise and context-aware feature extraction. ",
    "url": "https://arxiv.org/abs/2308.12938",
    "authors": [
      "Jia-Quan Yu",
      "Soo-Chang Pei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12939",
    "title": "Learning Only On Boundaries: a Physics-Informed Neural operator for  Solving Parametric Partial Differential Equations in Complex Geometries",
    "abstract": "Recently deep learning surrogates and neural operators have shown promise in solving partial differential equations (PDEs). However, they often require a large amount of training data and are limited to bounded domains. In this work, we present a novel physics-informed neural operator method to solve parametrized boundary value problems without labeled data. By reformulating the PDEs into boundary integral equations (BIEs), we can train the operator network solely on the boundary of the domain. This approach reduces the number of required sample points from $O(N^d)$ to $O(N^{d-1})$, where $d$ is the domain's dimension, leading to a significant acceleration of the training process. Additionally, our method can handle unbounded problems, which are unattainable for existing physics-informed neural networks (PINNs) and neural operators. Our numerical experiments show the effectiveness of parametrized complex geometries and unbounded problems. ",
    "url": "https://arxiv.org/abs/2308.12939",
    "authors": [
      "Zhiwei Fang",
      "Sifan Wang",
      "Paris Perdikaris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2308.12947",
    "title": "Counting Distinct Elements Under Person-Level Differential Privacy",
    "abstract": "We study the problem of counting the number of distinct elements in a dataset subject to the constraint of differential privacy. We consider the challenging setting of person-level DP (a.k.a. user-level DP) where each person may contribute an unbounded number of items and hence the sensitivity is unbounded. Our approach is to compute a bounded-sensitivity version of this query, which reduces to solving a max-flow problem. The sensitivity bound is optimized to balance the noise we must add to privatize the answer against the error of the approximation of the bounded-sensitivity query to the true number of unique elements. ",
    "url": "https://arxiv.org/abs/2308.12947",
    "authors": [
      "Alexander Knop",
      "Thomas Steinke"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2308.12950",
    "title": "Code Llama: Open Foundation Models for Code",
    "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use. ",
    "url": "https://arxiv.org/abs/2308.12950",
    "authors": [
      "Baptiste Rozi\u00e8re",
      "Jonas Gehring",
      "Fabian Gloeckle",
      "Sten Sootla",
      "Itai Gat",
      "Xiaoqing Ellen Tan",
      "Yossi Adi",
      "Jingyu Liu",
      "Tal Remez",
      "J\u00e9r\u00e9my Rapin",
      "Artyom Kozhevnikov",
      "Ivan Evtimov",
      "Joanna Bitton",
      "Manish Bhatt",
      "Cristian Canton Ferrer",
      "Aaron Grattafiori",
      "Wenhan Xiong",
      "Alexandre D\u00e9fossez",
      "Jade Copet",
      "Faisal Azhar",
      "Hugo Touvron",
      "Louis Martin",
      "Nicolas Usunier",
      "Thomas Scialom",
      "Gabriel Synnaeve"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2308.12961",
    "title": "Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via  Training-free Networks",
    "abstract": "To reduce the reliance on large-scale datasets, recent works in 3D segmentation resort to few-shot learning. Current 3D few-shot semantic segmentation methods first pre-train the models on `seen' classes, and then evaluate their generalization performance on `unseen' classes. However, the prior pre-training stage not only introduces excessive time overhead, but also incurs a significant domain gap on `unseen' classes. To tackle these issues, we propose an efficient Training-free Few-shot 3D Segmentation netwrok, TFS3D, and a further training-based variant, TFS3D-T. Without any learnable parameters, TFS3D extracts dense representations by trigonometric positional encodings, and achieves comparable performance to previous training-based methods. Due to the elimination of pre-training, TFS3D can alleviate the domain gap issue and save a substantial amount of time. Building upon TFS3D, TFS3D-T only requires to train a lightweight query-support transferring attention (QUEST), which enhances the interaction between the few-shot query and support data. Experiments demonstrate TFS3D-T improves previous state-of-the-art methods by +6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing the training time by -90%, indicating superior effectiveness and efficiency. ",
    "url": "https://arxiv.org/abs/2308.12961",
    "authors": [
      "Xiangyang Zhu",
      "Renrui Zhang",
      "Bowei He",
      "Ziyu Guo",
      "Jiaming Liu",
      "Hao Dong",
      "Peng Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12962",
    "title": "Motion-Guided Masking for Spatiotemporal Representation Learning",
    "abstract": "Several recent works have directly extended the image masked autoencoder (MAE) with random masking into video domain, achieving promising results. However, unlike images, both spatial and temporal information are important for video understanding. This suggests that the random masking strategy that is inherited from the image MAE is less effective for video MAE. This motivates the design of a novel masking algorithm that can more efficiently make use of video saliency. Specifically, we propose a motion-guided masking algorithm (MGM) which leverages motion vectors to guide the position of each mask over time. Crucially, these motion-based correspondences can be directly obtained from information stored in the compressed format of the video, which makes our method efficient and scalable. On two challenging large-scale video benchmarks (Kinetics-400 and Something-Something V2), we equip video MAE with our MGM and achieve up to +$1.3\\%$ improvement compared to previous state-of-the-art methods. Additionally, our MGM achieves equivalent performance to previous video MAE using up to $66\\%$ fewer training epochs. Lastly, we show that MGM generalizes better to downstream transfer learning and domain adaptation tasks on the UCF101, HMDB51, and Diving48 datasets, achieving up to +$4.9\\%$ improvement compared to baseline methods. ",
    "url": "https://arxiv.org/abs/2308.12962",
    "authors": [
      "David Fan",
      "Jue Wang",
      "Shuai Liao",
      "Yi Zhu",
      "Vimal Bhat",
      "Hector Santos-Villalobos",
      "Rohith MV",
      "Xinyu Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12967",
    "title": "NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes",
    "abstract": "Recent implicit neural representations have shown great results for novel view synthesis. However, existing methods require expensive per-scene optimization from many views hence limiting their application to real-world unbounded urban settings where the objects of interest or backgrounds are observed from very few views. To mitigate this challenge, we introduce a new approach called NeO 360, Neural fields for sparse view synthesis of outdoor scenes. NeO 360 is a generalizable method that reconstructs 360{\\deg} scenes from a single or a few posed RGB images. The essence of our approach is in capturing the distribution of complex real-world outdoor 3D scenes and using a hybrid image-conditional triplanar representation that can be queried from any world point. Our representation combines the best of both voxel-based and bird's-eye-view (BEV) representations and is more effective and expressive than each. NeO 360's representation allows us to learn from a large collection of unbounded 3D scenes while offering generalizability to new views and novel scenes from as few as a single image during inference. We demonstrate our approach on the proposed challenging 360{\\deg} unbounded dataset, called NeRDS 360, and show that NeO 360 outperforms state-of-the-art generalizable methods for novel view synthesis while also offering editing and composition capabilities. Project page: https://zubair-irshad.github.io/projects/neo360.html ",
    "url": "https://arxiv.org/abs/2308.12967",
    "authors": [
      "Muhammad Zubair Irshad",
      "Sergey Zakharov",
      "Katherine Liu",
      "Vitor Guizilini",
      "Thomas Kollar",
      "Adrien Gaidon",
      "Zsolt Kira",
      "Rares Ambrus"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12969",
    "title": "ROAM: Robust and Object-aware Motion Generation using Neural Pose  Descriptors",
    "abstract": "Existing automatic approaches for 3D virtual character motion synthesis supporting scene interactions do not generalise well to new objects outside training distributions, even when trained on extensive motion capture datasets with diverse objects and annotated interactions. This paper addresses this limitation and shows that robustness and generalisation to novel scene objects in 3D object-aware character synthesis can be achieved by training a motion model with as few as one reference object. We leverage an implicit feature representation trained on object-only datasets, which encodes an SE(3)-equivariant descriptor field around the object. Given an unseen object and a reference pose-object pair, we optimise for the object-aware pose that is closest in the feature space to the reference pose. Finally, we use l-NSM, i.e., our motion generation model that is trained to seamlessly transition from locomotion to object interaction with the proposed bidirectional pose blending scheme. Through comprehensive numerical comparisons to state-of-the-art methods and in a user study, we demonstrate substantial improvements in 3D virtual character motion and interaction quality and robustness to scenarios with unseen objects. Our project page is available at https://vcai.mpi-inf.mpg.de/projects/ROAM/. ",
    "url": "https://arxiv.org/abs/2308.12969",
    "authors": [
      "Wanyue Zhang",
      "Rishabh Dabral",
      "Thomas Leimk\u00fchler",
      "Vladislav Golyanik",
      "Marc Habermann",
      "Christian Theobalt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12970",
    "title": "NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin  Shell Theory",
    "abstract": "Cloth simulation is an extensively studied problem, with a plethora of solutions available in computer graphics literature. Existing cloth simulators produce realistic cloth deformations that obey different types of boundary conditions. Nevertheless, their operational principle remains limited in several ways: They operate on explicit surface representations with a fixed spatial resolution, perform a series of discretised updates (which bounds their temporal resolution), and require comparably large amounts of storage. Moreover, back-propagating gradients through the existing solvers is often not straightforward, which poses additional challenges when integrating them into modern neural architectures. In response to the limitations mentioned above, this paper takes a fundamentally different perspective on physically-plausible cloth simulation and re-thinks this long-standing problem: We propose NeuralClothSim, i.e., a new cloth simulation approach using thin shells, in which surface evolution is encoded in neural network weights. Our memory-efficient and differentiable solver operates on a new continuous coordinate-based representation of dynamic surfaces, i.e., neural deformation fields (NDFs); it supervises NDF evolution with the rules of the non-linear Kirchhoff-Love shell theory. NDFs are adaptive in the sense that they 1) allocate their capacity to the deformation details as the latter arise during the cloth evolution and 2) allow surface state queries at arbitrary spatial and temporal resolutions without retraining. We show how to train our NeuralClothSim solver while imposing hard boundary conditions and demonstrate multiple applications, such as material interpolation and simulation editing. The experimental results highlight the effectiveness of our formulation and its potential impact. ",
    "url": "https://arxiv.org/abs/2308.12970",
    "authors": [
      "Navami Kairanda",
      "Marc Habermann",
      "Christian Theobalt",
      "Vladislav Golyanik"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12299",
    "title": "Inverse Lithography Physics-informed Deep Neural Level Set for Mask  Optimization",
    "abstract": "As the feature size of integrated circuits continues to decrease, optical proximity correction (OPC) has emerged as a crucial resolution enhancement technology for ensuring high printability in the lithography process. Recently, level set-based inverse lithography technology (ILT) has drawn considerable attention as a promising OPC solution, showcasing its powerful pattern fidelity, especially in advanced process. However, massive computational time consumption of ILT limits its applicability to mainly correcting partial layers and hotspot regions. Deep learning (DL) methods have shown great potential in accelerating ILT. However, lack of domain knowledge of inverse lithography limits the ability of DL-based algorithms in process window (PW) enhancement and etc. In this paper, we propose an inverse lithography physics-informed deep neural level set (ILDLS) approach for mask optimization. This approach utilizes level set based-ILT as a layer within the DL framework and iteratively conducts mask prediction and correction to significantly enhance printability and PW in comparison with results from pure DL and ILT. With this approach, computation time is reduced by a few orders of magnitude versus ILT. By gearing up DL with knowledge of inverse lithography physics, ILDLS provides a new and efficient mask optimization solution. ",
    "url": "https://arxiv.org/abs/2308.12299",
    "authors": [
      "Xing-Yu Ma",
      "Shaogang Hao"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Hardware Architecture (cs.AR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12312",
    "title": "Physics informed Neural Networks applied to the description of  wave-particle resonance in kinetic simulations of fusion plasmas",
    "abstract": "The Vlasov-Poisson system is employed in its reduced form version (1D1V) as a test bed for the applicability of Physics Informed Neural Network (PINN) to the wave-particle resonance. Two examples are explored: the Landau damping and the bump-on-tail instability. PINN is first tested as a compression method for the solution of the Vlasov-Poisson system and compared to the standard neural networks. Second, the application of PINN to solving the Vlasov-Poisson system is also presented with the special emphasis on the integral part, which motivates the implementation of a PINN variant, called Integrable PINN (I-PINN), based on the automatic-differentiation to solve the partial differential equation and on the automatic-integration to solve the integral equation. ",
    "url": "https://arxiv.org/abs/2308.12312",
    "authors": [
      "Jai Kumar",
      "David Zarzoso",
      "Virginie Grandgirard",
      "Jan Ebert",
      "Stefan Kesselheim"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Artificial Intelligence (cs.AI)",
      "Plasma Physics (physics.plasm-ph)"
    ]
  },
  {
    "id": "arXiv:2308.12325",
    "title": "Predicting Drug Solubility Using Different Machine Learning Methods --  Linear Regression Model with Extracted Chemical Features vs Graph  Convolutional Neural Network",
    "abstract": "Predicting the solubility of given molecules is an important task in the pharmaceutical industry, and consequently this is a well-studied topic. In this research, we revisited this problem with the advantage of modern computing resources. We applied two machine learning models, a linear regression model and a graph convolutional neural network model, on multiple experimental datasets. Both methods can make reasonable predictions while the GCNN model had the best performance. However, the current GCNN model is a black box, while feature importance analysis from the linear regression model offers more insights into the underlying chemical influences. Using the linear regression model, we show how each functional group affects the overall solubility. Ultimately, knowing how chemical structure influences chemical properties is crucial when designing new drugs. Future work should aim to combine the high performance of GCNNs with the interpretability of linear regression, unlocking new advances in next generation high throughput screening. ",
    "url": "https://arxiv.org/abs/2308.12325",
    "authors": [
      "John Ho",
      "Zhao-Heng Yin",
      "Colin Zhang",
      "Henry Overhauser",
      "Kyle Swanson",
      "Yang Ha"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12416",
    "title": "Reframing the Brain Age Prediction Problem to a More Interpretable and  Quantitative Approach",
    "abstract": "Deep learning models have achieved state-of-the-art results in estimating brain age, which is an important brain health biomarker, from magnetic resonance (MR) images. However, most of these models only provide a global age prediction, and rely on techniques, such as saliency maps to interpret their results. These saliency maps highlight regions in the input image that were significant for the model's predictions, but they are hard to be interpreted, and saliency map values are not directly comparable across different samples. In this work, we reframe the age prediction problem from MR images to an image-to-image regression problem where we estimate the brain age for each brain voxel in MR images. We compare voxel-wise age prediction models against global age prediction models and their corresponding saliency maps. The results indicate that voxel-wise age prediction models are more interpretable, since they provide spatial information about the brain aging process, and they benefit from being quantitative. ",
    "url": "https://arxiv.org/abs/2308.12416",
    "authors": [
      "Neha Gianchandani",
      "Mahsa Dibaji",
      "Mariana Bento",
      "Ethan MacDonald",
      "Roberto Souza"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2308.12440",
    "title": "HNAS-reg: hierarchical neural architecture search for deformable medical  image registration",
    "abstract": "Convolutional neural networks (CNNs) have been widely used to build deep learning models for medical image registration, but manually designed network architectures are not necessarily optimal. This paper presents a hierarchical NAS framework (HNAS-Reg), consisting of both convolutional operation search and network topology search, to identify the optimal network architecture for deformable medical image registration. To mitigate the computational overhead and memory constraints, a partial channel strategy is utilized without losing optimization quality. Experiments on three datasets, consisting of 636 T1-weighted magnetic resonance images (MRIs), have demonstrated that the proposal method can build a deep learning model with improved image registration accuracy and reduced model size, compared with state-of-the-art image registration approaches, including one representative traditional approach and two unsupervised learning-based approaches. ",
    "url": "https://arxiv.org/abs/2308.12440",
    "authors": [
      "Jiong Wu",
      "Yong Fan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12481",
    "title": "Fall Detection using Knowledge Distillation Based Long short-term memory  for Offline Embedded and Low Power Devices",
    "abstract": "This paper presents a cost-effective, low-power approach to unintentional fall detection using knowledge distillation-based LSTM (Long Short-Term Memory) models to significantly improve accuracy. With a primary focus on analyzing time-series data collected from various sensors, the solution offers real-time detection capabilities, ensuring prompt and reliable identification of falls. The authors investigate fall detection models that are based on different sensors, comparing their accuracy rates and performance. Furthermore, they employ the technique of knowledge distillation to enhance the models' precision, resulting in refined accurate configurations that consume lower power. As a result, this proposed solution presents a compelling avenue for the development of energy-efficient fall detection systems for future advancements in this critical domain. ",
    "url": "https://arxiv.org/abs/2308.12481",
    "authors": [
      "Hannah Zhou",
      "Allison Chen",
      "Celine Buer",
      "Emily Chen",
      "Kayleen Tang",
      "Lauryn Gong",
      "Zhiqi Liu",
      "Jianbin Tang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.12508",
    "title": "FFEINR: Flow Feature-Enhanced Implicit Neural Representation for  Spatio-temporal Super-Resolution",
    "abstract": "Large-scale numerical simulations are capable of generating data up to terabytes or even petabytes. As a promising method of data reduction, super-resolution (SR) has been widely studied in the scientific visualization community. However, most of them are based on deep convolutional neural networks (CNNs) or generative adversarial networks (GANs) and the scale factor needs to be determined before constructing the network. As a result, a single training session only supports a fixed factor and has poor generalization ability. To address these problems, this paper proposes a Feature-Enhanced Implicit Neural Representation (FFEINR) for spatio-temporal super-resolution of flow field data. It can take full advantage of the implicit neural representation in terms of model structure and sampling resolution. The neural representation is based on a fully connected network with periodic activation functions, which enables us to obtain lightweight models. The learned continuous representation can decode the low-resolution flow field input data to arbitrary spatial and temporal resolutions, allowing for flexible upsampling. The training process of FFEINR is facilitated by introducing feature enhancements for the input layer, which complements the contextual information of the flow field.To demonstrate the effectiveness of the proposed method, a series of experiments are conducted on different datasets by setting different hyperparameters. The results show that FFEINR achieves significantly better results than the trilinear interpolation method. ",
    "url": "https://arxiv.org/abs/2308.12508",
    "authors": [
      "Chenyue Jiao",
      "Chongke Bi",
      "Lu Yang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2308.12591",
    "title": "SICNN: Soft Interference Cancellation Inspired Neural Network Equalizers",
    "abstract": "Equalization is an important task at the receiver side of a digital wireless communication system, which is traditionally conducted with model-based estimation methods. Among the numerous options for model-based equalization, iterative soft interference cancellation (SIC) is a well-performing approach since error propagation caused by hard decision data symbol estimation during the iterative estimation procedure is avoided. However, the model-based method suffers from high computational complexity and performance degradation due to required approximations. In this work, we propose a novel neural network (NN-)based equalization approach, referred to as SICNN, which is designed by deep unfolding of a model-based iterative SIC method, eliminating the main disadvantages of its model-based counterpart. We present different variants of SICNN. SICNNv1 is very similar to the model-based method, and is specifically tailored for single carrier frequency domain equalization systems, which is the communication system we regard in this work. The second variant, SICNNv2, is more universal, and is applicable as an equalizer in any communication system with a block-based data transmission scheme. We highlight the pros and cons of both variants. Moreover, for both SICNNv1 and SICNNv2 we present a version with a highly reduced number of learnable parameters. We compare the achieved bit error ratio performance of the proposed NN-based equalizers with state-of-the-art model-based and NN-based approaches, highlighting the superiority of SICNNv1 over all other methods. Also, we present a thorough complexity analysis of the proposed NN-based equalization approaches, and we investigate the influence of the training set size on the performance of NN-based equalizers. ",
    "url": "https://arxiv.org/abs/2308.12591",
    "authors": [
      "Stefan Baumgartner",
      "Oliver Lang",
      "Mario Huemer"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.12739",
    "title": "Practical limitations on robustness and scalability of quantum Internet",
    "abstract": "As quantum theory allows for information processing and computing tasks that otherwise are not possible with classical systems, there is a need and use of quantum Internet beyond existing network systems. At the same time, the realization of a desirably functional quantum Internet is hindered by fundamental and practical challenges such as high loss during transmission of quantum systems, decoherence due to interaction with the environment, fragility of quantum states, etc. We study the implications of these constraints by analyzing the limitations on the scaling and robustness of quantum Internet. Considering quantum networks, we present practical bottlenecks for secure communication, delegated computing, and resource distribution among end nodes. Motivated by the power of abstraction in graph theory (in association with quantum information theory), we consider graph-theoretic quantifiers to assess network robustness and provide critical values of communication lines for viable communication over quantum Internet. In particular, we begin by discussing limitations on usefulness of isotropic states as device-independent quantum key repeaters which otherwise could be useful for device-independent quantum key distribution. We consider some quantum networks of practical interest, ranging from satellite-based networks connecting far-off spatial locations to currently available quantum processor architectures within computers, and analyze their robustness to perform quantum information processing tasks. Some of these tasks form primitives for delegated quantum computing, e.g., entanglement distribution and quantum teleportation. For some examples of quantum networks, we present algorithms to perform different quantum network tasks of interest such as constructing the network structure, finding the shortest path between a pair of end nodes, and optimizing the flow of resources at a node. ",
    "url": "https://arxiv.org/abs/2308.12739",
    "authors": [
      "Abhishek Sadhu",
      "Meghana Ayyala Somayajula",
      "Karol Horodecki",
      "Siddhartha Das"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:1708.07604",
    "title": "A Mixed-Membership Model for Social Network Clustering",
    "abstract": " Title: A Mixed-Membership Model for Social Network Clustering ",
    "url": "https://arxiv.org/abs/1708.07604",
    "authors": [
      "Guang Ouyang",
      "Dipak K. Dey",
      "Panpan Zhang"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2107.10419",
    "title": "Trip-ROMA: Self-Supervised Learning with Triplets and Random Mappings",
    "abstract": " Comments: Accepted to Transactions on Machine Learning Research (TMLR) 2023 ",
    "url": "https://arxiv.org/abs/2107.10419",
    "authors": [
      "Wenbin Li",
      "Xuesong Yang",
      "Meihao Kong",
      "Lei Wang",
      "Jing Huo",
      "Yang Gao",
      "Jiebo Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2203.13310",
    "title": "MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection",
    "abstract": " Comments: Accepted by ICCV 2023. Code is available at this https URL ",
    "url": "https://arxiv.org/abs/2203.13310",
    "authors": [
      "Renrui Zhang",
      "Han Qiu",
      "Tai Wang",
      "Ziyu Guo",
      "Xuanzhuo Xu",
      "Ziteng Cui",
      "Yu Qiao",
      "Peng Gao",
      "Hongsheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2205.04701",
    "title": "StableDR: Stabilized Doubly Robust Learning for Recommendation on Data  Missing Not at Random",
    "abstract": " Comments: ICLR 23 ",
    "url": "https://arxiv.org/abs/2205.04701",
    "authors": [
      "Haoxuan Li",
      "Chunyuan Zheng",
      "Peng Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.11723",
    "title": "Self-Supervised Training with Autoencoders for Visual Anomaly Detection",
    "abstract": " Title: Self-Supervised Training with Autoencoders for Visual Anomaly Detection ",
    "url": "https://arxiv.org/abs/2206.11723",
    "authors": [
      "Alexander Bauer",
      "Shinichi Nakajima",
      "Klaus-Robert M\u00fcller"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2208.06228",
    "title": "Unifying Gradients to Improve Real-world Robustness for Deep Networks",
    "abstract": " Title: Unifying Gradients to Improve Real-world Robustness for Deep Networks ",
    "url": "https://arxiv.org/abs/2208.06228",
    "authors": [
      "Yingwen Wu",
      "Sizhe Chen",
      "Kun Fang",
      "Xiaolin Huang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2208.12263",
    "title": "Augmenting Reinforcement Learning with Transformer-based Scene  Representation Learning for Decision-making of Autonomous Driving",
    "abstract": " Title: Augmenting Reinforcement Learning with Transformer-based Scene  Representation Learning for Decision-making of Autonomous Driving ",
    "url": "https://arxiv.org/abs/2208.12263",
    "authors": [
      "Haochen Liu",
      "Zhiyu Huang",
      "Xiaoyu Mo",
      "Chen Lv"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2209.10634",
    "title": "Interneurons accelerate learning dynamics in recurrent neural networks  for statistical adaptation",
    "abstract": " Comments: 16 pages, 7 figures ",
    "url": "https://arxiv.org/abs/2209.10634",
    "authors": [
      "David Lipshutz",
      "Cengiz Pehlevan",
      "Dmitri B. Chklovskii"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2209.15596",
    "title": "Individual Privacy Accounting with Gaussian Differential Privacy",
    "abstract": " Comments: 31 pages, 10 figures ",
    "url": "https://arxiv.org/abs/2209.15596",
    "authors": [
      "Antti Koskela",
      "Marlon Tobaben",
      "Antti Honkela"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2210.01249",
    "title": "LOPR: Latent Occupancy PRediction using Generative Models",
    "abstract": " Title: LOPR: Latent Occupancy PRediction using Generative Models ",
    "url": "https://arxiv.org/abs/2210.01249",
    "authors": [
      "Bernard Lange",
      "Masha Itkina",
      "Mykel J. Kochenderfer"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2211.00642",
    "title": "Farm-wide virtual load monitoring for offshore wind structures via  Bayesian neural networks",
    "abstract": " Title: Farm-wide virtual load monitoring for offshore wind structures via  Bayesian neural networks ",
    "url": "https://arxiv.org/abs/2211.00642",
    "authors": [
      "N. Hlaing",
      "Pablo G. Morato",
      "F. d. N. Santos",
      "W. Weijtjens",
      "C. Devriendt",
      "P. Rigo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Computation (stat.CO)"
    ]
  },
  {
    "id": "arXiv:2212.01735",
    "title": "Neural Fourier Filter Bank",
    "abstract": " Title: Neural Fourier Filter Bank ",
    "url": "https://arxiv.org/abs/2212.01735",
    "authors": [
      "Zhijie Wu",
      "Yuhe Jin",
      "Kwang Moo Yi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2212.05976",
    "title": "DexBERT: Effective, Task-Agnostic and Fine-grained Representation  Learning of Android Bytecode",
    "abstract": " Comments: Accepted by IEEE TSE, 2023 ",
    "url": "https://arxiv.org/abs/2212.05976",
    "authors": [
      "Tiezhu Sun",
      "Kevin Allix",
      "Kisub Kim",
      "Xin Zhou",
      "Dongsun Kim",
      "David Lo",
      "Tegawend\u00e9 F. Bissyand\u00e9",
      "Jacques Klein"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2302.00747",
    "title": "Universal Soldier: Using Universal Adversarial Perturbations for  Detecting Backdoor Attacks",
    "abstract": " Title: Universal Soldier: Using Universal Adversarial Perturbations for  Detecting Backdoor Attacks ",
    "url": "https://arxiv.org/abs/2302.00747",
    "authors": [
      "Xiaoyun Xu",
      "Oguzhan Ersoy",
      "Stjepan Picek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.07584",
    "title": "Fast and Blind Speech Copy-Move Detection and Localization in Noise",
    "abstract": " Title: Fast and Blind Speech Copy-Move Detection and Localization in Noise ",
    "url": "https://arxiv.org/abs/2302.07584",
    "authors": [
      "Dong Yang",
      "Mingle Liu",
      "Muyong Cao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Information Theory (cs.IT)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2302.09624",
    "title": "Breaking the Communication-Privacy-Accuracy Tradeoff with  $f$-Differential Privacy",
    "abstract": " Title: Breaking the Communication-Privacy-Accuracy Tradeoff with  $f$-Differential Privacy ",
    "url": "https://arxiv.org/abs/2302.09624",
    "authors": [
      "Richeng Jin",
      "Zhonggen Su",
      "Caijun Zhong",
      "Zhaoyang Zhang",
      "Tony Quek",
      "Huaiyu Dai"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.13967",
    "title": "Opinion disparity in hypergraphs with community structure",
    "abstract": " Comments: 14 pages, 8 figures ",
    "url": "https://arxiv.org/abs/2302.13967",
    "authors": [
      "Nicholas W. Landry",
      "Juan G. Restrepo"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2303.12077",
    "title": "VAD: Vectorized Scene Representation for Efficient Autonomous Driving",
    "abstract": " Comments: Accepted to ICCV 2023. Code&Demos: this https URL ",
    "url": "https://arxiv.org/abs/2303.12077",
    "authors": [
      "Bo Jiang",
      "Shaoyu Chen",
      "Qing Xu",
      "Bencheng Liao",
      "Jiajie Chen",
      "Helong Zhou",
      "Qian Zhang",
      "Wenyu Liu",
      "Chang Huang",
      "Xinggang Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2304.01075",
    "title": "Conformal Prediction Regions for Time Series using Linear  Complementarity Programming",
    "abstract": " Title: Conformal Prediction Regions for Time Series using Linear  Complementarity Programming ",
    "url": "https://arxiv.org/abs/2304.01075",
    "authors": [
      "Matthew Cleaveland",
      "Insup Lee",
      "George J. Pappas",
      "Lars Lindemann"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2304.08847",
    "title": "BadVFL: Backdoor Attacks in Vertical Federated Learning",
    "abstract": " Comments: Accepted for publication at the 45th IEEE Symposium on Security & Privacy (S&P 2024). Please cite accordingly ",
    "url": "https://arxiv.org/abs/2304.08847",
    "authors": [
      "Mohammad Naseri",
      "Yufei Han",
      "Emiliano De Cristofaro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2304.09355",
    "title": "To Compress or Not to Compress- Self-Supervised Learning and Information  Theory: A Review",
    "abstract": " Title: To Compress or Not to Compress- Self-Supervised Learning and Information  Theory: A Review ",
    "url": "https://arxiv.org/abs/2304.09355",
    "authors": [
      "Ravid Shwartz-Ziv",
      "Yann LeCun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2305.06152",
    "title": "Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal  Structured Representations",
    "abstract": " Comments: Version 2.0. Improve grammar and experiments ",
    "url": "https://arxiv.org/abs/2305.06152",
    "authors": [
      "Yufeng Huang",
      "Jiji Tang",
      "Zhuo Chen",
      "Rongsheng Zhang",
      "Xinfeng Zhang",
      "Weijie Chen",
      "Zeng Zhao",
      "Zhou Zhao",
      "Tangjie Lv",
      "Zhipeng Hu",
      "Wen Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2305.10856",
    "title": "Towards an Accurate and Secure Detector against Adversarial  Perturbations",
    "abstract": " Title: Towards an Accurate and Secure Detector against Adversarial  Perturbations ",
    "url": "https://arxiv.org/abs/2305.10856",
    "authors": [
      "Chao Wang",
      "Shuren Qi",
      "Zhiqiu Huang",
      "Yushu Zhang",
      "Rushi Lan",
      "Xiaochun Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.17420",
    "title": "CCDWT-GAN: Generative Adversarial Networks Based on Color Channel Using  Discrete Wavelet Transform for Document Image Binarization",
    "abstract": " Comments: accepted by PRICAI 2023 ",
    "url": "https://arxiv.org/abs/2305.17420",
    "authors": [
      "Rui-Yang Ju",
      "Yu-Shian Lin",
      "Jen-Shiun Chiang",
      "Chih-Chia Chen",
      "Wei-Han Chen",
      "Chun-Tse Chien"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.02157",
    "title": "Transforming to Yoked Neural Networks to Improve ANN Structure",
    "abstract": " Comments: arXiv admin note: text overlap with arXiv:2008.08261 by other authors ",
    "url": "https://arxiv.org/abs/2306.02157",
    "authors": [
      "Xinshun Liu",
      "Yizhi Fang",
      "Yichao Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.04528",
    "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models  on Adversarial Prompts",
    "abstract": " Comments: Technical report; updated with new experiments and related work; 27 pages; code is at: this https URL ",
    "url": "https://arxiv.org/abs/2306.04528",
    "authors": [
      "Kaijie Zhu",
      "Jindong Wang",
      "Jiaheng Zhou",
      "Zichen Wang",
      "Hao Chen",
      "Yidong Wang",
      "Linyi Yang",
      "Wei Ye",
      "Neil Zhenqiang Gong",
      "Yue Zhang",
      "Xing Xie"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.05838",
    "title": "Expectation-Complete Graph Representations with Homomorphisms",
    "abstract": " Comments: accepted for publication at ICML 2023 ",
    "url": "https://arxiv.org/abs/2306.05838",
    "authors": [
      "Pascal Welke",
      "Maximilian Thiessen",
      "Fabian Jogl",
      "Thomas G\u00e4rtner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2306.07703",
    "title": "E2E-LOAD: End-to-End Long-form Online Action Detection",
    "abstract": " Title: E2E-LOAD: End-to-End Long-form Online Action Detection ",
    "url": "https://arxiv.org/abs/2306.07703",
    "authors": [
      "Shuqiang Cao",
      "Weixin Luo",
      "Bairui Wang",
      "Wei Zhang",
      "Lin Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.10294",
    "title": "A new approach based on quadratic forms to attack the McEliece  cryptosystem",
    "abstract": " Comments: 68 pages ",
    "url": "https://arxiv.org/abs/2306.10294",
    "authors": [
      "Alain Couvreur",
      "Rocco Mora",
      "Jean-Pierre Tillich"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2306.10466",
    "title": "Graph Ladling: Shockingly Simple Parallel GNN Training without  Intermediate Communication",
    "abstract": " Comments: Accepted in ICML 2023. Included comparison with a concurrent work (Jiong et. al. 2023) which independently presents similar ideas, among other SOTA distributed GNN training works ",
    "url": "https://arxiv.org/abs/2306.10466",
    "authors": [
      "Ajay Jaiswal",
      "Shiwei Liu",
      "Tianlong Chen",
      "Ying Ding",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.09323",
    "title": "Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking  Portrait Synthesis",
    "abstract": " Comments: Accepted by ICCV 2023. Project page: this https URL ",
    "url": "https://arxiv.org/abs/2307.09323",
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Xiao Bai",
      "Jun Zhou",
      "Lin Gu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2307.11482",
    "title": "R2Det: Redemption from Range-view for Accurate 3D Object Detection",
    "abstract": " Title: R2Det: Redemption from Range-view for Accurate 3D Object Detection ",
    "url": "https://arxiv.org/abs/2307.11482",
    "authors": [
      "Yihan Wang",
      "Qiao Yan",
      "Yi Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2307.11942",
    "title": "DeepMartNet -- A Martingale based Deep Neural Network Learning Algorithm  for Eigenvalue/BVP Problems and Optimal Stochastic Controls",
    "abstract": " Comments: update the loss function to enforce full Martingale property ",
    "url": "https://arxiv.org/abs/2307.11942",
    "authors": [
      "Wei Cai"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2307.14297",
    "title": "Robust Regret Optimal Control",
    "abstract": " Title: Robust Regret Optimal Control ",
    "url": "https://arxiv.org/abs/2307.14297",
    "authors": [
      "Jietian Liu",
      "Peter Seiler"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2308.03374",
    "title": "Heterogeneous Forgetting Compensation for Class-Incremental Learning",
    "abstract": " Comments: Accepted to ICCV2023 ",
    "url": "https://arxiv.org/abs/2308.03374",
    "authors": [
      "Jiahua Dong",
      "Wenqi Liang",
      "Yang Cong",
      "Gan Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.04706",
    "title": "Pareto Invariant Representation Learning for Multimedia Recommendation",
    "abstract": " Comments: ACM MM 2023 full paper ",
    "url": "https://arxiv.org/abs/2308.04706",
    "authors": [
      "Shanshan Huang",
      "Haoxuan Li",
      "Qingsong Li",
      "Chunyuan Zheng",
      "Li Liu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.05983",
    "title": "Face Encryption via Frequency-Restricted Identity-Agnostic Attacks",
    "abstract": " Title: Face Encryption via Frequency-Restricted Identity-Agnostic Attacks ",
    "url": "https://arxiv.org/abs/2308.05983",
    "authors": [
      "Xin Dong",
      "Rui Wang",
      "Siyuan Liang",
      "Aishan Liu",
      "Lihua Jing"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2308.06534",
    "title": "Dealing with Small Datasets for Deep Learning in Medical Imaging: An  Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive  and Masked Autoencoder Methods for Convolutional Models",
    "abstract": " Comments: This paper is under review. The code will be released if accepted ",
    "url": "https://arxiv.org/abs/2308.06534",
    "authors": [
      "Daniel Wolf",
      "Tristan Payer",
      "Catharina Silvia Lisson",
      "Christoph Gerhard Lisson",
      "Meinrad Beer",
      "Timo Ropinski",
      "Michael G\u00f6tz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.07134",
    "title": "Natural Language is All a Graph Needs",
    "abstract": " Comments: 21 pages, 2 figures, 5 tables ",
    "url": "https://arxiv.org/abs/2308.07134",
    "authors": [
      "Ruosong Ye",
      "Caiqi Zhang",
      "Runhui Wang",
      "Shuyuan Xu",
      "Yongfeng Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.08741",
    "title": "MIPS-Fusion: Multi-Implicit-Submaps for Scalable and Robust Online  Neural RGB-D Reconstruction",
    "abstract": " Title: MIPS-Fusion: Multi-Implicit-Submaps for Scalable and Robust Online  Neural RGB-D Reconstruction ",
    "url": "https://arxiv.org/abs/2308.08741",
    "authors": [
      "Yijie Tang",
      "Jiazhao Zhang",
      "Zhinan Yu",
      "He Wang",
      "Kai Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2308.09113",
    "title": "Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale  Geological Carbon Storage",
    "abstract": " Title: Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale  Geological Carbon Storage ",
    "url": "https://arxiv.org/abs/2308.09113",
    "authors": [
      "Hewei Tang",
      "Qingkai Kong",
      "Joseph P. Morris"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.10086",
    "title": "Connecting the Dots: Leveraging Social Network Analysis to Understand  and Optimize Collaborative Dynamics Within the Global Film Production Network",
    "abstract": " Title: Connecting the Dots: Leveraging Social Network Analysis to Understand  and Optimize Collaborative Dynamics Within the Global Film Production Network ",
    "url": "https://arxiv.org/abs/2308.10086",
    "authors": [
      "Mehrdad Maghsoudi",
      "Saeid Aliakbar",
      "Sajjad HabibiPour"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2308.10438",
    "title": "Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep  Neural Networks",
    "abstract": " Title: Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep  Neural Networks ",
    "url": "https://arxiv.org/abs/2308.10438",
    "authors": [
      "Kaixin Xu",
      "Zhe Wang",
      "Xue Geng",
      "Jie Lin",
      "Min Wu",
      "Xiaoli Li",
      "Weisi Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.11381",
    "title": "DALNet: A Rail Detection Network Based on Dynamic Anchor Line",
    "abstract": " Title: DALNet: A Rail Detection Network Based on Dynamic Anchor Line ",
    "url": "https://arxiv.org/abs/2308.11381",
    "authors": [
      "Zichen Yu",
      "Quanli Liu",
      "Wei Wang",
      "Liyong Zhang",
      "Xiaoguang Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.11881",
    "title": "Adversarial Training Using Feedback Loops",
    "abstract": " Title: Adversarial Training Using Feedback Loops ",
    "url": "https://arxiv.org/abs/2308.11881",
    "authors": [
      "Ali Haisam Muhammad Rafid",
      "Adrian Sandu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2308.11909",
    "title": "Edge-aware Hard Clustering Graph Pooling for Brain Imaging Data",
    "abstract": " Title: Edge-aware Hard Clustering Graph Pooling for Brain Imaging Data ",
    "url": "https://arxiv.org/abs/2308.11909",
    "authors": [
      "Cheng Zhu",
      "Jiayi Zhu",
      "Lijuan Zhang",
      "Xi Wu",
      "Shuqi Yang",
      "Ping Liang",
      "Honghan Chen",
      "Ying Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2308.11911",
    "title": "ACLS: Adaptive and Conditional Label Smoothing for Network Calibration",
    "abstract": " Comments: Accepted to ICCV 2023 (Oral presentation) ",
    "url": "https://arxiv.org/abs/2308.11911",
    "authors": [
      "Hyekang Park",
      "Jongyoun Noh",
      "Youngmin Oh",
      "Donghyeon Baek",
      "Bumsub Ham"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.12044",
    "title": "A multiobjective continuation method to compute the regularization path  of deep neural networks",
    "abstract": " Comments: 7 pages, 6 figures ",
    "url": "https://arxiv.org/abs/2308.12044",
    "authors": [
      "Augustina C. Amakor",
      "Konstantin Sonntag",
      "Sebastian Peitz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  }
]