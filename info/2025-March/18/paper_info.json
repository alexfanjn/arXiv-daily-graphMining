[
  {
    "id": "arXiv:2503.11657",
    "title": "Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs",
    "abstract": "           Large Language Models have demonstrated remarkable capabilities in natural language processing tasks, including mathematical problem-solving that requires multi-step logical reasoning. However, challenges persist in automating the identification of key mathematical concepts, understanding their interrelations, and formalizing proofs within a rigorous framework. We present a novel framework that leverages knowledge graphs to augment LLMs to construct and formalize mathematical proofs. Our results demonstrate significant performance improvements across multiple datasets, with using knowledge graphs, achieving up to a 34% success rate on the MUSTARDSAUCE dataset on o1-mini and consistently outperforming baseline approaches by 2-11% across different models. We show how this approach bridges the gap between natural language understanding and formal logic proof systems and achieve elevated results for foundation models over baseline.         ",
    "url": "https://arxiv.org/abs/2503.11657",
    "authors": [
      "Vincent Li",
      "Yule Fu",
      "Tim Knappe",
      "Kevin Han",
      "Kevin Zhu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.11658",
    "title": "Circuit Diagram Retrieval Based on Hierarchical Circuit Graph Representation",
    "abstract": "           In the domain of analog circuit design, the retrieval of circuit diagrams has drawn a great interest, primarily due to its vital role in the consultation of legacy designs and the detection of design plagiarism. Existing image retrieval techniques are adept at handling natural images, which converts images into feature vectors and retrieval similar images according to the closeness of these vectors. Nonetheless, these approaches exhibit limitations when applied to the more specialized and intricate domain of circuit diagrams. This paper presents a novel approach to circuit diagram retrieval by employing a graph representation of circuit diagrams, effectively reformulating the retrieval task as a graph retrieval problem. The proposed methodology consists of two principal components: a circuit diagram recognition algorithm designed to extract the circuit components and topological structure of the circuit using proposed GAM-YOLO model and a 2-step connected domain filtering algorithm, and a hierarchical retrieval strategy based on graph similarity and different graph representation methods for analog circuits. Our methodology pioneers the utilization of graph representation in the retrieval of circuit diagrams, incorporating topological features that are commonly overlooked by standard image retrieval methods. The results of our experiments substantiate the efficacy of our approach in retrieving circuit diagrams across of different types.         ",
    "url": "https://arxiv.org/abs/2503.11658",
    "authors": [
      "Ming Gao",
      "Ruichen Qiu",
      "Zeng Hui Chang",
      "Kanjian Zhang",
      "Haikun Wei",
      "Hong Cai Chen"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.11677",
    "title": "Simulation of prosthetic vision with PRIMA system and enhancement of face representation",
    "abstract": "           Objective. Patients implanted with the PRIMA photovoltaic subretinal prosthesis in geographic atrophy report form vision with the average acuity matching the 100um pixel size. Although this remarkable outcome enables them to read and write, they report difficulty with perceiving faces. This paper provides a novel, non-pixelated algorithm for simulating prosthetic vision the way it is experienced by PRIMA patients, compares the algorithm's predictions to clinical perceptual outcomes, and offers computer vision and machine learning (ML) methods to improve face representation. Approach. Our simulation algorithm integrates a grayscale filter, spatial resolution filter, and contrast filter. This accounts for the limited sampling density of the retinal implant, as well as the reduced contrast sensitivity of prosthetic vision. Patterns of Landolt C and faces created using this simulation algorithm are compared to reports from actual PRIMA users. To recover the facial features lost in prosthetic vision, we apply an ML facial landmarking model as well as contrast adjusting tone curves to the face image prior to its projection onto the implant. Main results. Simulated prosthetic vision matches the maximum letter acuity observed in clinical studies as well as patients' subjective descriptions. Application of the inversed contrast filter helps preserve the contrast in prosthetic vision. Identification of the facial features using an ML facial landmarking model and accentuating them further improve face representation. Significance. Spatial and contrast constraints of prosthetic vision limit resolvable features and degrade natural images. ML based methods and contrast adjustments mitigate some limitations and improve face representation. Even though higher spatial resolution can be expected with implants having smaller pixels, contrast enhancement still remains essential for face recognition.         ",
    "url": "https://arxiv.org/abs/2503.11677",
    "authors": [
      "Jungyeon Park",
      "Anna Kochnev Goldstein",
      "Yueming Zhou",
      "Nathan Jensen",
      "Daniel Palanker"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.11684",
    "title": "Exploring Causality for HRI: A Case Study on Robotic Mental Well-being Coaching",
    "abstract": "           One of the primary goals of Human-Robot Interaction (HRI) research is to develop robots that can interpret human behavior and adapt their responses accordingly. Adaptive learning models, such as continual and reinforcement learning, play a crucial role in improving robots' ability to interact effectively in real-world settings. However, these models face significant challenges due to the limited availability of real-world data, particularly in sensitive domains like healthcare and well-being. This data scarcity can hinder a robot's ability to adapt to new situations. To address these challenges, causality provides a structured framework for understanding and modeling the underlying relationships between actions, events, and outcomes. By moving beyond mere pattern recognition, causality enables robots to make more explainable and generalizable decisions. This paper presents an exploratory causality-based analysis through a case study of an adaptive robotic coach delivering positive psychology exercises over four weeks in a workplace setting. The robotic coach autonomously adapts to multimodal human behaviors, such as facial valence and speech duration. By conducting both macro- and micro-level causal analyses, this study aims to gain deeper insights into how adaptability can enhance well-being during interactions. Ultimately, this research seeks to advance our understanding of how causality can help overcome challenges in HRI, particularly in real-world applications.         ",
    "url": "https://arxiv.org/abs/2503.11684",
    "authors": [
      "Micol Spitale",
      "Srikar Babu",
      "Serhan Cakmak",
      "Jiaee Cheong",
      "Hatice Gunes"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.11699",
    "title": "Propensity Formation-Containment Control of Fully Heterogeneous Multi-Agent Systems via Online Data-Driven Learning",
    "abstract": "           This paper introduces an online data-driven learning scheme designed to address a novel problem in propensity formation and containment control for fully heterogeneous multi-agent systems. Unlike traditional approaches that rely on the eigenvalues of the Laplacian matrix, this problem considers the determination of follower positions based on propensity factors released by leaders. To address the challenge of incomplete utilization of leader information in existing multi-leader control methods, the concept of an influential transit formation leader (ITFL) is introduced. An adaptive observer is developed for the agents, including the ITFL, to estimate the state of the tracking leader or the leader's formation. Building on these observations, a model-based control protocol is proposed, elucidating the relationship between the regulation equations and control gains, ensuring the asymptotic convergence of the agent's state. To eliminate the necessity for model information throughout the control process, a new online data-driven learning algorithm is devised for the control protocol. Finally, numerical simulation results are given to verify the effectiveness of the proposed method.         ",
    "url": "https://arxiv.org/abs/2503.11699",
    "authors": [
      "Ao Cao",
      "Fuyong Wang",
      "Zhongxin Liu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.11703",
    "title": "Physical knowledge improves prediction of EM Fields",
    "abstract": "           We propose a 3D U-Net model to predict the spatial distribution of electromagnetic fields inside a radio-frequency (RF) coil with a subject present, using the phase, amplitude, and position of the coils, along with the density, permittivity, and conductivity of the surrounding medium as inputs. To improve accuracy, we introduce a physics-augmented variant, U-Net Phys, which incorporates Gauss's law of magnetism into the loss function using finite differences. We train our models on electromagnetic field simulations from CST Studio Suite for an eight-channel dipole array RF coil at 7T MRI. Experimental results show that U-Net Phys significantly outperforms the standard U-Net, particularly in predicting fields within the subject, demonstrating the advantage of integrating physical constraints into deep learning-based field prediction.         ",
    "url": "https://arxiv.org/abs/2503.11703",
    "authors": [
      "Andrzej Dulny",
      "Farzad Jabbarigargari",
      "Andreas Hotho",
      "Laura Maria Schreiber",
      "Maxim Terekhov",
      "Anna Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.11709",
    "title": "Conformal Prediction and Human Decision Making",
    "abstract": "           Methods to quantify uncertainty in predictions from arbitrary models are in demand in high-stakes domains like medicine and finance. Conformal prediction has emerged as a popular method for producing a set of predictions with specified average coverage, in place of a single prediction and confidence value. However, the value of conformal prediction sets to assist human decisions remains elusive due to the murky relationship between coverage guarantees and decision makers' goals and strategies. How should we think about conformal prediction sets as a form of decision support? Under what conditions do we expect the support they provide to be superior versus inferior to that of alternative presentations of predictive uncertainty? We outline a decision theoretic framework for evaluating predictive uncertainty as informative signals, then contrast what can be said within this framework about idealized use of calibrated probabilities versus conformal prediction sets. Informed by prior empirical results and theories of human decisions under uncertainty, we formalize a set of possible strategies by which a decision maker might use a prediction set. We identify ways in which conformal prediction sets and posthoc predictive uncertainty quantification more broadly are in tension with common goals and needs in human-AI decision making. We give recommendations for future research in predictive uncertainty quantification to support human decision makers.         ",
    "url": "https://arxiv.org/abs/2503.11709",
    "authors": [
      "Jessica Hullman",
      "Yifan Wu",
      "Dawei Xie",
      "Ziyang Guo",
      "Andrew Gelman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.11710",
    "title": "ConjointNet: Enhancing Conjoint Analysis for Preference Prediction with Representation Learning",
    "abstract": "           Understanding consumer preferences is essential to product design and predicting market response to these new products. Choice-based conjoint analysis is widely used to model user preferences using their choices in surveys. However, traditional conjoint estimation techniques assume simple linear models. This assumption may lead to limited predictability and inaccurate estimation of product attribute contributions, especially on data that has underlying non-linear relationships. In this work, we employ representation learning to efficiently alleviate this issue. We propose ConjointNet, which is composed of two novel neural architectures, to predict user preferences. We demonstrate that the proposed ConjointNet models outperform traditional conjoint estimate techniques on two preference datasets by over 5%, and offer insights into non-linear feature interactions.         ",
    "url": "https://arxiv.org/abs/2503.11710",
    "authors": [
      "Yanxia Zhang",
      "Francine Chen",
      "Shabnam Hakimi",
      "Totte Harinen",
      "Alex Filipowicz",
      "Yan-Ying Chen",
      "Rumen Iliev",
      "Nikos Arechiga",
      "Kalani Murakami",
      "Kent Lyons",
      "Charlene Wu",
      "Matt Klenk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.11713",
    "title": "Revisiting the Predictability of Performative, Social Events",
    "abstract": "           Social predictions do not passively describe the future; they actively shape it. They inform actions and change individual expectations in ways that influence the likelihood of the predicted outcome. Given these dynamics, to what extent can social events be predicted? This question was discussed throughout the 20th century by authors like Merton, Morgenstern, Simon, and others who considered it a central issue in social science methodology. In this work, we provide a modern answer to this old problem. Using recent ideas from performative prediction and outcome indistinguishability, we establish that one can always efficiently predict social events accurately, regardless of how predictions influence data. While achievable, we also show that these predictions are often undesirable, highlighting the limitations of previous desiderata. We end with a discussion of various avenues forward.         ",
    "url": "https://arxiv.org/abs/2503.11713",
    "authors": [
      "Juan C. Perdomo"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Theoretical Economics (econ.TH)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.11714",
    "title": "Conversation Networks",
    "abstract": "           Picture a community torn over a proposed zoning law. Some are angry, others defensive, and misunderstandings abound. On social media, they broadcast insults at one another; every nuanced perspective is reduced to a viral soundbite. Yet, when they meet face-to-face and start speaking, something changes: residents begin listening more than speaking, and people begin testing ideas together. Misunderstandings fade, and trust begins to form. By the end of their discussion, they have not only softened their hostility, but discovered actionable plans that benefit everyone. This is the kind of meaningful discourse our society desperately needs. Yet our digital platforms -- designed primarily for maximizing engagement through provocative content -- have pulled us away from these core community endeavours. As a constructive path forward, we introduce the idea of conversation networks as a basis for civic communication infrastructure that combines interoperable digital apps with the thoughtful integration of AI guided by human agency.         ",
    "url": "https://arxiv.org/abs/2503.11714",
    "authors": [
      "Deb Roy",
      "Lawrence Lessig",
      "Audrey Tang"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2503.11716",
    "title": "A Robust and Energy-Efficient Trajectory Planning Framework for High-Degree-of-Freedom Robots",
    "abstract": "           Energy efficiency and motion smoothness are essential in trajectory planning for high-degree-of-freedom robots to ensure optimal performance and reduce mechanical wear. This paper presents a novel framework integrating sinusoidal trajectory generation with velocity scaling to minimize energy consumption while maintaining motion accuracy and smoothness. The framework is evaluated using a physics-based simulation environment with metrics such as energy consumption, motion smoothness, and trajectory accuracy. Results indicate significant energy savings and smooth transitions, demonstrating the framework's effectiveness for precision-based applications. Future work includes real-time trajectory adjustments and enhanced energy models.         ",
    "url": "https://arxiv.org/abs/2503.11716",
    "authors": [
      "Sajjad Hussain",
      "Md Saad",
      "Almas Baimagambetov",
      "Khizer Saeed"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.11718",
    "title": "The Relativity of Causal Knowledge",
    "abstract": "           Recent advances in artificial intelligence reveal the limits of purely predictive systems and call for a shift toward causal and collaborative reasoning. Drawing inspiration from the revolution of Grothendieck in mathematics, we introduce the relativity of causal knowledge, which posits structural causal models (SCMs) are inherently imperfect, subjective representations embedded within networks of relationships. By leveraging category theory, we arrange SCMs into a functor category and show that their observational and interventional probability measures naturally form convex structures. This result allows us to encode non-intervened SCMs with convex spaces of probability measures. Next, using sheaf theory, we construct the network sheaf and cosheaf of causal knowledge. These structures enable the transfer of causal knowledge across the network while incorporating interventional consistency and the perspective of the subjects, ultimately leading to the formal, mathematical definition of relative causal knowledge.         ",
    "url": "https://arxiv.org/abs/2503.11718",
    "authors": [
      "Gabriele D'Acunto",
      "Claudio Battiloro"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Category Theory (math.CT)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2503.11726",
    "title": "SPECTra: Scalable Multi-Agent Reinforcement Learning with Permutation-Free Networks",
    "abstract": "           In cooperative multi-agent reinforcement learning (MARL), the permutation problem where the state space grows exponentially with the number of agents reduces sample efficiency. Additionally, many existing architectures struggle with scalability, relying on a fixed structure tied to a specific number of agents, limiting their applicability to environments with a variable number of entities. While approaches such as graph neural networks (GNNs) and self-attention mechanisms have progressed in addressing these challenges, they have significant limitations as dense GNNs and self-attention mechanisms incur high computational costs. To overcome these limitations, we propose a novel agent network and a non-linear mixing network that ensure permutation-equivariance and scalability, allowing them to generalize to environments with various numbers of agents. Our agent network significantly reduces computational complexity, and our scalable hypernetwork enables efficient weight generation for non-linear mixing. Additionally, we introduce curriculum learning to improve training efficiency. Experiments on SMACv2 and Google Research Football (GRF) demonstrate that our approach achieves superior learning performance compared to existing methods. By addressing both permutation-invariance and scalability in MARL, our work provides a more efficient and adaptable framework for cooperative MARL. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.11726",
    "authors": [
      "Hyunwoo Park",
      "Baekryun Seong",
      "Sang-Ki Ko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.11727",
    "title": "Survey of City-Wide Homelessness Detection Through Environmental Sensing",
    "abstract": "           The growing homelessness crisis in the U.S. presents complex social, economic, and public health challenges, straining shelters, healthcare, and social services while limiting effective interventions. Traditional assessment methods struggle to capture its dynamic, dispersed nature, highlighting the need for scalable, data-driven detection. This survey explores computational approaches across four domains: (1) computer vision and deep learning for identifying encampments and urban indicators, (2) air quality sensing via fixed, mobile, and crowdsourced deployments, (3) IoT and edge computing for real-time urban monitoring, and (4) pedestrian behavior analysis for mobility patterns. Despite advancements, challenges persist in computational constraints, data privacy, accurate environmental measurement, and adaptability. This survey synthesizes recent research, identifies key gaps, and highlights opportunities to enhance homelessness detection, optimize resource allocation, and improve urban planning and social support systems for equitable aid distribution and better neighborhood conditions.         ",
    "url": "https://arxiv.org/abs/2503.11727",
    "authors": [
      "Julia Gersey",
      "Rose Allegrette",
      "Joshua Lian",
      "Zawad Munshi",
      "Aarti Phatke"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2503.11730",
    "title": "BACE-RUL: A Bi-directional Adversarial Network with Covariate Encoding for Machine Remaining Useful Life Prediction",
    "abstract": "           Prognostic and Health Management (PHM) are crucial ways to avoid unnecessary maintenance for Cyber-Physical Systems (CPS) and improve system reliability. Predicting the Remaining Useful Life (RUL) is one of the most challenging tasks for PHM. Existing methods require prior knowledge about the system, contrived assumptions, or temporal mining to model the life cycles of machine equipment/devices, resulting in diminished accuracy and limited applicability in real-world scenarios. This paper proposes a Bi-directional Adversarial network with Covariate Encoding for machine Remaining Useful Life (BACE-RUL) prediction, which only adopts sensor measurements from the current life cycle to predict RUL rather than relying on previous consecutive cycle recordings. The current sensor measurements of mechanical devices are encoded to a conditional space to better understand the implicit inner mechanical status. The predictor is trained as a conditional generative network with the encoded sensor measurements as its conditions. Various experiments on several real-world datasets, including the turbofan aircraft engine dataset and the dataset collected from degradation experiments of Li-Ion battery cells, show that the proposed model is a general framework and outperforms state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2503.11730",
    "authors": [
      "Zekai Zhang",
      "Dan Li",
      "Shunyu Wu",
      "Junya Cai",
      "Bo Zhang",
      "See Kiong Ng",
      "Zibin Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.11736",
    "title": "A Smooth Analytical Formulation of Collision Detection and Rigid Body Dynamics With Contact",
    "abstract": "           Generating intelligent robot behavior in contact-rich settings is a research problem where zeroth-order methods currently prevail. A major contributor to the success of such methods is their robustness in the face of non-smooth and discontinuous optimization landscapes that are characteristic of contact interactions, yet zeroth-order methods remain computationally inefficient. It is therefore desirable to develop methods for perception, planning and control in contact-rich settings that can achieve further efficiency by making use of first and second order information (i.e., gradients and Hessians). To facilitate this, we present a joint formulation of collision detection and contact modelling which, compared to existing differentiable simulation approaches, provides the following benefits: i) it results in forward and inverse dynamics that are entirely analytical (i.e. do not require solving optimization or root-finding problems with iterative methods) and smooth (i.e. twice differentiable), ii) it supports arbitrary collision geometries without needing a convex decomposition, and iii) its runtime is independent of the number of contacts. Through simulation experiments, we demonstrate the validity of the proposed formulation as a \"physics for inference\" that can facilitate future development of efficient methods to generate intelligent contact-rich behavior.         ",
    "url": "https://arxiv.org/abs/2503.11736",
    "authors": [
      "Onur Beker",
      "Nico G\u00fcrtler",
      "Ji Shi",
      "A. Ren\u00e9 Geist",
      "Amirreza Razmjoo",
      "Georg Martius",
      "Sylvain Calinon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2503.11737",
    "title": "Multi-View Node Pruning for Accurate Graph Representation",
    "abstract": "           Graph pooling, which compresses a whole graph into a smaller coarsened graph, is an essential component of graph representation learning. To efficiently compress a given graph, graph pooling methods often drop their nodes with attention-based scoring with the task loss. However, this often results in simply removing nodes with lower degrees without consideration of their feature-level relevance to the given task. To fix this problem, we propose a Multi-View Pruning(MVP), a graph pruning method based on a multi-view framework and reconstruction loss. Given a graph, MVP first constructs multiple graphs for different views either by utilizing the predefined modalities or by randomly partitioning the input features, to consider the importance of each node in diverse perspectives. Then, it learns the score for each node by considering both the reconstruction and the task loss. MVP can be incorporated with any hierarchical pooling framework to score the nodes. We validate MVP on multiple benchmark datasets by coupling it with two graph pooling methods, and show that it significantly improves the performance of the base graph pooling method, outperforming all baselines. Further analysis shows that both the encoding of multiple views and the consideration of reconstruction loss are the key to the success of MVP, and that it indeed identifies nodes that are less important according to domain knowledge.         ",
    "url": "https://arxiv.org/abs/2503.11737",
    "authors": [
      "Jiseong Park",
      "Hanjin Kim",
      "Seojin Kim",
      "Jueun Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.11741",
    "title": "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba for Enhanced Biosignal Classification",
    "abstract": "           Biological signals, such as electroencephalograms (EEGs) and electrocardiograms (ECGs), play a pivotal role in numerous clinical practices, such as diagnosing brain and cardiac arrhythmic diseases. Existing methods for biosignal classification rely on Attention-based frameworks with dense Feed Forward layers, which lead to inefficient learning, high computational overhead, and suboptimal performance. In this work, we introduce BioMamba, a Spectro-Temporal Embedding strategy applied to the Bidirectional Mamba framework with Sparse Feed Forward layers to enable effective learning of biosignal sequences. By integrating these three key components, BioMamba effectively addresses the limitations of existing methods. Extensive experiments demonstrate that BioMamba significantly outperforms state-of-the-art methods with marked improvement in classification performance. The advantages of the proposed BioMamba include (1) Reliability: BioMamba consistently delivers robust results, confirmed across six evaluation metrics. (2) Efficiency: We assess both model and training efficiency, the BioMamba demonstrates computational effectiveness by reducing model size and resource consumption compared to existing approaches. (3) Generality: With the capacity to effectively classify a diverse set of tasks, BioMamba demonstrates adaptability and effectiveness across various domains and applications.         ",
    "url": "https://arxiv.org/abs/2503.11741",
    "authors": [
      "Jian Qian",
      "Teck Lun Goh",
      "Bingyu Xie",
      "Chengyao Zhu",
      "Biao Wan",
      "Yawen Guan",
      "Patrick Yin Chiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.11751",
    "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs",
    "abstract": "           Reward models have become a staple in modern NLP, serving as not only a scalable text evaluator, but also an indispensable component in many alignment recipes and inference-time algorithms. However, while recent reward models increase performance on standard benchmarks, this may partly be due to overfitting effects, which would confound an understanding of their true capability. In this work, we scrutinize the robustness of reward models and the extent of such overfitting. We build **reWordBench**, which systematically transforms reward model inputs in meaning- or ranking-preserving ways. We show that state-of-the-art reward models suffer from substantial performance degradation even with minor input transformations, sometimes dropping to significantly below-random accuracy, suggesting brittleness. To improve reward model robustness, we propose to explicitly train them to assign similar scores to paraphrases, and find that this approach also improves robustness to other distinct kinds of transformations. For example, our robust reward model reduces such degradation by roughly half for the Chat Hard subset in RewardBench. Furthermore, when used in alignment, our robust reward models demonstrate better utility and lead to higher-quality outputs, winning in up to 59% of instances against a standardly trained RM.         ",
    "url": "https://arxiv.org/abs/2503.11751",
    "authors": [
      "Zhaofeng Wu",
      "Michihiro Yasunaga",
      "Andrew Cohen",
      "Yoon Kim",
      "Asli Celikyilmaz",
      "Marjan Ghazvininejad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.11780",
    "title": "Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning",
    "abstract": "           Multi-Modal Object Detection (MMOD), due to its stronger adaptability to various complex environments, has been widely applied in various applications. Extensive research is dedicated to the RGB-IR object detection, primarily focusing on how to integrate complementary features from RGB-IR modalities. However, they neglect the mono-modality insufficient learning problem that the decreased feature extraction capability in multi-modal joint learning. This leads to an unreasonable but prevalent phenomenon--Fusion Degradation, which hinders the performance improvement of the MMOD model. Motivated by this, in this paper, we introduce linear probing evaluation to the multi-modal detectors and rethink the multi-modal object detection task from the mono-modality learning perspective. Therefore, we construct an novel framework called M$^2$D-LIF, which consists of the Mono-Modality Distillation (M$^2$D) method and the Local Illumination-aware Fusion (LIF) module. The M$^2$D-LIF framework facilitates the sufficient learning of mono-modality during multi-modal joint training and explores a lightweight yet effective feature fusion manner to achieve superior object detection performance. Extensive experiments conducted on three MMOD datasets demonstrate that our M$^2$D-LIF effectively mitigates the Fusion Degradation phenomenon and outperforms the previous SOTA detectors.         ",
    "url": "https://arxiv.org/abs/2503.11780",
    "authors": [
      "Tianyi Zhao",
      "Boyang Liu",
      "Yanglei Gao",
      "Yiming Sun",
      "Maoxun Yuan",
      "Xingxing Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.11781",
    "title": "Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks",
    "abstract": "           We present cmKAN, a versatile framework for color matching. Given an input image with colors from a source color distribution, our method effectively and accurately maps these colors to match a target color distribution in both supervised and unsupervised settings. Our framework leverages the spline capabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching between source and target distributions. Specifically, we developed a hypernetwork that generates spatially varying weight maps to control the nonlinear splines of a KAN, enabling accurate color matching. As part of this work, we introduce a first large-scale dataset of paired images captured by two distinct cameras and evaluate the efficacy of our and existing methods in matching colors. We evaluated our approach across various color-matching tasks, including: (1) raw-to-raw mapping, where the source color distribution is in one camera's raw color space and the target in another camera's raw space; (2) raw-to-sRGB mapping, where the source color distribution is in a camera's raw space and the target is in the display sRGB space, emulating the color rendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to transfer colors from a source sRGB space (e.g., produced by a source camera ISP) to a target sRGB space (e.g., from a different camera ISP). The results show that our method outperforms existing approaches by 37.3% on average for supervised and unsupervised cases while remaining lightweight compared to other methods. The codes, dataset, and pre-trained models are available at: this https URL ",
    "url": "https://arxiv.org/abs/2503.11781",
    "authors": [
      "Artem Nikonorov",
      "Georgy Perevozchikov",
      "Andrei Korepanov",
      "Nancy Mehta",
      "Mahmoud Afifi",
      "Egor Ershov",
      "Radu Timofte"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.11786",
    "title": "Tensor Convolutional Network for Higher-Order Interaction Prediction in Sparse Tensors",
    "abstract": "           Many real-world data, such as recommendation data and temporal graphs, can be represented as incomplete sparse tensors where most entries are unobserved. For such sparse tensors, identifying the top-k higher-order interactions that are most likely to occur among unobserved ones is crucial. Tensor factorization (TF) has gained significant attention in various tensor-based applications, serving as an effective method for finding these top-k potential interactions. However, existing TF methods primarily focus on effectively fusing latent vectors of entities, which limits their expressiveness. Since most entities in sparse tensors have only a few interactions, their latent representations are often insufficiently trained. In this paper, we propose TCN, an accurate and compatible tensor convolutional network that integrates seamlessly with existing TF methods for predicting higher-order interactions. We design a highly effective encoder to generate expressive latent vectors of entities. To achieve this, we propose to (1) construct a graph structure derived from a sparse tensor and (2) develop a relation-aware encoder, TCN, that learns latent representations of entities by leveraging the graph structure. Since TCN complements traditional TF methods, we seamlessly integrate TCN with existing TF methods, enhancing the performance of predicting top-k interactions. Extensive experiments show that TCN integrated with a TF method outperforms competitors, including TF methods and a hyperedge prediction method. Moreover, TCN is broadly compatible with various TF methods and GNNs (Graph Neural Networks), making it a versatile solution.         ",
    "url": "https://arxiv.org/abs/2503.11786",
    "authors": [
      "Jun-Gi Jang",
      "Jingrui He",
      "Andrew Margenot",
      "Hanghang Tong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.11790",
    "title": "Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs",
    "abstract": "           Human reasoning relies on constructing and manipulating mental models-simplified internal representations of situations that we use to understand and solve problems. Conceptual diagrams (for example, sketches drawn by humans to aid reasoning) externalize these mental models, abstracting irrelevant details to efficiently capture relational and spatial information. In contrast, Large Language Models (LLMs) and Large Multimodal Models (LMMs) predominantly reason through textual representations, limiting their effectiveness in complex multi-step combinatorial and planning tasks. In this paper, we propose a zero-shot fully automatic framework that enables LMMs to reason through multiple chains of self-generated intermediate conceptual diagrams, significantly enhancing their combinatorial planning capabilities. Our approach does not require any human initialization beyond a natural language description of the task. It integrates both textual and diagrammatic reasoning within an optimized graph-of-thought inference framework, enhanced by beam search and depth-wise backtracking. Evaluated on multiple challenging PDDL planning domains, our method substantially improves GPT-4o's performance (for example, from 35.5% to 90.2% in Blocksworld). On more difficult planning domains with solution depths up to 40, our approach outperforms even the o1-preview reasoning model (for example, over 13% improvement in Parking). These results highlight the value of conceptual diagrams as a complementary reasoning medium in LMMs.         ",
    "url": "https://arxiv.org/abs/2503.11790",
    "authors": [
      "Nasim Borazjanizadeh",
      "Roei Herzig",
      "Eduard Oks",
      "Trevor Darrell",
      "Rogerio Feris",
      "Leonid Karlinsky"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.11808",
    "title": "The Architecture and Evaluation of Bayesian Neural Networks",
    "abstract": "           As modern neural networks get more complex, specifying a model with high predictive performance and sound uncertainty quantification becomes a more challenging task. Despite some promising theoretical results on the true posterior predictive distribution of Bayesian neural networks, the properties of even the most commonly used posterior approximations are often questioned. Computational burdens and intractable posteriors expose miscalibrated Bayesian neural networks to poor accuracy and unreliable uncertainty estimates. Approximate Bayesian inference aims to replace unknown and intractable posterior distributions with some simpler but feasible distributions. The dimensions of modern deep models coupled with the lack of identifiability make Markov chain Monte Carlo tremendously expensive and unable to fully explore the multimodal posterior. On the other hand, variational inference benefits from improved computational complexity but lacks the asymptotical guarantees of sampling-based inference and tends to concentrate around a single mode. The performance of both approaches heavily depends on architectural choices; this paper aims to shed some light on this, by considering the computational costs, accuracy and uncertainty quantification in different scenarios including large width and out-of-sample data. To improve posterior exploration, different model averaging and ensembling techniques are studied, along with their benefits on predictive performance. In our experiments, variational inference overall provided better uncertainty quantification than Markov chain Monte Carlo; further, stacking and ensembles of variational approximations provided comparable to Markov chain Monte Carlo accuracy at a much-reduced cost.         ",
    "url": "https://arxiv.org/abs/2503.11808",
    "authors": [
      "Alisa Sheinkman",
      "Sara Wade"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.11838",
    "title": "A Transformer and Prototype-based Interpretable Model for Contextual Sarcasm Detection",
    "abstract": "           Sarcasm detection, with its figurative nature, poses unique challenges for affective systems designed to perform sentiment analysis. While these systems typically perform well at identifying direct expressions of emotion, they struggle with sarcasm's inherent contradiction between literal and intended sentiment. Since transformer-based language models (LMs) are known for their efficient ability to capture contextual meanings, we propose a method that leverages LMs and prototype-based networks, enhanced by sentiment embeddings to conduct interpretable sarcasm detection. Our approach is intrinsically interpretable without extra post-hoc interpretability techniques. We test our model on three public benchmark datasets and show that our model outperforms the current state-of-the-art. At the same time, the prototypical layer enhances the model's inherent interpretability by generating explanations through similar examples in the reference time. Furthermore, we demonstrate the effectiveness of incongruity loss in the ablation study, which we construct using sentiment prototypes.         ",
    "url": "https://arxiv.org/abs/2503.11838",
    "authors": [
      "Ximing Wen",
      "Rezvaneh Rezapour"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.11841",
    "title": "Trust Under Siege: Label Spoofing Attacks against Machine Learning for Android Malware Detection",
    "abstract": "           Machine learning (ML) malware detectors rely heavily on crowd-sourced AntiVirus (AV) labels, with platforms like VirusTotal serving as a trusted source of malware annotations. But what if attackers could manipulate these labels to classify benign software as malicious? We introduce label spoofing attacks, a new threat that contaminates crowd-sourced datasets by embedding minimal and undetectable malicious patterns into benign samples. These patterns coerce AV engines into misclassifying legitimate files as harmful, enabling poisoning attacks against ML-based malware classifiers trained on those data. We demonstrate this scenario by developing AndroVenom, a methodology for polluting realistic data sources, causing consequent poisoning attacks against ML malware detectors. Experiments show that not only state-of-the-art feature extractors are unable to filter such injection, but also various ML models experience Denial of Service already with 1% poisoned samples. Additionally, attackers can flip decisions of specific unaltered benign samples by modifying only 0.015% of the training data, threatening their reputation and market share and being unable to be stopped by anomaly detectors on training data. We conclude our manuscript by raising the alarm on the trustworthiness of the training process based on AV annotations, requiring further investigation on how to produce proper labels for ML malware detectors.         ",
    "url": "https://arxiv.org/abs/2503.11841",
    "authors": [
      "Tianwei Lan",
      "Luca Demetrio",
      "Farid Nait-Abdesselam",
      "Yufei Han",
      "Simone Aonzo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.11845",
    "title": "Systematic Classification of Studies Investigating Social Media Conversations about Long COVID Using a Novel Zero-Shot Transformer Framework",
    "abstract": "           Long COVID continues to challenge public health by affecting a considerable number of individuals who have recovered from acute SARS-CoV-2 infection yet endure prolonged and often debilitating symptoms. Social media has emerged as a vital resource for those seeking real-time information, peer support, and validating their health concerns related to Long COVID. This paper examines recent works focusing on mining, analyzing, and interpreting user-generated content on social media platforms to capture the broader discourse on persistent post-COVID conditions. A novel transformer-based zero-shot learning approach serves as the foundation for classifying research papers in this area into four primary categories: Clinical or Symptom Characterization, Advanced NLP or Computational Methods, Policy Advocacy or Public Health Communication, and Online Communities and Social Support. This methodology achieved an average confidence of 0.7788, with the minimum and maximum confidence being 0.1566 and 0.9928, respectively. This model showcases the ability of advanced language models to categorize research papers without any training data or predefined classification labels, thus enabling a more rapid and scalable assessment of existing literature. This paper also highlights the multifaceted nature of Long COVID research by demonstrating how advanced computational techniques applied to social media conversations can reveal deeper insights into the experiences, symptoms, and narratives of individuals affected by Long COVID.         ",
    "url": "https://arxiv.org/abs/2503.11845",
    "authors": [
      "Nirmalya Thakur",
      "Niven Francis Da Guia Fernandes",
      "Madje Tobi Marc'Avent Tchona"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.11857",
    "title": "Robust Model Predictive Control of Fast Lithium-ion Battery Pretreatment for Safe Recycling",
    "abstract": "           The proper disposal and repurposing of end-of-life electric vehicle batteries are critical for maximizing their environmental benefits. This study introduces a robust model predictive control (MPC) framework designed to optimize the battery discharging process during pre-treatment, ensuring both efficiency and safety. The proposed method explicitly incorporates temperature constraints to prevent overheating and potential hazards. By leveraging a control-oriented equivalent circuit model integrated with thermal dynamics, the MPC algorithm dynamically adjusts the discharging profile to maintain safe operating temperatures. Additionally, the robust controller is designed to account for model mismatches between the nonlinear battery dynamics and the linearized model, ensuring reliable performance under varying conditions. The effectiveness of this approach is demonstrated through simulations comparing the robust MPC method with conventional discharging strategies, including constant current-constant voltage (CC-CV) and constant current-constant temperature (CC-CT) methods. Results indicate that the robust MPC framework significantly reduces discharging time while adhering to safety constraints, offering a promising solution for the recycling and second-life applications of lithium-ion batteries.         ",
    "url": "https://arxiv.org/abs/2503.11857",
    "authors": [
      "Meng Yuan",
      "Adam Burman",
      "Changfu Zou"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.11881",
    "title": "GPT's Devastated and LLaMA's Content: Emotion Representation Alignment in LLMs for Keyword-based Generation",
    "abstract": "           In controlled text generation using large language models (LLMs), gaps arise between the language model's interpretation and human expectations. We look at the problem of controlling emotions in keyword-based sentence generation for both GPT-4 and LLaMA-3. We selected four emotion representations: Words, Valence-Arousal-Dominance (VAD) dimensions expressed in both Lexical and Numeric forms, and Emojis. Our human evaluation looked at the Human-LLM alignment for each representation, as well as the accuracy and realism of the generated sentences. While representations like VAD break emotions into easy-to-compute components, our findings show that people agree more with how LLMs generate when conditioned on English words (e.g., \"angry\") rather than VAD scales. This difference is especially visible when comparing Numeric VAD to words. However, we found that converting the originally-numeric VAD scales to Lexical scales (e.g., +4.0 becomes \"High\") dramatically improved agreement. Furthermore, the perception of how much a generated sentence conveys an emotion is highly dependent on the LLM, representation type, and which emotion it is.         ",
    "url": "https://arxiv.org/abs/2503.11881",
    "authors": [
      "Shadab Choudhury",
      "Asha Kumar",
      "Lara J. Martin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.11891",
    "title": "Training Diagonal Linear Networks with Stochastic Sharpness-Aware Minimization",
    "abstract": "           We analyze the landscape and training dynamics of diagonal linear networks in a linear regression task, with the network parameters being perturbed by small isotropic normal noise. The addition of such noise may be interpreted as a stochastic form of sharpness-aware minimization (SAM) and we prove several results that relate its action on the underlying landscape and training dynamics to the sharpness of the loss. In particular, the noise changes the expected gradient to force balancing of the weight matrices at a fast rate along the descent trajectory. In the diagonal linear model, we show that this equates to minimizing the average sharpness, as well as the trace of the Hessian matrix, among all possible factorizations of the same matrix. Further, the noise forces the gradient descent iterates towards a shrinkage-thresholding of the underlying true parameter, with the noise level explicitly regulating both the shrinkage factor and the threshold.         ",
    "url": "https://arxiv.org/abs/2503.11891",
    "authors": [
      "Gabriel Clara",
      "Sophie Langer",
      "Johannes Schmidt-Hieber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.11892",
    "title": "DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning",
    "abstract": "           Multimodal representation learning aims to capture both shared and complementary semantic information across multiple modalities. However, the intrinsic heterogeneity of diverse modalities presents substantial challenges to achieve effective cross-modal collaboration and integration. To address this, we introduce DecAlign, a novel hierarchical cross-modal alignment framework designed to decouple multimodal representations into modality-unique (heterogeneous) and modality-common (homogeneous) features. For handling heterogeneity, we employ a prototype-guided optimal transport alignment strategy leveraging gaussian mixture modeling and multi-marginal transport plans, thus mitigating distribution discrepancies while preserving modality-unique characteristics. To reinforce homogeneity, we ensure semantic consistency across modalities by aligning latent distribution matching with Maximum Mean Discrepancy regularization. Furthermore, we incorporate a multimodal transformer to enhance high-level semantic feature fusion, thereby further reducing cross-modal inconsistencies. Our extensive experiments on four widely used multimodal benchmarks demonstrate that DecAlign consistently outperforms existing state-of-the-art methods across five metrics. These results highlight the efficacy of DecAlign in enhancing superior cross-modal alignment and semantic consistency while preserving modality-unique features, marking a significant advancement in multimodal representation learning scenarios. Our project page is at this https URL and the code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.11892",
    "authors": [
      "Chengxuan Qian",
      "Shuo Xing",
      "Shawn Li",
      "Yue Zhao",
      "Zhengzhong Tu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.11899",
    "title": "Spatio-temporal Fourier Transformer (StFT) for Long-term Dynamics Prediction",
    "abstract": "           Simulating the long-term dynamics of multi-scale and multi-physics systems poses a significant challenge in understanding complex phenomena across science and engineering. The complexity arises from the intricate interactions between scales and the interplay of diverse physical processes. Neural operators have emerged as promising models for predicting such dynamics due to their flexibility and computational efficiency. However, they often fail to effectively capture multi-scale interactions or quantify the uncertainties inherent in the predictions. These limitations lead to rapid error accumulation, particularly in long-term forecasting of systems characterized by complex and coupled dynamics. To address these challenges, we propose a spatio-temporal Fourier transformer (StFT), in which each transformer block is designed to learn dynamics at a specific scale. By leveraging a structured hierarchy of StFT blocks, the model explicitly captures dynamics across both macro- and micro- spatial scales. Furthermore, a generative residual correction mechanism is integrated to estimate and mitigate predictive uncertainties, enhancing both the accuracy and reliability of long-term forecasts. Evaluations conducted on three benchmark datasets (plasma, fluid, and atmospheric dynamics) demonstrate the advantages of our approach over state-of-the-art ML methods.         ",
    "url": "https://arxiv.org/abs/2503.11899",
    "authors": [
      "Da Long",
      "Shandian Zhe",
      "Samuel Williams",
      "Leonid Oliker",
      "Zhe Bai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.11900",
    "title": "Heterogenous graph neural networks for species distribution modeling",
    "abstract": "           Species distribution models (SDMs) are necessary for measuring and predicting occurrences and habitat suitability of species and their relationship with environmental factors. We introduce a novel presence-only SDM with graph neural networks (GNN). In our model, species and locations are treated as two distinct node sets, and the learning task is predicting detection records as the edges that connect locations to species. Using GNN for SDM allows us to model fine-grained interactions between species and the environment. We evaluate the potential of this methodology on the six-region dataset compiled by National Center for Ecological Analysis and Synthesis (NCEAS) for benchmarking SDMs. For each of the regions, the heterogeneous GNN model is comparable to or outperforms previously-benchmarked single-species SDMs as well as a feed-forward neural network baseline model.         ",
    "url": "https://arxiv.org/abs/2503.11900",
    "authors": [
      "Lauren Harrell",
      "Christine Kaeser-Chen",
      "Burcu Karagol Ayan",
      "Keith Anderson",
      "Michelangelo Conserva",
      "Elise Kleeman",
      "Maxim Neumann",
      "Matt Overlan",
      "Melissa Chapman",
      "Drew Purves"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Populations and Evolution (q-bio.PE)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.11910",
    "title": "RTD-Lite: Scalable Topological Analysis for Comparing Weighted Graphs in Learning Tasks",
    "abstract": "           Topological methods for comparing weighted graphs are valuable in various learning tasks but often suffer from computational inefficiency on large datasets. We introduce RTD-Lite, a scalable algorithm that efficiently compares topological features, specifically connectivity or cluster structures at arbitrary scales, of two weighted graphs with one-to-one correspondence between vertices. Using minimal spanning trees in auxiliary graphs, RTD-Lite captures topological discrepancies with $O(n^2)$ time and memory complexity. This efficiency enables its application in tasks like dimensionality reduction and neural network training. Experiments on synthetic and real-world datasets demonstrate that RTD-Lite effectively identifies topological differences while significantly reducing computation time compared to existing methods. Moreover, integrating RTD-Lite into neural network training as a loss function component enhances the preservation of topological structures in learned representations. Our code is publicly available at this https URL ",
    "url": "https://arxiv.org/abs/2503.11910",
    "authors": [
      "Eduard Tulchinskii",
      "Daria Voronkova",
      "Ilya Trofimov",
      "Evgeny Burnaev",
      "Serguei Barannikov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Symplectic Geometry (math.SG)"
    ]
  },
  {
    "id": "arXiv:2503.11914",
    "title": "Curves Ahead: Enhancing the Steering Law for Complex Curved Trajectories",
    "abstract": "           The Steering Law has long been a fundamental model in predicting movement time for tasks involving navigating through constrained paths, such as in selecting sub-menu options, particularly for straight and circular arc trajectories. However, this does not reflect the complexities of real-world tasks where curvatures can vary arbitrarily, limiting its applications. This study aims to address this gap by introducing the total curvature parameter K into the equation to account for the overall curviness characteristic of a path. To validate this extension, we conducted a mouse-steering experiment on fixed-width paths with varying lengths and curviness levels. Our results demonstrate that the introduction of K significantly improves model fitness for movement time prediction over traditional models. These findings advance our understanding of movement in complex environments and support potential applications in fields like speech motor control and virtual navigation.         ",
    "url": "https://arxiv.org/abs/2503.11914",
    "authors": [
      "Jennie J.Y. Chen",
      "Sidney S. Fels"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.11916",
    "title": "Formally Proving Invariant Systemic Properties of Control Programs Using Ghost Code and Integral Quadratic Constraints",
    "abstract": "           This paper focuses on formally verifying invariant properties of control programs both at the model and code levels. The physical process is described by an uncertain discrete-time state-space system, where the dependence of the state-space matrix-valued functions defining the system on the uncertainties can be rational. The proposed approaches make use of pointwise integral quadratic constraints (IQCs) to characterize the uncertainties affecting the behavior of the system. Various uncertainties can be characterized by pointwise IQCs, including static linear time-varying perturbations and sector-bounded nonlinearities. Using the IQC framework, a sound overapproximation of the uncertain system, which is expressible at the code level, is constructed. Tools such as Frama-C, ACSL, WP, and an Alt-Ergo plugin are employed to ensure the validity of the state and output invariant properties across both real and float models. The first proposed approach can be used to formally verify (local) invariant properties of the control code. This capability is demonstrated in a couple of examples involving gain-scheduled path-following controllers designed for an uncrewed aircraft system and an autonomous underwater vehicle. The second approach enables the verification of closed-loop invariant properties, i.e., invariant properties of the controlled system as a whole, in both real and float models, while preserving the integrity of the executable controller code. This is achieved by using ghost code attached to the control code for all elements related to the plant model with uncertainties, as the ghost code does not interfere with the executable code. The effectiveness of this approach is demonstrated in two examples on the control of a four-thruster hovercraft and the control of a two-mass rotational system.         ",
    "url": "https://arxiv.org/abs/2503.11916",
    "authors": [
      "Elias Khalife",
      "Pierre-Loic Garoche",
      "Mazen Farhood"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2503.11920",
    "title": "Practical Implications of Implementing Local Differential Privacy for Smart grids",
    "abstract": "           Recent smart grid advancements enable near-realtime reporting of electricity consumption, raising concerns about consumer privacy. Differential privacy (DP) has emerged as a viable privacy solution, where a calculated amount of noise is added to the data by a trusted third party, or individual users perturb their information locally, and only send the randomized data to an aggregator for analysis safeguarding users and aggregators privacy. However, the practical implementation of a Local DP-based (LDP) privacy model for smart grids has its own challenges. In this paper, we discuss the challenges of implementing an LDP-based model for smart grids. We compare existing LDP mechanisms in smart grids for privacy preservation of numerical data and discuss different methods for selecting privacy parameters in the existing literature, their limitations and the non-existence of an optimal method for selecting the privacy parameters. We also discuss the challenges of translating theoretical models of LDP into a practical setting for smart grids for different utility functions, the impact of the size of data set on privacy and accuracy, and vulnerability of LDP-based smart grids to manipulation attacks. Finally, we discuss future directions in research for better practical applications in LDP based models for smart grids.         ",
    "url": "https://arxiv.org/abs/2503.11920",
    "authors": [
      "Khadija Hafeez",
      "Mubashir Husain Rehmani",
      "Sumita Mishra",
      "Donna OShea"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.11937",
    "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder",
    "abstract": "           Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the Attribute (Att) Adapter, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning. We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world. Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model.         ",
    "url": "https://arxiv.org/abs/2503.11937",
    "authors": [
      "Wonwoong Cho",
      "Yan-Ying Chen",
      "Matthew Klenk",
      "David I. Inouye",
      "Yanxia Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.11946",
    "title": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge Computing Networks",
    "abstract": "           In satellite computing applications, such as remote sensing, tasks often involve similar or identical input data, leading to the same processing results. Computation reuse is an emerging paradigm that leverages the execution results of previous tasks to enhance the utilization of computational resources. While this paradigm has been extensively studied in terrestrial networks with abundant computing and caching resources, such as named data networking (NDN), it is essential to develop a framework appropriate for resource-constrained satellite networks, which are expected to have longer task completion times. In this paper, we propose CCRSat, a collaborative computation reuse framework for satellite edge computing networks. CCRSat initially implements local computation reuse on an independent satellite, utilizing a satellite reuse state (SRS) to assess the efficiency of computation reuse. Additionally, an inter-satellite computation reuse algorithm is introduced, which utilizes the collaborative sharing of similarity in previously processed data among multiple satellites. The evaluation results tested on real-world datasets demonstrate that, compared to comparative scenarios, our proposed CCRSat can significantly reduce task completion time by up to 62.1% and computational resource consumption by up to 28.8%.         ",
    "url": "https://arxiv.org/abs/2503.11946",
    "authors": [
      "Ye Zhang",
      "Zhishu Shen",
      "Dawen Jiang",
      "Xiangrui Liu",
      "Qiushi Zheng",
      "Jiong Jin"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2503.11947",
    "title": "Ethical AI for Young Digital Citizens: A Call to Action on Privacy Governance",
    "abstract": "           The rapid expansion of Artificial Intelligence (AI) in digital platforms used by youth has created significant challenges related to privacy, autonomy, and data protection. While AI-driven personalization offers enhanced user experiences, it often operates without clear ethical boundaries, leaving young users vulnerable to data exploitation and algorithmic biases. This paper presents a call to action for ethical AI governance, advocating for a structured framework that ensures youth-centred privacy protections, transparent data practices, and regulatory oversight. We outline key areas requiring urgent intervention, including algorithmic transparency, privacy education, parental data-sharing ethics, and accountability measures. Through this approach, we seek to empower youth with greater control over their digital identities and propose actionable strategies for policymakers, AI developers, and educators to build a fairer and more accountable AI ecosystem.         ",
    "url": "https://arxiv.org/abs/2503.11947",
    "authors": [
      "Austin Shouli",
      "Ankur Barthwal",
      "Molly Campbell",
      "Ajay Kumar Shrestha"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.11950",
    "title": "Privacy Ethics Alignment in AI (PEA-AI): A Stakeholder-Centric Based Framework for Ethcial AI",
    "abstract": "           The increasing integration of Artificial Intelligence (AI) in digital ecosystems has reshaped privacy dynamics, particularly for young digital citizens navigating data-driven environments. This study explores evolving privacy concerns across three key stakeholder groups, digital citizens (ages 16-19), parents, educators, and AI professionals, and assesses differences in data ownership, trust, transparency, parental mediation, education, and risk-benefit perceptions. Employing a grounded theory methodology, this research synthesizes insights from 482 participants through structured surveys, qualitative interviews, and focus groups. The findings reveal distinct privacy expectations- Young users emphasize autonomy and digital freedom, while parents and educators advocate for regulatory oversight and AI literacy programs. AI professionals, in contrast, prioritize the balance between ethical system design and technological efficiency. The data further highlights gaps in AI literacy and transparency, emphasizing the need for comprehensive, stakeholder-driven privacy frameworks that accommodate diverse user needs. Using comparative thematic analysis, this study identifies key tensions in privacy governance and develops the novel Privacy-Ethics Alignment in AI (PEA-AI) model, which structures privacy decision-making as a dynamic negotiation between stakeholders. By systematically analyzing themes such as transparency, user control, risk perception, and parental mediation, this research provides a scalable, adaptive foundation for AI governance, ensuring that privacy protections evolve alongside emerging AI technologies and youth-centric digital interactions.         ",
    "url": "https://arxiv.org/abs/2503.11950",
    "authors": [
      "Ankur Barthwal",
      "Molly Campbell",
      "Ajay Kumar Shrestha"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.11973",
    "title": "Machine Learning-Based Model for Postoperative Stroke Prediction in Coronary Artery Disease",
    "abstract": "           Coronary artery disease remains one of the leading causes of mortality globally. Despite advances in revascularization treatments like PCI and CABG, postoperative stroke is inevitable. This study aims to develop and evaluate a sophisticated machine learning prediction model to assess postoperative stroke risk in coronary revascularization this http URL research employed data from the MIMIC-IV database, consisting of a cohort of 7023 individuals. Study data included clinical, laboratory, and comorbidity variables. To reduce multicollinearity, variables with over 30% missing values and features with a correlation coefficient larger than 0.9 were deleted. The dataset has 70% training and 30% test. The Random Forest technique interpolated residual dataset missing values. Numerical values were normalized, whereas categorical variables were one-hot encoded. LASSO regularization selected features, and grid search found model hyperparameters. Finally, Logistic Regression, XGBoost, SVM, and CatBoost were employed for predictive modeling, and SHAP analysis assessed stroke risk for each variable. AUC of 0.855 (0.829-0.878) showed that SVM model outperformed logistic regression and CatBoost models in prior research. SHAP research showed that the Charlson Comorbidity Index (CCI), diabetes, chronic kidney disease, and heart failure are significant prognostic factors for postoperative stroke. This study shows that improved machine learning reduces overfitting and improves model predictive accuracy. Models using the CCI alone cannot predict postoperative stroke risk as accurately as those using independent comorbidity variables. The suggested technique provides a more thorough and individualized risk assessment by encompassing a wider range of clinically relevant characteristics, making it a better reference for preoperative risk assessments and targeted intervention.         ",
    "url": "https://arxiv.org/abs/2503.11973",
    "authors": [
      "Haonan Pan",
      "Shuheng Chen",
      "Elham Pishgar",
      "Kamiar Alaei",
      "Greg Placencia",
      "Maryam Pishgar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.11974",
    "title": "Weighted cycle-based identification of influential node groups in complex networks",
    "abstract": "           Identifying influential node groups in complex networks is crucial for optimizing information dissemination, epidemic control, and viral marketing. However, traditional centrality-based methods often focus on individual nodes, resulting in overlapping influence zones and diminished collective effectiveness. To overcome these limitations, we propose Weighted Cycle (WCycle), a novel indicator that incorporates basic cycle structures and node behavior traits (edge weights) to comprehensively assess node importance. WCycle effectively identifies spatially dispersed and structurally diverse key node group, thereby reducing influence redundancy and enhancing network-wide propagation. Extensive experiments on six real-world networks demonstrate WCycle's superior performance compared to five benchmark methods across multiple evaluation dimensions, including influence propagation efficiency, structural differentiation, and cost-effectiveness. The findings highlight WCycle's robustness and scalability, establishing it as a promising tool for complex network analysis and practical applications requiring effective influence maximization.         ",
    "url": "https://arxiv.org/abs/2503.11974",
    "authors": [
      "Wenxin Zheng",
      "Wenfeng Shi",
      "Tianlong Fan",
      "Linyuan L\u00fc"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2503.12008",
    "title": "Winning the MIDST Challenge: New Membership Inference Attacks on Diffusion Models for Tabular Data Synthesis",
    "abstract": "           Tabular data synthesis using diffusion models has gained significant attention for its potential to balance data utility and privacy. However, existing privacy evaluations often rely on heuristic metrics or weak membership inference attacks (MIA), leaving privacy risks inadequately assessed. In this work, we conduct a rigorous MIA study on diffusion-based tabular synthesis, revealing that state-of-the-art attacks designed for image models fail in this setting. We identify noise initialization as a key factor influencing attack efficacy and propose a machine-learning-driven approach that leverages loss features across different noises and time steps. Our method, implemented with a lightweight MLP, effectively learns membership signals, eliminating the need for manual optimization. Experimental results from the MIDST Challenge @ SaTML 2025 demonstrate the effectiveness of our approach, securing first place across all tracks. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12008",
    "authors": [
      "Xiaoyu Wu",
      "Yifei Pang",
      "Terrance Liu",
      "Steven Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.12009",
    "title": "UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection",
    "abstract": "           Recent advances in LiDAR 3D detection have demonstrated the effectiveness of Transformer-based frameworks in capturing the global dependencies from point cloud spaces, which serialize the 3D voxels into the flattened 1D sequence for iterative self-attention. However, the spatial structure of 3D voxels will be inevitably destroyed during the serialization process. Besides, due to the considerable number of 3D voxels and quadratic complexity of Transformers, multiple sequences are grouped before feeding to Transformers, leading to a limited receptive field. Inspired by the impressive performance of State Space Models (SSM) achieved in the field of 2D vision tasks, in this paper, we propose a novel Unified Mamba (UniMamba), which seamlessly integrates the merits of 3D convolution and SSM in a concise multi-head manner, aiming to perform \"local and global\" spatial context aggregation efficiently and simultaneously. Specifically, a UniMamba block is designed which mainly consists of spatial locality modeling, complementary Z-order serialization and local-global sequential aggregator. The spatial locality modeling module integrates 3D submanifold convolution to capture the dynamic spatial position embedding before serialization. Then the efficient Z-order curve is adopted for serialization both horizontally and vertically. Furthermore, the local-global sequential aggregator adopts the channel grouping strategy to efficiently encode both \"local and global\" spatial inter-dependencies using multi-head SSM. Additionally, an encoder-decoder architecture with stacked UniMamba blocks is formed to facilitate multi-scale spatial learning hierarchically. Extensive experiments are conducted on three popular datasets: nuScenes, Waymo and Argoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes dataset.         ",
    "url": "https://arxiv.org/abs/2503.12009",
    "authors": [
      "Xin Jin",
      "Haisheng Su",
      "Kai Liu",
      "Cong Ma",
      "Wei Wu",
      "Fei Hui",
      "Junchi Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12012",
    "title": "Mixed-feature Logistic Regression Robust to Distribution Shifts",
    "abstract": "           Logistic regression models are widely used in the social and behavioral sciences and in high-stakes domains, due to their simplicity and interpretability properties. At the same time, such domains are permeated by distribution shifts, where the distribution generating the data changes between training and deployment. In this paper, we study a distributionally robust logistic regression problem that seeks the model that will perform best against adversarial realizations of the data distribution drawn from a suitably constructed Wasserstein ambiguity set. Our model and solution approach differ from prior work in that we can capture settings where the likelihood of distribution shifts can vary across features, significantly broadening the applicability of our model relative to the state-of-the-art. We propose a graph-based solution approach that can be integrated into off-the-shelf optimization solvers. We evaluate the performance of our model and algorithms on numerous publicly available datasets. Our solution achieves a 408x speed-up relative to the state-of-the-art. Additionally, compared to the state-of-the-art, our model reduces average calibration error by up to 36.19% and worst-case calibration error by up to 41.70%, while increasing the average area under the ROC curve (AUC) by up to 18.02% and worst-case AUC by up to 48.37%.         ",
    "url": "https://arxiv.org/abs/2503.12012",
    "authors": [
      "Qingshi Sun",
      "Nathan Justin",
      "Andres Gomez",
      "Phebe Vayanos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.12026",
    "title": "Leveraging Motion Information for Better Self-Supervised Video Correspondence Learning",
    "abstract": "           Self-supervised video correspondence learning depends on the ability to accurately associate pixels between video frames that correspond to the same visual object. However, achieving reliable pixel matching without supervision remains a major challenge. To address this issue, recent research has focused on feature learning techniques that aim to encode unique pixel representations for matching. Despite these advances, existing methods still struggle to achieve exact pixel correspondences and often suffer from false matches, limiting their effectiveness in self-supervised settings. To this end, we explore an efficient self-supervised Video Correspondence Learning framework (MER) that aims to accurately extract object details from unlabeled videos. First, we design a dedicated Motion Enhancement Engine that emphasizes capturing the dynamic motion of objects in videos. In addition, we introduce a flexible sampling strategy for inter-pixel correspondence information (Multi-Cluster Sampler) that enables the model to pay more attention to the pixel changes of important objects in motion. Through experiments, our algorithm outperforms the state-of-the-art competitors on video correspondence learning tasks such as video object segmentation and video object keypoint tracking.         ",
    "url": "https://arxiv.org/abs/2503.12026",
    "authors": [
      "Zihan Zhoua",
      "Changrui Daia",
      "Aibo Songa",
      "Xiaolin Fang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12029",
    "title": "Is Multi-Agent Debate (MAD) the Silver Bullet? An Empirical Analysis of MAD in Code Summarization and Translation",
    "abstract": "           Large Language Models (LLMs) have advanced autonomous agents' planning and decision-making, yet they struggle with complex tasks requiring diverse expertise and multi-step reasoning. Multi-Agent Debate (MAD) systems, introduced in NLP research, address this gap by enabling structured debates among LLM-based agents to refine solutions iteratively. MAD promotes divergent thinking through role-specific agents, dynamic interactions, and structured decision-making. Recognizing parallels between Software Engineering (SE) and collaborative human problem-solving, this study investigates MAD's effectiveness on two SE tasks. We adapt MAD systems from NLP, analyze agent interactions to assess consensus-building and iterative refinement, and propose two enhancements targeting observed weaknesses. Our findings show that structured debate and collaboration improve problem-solving and yield strong performance in some cases, highlighting MAD's potential for SE automation while identifying areas for exploration.         ",
    "url": "https://arxiv.org/abs/2503.12029",
    "authors": [
      "Jina Chun",
      "Qihong Chen",
      "Jiawei Li",
      "Iftekhar Ahmed"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.12030",
    "title": "Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training",
    "abstract": "           End-to-end autonomous driving research currently faces a critical challenge in bridging the gap between open-loop training and closed-loop deployment. Current approaches are trained to predict trajectories in an open-loop environment, which struggle with quick reactions to other agents in closed-loop environments and risk generating kinematically infeasible plans due to the gap between open-loop training and closed-loop driving. In this paper, we introduce Hydra-NeXt, a novel multi-branch planning framework that unifies trajectory prediction, control prediction, and a trajectory refinement network in one model. Unlike current open-loop trajectory prediction models that only handle general-case planning, Hydra-NeXt further utilizes a control decoder to focus on short-term actions, which enables faster responses to dynamic situations and reactive agents. Moreover, we propose the Trajectory Refinement module to augment and refine the planning decisions by effectively adhering to kinematic constraints in closed-loop environments. This unified approach bridges the gap between open-loop training and closed-loop driving, demonstrating superior performance of 65.89 Driving Score (DS) and 48.20% Success Rate (SR) on the Bench2Drive dataset without relying on external experts for data collection. Hydra-NeXt surpasses the previous state-of-the-art by 22.98 DS and 17.49 SR, marking a significant advancement in autonomous driving. Code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12030",
    "authors": [
      "Zhenxin Li",
      "Shihao Wang",
      "Shiyi Lan",
      "Zhiding Yu",
      "Zuxuan Wu",
      "Jose M. Alvarez"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12034",
    "title": "Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder",
    "abstract": "           Recognition of human manipulation actions in real-time is essential for safe and effective human-robot interaction and collaboration. The challenge lies in developing a model that is both lightweight enough for real-time execution and capable of generalization. While some existing methods in the literature can run in real-time, they struggle with temporal scalability, i.e., they fail to adapt to long-duration manipulations effectively. To address this, leveraging the generalizable scene graph representations, we propose a new Factorized Graph Sequence Encoder network that not only runs in real-time but also scales effectively in the temporal dimension, thanks to its factorized encoder architecture. Additionally, we introduce Hand Pooling operation, a simple pooling operation for more focused extraction of the graph-level embeddings. Our model outperforms the previous state-of-the-art real-time approach, achieving a 14.3\\% and 5.6\\% improvement in F1-macro score on the KIT Bimanual Action (Bimacs) Dataset and Collaborative Action (CoAx) Dataset, respectively. Moreover, we conduct an extensive ablation study to validate our network design choices. Finally, we compare our model with its architecturally similar RGB-based model on the Bimacs dataset and show the limitations of this model in contrast to ours on such an object-centric manipulation dataset.         ",
    "url": "https://arxiv.org/abs/2503.12034",
    "authors": [
      "Enes Erdogan",
      "Eren Erdal Aksoy",
      "Sanem Sariel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12037",
    "title": "Unsupervised Graph Anomaly Detection via Multi-Hypersphere Heterophilic Graph Learning",
    "abstract": "           Graph Anomaly Detection (GAD) plays a vital role in various data mining applications such as e-commerce fraud prevention and malicious user detection. Recently, Graph Neural Network (GNN) based approach has demonstrated great effectiveness in GAD by first encoding graph data into low-dimensional representations and then identifying anomalies under the guidance of supervised or unsupervised signals. However, existing GNN-based approaches implicitly follow the homophily principle (i.e., the \"like attracts like\" phenomenon) and fail to learn discriminative embedding for anomalies that connect vast normal nodes. Moreover, such approaches identify anomalies in a unified global perspective but overlook diversified abnormal patterns conditioned on local graph context, leading to suboptimal performance. To overcome the aforementioned limitations, in this paper, we propose a Multi-hypersphere Heterophilic Graph Learning (MHetGL) framework for unsupervised GAD. Specifically, we first devise a Heterophilic Graph Encoding (HGE) module to learn distinguishable representations for potential anomalies by purifying and augmenting their neighborhood in a fully unsupervised manner. Then, we propose a Multi-Hypersphere Learning (MHL) module to enhance the detection capability for context-dependent anomalies by jointly incorporating critical patterns from both global and local perspectives. Extensive experiments on ten real-world datasets show that MHetGL outperforms 14 baselines. Our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12037",
    "authors": [
      "Hang Ni",
      "Jindong Han",
      "Nengjun Zhu",
      "Hao Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12050",
    "title": "Hierarchical Evolutionary Optimization with Predictive Modeling for Stable Delay-Constrained Routing in Vehicular Networks",
    "abstract": "           Vehicular Ad Hoc Networks (VANETs) are a cornerstone of intelligent transportation systems, facilitating real-time communication between vehicles and infrastructure. However, the dynamic nature of VANETs introduces significant challenges in routing, especially in minimizing communication delay while ensuring route stability. This paper proposes a hierarchical evolutionary optimization framework for delay-constrained routing in vehicular networks. Leveraging multi-objective optimization, the framework balances delay and stability objectives and incorporates adaptive mechanisms like incremental route adjustments and LSTM-based predictive modeling. Simulation results confirm that the proposed framework maintains low delay and high stability, adapting effectively to frequent topology changes in dynamic vehicular environments.         ",
    "url": "https://arxiv.org/abs/2503.12050",
    "authors": [
      "Zhang Zhiou",
      "Guo Weian",
      "Zhang Qin",
      "Lin Haibin",
      "Li Dongyang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.12055",
    "title": "Generative Modeling of Adversarial Lane-Change Scenario",
    "abstract": "           Decision-making in long-tail scenarios is crucial to autonomous driving development, with realistic and challenging simulations playing a pivotal role in testing safety-critical situations. However, the current open-source datasets do not systematically include long-tail distributed scenario data, making acquiring such scenarios a formidable task. To address this problem, a data mining framework is proposed, which performs in-depth analysis on two widely-used datasets, NGSIM and INTERACTION, to pinpoint data with hazardous behavioral traits, aiming to bridge the gap in these overlooked scenarios. The approach utilizes Generative Adversarial Imitation Learning (GAIL) based on an enhanced Proximal Policy Optimization (PPO) model, integrated with the vehicle's environmental analysis, to iteratively refine and represent the newly generated vehicle trajectory. Innovatively, the solution optimizes the generation of adversarial scenario data from the perspectives of sensitivity and reasonable adversarial. It is demonstrated through experiments that, compared to the unfiltered data and baseline models, the approach exhibits more adversarial yet natural behavior regarding collision rate, acceleration, and lane changes, thereby validating its suitability for generating scenario data and providing constructive insights for the development of future scenarios and subsequent decision training.         ",
    "url": "https://arxiv.org/abs/2503.12055",
    "authors": [
      "Chuancheng Zhang",
      "Zhenhao Wang",
      "Jiangcheng Wang",
      "Kun Su",
      "Qiang Lv",
      "Bin Jiang",
      "Kunkun Hao",
      "Wenyu Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.12058",
    "title": "Revisiting Training-Inference Trigger Intensity in Backdoor Attacks",
    "abstract": "           Backdoor attacks typically place a specific trigger on certain training data, such that the model makes prediction errors on inputs with that trigger during inference. Despite the core role of the trigger, existing studies have commonly believed a perfect match between training-inference triggers is optimal. In this paper, for the first time, we systematically explore the training-inference trigger relation, particularly focusing on their mismatch, based on a Training-Inference Trigger Intensity Manipulation (TITIM) workflow. TITIM specifically investigates the training-inference trigger intensity, such as the size or the opacity of a trigger, and reveals new insights into trigger generalization and overfitting. These new insights challenge the above common belief by demonstrating that the training-inference trigger mismatch can facilitate attacks in two practical scenarios, posing more significant security threats than previously thought. First, when the inference trigger is fixed, using training triggers with mixed intensities leads to stronger attacks than using any single intensity. For example, on CIFAR-10 with ResNet-18, mixing training triggers with 1.0 and 0.1 opacities improves the worst-case attack success rate (ASR) (over different testing opacities) of the best single-opacity attack from 10.61\\% to 92.77\\%. Second, intentionally using certain mismatched training-inference triggers can improve the attack stealthiness, i.e., better bypassing defenses. For example, compared to the training/inference intensity of 1.0/1.0, using 1.0/0.7 decreases the area under the curve (AUC) of the Scale-Up defense from 0.96 to 0.62, while maintaining a high attack ASR (99.65\\% vs. 91.62\\%). The above new insights are validated to be generalizable across different backdoor attacks, models, datasets, tasks, and (digital/physical) domains.         ",
    "url": "https://arxiv.org/abs/2503.12058",
    "authors": [
      "Chenhao Lin",
      "Chenyang Zhao",
      "Shiwei Wang",
      "Longtian Wang",
      "Chao Shen",
      "Zhengyu Zhao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12061",
    "title": "EHNet: An Efficient Hybrid Network for Crowd Counting and Localization",
    "abstract": "           In recent years, crowd counting and localization have become crucial techniques in computer vision, with applications spanning various domains. The presence of multi-scale crowd distributions within a single image remains a fundamental challenge in crowd counting tasks. To address these challenges, we introduce the Efficient Hybrid Network (EHNet), a novel framework for efficient crowd counting and localization. By reformulating crowd counting into a point regression framework, EHNet leverages the Spatial-Position Attention Module (SPAM) to capture comprehensive spatial contexts and long-range dependencies. Additionally, we develop an Adaptive Feature Aggregation Module (AFAM) to effectively fuse and harmonize multi-scale feature representations. Building upon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental results on four benchmark datasets demonstrate that EHNet achieves competitive performance with reduced computational overhead, outperforming existing methods on ShanghaiTech Part \\_A, ShanghaiTech Part \\_B, UCF-CC-50, and UCF-QNRF. Our code is in this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12061",
    "authors": [
      "Yuqing Yan",
      "Yirui Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12063",
    "title": "DLA-Count: Dynamic Label Assignment Network for Dense Cell Distribution Counting",
    "abstract": "           Cell counting remains a fundamental yet challenging task in medical and biological research due to the diverse morphology of cells, their dense distribution, and variations in image quality. We present DLA-Count, a breakthrough approach to cell counting that introduces three key innovations: (1) K-adjacent Hungarian Matching (KHM), which dramatically improves cell matching in dense regions, (2) Multi-scale Deformable Gaussian Convolution (MDGC), which adapts to varying cell morphologies, and (3) Gaussian-enhanced Feature Decoder (GFD) for efficient multi-scale feature fusion. Our extensive experiments on four challenging cell counting datasets (ADI, MBM, VGG, and DCC) demonstrate that our method outperforms previous methods across diverse datasets, with improvements in Mean Absolute Error of up to 46.7\\% on ADI and 42.5\\% on MBM datasets. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12063",
    "authors": [
      "Yuqing Yan",
      "Yirui Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12069",
    "title": "Robust Dataset Distillation by Matching Adversarial Trajectories",
    "abstract": "           Dataset distillation synthesizes compact datasets that enable models to achieve performance comparable to training on the original large-scale datasets. However, existing distillation methods overlook the robustness of the model, resulting in models that are vulnerable to adversarial attacks when trained on distilled data. To address this limitation, we introduce the task of ``robust dataset distillation\", a novel paradigm that embeds adversarial robustness into the synthetic datasets during the distillation process. We propose Matching Adversarial Trajectories (MAT), a method that integrates adversarial training into trajectory-based dataset distillation. MAT incorporates adversarial samples during trajectory generation to obtain robust training trajectories, which are then used to guide the distillation process. As experimentally demonstrated, even through natural training on our distilled dataset, models can achieve enhanced adversarial robustness while maintaining competitive accuracy compared to existing distillation methods. Our work highlights robust dataset distillation as a new and important research direction and provides a strong baseline for future research to bridge the gap between efficient training and adversarial robustness.         ",
    "url": "https://arxiv.org/abs/2503.12069",
    "authors": [
      "Wei Lai",
      "Tianyu Ding",
      "ren dongdong",
      "Lei Wang",
      "Jing Huo",
      "Yang Gao",
      "Wenbin Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12083",
    "title": "Proof-Driven Clause Learning in Neural Network Verification",
    "abstract": "           The widespread adoption of deep neural networks (DNNs) requires efficient techniques for safety verification. Existing methods struggle to scale to real-world DNNs, and tremendous efforts are being put into improving their scalability. In this work, we propose an approach for improving the scalability of DNN verifiers using Conflict-Driven Clause Learning (CDCL) -- an approach that has proven highly successful in SAT and SMT solving. We present a novel algorithm for deriving conflict clauses using UNSAT proofs, and propose several optimizations for expediting it. Our approach allows a modular integration of SAT solvers and DNN verifiers, and we implement it on top of an interface designed for this purpose. The evaluation of our implementation over several benchmarks suggests a 2X--3X improvement over a similar approach, with specific cases outperforming the state of the art.         ",
    "url": "https://arxiv.org/abs/2503.12083",
    "authors": [
      "Omri Isac",
      "Idan Refaeli",
      "Haoze Wu",
      "Clark Barrett",
      "Guy Katz"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12086",
    "title": "FA-BARF: Frequency Adapted Bundle-Adjusting Neural Radiance Fields",
    "abstract": "           Neural Radiance Fields (NeRF) have exhibited highly effective performance for photorealistic novel view synthesis recently. However, the key limitation it meets is the reliance on a hand-crafted frequency annealing strategy to recover 3D scenes with imperfect camera poses. The strategy exploits a temporal low-pass filter to guarantee convergence while decelerating the joint optimization of implicit scene reconstruction and camera registration. In this work, we introduce the Frequency Adapted Bundle Adjusting Radiance Field (FA-BARF), substituting the temporal low-pass filter for a frequency-adapted spatial low-pass filter to address the decelerating problem. We establish a theoretical framework to interpret the relationship between position encoding of NeRF and camera registration and show that our frequency-adapted filter can mitigate frequency fluctuation caused by the temporal filter. Furthermore, we show that applying a spatial low-pass filter in NeRF can optimize camera poses productively through radial uncertainty overlaps among various views. Extensive experiments show that FA-BARF can accelerate the joint optimization process under little perturbations in object-centric scenes and recover real-world scenes with unknown camera poses. This implies wider possibilities for NeRF applied in dense 3D mapping and reconstruction under real-time requirements. The code will be released upon paper acceptance.         ",
    "url": "https://arxiv.org/abs/2503.12086",
    "authors": [
      "Rui Qian",
      "Chenyangguang Zhang",
      "Yan Di",
      "Guangyao Zhai",
      "Ruida Zhang",
      "Jiayu Guo",
      "Benjamin Busam",
      "Jian Pu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12093",
    "title": "SFMNet: Sparse Focal Modulation for 3D Object Detection",
    "abstract": "           We propose SFMNet, a novel 3D sparse detector that combines the efficiency of sparse convolutions with the ability to model long-range dependencies. While traditional sparse convolution techniques efficiently capture local structures, they struggle with modeling long-range relationships. However, capturing long-range dependencies is fundamental for 3D object detection. In contrast, transformers are designed to capture these long-range dependencies through attention mechanisms. But, they come with high computational costs, due to their quadratic query-key-value interactions. Furthermore, directly applying attention to non-empty voxels is inefficient due to the sparse nature of 3D scenes. Our SFMNet is built on a novel Sparse Focal Modulation (SFM) module, which integrates short- and long-range contexts with linear complexity by leveraging a new hierarchical sparse convolution design. This approach enables SFMNet to achieve high detection performance with improved efficiency, making it well-suited for large-scale LiDAR scenes. We show that our detector achieves state-of-the-art performance on autonomous driving datasets.         ",
    "url": "https://arxiv.org/abs/2503.12093",
    "authors": [
      "Oren Shrout",
      "Ayellet Tal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12115",
    "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
    "abstract": "           Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.         ",
    "url": "https://arxiv.org/abs/2503.12115",
    "authors": [
      "Xue Jiang",
      "Xiulian Peng",
      "Yuan Zhang",
      "Yan Lu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2503.12125",
    "title": "Robust Isolation Forest using Soft Sparse Random Projection and Valley Emphasis Method",
    "abstract": "           Isolation Forest (iForest) is an unsupervised anomaly detection algorithm designed to effectively detect anomalies under the assumption that anomalies are ``few and different.\" Various studies have aimed to enhance iForest, but the resulting algorithms often exhibited significant performance disparities across datasets. Additionally, the challenge of isolating rare and widely distributed anomalies persisted in research focused on improving splits. To address these challenges, we introduce Robust iForest (RiForest). RiForest leverages both existing features and random hyperplanes obtained through soft sparse random projection to identify superior split features for anomaly detection, independent of datasets. It utilizes the underutilized valley emphasis method for optimal split point determination and incorporates sparsity randomization in soft sparse random projection for enhanced anomaly detection robustness. Across 24 benchmark datasets, experiments demonstrate RiForest's consistent outperformance of existing algorithms in anomaly detection, emphasizing stability and robustness to noise variables.         ",
    "url": "https://arxiv.org/abs/2503.12125",
    "authors": [
      "Hun Kang",
      "Kyoungok Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12139",
    "title": "A Mechanistic Study on the Impact of Entity Degree Distribution in Open-World Link Prediction",
    "abstract": "           Open-world link prediction supports the knowledge representation and link prediction of new entities, enhancing the practical value of knowledge graphs in real-world applications. However, as research deepens, the performance improvements in open-world link prediction have gradually reached a bottleneck. Understanding its intrinsic impact mechanisms is crucial for identifying the key factors that limit performance, offering new theoretical insights and optimization strategies to overcome these bottlenecks. This study focuses on entity degree distribution, a core structural feature of knowledge graphs, and investigates its impact on the performance of open-world link prediction tasks. First, through experimental analysis, we confirm that entity degree distribution significantly affects link prediction model performance. Second, we reveal a strong positive correlation between entity degree and link prediction accuracy. Moreover, this study explores how entity degree influences embedding space distribution and weight updates during neural network training, uncovering the deeper mechanisms affecting open-world link prediction performance. The findings show that entity degree distribution has a significant impact on model training. By influencing the quality of the embedding space and weight updates, it indirectly affects the overall prediction performance of the model. In summary, this study not only highlights the critical role of entity degree distribution in open-world link prediction but also uncovers the intrinsic mechanisms through which it impacts model performance, providing valuable insights and directions for future research in this field.         ",
    "url": "https://arxiv.org/abs/2503.12139",
    "authors": [
      "Jiang Xiaobo",
      "Yongru Chen"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.12150",
    "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis",
    "abstract": "           This paper proposes a general solution to enable point cloud recognition models to handle distribution shifts at test time. Unlike prior methods, which rely heavily on training data-often inaccessible during online inference-and are limited to recognizing a fixed set of point cloud classes predefined during training, we explore a more practical and challenging scenario: adapting the model solely based on online test data to recognize both previously seen classes and novel, unseen classes at test time. To this end, we develop Point-Cache, a hierarchical cache model that captures essential clues of online test samples, particularly focusing on the global structure of point clouds and their local-part details. Point-Cache, which serves as a rich 3D knowledge base, is dynamically managed to prioritize the inclusion of high-quality samples. Designed as a plug-and-play module, our method can be flexibly integrated into large multimodal 3D models to support open-vocabulary point cloud recognition. Notably, our solution operates with efficiency comparable to zero-shot inference, as it is entirely training-free. Point-Cache demonstrates substantial gains across 8 challenging benchmarks and 4 representative large 3D models, highlighting its effectiveness. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12150",
    "authors": [
      "Hongyu Sun",
      "Qiuhong Ke",
      "Ming Cheng",
      "Yongcai Wang",
      "Deying Li",
      "Chenhui Gou",
      "Jianfei Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12156",
    "title": "Efficient and Privacy-Preserved Link Prediction via Condensed Graphs",
    "abstract": "           Link prediction is crucial for uncovering hidden connections within complex networks, enabling applications such as identifying potential customers and products. However, this research faces significant challenges, including concerns about data privacy, as well as high computational and storage costs, especially when dealing with large-scale networks. Condensed graphs, which are much smaller than the original graphs while retaining essential information, has become an effective solution to both maintain data utility and preserve privacy. Existing methods, however, initialize synthetic graphs through random node selection without considering node connectivity, and are mainly designed for node classification tasks. As a result, their potential for privacy-preserving link prediction remains largely unexplored. We introduce HyDRO\\textsuperscript{+}, a graph condensation method guided by algebraic Jaccard similarity, which leverages local connectivity information to optimize condensed graph structures. Extensive experiments on four real-world networks show that our method outperforms state-of-the-art methods and even the original networks in balancing link prediction accuracy and privacy preservation. Moreover, our method achieves nearly 20* faster training and reduces storage requirements by 452*, as demonstrated on the Computers dataset, compared to link prediction on the original networks. This work represents the first attempt to leverage condensed graphs for privacy-preserving link prediction information sharing in real-world complex networks. It offers a promising pathway for preserving link prediction information while safeguarding privacy, advancing the use of graph condensation in large-scale networks with privacy concerns.         ",
    "url": "https://arxiv.org/abs/2503.12156",
    "authors": [
      "Yunbo Long",
      "Liming Xu",
      "Alexandra Brintrup"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.12157",
    "title": "Weighted Graph Structure Learning with Attention Denoising for Node Classification",
    "abstract": "           Node classification in graphs aims to predict the categories of unlabeled nodes by utilizing a small set of labeled nodes. However, weighted graphs often contain noisy edges and anomalous edge weights, which can distort fine-grained relationships between nodes and hinder accurate classification. We propose the Edge Weight-aware Graph Structure Learning (EWGSL) method, which combines weight learning and graph structure learning to address these issues. EWGSL improves node classification by redefining attention coefficients in graph attention networks to incorporate node features and edge weights. It also applies graph structure learning to sparsify attention coefficients and uses a modified InfoNCE loss function to enhance performance by adapting to denoised graph weights. Extensive experimental results show that EWGSL has an average Micro-F1 improvement of 17.8% compared with the best baseline.         ",
    "url": "https://arxiv.org/abs/2503.12157",
    "authors": [
      "Tingting Wang",
      "Jiaxin Su",
      "Haobing Liu",
      "Ruobing Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12162",
    "title": "Probabilistic Graph Circuits: Deep Generative Models for Tractable Probabilistic Inference over Graphs",
    "abstract": "           Deep generative models (DGMs) have recently demonstrated remarkable success in capturing complex probability distributions over graphs. Although their excellent performance is attributed to powerful and scalable deep neural networks, it is, at the same time, exactly the presence of these highly non-linear transformations that makes DGMs intractable. Indeed, despite representing probability distributions, intractable DGMs deny probabilistic foundations by their inability to answer even the most basic inference queries without approximations or design choices specific to a very narrow range of queries. To address this limitation, we propose probabilistic graph circuits (PGCs), a framework of tractable DGMs that provide exact and efficient probabilistic inference over (arbitrary parts of) graphs. Nonetheless, achieving both exactness and efficiency is challenging in the permutation-invariant setting of graphs. We design PGCs that are inherently invariant and satisfy these two requirements, yet at the cost of low expressive power. Therefore, we investigate two alternative strategies to achieve the invariance: the first sacrifices the efficiency, and the second sacrifices the exactness. We demonstrate that ignoring the permutation invariance can have severe consequences in anomaly detection, and that the latter approach is competitive with, and sometimes better than, existing intractable DGMs in the context of molecular graph generation.         ",
    "url": "https://arxiv.org/abs/2503.12162",
    "authors": [
      "Milan Pape\u017e",
      "Martin Rektoris",
      "V\u00e1clav \u0160m\u00eddl",
      "Tom\u00e1\u0161 Pevn\u00fd"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12188",
    "title": "Multi-Agent Systems Execute Arbitrary Malicious Code",
    "abstract": "           Multi-agent systems coordinate LLM-based agents to perform tasks on users' behalf. In real-world applications, multi-agent systems will inevitably interact with untrusted inputs, such as malicious Web content, files, email attachments, etc. Using several recently proposed multi-agent frameworks as concrete examples, we demonstrate that adversarial content can hijack control and communication within the system to invoke unsafe agents and functionalities. This results in a complete security breach, up to execution of arbitrary malicious code on the user's device and/or exfiltration of sensitive data from the user's containerized environment. We show that control-flow hijacking attacks succeed even if the individual agents are not susceptible to direct or indirect prompt injection, and even if they refuse to perform harmful actions.         ",
    "url": "https://arxiv.org/abs/2503.12188",
    "authors": [
      "Harold Triedman",
      "Rishi Jha",
      "Vitaly Shmatikov"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12200",
    "title": "MODRIC: A Cost Effective MODular Data Center Network Architecture with Rich InterConnections",
    "abstract": "           Shipping container based modular architectures provide design flexibility in data centers with building blocks to expand the network as and when needed. In this paper, high capacity Modular Data Center (MDC) network architecture with Rich Inter Connections named MODRIC is proposed. MODRIC is a cost-effective switch-centric network design which allows building a flexible MDC network with commodity switches. It uses an inter-container connectivity similar to the structure of generalized hypercube in order to provide high inter-container bandwidth. Further, a hybrid Clos topology is used to build the container network. MODRIC is highly suitable for cost effectively building mega data centers requiring high throughput capacity and resilience against failures. This paper presents the proposed architecture, discusses its relevant properties, and proposes suitable addressing, routing and network construction schemes. The paper also presents comparative studies on its cost and performance with existing network topologies.         ",
    "url": "https://arxiv.org/abs/2503.12200",
    "authors": [
      "Nabajyoti Medhi",
      "Kumarjit Ray",
      "Rajdeep Ghosh",
      "Dilip Kumar Saikia"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.12207",
    "title": "ReDefining Code Comprehension: Function Naming as a Mechanism for Evaluating Code Comprehension",
    "abstract": "           \"Explain in Plain English\" (EiPE) questions are widely used to assess code comprehension skills but are challenging to grade automatically. Recent approaches like Code Generation Based Grading (CGBG) leverage large language models (LLMs) to generate code from student explanations and validate its equivalence to the original code using unit tests. However, this approach does not differentiate between high-level, purpose-focused responses and low-level, implementation-focused ones, limiting its effectiveness in assessing comprehension level. We propose a modified approach where students generate function names, emphasizing the function's purpose over implementation details. We evaluate this method in an introductory programming course and analyze it using Item Response Theory (IRT) to understand its effectiveness as exam items and its alignment with traditional EiPE grading standards. We also publish this work as an open source Python package for autograding EiPE questions, providing a scalable solution for adoption.         ",
    "url": "https://arxiv.org/abs/2503.12207",
    "authors": [
      "David H. Smith IV",
      "Max Fowler",
      "Paul Denny",
      "Craig Zilles"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2503.12215",
    "title": "Gun Detection Using Combined Human Pose and Weapon Appearance",
    "abstract": "           The increasing frequency of firearm-related incidents has necessitated advancements in security and surveillance systems, particularly in firearm detection within public spaces. Traditional gun detection methods rely on manual inspections and continuous human monitoring of CCTV footage, which are labor-intensive and prone to high false positive and negative rates. To address these limitations, we propose a novel approach that integrates human pose estimation with weapon appearance recognition using deep learning techniques. Unlike prior studies that focus on either body pose estimation or firearm detection in isolation, our method jointly analyzes posture and weapon presence to enhance detection accuracy in real-world, dynamic environments. To train our model, we curated a diverse dataset comprising images from open-source repositories such as IMFDB and Monash Guns, supplemented with AI-generated and manually collected images from web sources. This dataset ensures robust generalization and realistic performance evaluation under various surveillance conditions. Our research aims to improve the precision and reliability of firearm detection systems, contributing to enhanced public safety and threat mitigation in high-risk areas.         ",
    "url": "https://arxiv.org/abs/2503.12215",
    "authors": [
      "Amulya Reddy Maligireddy",
      "Manohar Reddy Uppula",
      "Nidhi Rastogi",
      "Yaswanth Reddy Parla"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12216",
    "title": "Counting the Trees in the Forest: Evaluating Prompt Segmentation for Classifying Code Comprehension Level",
    "abstract": "           Reading and understanding code are fundamental skills for novice programmers, and especially important with the growing prevalence of AI-generated code and the need to evaluate its accuracy and reliability. ``Explain in Plain English'' questions are a widely used approach for assessing code comprehension, but providing automated feedback, particularly on comprehension levels, is a challenging task. This paper introduces a novel method for automatically assessing the comprehension level of responses to ``Explain in Plain English'' questions. Central to this is the ability to distinguish between two response types: multi-structural, where students describe the code line-by-line, and relational, where they explain the code's overall purpose. Using a Large Language Model (LLM) to segment both the student's description and the code, we aim to determine whether the student describes each line individually (many segments) or the code as a whole (fewer segments). We evaluate this approach's effectiveness by comparing segmentation results with human classifications, achieving substantial agreement. We conclude with how this approach, which we release as an open source Python package, could be used as a formative feedback mechanism.         ",
    "url": "https://arxiv.org/abs/2503.12216",
    "authors": [
      "David H. Smith IV",
      "Max Fowler",
      "Paul Denny",
      "Craig Zilles"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.12217",
    "title": "TFHE-Coder: Evaluating LLM-agentic Fully Homomorphic Encryption Code Generation",
    "abstract": "           Fully Homomorphic Encryption over the torus (TFHE) enables computation on encrypted data without decryption, making it a cornerstone of secure and confidential computing. Despite its potential in privacy preserving machine learning, secure multi party computation, private blockchain transactions, and secure medical diagnostics, its adoption remains limited due to cryptographic complexity and usability challenges. While various TFHE libraries and compilers exist, practical code generation remains a hurdle. We propose a compiler integrated framework to evaluate LLM inference and agentic optimization for TFHE code generation, focusing on logic gates and ReLU activation. Our methodology assesses error rates, compilability, and structural similarity across open and closedsource LLMs. Results highlight significant limitations in off-the-shelf models, while agentic optimizations such as retrieval augmented generation (RAG) and few-shot prompting reduce errors and enhance code fidelity. This work establishes the first benchmark for TFHE code generation, demonstrating how LLMs, when augmented with domain-specific feedback, can bridge the expertise gap in FHE code generation.         ",
    "url": "https://arxiv.org/abs/2503.12217",
    "authors": [
      "Mayank Kumar",
      "Jiaqi Xue",
      "Mengxin Zheng",
      "Qian Lou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12218",
    "title": "Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels",
    "abstract": "           Deep learning has shown remarkable success in medical image analysis, but its reliance on large volumes of high-quality labeled data limits its applicability. While noisy labeled data are easier to obtain, directly incorporating them into training can degrade model performance. To address this challenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC) self-ensemble framework for robust medical image segmentation with noisy labels. The framework leverages the Mean Teacher architecture to ensure consistent learning under noise perturbations. It includes an adaptive label refinement mechanism that dynamically captures and weights differences across multiple disturbance versions to enhance the quality of noisy labels. Additionally, a sample-level uncertainty-based label selection algorithm is introduced to prioritize high-confidence samples for network updates, mitigating the impact of noisy annotations. Consistency learning is integrated to align the predictions of the student and teacher networks, further enhancing model robustness. Extensive experiments on two public datasets demonstrate the effectiveness of the proposed framework, showing significant improvements in segmentation performance. By fully exploiting the strengths of the Mean Teacher structure, the ALC framework effectively processes noisy labels, adapts to challenging scenarios, and achieves competitive results compared to state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2503.12218",
    "authors": [
      "Chengxuan Qian",
      "Kai Han",
      "Siqi Ma",
      "Chongwen Lyu",
      "Zhenlong Yuan",
      "Jun Chen",
      "Zhe Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12220",
    "title": "A Bubble-Cluster Federated Learning Framework for Privacy-Preserving Demand Forecasting on Heterogeneous Retail Data",
    "abstract": "           Federated learning (FL) enables retailers to share model parameters for demand forecasting while maintaining privacy. However, heterogeneous data across diverse regions, driven by factors such as varying consumer behavior, poses challenges to the effectiveness of federated learning. To tackle this challenge, we propose Bubble-Cluster Federated Learning (BFL), a novel clustering-based federated learning framework tailored for sales prediction. By leveraging differential privacy and feature importance distribution, BFL groups retailers into distinct \"bubbles\", each forming its own federated learning (FL) system to effectively isolate data heterogeneity. Within each bubble, Transformer models are designed to predict local sales for each client. Our experiments demonstrate that BFL significantly surpasses FedAvg and outperforms local learning in demand forecasting performance across all participating clients. Compared to local learning, BFL can achieve a 5.4\\% improvement in R\\textsuperscript{2}, a 69\\% reduction in RMSE, and a 45\\% decrease in MAE. Our study highlights BFL's adaptability in enabling effective federated learning through dynamic adjustments to noise levels and the range of clients participating in each bubble. This approach strategically groups participants into distinct \"bubbles\" while proactively identifying and filtering out risky clients that could compromise the FL system. The findings demonstrate BFL's ability to enhance collaborative learning in regression tasks on heterogeneous data, achieving a balance between forecasting accuracy and privacy preservation in retail applications. Additionally, BFL's capability to detect and neutralize poisoned data from clients enhances the system's robustness and reliability, ensuring more secure and effective federated learning.         ",
    "url": "https://arxiv.org/abs/2503.12220",
    "authors": [
      "Yunbo Long",
      "Liming Xu",
      "Ge Zheng",
      "Alexandra Brintrup"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.12225",
    "title": "Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents",
    "abstract": "           This article explores the gaps that can manifest when using a large language model (LLM) to obtain simplified interpretations of data practices from a complex privacy policy. We exemplify these gaps to showcase issues in accuracy, completeness, clarity and representation, while advocating for continued research to realize an LLM's true potential in revolutionizing privacy management through personal assistants and automated compliance checking.         ",
    "url": "https://arxiv.org/abs/2503.12225",
    "authors": [
      "Rinku Dewri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.12226",
    "title": "Research on Large Language Model Cross-Cloud Privacy Protection and Collaborative Training based on Federated Learning",
    "abstract": "           The fast development of large language models (LLMs) and popularization of cloud computing have led to increasing concerns on privacy safeguarding and data security of cross-cloud model deployment and training as the key challenges. We present a new framework for addressing these issues along with enabling privacy preserving collaboration on training between distributed clouds based on federated learning. Our mechanism encompasses cutting-edge cryptographic primitives, dynamic model aggregation techniques, and cross-cloud data harmonization solutions to enhance security, efficiency, and scalability to the traditional federated learning paradigm. Furthermore, we proposed a hybrid aggregation scheme to mitigate the threat of Data Leakage and to optimize the aggregation of model updates, thus achieving substantial enhancement on the model effectiveness and stability. Experimental results demonstrate that the training efficiency, privacy protection, and model accuracy of the proposed model compare favorably to those of the traditional federated learning method.         ",
    "url": "https://arxiv.org/abs/2503.12226",
    "authors": [
      "Ze Yang",
      "Yihong Jin",
      "Yihan Zhang",
      "Juntian Liu",
      "Xinhe Xu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12233",
    "title": "Robust Full-Space Physical Layer Security for STAR-RIS-Aided Wireless Networks: Eavesdropper with Uncertain Location and Channel",
    "abstract": "           A robust full-space physical layer security (PLS) transmission scheme is proposed in this paper considering the full-space wiretapping challenge of wireless networks supported by simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS). Different from the existing schemes, the proposed PLS scheme takes account of the uncertainty on the eavesdropper's position within the 360$^\\circ$ service area offered by the STAR-RIS. Specifically, the large system analytical method is utilized to derive the asymptotic expression of the average security rate achieved by the security user, considering that the base station (BS) only has the statistical information of the eavesdropper's channel state information (CSI) and the uncertainty of its location. To evaluate the effectiveness of the proposed PLS scheme, we first formulate an optimization problem aimed at maximizing the weighted sum rate of the security user and the public user. This optimization is conducted under the power allocation constraint, and some practical limitations for STAR-RIS implementation, through jointly designing the active and passive beamforming variables. A novel iterative algorithm based on the minimum mean-square error (MMSE) and cross-entropy optimization (CEO) methods is proposed to effectively address the established non-convex optimization problem with discrete variables. Simulation results indicate that the proposed robust PLS scheme can effectively mitigate the information leakage across the entire coverage area of the STAR-RIS-assisted system, leading to superior performance gain when compared to benchmark schemes encompassing traditional RIS-aided scheme.         ",
    "url": "https://arxiv.org/abs/2503.12233",
    "authors": [
      "Han Xiao",
      "Xiaoyan Hu",
      "Ang Li",
      "Wenjie Wang",
      "Kun Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.12249",
    "title": "Minuscule Cell Detection in AS-OCT Images with Progressive Field-of-View Focusing",
    "abstract": "           Anterior Segment Optical Coherence Tomography (AS-OCT) is an emerging imaging technique with great potential for diagnosing anterior uveitis, a vision-threatening ocular inflammatory condition. A hallmark of this condition is the presence of inflammatory cells in the eye's anterior chamber, and detecting these cells using AS-OCT images has attracted research interest. While recent efforts aim to replace manual cell detection with automated computer vision approaches, detecting extremely small (minuscule) objects in high-resolution images, such as AS-OCT, poses substantial challenges: (1) each cell appears as a minuscule particle, representing less than 0.005\\% of the image, making the detection difficult, and (2) OCT imaging introduces pixel-level noise that can be mistaken for cells, leading to false positive detections. To overcome these challenges, we propose a minuscule cell detection framework through a progressive field-of-view focusing strategy. This strategy systematically refines the detection scope from the whole image to a target region where cells are likely to be present, and further to minuscule regions potentially containing individual cells. Our framework consists of two modules. First, a Field-of-Focus module uses a vision foundation model to segment the target region. Subsequently, a Fine-grained Object Detection module introduces a specialized Minuscule Region Proposal followed by a Spatial Attention Network to distinguish individual cells from noise within the segmented region. Experimental results demonstrate that our framework outperforms state-of-the-art methods for cell detection, providing enhanced efficacy for clinical applications. Our code is publicly available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12249",
    "authors": [
      "Boyu Chen",
      "Ameenat L. Solebo",
      "Daqian Shi",
      "Jinge Wu",
      "Paul Taylor"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12258",
    "title": "Lithium-ion Battery Capacity Prediction via Conditional Recurrent Generative Adversarial Network-based Time-Series Regeneration",
    "abstract": "           Accurate capacity prediction is essential for the safe and reliable operation of batteries by anticipating potential failures beforehand. The performance of state-of-the-art capacity prediction methods is significantly hindered by the limited availability of training data, primarily attributed to the expensive experimentation and data sharing restrictions. To tackle this issue, this paper presents a recurrent conditional generative adversarial network (RCGAN) scheme to enrich the limited battery data by adding high-fidelity synthetic ones to improve the capacity prediction. The proposed RCGAN scheme consists of a generator network to generate synthetic samples that closely resemble the true data and a discriminator network to differentiate real and synthetic samples. Long shortterm memory (LSTM)-based generator and discriminator are leveraged to learn the temporal and spatial distributions in the multivariate time-series battery data. Moreover, the generator is conditioned on the capacity value to account for changes in battery dynamics due to the degradation over usage cycles. The effectiveness of the RCGAN is evaluated across six batteries from two benchmark datasets (NASA and MIT). The raw data is then augmented with synthetic samples from the RCGAN to train LSTM and gate recurrent unit (GRU) models for capacity prediction. Simulation results show that the models trained with augmented datasets significantly outperform those trained with the original datasets in capacity prediction.         ",
    "url": "https://arxiv.org/abs/2503.12258",
    "authors": [
      "Myisha A. Chowdhury",
      "Gift Modekwe",
      "Qiugang Lu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.12260",
    "title": "Enhancing Facial Expression Recognition through Dual-Direction Attention Mixed Feature Networks and CLIP: Application to 8th ABAW Challenge",
    "abstract": "           We present our contribution to the 8th ABAW challenge at CVPR 2025, where we tackle valence-arousal estimation, emotion recognition, and facial action unit detection as three independent challenges. Our approach leverages the well-known Dual-Direction Attention Mixed Feature Network (DDAMFN) for all three tasks, achieving results that surpass the proposed baselines. Additionally, we explore the use of CLIP for the emotion recognition challenge as an additional experiment. We provide insights into the architectural choices that contribute to the strong performance of our methods.         ",
    "url": "https://arxiv.org/abs/2503.12260",
    "authors": [
      "Josep Cabacas-Maso",
      "Elena Ortega-Beltr\u00e1n",
      "Ismael Benito-Altamirano",
      "Carles Ventura"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12282",
    "title": "Toward Foundation Models for Online Complex Event Detection in CPS-IoT: A Case Study",
    "abstract": "           Complex events (CEs) play a crucial role in CPS-IoT applications, enabling high-level decision-making in domains such as smart monitoring and autonomous systems. However, most existing models focus on short-span perception tasks, lacking the long-term reasoning required for CE detection. CEs consist of sequences of short-time atomic events (AEs) governed by spatiotemporal dependencies. Detecting them is difficult due to long, noisy sensor data and the challenge of filtering out irrelevant AEs while capturing meaningful patterns. This work explores CE detection as a case study for CPS-IoT foundation models capable of long-term reasoning. We evaluate three approaches: (1) leveraging large language models (LLMs), (2) employing various neural architectures that learn CE rules from data, and (3) adopting a neurosymbolic approach that integrates neural models with symbolic engines embedding human knowledge. Our results show that the state-space model, Mamba, which belongs to the second category, outperforms all methods in accuracy and generalization to longer, unseen sensor traces. These findings suggest that state-space models could be a strong backbone for CPS-IoT foundation models for long-span reasoning tasks.         ",
    "url": "https://arxiv.org/abs/2503.12282",
    "authors": [
      "Liying Han",
      "Gaofeng Dong",
      "Xiaomin Ouyang",
      "Lance Kaplan",
      "Federico Cerutti",
      "Mani Srivastava"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12293",
    "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models",
    "abstract": "           The Unified Modeling Language is a standardized visual language widely used for modeling and documenting the design of software systems. Although many tools generate UML diagrams from UML code, generating executable UML code from image-based UML diagrams remains challenging. This paper proposes a new approach to generate UML code using a large multimodal language model automatically. Synthetic UML activity and sequence diagram datasets were created to train and test the model. We compared standard fine-tuning with LoRA techniques to optimize base models. The experiments measured code generation accuracy across different model sizes and training strategies. These results demonstrated that domain-adapted MM-LLMs perform for UML code generation automation, whereby, at the best model, it achieved BLEU and SSIM scores of 0.779 and 0.942 on sequence diagrams. This will enable the modernization of legacy systems and decrease the manual effort in software development workflows.         ",
    "url": "https://arxiv.org/abs/2503.12293",
    "authors": [
      "Averi Bates",
      "Ryan Vavricka",
      "Shane Carleton",
      "Ruosi Shao",
      "Chongle Pan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12298",
    "title": "Computing Modes of Instability of Parameterized Nonlinear Systems for Vulnerability Assessment",
    "abstract": "           Engineered systems naturally experience large disturbances that can disrupt desired operation because the system may fail to recover to a stable equilibrium point. It is valuable to determine the mechanism of instability when the system is subject to a particular finite-time disturbance, because this information can be used to improve vulnerability detection, and to design controllers to reduce vulnerability. Often there exists a particular unstable equilibrium point on the region of attraction boundary of the stable equilibrium point such that the unstable eigenvector of the Jacobian at this unstable equilibrium point represents the mode of instability for the disturbance. Unfortunately, it is challenging to find this mode of instability, especially in high dimensional systems, because it is computationally intractable to obtain this particular unstable equilibrium point. This paper develops a novel algorithm for numerically computing the mode of instability for parameterized nonlinear systems without identifying the particular unstable equilibrium point, resulting in a computationally efficient method. The key idea is to first consider the setting where the system recovers, and to average the Jacobian along the system trajectory from the post-disturbance state up until the Jacobian becomes stable. As the system approaches inability to recover, the averaged Jacobians converge to the Jacobian at the particular unstable equilibrium point, and can be used to extract the unstable eigenvector representing the mode of instability. Convergence guarantees are provided for computing the mode of instability, both for the theoretical setting in continuous time, and for the proposed algorithm which relies on numerical integration. Numerical examples illustrate the successful application of the method to identify the mechanism of instability in power systems subject to temporary short circuits.         ",
    "url": "https://arxiv.org/abs/2503.12298",
    "authors": [
      "Jinghan Wang",
      "Michael W. Fisher"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.12301",
    "title": "One Goal, Many Challenges: Robust Preference Optimization Amid Content-Aware and Multi-Source Noise",
    "abstract": "           Large Language Models (LLMs) have made significant strides in generating human-like responses, largely due to preference alignment techniques. However, these methods often assume unbiased human feedback, which is rarely the case in real-world scenarios. This paper introduces Content-Aware Noise-Resilient Preference Optimization (CNRPO), a novel framework that addresses multiple sources of content-dependent noise in preference learning. CNRPO employs a multi-objective optimization approach to separate true preferences from content-aware noises, effectively mitigating their impact. We leverage backdoor attack mechanisms to efficiently learn and control various noise sources within a single model. Theoretical analysis and extensive experiments on different synthetic noisy datasets demonstrate that CNRPO significantly improves alignment with primary human preferences while controlling for secondary noises and biases, such as response length and harmfulness.         ",
    "url": "https://arxiv.org/abs/2503.12301",
    "authors": [
      "Amirabbas Afzali",
      "Amirhossein Afsharrad",
      "Seyed Shahabeddin Mousavi",
      "Sanjay Lall"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.12314",
    "title": "Empirical Privacy Variance",
    "abstract": "           We propose the notion of empirical privacy variance and study it in the context of differentially private fine-tuning of language models. Specifically, we show that models calibrated to the same $(\\varepsilon, \\delta)$-DP guarantee using DP-SGD with different hyperparameter configurations can exhibit significant variations in empirical privacy, which we quantify through the lens of memorization. We investigate the generality of this phenomenon across multiple dimensions and discuss why it is surprising and relevant. Through regression analysis, we examine how individual and composite hyperparameters influence empirical privacy. The results reveal a no-free-lunch trade-off: existing practices of hyperparameter tuning in DP-SGD, which focus on optimizing utility under a fixed privacy budget, often come at the expense of empirical privacy. To address this, we propose refined heuristics for hyperparameter selection that explicitly account for empirical privacy, showing that they are both precise and practically useful. Finally, we take preliminary steps to understand empirical privacy variance. We propose two hypotheses, identify limitations in existing techniques like privacy auditing, and outline open questions for future research.         ",
    "url": "https://arxiv.org/abs/2503.12314",
    "authors": [
      "Yuzheng Hu",
      "Fan Wu",
      "Ruicheng Xian",
      "Yuhang Liu",
      "Lydia Zakynthinou",
      "Pritish Kamath",
      "Chiyuan Zhang",
      "David Forsyth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.12317",
    "title": "A Transformer-based survival model for prediction of all-cause mortality in heart failure patients: a multi-cohort study",
    "abstract": "           We developed and validated TRisk, a Transformer-based AI model predicting 36-month mortality in heart failure patients by analysing temporal patient journeys from UK electronic health records (EHR). Our study included 403,534 heart failure patients (ages 40-90) from 1,418 English general practices, with 1,063 practices for model derivation and 355 for external validation. TRisk was compared against the MAGGIC-EHR model across various patient subgroups. With median follow-up of 9 months, TRisk achieved a concordance index of 0.845 (95% confidence interval: [0.841, 0.849]), significantly outperforming MAGGIC-EHR's 0.728 (0.723, 0.733) for predicting 36-month all-cause mortality. TRisk showed more consistent performance across sex, age, and baseline characteristics, suggesting less bias. We successfully adapted TRisk to US hospital data through transfer learning, achieving a C-index of 0.802 (0.789, 0.816) with 21,767 patients. Explainability analyses revealed TRisk captured established risk factors while identifying underappreciated predictors like cancers and hepatic failure that were important across both cohorts. Notably, cancers maintained strong prognostic value even a decade after diagnosis. TRisk demonstrated well-calibrated mortality prediction across both healthcare systems. Our findings highlight the value of tracking longitudinal health profiles and revealed risk factors not included in previous expert-driven models.         ",
    "url": "https://arxiv.org/abs/2503.12317",
    "authors": [
      "Shishir Rao",
      "Nouman Ahmed",
      "Gholamreza Salimi-Khorshidi",
      "Christopher Yau",
      "Huimin Su",
      "Nathalie Conrad",
      "Folkert W Asselbergs",
      "Mark Woodward",
      "Rod Jackson",
      "John GF Cleland",
      "Kazem Rahimi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12339",
    "title": "Augmented Adversarial Trigger Learning",
    "abstract": "           Gradient optimization-based adversarial attack methods automate the learning of adversarial triggers to generate jailbreak prompts or leak system prompts. In this work, we take a closer look at the optimization objective of adversarial trigger learning and propose ATLA: Adversarial Trigger Learning with Augmented objectives. ATLA improves the negative log-likelihood loss used by previous studies into a weighted loss formulation that encourages the learned adversarial triggers to optimize more towards response format tokens. This enables ATLA to learn an adversarial trigger from just one query-response pair and the learned trigger generalizes well to other similar queries. We further design a variation to augment trigger optimization with an auxiliary loss that suppresses evasive responses. We showcase how to use ATLA to learn adversarial suffixes jailbreaking LLMs and to extract hidden system prompts. Empirically we demonstrate that ATLA consistently outperforms current state-of-the-art techniques, achieving nearly 100% success in attacking while requiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high generalization to unseen queries and transfer well to new LLMs.         ",
    "url": "https://arxiv.org/abs/2503.12339",
    "authors": [
      "Zhe Wang",
      "Yanjun Qi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12350",
    "title": "ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions",
    "abstract": "           LiDAR-based place recognition (LPR) is a key component for autonomous driving, and its resilience to environmental corruption is critical for safety in high-stakes applications. While state-of-the-art (SOTA) LPR methods perform well in clean weather, they still struggle with weather-induced corruption commonly encountered in driving scenarios. To tackle this, we propose ResLPRNet, a novel LiDAR data restoration network that largely enhances LPR performance under adverse weather by restoring corrupted LiDAR scans using a wavelet transform-based network. ResLPRNet is efficient, lightweight and can be integrated plug-and-play with pretrained LPR models without substantial additional computational cost. Given the lack of LPR datasets under adverse weather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods under a wide range of LiDAR distortions induced by severe snow, fog, and rain conditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets demonstrate the resilience and notable gains achieved by using our restoration method with multiple LPR approaches in challenging weather scenarios. Our code and benchmark are publicly available here: this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12350",
    "authors": [
      "Wenqing Kuang",
      "Xiongwei Zhao",
      "Yehui Shen",
      "Congcong Wen",
      "Huimin Lu",
      "Zongtan Zhou",
      "Xieyuanli Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12353",
    "title": "Synthetic Data for Robust AI Model Development in Regulated Enterprises",
    "abstract": "           In today's business landscape, organizations need to find the right balance between using their customers' data ethically to power AI solutions and being compliant regarding data privacy and data usage regulations. In this paper, we discuss synthetic data as a possible solution to this dilemma. Synthetic data is simulated data that mimics the real data. We explore how organizations in heavily regulated industries, such as financial institutions or healthcare organizations, can leverage synthetic data to build robust AI solutions while staying compliant. We demonstrate that synthetic data offers two significant advantages by allowing AI models to learn from more diverse data and by helping organizations stay compliant against data privacy laws with the use of synthetic data instead of customer information. We discuss case studies to show how synthetic data can be effectively used in the finance and healthcare sector while discussing the challenges of using synthetic data and some ethical questions it raises. Our research finds that synthetic data could be a game-changer for AI in regulated industries. The potential can be realized when industry, academia, and regulators collaborate to build solutions. We aim to initiate discussions on the use of synthetic data to build ethical, responsible, and effective AI systems in regulated enterprise industries.         ",
    "url": "https://arxiv.org/abs/2503.12353",
    "authors": [
      "Aditi Godbole"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12354",
    "title": "Probabilistic Neural Networks (PNNs) with t-Distributed Outputs: Adaptive Prediction Intervals Beyond Gaussian Assumptions",
    "abstract": "           Traditional neural network regression models provide only point estimates, failing to capture predictive uncertainty. Probabilistic neural networks (PNNs) address this limitation by producing output distributions, enabling the construction of prediction intervals. However, the common assumption of Gaussian output distributions often results in overly wide intervals, particularly in the presence of outliers or deviations from normality. To enhance the adaptability of PNNs, we propose t-Distributed Neural Networks (TDistNNs), which generate t-distributed outputs, parameterized by location, scale, and degrees of freedom. The degrees of freedom parameter allows TDistNNs to model heavy-tailed predictive distributions, improving robustness to non-Gaussian data and enabling more adaptive uncertainty quantification. We develop a novel loss function tailored for the t-distribution and derive efficient gradient computations for seamless integration into deep learning frameworks. Empirical evaluations on synthetic and real-world data demonstrate that TDistNNs improve the balance between coverage and interval width. Notably, for identical architectures, TDistNNs consistently produce narrower prediction intervals than Gaussian-based PNNs while maintaining proper coverage. This work contributes a flexible framework for uncertainty estimation in neural networks tasked with regression, particularly suited to settings involving complex output distributions.         ",
    "url": "https://arxiv.org/abs/2503.12354",
    "authors": [
      "Farhad Pourkamali-Anaraki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.12365",
    "title": "HyperKAN: Hypergraph Representation Learning with Kolmogorov-Arnold Networks",
    "abstract": "           Hypergraph representation learning has garnered increasing attention across various domains due to its capability to model high-order relationships. Traditional methods often rely on hypergraph neural networks (HNNs) employing message passing mechanisms to aggregate vertex and hyperedge features. However, these methods are constrained by their dependence on hypergraph topology, leading to the challenge of imbalanced information aggregation, where high-degree vertices tend to aggregate redundant features, while low-degree vertices often struggle to capture sufficient structural features. To overcome the above challenges, we introduce HyperKAN, a novel framework for hypergraph representation learning that transcends the limitations of message-passing techniques. HyperKAN begins by encoding features for each vertex and then leverages Kolmogorov-Arnold Networks (KANs) to capture complex nonlinear relationships. By adjusting structural features based on similarity, our approach generates refined vertex representations that effectively addresses the challenge of imbalanced information aggregation. Experiments conducted on the real-world datasets demonstrate that HyperKAN significantly outperforms state of-the-art HNN methods, achieving nearly a 9% performance improvement on the Senate dataset.         ",
    "url": "https://arxiv.org/abs/2503.12365",
    "authors": [
      "Xiangfei Fang",
      "Boying Wang",
      "Chengying Huan",
      "Shaonan Ma",
      "Heng Zhang",
      "Chen Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.12366",
    "title": "ASD Classification on Dynamic Brain Connectome using Temporal Random Walk with Transformer-based Dynamic Network Embedding",
    "abstract": "           Autism Spectrum Disorder (ASD) is a complex neurological condition characterized by varied developmental impairments, especially in communication and social interaction. Accurate and early diagnosis of ASD is crucial for effective intervention, which is enhanced by richer representations of brain activity. The brain functional connectome, which refers to the statistical relationships between different brain regions measured through neuroimaging, provides crucial insights into brain function. Traditional static methods often fail to capture the dynamic nature of brain activity, in contrast, dynamic brain connectome analysis provides a more comprehensive view by capturing the temporal variations in the brain. We propose BrainTWT, a novel dynamic network embedding approach that captures temporal evolution of the brain connectivity over time and considers also the dynamics between different temporal network snapshots. BrainTWT employs temporal random walks to capture dynamics across different temporal network snapshots and leverages the Transformer's ability to model long term dependencies in sequential data to learn the discriminative embeddings from these temporal sequences using temporal structure prediction tasks. The experimental evaluation, utilizing the Autism Brain Imaging Data Exchange (ABIDE) dataset, demonstrates that BrainTWT outperforms baseline methods in ASD classification.         ",
    "url": "https://arxiv.org/abs/2503.12366",
    "authors": [
      "Suchanuch Piriyasatit",
      "Chaohao Yuan",
      "Ercan Engin Kuruoglu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2503.12374",
    "title": "Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at GitHub Issue Resolution",
    "abstract": "           AI-driven software development has rapidly advanced with the emergence of software development agents that leverage large language models (LLMs) to tackle complex, repository-level software engineering tasks. These agents go beyond just generation of final code; they engage in multi-step reasoning, utilize various tools for code modification and debugging, and interact with execution environments to diagnose and iteratively resolve issues. However, most existing evaluations focus primarily on static analyses of final code outputs, yielding limited insights into the agents' dynamic problem-solving processes. To fill this gap, we conduct an in-depth empirical study on 3,977 solving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked agents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our exploratory analysis shows that Python execution errors during the issue resolution phase correlate with lower resolution rates and increased reasoning overheads. We have identified the most prevalent errors -- such as ModuleNotFoundError and TypeError -- and highlighted particularly challenging errors like OSError and database-related issues (e.g., IntegrityError) that demand significantly more debugging effort. Furthermore, we have discovered 3 bugs in the SWE-Bench platform that affect benchmark fairness and accuracy; these issues have been reported to and confirmed by the maintainers. To promote transparency and foster future research, we publicly share our datasets and analysis scripts.         ",
    "url": "https://arxiv.org/abs/2503.12374",
    "authors": [
      "Zhi Chen",
      "Wei Ma",
      "Lingxiao Jiang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12377",
    "title": "GCBLANE: A graph-enhanced convolutional BiLSTM attention network for improved transcription factor binding site prediction",
    "abstract": "           Identifying transcription factor binding sites (TFBS) is crucial for understanding gene regulation, as these sites enable transcription factors (TFs) to bind to DNA and modulate gene expression. Despite advances in high-throughput sequencing, accurately identifying TFBS remains challenging due to the vast genomic data and complex binding patterns. GCBLANE, a graph-enhanced convolutional bidirectional Long Short-Term Memory (LSTM) attention network, is introduced to address this issue. It integrates convolutional, multi-head attention, and recurrent layers with a graph neural network to detect key features for TFBS prediction. On 690 ENCODE ChIP-Seq datasets, GCBLANE achieved an average AUC of 0.943, and on 165 ENCODE datasets, it reached an AUC of 0.9495, outperforming advanced models that utilize multimodal approaches, including DNA shape information. This result underscores GCBLANE's effectiveness compared to other methods. By combining graph-based learning with sequence analysis, GCBLANE significantly advances TFBS prediction.         ",
    "url": "https://arxiv.org/abs/2503.12377",
    "authors": [
      "Jonas Chris Ferrao",
      "Dickson Dias",
      "Sweta Morajkar",
      "Manisha Gokuldas Fal Dessai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Genomics (q-bio.GN)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2503.12380",
    "title": "A Unified Approach to Enforce Non-Negativity Constraint in Neural Network Approximation for Optimal Voltage Regulation",
    "abstract": "           Power system voltage regulation is crucial to maintain power quality while integrating intermittent renewable resources in distribution grids. However, the system model on the grid edge is often unknown, making it difficult to model physical equations for optimal control. Therefore, previous work proposes structured data-driven methods like input convex neural networks (ICNN) for \"optimal\" control without relying on a physical model. While ICNNs offer theoretical guarantees based on restrictive assumptions of non-negative neural network (NN) parameters, can one improve the approximation power with an extra step on negative duplication of inputs? We show that such added mirroring step fails to improve accuracy, as a linear combination of the original input and duplicated input is equivalent to a linear operation of ICNN's input without duplication. While this design can not improve performance, we propose a unified approach to embed the non-negativity constraint as a regularized optimization of NN, contrary to the existing methods, which added a loosely integrated second step for post-processing on parameter negation. Our integration directly ties back-propagation to simultaneously minimizing the approximation error while enforcing the convexity constraints. Numerical experiments validate the issues of the mirroring method and show that our integrated objective can avoid problems such as unstable training and non-convergence existing in other methods for optimal control.         ",
    "url": "https://arxiv.org/abs/2503.12380",
    "authors": [
      "Jiaqi Wu",
      "Jingyi Yuan",
      "Yang Weng",
      "Guangwen Wang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.12381",
    "title": "Deepfake Detection with Optimized Hybrid Model: EAR Biometric Descriptor via Improved RCNN",
    "abstract": "           Deepfake is a widely used technology employed in recent years to create pernicious content such as fake news, movies, and rumors by altering and substituting facial information from various sources. Given the ongoing evolution of deepfakes investigation of continuous identification and prevention is crucial. Due to recent technological advancements in AI (Artificial Intelligence) distinguishing deepfakes and artificially altered images has become challenging. This approach introduces the robust detection of subtle ear movements and shape changes to generate ear descriptors. Further, we also propose a novel optimized hybrid deepfake detection model that considers the ear biometric descriptors via enhanced RCNN (Region-Based Convolutional Neural Network). Initially, the input video is converted into frames and preprocessed through resizing, normalization, grayscale conversion, and filtering processes followed by face detection using the Viola-Jones technique. Next, a hybrid model comprising DBN (Deep Belief Network) and Bi-GRU (Bidirectional Gated Recurrent Unit) is utilized for deepfake detection based on ear descriptors. The output from the detection phase is determined through improved score-level fusion. To enhance the performance, the weights of both detection models are optimally tuned using the SU-JFO (Self-Upgraded Jellyfish Optimization method). Experimentation is conducted based on four scenarios: compression, noise, rotation, pose, and illumination on three different datasets. The performance results affirm that our proposed method outperforms traditional models such as CNN (Convolution Neural Network), SqueezeNet, LeNet, LinkNet, LSTM (Long Short-Term Memory), DFP (Deepfake Predictor) [1], and ResNext+CNN+LSTM [2] in terms of various performance metrics viz. accuracy, specificity, and precision.         ",
    "url": "https://arxiv.org/abs/2503.12381",
    "authors": [
      "Ruchika Sharma",
      "Rudresh Dwivedi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2503.12382",
    "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
    "abstract": "           Despite the substantial advancements demonstrated by learning-based neural models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time compression - an indispensable criterion for numerous industrial applications - remains a formidable challenge. This paper proposes RENO, the first real-time neural codec for 3D LiDAR point clouds, achieving superior performance with a lightweight model. RENO skips the octree construction and directly builds upon the multiscale sparse tensor representation. Instead of the multi-stage inferring, RENO devises sparse occupancy codes, which exploit cross-scale correlation and derive voxels' occupancy in a one-shot manner, greatly saving processing time. Experimental results demonstrate that the proposed RENO achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform (e.g., one RTX 3090 GPU) for both encoding and decoding processes, while providing 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco, respectively, at a similar quality. RENO model size is merely 1MB, making it attractive for practical applications. The source code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12382",
    "authors": [
      "Kang You",
      "Tong Chen",
      "Dandan Ding",
      "M. Salman Asif",
      "Zhan Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2503.12406",
    "title": "Bio-Inspired Plastic Neural Networks for Zero-Shot Out-of-Distribution Generalization in Complex Animal-Inspired Robots",
    "abstract": "           Artificial neural networks can be used to solve a variety of robotic tasks. However, they risk failing catastrophically when faced with out-of-distribution (OOD) situations. Several approaches have employed a type of synaptic plasticity known as Hebbian learning that can dynamically adjust weights based on local neural activities. Research has shown that synaptic plasticity can make policies more robust and help them adapt to unforeseen changes in the environment. However, networks augmented with Hebbian learning can lead to weight divergence, resulting in network instability. Furthermore, such Hebbian networks have not yet been applied to solve legged locomotion in complex real robots with many degrees of freedom. In this work, we improve the Hebbian network with a weight normalization mechanism for preventing weight divergence, analyze the principal components of the Hebbian's weights, and perform a thorough evaluation of network performance in locomotion control for real 18-DOF dung beetle-like and 16-DOF gecko-like robots. We find that the Hebbian-based plastic network can execute zero-shot sim-to-real adaptation locomotion and generalize to unseen conditions, such as uneven terrain and morphological damage.         ",
    "url": "https://arxiv.org/abs/2503.12406",
    "authors": [
      "Binggwong Leung",
      "Worasuchad Haomachai",
      "Joachim Winther Pedersen",
      "Sebastian Risi",
      "Poramate Manoonpong"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12435",
    "title": "XAI-Driven Client Selection for Federated Learning in Scalable 6G Network Slicing",
    "abstract": "           In recent years, network slicing has embraced artificial intelligence (AI) models to manage the growing complexity of communication networks. In such a situation, AI-driven zero-touch network automation should present a high degree of flexibility and viability, especially when deployed in live production networks. However, centralized controllers suffer from high data communication overhead due to the vast amount of user data, and most network slices are reluctant to share private data. In federated learning systems, selecting trustworthy clients to participate in training is critical for ensuring system performance and reliability. The present paper proposes a new approach to client selection by leveraging an XAI method to guarantee scalable and fast operation of federated learning based analytic engines that implement slice-level resource provisioning at the RAN-Edge in a non-IID scenario. Attributions from XAI are used to guide the selection of devices participating in training. This approach enhances network trustworthiness for users and addresses the black-box nature of neural network models. The simulations conducted outperformed the standard approach in terms of both convergence time and computational cost, while also demonstrating high scalability.         ",
    "url": "https://arxiv.org/abs/2503.12435",
    "authors": [
      "Martino Chiarani",
      "Swastika Roy",
      "Christos Verikoukis",
      "Fabrizio Granelli"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12447",
    "title": "Causality Model for Semantic Understanding on Videos",
    "abstract": "           After a decade of prosperity, the development of video understanding has reached a critical juncture, where the sole reliance on massive data and complex architectures is no longer a one-size-fits-all solution to all situations. The presence of ubiquitous data imbalance hampers DNNs from effectively learning the underlying causal mechanisms, leading to significant performance drops when encountering distribution shifts, such as long-tail imbalances and perturbed imbalances. This realization has prompted researchers to seek alternative methodologies to capture causal patterns in video data. To tackle these challenges and increase the robustness of DNNs, causal modeling emerged as a principle to discover the true causal patterns behind the observed correlations. This thesis focuses on the domain of semantic video understanding and explores the potential of causal modeling to advance two fundamental tasks: Video Relation Detection (VidVRD) and Video Question Answering (VideoQA).         ",
    "url": "https://arxiv.org/abs/2503.12447",
    "authors": [
      "Li Yicong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12453",
    "title": "Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation",
    "abstract": "           Previous works studied how deep neural networks (DNNs) perceive image content in terms of their biases towards different image cues, such as texture and shape. Previous methods to measure shape and texture biases are typically style-transfer-based and limited to DNNs for image classification. In this work, we provide a new evaluation procedure consisting of 1) a cue-decomposition method that comprises two AI-free data pre-processing methods extracting shape and texture cues, respectively, and 2) a novel cue-decomposition shape bias evaluation metric that leverages the cue-decomposition data. For application purposes we introduce a corresponding cue-decomposition robustness metric that allows for the estimation of the robustness of a DNN w.r.t. image corruptions. In our numerical experiments, our findings for biases in image classification DNNs align with those of previous evaluation metrics. However, our cue-decomposition robustness metric shows superior results in terms of estimating the robustness of DNNs. Furthermore, our results for DNNs on the semantic segmentation datasets Cityscapes and ADE20k for the first time shed light into the biases of semantic segmentation DNNs.         ",
    "url": "https://arxiv.org/abs/2503.12453",
    "authors": [
      "Edgar Heinert",
      "Thomas Gottwald",
      "Annika M\u00fctze",
      "Matthias Rottmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12464",
    "title": "Learning Privacy from Visual Entities",
    "abstract": "           Subjective interpretation and content diversity make predicting whether an image is private or public a challenging task. Graph neural networks combined with convolutional neural networks (CNNs), which consist of 14,000 to 500 millions parameters, generate features for visual entities (e.g., scene and object types) and identify the entities that contribute to the decision. In this paper, we show that using a simpler combination of transfer learning and a CNN to relate privacy with scene types optimises only 732 parameters while achieving comparable performance to that of graph-based methods. On the contrary, end-to-end training of graph-based methods can mask the contribution of individual components to the classification performance. Furthermore, we show that a high-dimensional feature vector, extracted with CNNs for each visual entity, is unnecessary and complexifies the model. The graph component has also negligible impact on performance, which is driven by fine-tuning the CNN to optimise image features for privacy nodes.         ",
    "url": "https://arxiv.org/abs/2503.12464",
    "authors": [
      "Alessio Xompero",
      "Andrea Cavallaro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12478",
    "title": "KDSelector: A Knowledge-Enhanced and Data-Efficient Model Selector Learning Framework for Time Series Anomaly Detection",
    "abstract": "           Model selection has been raised as an essential problem in the area of time series anomaly detection (TSAD), because there is no single best TSAD model for the highly heterogeneous time series in real-world applications. However, despite the success of existing model selection solutions that train a classification model (especially neural network, NN) using historical data as a selector to predict the correct TSAD model for each series, the NN-based selector learning methods used by existing solutions do not make full use of the knowledge in the historical data and require iterating over all training samples, which limits the accuracy and training speed of the selector. To address these limitations, we propose KDSelector, a novel knowledge-enhanced and data-efficient framework for learning the NN-based TSAD model selector, of which three key components are specifically designed to integrate available knowledge into the selector and dynamically prune less important and redundant samples during the learning. We develop a TSAD model selection system with KDSelector as the internal, to demonstrate how users improve the accuracy and training speed of their selectors by using KDSelector as a plug-and-play module. Our demonstration video is hosted at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12478",
    "authors": [
      "Zhiyu Liang",
      "Dongrui Cai",
      "Chenyuan Zhang",
      "Zheng Liang",
      "Chen Liang",
      "Bo Zheng",
      "Shi Qiu",
      "Jin Wang",
      "Hongzhi Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2503.12483",
    "title": "Modularization is Better: Effective Code Generation with Modular Prompting",
    "abstract": "           Large Language Models are transforming software development by automatically generating code. Current prompting techniques such as Chain-of-Thought (CoT) suggest tasks step by step and the reasoning process follows a linear structure, which hampers the understanding of complex programming problems, particularly those requiring hierarchical solutions. Inspired by the principle of modularization in software development, in this work, we propose a novel prompting technique, called MoT, to enhance the code generation performance of LLMs. At first, MoT exploits modularization principles to decompose complex programming problems into smaller, independent reasoning steps, enabling a more structured and interpretable problem-solving process. This hierarchical structure improves the LLM's ability to comprehend complex programming problems. Then, it structures the reasoning process using an MLR Graph (Multi-Level Reasoning Graph), which hierarchically organizes reasoning steps. This approach enhances modular understanding and ensures better alignment between reasoning steps and the generated code, significantly improving code generation performance. Our experiments on two advanced LLMs (GPT-4o-mini and DeepSeek-R1), comparing MoT to six baseline prompting techniques across six widely used datasets, HumanEval, HumanEval-ET, HumanEval+, MBPP, MBPP-ET, and MBPP+, demonstrate that MoT significantly outperforms existing baselines (e.g., CoT and SCoT), achieving Pass@1 scores ranging from 58.1% to 95.1%. The experimental results confirm that MoT significantly enhances the performance of LLM-based code generation.         ",
    "url": "https://arxiv.org/abs/2503.12483",
    "authors": [
      "Ruwei Pan",
      "Hongyu Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.12495",
    "title": "BS-Mamba for Black-Soil Area Detection On the Qinghai-Tibetan Plateau",
    "abstract": "           Extremely degraded grassland on the Qinghai-Tibetan Plateau (QTP) presents a significant environmental challenge due to overgrazing, climate change, and rodent activity, which degrade vegetation cover and soil quality. These extremely degraded grassland on QTP, commonly referred to as black-soil area, require accurate assessment to guide effective restoration efforts. In this paper, we present a newly created QTP black-soil dataset, annotated under expert guidance. We introduce a novel neural network model, BS-Mamba, specifically designed for the black-soil area detection using UAV remote sensing imagery. The BS-Mamba model demonstrates higher accuracy in identifying black-soil area across two independent test datasets than the state-of-the-art models. This research contributes to grassland restoration by providing an efficient method for assessing the extent of black-soil area on the QTP.         ",
    "url": "https://arxiv.org/abs/2503.12495",
    "authors": [
      "Xuan Ma",
      "Zewen Lv",
      "Chengcai Ma",
      "Tao Zhang",
      "Yuelan Xin",
      "Kun Zhan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12497",
    "title": "Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy",
    "abstract": "           Malicious users attempt to replicate commercial models functionally at low cost by training a clone model with query responses. It is challenging to timely prevent such model-stealing attacks to achieve strong protection and maintain utility. In this paper, we propose a novel non-parametric detector called Account-aware Distribution Discrepancy (ADD) to recognize queries from malicious users by leveraging account-wise local dependency. We formulate each class as a Multivariate Normal distribution (MVN) in the feature space and measure the malicious score as the sum of weighted class-wise distribution discrepancy. The ADD detector is combined with random-based prediction poisoning to yield a plug-and-play defense module named D-ADD for image classification models. Results of extensive experimental studies show that D-ADD achieves strong defense against different types of attacks with little interference in serving benign users for both soft and hard-label settings.         ",
    "url": "https://arxiv.org/abs/2503.12497",
    "authors": [
      "Jian-Ping Mei",
      "Weibin Zhang",
      "Jie Chen",
      "Xuyun Zhang",
      "Tiantian Zhu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12527",
    "title": "A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry",
    "abstract": "           The bias of low-cost Inertial Measurement Units (IMU) is a critical factor affecting the performance of Visual-Inertial Odometry (VIO). In particular, when visual tracking encounters errors, the optimized bias results may deviate significantly from the true values, adversely impacting the system's stability and localization precision. In this paper, we propose a novel plug-and-play framework featuring the Inertial Prior Network (IPNet), which is designed to accurately estimate IMU bias. Recognizing the substantial impact of initial bias errors in low-cost inertial devices on system performance, our network directly leverages raw IMU data to estimate the mean bias, eliminating the dependency on historical estimates in traditional recursive predictions and effectively preventing error propagation. Furthermore, we introduce an iterative approach to calculate the mean value of the bias for network training, addressing the lack of bias labels in many visual-inertial datasets. The framework is evaluated on two public datasets and one self-collected dataset. Extensive experiments demonstrate that our method significantly enhances both localization precision and robustness, with the ATE-RMSE metric improving on average by 46\\%. The source code and video will be available at \\textcolor{red}{this https URL}.         ",
    "url": "https://arxiv.org/abs/2503.12527",
    "authors": [
      "Yang Yi",
      "Kunqing Wang",
      "Jinpu Zhang",
      "Zhen Tan",
      "Xiangke Wang",
      "Hui Shen",
      "Dewen Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12534",
    "title": "Time-EAPCR-T: A Universal Deep Learning Approach for Anomaly Detection in Industrial Equipment",
    "abstract": "           With the advancement of Industry 4.0, intelligent manufacturing extensively employs sensors for real-time multidimensional data collection, playing a crucial role in equipment monitoring, process optimisation, and efficiency enhancement. Industrial data exhibit characteristics such as multi-source heterogeneity, nonlinearity, strong coupling, and temporal interactions, while also being affected by noise interference. These complexities make it challenging for traditional anomaly detection methods to extract key features, impacting detection accuracy and stability. Traditional machine learning approaches often struggle with such complex data due to limitations in processing capacity and generalisation ability, making them inadequate for practical applications. While deep learning feature extraction modules have demonstrated remarkable performance in image and text processing, they remain ineffective when applied to multi-source heterogeneous industrial data lacking explicit correlations. Moreover, existing multi-source heterogeneous data processing techniques still rely on dimensionality reduction and feature selection, which can lead to information loss and difficulty in capturing high-order interactions. To address these challenges, this study applies the EAPCR and Time-EAPCR models proposed in previous research and introduces a new model, Time-EAPCR-T, where Transformer replaces the LSTM module in the time-series processing component of Time-EAPCR. This modification effectively addresses multi-source data heterogeneity, facilitates efficient multi-source feature fusion, and enhances the temporal feature extraction capabilities of multi-source industrial this http URL results demonstrate that the proposed method outperforms existing approaches across four industrial datasets, highlighting its broad application potential.         ",
    "url": "https://arxiv.org/abs/2503.12534",
    "authors": [
      "Huajie Liang",
      "Di Wang",
      "Yuchao Lu",
      "Mengke Song",
      "Lei Liu",
      "Ling An",
      "Ying Liang",
      "Xingjie Ma",
      "Zhenyu Zhang",
      "Chichun Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12536",
    "title": "Debiasing Diffusion Model: Enhancing Fairness through Latent Representation Learning in Stable Diffusion Model",
    "abstract": "           Image generative models, particularly diffusion-based models, have surged in popularity due to their remarkable ability to synthesize highly realistic images. However, since these models are data-driven, they inherit biases from the training datasets, frequently leading to disproportionate group representations that exacerbate societal inequities. Traditionally, efforts to debiase these models have relied on predefined sensitive attributes, classifiers trained on such attributes, or large language models to steer outputs toward fairness. However, these approaches face notable drawbacks: predefined attributes do not adequately capture complex and continuous variations among groups. To address these issues, we introduce the Debiasing Diffusion Model (DDM), which leverages an indicator to learn latent representations during training, promoting fairness through balanced representations without requiring predefined sensitive attributes. This approach not only demonstrates its effectiveness in scenarios previously addressed by conventional techniques but also enhances fairness without relying on predefined sensitive attributes as conditions. In this paper, we discuss the limitations of prior bias mitigation techniques in diffusion-based models, elaborate on the architecture of the DDM, and validate the effectiveness of our approach through experiments.         ",
    "url": "https://arxiv.org/abs/2503.12536",
    "authors": [
      "Lin-Chun Huang",
      "Ching Chieh Tsao",
      "Fang-Yi Su",
      "Jung-Hsien Chiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2503.12538",
    "title": "EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots with Deep Reinforcement Learning",
    "abstract": "           This study presents an emotion-aware navigation framework -- EmoBipedNav -- using deep reinforcement learning (DRL) for bipedal robots walking in socially interactive environments. The inherent locomotion constraints of bipedal robots challenge their safe maneuvering capabilities in dynamic environments. When combined with the intricacies of social environments, including pedestrian interactions and social cues, such as emotions, these challenges become even more pronounced. To address these coupled problems, we propose a two-stage pipeline that considers both bipedal locomotion constraints and complex social environments. Specifically, social navigation scenarios are represented using sequential LiDAR grid maps (LGMs), from which we extract latent features, including collision regions, emotion-related discomfort zones, social interactions, and the spatio-temporal dynamics of evolving environments. The extracted features are directly mapped to the actions of reduced-order models (ROMs) through a DRL architecture. Furthermore, the proposed framework incorporates full-order dynamics and locomotion constraints during training, effectively accounting for tracking errors and restrictions of the locomotion controller while planning the trajectory with ROMs. Comprehensive experiments demonstrate that our approach exceeds both model-based planners and DRL-based baselines. The hardware videos and open-source code are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12538",
    "authors": [
      "Wei Zhu",
      "Abirath Raju",
      "Abdulaziz Shamsah",
      "Anqi Wu",
      "Seth Hutchinson",
      "Ye Zhao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12547",
    "title": "LLMSeR: Enhancing Sequential Recommendation via LLM-based Data Augmentation",
    "abstract": "           Sequential Recommender Systems (SRS) have become a cornerstone of online platforms, leveraging users' historical interaction data to forecast their next potential engagement. Despite their widespread adoption, SRS often grapple with the long-tail user dilemma, resulting in less effective recommendations for individuals with limited interaction records. The advent of Large Language Models (LLMs), with their profound capability to discern semantic relationships among items, has opened new avenues for enhancing SRS through data augmentation. Nonetheless, current methodologies encounter obstacles, including the absence of collaborative signals and the prevalence of hallucination this http URL this work, we present LLMSeR, an innovative framework that utilizes Large Language Models (LLMs) to generate pseudo-prior items, thereby improving the efficacy of Sequential Recommender Systems (SRS). To alleviate the challenge of insufficient collaborative signals, we introduce the Semantic Interaction Augmentor (SIA), a method that integrates both semantic and collaborative information to comprehensively augment user interaction data. Moreover, to weaken the adverse effects of hallucination in SRS, we develop the Adaptive Reliability Validation (ARV), a validation technique designed to assess the reliability of the generated pseudo items. Complementing these advancements, we also devise a Dual-Channel Training strategy, ensuring seamless integration of data augmentation into the SRS training this http URL experiments conducted with three widely-used SRS models demonstrate the generalizability and efficacy of LLMSeR.         ",
    "url": "https://arxiv.org/abs/2503.12547",
    "authors": [
      "Yuqi Sun",
      "Qidong Liu",
      "Haiping Zhu",
      "Feng Tian"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.12563",
    "title": "Diffusion on Graph: Augmentation of Graph Structure for Node Classification",
    "abstract": "           Graph diffusion models have recently been proposed to synthesize entire graphs, such as molecule graphs. Although existing methods have shown great performance in generating entire graphs for graph-level learning tasks, no graph diffusion models have been developed to generate synthetic graph structures, that is, synthetic nodes and associated edges within a given graph, for node-level learning tasks. Inspired by the research in the computer vision literature using synthetic data for enhanced performance, we propose Diffusion on Graph (DoG), which generates synthetic graph structures to boost the performance of GNNs. The synthetic graph structures generated by DoG are combined with the original graph to form an augmented graph for the training of node-level learning tasks, such as node classification and graph contrastive learning (GCL). To improve the efficiency of the generation process, a Bi-Level Neighbor Map Decoder (BLND) is introduced in DoG. To mitigate the adverse effect of the noise introduced by the synthetic graph structures, a low-rank regularization method is proposed for the training of graph neural networks (GNNs) on the augmented graphs. Extensive experiments on various graph datasets for semi-supervised node classification and graph contrastive learning have been conducted to demonstrate the effectiveness of DoG with low-rank regularization. The code of DoG is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12563",
    "authors": [
      "Yancheng Wang",
      "Changyu Liu",
      "Yingzhen Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12567",
    "title": "GAN-Based Single-Stage Defense for Traffic Sign Classification Under Adversarial Patch Attack",
    "abstract": "           Computer Vision plays a critical role in ensuring the safe navigation of autonomous vehicles (AVs). An AV perception module is responsible for capturing and interpreting the surrounding environment to facilitate safe navigation. This module enables AVs to recognize traffic signs, traffic lights, and various road users. However, the perception module is vulnerable to adversarial attacks, which can compromise their accuracy and reliability. One such attack is the adversarial patch attack (APA), a physical attack in which an adversary strategically places a specially crafted sticker on an object to deceive object classifiers. In APA, an adversarial patch is positioned on a target object, leading the classifier to misidentify it. Such an APA can cause AVs to misclassify traffic signs, leading to catastrophic incidents. To enhance the security of an AV perception system against APAs, this study develops a Generative Adversarial Network (GAN)-based single-stage defense strategy for traffic sign classification. This approach is tailored to defend against APAs on different classes of traffic signs without prior knowledge of a patch's design. This study found this approach to be effective against patches of varying sizes. Our experimental analysis demonstrates that the defense strategy presented in this paper improves the classifier's accuracy under APA conditions by up to 80.8% and enhances overall classification accuracy for all the traffic signs considered in this study by 58%, compared to a classifier without any defense mechanism. Our defense strategy is model-agnostic, making it applicable to any traffic sign classifier, regardless of the underlying classification model.         ",
    "url": "https://arxiv.org/abs/2503.12567",
    "authors": [
      "Abyad Enan",
      "Mashrur Chowdhury"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12608",
    "title": "UniBERTs: Adversarial Training for Language-Universal Representations",
    "abstract": "           This paper presents UniBERT, a compact multilingual language model that leverages an innovative training framework integrating three components: masked language modeling, adversarial training, and knowledge distillation. Pre-trained on a meticulously curated Wikipedia corpus spanning 107 languages, UniBERT is designed to reduce the computational demands of large-scale models while maintaining competitive performance across various natural language processing tasks. Comprehensive evaluations on four tasks -- named entity recognition, natural language inference, question answering, and semantic textual similarity -- demonstrate that our multilingual training strategy enhanced by an adversarial objective significantly improves cross-lingual generalization. Specifically, UniBERT models show an average relative improvement of 7.72% over traditional baselines, which achieved an average relative improvement of only 1.17%, with statistical analysis confirming the significance of these gains (p-value = 0.0181). This work highlights the benefits of combining adversarial training and knowledge distillation to build scalable and robust language models, thereby advancing the field of multilingual and cross-lingual natural language processing.         ",
    "url": "https://arxiv.org/abs/2503.12608",
    "authors": [
      "Andrei-Marius Avram",
      "Marian Lupa\u015fcu",
      "Dumitru-Clementin Cercel",
      "Ionu\u0163 Mironic\u0103",
      "\u015etefan Tr\u0103u\u015fan-Matu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.12623",
    "title": "MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network",
    "abstract": "           This paper introduces MAVEN (Multi-modal Attention for Valence-Arousal Emotion Network), a novel architecture for dynamic emotion recognition through dimensional modeling of affect. The model uniquely integrates visual, audio, and textual modalities via a bi-directional cross-modal attention mechanism with six distinct attention pathways, enabling comprehensive interactions between all modality pairs. Our proposed approach employs modality-specific encoders to extract rich feature representations from synchronized video frames, audio segments, and transcripts. The architecture's novelty lies in its cross-modal enhancement strategy, where each modality representation is refined through weighted attention from other modalities, followed by self-attention refinement through modality-specific encoders. Rather than directly predicting valence-arousal values, MAVEN predicts emotions in a polar coordinate form, aligning with psychological models of the emotion circumplex. Experimental evaluation on the Aff-Wild2 dataset demonstrates the effectiveness of our approach, with performance measured using Concordance Correlation Coefficient (CCC). The multi-stage architecture demonstrates superior ability to capture the complex, nuanced nature of emotional expressions in conversational videos, advancing the state-of-the-art (SOTA) in continuous emotion recognition in-the-wild. Code can be found at: this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12623",
    "authors": [
      "Vrushank Ahire",
      "Kunal Shah",
      "Mudasir Nazir Khan",
      "Nikhil Pakhale",
      "Lownish Rai Sookha",
      "M. A. Ganaie",
      "Abhinav Dhall"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2503.12625",
    "title": "SCOOP: CoSt-effective COngestiOn Attacks in Payment Channel Networks",
    "abstract": "           Payment channel networks (PCNs) are a promising solution to address blockchain scalability and throughput challenges, However, the security of PCNs and their vulnerability to attacks are not sufficiently studied. In this paper, we introduce SCOOP, a framework that includes two novel congestion attacks on PCNs. These attacks consider the minimum transferable amount along a path (path capacity) and the number of channels involved (path length), formulated as linear optimization problems. The first attack allocates the attacker's budget to achieve a specific congestion threshold, while the second maximizes congestion under budget constraints. Simulation results show the effectiveness of the proposed attack formulations in comparison to other attack strategies. Specifically, the results indicate that the first attack provides around a 40\\% improvement in congestion performance, while the second attack offers approximately a 50\\% improvement in comparison to the state-of-the-art. Moreover, in terms of payment to congestion efficiency, the first attack is about 60\\% more efficient, and the second attack is around 90\\% more efficient in comparison to state-of-the-art         ",
    "url": "https://arxiv.org/abs/2503.12625",
    "authors": [
      "Mohammed Ababneh",
      "Kartick Kolachala",
      "Roopa Vishwanathan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.12627",
    "title": "Online Misinformation Detection in Live Streaming Videos",
    "abstract": "           Online misinformation detection is an important issue and methods are proposed to detect and curb misinformation in various forms. However, previous studies are conducted in an offline manner. We claim a realistic misinformation detection setting that has not been studied yet is online misinformation detection in live streaming videos (MDLS). In the proposal, we formulate the problem of MDLS and illustrate the importance and the challenge of the task. Besides, we propose feasible ways of developing the problem into AI challenges as well as potential solutions to the problem.         ",
    "url": "https://arxiv.org/abs/2503.12627",
    "authors": [
      "Rui Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.12631",
    "title": "Omega-Regular Robustness",
    "abstract": "           Roughly speaking, a system is said to be robust if it can resist disturbances and still function correctly. For instance, if the requirement is that the temperature remains in an allowed range $[l,h]$, then a system that remains in a range $[l',h']\\subset[l,h]$ is more robust than one that reaches $l$ and $h$ from time to time. In this example the initial specification is quantitative in nature, this is not the case in $\\omega$-regular properties. Still, it seems there is a natural robustness preference relation induced by an $\\omega$-regular property. E.g. for a property requiring that every request is eventually granted, one would say that a system where requests are granted two ticks after they are issued is more robust than one in which requests are answered ninety ticks after they are issued. In this work we manage to distill a robustness preference relation that is induced by a given $\\omega$-regular language. The relation is a semantic notion (agnostic to the given representation of $L$) that satisfies some natural criteria.         ",
    "url": "https://arxiv.org/abs/2503.12631",
    "authors": [
      "Dana Fisman",
      "Elina Sudit"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ]
  },
  {
    "id": "arXiv:2503.12679",
    "title": "Discovering uncertainty: Gaussian constitutive neural networks with correlated weights",
    "abstract": "           When characterizing materials, it can be important to not only predict their mechanical properties, but also to estimate the probability distribution of these properties across a set of samples. Constitutive neural networks allow for the automated discovery of constitutive models that exactly satisfy physical laws given experimental testing data, but are only capable of predicting the mean stress response. Stochastic methods treat each weight as a random variable and are capable of learning their probability distributions. Bayesian constitutive neural networks combine both methods, but their weights lack physical interpretability and we must sample each weight from a probability distribution to train or evaluate the model. Here we introduce a more interpretable network with fewer parameters, simpler training, and the potential to discover correlated weights: Gaussian constitutive neural networks. We demonstrate the performance of our new Gaussian network on biaxial testing data, and discover a sparse and interpretable four-term model with correlated weights. Importantly, the discovered distributions of material parameters across a set of samples can serve as priors to discover better constitutive models for new samples with limited data. We anticipate that Gaussian constitutive neural networks are a natural first step towards generative constitutive models informed by physical laws and parameter uncertainty.         ",
    "url": "https://arxiv.org/abs/2503.12679",
    "authors": [
      "Jeremy A. McCulloch",
      "Ellen Kuhl"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12683",
    "title": "Algebraic Adversarial Attacks on Explainability Models",
    "abstract": "           Classical adversarial attacks are phrased as a constrained optimisation problem. Despite the efficacy of a constrained optimisation approach to adversarial attacks, one cannot trace how an adversarial point was generated. In this work, we propose an algebraic approach to adversarial attacks and study the conditions under which one can generate adversarial examples for post-hoc explainability models. Phrasing neural networks in the framework of geometric deep learning, algebraic adversarial attacks are constructed through analysis of the symmetry groups of neural networks. Algebraic adversarial examples provide a mathematically tractable approach to adversarial examples. We validate our approach of algebraic adversarial examples on two well-known and one real-world dataset.         ",
    "url": "https://arxiv.org/abs/2503.12683",
    "authors": [
      "Lachlan Simpson",
      "Federico Costanza",
      "Kyle Millar",
      "Adriel Cheng",
      "Cheng-Chew Lim",
      "Hong Gunn Chew"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Group Theory (math.GR)"
    ]
  },
  {
    "id": "arXiv:2503.12684",
    "title": "On Some Fundamental Problems for Multi-Agent Systems Over Multilayer Networks",
    "abstract": "           Many researchers have considered multi-agent systems over single-layer networks as models for studying diffusion phenomena. Since real-world networks involve connections between agents with different semantics (e.g., family member, friend, colleague), the study of multi-agent systems over multilayer networks has assumed importance. Our focus is on one class of multi-agent system models over multilayer networks, namely multilayer synchronous dynamical systems (MSyDSs). We study several fundamental problems for this model. We establish properties of the phase spaces of MSyDSs and bring out interesting differences between single-layer and multilayer dynamical systems. We show that, in general, the problem of determining whether two given MSyDSs are inequivalent is NP-complete. This hardness result holds even when the only difference between the two systems is the local function at just one node in one layer. We also present efficient algorithms for the equivalence problem for restricted versions of MSyDSs (e.g., systems where each local function is a bounded-threshold function, systems where the number of layers is fixed and each local function is symmetric). In addition, we investigate the expressive power of MSyDSs based on the number of layers. In particular, we examine conditions under which a system with k >= 2 layers has an equivalent system with k-1 or fewer layers.         ",
    "url": "https://arxiv.org/abs/2503.12684",
    "authors": [
      "Daniel J. Rosenkrantz",
      "Madhav V. Marathe",
      "Zirou Qiu",
      "S. S. Ravi",
      "Richard E. Stearns"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2503.12695",
    "title": "CDKFormer: Contextual Deviation Knowledge-Based Transformer for Long-Tail Trajectory Prediction",
    "abstract": "           Predicting the future movements of surrounding vehicles is essential for ensuring the safe operation and efficient navigation of autonomous vehicles (AVs) in urban traffic environments. Existing vehicle trajectory prediction methods primarily focus on improving overall performance, yet they struggle to address long-tail scenarios effectively. This limitation often leads to poor predictions in rare cases, significantly increasing the risk of safety incidents. Taking Argoverse 2 motion forecasting dataset as an example, we first investigate the long-tail characteristics in trajectory samples from two perspectives, individual motion and group interaction, and deriving deviation features to distinguish abnormal from regular scenarios. On this basis, we propose CDKFormer, a Contextual Deviation Knowledge-based Transformer model for long-tail trajectory prediction. CDKFormer integrates an attention-based scene context fusion module to encode spatiotemporal interaction and road topology. An additional deviation feature fusion module is proposed to capture the dynamic deviations in the target vehicle status. We further introduce a dual query-based decoder, supported by a multi-stream decoder block, to sequentially decode heterogeneous scene deviation features and generate multimodal trajectory predictions. Extensive experiments demonstrate that CDKFormer achieves state-of-the-art performance, significantly enhancing prediction accuracy and robustness for long-tailed trajectories compared to existing methods, thus advancing the reliability of AVs in complex real-world environments.         ",
    "url": "https://arxiv.org/abs/2503.12695",
    "authors": [
      "Yuansheng Lian",
      "Ke Zhang",
      "Meng Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.12717",
    "title": "Neural network-enhanced $hr$-adaptive finite element algorithm for parabolic equations",
    "abstract": "           In this paper, we present a novel enhancement to the conventional $hr$-adaptive finite element methods for parabolic equations, integrating traditional $h$-adaptive and $r$-adaptive methods via neural networks. A major challenge in $hr$-adaptive finite element methods lies in projecting the previous step's finite element solution onto the updated mesh. This projection depends on the new mesh and must be recomputed for each adaptive iteration. To address this, we introduce a neural network to construct a mesh-free surrogate of the previous step finite element solution. Since the neural network is mesh-free, it only requires training once per time step, with its parameters initialized using the optimizer from the previous time step. This approach effectively overcomes the interpolation challenges associated with non-nested meshes in computation, making node insertion and movement more convenient and efficient. The new algorithm also emphasizes SIZING and GENERATE, allowing each refinement to roughly double the number of mesh nodes of the previous iteration and then redistribute them to form a new mesh that effectively captures the singularities. It significantly reduces the time required for repeated refinement and achieves the desired accuracy in no more than seven space-adaptive iterations per time step. Numerical experiments confirm the efficiency of the proposed algorithm in capturing dynamic changes of singularities.         ",
    "url": "https://arxiv.org/abs/2503.12717",
    "authors": [
      "Jiaxiong Hao",
      "Yunqing Huang",
      "Nianyu Yi",
      "Peimeng Yin"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2503.12722",
    "title": "Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering",
    "abstract": "           As Large Language Models (LLMs) gain autonomous capabilities, their coordination in multi-agent settings becomes increasingly important. However, they often struggle with cooperation, leading to suboptimal outcomes. Inspired by Axelrod's Iterated Prisoner's Dilemma (IPD) tournaments, we explore how personality traits influence LLM cooperation. Using representation engineering, we steer Big Five traits (e.g., Agreeableness, Conscientiousness) in LLMs and analyze their impact on IPD decision-making. Our results show that higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation, highlighting both the potential and limitations of personality-based steering for aligning AI agents.         ",
    "url": "https://arxiv.org/abs/2503.12722",
    "authors": [
      "Kenneth J. K. Ong",
      "Lye Jia Jun",
      "Hieu Minh \"Jord\" Nguyen",
      "Seong Hah Cho",
      "Natalia P\u00e9rez-Campanero Antol\u00edn"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2503.12726",
    "title": "Indoor Fusion Positioning Based on \"IMU-Ultrasonic-UWB\" and Factor Graph Optimization Method",
    "abstract": "           This paper presents a high-precision positioning system that integrates ultra-wideband (UWB) time difference of arrival (TDoA) measurements, inertial measurement unit (IMU) data, and ultrasonic sensors through factor graph optimization. To overcome the shortcomings of standalone UWB systems in non-line-of-sight (NLOS) scenarios and the inherent drift associated with inertial navigation, we developed a novel hybrid fusion framework. First, a dynamic covariance estimation mechanism is incorporated, which automatically adjusts measurement weights based on real-time channel conditions. Then, a tightly-coupled sensor fusion architecture is employed, utilizing IMU pre-integration theory for temporal synchronization. Finally, a sliding-window factor graph optimization backend is utilized, incorporating NLOS mitigation constraints. Experimental results in complex indoor environments show a 38\\% improvement in positioning accuracy compared to conventional Kalman filter-based approaches, achieving a 12.3 cm root mean square (RMS) error under dynamic motion conditions. The system maintains robust performance even with intermittent UWB signal availability, down to a 40\\% packet reception rate, effectively suppressing IMU drift through multi-modal constraint fusion. This work offers a practical solution for applications that require reliable indoor positioning in GPS-denied environments.         ",
    "url": "https://arxiv.org/abs/2503.12726",
    "authors": [
      "Fengyun Zhang",
      "Jia Li",
      "Xiaoqing Zhang",
      "Shukai Duan",
      "Shuang-Hua Yang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.12744",
    "title": "Finite Samples for Shallow Neural Networks",
    "abstract": "           This paper investigates the ability of finite samples to identify two-layer irreducible shallow networks with various nonlinear activation functions, including rectified linear units (ReLU) and analytic functions such as the logistic sigmoid and hyperbolic tangent. An ``irreducible\" network is one whose function cannot be represented by another network with fewer neurons. For ReLU activation functions, we first establish necessary and sufficient conditions for determining the irreducibility of a network. Subsequently, we prove a negative result: finite samples are insufficient for definitive identification of any irreducible ReLU shallow network. Nevertheless, we demonstrate that for a given irreducible network, one can construct a finite set of sampling points that can distinguish it from other network with the same neuron count. Conversely, for logistic sigmoid and hyperbolic tangent activation functions, we provide a positive result. We construct finite samples that enable the recovery of two-layer irreducible shallow analytic networks. To the best of our knowledge, this is the first study to investigate the exact identification of two-layer irreducible networks using finite sample function values. Our findings provide insights into the comparative performance of networks with different activation functions under limited sampling conditions.         ",
    "url": "https://arxiv.org/abs/2503.12744",
    "authors": [
      "Yu Xia",
      "Zhiqiang Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2503.12755",
    "title": "Cohort-attention Evaluation Metric against Tied Data: Studying Performance of Classification Models in Cancer Detection",
    "abstract": "           Artificial intelligence (AI) has significantly improved medical screening accuracy, particularly in cancer detection and risk assessment. However, traditional classification metrics often fail to account for imbalanced data, varying performance across cohorts, and patient-level inconsistencies, leading to biased evaluations. We propose the Cohort-Attention Evaluation Metrics (CAT) framework to address these challenges. CAT introduces patient-level assessment, entropy-based distribution weighting, and cohort-weighted sensitivity and specificity. Key metrics like CATSensitivity (CATSen), CATSpecificity (CATSpe), and CATMean ensure balanced and fair evaluation across diverse populations. This approach enhances predictive reliability, fairness, and interpretability, providing a robust evaluation method for AI-driven medical screening models.         ",
    "url": "https://arxiv.org/abs/2503.12755",
    "authors": [
      "Longfei Wei",
      "Fang Sheng",
      "Jianfei Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.12783",
    "title": "Mixed-granularity Implicit Representation for Continuous Hyperspectral Compressive Reconstruction",
    "abstract": "           Hyperspectral Images (HSIs) are crucial across numerous fields but are hindered by the long acquisition times associated with traditional spectrometers. The Coded Aperture Snapshot Spectral Imaging (CASSI) system mitigates this issue through a compression technique that accelerates the acquisition process. However, reconstructing HSIs from compressed data presents challenges due to fixed spatial and spectral resolution constraints. This study introduces a novel method using implicit neural representation for continuous hyperspectral image reconstruction. We propose the Mixed Granularity Implicit Representation (MGIR) framework, which includes a Hierarchical Spectral-Spatial Implicit Encoder for efficient multi-scale implicit feature extraction. This is complemented by a Mixed-Granularity Local Feature Aggregator that adaptively integrates local features across scales, combined with a decoder that merges coordinate information for precise reconstruction. By leveraging implicit neural representations, the MGIR framework enables reconstruction at any desired spatial-spectral resolution, significantly enhancing the flexibility and adaptability of the CASSI system. Extensive experimental evaluations confirm that our model produces reconstructed images at arbitrary resolutions and matches state-of-the-art methods across varying spectral-spatial compression ratios. The code will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12783",
    "authors": [
      "Jianan Li",
      "Huan Chen",
      "Wangcai Zhao",
      "Rui Chen",
      "Tingfa Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2503.12793",
    "title": "Improving Generalization of Universal Adversarial Perturbation via Dynamic Maximin Optimization",
    "abstract": "           Deep neural networks (DNNs) are susceptible to universal adversarial perturbations (UAPs). These perturbations are meticulously designed to fool the target model universally across all sample classes. Unlike instance-specific adversarial examples (AEs), generating UAPs is more complex because they must be generalized across a wide range of data samples and models. Our research reveals that existing universal attack methods, which optimize UAPs using DNNs with static model parameter snapshots, do not fully leverage the potential of DNNs to generate more effective UAPs. Rather than optimizing UAPs against static DNN models with a fixed training set, we suggest using dynamic model-data pairs to generate UAPs. In particular, we introduce a dynamic maximin optimization strategy, aiming to optimize the UAP across a variety of optimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an iterative max-min-min optimization framework that refines the model-data pairs, coupled with a curriculum UAP learning algorithm to examine the combined space of model parameters and data thoroughly. Comprehensive experiments on the ImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both cross-sample universality and cross-model transferability of UAPs. Using only 500 samples for UAP generation, DM-UAP outperforms the state-of-the-art approach with an average increase in fooling ratio of 12.108%.         ",
    "url": "https://arxiv.org/abs/2503.12793",
    "authors": [
      "Yechao Zhang",
      "Yingzhe Xu",
      "Junyu Shi",
      "Leo Yu Zhang",
      "Shengshan Hu",
      "Minghui Li",
      "Yanjun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12800",
    "title": "Pairwise Similarity Regularization for Semi-supervised Graph Medical Image Segmentation",
    "abstract": "           With fully leveraging the value of unlabeled data, semi-supervised medical image segmentation algorithms significantly reduces the limitation of limited labeled data, achieving a significant improvement in accuracy. However, the distributional shift between labeled and unlabeled data weakens the utilization of information from the labeled data. To alleviate the problem, we propose a graph network feature alignment method based on pairwise similarity regularization (PaSR) for semi-supervised medical image segmentation. PaSR aligns the graph structure of images in different domains by maintaining consistency in the pairwise structural similarity of feature graphs between the target domain and the source domain, reducing distribution shift issues in medical images. Meanwhile, further improving the accuracy of pseudo-labels in the teacher network by aligning graph clustering information to enhance the semi-supervised efficiency of the model. The experimental part was verified on three medical image segmentation benchmark datasets, with results showing improvements over advanced methods in various metrics. On the ACDC dataset, it achieved an average improvement of more than 10.66%.         ",
    "url": "https://arxiv.org/abs/2503.12800",
    "authors": [
      "Jialu Zhou",
      "Dianxi Shi",
      "Shaowu Yang",
      "Chunping Qiu",
      "Luoxi Jing",
      "Mengzhu Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12801",
    "title": "BLIA: Detect model memorization in binary classification model through passive Label Inference attack",
    "abstract": "           Model memorization has implications for both the generalization capacity of machine learning models and the privacy of their training data. This paper investigates label memorization in binary classification models through two novel passive label inference attacks (BLIA). These attacks operate passively, relying solely on the outputs of pre-trained models, such as confidence scores and log-loss values, without interacting with or modifying the training process. By intentionally flipping 50% of the labels in controlled subsets, termed \"canaries,\" we evaluate the extent of label memorization under two conditions: models trained without label differential privacy (Label-DP) and those trained with randomized response-based Label-DP. Despite the application of varying degrees of Label-DP, the proposed attacks consistently achieve success rates exceeding 50%, surpassing the baseline of random guessing and conclusively demonstrating that models memorize training labels, even when these labels are deliberately uncorrelated with the features.         ",
    "url": "https://arxiv.org/abs/2503.12801",
    "authors": [
      "Mohammad Wahiduzzaman Khan",
      "Sheng Chen",
      "Ilya Mironov",
      "Leizhen Zhang",
      "Rabib Noor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.12803",
    "title": "Leveraging Deep Neural Networks for Aspect-Based Sentiment Classification",
    "abstract": "           Aspect-based sentiment analysis seeks to determine sentiment with a high level of detail. While graph convolutional networks (GCNs) are commonly used for extracting sentiment features, their straightforward use in syntactic feature extraction can lead to a loss of crucial information. This paper presents a novel edge-enhanced GCN, called EEGCN, which improves performance by preserving feature integrity as it processes syntactic graphs. We incorporate a bidirectional long short-term memory (Bi-LSTM) network alongside a self-attention-based transformer for effective text encoding, ensuring the retention of long-range dependencies. A bidirectional GCN (Bi-GCN) with message passing then captures the relationships between entities, while an aspect-specific masking technique removes extraneous information. Extensive evaluations and ablation studies on four benchmark datasets show that EEGCN significantly enhances aspect-based sentiment analysis, overcoming issues with syntactic feature extraction and advancing the field's methodologies.         ",
    "url": "https://arxiv.org/abs/2503.12803",
    "authors": [
      "Chen Li",
      "Debo Cheng",
      "Yasuhiko Morimoto"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12810",
    "title": "Layered Nonlinear Model Predictive Control for Robust Stabilization of Hybrid Systems",
    "abstract": "           Computing the receding horizon optimal control of nonlinear hybrid systems is typically prohibitively slow, limiting real-time implementation. To address this challenge, we propose a layered Model Predictive Control (MPC) architecture for robust stabilization of hybrid systems. A high level \"hybrid\" MPC is solved at a slow rate to produce a stabilizing hybrid trajectory, potentially sub-optimally, including a domain and guard sequence. This domain and guard sequence is passed to a low level \"fixed mode\" MPC which is a traditional, time-varying, state-constrained MPC that can be solved rapidly, e.g., using nonlinear programming (NLP) tools. A robust version of the fixed mode MPC is constructed by using tracking error tubes that are not guaranteed to have finite size for all time. Using these tubes, we demonstrate that the speed at which the fixed mode MPC is re-calculated is directly tied to the robustness of the system, thereby justifying the layered approach. Finally, simulation examples of a five link bipedal robot and a controlled nonlinear bouncing ball are used to illustrate the formal results.         ",
    "url": "https://arxiv.org/abs/2503.12810",
    "authors": [
      "Zachary Olkin",
      "Aaron D. Ames"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.12811",
    "title": "A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules",
    "abstract": "           Training large models is both resource-intensive and time-consuming, making it crucial to understand the quantitative relationship between model performance and hyperparameters. In this paper, we present an empirical law that describes how the pretraining loss of large language models evolves under different learning rate schedules, such as constant, cosine, and step decay schedules. Our proposed law takes a multi-power form, combining a power law based on the sum of learning rates and additional power laws to account for a loss reduction effect induced by learning rate decay. We extensively validate this law on various model sizes and architectures, and demonstrate that after fitting on a few learning rate schedules, the law accurately predicts the loss curves for unseen schedules of different shapes and horizons. Moreover, by minimizing the predicted final pretraining loss across learning rate schedules, we are able to find a schedule that outperforms the widely used cosine learning rate schedule. Interestingly, this automatically discovered schedule bears some resemblance to the recently proposed Warmup-Stable-Decay (WSD) schedule (Hu et al, 2024) but achieves a slightly lower final loss. We believe these results could offer valuable insights for understanding the dynamics of pretraining and designing learning rate schedules to improve efficiency.         ",
    "url": "https://arxiv.org/abs/2503.12811",
    "authors": [
      "Kairong Luo",
      "Haodong Wen",
      "Shengding Hu",
      "Zhenbo Sun",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Kaifeng Lyu",
      "Wenguang Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.12814",
    "title": "Versatile Physics-based Character Control with Hybrid Latent Representation",
    "abstract": "           We present a versatile latent representation that enables physically simulated character to efficiently utilize motion priors. To build a powerful motion embedding that is shared across multiple tasks, the physics controller should employ rich latent space that is easily explored and capable of generating high-quality motion. We propose integrating continuous and discrete latent representations to build a versatile motion prior that can be adapted to a wide range of challenging control tasks. Specifically, we build a discrete latent model to capture distinctive posterior distribution without collapse, and simultaneously augment the sampled vector with the continuous residuals to generate high-quality, smooth motion without jittering. We further incorporate Residual Vector Quantization, which not only maximizes the capacity of the discrete motion prior, but also efficiently abstracts the action space during the task learning phase. We demonstrate that our agent can produce diverse yet smooth motions simply by traversing the learned motion prior through unconditional motion generation. Furthermore, our model robustly satisfies sparse goal conditions with highly expressive natural motions, including head-mounted device tracking and motion in-betweening at irregular intervals, which could not be achieved with existing latent representations.         ",
    "url": "https://arxiv.org/abs/2503.12814",
    "authors": [
      "Jinseok Bae",
      "Jungdam Won",
      "Donggeun Lim",
      "Inwoo Hwang",
      "Young Min Kim"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.12821",
    "title": "From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration",
    "abstract": "           Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data distribution is highly imbalanced. Previous works have mainly focused on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as recognition and classification. Nevertheless, the exploration of LVLM (e.g. LLaVA) and more general tasks (e.g. Visual Question Answering and Visual Reasoning) remains under-explored. In this paper, we first conduct an in-depth analysis of the LT issues in LVLMs and identify two core causes: the overrepresentation of head concepts and the underrepresentation of tail concepts. Based on the above observation, we propose an $\\textbf{A}$daptive $\\textbf{D}$ata $\\textbf{R}$efinement Framework ($\\textbf{ADR}$), which consists of two stages: $\\textbf{D}$ata $\\textbf{R}$ebalancing ($\\textbf{DR}$) and $\\textbf{D}$ata $\\textbf{S}$ynthesis ($\\textbf{DS}$). In the DR stage, we adaptively rebalance the redundant data based on entity distributions, while in the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and scarce images to supplement underrepresented portions. Through comprehensive evaluations across eleven benchmarks, our proposed ADR effectively mitigates the long-tail problem in the training data, improving the average performance of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.         ",
    "url": "https://arxiv.org/abs/2503.12821",
    "authors": [
      "Mingyang Song",
      "Xiaoye Qu",
      "Jiawei Zhou",
      "Yu Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12827",
    "title": "GSBAK$^K$: $top$-$K$ Geometric Score-based Black-box Attack",
    "abstract": "           Existing score-based adversarial attacks mainly focus on crafting $top$-1 adversarial examples against classifiers with single-label classification. Their attack success rate and query efficiency are often less than satisfactory, particularly under small perturbation requirements; moreover, the vulnerability of classifiers with multi-label learning is yet to be studied. In this paper, we propose a comprehensive surrogate free score-based attack, named \\b geometric \\b score-based \\b black-box \\b attack (GSBAK$^K$), to craft adversarial examples in an aggressive $top$-$K$ setting for both untargeted and targeted attacks, where the goal is to change the $top$-$K$ predictions of the target classifier. We introduce novel gradient-based methods to find a good initial boundary point to attack. Our iterative method employs novel gradient estimation techniques, particularly effective in $top$-$K$ setting, on the decision boundary to effectively exploit the geometry of the decision boundary. Additionally, GSBAK$^K$ can be used to attack against classifiers with $top$-$K$ multi-label learning. Extensive experimental results on ImageNet and PASCAL VOC datasets validate the effectiveness of GSBAK$^K$ in crafting $top$-$K$ adversarial examples.         ",
    "url": "https://arxiv.org/abs/2503.12827",
    "authors": [
      "Md Farhamdur Reza",
      "Richeng Jin",
      "Tianfu Wu",
      "Huaiyu Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12829",
    "title": "SparseLUT: Sparse Connectivity Optimization for Lookup Table-based Deep Neural Networks",
    "abstract": "           The deployment of deep neural networks (DNNs) on resource-constrained edge devices such as field-programmable gate arrays (FPGAs) requires a careful balance of latency, power, and resource usage while maintaining high accuracy. Existing Lookup Table (LUT)-based DNNs, including LogicNets, PolyLUT, PolyLUT-Add, and NeuraLUT, exploit native FPGA resources with random sparse connectivity. This paper introduces SparseLUT, a connectivity-centric training technique tailored for LUT-based DNNs. SparseLUT leverages a non-greedy training strategy that prioritizes the pruning of less significant connections and strategically regrows alternative ones, resulting in efficient convergence to the target sparsity. Experimental results show consistent accuracy improvements across benchmarks, including up to a 2.13\\% increase on MNIST and a 0.94\\% improvement for Jet Substructure Classification compared to random sparsity. This is done without any hardware overhead and achieves state-of-the-art results for LUT-based DNNs.         ",
    "url": "https://arxiv.org/abs/2503.12829",
    "authors": [
      "Binglei Lou",
      "Ruilin Wu",
      "Philip Leong"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12836",
    "title": "CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting",
    "abstract": "           3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D reconstruction and novel view synthesis, leading to its widespread commercial use. Consequently, copyright protection via watermarking has become critical. However, because 3DGS relies on millions of Gaussians, which require gigabytes of storage, efficient transfer and storage require compression. Existing 3DGS watermarking methods are vulnerable to quantization-based compression, often resulting in the loss of the embedded watermark. To address this challenge, we propose a novel watermarking method that ensures watermark robustness after model compression while maintaining high rendering quality. In detail, we incorporate a quantization distortion layer that simulates compression during training, preserving the watermark under quantization-based compression. Also, we propose a learnable watermark embedding feature that embeds the watermark into the anchor feature, ensuring structural consistency and seamless integration into the 3D scene. Furthermore, we present a frequency-aware anchor growing mechanism to enhance image quality in high-frequency regions by effectively identifying Guassians within these regions. Experimental results confirm that our method preserves the watermark and maintains superior image quality under high compression, validating it as a promising approach for a secure 3DGS model.         ",
    "url": "https://arxiv.org/abs/2503.12836",
    "authors": [
      "Sumin In",
      "Youngdong Jang",
      "Utae Jeong",
      "MinHyuk Jang",
      "Hyeongcheol Park",
      "Eunbyung Park",
      "Sangpil Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12847",
    "title": "Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment",
    "abstract": "           Accurately localizing audible objects based on audio-visual cues is the core objective of audio-visual segmentation. Most previous methods emphasize spatial or temporal multi-modal modeling, yet overlook challenges from ambiguous audio-visual correspondences such as nearby visually similar but acoustically different objects and frequent shifts in objects' sounding status. Consequently, they may struggle to reliably correlate audio and visual cues, leading to over- or under-segmentation. To address these limitations, we propose a novel framework with two primary components: an audio-guided modality alignment (AMA) module and an uncertainty estimation (UE) module. Instead of indiscriminately correlating audio-visual cues through a global attention mechanism, AMA performs audio-visual interactions within multiple groups and consolidates group features into compact representations based on their responsiveness to audio cues, effectively directing the model's attention to audio-relevant areas. Leveraging contrastive learning, AMA further distinguishes sounding regions from silent areas by treating features with strong audio responses as positive samples and weaker responses as negatives. Additionally, UE integrates spatial and temporal information to identify high-uncertainty regions caused by frequent changes in sound state, reducing prediction errors by lowering confidence in these areas. Experimental results demonstrate that our approach achieves superior accuracy compared to existing state-of-the-art methods, particularly in challenging scenarios where traditional approaches struggle to maintain reliable segmentation.         ",
    "url": "https://arxiv.org/abs/2503.12847",
    "authors": [
      "Chen Liu",
      "Peike Li",
      "Liying Yang",
      "Dadong Wang",
      "Lincheng Li",
      "Xin Yu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12852",
    "title": "ACT360: An Efficient 360-Degree Action Detection and Summarization Framework for Mission-Critical Training and Debriefing",
    "abstract": "           Effective training and debriefing are critical in high-stakes, mission-critical environments such as disaster response, military simulations, and industrial safety, where precision and minimizing errors are paramount. The traditional post-training analysis relies on manually reviewing 2D videos, a time-consuming process that lacks comprehensive situational awareness. To address these limitations, we introduce ACT360, a system that leverages 360-degree videos and machine learning for automated action detection and structured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch Once (YOWO) model with spatial attention and equirectangular-aware convolution (EAC) to mitigate panoramic video distortions. To enable deployment in resource-constrained environments, we apply quantization and model pruning, reducing the model size by 74% while maintaining robust accuracy (mAP drop of only 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our approach on a publicly available dataset of 55 labeled 360-degree videos covering seven key operational actions, recorded across various real-world training sessions and environmental conditions. Additionally, ACT360 integrates 360AIE (Action Insight Explorer), a web-based interface for automatic action detection, retrieval, and textual summarization using large language models (LLMs), significantly enhancing post-incident analysis efficiency. ACT360 serves as a generalized framework for mission-critical debriefing, incorporating EAC, spatial attention, summarization, and model optimization. These innovations apply to any training environment requiring lightweight action detection and structured post-exercise analysis.         ",
    "url": "https://arxiv.org/abs/2503.12852",
    "authors": [
      "Aditi Tiwari",
      "Klara Nahrstedt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2503.12856",
    "title": "Island-Based Evolutionary Computation with Diverse Surrogates and Adaptive Knowledge Transfer for High-Dimensional Data-Driven Optimization",
    "abstract": "           In recent years, there has been a growing interest in data-driven evolutionary algorithms (DDEAs) employing surrogate models to approximate the objective functions with limited data. However, current DDEAs are primarily designed for lower-dimensional problems and their performance drops significantly when applied to large-scale optimization problems (LSOPs). To address the challenge, this paper proposes an offline DDEA named DSKT-DDEA. DSKT-DDEA leverages multiple islands that utilize different data to establish diverse surrogate models, fostering diverse subpopulations and mitigating the risk of premature convergence. In the intra-island optimization phase, a semi-supervised learning method is devised to fine-tune the surrogates. It not only facilitates data argumentation, but also incorporates the distribution information gathered during the search process to align the surrogates with the evolving local landscapes. Then, in the inter-island knowledge transfer phase, the algorithm incorporates an adaptive strategy that periodically transfers individual information and evaluates the transfer effectiveness in the new environment, facilitating global optimization efficacy. Experimental results demonstrate that our algorithm is competitive with state-of-the-art DDEAs on problems with up to 1000 dimensions, while also exhibiting decent parallelism and scalability. Our DSKT-DDEA is open-source and accessible at: this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12856",
    "authors": [
      "Xian-Rong Zhang",
      "Yue-Jiao Gong",
      "Zhiguang Cao",
      "Jun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.12864",
    "title": "Robust Co-Optimization of Distribution Network Hardening and Mobile Resource Scheduling with Decision-Dependent Uncertainty",
    "abstract": "           This paper studies the robust co-planning of proactive network hardening and mobile hydrogen energy resources (MHERs) scheduling, which is to enhance the resilience of power distribution network (PDN) against the disastrous events. A decision-dependent robust optimization model is formulated with min-max resilience constraint and discrete recourse structure, which helps achieve the load survivability target considering endogenous uncertainties. Different from the traditional model with a fixed uncertainty set, we adopt a dynamic representation that explicitly captures the endogenous uncertainties of network contingency as well as the available hydrogen storage levels of MHERs, which induces a decision-dependent uncertainty (DDU) set. Also, the multi-period adaptive routing and energy scheduling of MHERs are modeled as a mixed-integer recourse problem for further decreasing the resilience cost. Then, a nested parametric column-and-constraint generation (N-PC&CG) algorithm is customized and developed to solve this challenging formulation. By leveraging the structural property of the DDU set as well as the combination of discrete recourse decisions and the corresponding extreme points, we derive a strengthened solution scheme with nontrivial enhancement strategies to realize efficient and exact computation. Numerical results on 14-bus test system and 56-bus real-world distribution network demonstrate the resilience benefits and economical feasibility of the proposed method under different damage severity levels. Moreover, the enhanced N-PC&CG shows a superior solution capability to support prompt decisions for resilient planning with DDU models.         ",
    "url": "https://arxiv.org/abs/2503.12864",
    "authors": [
      "Donglai Ma",
      "Xiaoyu Cao",
      "Bo Zeng",
      "Chen Chen",
      "Qiaozhu Zhai",
      "Qing-Shan Jia",
      "Xiaohong Guan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.12871",
    "title": "A Reference Architecture for Autonomous Networks An Agent-Based Approach",
    "abstract": "           The vision of autonomous systems is becoming increasingly important in many application areas, where the aim is to replace humans with agents. These include autonomous vehicles and other agents' applications in business processes and problem-solving. For networks, the increasing scale and operation and management (O&M) complexity drive the need for autonomous networks (AN). The technical objective of AN is to ensure trustworthy O&M without human intervention for higher efficiency and lower operating costs. However, realizing AN seems more difficult than autonomous vehicles. It encounters challenges of networks' structural and functional complexity, which operate as distributed dynamic systems governed by various technical and economic constraints. A key problem lies in formulating a rigorous development methodology that facilitates a seamless transition from traditional networks to AN. Central to this methodology is the definition of a reference architecture for network agents, which specifies the required functionalities for their realization, regardless of implementation choices. This article proposes a reference architecture characterizing main functional features, illustrating its application with network use cases. It shows how artificial intelligence components can be used to implement the required functionality and its coordination. The latter is achieved through the management and generation of shared domain-specific knowledge stored in long-term memory, ensuring the overall consistency of decisions and their execution. The article concludes with a discussion of architecture specialization for building network layer agents. It also identifies the main technical challenges ahead, such as satisfying essential requirements at development or runtime, as well as the issue of coordinating agents to achieve collective intelligence in meeting overall network goals.         ",
    "url": "https://arxiv.org/abs/2503.12871",
    "authors": [
      "Joseph Sifakis",
      "Dongming Li",
      "Hairong Huang",
      "Yong Zhang",
      "Wenshuan Dang",
      "River Huang",
      "Yijun Yu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.12874",
    "title": "Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models",
    "abstract": "           Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate impressive generalization but remain highly vulnerable to adversarial examples (AEs). Previous work has explored robust text prompts through adversarial training, achieving some improvement in both robustness and generalization. However, they primarily rely on singlegradient direction perturbations (e.g., PGD) to generate AEs, which lack diversity, resulting in limited improvement in adversarial robustness. To address these limitations, we propose an evolution-based region adversarial prompt tuning method called ER-APT, which combines gradient methods with genetic evolution to generate more diverse and challenging AEs. In each training iteration, we first generate AEs using traditional gradient-based methods. Subsequently, a genetic evolution mechanism incorporating selection, mutation, and crossover is applied to optimize the AEs, ensuring a broader and more aggressive perturbation this http URL final evolved AEs are used for prompt tuning, achieving region-based adversarial optimization instead of conventional single-point adversarial prompt tuning. We also propose a dynamic loss weighting method to adjust prompt learning efficiency for accuracy and robustness. Experimental evaluations on various benchmark datasets demonstrate the superiority of our proposed method, outperforming stateof-the-art APT methods. The code is released at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12874",
    "authors": [
      "Xiaojun Jia",
      "Sensen Gao",
      "Simeng Qin",
      "Ke Ma",
      "Xinfeng Li",
      "Yihao Huang",
      "Wei Dong",
      "Yang Liu",
      "Xiaochun Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12878",
    "title": "Enable Time-Sensitive Applications in Kubernetes with Container Network Interface Plugin Agnostic Metadata Proxy",
    "abstract": "           Application deployment in cloud environment is dominated by Kubernetes-orchestrated microservices. Provides a secure environment, networking, storage, isolation, scheduling, and many other abstractions that can be easily extended to meet our needs. Time-Sensitive Applications (TSAs) have special requirements for compute and network. Deploying TSAs in Kubernetes is challenging because the networking implemented by Container Network Interface (CNI) plugins is not aware of the traffic characteristic required by Time-Sensitive Network. Even if a network interface supports TSN features (e.g.: Scheduled Traffic) and a modified CNI plugin is aware of this interface, the pod network isolation built on top of Linux deletes the metadata required for TSN protocols to work with. We propose TSN metadata proxy, a simple architecture that allows any TSA microservice to use the TSN capabilities of the physical NIC, without any modification. This architecture is tightly integrated with the Kubernetes networking model, works with popular CNI plugins, and supports services such as ClusterIP, NodePort, or LoadBalancer without additional configuration. Unlike former proposals, this architecture does not require either bypassing the Linux kernel network stack, direct access to the physical NIC, escalated privileges for the TSA microservice, or even modification of the TSA.         ",
    "url": "https://arxiv.org/abs/2503.12878",
    "authors": [
      "Ferenc Orosi",
      "Ferenc Fejes"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.12883",
    "title": "Early Detection of Forest Calamities in Homogeneous Stands -- Deep Learning Applied to Bark-Beetle Outbreaks",
    "abstract": "           Climate change has increased the vulnerability of forests to insect-related damage, resulting in widespread forest loss in Central Europe and highlighting the need for effective, continuous monitoring systems. Remote sensing based forest health monitoring, oftentimes, relies on supervised machine learning algorithms that require labeled training data. Monitoring temporal patterns through time series analysis offers a potential alternative for earlier detection of disturbance but requires substantial storage resources. This study investigates the potential of a Deep Learning algorithm based on a Long Short Term Memory (LSTM) Autoencoder for the detection of anomalies in forest health (e.g. bark beetle outbreaks), utilizing Sentinel-2 time series data. This approach is an alternative to supervised machine learning methods, avoiding the necessity for labeled training data. Furthermore, it is more memory-efficient than other time series analysis approaches, as a robust model can be created using only a 26-week-long time series as input. In this study, we monitored pure stands of spruce in Thuringia, Germany, over a 7-year period from 2018 to the end of 2024. Our best model achieved a detection accuracy of 87% on test data and was able to detect 61% of all anomalies at a very early stage (more than a month before visible signs of forest degradation). Compared to another widely used time series break detection algorithm - BFAST (Breaks For Additive Season and Trend), our approach consistently detected higher percentage of anomalies at an earlier stage. These findings suggest that LSTM-based Autoencoders could provide a promising, resource-efficient approach to forest health monitoring, enabling more timely responses to emerging threats.         ",
    "url": "https://arxiv.org/abs/2503.12883",
    "authors": [
      "Maximilian Kirsch",
      "Jakob Wernicke",
      "Pawan Datta",
      "Christine Preisach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12888",
    "title": "UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware Prototype Memory Network",
    "abstract": "           Transformer-based trackers have achieved promising success and become the dominant tracking paradigm due to their accuracy and efficiency. Despite the substantial progress, most of the existing approaches tackle object tracking as a deterministic coordinate regression problem, while the target localization uncertainty has been greatly overlooked, which hampers trackers' ability to maintain reliable target state prediction in challenging scenarios. To address this issue, we propose UncTrack, a novel uncertainty-aware transformer tracker that predicts the target localization uncertainty and incorporates this uncertainty information for accurate target state inference. Specifically, UncTrack utilizes a transformer encoder to perform feature interaction between template and search images. The output features are passed into an uncertainty-aware localization decoder (ULD) to coarsely predict the corner-based localization and the corresponding localization uncertainty. Then the localization uncertainty is sent into a prototype memory network (PMN) to excavate valuable historical information to identify whether the target state prediction is reliable or not. To enhance the template representation, the samples with high confidence are fed back into the prototype memory bank for memory updating, making the tracker more robust to challenging appearance variations. Extensive experiments demonstrate that our method outperforms other state-of-the-art methods. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12888",
    "authors": [
      "Siyuan Yao",
      "Yang Guo",
      "Yanyang Yan",
      "Wenqi Ren",
      "Xiaochun Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12899",
    "title": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on Code Generation",
    "abstract": "           Language Models (LMs) are widely used in software engineering for code generation, but they may produce code with errors. Rather than repairing the generated code, an alternative way is to address the underlying failures of models. LM repair offers a lightweight solution to this challenge: it requires minimal data, reduces computational costs, and reduces the side effects. Unlike retraining, LM repair focuses on applying tailored updates to targeted neurons, making it ideal for scenarios with limited resources, high-performance demands, or strict safety requirements. In this paper, we propose \\ul{S}emantic \\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering and novel semantic-based optimization approach for repairing LLMs. \\textsc{STAR} realizes main operations in LM repair methods in an optimization process, including locating ``buggy neurons'', solving ``neuron patches'', and patching ``buggy neurons''. Correspondingly, it computes the deltas of weight matrix as the prior information to guide optimization; and attributes the targeted layers and neurons leveraging statistical insights. The neuron patches are computed with a solid semantic-based analytical formula, which directly bridges the changes to logits with the deltas of neurons, by steering latent representations. Compared to the prior work of LM repair (\\textsc{MINT}) and optimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths while mitigating their limitations. \\textsc{STAR} supports solving multiple failures together, significantly improving the usefulness. Evaluated on three code generation tasks using popular code LMs, \\textsc{STAR} demonstrates superior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency. In terms of side effects, namely the balance between generalization and specificity, \\textsc{STAR} outperforms prior work by a significant margin.         ",
    "url": "https://arxiv.org/abs/2503.12899",
    "authors": [
      "Jian Gu",
      "Aldeida Aleti",
      "Chunyang Chen",
      "Hongyu Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12905",
    "title": "UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks",
    "abstract": "           Video anomaly detection plays a significant role in intelligent surveillance systems. To enhance model's anomaly recognition ability, previous works have typically involved RGB, optical flow, and text features. Recently, dynamic vision sensors (DVS) have emerged as a promising technology, which capture visual information as discrete events with a very high dynamic range and temporal resolution. It reduces data redundancy and enhances the capture capacity of moving objects compared to conventional camera. To introduce this rich dynamic information into the surveillance field, we created the first DVS video anomaly detection benchmark, namely UCF-Crime-DVS. To fully utilize this new data modality, a multi-scale spiking fusion network (MSF) is designed based on spiking neural networks (SNNs). This work explores the potential application of dynamic information from event data in video anomaly detection. Our experiments demonstrate the effectiveness of our framework on UCF-Crime-DVS and its superior performance compared to other models, establishing a new baseline for SNN-based weakly supervised video anomaly detection.         ",
    "url": "https://arxiv.org/abs/2503.12905",
    "authors": [
      "Yuanbin Qian",
      "Shuhan Ye",
      "Chong Wang",
      "Xiaojie Cai",
      "Jiangbo Qian",
      "Jiafei Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.12910",
    "title": "MFP-CLIP: Exploring the Efficacy of Multi-Form Prompts for Zero-Shot Industrial Anomaly Detection",
    "abstract": "           Recently, zero-shot anomaly detection (ZSAD) has emerged as a pivotal paradigm for identifying defects in unseen categories without requiring target samples in training phase. However, existing ZSAD methods struggle with the boundary of small and complex defects due to insufficient representations. Most of them use the single manually designed prompts, failing to work for diverse objects and anomalies. In this paper, we propose MFP-CLIP, a novel prompt-based CLIP framework which explores the efficacy of multi-form prompts for zero-shot industrial anomaly detection. We employ an image to text prompting(I2TP) mechanism to better represent the object in the image. MFP-CLIP enhances perception to multi-scale and complex anomalies by self prompting(SP) and a multi-patch feature aggregation(MPFA) module. To precisely localize defects, we introduce the mask prompting(MP) module to guide model to focus on potential anomaly regions. Extensive experiments are conducted on two wildly used industrial anomaly detection benchmarks, MVTecAD and VisA, demonstrating MFP-CLIP's superiority in ZSAD.         ",
    "url": "https://arxiv.org/abs/2503.12910",
    "authors": [
      "Jingyi Yuan",
      "Pengyu Jie",
      "Junyin Zhang",
      "Ziao Li",
      "Chenqiang Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12912",
    "title": "Pose as a Modality: A Psychology-Inspired Network for Personality Recognition with a New Multimodal Dataset",
    "abstract": "           In recent years, predicting Big Five personality traits from multimodal data has received significant attention in artificial intelligence (AI). However, existing computational models often fail to achieve satisfactory performance. Psychological research has shown a strong correlation between pose and personality traits, yet previous research has largely ignored pose data in computational models. To address this gap, we develop a novel multimodal dataset that incorporates full-body pose data. The dataset includes video recordings of 287 participants completing a virtual interview with 36 questions, along with self-reported Big Five personality scores as labels. To effectively utilize this multimodal data, we introduce the Psychology-Inspired Network (PINet), which consists of three key modules: Multimodal Feature Awareness (MFA), Multimodal Feature Interaction (MFI), and Psychology-Informed Modality Correlation Loss (PIMC Loss). The MFA module leverages the Vision Mamba Block to capture comprehensive visual features related to personality, while the MFI module efficiently fuses the multimodal features. The PIMC Loss, grounded in psychological theory, guides the model to emphasize different modalities for different personality dimensions. Experimental results show that the PINet outperforms several state-of-the-art baseline models. Furthermore, the three modules of PINet contribute almost equally to the model's overall performance. Incorporating pose data significantly enhances the model's performance, with the pose modality ranking mid-level in importance among the five modalities. These findings address the existing gap in personality-related datasets that lack full-body pose data and provide a new approach for improving the accuracy of personality prediction models, highlighting the importance of integrating psychological insights into AI frameworks.         ",
    "url": "https://arxiv.org/abs/2503.12912",
    "authors": [
      "Bin Tang",
      "Keqi Pan",
      "Miao Zheng",
      "Ning Zhou",
      "Jialu Sui",
      "Dandan Zhu",
      "Cheng-Long Deng",
      "Shu-Guang Kuai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12919",
    "title": "COSMOS: Continuous Simplicial Neural Networks",
    "abstract": "           Simplicial complexes provide a powerful framework for modeling high-order interactions in structured data, making them particularly suitable for applications such as trajectory prediction and mesh processing. However, existing simplicial neural networks (SNNs), whether convolutional or attention-based, rely primarily on discrete filtering techniques, which can be restrictive. In contrast, partial differential equations (PDEs) on simplicial complexes offer a principled approach to capture continuous dynamics in such structures. In this work, we introduce COntinuous SiMplicial neural netwOrkS (COSMOS), a novel SNN architecture derived from PDEs on simplicial complexes. We provide theoretical and experimental justifications of COSMOS's stability under simplicial perturbations. Furthermore, we investigate the over-smoothing phenomenon, a common issue in geometric deep learning, demonstrating that COSMOS offers better control over this effect than discrete SNNs. Our experiments on real-world datasets of ocean trajectory prediction and regression on partial deformable shapes demonstrate that COSMOS achieves competitive performance compared to state-of-the-art SNNs in complex and noisy environments.         ",
    "url": "https://arxiv.org/abs/2503.12919",
    "authors": [
      "Aref Einizade",
      "Dorina Thanou",
      "Fragkiskos D. Malliaros",
      "Jhony H. Giraldo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12924",
    "title": "Social Media Journeys -- Mapping Platform Migration",
    "abstract": "           As people engage with the social media landscape, popular platforms rise and fall. As current research uncovers the experiences people have on various platforms, rarely do we engage with the sociotechnical migration processes when joining and leaving them. In this paper, we asked 32 visitors of a science communication festival to draw out artifacts that we call Social Media Journey Maps about the social media platforms they frequented, and why. By combining qualitative content analysis with a graph representation of Social Media Journeys, we present how social media migration processes are motivated by the interplay of environmental and platform factors. We find that peer-driven popularity, the timing of feature adoption, and personal perceptions of migration causes - such as security - shape individuals' reasoning for migrating between social media platforms. With this work, we aim to pave the way for future social media platforms that foster meaningful and enriching online experiences for users.         ",
    "url": "https://arxiv.org/abs/2503.12924",
    "authors": [
      "Artur Solomonik",
      "Hendrik Heuer"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.12929",
    "title": "AR-1-to-3: Single Image to Consistent 3D Object Generation via Next-View Prediction",
    "abstract": "           Novel view synthesis (NVS) is a cornerstone for image-to-3d creation. However, existing works still struggle to maintain consistency between the generated views and the input views, especially when there is a significant camera pose difference, leading to poor-quality 3D geometries and textures. We attribute this issue to their treatment of all target views with equal priority according to our empirical observation that the target views closer to the input views exhibit higher fidelity. With this inspiration, we propose AR-1-to-3, a novel next-view prediction paradigm based on diffusion models that first generates views close to the input views, which are then utilized as contextual information to progressively synthesize farther views. To encode the generated view subsequences as local and global conditions for the next-view prediction, we accordingly develop a stacked local feature encoding strategy (Stacked-LE) and an LSTM-based global feature encoding strategy (LSTM-GE). Extensive experiments demonstrate that our method significantly improves the consistency between the generated views and the input views, producing high-fidelity 3D assets.         ",
    "url": "https://arxiv.org/abs/2503.12929",
    "authors": [
      "Xuying Zhang",
      "Yupeng Zhou",
      "Kai Wang",
      "Yikai Wang",
      "Zhen Li",
      "Xiuli Shao",
      "Daquan Zhou",
      "Qibin Hou",
      "Ming-Ming Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12931",
    "title": "MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided Mirror Crafting",
    "abstract": "           Defending large language models (LLMs) against jailbreak attacks is crucial for ensuring their safe deployment. Existing defense strategies generally rely on predefined static criteria to differentiate between harmful and benign prompts. However, such rigid rules are incapable of accommodating the inherent complexity and dynamic nature of real jailbreak attacks. In this paper, we propose a novel concept of ``mirror'' to enable dynamic and adaptive defense. A mirror refers to a dynamically generated prompt that mirrors the syntactic structure of the input while ensuring semantic safety. The personalized discrepancies between the input prompts and their corresponding mirrors serve as the guiding principles for defense. A new defense paradigm, MirrorGuard, is further proposed to detect and calibrate risky inputs based on such mirrors. An entropy-based detection metric, Relative Input Uncertainty (RIU), is integrated into MirrorGuard to quantify the discrepancies between input prompts and mirrors. MirrorGuard is evaluated on several popular datasets, demonstrating state-of-the-art defense performance while maintaining general effectiveness.         ",
    "url": "https://arxiv.org/abs/2503.12931",
    "authors": [
      "Rui Pu",
      "Chaozhuo Li",
      "Rui Ha",
      "Litian Zhang",
      "Lirong Qiu",
      "Xi Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12953",
    "title": "Frame-wise Conditioning Adaptation for Fine-Tuning Diffusion Models in Text-to-Video Prediction",
    "abstract": "           Text-video prediction (TVP) is a downstream video generation task that requires a model to produce subsequent video frames given a series of initial video frames and text describing the required motion. In practice TVP methods focus on a particular category of videos depicting manipulations of objects carried out by human beings or robot arms. Previous methods adapt models pre-trained on text-to-image tasks, and thus tend to generate video that lacks the required continuity. A natural progression would be to leverage more recent pre-trained text-to-video (T2V) models. This approach is rendered more challenging by the fact that the most common fine-tuning technique, low-rank adaptation (LoRA), yields undesirable results. In this work, we propose an adaptation-based strategy we label Frame-wise Conditioning Adaptation (FCA). Within the module, we devise a sub-module that produces frame-wise text embeddings from the input text, which acts as an additional text condition to aid generation. We use FCA to fine-tune the T2V model, which incorporates the initial frame(s) as an extra condition. We compare and discuss the more effective strategy for injecting such embeddings into the T2V model. We conduct extensive ablation studies on our design choices with quantitative and qualitative performance analysis. Our approach establishes a new state-of-the-art for the task of TVP. The project page is at this https URL .         ",
    "url": "https://arxiv.org/abs/2503.12953",
    "authors": [
      "Zheyuan Liu",
      "Junyan Wang",
      "Zicheng Duan",
      "Cristian Rodriguez-Opazo",
      "Anton van den Hengel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12958",
    "title": "FedSDP: Explainable Differential Privacy in Federated Learning via Shapley Values",
    "abstract": "           Federated learning (FL) enables participants to store data locally while collaborating in training, yet it remains vulnerable to privacy attacks, such as data reconstruction. Existing differential privacy (DP) technologies inject noise dynamically into the training process to mitigate the impact of excessive noise. However, this dynamic scheduling is often grounded in factors indirectly related to privacy, making it difficult to clearly explain the intricate relationship between dynamic noise adjustments and privacy requirements. To address this issue, we propose FedSDP, a novel and explainable DP-based privacy protection mechanism that guides noise injection based on privacy contribution. Specifically, FedSDP leverages Shapley values to assess the contribution of private attributes to local model training and dynamically adjusts the amount of noise injected accordingly. By providing theoretical insights into the injection of varying scales of noise into local training, FedSDP enhances interpretability. Extensive experiments demonstrate that FedSDP can achieve a superior balance between privacy preservation and model performance, surpassing state-of-the-art (SOTA) solutions.         ",
    "url": "https://arxiv.org/abs/2503.12958",
    "authors": [
      "Yunbo Li",
      "Jiaping Gui",
      "Yue Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.12969",
    "title": "Action tube generation by person query matching for spatio-temporal action detection",
    "abstract": "           This paper proposes a method for spatio-temporal action detection (STAD) that directly generates action tubes from the original video without relying on post-processing steps such as IoU-based linking and clip splitting. Our approach applies query-based detection (DETR) to each frame and matches DETR queries to link the same person across frames. We introduce the Query Matching Module (QMM), which uses metric learning to bring queries for the same person closer together across frames compared to queries for different people. Action classes are predicted using the sequence of queries obtained from QMM matching, allowing for variable-length inputs from videos longer than a single clip. Experimental results on JHMDB, UCF101-24, and AVA datasets demonstrate that our method performs well for large position changes of people while offering superior computational efficiency and lower resource requirements.         ",
    "url": "https://arxiv.org/abs/2503.12969",
    "authors": [
      "Kazuki Omi",
      "Jion Oshima",
      "Toru Tamaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12972",
    "title": "Aligning Vision to Language: Text-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning",
    "abstract": "           Multimodal reasoning in Large Language Models (LLMs) struggles with incomplete knowledge and hallucination artifacts, challenges that textual Knowledge Graphs (KGs) only partially mitigate due to their modality isolation. While Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal understanding, their practical construction is impeded by semantic narrowness of manual text annotations and inherent noise in visual-semantic entity linkages. In this paper, we propose Vision-align-to-Language integrated Knowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances LLMs reasoning through cross-modal information supplementation. Specifically, we cascade pre-trained Vision-Language Models (VLMs) to align image features with text, transforming them into descriptions that encapsulate image-specific information. Furthermore, we developed a cross-modal similarity verification mechanism to quantify semantic consistency, effectively filtering out noise introduced during feature alignment. Even without manually annotated image captions, the refined descriptions alone suffice to construct the MMKG. Compared to conventional MMKGs construction paradigms, our approach achieves substantial storage efficiency gains while maintaining direct entity-to-image linkage capability. Experimental results on multimodal reasoning tasks demonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art models. Our code is published at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12972",
    "authors": [
      "Junming Liu",
      "Siyuan Meng",
      "Yanting Gao",
      "Song Mao",
      "Pinlong Cai",
      "Guohang Yan",
      "Yirong Chen",
      "Zilin Bian",
      "Botian Shi",
      "Ding Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12973",
    "title": "Prospects for Mitigating Spectral Variability in Tropical Species Classification Using Self-Supervised Learning",
    "abstract": "           Airborne hyperspectral imaging is a promising method for identifying tropical species, but spectral variability between acquisitions hinders consistent results. This paper proposes using Self-Supervised Learning (SSL) to encode spectral features that are robust to abiotic variability and relevant for species identification. By employing the state-of-the-art Barlow-Twins approach on repeated spectral acquisitions, we demonstrate the ability to develop stable features. For the classification of 40 tropical species, experiments show that these features can outperform typical reflectance products in terms of robustness to spectral variability by 10 points of accuracy across dates.         ",
    "url": "https://arxiv.org/abs/2503.12973",
    "authors": [
      "Colin Prieur",
      "Nassim Ait Ali Braham",
      "Paul Tresson",
      "Gr\u00e9goire Vincent",
      "Jocelyn Chanussot"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12978",
    "title": "Enhancing Job Salary Prediction with Disentangled Composition Effect Modeling: A Neural Prototyping Approach",
    "abstract": "           In the era of the knowledge economy, understanding how job skills influence salary is crucial for promoting recruitment with competitive salary systems and aligned salary expectations. Despite efforts on salary prediction based on job positions and talent demographics, there still lacks methods to effectively discern the set-structured skills' intricate composition effect on job salary. While recent advances in neural networks have significantly improved accurate set-based quantitative modeling, their lack of explainability hinders obtaining insights into the skills' composition effects. Indeed, model explanation for set data is challenging due to the combinatorial nature, rich semantics, and unique format. To this end, in this paper, we propose a novel intrinsically explainable set-based neural prototyping approach, namely \\textbf{LGDESetNet}, for explainable salary prediction that can reveal disentangled skill sets that impact salary from both local and global perspectives. Specifically, we propose a skill graph-enhanced disentangled discrete subset selection layer to identify multi-faceted influential input subsets with varied semantics. Furthermore, we propose a set-oriented prototype learning method to extract globally influential prototypical sets. The resulting output is transparently derived from the semantic interplay between these input subsets and global prototypes. Extensive experiments on four real-world datasets demonstrate that our method achieves superior performance than state-of-the-art baselines in salary prediction while providing explainable insights into salary-influencing patterns.         ",
    "url": "https://arxiv.org/abs/2503.12978",
    "authors": [
      "Yang Ji",
      "Ying Sun",
      "Hengshu Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12982",
    "title": "SparseAlign: A Fully Sparse Framework for Cooperative Object Detection",
    "abstract": "           Cooperative perception can increase the view field and decrease the occlusion of an ego vehicle, hence improving the perception performance and safety of autonomous driving. Despite the success of previous works on cooperative object detection, they mostly operate on dense Bird's Eye View (BEV) feature maps, which are computationally demanding and can hardly be extended to long-range detection problems. More efficient fully sparse frameworks are rarely explored. In this work, we design a fully sparse framework, SparseAlign, with three key features: an enhanced sparse 3D backbone, a query-based temporal context learning module, and a robust detection head specially tailored for sparse features. Extensive experimental results on both OPV2V and DairV2X datasets show that our framework, despite its sparsity, outperforms the state of the art with less communication bandwidth requirements. In addition, experiments on the OPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also show a significant performance gain compared to the baseline works.         ",
    "url": "https://arxiv.org/abs/2503.12982",
    "authors": [
      "Yunshuang Yuan",
      "Yan Xia",
      "Daniel Cremers",
      "Monika Sester"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12994",
    "title": "Conversation-Based Multimodal Abuse Detection Through Text and Graph Embeddings",
    "abstract": "           Abusive behavior is common on online social networks, and forces the hosts of such platforms to find new solutions to address this problem. Various methods have been proposed to automate this task in the past decade. Most of them rely on the exchanged content, but ignore the structure and dynamics of the conversation, which could provide some relevant information. In this article, we propose to use representation learning methods to automatically produce embeddings of this textual content and of the conversational graphs depicting message exchanges. While the latter could be enhanced by including additional information on top of the raw conversational structure, no method currently exists to learn wholegraph representations using simultaneously edge directions, weights, signs, and vertex attributes. We propose two such methods to fill this gap in the literature. We experiment with 5 textual and 13 graph embedding methods, and apply them to a dataset of online messages annotated for abuse detection. Our best results achieve an F -measure of 81.02 using text alone and 80.61 using graphs alone. We also combine both modalities of information (text and graphs) through three fusion strategies, and show that this strongly improves abuse detection performance, increasing the F -measure to 87.06. Finally, we identify which specific engineered features are captured by the embedding methods under consideration. These features have clear interpretations and help explain what information the representation learning methods deem discriminative.         ",
    "url": "https://arxiv.org/abs/2503.12994",
    "authors": [
      "No\u00e9 Cecillon",
      "Vincent Labatut",
      "Richard Dufour"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.12996",
    "title": "Semi-Streaming Algorithms for Graph Property Certification",
    "abstract": "           We introduce the {\\em certification} of solutions to graph problems when access to the input is restricted. This topic has received a lot of attention in the distributed computing setting, and we introduce it here in the context of \\emph{streaming} algorithms, where the input is too large to be stored in memory. Given a graph property $\\mbox{P}$, a \\emph{streaming certification scheme} for $\\mbox{P}$ is a \\emph{prover-verifier} pair where the prover is a computationally unlimited but non-trustable oracle, and the verifier is a streaming algorithm. For any input graph, the prover provides the verifier with a \\emph{certificate}. The verifier then receives the input graph as a stream of edges in an adversarial order, and must check whether the certificate is indeed a \\emph{proof} that the input graph satisfies $\\mbox{P}$. The main complexity measure for a streaming certification scheme is its \\emph{space complexity}, defined as the sum of the size of the certificate provided by the oracle, and of the memory space required by the verifier. We give streaming certification schemes for several graph properties, including maximum matching, diameter, degeneracy, and coloring, with space complexity matching the requirement of \\emph{semi-streaming}, i.e., with space complexity $O(n\\,\\mbox{polylog}\\, n)$ for $n$-node graphs. All these problems do {\\em not} admit semi-streaming algorithms, showing that also in the (semi) streaming setting, certification is sometimes easier than calculation (like $NP$). For each of these properties, we provide upper and lower bounds on the space complexity of the corresponding certification schemes, many being tight up to logarithmic multiplicative factors. We also show that some graph properties are hard for streaming certification, in the sense that they cannot be certified in semi-streaming, as they require $\\Omega(n^2)$-bit certificates.         ",
    "url": "https://arxiv.org/abs/2503.12996",
    "authors": [
      "Avinandan Das",
      "Pierre Fraigniaud",
      "Ami Paz",
      "Adi Rosen"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2503.13001",
    "title": "Linear-Size Neural Network Representation of Piecewise Affine Functions in $\\mathbb{R}^2$",
    "abstract": "           It is shown that any continuous piecewise affine (CPA) function $\\mathbb{R}^2\\to\\mathbb{R}$ with $p$ pieces can be represented by a ReLU neural network with two hidden layers and $O(p)$ neurons. Unlike prior work, which focused on convex pieces, this analysis considers CPA functions with connected but potentially non-convex pieces.         ",
    "url": "https://arxiv.org/abs/2503.13001",
    "authors": [
      "Leo Zanotti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Metric Geometry (math.MG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.13008",
    "title": "Knowledge Distillation: Enhancing Neural Network Compression with Integrated Gradients",
    "abstract": "           Efficient deployment of deep neural networks on resource-constrained devices demands advanced compression techniques that preserve accuracy and interoperability. This paper proposes a machine learning framework that augments Knowledge Distillation (KD) with Integrated Gradients (IG), an attribution method, to optimise the compression of convolutional neural networks. We introduce a novel data augmentation strategy where IG maps, precomputed from a teacher model, are overlaid onto training images to guide a compact student model toward critical feature representations. This approach leverages the teacher's decision-making insights, enhancing the student's ability to replicate complex patterns with reduced parameters. Experiments on CIFAR-10 demonstrate the efficacy of our method: a student model, compressed 4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy, surpassing the baseline student's 91.4% and traditional KD approaches, while reducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform hyperparameter optimisation for efficient learning. Comprehensive ablation studies dissect the contributions of KD and IG, revealing synergistic effects that boost both performance and model explainability. Our method's emphasis on feature-level guidance via IG distinguishes it from conventional KD, offering a data-driven solution for mining transferable knowledge in neural architectures. This work contributes to machine learning by providing a scalable, interpretable compression technique, ideal for edge computing applications where efficiency and transparency are paramount.         ",
    "url": "https://arxiv.org/abs/2503.13008",
    "authors": [
      "David E. Hernandez",
      "Jose Ramon Chang",
      "Torbj\u00f6rn E. M. Nordling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13034",
    "title": "TA-GNN: Physics Inspired Time-Agnostic Graph Neural Network for Finger Motion Prediction",
    "abstract": "           Continuous prediction of finger joint movement using historical joint positions/rotations is vital in a multitude of applications, especially related to virtual reality, computer graphics, robotics, and rehabilitation. However, finger motions are highly articulated with multiple degrees of freedom, making them significantly harder to model and predict. To address this challenge, we propose a physics-inspired time-agnostic graph neural network (TA-GNN) to accurately predict human finger motions. The proposed encoder comprises a kinematic feature extractor to generate filtered velocity and acceleration and a physics-based encoder that follows linear kinematics. The model is designed to be prediction-time-agnostic so that it can seamlessly provide continuous predictions. The graph-based decoder for learning the topological motion between finger joints is designed to address the higher degree articulation of fingers. We show the superiority of our model performance in virtual reality context. This novel approach enhances finger tracking without additional sensors, enabling predictive interactions such as haptic re-targeting and improving predictive rendering quality.         ",
    "url": "https://arxiv.org/abs/2503.13034",
    "authors": [
      "Tinghui Li",
      "Pamuditha Somarathne",
      "Zhanna Sarsenbayeva",
      "Anusha Withana"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.13041",
    "title": "Study Protocol: Shared Achievements: Exploring the Design of Gameful Collaborative Elements and Fostering Social Relatedness through Team Effort Contributions in a Social Physical Activity App",
    "abstract": "           This study protocol outlines the design and methodology of a research study investigating collaborative game elements to promote physical activity within digital health interventions. The study aims to examine how social relatedness influences motivation and adherence to step-count goals. Participants will use Shared Achievements, a minimalistic multiplayer step counter game, over two weeks, one week contributing absolute step counts and one week sharing step counts as a relative percentage of a team goal. Data will be collected through usage metrics and participant feedback to evaluate engagement, motivation, and perceived challenges. Findings will inform the design of digital health tools that balance competition and collaboration, optimising social and behavioural support mechanisms.         ",
    "url": "https://arxiv.org/abs/2503.13041",
    "authors": [
      "Faith Young",
      "Dmitry Alexandrovsky",
      "Daniela Wurhofer",
      "Eva-Maria Krah",
      "Jan Smeddinck"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.13047",
    "title": "InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving",
    "abstract": "           Directly generating planning results from raw sensors has become increasingly prevalent due to its adaptability and robustness in complex scenarios. Scene representation, as a key module in the pipeline, has traditionally relied on conventional perception, which focus on the global scene. However, in driving scenarios, human drivers typically focus only on regions that directly impact driving, which often coincide with those required for end-to-end autonomous driving. In this paper, a novel end-to-end autonomous driving method called InsightDrive is proposed, which organizes perception by language-guided scene representation. We introduce an instance-centric scene tokenizer that transforms the surrounding environment into map- and object-aware instance tokens. Scene attention language descriptions, which highlight key regions and obstacles affecting the ego vehicle's movement, are generated by a vision-language model that leverages the cognitive reasoning capabilities of foundation models. We then align scene descriptions with visual features using the vision-language model, guiding visual attention through these descriptions to give effectively scene representation. Furthermore, we employ self-attention and cross-attention mechanisms to model the ego-agents and ego-map relationships to comprehensively build the topological relationships of the scene. Finally, based on scene understanding, we jointly perform motion prediction and planning. Extensive experiments on the widely used nuScenes benchmark demonstrate that the proposed InsightDrive achieves state-of-the-art performance in end-to-end autonomous driving. The code is available at this https URL ",
    "url": "https://arxiv.org/abs/2503.13047",
    "authors": [
      "Ruiqi Song",
      "Xianda Guo",
      "Hangbin Wu",
      "Qinggong Wei",
      "Long Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13086",
    "title": "Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization",
    "abstract": "           3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast real-time performance, but existing methods rely on offline training after full Structure-from-Motion (SfM) processing. In contrast, this work introduces On-the-Fly GS, a progressive framework enabling near real-time 3DGS optimization during image capture. As each image arrives, its pose and sparse points are updated via on-the-fly SfM, and newly optimized Gaussians are immediately integrated into the 3DGS field. We propose a progressive local optimization strategy to prioritize new images and their neighbors by their corresponding overlapping relationship, allowing the new image and its overlapping images to get more training. To further stabilize training across old and new images, an adaptive learning rate schedule balances the iterations and the learning rate. Moreover, to maintain overall quality of the 3DGS field, an efficient global optimization scheme prevents overfitting to the newly added images. Experiments on multiple benchmark datasets show that our On-the-Fly GS reduces training time significantly, optimizing each new image in seconds with minimal rendering loss, offering the first practical step toward rapid, progressive 3DGS reconstruction.         ",
    "url": "https://arxiv.org/abs/2503.13086",
    "authors": [
      "Yiwei Xu",
      "Yifei Yu",
      "Wentian Gan",
      "Tengfei Wang",
      "Zongqian Zhan",
      "Hao Cheng",
      "Xin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13113",
    "title": "Exploring the Potential of Bilevel Optimization for Calibrating Neural Networks",
    "abstract": "           Handling uncertainty is critical for ensuring reliable decision-making in intelligent systems. Modern neural networks are known to be poorly calibrated, resulting in predicted confidence scores that are difficult to use. This article explores improving confidence estimation and calibration through the application of bilevel optimization, a framework designed to solve hierarchical problems with interdependent optimization levels. A self-calibrating bilevel neural-network training approach is introduced to improve a model's predicted confidence scores. The effectiveness of the proposed framework is analyzed using toy datasets, such as Blobs and Spirals, as well as more practical simulated datasets, such as Blood Alcohol Concentration (BAC). It is compared with a well-known and widely used calibration strategy, isotonic regression. The reported experimental results reveal that the proposed bilevel optimization approach reduces the calibration error while preserving accuracy.         ",
    "url": "https://arxiv.org/abs/2503.13113",
    "authors": [
      "Gabriele Sanguin",
      "Arjun Pakrashi",
      "Marco Viola",
      "Francesco Rinaldi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.13123",
    "title": "MIXPINN: Mixed-Material Simulations by Physics-Informed Neural Network",
    "abstract": "           Simulating the complex interactions between soft tissues and rigid anatomy is critical for applications in surgical training, planning, and robotic-assisted interventions. Traditional Finite Element Method (FEM)-based simulations, while accurate, are computationally expensive and impractical for real-time scenarios. Learning-based approaches have shown promise in accelerating predictions but have fallen short in modeling soft-rigid interactions effectively. We introduce MIXPINN, a physics-informed Graph Neural Network (GNN) framework for mixed-material simulations, explicitly capturing soft-rigid interactions using graph-based augmentations. Our approach integrates Virtual Nodes (VNs) and Virtual Edges (VEs) to enhance rigid body constraint satisfaction while preserving computational efficiency. By leveraging a graph-based representation of biomechanical structures, MIXPINN learns high-fidelity deformations from FEM-generated data and achieves real-time inference with sub-millimeter accuracy. We validate our method in a realistic clinical scenario, demonstrating superior performance compared to baseline GNN models and traditional FEM methods. Our results show that MIXPINN reduces computational cost by an order of magnitude while maintaining high physical accuracy, making it a viable solution for real-time surgical simulation and robotic-assisted procedures.         ",
    "url": "https://arxiv.org/abs/2503.13123",
    "authors": [
      "Xintian Yuan",
      "Yunke Ao",
      "Boqi Chen",
      "Philipp Fuernstahl"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.13125",
    "title": "Non-Destructive Detection of Sub-Micron Imperceptible Scratches On Laser Chips Based On Consistent Texture Entropy Recursive Optimization Semi-Supervised Network",
    "abstract": "           Laser chips, the core components of semiconductor lasers, are extensively utilized in various industries, showing great potential for future application. Smoothness emitting surfaces are crucial in chip production, as even imperceptible scratches can significantly degrade performance and lifespan, thus impeding production efficiency and yield. Therefore, non-destructively detecting these imperceptible scratches on the emitting surfaces is essential for enhancing yield and reducing costs. These sub-micron level scratches, barely visible against the background, are extremely difficult to detect with conventional methods, compounded by a lack of labeled datasets. To address this challenge, this paper introduces TexRecNet, a consistent texture entropy recursive optimization semi-supervised network. The network, based on a recursive optimization architecture, iteratively improves the detection accuracy of imperceptible scratch edges, using outputs from previous cycles to inform subsequent inputs and guide the network's positional encoding. It also introduces image texture entropy, utilizing a substantial amount of unlabeled data to expand the training set while maintaining training signal reliability. Ultimately, by analyzing the inconsistency of the network output sequences obtained during the recursive process, a semi-supervised training strategy with recursive consistency constraints is proposed, using outputs from the recursive process for non-destructive signal augmentation and consistently optimizes the loss function for efficient end-to-end training. Experimental results show that this method, utilizing a substantial amount of unsupervised data, achieves 75.6% accuracy and 74.8% recall in detecting imperceptible scratches, an 8.5% and 33.6% improvement over conventional Unet, enhancing quality control in laser chips.         ",
    "url": "https://arxiv.org/abs/2503.13125",
    "authors": [
      "Pan Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13145",
    "title": "High-entropy Advantage in Neural Networks' Generalizability",
    "abstract": "           While the 2024 Nobel Prize in Physics ignites a worldwide discussion on the origins of neural networks and their foundational links to physics, modern machine learning research predominantly focuses on computational and algorithmic advancements, overlooking a picture of physics. Here we introduce the concept of entropy into neural networks by reconceptualizing them as hypothetical physical systems where each parameter is a non-interacting 'particle' within a one-dimensional space. By employing a Wang-Landau algorithms, we construct the neural networks' (with up to 1 million parameters) entropy landscapes as functions of training loss and test accuracy (or loss) across four distinct machine learning tasks, including arithmetic question, real-world tabular data, image recognition, and language modeling. Our results reveal the existence of \\textit{entropy advantage}, where the high-entropy states generally outperform the states reached via classical training optimizer like stochastic gradient descent. We also find this advantage is more pronounced in narrower networks, indicating a need of different training optimizers tailored to different sizes of neural networks.         ",
    "url": "https://arxiv.org/abs/2503.13145",
    "authors": [
      "Entao Yang",
      "Xiaotian Zhang",
      "Yue Shang",
      "Ge Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Mechanics (cond-mat.stat-mech)"
    ]
  },
  {
    "id": "arXiv:2503.13147",
    "title": "Iterative Predictor-Critic Code Decoding for Real-World Image Dehazing",
    "abstract": "           We propose a novel Iterative Predictor-Critic Code Decoding framework for real-world image dehazing, abbreviated as IPC-Dehaze, which leverages the high-quality codebook prior encapsulated in a pre-trained VQGAN. Apart from previous codebook-based methods that rely on one-shot decoding, our method utilizes high-quality codes obtained in the previous iteration to guide the prediction of the Code-Predictor in the subsequent iteration, improving code prediction accuracy and ensuring stable dehazing performance. Our idea stems from the observations that 1) the degradation of hazy images varies with haze density and scene depth, and 2) clear regions play crucial cues in restoring dense haze regions. However, it is non-trivial to progressively refine the obtained codes in subsequent iterations, owing to the difficulty in determining which codes should be retained or replaced at each iteration. Another key insight of our study is to propose Code-Critic to capture interrelations among codes. The Code-Critic is used to evaluate code correlations and then resample a set of codes with the highest mask scores, i.e., a higher score indicates that the code is more likely to be rejected, which helps retain more accurate codes and predict difficult ones. Extensive experiments demonstrate the superiority of our method over state-of-the-art methods in real-world dehazing.         ",
    "url": "https://arxiv.org/abs/2503.13147",
    "authors": [
      "Jiayi Fu",
      "Siyu Liu",
      "Zikun Liu",
      "Chun-Le Guo",
      "Hyunhee Park",
      "Ruiqi Wu",
      "Guoqing Wang",
      "Chongyi Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13156",
    "title": "DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph Knowledge Distillation for Gait Disorders Recognition",
    "abstract": "           Gait disorder recognition plays a crucial role in the early diagnosis and monitoring of movement disorders. Existing approaches, including spatio-temporal graph convolutional networks (ST-GCNs), often face high memory demands and struggle to capture complex spatio-temporal dependencies, limiting their efficiency in clinical applications. To address these challenges, we introduce DynSTG-Mamba (Dynamic Spatio-Temporal Graph Mamba), a novel framework that combines DF-STGNN and STG-Mamba to enhance motion sequence modeling. The DF-STGNN incorporates a dynamic spatio-temporal filter that adaptively adjusts spatial connections between skeletal joints and temporal interactions across different movement phases. This approach ensures better feature propagation through dynamic graph structures by considering the hierarchical nature and dynamics of skeletal gait data. Meanwhile, STG-Mamba, an extension of Mamba adapted for skeletal motion data, ensures a continuous propagation of states, facilitating the capture of long-term dependencies while reducing computational complexity. To reduce the number of model parameters and computational costs while maintaining consistency, we propose Cross-Graph Relational Knowledge Distillation, a novel knowledge transfer mechanism that aligns relational information between teacher (large architecture) and student models (small architecture) while using shared memory. This ensures that the interactions and movement patterns of the joints are accurately preserved in the motion sequences. We validate our DynSTG-Mamba on KOA-NM, PD-WALK, and ATAXIA datasets, where it outperforms state-of-the-art approaches by achieving in terms of Accuracy, F1-score, and Recall. Our results highlight the efficiency and robustness of our approach, offering a lightweight yet highly accurate solution for automated gait analysis and movement disorder assessment.         ",
    "url": "https://arxiv.org/abs/2503.13156",
    "authors": [
      "Zakariae Zrimek",
      "Youssef Mourchid",
      "Mohammed El Hassouni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13160",
    "title": "Language-guided Open-world Video Anomaly Detection",
    "abstract": "           Video anomaly detection models aim to detect anomalies that deviate from what is expected. In open-world scenarios, the expected events may change as requirements change. For example, not wearing a mask is considered abnormal during a flu outbreak but normal otherwise. However, existing methods assume that the definition of anomalies is invariable, and thus are not applicable to the open world. To address this, we propose a novel open-world VAD paradigm with variable definitions, allowing guided detection through user-provided natural language at inference time. This paradigm necessitates establishing a robust mapping from video and textual definition to anomaly score. Therefore, we propose LaGoVAD (Language-guided Open-world VAD), a model that dynamically adapts anomaly definitions through two regularization strategies: diversifying the relative durations of anomalies via dynamic video synthesis, and enhancing feature robustness through contrastive learning with negative mining. Training such adaptable models requires diverse anomaly definitions, but existing datasets typically provide given labels without semantic descriptions. To bridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the largest and most diverse video anomaly dataset to date, featuring 35,279 annotated videos with multi-level category labels and descriptions that explicitly define anomalies. Zero-shot experiments on seven datasets demonstrate SOTA performance. Data and code will be released.         ",
    "url": "https://arxiv.org/abs/2503.13160",
    "authors": [
      "Zihao Liu",
      "Xiaoyu Wu",
      "Jianqin Wu",
      "Xuxu Wang",
      "Linlin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13163",
    "title": "Beyond RGB: Adaptive Parallel Processing for RAW Object Detection",
    "abstract": "           Object detection models are typically applied to standard RGB images processed through Image Signal Processing (ISP) pipelines, which are designed to enhance sensor-captured RAW images for human vision. However, these ISP functions can lead to a loss of critical information that may be essential in optimizing for computer vision tasks, such as object detection. In this work, we introduce Raw Adaptation Module (RAM), a module designed to replace the traditional ISP, with parameters optimized specifically for RAW object detection. Inspired by the parallel processing mechanisms of the human visual system, RAM departs from existing learned ISP methods by applying multiple ISP functions in parallel rather than sequentially, allowing for a more comprehensive capture of image features. These processed representations are then fused in a specialized module, which dynamically integrates and optimizes the information for the target task. This novel approach not only leverages the full potential of RAW sensor data but also enables task-specific pre-processing, resulting in superior object detection performance. Our approach outperforms RGB-based methods and achieves state-of-the-art results across diverse RAW image datasets under varying lighting conditions and dynamic ranges.         ",
    "url": "https://arxiv.org/abs/2503.13163",
    "authors": [
      "Shani Gamrian",
      "Hila Barel",
      "Feiran Li",
      "Masakazu Yoshimura",
      "Daisuke Iso"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13174",
    "title": "Examining the Effects of Immersive and Non-Immersive Presenter Modalities on Engagement and Social Interaction in Co-located Augmented Presentations",
    "abstract": "           Head-worn augmented reality (AR) allows audiences to be immersed and engaged in stories told by live presenters. While presenters may also be in AR to have the same level of immersion and awareness as their audience, this symmetric presentation style may diminish important social cues such as eye contact. In this work, we examine the effects this (a)symmetry has on engagement, group awareness, and social interaction in co-located one-on-one augmented presentations. We developed a presentation system incorporating 2D/3D content that audiences can view and interact with in AR, with presenters controlling and delivering the presentation in either a symmetric style in AR, or an asymmetric style with a handheld tablet. We conducted a within- and between-subjects evaluation with 12 participant pairs to examine the differences between these symmetric and asymmetric presentation modalities. From our findings, we extracted four themes and derived strategies and guidelines for designers interested in augmented presentations.         ",
    "url": "https://arxiv.org/abs/2503.13174",
    "authors": [
      "Matt Gottsacker",
      "Mengyu Chen",
      "David Saffo",
      "Feiyu Lu",
      "Benjamin Lee",
      "Blair MacIntyre"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.13178",
    "title": "Rapfi: Distilling Efficient Neural Network for the Game of Gomoku",
    "abstract": "           Games have played a pivotal role in advancing artificial intelligence, with AI agents using sophisticated techniques to compete. Despite the success of neural network based game AIs, their performance often requires significant computational resources. In this paper, we present Rapfi, an efficient Gomoku agent that outperforms CNN-based agents in limited computation environments. Rapfi leverages a compact neural network with a pattern-based codebook distilled from CNNs, and an incremental update scheme that minimizes computation when input changes are minor. This new network uses computation that is orders of magnitude less to reach a similar accuracy of much larger neural networks such as Resnet. Thanks to our incremental update scheme, depth-first search methods such as the alpha-beta search can be significantly accelerated. With a carefully tuned evaluation and search, Rapfi reached strength surpassing Katagomo, the strongest open-source Gomoku AI based on AlphaZero's algorithm, under limited computational resources where accelerators like GPUs are absent. Rapfi ranked first among 520 Gomoku agents on Botzone and won the championship in GomoCup 2024.         ",
    "url": "https://arxiv.org/abs/2503.13178",
    "authors": [
      "Zhanggen Jin",
      "Haobin Duan",
      "Zhiyang Hang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.13179",
    "title": "A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network",
    "abstract": "           This study proposes a lightweight method for building image super-resolution using a Dilated Contextual Feature Modulation Network (DCFMN). The process includes obtaining high-resolution images, down-sampling them to low-resolution, enhancing the low-resolution images, constructing and training a lightweight network model, and generating super-resolution outputs. To address challenges such as regular textures and long-range dependencies in building images, the DCFMN integrates an expansion separable modulation unit and a local feature enhancement module. The former employs multiple expansion convolutions equivalent to a large kernel to efficiently aggregate multi-scale features while leveraging a simple attention mechanism for adaptivity. The latter encodes local features, mixes channel information, and ensures no additional computational burden during inference through reparameterization. This approach effectively resolves the limitations of existing lightweight super-resolution networks in modeling long-range dependencies, achieving accurate and efficient global feature modeling without increasing computational costs, and significantly improving both reconstruction quality and lightweight efficiency for building image super-resolution models.         ",
    "url": "https://arxiv.org/abs/2503.13179",
    "authors": [
      "Yi Zhang",
      "Wenye Zhou",
      "Ruonan Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13184",
    "title": "Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided Visual Tokenizer and Manufacturing Process",
    "abstract": "           Although recent methods have tried to introduce large multimodal models (LMMs) into industrial anomaly detection (IAD), their generalization in the IAD field is far inferior to that for general purposes. We summarize the main reasons for this gap into two aspects. On one hand, general-purpose LMMs lack cognition of defects in the visual modality, thereby failing to sufficiently focus on defect areas. Therefore, we propose to modify the AnyRes structure of the LLaVA model, providing the potential anomalous areas identified by existing IAD models to the LMMs. On the other hand, existing methods mainly focus on identifying defects by learning defect patterns or comparing with normal samples, yet they fall short of understanding the causes of these defects. Considering that the generation of defects is closely related to the manufacturing process, we propose a manufacturing-driven IAD paradigm. An instruction-tuning dataset for IAD (InstructIAD) and a data organization approach for Chain-of-Thought with manufacturing (CoT-M) are designed to leverage the manufacturing process for IAD. Based on the above two modifications, we present Triad, a novel LMM-based method incorporating an expert-guided region-of-interest tokenizer and manufacturing process for industrial anomaly detection. Extensive experiments show that our Triad not only demonstrates competitive performance against current LMMs but also achieves further improved accuracy when equipped with manufacturing processes. Source code, training data, and pre-trained models will be publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.13184",
    "authors": [
      "Yuanze Li",
      "Shihao Yuan",
      "Haolin Wang",
      "Qizhang Li",
      "Ming Liu",
      "Chen Xu",
      "Guangming Shi",
      "Wangmeng Zuo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13193",
    "title": "The deep multi-FBSDE method: a robust deep learning method for coupled FBSDEs",
    "abstract": "           We introduce the deep multi-FBSDE method for robust approximation of coupled forward-backward stochastic differential equations (FBSDEs), focusing on cases where the deep BSDE method of Han, Jentzen, and E (2018) fails to converge. To overcome the convergence issues, we consider a family of FBSDEs that are equivalent to the original problem in the sense that they satisfy the same associated partial differential equation (PDE). Our algorithm proceeds in two phases: first, we approximate the initial condition for the FBSDE family, and second, we approximate the original FBSDE using the initial condition approximated in the first phase. Numerical experiments show that our method converges even when the standard deep BSDE method does not.         ",
    "url": "https://arxiv.org/abs/2503.13193",
    "authors": [
      "Kristoffer Andersson",
      "Adam Andersson",
      "Cornelis W. Oosterlee"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)",
      "Computational Finance (q-fin.CP)"
    ]
  },
  {
    "id": "arXiv:2503.13194",
    "title": "A representational framework for learning and encoding structurally enriched trajectories in complex agent environments",
    "abstract": "           The ability of artificial intelligence agents to make optimal decisions and generalise them to different domains and tasks is compromised in complex scenarios. One way to address this issue has focused on learning efficient representations of the world and on how the actions of agents affect them, such as disentangled representations that exploit symmetries. Whereas such representations are procedurally efficient, they are based on the compression of low-level state-action transitions, which lack structural richness. To address this problem, we propose to enrich the agent's ontology and extend the traditional conceptualisation of trajectories to provide a more nuanced view of task execution. Structurally Enriched Trajectories (SETs) extend the encoding of sequences of states and their transitions by incorporating hierarchical relations between objects, interactions and affordances. SETs are built as multi-level graphs, providing a detailed representation of the agent dynamics and a transferable functional abstraction of the task. SETs are integrated into an architecture, Structurally Enriched Trajectory Learning and Encoding (SETLE), that employs a heterogeneous graph-based memory structure of multi-level relational dependencies essential for generalisation. Using reinforcement learning as a data generation tool, we demonstrate that SETLE can support downstream tasks, enabling agents to recognise task-relevant structural patterns across diverse environments.         ",
    "url": "https://arxiv.org/abs/2503.13194",
    "authors": [
      "Corina Catarau-Cotutiu",
      "Esther Mondragon",
      "Eduardo Alonso"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.13208",
    "title": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft prompt Optimization Approach",
    "abstract": "           Prompt-tuning (PT) for large language models (LLMs) can facilitate the performance on various conventional NLP tasks with significantly fewer trainable parameters. However, our investigation reveals that PT provides limited improvement and may even degrade the primitive performance of LLMs on complex reasoning tasks. Such a phenomenon suggests that soft prompts can positively impact certain instances while negatively affecting others, particularly during the later phases of reasoning. To address these challenges, We first identify an information accumulation within the soft prompts. Through detailed analysis, we demonstrate that this phenomenon is often accompanied by erroneous information flow patterns in the deeper layers of the model, which ultimately lead to incorrect reasoning outcomes. we propose a novel method called \\textbf{D}ynamic \\textbf{P}rompt \\textbf{C}orruption (DPC) to take better advantage of soft prompts in complex reasoning tasks, which dynamically adjusts the influence of soft prompts based on their impact on the reasoning process. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic Corruption. First, Dynamic Trigger measures the impact of soft prompts, identifying whether beneficial or detrimental. Then, Dynamic Corruption mitigates the negative effects of soft prompts by selectively masking key tokens that interfere with the reasoning process. We validate the proposed approach through extensive experiments on various LLMs and reasoning tasks, including GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can consistently enhance the performance of PT, achieving 4\\%-8\\% accuracy gains compared to vanilla prompt tuning, highlighting the effectiveness of our approach and its potential to enhance complex reasoning in LLMs.         ",
    "url": "https://arxiv.org/abs/2503.13208",
    "authors": [
      "Sinan Fan",
      "Liang Xie",
      "Chen Shen",
      "Ge Teng",
      "Xiaosong Yuan",
      "Xiaofeng Zhang",
      "Chenxi Huang",
      "Wenxiao Wang",
      "Xiaofei He",
      "Jieping Ye"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.13223",
    "title": "Robust Decision-Making Via Free Energy Minimization",
    "abstract": "           Despite their groundbreaking performance, state-of-the-art autonomous agents can misbehave when training and environmental conditions become inconsistent, with minor mismatches leading to undesirable behaviors or even catastrophic failures. Robustness towards these training/environment ambiguities is a core requirement for intelligent agents and its fulfillment is a long-standing challenge when deploying agents in the real world. Here, departing from mainstream views seeking robustness through training, we introduce DR-FREE, a free energy model that installs this core property by design. It directly wires robustness into the agent decision-making mechanisms via free energy minimization. By combining a robust extension of the free energy principle with a novel resolution engine, DR-FREE returns a policy that is optimal-yet-robust against ambiguity. Moreover, for the first time, it reveals the mechanistic role of ambiguity on optimal decisions and requisite Bayesian belief updating. We evaluate DR-FREE on an experimental testbed involving real rovers navigating an ambiguous environment filled with obstacles. Across all the experiments, DR-FREE enables robots to successfully navigate towards their goal even when, in contrast, standard free energy minimizing agents that do not use DR-FREE fail. In short, DR-FREE can tackle scenarios that elude previous methods: this milestone may inspire both deployment in multi-agent settings and, at a perhaps deeper level, the quest for a biologically plausible explanation of how natural agents - with little or no training - survive in capricious environments.         ",
    "url": "https://arxiv.org/abs/2503.13223",
    "authors": [
      "Allahkaram Shafiei",
      "Hozefa Jesawada",
      "Karl Friston",
      "Giovanni Russo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2503.13236",
    "title": "Gradient Extrapolation for Debiased Representation Learning",
    "abstract": "           Machine learning classification models trained with empirical risk minimization (ERM) often inadvertently rely on spurious correlations. When absent in the test data, these unintended associations between non-target attributes and target labels lead to poor generalization. This paper addresses this problem from a model optimization perspective and proposes a novel method, Gradient Extrapolation for Debiased Representation Learning (GERNE), designed to learn debiased representations in both known and unknown attribute training cases. GERNE uses two distinct batches with different amounts of spurious correlations to define the target gradient as the linear extrapolation of two gradients computed from each batch's loss. It is demonstrated that the extrapolated gradient, if directed toward the gradient of the batch with fewer amount of spurious correlation, can guide the training process toward learning a debiased model. GERNE can serve as a general framework for debiasing with methods, such as ERM, reweighting, and resampling, being shown as special cases. The theoretical upper and lower bounds of the extrapolation factor are derived to ensure convergence. By adjusting this factor, GERNE can be adapted to maximize the Group-Balanced Accuracy (GBA) or the Worst-Group Accuracy. The proposed approach is validated on five vision and one NLP benchmarks, demonstrating competitive and often superior performance compared to state-of-the-art baseline methods.         ",
    "url": "https://arxiv.org/abs/2503.13236",
    "authors": [
      "Ihab Asaad",
      "Maha Shadaydeh",
      "Joachim Denzler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13240",
    "title": "Full-body NFC: body-scale near-field sensor networks with machine-knittable meandered e-textiles",
    "abstract": "           Wireless body networks comprising battery-free on-body sensors and textile-based wireless readers can enable daily health monitoring and activity tracking by continuously monitoring physiological signals across the body. However, previous textile-based wireless networks made of coils or antennas have limited the data and power transmission area because covering the whole body results in undesirable levels of electromagnetic interactions with the body, degrading the scalability, power consumption, and data rate. Here, we report Full-body NFC, digitally-knitted electronic textiles based on a twin meander coil design that enables body-scale near-field communication (NFC) with battery-free sensor tags arbitrarily placed around the body. Full-body NFC features i) a meander coil that enhances the magnetic field intensity on the body's surface while suppressing undesired interactions with deep tissues, in addition to ii) paired identical coil structure that enables highly-sensitive and motion-robust NFC using a differential architecture. Additionally, industrial digital knitting machines loaded with conductive yarn allow the integration of the Full-body NFC system into daily garments supporting approximately $70-80\\%$ large-scale NFC-enabled area of the body. We demonstrate Full-body NFC could achieve mW-class energy-efficient near-field sensor networks with hundreds of kbps-class NFC battery-free sensor tags occupying less than $0.3\\%$ of the coverage area under severe body movements.         ",
    "url": "https://arxiv.org/abs/2503.13240",
    "authors": [
      "Ryo Takahashi",
      "Changyo Han",
      "Wakako Yukita",
      "John S. Ho",
      "Takuya Sasatani",
      "Akihito Noda",
      "Tomoyuki Yokota",
      "Takao Someya",
      "Yoshihiro Kawahara"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.13248",
    "title": "Neural network-based Godunov corrections for approximate Riemann solvers using bi-fidelity learning",
    "abstract": "           The Riemann problem is fundamental in the computational modeling of hyperbolic partial differential equations, enabling the development of stable and accurate upwind schemes. While exact solvers provide robust upwinding fluxes, their high computational cost necessitates approximate solvers. Although approximate solvers achieve accuracy in many scenarios, they produce inaccurate solutions in certain cases. To overcome this limitation, we propose constructing neural network-based surrogate models, trained using supervised learning, designed to map interior and exterior conservative state variables to the corresponding exact flux. Specifically, we propose two distinct approaches: one utilizing a vanilla neural network and the other employing a bi-fidelity neural network. The performance of the proposed approaches is demonstrated through applications to one-dimensional and two-dimensional partial differential equations, showcasing their robustness and accuracy.         ",
    "url": "https://arxiv.org/abs/2503.13248",
    "authors": [
      "Akshay Thakur",
      "Matthew J. Zahr"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2503.13271",
    "title": "Graph Generative Models Evaluation with Masked Autoencoder",
    "abstract": "           In recent years, numerous graph generative models (GGMs) have been proposed. However, evaluating these models remains a considerable challenge, primarily due to the difficulty in extracting meaningful graph features that accurately represent real-world graphs. The traditional evaluation techniques, which rely on graph statistical properties like node degree distribution, clustering coefficients, or Laplacian spectrum, overlook node features and lack scalability. There are newly proposed deep learning-based methods employing graph random neural networks or contrastive learning to extract graph features, demonstrating superior performance compared to traditional statistical methods, but their experimental results also demonstrate that these methods do not always working well across different metrics. Although there are overlaps among these metrics, they are generally not interchangeable, each evaluating generative models from a different perspective. In this paper, we propose a novel method that leverages graph masked autoencoders to effectively extract graph features for GGM evaluations. We conduct extensive experiments on graphs and empirically demonstrate that our method can be more reliable and effective than previously proposed methods across a number of GGM evaluation metrics, such as \"Fr\u00e9chet Distance (FD)\" and \"MMD Linear\". However, no single method stands out consistently across all metrics and datasets. Therefore, this study also aims to raise awareness of the significance and challenges associated with GGM evaluation techniques, especially in light of recent advances in generative models.         ",
    "url": "https://arxiv.org/abs/2503.13271",
    "authors": [
      "Chengen Wang",
      "Murat Kantarcioglu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.13322",
    "title": "SMPR: A structure-enhanced multimodal drug-disease prediction model for drug repositioning and cold start",
    "abstract": "           Repositioning drug-disease relationships has always been a hot field of research. However, actual cases of biologically validated drug relocation remain very limited, and existing models have not yet fully utilized the structural information of the drug. Furthermore, most repositioning models are only used to complete the relationship matrix, and their practicality is poor when dealing with drug cold start problems. This paper proposes a structure-enhanced multimodal relationship prediction model (SMRP). SMPR is based on the SMILE structure of the drug, using the Mol2VEC method to generate drug embedded representations, and learn disease embedded representations through heterogeneous network graph neural networks. Ultimately, a drug-disease relationship matrix is constructed. In addition, to reduce the difficulty of users' use, SMPR also provides a cold start interface based on structural similarity based on reposition results to simply and quickly predict drug-related diseases. The repositioning ability and cold start capability of the model are verified from multiple perspectives. While the AUC and ACUPR scores of repositioning reach 99% and 61% respectively, the AUC of cold start achieve 80%. In particular, the cold start Recall indicator can reach more than 70%, which means that SMPR is more sensitive to positive samples. Finally, case analysis is used to verify the practical value of the model and visual analysis directly demonstrates the improvement of the structure to the model. For quick use, we also provide local deployment of the model and package it into an executable program.         ",
    "url": "https://arxiv.org/abs/2503.13322",
    "authors": [
      "Xin Dong",
      "Rui Miao",
      "Suyan Zhang",
      "Shuaibing Jia",
      "Leifeng Zhang",
      "Yong Liang",
      "Jianhua Zhang",
      "Yi Zhun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.13366",
    "title": "Follow-the-Regularized-Leader with Adversarial Constraints",
    "abstract": "           Constrained Online Convex Optimization (COCO) can be seen as a generalization of the standard Online Convex Optimization (OCO) framework. At each round, a cost function and constraint function are revealed after a learner chooses an action. The goal is to minimize both the regret and cumulative constraint violation (CCV) against an adaptive adversary. We show for the first time that is possible to obtain the optimal $O(\\sqrt{T})$ bound on both regret and CCV, improving the best known bounds of $O \\left( \\sqrt{T} \\right)$ and $\u00d5 \\left( \\sqrt{T} \\right)$ for the regret and CCV, respectively.         ",
    "url": "https://arxiv.org/abs/2503.13366",
    "authors": [
      "Ricardo N. Ferreira",
      "Cl\u00e1udia Soares"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.13389",
    "title": "Investigating the effect of CPT in lateral spreading prediction using Explainable AI",
    "abstract": "           This study proposes an autoencoder approach to extract latent features from cone penetration test profiles to evaluate the potential of incorporating CPT data in an AI model. We employ autoencoders to compress 200 CPT profiles of soil behavior type index (Ic) and normalized cone resistance (qc1Ncs) into ten latent features while preserving critical information. We then utilize the extracted latent features with site parameters to train XGBoost models for predicting lateral spreading occurrences in the 2011 Christchurch earthquake. Models using the latent CPT features outperformed models with conventional CPT metrics or no CPT data, achieving over 83% accuracy. Explainable AI revealed the most crucial latent feature corresponding to soil behavior between 1-3 meter depths, highlighting this depth range's criticality for liquefaction evaluation. The autoencoder approach provides an automated technique for condensing CPT profiles into informative latent features for machine-learning liquefaction models.         ",
    "url": "https://arxiv.org/abs/2503.13389",
    "authors": [
      "Cheng-Hsi Hsiao",
      "Ellen Rathje",
      "Krishna Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Geophysics (physics.geo-ph)"
    ]
  },
  {
    "id": "arXiv:2503.13395",
    "title": "Causal Emergence 2.0: Quantifying emergent complexity",
    "abstract": "           Complex systems can be described at myriad different scales, and their causal workings often have multiscale structure (e.g., a computer can be described at the microscale of its hardware circuitry, the mesoscale of its machine code, and the macroscale of its operating system). While scientists study and model systems across the full hierarchy of their scales, from microphysics to macroeconomics, there is debate about what the macroscales of systems can possibly add beyond mere compression. To resolve this longstanding issue, here a new theory of emergence is introduced wherein the different scales of a system are treated like slices of a higher-dimensional object. The theory can distinguish which of these scales possess unique causal contributions, and which are not causally relevant. Constructed from an axiomatic notion of causation, the theory's application is demonstrated in coarse-grains of Markov chains. It identifies all cases of macroscale causation: instances where reduction to a microscale is possible, yet lossy about causation. Furthermore, the theory posits a causal apportioning schema that calculates the causal contribution of each scale, showing what each uniquely adds. Finally, it reveals a novel measure of emergent complexity: how widely distributed a system's causal workings are across its hierarchy of scales.         ",
    "url": "https://arxiv.org/abs/2503.13395",
    "authors": [
      "Erik Hoel"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2503.13404",
    "title": "Fed-Joint: Joint Modeling of Nonlinear Degradation Signals and Failure Events for Remaining Useful Life Prediction using Federated Learning",
    "abstract": "           Many failure mechanisms of machinery are closely related to the behavior of condition monitoring (CM) signals. To achieve a cost-effective preventive maintenance strategy, accurate remaining useful life (RUL) prediction based on the signals is of paramount importance. However, the CM signals are often recorded at different factories and production lines, with limited amounts of data. Unfortunately, these datasets have rarely been shared between the sites due to data confidentiality and ownership issues, a lack of computing and storage power, and high communication costs associated with data transfer between sites and a data center. Another challenge in real applications is that the CM signals are often not explicitly specified \\textit{a priori}, meaning that existing methods, which often usually a parametric form, may not be applicable. To address these challenges, we propose a new prognostic framework for RUL prediction using the joint modeling of nonlinear degradation signals and time-to-failure data within a federated learning scheme. The proposed method constructs a nonparametric degradation model using a federated multi-output Gaussian process and then employs a federated survival model to predict failure times and probabilities for in-service machinery. The superiority of the proposed method over other alternatives is demonstrated through comprehensive simulation studies and a case study using turbofan engine degradation signal data that include run-to-failure events.         ",
    "url": "https://arxiv.org/abs/2503.13404",
    "authors": [
      "Cheoljoon Jeong",
      "Xubo Yue",
      "Seokhyun Chung"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.13409",
    "title": "A $(1+\u03b5)$-Approximation for Ultrametric Embedding in Subquadratic Time",
    "abstract": "           Efficiently computing accurate representations of high-dimensional data is essential for data analysis and unsupervised learning. Dendrograms, also known as ultrametrics, are widely used representations that preserve hierarchical relationships within the data. However, popular methods for computing them, such as linkage algorithms, suffer from quadratic time and space complexity, making them impractical for large datasets. The \"best ultrametric embedding\" (a.k.a. \"best ultrametric fit\") problem, which aims to find the ultrametric that best preserves the distances between points in the original data, is known to require at least quadratic time for an exact solution. Recent work has focused on improving scalability by approximating optimal solutions in subquadratic time, resulting in a $(\\sqrt{2} + \\epsilon)$-approximation (Cohen-Addad, de Joannis de Verclos and Lagarde, 2021). In this paper, we present the first subquadratic algorithm that achieves arbitrarily precise approximations of the optimal ultrametric embedding. Specifically, we provide an algorithm that, for any $c \\geq 1$, outputs a $c$-approximation of the best ultrametric in time $\\tilde{O}(n^{1 + 1/c})$. In particular, for any fixed $\\epsilon > 0$, the algorithm computes a $(1+\\epsilon)$-approximation in time $\\tilde{O}(n^{2 - \\epsilon + o(\\epsilon^2)})$. Experimental results show that our algorithm improves upon previous methods in terms of approximation quality while maintaining comparable running times.         ",
    "url": "https://arxiv.org/abs/2503.13409",
    "authors": [
      "Gabriel Bathie",
      "Guillaume Lagarde"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2503.13419",
    "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
    "abstract": "           The synergy between virtual reality (VR) and artificial intelligence (AI), specifically deep learning (DL)-based cybersickness detection models, has ushered in unprecedented advancements in immersive experiences by automatically detecting cybersickness severity and adaptively various mitigation techniques, offering a smooth and comfortable VR experience. While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks. In this paper, we present a new type of VR attack, i.e., a cybersickness attack, which successfully stops the triggering of cybersickness mitigation by fooling DL-based cybersickness detection models and dramatically hinders the UIX. Next, we propose a novel explainable artificial intelligence (XAI)-guided cybersickness attack detection framework to detect such attacks in VR to ensure UIX and a comfortable VR experience. We evaluate the proposed attack and the detection framework using two state-of-the-art open-source VR cybersickness datasets: Simulation 2021 and Gameplay dataset. Finally, to verify the effectiveness of our proposed method, we implement the attack and the XAI-based detection using a testbed with a custom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and perform a user study. Our study shows that such an attack can dramatically hinder the UIX. However, our proposed XAI-guided cybersickness attack detection can successfully detect cybersickness attacks and trigger the proper mitigation, effectively reducing VR cybersickness.         ",
    "url": "https://arxiv.org/abs/2503.13419",
    "authors": [
      "Ripan Kumar Kundu",
      "Matthew Denton",
      "Genova Mongalo",
      "Prasad Calyam",
      "Khaza Anuarul Hoque"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.13429",
    "title": "Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes",
    "abstract": "           With the rise of neural networks, especially in high-stakes applications, these networks need two properties (i) robustness and (ii) interpretability to ensure their safety. Recent advances in classifiers with 3D volumetric object representations have demonstrated a greatly enhanced robustness in out-of-distribution data. However, these 3D-aware classifiers have not been studied from the perspective of interpretability. We introduce CAVE - Concept Aware Volumes for Explanations - a new direction that unifies interpretability and robustness in image classification. We design an inherently-interpretable and robust classifier by extending existing 3D-aware classifiers with concepts extracted from their volumetric representations for classification. In an array of quantitative metrics for interpretability, we compare against different concept-based approaches across the explainable AI literature and show that CAVE discovers well-grounded concepts that are used consistently across images, while achieving superior robustness.         ",
    "url": "https://arxiv.org/abs/2503.13429",
    "authors": [
      "Nhi Pham",
      "Bernt Schiele",
      "Adam Kortylewski",
      "Jonas Fischer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13430",
    "title": "AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction",
    "abstract": "           Autonomous driving requires an understanding of the infrastructure elements, such as lanes and crosswalks. To navigate safely, this understanding must be derived from sensor data in real-time and needs to be represented in vectorized form. Learned Bird's-Eye View (BEV) encoders are commonly used to combine a set of camera images from multiple views into one joint latent BEV grid. Traditionally, from this latent space, an intermediate raster map is predicted, providing dense spatial supervision but requiring post-processing into the desired vectorized form. More recent models directly derive infrastructure elements as polylines using vectorized map decoders, providing instance-level information. Our approach, Augmentation Map Network (AugMapNet), proposes latent BEV grid augmentation, a novel technique that significantly enhances the latent BEV representation. AugMapNet combines vector decoding and dense spatial supervision more effectively than existing architectures while remaining as straightforward to integrate and as generic as auxiliary supervision. Experiments on nuScenes and Argoverse2 datasets demonstrate significant improvements in vectorized map prediction performance up to 13.3% over the StreamMapNet baseline on 60m range and greater improvements on larger ranges. We confirm transferability by applying our method to another baseline and find similar improvements. A detailed analysis of the latent BEV grid confirms a more structured latent space of AugMapNet and shows the value of our novel concept beyond pure performance improvement. The code will be released soon.         ",
    "url": "https://arxiv.org/abs/2503.13430",
    "authors": [
      "Thomas Monninger",
      "Md Zafar Anwar",
      "Stanislaw Antol",
      "Steffen Staab",
      "Sihao Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.13431",
    "title": "Measuring In-Context Computation Complexity via Hidden State Prediction",
    "abstract": "           Detecting when a neural sequence model does \"interesting\" computation is an open problem. The next token prediction loss is a poor indicator: Low loss can stem from trivially predictable sequences that are uninteresting, while high loss may reflect unpredictable but also irrelevant information that can be ignored by the model. We propose a better metric: measuring the model's ability to predict its own future hidden states. We show empirically that this metric -- in contrast to the next token prediction loss -- correlates with the intuitive interestingness of the task. To measure predictability, we introduce the architecture-agnostic \"prediction of hidden states\" (PHi) layer that serves as an information bottleneck on the main pathway of the network (e.g., the residual stream in Transformers). We propose a novel learned predictive prior that enables us to measure the novel information gained in each computation step, which serves as our metric. We show empirically that our metric predicts the description length of formal languages learned in-context, the complexity of mathematical reasoning problems, and the correctness of self-generated reasoning chains.         ",
    "url": "https://arxiv.org/abs/2503.13431",
    "authors": [
      "Vincent Herrmann",
      "R\u00f3bert Csord\u00e1s",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12044",
    "title": "A Simple and Explainable Model for Park-and-Ride Car Park Occupancy Prediction",
    "abstract": "           In a scenario of growing usage of park-and-ride facilities, understanding and predicting car park occupancy is becoming increasingly important. This study presents a model that effectively captures the occupancy patterns of park-and-ride car parks for commuters using truncated normal distributions for vehicle arrival and departure times. The objective is to develop a predictive model with minimal parameters corresponding to commuter behaviour, enabling the estimation of parking saturation and unfulfilled demand. The proposed model successfully identifies the regular, periodic nature of commuter parking behaviour, where vehicles arrive in the morning and depart in the afternoon. It operates using aggregate data, eliminating the need for individual tracking of arrivals and departures. The model's predictive and now-casting capabilities are demonstrated through real-world data from car parks in the Barcelona Metropolitan Area. A simple model extension furthermore enables the prediction of when a car park will reach its occupancy limit and estimates the additional spaces required to accommodate such excess demand. Thus, beyond forecasting, the model serves as a valuable tool for evaluating interventions, such as expanding parking capacity, to optimize park-and-ride facilities.         ",
    "url": "https://arxiv.org/abs/2503.12044",
    "authors": [
      "Andreas Kaltenbrunner",
      "Josep Ferrer",
      "David Moreno",
      "Vicen\u00e7 G\u00f3mez"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Emerging Technologies (cs.ET)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2503.12045",
    "title": "Auditing Differential Privacy in the Black-Box Setting",
    "abstract": "           This paper introduces a novel theoretical framework for auditing differential privacy (DP) in a black-box setting. Leveraging the concept of $f$-differential privacy, we explicitly define type I and type II errors and propose an auditing mechanism based on conformal inference. Our approach robustly controls the type I error rate under minimal assumptions. Furthermore, we establish a fundamental impossibility result, demonstrating the inherent difficulty of simultaneously controlling both type I and type II errors without additional assumptions. Nevertheless, under a monotone likelihood ratio (MLR) assumption, our auditing mechanism effectively controls both errors. We also extend our method to construct valid confidence bands for the trade-off function in the finite-sample regime.         ",
    "url": "https://arxiv.org/abs/2503.12045",
    "authors": [
      "Kaining Shi",
      "Cong Ma"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.12276",
    "title": "Quantum-enhanced quickest change detection of transmission loss",
    "abstract": "           A sudden increase of loss in an optical communications channel can be caused by a malicious wiretapper, or for a benign reason such as inclement weather in a free-space channel or an unintentional bend in an optical fiber. We show that adding a small amount of squeezing to bright phase-modulated coherent-state pulses can dramatically increase the homodyne detection receiver's sensitivity to change detection in channel loss, without affecting the communications rate. We further show that augmenting blocks of $n$ pulses of a coherent-state codeword with weak continuous-variable entanglement generated by splitting squeezed vacuum pulses in a temporal $n$-mode equal splitter progressively enhances this change-detection sensitivity as $n$ increases; the aforesaid squeezed-light augmentation being the $n=1$ special case. For $n$ high enough, an arbitrarily small amount of quantum-augmented photons per pulse diminishes the change-detection latency by the inverse of the pre-detection channel loss. This superadditivity-like phenomenon in the entanglement-augmented relative entropy rate, which quantifies the latency of change-point detection, may find other uses. We discuss the quantum limit of quickest change detection and a receiver that achieves it, tradeoffs between continuous and discrete-variable quantum augmentation, and the broad problem of joint classical-and-quantum communications and channel-change-detection that our study opens up.         ",
    "url": "https://arxiv.org/abs/2503.12276",
    "authors": [
      "Saikat Guha",
      "Tiju Cherian John",
      "Prithwish Basu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2503.12334",
    "title": "When neural implant meets multimodal LLM: A dual-loop system for neuromodulation and naturalistic neuralbehavioral research",
    "abstract": "           We propose a novel dual-loop system that synergistically combines responsive neurostimulation (RNS) implants with artificial intelligence-driven wearable devices for treating post-traumatic stress disorder (PTSD) and enabling naturalistic brain research. In PTSD Therapy Mode, an implanted closed-loop neural device monitors amygdala activity and provides on-demand stimulation upon detecting pathological theta oscillations, while an ensemble of wearables (smart glasses, smartwatches, smartphones) uses multimodal large language model (LLM) analysis of sensory data to detect environmental or physiological PTSD triggers and deliver timely audiovisual interventions. Logged events from both the neural and wearable loops are analyzed to personalize trigger detection and progressively transition patients to non-invasive interventions. In Neuroscience Research Mode, the same platform is adapted for real-world brain activity capture. Wearable-LLM systems recognize naturalistic events (social interactions, emotional situations, compulsive behaviors, decision making) and signal implanted RNS devices (via wireless triggers) to record synchronized intracranial data during these moments. This approach builds on recent advances in mobile intracranial EEG recording and closed-loop neuromodulation in humans (BRAIN Initiative, 2023) (Mobbs et al., 2021). We discuss how our interdisciplinary system could revolutionize PTSD therapy and cognitive neuroscience by enabling 24/7 monitoring, context-aware intervention, and rich data collection outside traditional labs. The vision is a future where AI-enhanced devices continuously collaborate with the human brain, offering therapeutic support and deep insights into neural function, with the resulting real-world context rich neural data, in turn, accelerating the development of more biologically-grounded and human-centric AI.         ",
    "url": "https://arxiv.org/abs/2503.12334",
    "authors": [
      "Edward Hong Wang",
      "Cynthia Xin Wen"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.12368",
    "title": "SCReedSolo: A Secure and Robust LSB Image Steganography Framework with Randomized Symmetric Encryption and Reed-Solomon Coding",
    "abstract": "           Image steganography is an information-hiding technique that involves the surreptitious concealment of covert informational content within digital images. In this paper, we introduce ${\\rm SCR{\\small EED}S{\\small OLO}}$, a novel framework for concealing arbitrary binary data within images. Our approach synergistically leverages Random Shuffling, Fernet Symmetric Encryption, and Reed-Solomon Error Correction Codes to encode the secret payload, which is then discretely embedded into the carrier image using LSB (Least Significant Bit) Steganography. The combination of these methods addresses the vulnerability vectors of both security and resilience against bit-level corruption in the resultant stego-images. We show that our framework achieves a data payload of 3 bits per pixel for an RGB image, and mathematically assess the probability of successful transmission for the amalgamated $n$ message bits and $k$ error correction bits. Additionally, we find that ${\\rm SCR{\\small EED}S{\\small OLO}}$ yields good results upon being evaluated with multiple performance metrics, successfully eludes detection by various passive steganalysis tools, and is immune to simple active steganalysis attacks. Our code and data are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.12368",
    "authors": [
      "Syed Rifat Raiyan",
      "Md. Hasanul Kabir"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Cryptography and Security (cs.CR)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.12509",
    "title": "A Reservoir-based Model for Human-like Perception of Complex Rhythm Pattern",
    "abstract": "           Rhythm is a fundamental aspect of human behaviour, present from infancy and deeply embedded in cultural practices. Rhythm anticipation is a spontaneous cognitive process that typically occurs before the onset of actual beats. While most research in both neuroscience and artificial intelligence has focused on metronome-based rhythm tasks, studies investigating the perception of complex musical rhythm patterns remain limited. To address this gap, we propose a hierarchical oscillator-based model to better understand the perception of complex musical rhythms in biological systems. The model consists of two types of coupled neurons that generate oscillations, with different layers tuned to respond to distinct perception levels. We evaluate the model using several representative rhythm patterns spanning the upper, middle, and lower bounds of human musical perception. Our findings demonstrate that, while maintaining a high degree of synchronization accuracy, the model exhibits human-like rhythmic behaviours. Additionally, the beta band neuronal activity in the model mirrors patterns observed in the human brain, further validating the biological plausibility of the approach.         ",
    "url": "https://arxiv.org/abs/2503.12509",
    "authors": [
      "Zhongju Yuan",
      "Geraint Wiggins",
      "Dick Botteldooren"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.12582",
    "title": "A Modular Quantum Network Architecture for Integrating Network Scheduling with Local Program Execution",
    "abstract": "           We propose an architecture for scheduling network operations enabling the end-to-end generation of entanglement according to user demand. The main challenge solved by this architecture is to allow for the integration of a network schedule with the execution of quantum programs running on processing end nodes in order to realise quantum network applications. A key element of this architecture is the definition of an entanglement packet to meet application requirements on near-term quantum networks where the lifetimes of the qubits stored at the end nodes are limited. Our architecture is fully modular and hardware agnostic, and defines a framework for further research on specific components that can now be developed independently of each other. In order to evaluate our architecture, we realise a proof of concept implementation on a simulated 6-node network in a star topology. We show our architecture facilitates the execution of quantum network applications, and that robust admission control is required to maintain quality of service. Finally, we comment on potential bottlenecks in our architecture and provide suggestions for future improvements.         ",
    "url": "https://arxiv.org/abs/2503.12582",
    "authors": [
      "Thomas R. Beauchamp",
      "Hana Jirovsk\u00e1",
      "Scarlett Gauthier",
      "Stephanie Wehner"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.12784",
    "title": "Causal Feature Learning in the Social Sciences",
    "abstract": "           Variable selection poses a significant challenge in causal modeling, particularly within the social sciences, where constructs often rely on inter-related factors such as age, socioeconomic status, gender, and race. Indeed, it has been argued that such attributes must be modeled as macro-level abstractions of lower-level manipulable features, in order to preserve the modularity assumption essential to causal inference. This paper accordingly extends the theoretical framework of Causal Feature Learning (CFL). Empirically, we apply the CFL algorithm to diverse social science datasets, evaluating how CFL-derived macrostates compare with traditional microstates in downstream modeling tasks.         ",
    "url": "https://arxiv.org/abs/2503.12784",
    "authors": [
      "Jingzhou Huang",
      "Jiuyao Lu",
      "Alexander Williams Tolbert"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2503.12907",
    "title": "Robust Deep Joint Source Channel Coding for Task-Oriented Semantic Communications",
    "abstract": "           Semantic communications based on deep joint source-channel coding (JSCC) aim to improve communication efficiency by transmitting only task-relevant information. However, ensuring robustness to the stochasticity of communication channels remains a key challenge in learning-based JSCC. In this paper, we propose a novel regularization technique for learning-based JSCC to enhance robustness against channel noise. The proposed method utilizes the Kullback-Leibler (KL) divergence as a regularizer term in the training loss, measuring the discrepancy between two posterior distributions: one under noisy channel conditions (noisy posterior) and one for a noise-free system (noise-free posterior). Reducing this KL divergence mitigates the impact of channel noise on task performance by keeping the noisy posterior close to the noise-free posterior. We further show that the expectation of the KL divergence given the encoded representation can be analytically approximated using the Fisher information matrix and the covariance matrix of the channel noise. Notably, the proposed regularization is architecture-agnostic, making it broadly applicable to general semantic communication systems over noisy channels. Our experimental results validate that the proposed regularization consistently improves task performance across diverse semantic communication systems and channel conditions.         ",
    "url": "https://arxiv.org/abs/2503.12907",
    "authors": [
      "Taewoo Park",
      "Eunhye Hong",
      "Yo-Seb Jeon",
      "Namyoon Lee",
      "Yongjune Kim"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2503.13050",
    "title": "E-Values Expand the Scope of Conformal Prediction",
    "abstract": "           Conformal prediction is a powerful framework for distribution-free uncertainty quantification. The standard approach to conformal prediction relies on comparing the ranks of prediction scores: under exchangeability, the rank of a future test point cannot be too extreme relative to a calibration set. This rank-based method can be reformulated in terms of p-values. In this paper, we explore an alternative approach based on e-values, known as conformal e-prediction. E-values offer key advantages that cannot be achieved with p-values, enabling new theoretical and practical capabilities. In particular, we present three applications that leverage the unique strengths of e-values: batch anytime-valid conformal prediction, fixed-size conformal sets with data-dependent coverage, and conformal prediction under ambiguous ground truth. Overall, these examples demonstrate that e-value-based constructions provide a flexible expansion of the toolbox of conformal prediction.         ",
    "url": "https://arxiv.org/abs/2503.13050",
    "authors": [
      "Etienne Gauthier",
      "Francis Bach",
      "Michael I. Jordan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.13316",
    "title": "RainScaleGAN: a Conditional Generative Adversarial Network for Rainfall Downscaling",
    "abstract": "           To this day, accurately simulating local-scale precipitation and reliably reproducing its distribution remains a challenging task. The limited horizontal resolution of Global Climate Models is among the primary factors undermining their skill in this context. The physical mechanisms driving the onset and development of precipitation, especially in extreme events, operate at spatio-temporal scales smaller than those numerically resolved, thus struggling to be captured accurately. In order to circumvent this limitation, several downscaling approaches have been developed over the last decades to address the discrepancy between the spatial resolution of models output and the resolution required by local-scale applications. In this paper, we introduce RainScaleGAN, a conditional deep convolutional Generative Adversarial Network (GAN) for precipitation downscaling. GANs have been effectively used in image super-resolution, an approach highly relevant for downscaling tasks. RainScaleGAN's capabilities are tested in a perfect-model setup, where the spatial resolution of a precipitation dataset is artificially degraded from 0.25$^{\\circ}\\times$0.25$^{\\circ}$ to 2$^{\\circ}\\times$2$^\\circ$, and RainScaleGAN is used to restore it. The developed model outperforms one of the leading precipitation downscaling method found in the literature. RainScaleGAN not only generates a synthetic dataset featuring plausible high-resolution spatial patterns and intensities, but also produces a precipitation distribution with statistics closely mirroring those of the ground-truth dataset. Given that RainScaleGAN's approach is agnostic with respect to the underlying physics, the method has the potential to be applied to other physical variables such as surface winds or temperature.         ",
    "url": "https://arxiv.org/abs/2503.13316",
    "authors": [
      "Marcello Iotti",
      "Paolo Davini",
      "Jost von Hardenberg",
      "Giuseppe Zappa"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.13367",
    "title": "Mixed Small Gain and Phase Theorem: A new view using Scale Relative Graphs",
    "abstract": "           We introduce a novel approach to feedback stability analysis for linear time-invariant (LTI) systems, overcoming the limitations of the sectoriality assumption in the small phase theorem. While phase analysis for single-input single-output (SISO) systems is well-established, multi-input multi-output (MIMO) systems lack a comprehensive phase analysis until recent advances introduced with the small-phase theorem. A limitation of the small-phase theorem is the sectorial condition, which states that an operator's eigenvalues must lie within a specified angle sector of the complex plane. We propose a framework based on Scaled Relative Graphs (SRGs) to remove this assumption. We derive two main results: a graphical set-based stability condition using SRGs and a small-phase theorem with no sectorial assumption. These results broaden the scope of phase analysis and feedback stability for MIMO systems.         ",
    "url": "https://arxiv.org/abs/2503.13367",
    "authors": [
      "Eder Baron-Prada",
      "Adolfo Anta",
      "Alberto Padoan",
      "Florian D\u00f6rfler"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.13400",
    "title": "U2AD: Uncertainty-based Unsupervised Anomaly Detection Framework for Detecting T2 Hyperintensity in MRI Spinal Cord",
    "abstract": "           T2 hyperintensities in spinal cord MR images are crucial biomarkers for conditions such as degenerative cervical myelopathy. However, current clinical diagnoses primarily rely on manual evaluation. Deep learning methods have shown promise in lesion detection, but most supervised approaches are heavily dependent on large, annotated datasets. Unsupervised anomaly detection (UAD) offers a compelling alternative by eliminating the need for abnormal data annotations. However, existing UAD methods rely on curated normal datasets and their performance frequently deteriorates when applied to clinical datasets due to domain shifts. We propose an Uncertainty-based Unsupervised Anomaly Detection framework, termed U2AD, to address these limitations. Unlike traditional methods, U2AD is designed to be trained and tested within the same clinical dataset, following a \"mask-and-reconstruction\" paradigm built on a Vision Transformer-based architecture. We introduce an uncertainty-guided masking strategy to resolve task conflicts between normal reconstruction and anomaly detection to achieve an optimal balance. Specifically, we employ a Monte-Carlo sampling technique to estimate reconstruction uncertainty mappings during training. By iteratively optimizing reconstruction training under the guidance of both epistemic and aleatoric uncertainty, U2AD reduces overall reconstruction variance while emphasizing regions. Experimental results demonstrate that U2AD outperforms existing supervised and unsupervised methods in patient-level identification and segment-level localization tasks. This framework establishes a new benchmark for incorporating uncertainty guidance into UAD, highlighting its clinical utility in addressing domain shifts and task conflicts in medical image anomaly detection. Our code is available: this https URL ",
    "url": "https://arxiv.org/abs/2503.13400",
    "authors": [
      "Qi Zhang",
      "Xiuyuan Chen",
      "Ziyi He",
      "Kun Wang",
      "Lianming Wu",
      "Hongxing Shen",
      "Jianqi Sun"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:1911.07308",
    "title": "Counterfactual Vision-and-Language Navigation via Adversarial Path Sampling",
    "abstract": "           Vision-and-Language Navigation (VLN) is a task where agents must decide how to move through a 3D environment to reach a goal by grounding natural language instructions to the visual surroundings. One of the problems of the VLN task is data scarcity since it is difficult to collect enough navigation paths with human-annotated instructions for interactive environments. In this paper, we explore the use of counterfactual thinking as a human-inspired data augmentation method that results in robust models. Counterfactual thinking is a concept that describes the human propensity to create possible alternatives to life events that have already occurred. We propose an adversarial-driven counterfactual reasoning model that can consider effective conditions instead of low-quality augmented data. In particular, we present a model-agnostic adversarial path sampler (APS) that learns to sample challenging paths that force the navigator to improve based on the navigation performance. APS also serves to do pre-exploration of unseen environments to strengthen the model's ability to generalize. We evaluate the influence of APS on the performance of different VLN baseline models using the room-to-room dataset (R2R). The results show that the adversarial training process with our proposed APS benefits VLN models under both seen and unseen environments. And the pre-exploration process can further gain additional improvements under unseen environments.         ",
    "url": "https://arxiv.org/abs/1911.07308",
    "authors": [
      "Tsu-Jui Fu",
      "Xin Eric Wang",
      "Matthew Peterson",
      "Scott Grafton",
      "Miguel Eckstein",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2009.09566",
    "title": "SSCR: Iterative Language-Based Image Editing via Self-Supervised Counterfactual Reasoning",
    "abstract": "           Iterative Language-Based Image Editing (IL-BIE) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes. However, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair. Such ability results from counterfactual thinking and the ability to think about alternatives to events that have happened already. In this paper, we introduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that incorporates counterfactual thinking to overcome data scarcity. SSCR allows the model to consider out-of-distribution instructions paired with previous images. With the help of cross-task consistency (CTC), we train these counterfactual instructions in a self-supervised scenario. Extensive results show that SSCR improves the correctness of ILBIE in terms of both object identity and position, establishing a new state of the art (SOTA) on two IBLIE datasets (i-CLEVR and CoDraw). Even with only 50% of the training data, SSCR achieves a comparable result to using complete data.         ",
    "url": "https://arxiv.org/abs/2009.09566",
    "authors": [
      "Tsu-Jui Fu",
      "Xin Eric Wang",
      "Scott Grafton",
      "Miguel Eckstein",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2202.09134",
    "title": "Gaussian and Non-Gaussian Universality of Data Augmentation",
    "abstract": "           We provide universality results that quantify how data augmentation affects the variance and limiting distribution of estimates through simple surrogates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. As our main theoretical tool, we develop an adaptation of Lindeberg's technique for block dependence. The resulting universality regime may be Gaussian or non-Gaussian.         ",
    "url": "https://arxiv.org/abs/2202.09134",
    "authors": [
      "Kevin Han Huang",
      "Peter Orbanz",
      "Morgane Austern"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2210.16371",
    "title": "Distributed Black-box Attack: Do Not Overestimate Black-box Attacks",
    "abstract": "           As cloud computing becomes pervasive, deep learning models are deployed on cloud servers and then provided as APIs to end users. However, black-box adversarial attacks can fool image classification models without access to model structure and weights. Recent studies have reported attack success rates of over 95% with fewer than 1,000 queries. Then the question arises: whether black-box attacks have become a real threat against cloud APIs? To shed some light on this, our research indicates that black-box attacks are not as effective against cloud APIs as proposed in research papers due to several common mistakes that overestimate the efficiency of black-box attacks. To avoid similar mistakes, we conduct black-box attacks directly on cloud APIs rather than local models.         ",
    "url": "https://arxiv.org/abs/2210.16371",
    "authors": [
      "Han Wu",
      "Sareh Rowlands",
      "Johan Wahlstrom"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.09019",
    "title": "Tensor Networks Meet Neural Networks: A Survey and Future Perspectives",
    "abstract": "           Tensor networks (TNs) and neural networks (NNs) are two fundamental data modeling approaches. TNs were introduced to solve the curse of dimensionality in large-scale tensors by converting an exponential number of dimensions to polynomial complexity. As a result, they have attracted significant attention in the fields of quantum physics and machine learning. Meanwhile, NNs have displayed exceptional performance in various applications, e.g., computer vision, natural language processing, and robotics research. Interestingly, although these two types of networks originate from different observations, they are inherently linked through the typical multilinearity structure underlying both TNs and NNs, thereby motivating a significant number of developments regarding combinations of TNs and NNs. In this paper, we refer to these combinations as tensorial neural networks~(TNNs) and present an introduction to TNNs from both data processing and model architecture perspectives. From the data perspective, we explore the capabilities of TNNs in multi-source fusion, multimodal pooling, data compression, multi-task training, and quantum data processing. From the model perspective, we examine TNNs' integration with various architectures, including Convolutional Neural Networks, Recurrent Neural Networks, Graph Neural Networks, Transformers, Large Language Models, and Quantum Neural Networks. Furthermore, this survey also explores methods for improving TNNs, examines flexible toolboxes for implementing TNNs, and documents TNN development while highlighting potential future directions. To the best of our knowledge, this is the first comprehensive survey that bridges the connections among NNs and TNs. We provide a curated list of TNNs at this https URL.         ",
    "url": "https://arxiv.org/abs/2302.09019",
    "authors": [
      "Maolin Wang",
      "Yu Pan",
      "Zenglin Xu",
      "Guangxi Li",
      "Xiangli Yang",
      "Danilo Mandic",
      "Andrzej Cichocki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.13584",
    "title": "A Model Stealing Attack Against Multi-Exit Networks",
    "abstract": "           Compared to traditional neural networks with a single output channel, a multi-exit network has multiple exits that allow for early outputs from the model's intermediate layers, thus significantly improving computational efficiency while maintaining similar main task accuracy. Existing model stealing attacks can only steal the model's utility while failing to capture its output strategy, i.e., a set of thresholds used to determine from which exit to output. This leads to a significant decrease in computational efficiency for the extracted model, thereby losing the advantage of multi-exit networks. In this paper, we propose the first model stealing attack against multi-exit networks to extract both the model utility and the output strategy. We employ Kernel Density Estimation to analyze the target model's output strategy and use performance loss and strategy loss to guide the training of the extracted model. Furthermore, we design a novel output strategy search algorithm to maximize the consistency between the victim model and the extracted model's output behaviors. In experiments across multiple multi-exit networks and benchmark datasets, our method always achieves accuracy and efficiency closest to the victim models.         ",
    "url": "https://arxiv.org/abs/2305.13584",
    "authors": [
      "Li Pan",
      "Lv Peizhuo",
      "Chen Kai",
      "Zhang Shengzhi",
      "Cai Yuling",
      "Xiang Fan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.01697",
    "title": "Exploring Robustness of Image Recognition Models on Hardware Accelerators",
    "abstract": "           As the usage of Artificial Intelligence (AI) on resource-intensive and safety-critical tasks increases, a variety of Machine Learning (ML) compilers have been developed, enabling compatibility of Deep Neural Networks (DNNs) with a variety of hardware acceleration devices. However, given that DNNs are widely utilized for challenging and demanding tasks, the behavior of these compilers must be verified. To this direction, we propose MutateNN, a tool that utilizes elements of both differential and mutation testing in order to examine the robustness of image recognition models when deployed on hardware accelerators with different capabilities, in the presence of faults in their target device code - introduced either by developers, or problems in their compilation process. We focus on the image recognition domain by applying mutation testing to 7 well-established DNN models, introducing 21 mutations of 6 different categories. We deployed our mutants on 4 different hardware acceleration devices of varying capabilities and observed that DNN models presented discrepancies of up to 90.3% in mutants related to conditional operators across devices. We also observed that mutations related to layer modification, arithmetic types and input affected severely the overall model performance (up to 99.8%) or led to model crashes, in a consistent manner across devices.         ",
    "url": "https://arxiv.org/abs/2306.01697",
    "authors": [
      "Nikolaos Louloudakis",
      "Perry Gibson",
      "Jos\u00e9 Cano",
      "Ajitha Rajan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2308.13934",
    "title": "Patch-Grid: An Efficient and Feature-Preserving Neural Implicit Surface Representation",
    "abstract": "           Neural implicit representations are widely used for 3D shape modeling due to their smoothness and compactness, but traditional MLP-based methods struggle with sharp features, such as edges and corners in CAD models, and require long training times. To address these limitations, we propose Patch-Grid, a unified neural implicit representation that efficiently fits complex shapes, preserves sharp features, and handles open boundaries and thin geometric structures. Patch-Grid learns a signed distance field (SDF) for each surface patch using a learnable patch feature volume. To represent sharp edges and corners, it merges the learned SDFs via constructive solid geometry (CSG) operations. A novel merge grid organizes patch feature volumes within a shared octree structure, localizing and simplifying CSG operations. This design ensures robust merging of SDFs and significantly reduces computational complexity, enabling training within seconds while maintaining high fidelity. Experimental results show that Patch-Grid achieves state-of-the-art reconstruction quality for shapes with intricate sharp features, open surfaces, and thin structures, offering superior robustness, efficiency, and accuracy.         ",
    "url": "https://arxiv.org/abs/2308.13934",
    "authors": [
      "Guying Lin",
      "Lei Yang",
      "Congyi Zhang",
      "Hao Pan",
      "Yuhan Ping",
      "Guodong Wei",
      "Taku Komura",
      "John Keyser",
      "Wenping Wang"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2309.01102",
    "title": "CARNet: Collaborative Adversarial Resilience for Robust Underwater Image Enhancement and Perception",
    "abstract": "           Due to the uneven absorption of different light wavelengths in aquatic environments, underwater images suffer from low visibility and clear color deviations. With the advancement of autonomous underwater vehicles, extensive research has been conducted on learning-based underwater enhancement algorithms. These works can generate visually pleasing enhanced images and mitigate the adverse effects of degraded images on subsequent perception tasks. However, learning-based methods are susceptible to the inherent fragility of adversarial attacks, causing significant disruption in enhanced results. In this work, we introduce a collaborative adversarial resilience network, dubbed CARNet, for underwater image enhancement and subsequent detection tasks. Concretely, we first introduce an invertible network with strong perturbation-perceptual abilities to isolate attacks from underwater images, preventing interference with visual quality enhancement and perceptual tasks. Furthermore, an attack pattern discriminator is introduced to adaptively identify and eliminate various types of attacks. Additionally, we propose a bilevel attack optimization strategy to heighten the robustness of the network against different types of attacks under the collaborative adversarial training of vision-driven and perception-driven attacks. Extensive experiments demonstrate that the proposed method outputs visually appealing enhancement images and performs an average 6.71% higher detection mAP than state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2309.01102",
    "authors": [
      "Zengxi Zhang",
      "Zeru Shi",
      "Zhiying Jiang",
      "Jinyuan Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2309.07492",
    "title": "Robust Model Reductions for the Boundary Feedback Stabilization of Magnetizable Piezoelectric Beams",
    "abstract": "           Magnetizable piezoelectric beams exhibit strong couplings between mechanical, electric, and magnetic fields, significantly affecting their high-frequency vibrational behavior. Ensuring exponential stability under boundary feedback controllers is challenging due to the uneven distribution of high-frequency eigenvalues in standard Finite Difference models. While numerical filtering can mitigate instability as the discretization parameter tends to zero, its reliance on explicit spectral computations is computationally demanding. This work introduces two novel model reduction techniques for stabilizing magnetizable piezoelectric beams. First, a Finite Element discretization using linear splines is developed, improving numerical stability over standard Finite Differences. However, this method still requires numerical filtering to eliminate spurious high-frequency modes, necessitating full spectral decomposition. Numerical investigations further reveal a direct dependence of the optimal filtering threshold on feedback amplifiers. To overcome these limitations, an alternative order-reduction Finite Difference scheme is proposed, eliminating the need for numerical filtering. Using a Lyapunov-based framework, we establish exponential stability with decay rates independent of the discretization parameter. The reduced model also exhibits exponential error decay and uniform energy convergence to the original system. Numerical simulations validate the effectiveness of the proposed methods, and we construct an algorithm for separating eigenpairs for the proper application of the numerical filtering. Comparative spectral analyses and energy decay results confirm the superior stability and efficiency of the proposed approach, providing a robust framework for model reduction in coupled partial differential equation systems.         ",
    "url": "https://arxiv.org/abs/2309.07492",
    "authors": [
      "Ahmet Kaan Aydin",
      "Ahmet Ozkan Ozer",
      "Jacob Walterman"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2311.16912",
    "title": "Continuous optimization methods for the graph isomorphism problem",
    "abstract": "           The graph isomorphism problem looks deceptively simple, but although polynomial-time algorithms exist for certain types of graphs such as planar graphs and graphs with bounded degree or eigenvalue multiplicity, its complexity class is still unknown. Information about potential isomorphisms between two graphs is contained in the eigenvalues and eigenvectors of their adjacency matrices. However, symmetries of graphs often lead to repeated eigenvalues so that associated eigenvectors are determined only up to basis rotations, which complicates graph isomorphism testing. We consider orthogonal and doubly stochastic relaxations of the graph isomorphism problem, analyze the geometric properties of the resulting solution spaces, and show that their complexity increases significantly if repeated eigenvalues exist. By restricting the search space to suitable subspaces, we derive an efficient Frank-Wolfe based continuous optimization approach for detecting isomorphisms. We illustrate the efficacy of the algorithm with the aid of various highly symmetric graphs.         ",
    "url": "https://arxiv.org/abs/2311.16912",
    "authors": [
      "Stefan Klus",
      "Patrick Gel\u00df"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2312.00846",
    "title": "NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance",
    "abstract": "           Existing neural implicit surface reconstruction methods have achieved impressive performance in multi-view 3D reconstruction by leveraging explicit geometry priors such as depth maps or point clouds as regularization. However, the reconstruction results still lack fine details because of the over-smoothed depth map or sparse point cloud. In this work, we propose a neural implicit surface reconstruction pipeline with guidance from 3D Gaussian Splatting to recover highly detailed surfaces. The advantage of 3D Gaussian Splatting is that it can generate dense point clouds with detailed structure. Nonetheless, a naive adoption of 3D Gaussian Splatting can fail since the generated points are the centers of 3D Gaussians that do not necessarily lie on the surface. We thus introduce a scale regularizer to pull the centers close to the surface by enforcing the 3D Gaussians to be extremely thin. Moreover, we propose to refine the point cloud from 3D Gaussians Splatting with the normal priors from the surface predicted by neural implicit models instead of using a fixed set of points as guidance. Consequently, the quality of surface reconstruction improves from the guidance of the more accurate 3D Gaussian splatting. By jointly optimizing the 3D Gaussian Splatting and the neural implicit model, our approach benefits from both representations and generates complete surfaces with intricate details. Experiments on Tanks and Temples verify the effectiveness of our proposed method.         ",
    "url": "https://arxiv.org/abs/2312.00846",
    "authors": [
      "Hanlin Chen",
      "Chen Li",
      "Yunsong Wang",
      "Gim Hee Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.06085",
    "title": "SFDM: Robust Decomposition of Geometry and Reflectance for Realistic Face Rendering from Sparse-view Images",
    "abstract": "           In this study, we introduce a novel two-stage technique for decomposing and reconstructing facial features from sparse-view images, a task made challenging by the unique geometry and complex skin reflectance of each individual. To synthesize 3D facial models more realistically, we endeavor to decouple key facial attributes from the RGB color, including geometry, diffuse reflectance, and specular reflectance. Specifically, we design a Sparse-view Face Decomposition Model (SFDM): 1) In the first stage, we create a general facial template from a wide array of individual faces, encapsulating essential geometric and reflectance characteristics. 2) Guided by this template, we refine a specific facial model for each individual in the second stage, considering the interaction between geometry and reflectance, as well as the effects of subsurface scattering on the skin. With these advances, our method can reconstruct high-quality facial representations from as few as three images. The comprehensive evaluation and comparison reveal that our approach outperforms existing methods by effectively disentangling geometric and reflectance components, significantly enhancing the quality of synthesized novel views, and paving the way for applications in facial relighting and reflectance editing.         ",
    "url": "https://arxiv.org/abs/2312.06085",
    "authors": [
      "Daisheng Jin",
      "Jiangbei Hu",
      "Baixin Xu",
      "Yuxin Dai",
      "Chen Qian",
      "Ying He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.10766",
    "title": "JailGuard: A Universal Detection Framework for LLM Prompt-based Attacks",
    "abstract": "           The systems and software powered by Large Language Models (LLMs) and Multi-Modal LLMs (MLLMs) have played a critical role in numerous scenarios. However, current LLM systems are vulnerable to prompt-based attacks, with jailbreaking attacks enabling the LLM system to generate harmful content, while hijacking attacks manipulate the LLM system to perform attacker-desired tasks, underscoring the necessity for detection tools. Unfortunately, existing detecting approaches are usually tailored to specific attacks, resulting in poor generalization in detecting various attacks across different modalities. To address it, we propose JailGuard, a universal detection framework deployed on top of LLM systems for prompt-based attacks across text and image modalities. JailGuard operates on the principle that attacks are inherently less robust than benign ones. Specifically, JailGuard mutates untrusted inputs to generate variants and leverages the discrepancy of the variants' responses on the target model to distinguish attack samples from benign samples. We implement 18 mutators for text and image inputs and design a mutator combination policy to further improve detection generalization. The evaluation on the dataset containing 15 known attack types suggests that JailGuard achieves the best detection accuracy of 86.14%/82.90% on text and image inputs, outperforming state-of-the-art methods by 11.81%-25.73% and 12.20%-21.40%.         ",
    "url": "https://arxiv.org/abs/2312.10766",
    "authors": [
      "Xiaoyu Zhang",
      "Cen Zhang",
      "Tianlin Li",
      "Yihao Huang",
      "Xiaojun Jia",
      "Ming Hu",
      "Jie Zhang",
      "Yang Liu",
      "Shiqing Ma",
      "Chao Shen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2312.15144",
    "title": "A Generically Contrastive Spatiotemporal Representation Enhancement for 3D Skeleton Action Recognition",
    "abstract": "           Skeleton-based action recognition is a central task in computer vision and human-robot interaction. However, most previous methods suffer from overlooking the explicit exploitation of the latent data distributions (i.e., the intra-class variations and inter-class relations), thereby leading to confusion about ambiguous samples and sub-optimum solutions of the skeleton encoders. To mitigate this, we propose a Contrastive Spatiotemporal Representation Enhancement (CSRE) framework to obtain more discriminative representations from the sequences, which can be incorporated into various previous skeleton encoders and can be removed when testing. Specifically, we decompose the representation into spatial- and temporal-specific features to explore fine-grained motion patterns along the corresponding dimensions. Furthermore, to explicitly exploit the latent data distributions, we employ the attentive features to contrastive learning, which models the cross-sequence semantic relations by pulling together the features from the positive pairs and pushing away the negative pairs. Extensive experiments show that CSRE with five various skeleton encoders (HCN, 2S-AGCN, CTR-GCN, Hyperformer, and BlockGCN) achieves solid improvements on five benchmarks. The code will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2312.15144",
    "authors": [
      "Shaojie Zhang",
      "Jianqin Yin",
      "Yonghao Dang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.06423",
    "title": "CurveFormer++: 3D Lane Detection by Curve Propagation with Temporal Curve Queries and Attention",
    "abstract": "           In autonomous driving, accurate 3D lane detection using monocular cameras is important for downstream tasks. Recent CNN and Transformer approaches usually apply a two-stage model design. The first stage transforms the image feature from a front image into a bird's-eye-view (BEV) representation. Subsequently, a sub-network processes the BEV feature to generate the 3D detection results. However, these approaches heavily rely on a challenging image feature transformation module from a perspective view to a BEV representation. In our work, we present CurveFormer++, a single-stage Transformer-based method that does not require the view transform module and directly infers 3D lane results from the perspective image features. Specifically, our approach models the 3D lane detection task as a curve propagation problem, where each lane is represented by a curve query with a dynamic and ordered anchor point set. By employing a Transformer decoder, the model can iteratively refine the 3D lane results. A curve cross-attention module is introduced to calculate similarities between image features and curve queries. To handle varying lane lengths, we employ context sampling and anchor point restriction techniques to compute more relevant image features. Furthermore, we apply a temporal fusion module that incorporates selected informative sparse curve queries and their corresponding anchor point sets to leverage historical information. In the experiments, we evaluate our approach on two publicly real-world datasets. The results demonstrate that our method provides outstanding performance compared with both CNN and Transformer based methods. We also conduct ablation studies to analyze the impact of each component.         ",
    "url": "https://arxiv.org/abs/2402.06423",
    "authors": [
      "Yifeng Bai",
      "Zhirong Chen",
      "Pengpeng Liang",
      "Bo Song",
      "Erkang Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.10601",
    "title": "When \"Competency\" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers",
    "abstract": "           Recent advancements in Large Language Model (LLM) safety have primarily focused on mitigating attacks crafted in natural language or common ciphers (e.g. Base64), which are likely integrated into newer models' safety training. However, we reveal a paradoxical vulnerability: as LLMs advance in reasoning, they inadvertently become more susceptible to novel jailbreaking attacks. Enhanced reasoning enables LLMs to interpret complex instructions and decode complex user-defined ciphers, creating an exploitable security gap. To study this vulnerability, we introduce Attacks using Custom Encryptions (ACE), a jailbreaking technique that encodes malicious queries with novel ciphers. Extending ACE, we introduce Layered Attacks using Custom Encryptions (LACE), which applies multi-layer ciphers to amplify attack complexity. Furthermore, we develop CipherBench, a benchmark designed to evaluate LLMs' accuracy in decoding encrypted benign text. Our experiments reveal a critical trade-off: LLMs that are more capable of decoding ciphers are more vulnerable to these jailbreaking attacks, with success rates on GPT-4o escalating from 40% under ACE to 78% with LACE. These findings highlight a critical insight: as LLMs become more adept at deciphering complex user ciphers--many of which cannot be preemptively included in safety training--they become increasingly exploitable.         ",
    "url": "https://arxiv.org/abs/2402.10601",
    "authors": [
      "Divij Handa",
      "Zehua Zhang",
      "Amir Saeidi",
      "Shrinidhi Kumbhar",
      "Chitta Baral"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.14598",
    "title": "EMN: Brain-inspired Elastic Memory Network for Quick Domain Adaptive Feature Mapping",
    "abstract": "           Utilizing unlabeled data in the target domain to perform continuous optimization is critical to enhance the generalization ability of neural networks. Most domain adaptation methods focus on time-consuming optimization of deep feature extractors, which limits the deployment on lightweight edge devices. Inspired by the memory mechanism and powerful generalization ability of biological neural networks in human brains, we propose a novel gradient-free Elastic Memory Network, namely EMN, to support quick fine-tuning of the mapping between features and prediction without heavy optimization of deep features. In particular, EMN adopts randomly connected neurons to memorize the association of features and labels, where the signals in the network are propagated as impulses, and the prediction is made by associating the memories stored on neurons based on their confidence. More importantly, EMN supports reinforced memorization of feature mapping based on unlabeled data to quickly adapt to a new domain. Experiments based on four cross-domain real-world datasets show that EMN can achieve up to 10% enhancement of performance while only needing less than 1% timing cost of traditional domain adaptation methods.         ",
    "url": "https://arxiv.org/abs/2402.14598",
    "authors": [
      "Jianming Lv",
      "Chengjun Wang",
      "Depin Liang",
      "Qianli Ma",
      "Wei Chen",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.06348",
    "title": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized Representation",
    "abstract": "           High-dimensional sparse data emerge in many critical application domains such as healthcare and cybersecurity. To extract meaningful insights from massive volumes of these multi-dimensional data, scientists employ unsupervised analysis tools based on tensor decomposition (TD) methods. However, real-world sparse tensors exhibit highly irregular shapes and data distributions, which pose significant challenges for making efficient use of modern parallel processors. This study breaks the prevailing assumption that compressing sparse tensors into coarse-grained structures or along a particular dimension/mode is more efficient than keeping them in a fine-grained, mode-agnostic form. Our novel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO), encodes tensors in a compact format that can be easily streamed from memory and is amenable to both caching and parallel execution. In contrast to existing compressed tensor formats, ALTO constructs one tensor copy that is agnostic to both the mode orientation and the irregular distribution of nonzero elements. To demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms that exploit the inherent data reuse of tensor computations to substantially reduce synchronization overhead, decrease memory footprint, and improve parallel performance. Additionally, we characterize the major execution bottlenecks of TD methods on the latest Intel Xeon Scalable processors and introduce dynamic adaptation heuristics to automatically select the best algorithm based on the sparse tensor characteristics. Across a diverse set of real-world data sets, ALTO outperforms the state-of-the-art approaches, achieving more than an order-of-magnitude speedup over the best mode-agnostic formats. Compared to the best mode-specific formats, ALTO achieves 5.1X geometric mean speedup at a fraction (25%) of their storage costs.         ",
    "url": "https://arxiv.org/abs/2403.06348",
    "authors": [
      "Jan Laukemann",
      "Ahmed E. Helal",
      "S. Isaac Geronimo Anderson",
      "Fabio Checconi",
      "Yongseok Soh",
      "Jesmin Jahan Tithi",
      "Teresa Ranadive",
      "Brian J Gravelle",
      "Fabrizio Petrini",
      "Jee Choi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2404.00769",
    "title": "An Active Perception Game for Robust Information Gathering",
    "abstract": "           Active perception approaches select future viewpoints by using some estimate of the information gain. An inaccurate estimate can be detrimental in critical situations, e.g., locating a person in distress. However the true information gained can only be calculated post hoc, i.e., after the observation is realized. We present an approach to estimate the discrepancy between the estimated information gain (which is the expectation over putative future observations while neglecting correlations among them) and the true information gain. The key idea is to analyze the mathematical relationship between active perception and the estimation error of the information gain in a game-theoretic setting. Using this, we develop an online estimation approach that achieves sub-linear regret (in the number of time-steps) for the estimation of the true information gain and reduces the sub-optimality of active perception systems. We demonstrate our approach for active perception using a comprehensive set of experiments on: (a) different types of environments, including a quadrotor in a photorealistic simulation, real-world robotic data, and real-world experiments with ground robots exploring indoor and outdoor scenes; (b) different types of robotic perception data; and (c) different map representations. On average, our approach reduces information gain estimation errors by 42%, increases the information gain by 7%, PSNR by 5%, and semantic accuracy (measured as the number of objects that are localized correctly) by 6%. In real-world experiments with a Jackal ground robot, our approach demonstrated complex trajectories to explore occluded regions.         ",
    "url": "https://arxiv.org/abs/2404.00769",
    "authors": [
      "Siming He",
      "Yuezhan Tao",
      "Igor Spasojevic",
      "Vijay Kumar",
      "Pratik Chaudhari"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2404.02521",
    "title": "Space-time parallel scaling of Parareal with a physics-informed Fourier Neural Operator coarse propagator applied to the Black-Scholes equation",
    "abstract": "           Iterative parallel-in-time algorithms like Parareal can extend scaling beyond the saturation of purely spatial parallelization when solving initial value problems. However, they require the user to build coarse models to handle the inevitably serial transport of information in this http URL is a time consuming and difficult process since there is still only limited theoretical insight into what constitutes a good and efficient coarse model. Novel approaches from machine learning to solve differential equations could provide a more generic way to find coarse level models for parallel-in-time algorithms. This paper demonstrates that a physics-informed Fourier Neural Operator (PINO) is an effective coarse model for the parallelization in time of the two-asset Black-Scholes equation using Parareal. We demonstrate that PINO-Parareal converges as fast as a bespoke numerical coarse model and that, in combination with spatial parallelization by domain decomposition, it provides better overall speedup than both purely spatial parallelization and space-time parallelizaton with a numerical coarse propagator.         ",
    "url": "https://arxiv.org/abs/2404.02521",
    "authors": [
      "Abdul Qadir Ibrahim",
      "Sebastian G\u00f6tschel",
      "Daniel Ruprecht"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2404.05583",
    "title": "Towards More General Video-based Deepfake Detection through Facial Component Guided Adaptation for Foundation Model",
    "abstract": "           Generative models have enabled the creation of highly realistic facial-synthetic images, raising significant concerns due to their potential for misuse. Despite rapid advancements in the field of deepfake detection, developing efficient approaches to leverage foundation models for improved generalizability to unseen forgery samples remains challenging. To address this challenge, we propose a novel side-network-based decoder that extracts spatial and temporal cues using the CLIP image encoder for generalized video-based Deepfake detection. Additionally, we introduce Facial Component Guidance (FCG) to enhance spatial learning generalizability by encouraging the model to focus on key facial regions. By leveraging the generic features of a vision-language foundation model, our approach demonstrates promising generalizability on challenging Deepfake datasets while also exhibiting superiority in training data efficiency, parameter efficiency, and model robustness.         ",
    "url": "https://arxiv.org/abs/2404.05583",
    "authors": [
      "Yue-Hua Han",
      "Tai-Ming Huang",
      "Kai-Lung Hua",
      "Jun-Cheng Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2404.17092",
    "title": "Robust and Efficient Adversarial Defense in SNNs via Image Purification and Joint Detection",
    "abstract": "           Spiking Neural Networks (SNNs) aim to bridge the gap between neuroscience and machine learning by emulating the structure of the human nervous system. However, like convolutional neural networks, SNNs are vulnerable to adversarial attacks. To tackle the challenge, we propose a biologically inspired methodology to enhance the robustness of SNNs, drawing insights from the visual masking effect and filtering theory. First, an end-to-end SNN-based image purification model is proposed to defend against adversarial attacks, including a noise extraction network and a non-blind denoising network. The former network extracts noise features from noisy images, while the latter component employs a residual U-Net structure to reconstruct high-quality noisy images and generate clean images. Simultaneously, a multi-level firing SNN based on Squeeze-and-Excitation Network is introduced to improve the robustness of the classifier. Crucially, the proposed image purification network serves as a pre-processing module, avoiding modifications to classifiers. Unlike adversarial training, our method is highly flexible and can be seamlessly integrated with other defense strategies. Experimental results on various datasets demonstrate that the proposed methodology outperforms state-of-the-art baselines in terms of defense effectiveness, training time, and resource consumption.         ",
    "url": "https://arxiv.org/abs/2404.17092",
    "authors": [
      "Weiran Chen",
      "Qi Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2404.19597",
    "title": "TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning",
    "abstract": "           The implications of backdoor attacks on English-centric large language models (LLMs) have been widely examined - such attacks can be achieved by embedding malicious behaviors during training and activated under specific conditions that trigger malicious outputs. Despite the increasing support for multilingual capabilities in open-source and proprietary LLMs, the impact of backdoor attacks on these systems remains largely under-explored. Our research focuses on cross-lingual backdoor attacks against multilingual LLMs, particularly investigating how poisoning the instruction-tuning data for one or two languages can affect the outputs for languages whose instruction-tuning data were not poisoned. Despite its simplicity, our empirical analysis reveals that our method exhibits remarkable efficacy in models like mT5 and GPT-4o, with high attack success rates, surpassing 90% in more than 7 out of 12 languages across various scenarios. Our findings also indicate that more powerful models show increased susceptibility to transferable cross-lingual backdoor attacks, which also applies to LLMs predominantly pre-trained on English data, such as Llama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High Transferability: the backdoor mechanism operates successfully in cross-lingual response scenarios across 26 languages, achieving an average attack success rate of 99%, and 2) Robustness: the proposed attack remains effective even after defenses are applied. These findings expose critical security vulnerabilities in multilingual LLMs and highlight the urgent need for more robust, targeted defense strategies to address the unique challenges posed by cross-lingual backdoor transfer.         ",
    "url": "https://arxiv.org/abs/2404.19597",
    "authors": [
      "Xuanli He",
      "Jun Wang",
      "Qiongkai Xu",
      "Pasquale Minervini",
      "Pontus Stenetorp",
      "Benjamin I. P. Rubinstein",
      "Trevor Cohn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2405.00604",
    "title": "Toward Unified Practices in Trajectory Prediction Research on Drone Datasets",
    "abstract": "           The availability of high-quality datasets is crucial for the development of behavior prediction algorithms in autonomous vehicles. This paper highlights the need to standardize the use of certain datasets for motion forecasting research to simplify comparative analysis and proposes a set of tools and practices to achieve this. Drawing on extensive experience and a comprehensive review of current literature, we summarize our proposals for preprocessing, visualization, and evaluation in the form of an open-sourced toolbox designed for researchers working on trajectory prediction problems. The clear specification of necessary preprocessing steps and evaluation metrics is intended to alleviate development efforts and facilitate the comparison of results across different studies. The toolbox is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2405.00604",
    "authors": [
      "Theodor Westny",
      "Bj\u00f6rn Olofsson",
      "Erik Frisk"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2405.03169",
    "title": "SOC-MartNet: A Martingale Neural Network for the Hamilton-Jacobi-Bellman Equation without Explicit inf H in Stochastic Optimal Controls",
    "abstract": "           In this paper, we propose a martingale-based neural network, SOC-MartNet, for solving high-dimensional Hamilton-Jacobi-Bellman (HJB) equations where no explicit expression is needed for the infimum of the Hamiltonian, $\\inf_{u \\in U} H(t,x,u, z,p)$, and stochastic optimal control problems (SOCPs) with controls on both drift and volatility. We reformulate the HJB equations for the value function by training two neural networks, one for the value function and one for the optimal control with the help of two stochastic processes - a Hamiltonian process and a cost process. The control and value networks are trained such that the associated Hamiltonian process is minimized to satisfy the minimum principle of a feedback SOCP, and the cost process becomes a martingale, thus, ensuring the value function network as the solution to the corresponding HJB equation. Moreover, to enforce the martingale property for the cost process, we employ an adversarial network and construct a loss function characterizing the projection property of the conditional expectation condition of the martingale. Numerical results show that the proposed SOC-MartNet is effective and efficient for solving HJB-type equations and SOCPs with a dimension up to 10,000 in a small number of iteration steps (less than 6000) of training.         ",
    "url": "https://arxiv.org/abs/2405.03169",
    "authors": [
      "Wei Cai",
      "Shuixin Fang",
      "Tao Zhou"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2405.12419",
    "title": "GeoMask3D: Geometrically Informed Mask Selection for Self-Supervised Point Cloud Learning in 3D",
    "abstract": "           We introduce a pioneering approach to self-supervised learning for point clouds, employing a geometrically informed mask selection strategy called GeoMask3D (GM3D) to boost the efficiency of Masked Auto Encoders (MAE). Unlike the conventional method of random masking, our technique utilizes a teacher-student model to focus on intricate areas within the data, guiding the model's focus toward regions with higher geometric complexity. This strategy is grounded in the hypothesis that concentrating on harder patches yields a more robust feature representation, as evidenced by the improved performance on downstream tasks. Our method also presents a complete-to-partial feature-level knowledge distillation technique designed to guide the prediction of geometric complexity utilizing a comprehensive context from feature-level information. Extensive experiments confirm our method's superiority over State-Of-The-Art (SOTA) baselines, demonstrating marked improvements in classification, and few-shot tasks.         ",
    "url": "https://arxiv.org/abs/2405.12419",
    "authors": [
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Mehrdad Noori",
      "Milad Cheraghalikhani",
      "Gustavo Adolfo Vargas Hakim",
      "David Osowiechi",
      "Farzad Beizaee",
      "Ismail Ben Ayed",
      "Christian Desrosiers"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.16868",
    "title": "RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling",
    "abstract": "           Collaborative perception is dedicated to tackling the constraints of single-agent perception, such as occlusions, based on the multiple agents' multi-view sensor inputs. However, most existing works assume an ideal condition that all agents' multi-view cameras are continuously available. In reality, cameras may be highly noisy, obscured or even failed during the collaboration. In this work, we introduce a new robust camera-insensitivity problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? To address above problems, we propose RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism. The key intuition of RCDN is to construct collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. To better model collaborative neural rendering field, RCDN first establishes a geometry BEV feature based time-invariant static field with other agents via fast hash grid modeling. Based on the static background field, the proposed time-varying dynamic field can model corresponding motion vectors for foregrounds with appropriate positions. To validate RCDN, we create OPV2V-N, a new large-scale dataset with manual labelling under different camera failed scenarios. Extensive experiments conducted on OPV2V-N show that RCDN can be ported to other baselines and improve their robustness in extreme camera-insensitivity settings.         ",
    "url": "https://arxiv.org/abs/2405.16868",
    "authors": [
      "Tianhang Wang",
      "Fan Lu",
      "Zehan Zheng",
      "Zhijun Li",
      "Guang Chen",
      "Changjun Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2405.18680",
    "title": "Navigable Graphs for High-Dimensional Nearest Neighbor Search: Constructions and Limits",
    "abstract": "           There has been significant recent interest in graph-based nearest neighbor search methods, many of which are centered on the construction of navigable graphs over high-dimensional point sets. A graph is navigable if we can successfully move from any starting node to any target node using a greedy routing strategy where we always move to the neighbor that is closest to the destination according to a given distance function. The complete graph is navigable for any point set, but the important question for applications is if sparser graphs can be constructed. While this question is fairly well understood in low-dimensions, we establish some of the first upper and lower bounds for high-dimensional point sets. First, we give a simple and efficient way to construct a navigable graph with average degree $O(\\sqrt{n \\log n })$ for any set of $n$ points, in any dimension, for any distance function. We compliment this result with a nearly matching lower bound: even under the Euclidean metric in $O(\\log n)$ dimensions, a random point set has no navigable graph with average degree $O(n^{\\alpha})$ for any $\\alpha < 1/2$. Our lower bound relies on sharp anti-concentration bounds for binomial random variables, which we use to show that the near-neighborhoods of a set of random points do not overlap significantly, forcing any navigable graph to have many edges.         ",
    "url": "https://arxiv.org/abs/2405.18680",
    "authors": [
      "Haya Diwan",
      "Jinrui Gou",
      "Cameron Musco",
      "Christopher Musco",
      "Torsten Suel"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Geometry (cs.CG)",
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.18684",
    "title": "Learning Diffeomorphism for Image Registration with Time-Continuous Networks using Semigroup Regularization",
    "abstract": "           Diffeomorphic image registration (DIR) is a fundamental task in 3D medical image analysis that seeks topology-preserving deformations between image pairs. To ensure diffeomorphism, a common approach is to model the deformation field as the flow map solution of a differential equation, which is solved using efficient schemes such as scaling and squaring along with multiple smoothness regularization terms. In this paper, we propose a novel learning-based approach for diffeomorphic 3D image registration that models diffeomorphisms in a continuous-time framework using only a single regularization term, without requiring additional integration. We exploit the semigroup property-a fundamental characteristic of flow maps-as the sole form of regularization, ensuring temporally continuous diffeomorphic flows between image pairs. Leveraging this property, we prove that our formulation directly learns the flow map solution of an ODE, ensuring continuous inverse and cycle consistencies without explicit enforcement, while eliminating additional integration schemes and regularization terms. To achieve time-continuous diffeomorphisms, we employ time-embedded UNets, an architecture commonly used in diffusion models. Our results demonstrate that modeling diffeomorphism continuously in time improves registration performance. Experimental results on four public datasets demonstrate the superiority of our model over state-of-the-art diffeomorphic methods. Additionally, comparison to several recent non-diffeomorphic deformable image registration methods shows that our method achieves competitive Dice scores while significantly improving topology preservation.         ",
    "url": "https://arxiv.org/abs/2405.18684",
    "authors": [
      "Mohammadjavad Matinkia",
      "Nilanjan Ray"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.00430",
    "title": "Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners",
    "abstract": "           Recently, Large Language Models (LLMs) have witnessed remarkable performance as zero-shot task planners for robotic manipulation tasks. However, the open-loop nature of previous works makes LLM-based planning error-prone and fragile. On the other hand, failure detection approaches for closed-loop planning are often limited by task-specific heuristics or following an unrealistic assumption that the prediction is trustworthy all the time. As a general-purpose reasoning machine, LLMs or Multimodal Large Language Models (MLLMs) are promising for detecting failures. However, However, the appropriateness of the aforementioned assumption diminishes due to the notorious hullucination problem. In this work, we attempt to mitigate these issues by introducing a framework for closed-loop LLM-based planning called KnowLoop, backed by an uncertainty-based MLLMs failure detector, which is agnostic to any used MLLMs or LLMs. Specifically, we evaluate three different ways for quantifying the uncertainty of MLLMs, namely token probability, entropy, and self-explained confidence as primary metrics based on three carefully designed representative prompting strategies. With a self-collected dataset including various manipulation tasks and an LLM-based robot system, our experiments demonstrate that token probability and entropy are more reflective compared to self-explained confidence. By setting an appropriate threshold to filter out uncertain predictions and seek human help actively, the accuracy of failure detection can be significantly enhanced. This improvement boosts the effectiveness of closed-loop planning and the overall success rate of tasks.         ",
    "url": "https://arxiv.org/abs/2406.00430",
    "authors": [
      "Zhi Zheng",
      "Qian Feng",
      "Hang Li",
      "Alois Knoll",
      "Jianxiang Feng"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.03044",
    "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
    "abstract": "           We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained temporal embeddings and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained and fine-tuned PopT models to show how they can be used to extract neuroscience insights from large amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.03044",
    "authors": [
      "Geeling Chau",
      "Christopher Wang",
      "Sabera Talukder",
      "Vighnesh Subramaniam",
      "Saraswati Soedarmadji",
      "Yisong Yue",
      "Boris Katz",
      "Andrei Barbu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2406.04724",
    "title": "On Minimizing Adversarial Counterfactual Error in Adversarial RL",
    "abstract": "           Deep Reinforcement Learning (DRL) policies are highly susceptible to adversarial noise in observations, which poses significant risks in safety-critical scenarios. The challenge inherent to adversarial perturbations is that by altering the information observed by the agent, the state becomes only partially observable. Existing approaches address this by either enforcing consistent actions across nearby states or maximizing the worst-case value within adversarially perturbed observations. However, the former suffers from performance degradation when attacks succeed, while the latter tends to be overly conservative, leading to suboptimal performance in benign settings. We hypothesize that these limitations stem from their failing to account for partial observability directly. To this end, we introduce a novel objective called Adversarial Counterfactual Error (ACoE), defined on the beliefs about the true state and balancing value optimization with robustness. To make ACoE scalable in model-free settings, we propose the theoretically-grounded surrogate objective Cumulative-ACoE (C-ACoE). Our empirical evaluations on standard benchmarks (MuJoCo, Atari, and Highway) demonstrate that our method significantly outperforms current state-of-the-art approaches for addressing adversarial RL challenges, offering a promising direction for improving robustness in DRL under adversarial conditions. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.04724",
    "authors": [
      "Roman Belaire",
      "Arunesh Sinha",
      "Pradeep Varakantham"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.08267",
    "title": "A deep cut into Split Federated Self-supervised Learning",
    "abstract": "           Collaborative self-supervised learning has recently become feasible in highly distributed environments by dividing the network layers between client devices and a central server. However, state-of-the-art methods, such as MocoSFL, are optimized for network division at the initial layers, which decreases the protection of the client data and increases communication overhead. In this paper, we demonstrate that splitting depth is crucial for maintaining privacy and communication efficiency in distributed training. We also show that MocoSFL suffers from a catastrophic quality deterioration for the minimal communication overhead. As a remedy, we introduce Momentum-Aligned contrastive Split Federated Learning (MonAcoSFL), which aligns online and momentum client models during training procedure. Consequently, we achieve state-of-the-art accuracy while significantly reducing the communication overhead, making MonAcoSFL more practical in real-world scenarios.         ",
    "url": "https://arxiv.org/abs/2406.08267",
    "authors": [
      "Marcin Przewi\u0119\u017alikowski",
      "Marcin Osial",
      "Bartosz Zieli\u0144ski",
      "Marek \u015amieja"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2406.11601",
    "title": "Standardizing Structural Causal Models",
    "abstract": "           Synthetic datasets generated by structural causal models (SCMs) are commonly used for benchmarking causal structure learning algorithms. However, the variances and pairwise correlations in SCM data tend to increase along the causal ordering. Several popular algorithms exploit these artifacts, possibly leading to conclusions that do not generalize to real-world settings. Existing metrics like $\\operatorname{Var}$-sortability and $\\operatorname{R^2}$-sortability quantify these patterns, but they do not provide tools to remedy them. To address this, we propose internally-standardized structural causal models (iSCMs), a modification of SCMs that introduces a standardization operation at each variable during the generative process. By construction, iSCMs are not $\\operatorname{Var}$-sortable. We also find empirical evidence that they are mostly not $\\operatorname{R^2}$-sortable for commonly-used graph families. Moreover, contrary to the post-hoc standardization of data generated by standard SCMs, we prove that linear iSCMs are less identifiable from prior knowledge on the weights and do not collapse to deterministic relationships in large systems, which may make iSCMs a useful model in causal inference beyond the benchmarking problem studied here. Our code is publicly available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2406.11601",
    "authors": [
      "Weronika Ormaniec",
      "Scott Sussex",
      "Lars Lorch",
      "Bernhard Sch\u00f6lkopf",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.13378",
    "title": "PanDA: Towards Panoramic Depth Anything with Unlabeled Panoramas and Mobius Spatial Augmentation",
    "abstract": "           Recently, Depth Anything Models (DAMs) - a type of depth foundation models - have demonstrated impressive zero-shot capabilities across diverse perspective images. Despite its success, it remains an open question regarding DAMs' performance on panorama images that enjoy a large field-of-view (180x360) but suffer from spherical distortions. To address this gap, we conduct an empirical analysis to evaluate the performance of DAMs on panoramic images and identify their limitations. For this, we undertake comprehensive experiments to assess the performance of DAMs from three key factors: panoramic representations, 360 camera positions for capturing scenarios, and spherical spatial transformations. This way, we reveal some key findings, e.g., DAMs are sensitive to spatial transformations. We then propose a semi-supervised learning (SSL) framework to learn a panoramic DAM, dubbed PanDA. Under the umbrella of SSL, PanDA first learns a teacher model by fine-tuning DAM through joint training on synthetic indoor and outdoor panoramic datasets. Then, a student model is trained using large-scale unlabeled data, leveraging pseudo-labels generated by the teacher model. To enhance PanDA's generalization capability, M\"obius transformation-based spatial augmentation (MTSA) is proposed to impose consistency regularization between the predicted depth maps from the original and spatially transformed ones. This subtly improves the student model's robustness to various spatial transformations, even under severe distortions. Extensive experiments demonstrate that PanDA exhibits remarkable zero-shot capability across diverse scenes, and outperforms the data-specific panoramic depth estimation methods on two popular real-world benchmarks.         ",
    "url": "https://arxiv.org/abs/2406.13378",
    "authors": [
      "Zidong Cao",
      "Jinjing Zhu",
      "Weiming Zhang",
      "Hao Ai",
      "Haotian Bai",
      "Hengshuang Zhao",
      "Lin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.17714",
    "title": "Compositional Models for Estimating Causal Effects",
    "abstract": "           Many real-world systems can be usefully represented as sets of interacting components. Examples include computational systems, such as query processors and compilers, natural systems, such as cells and ecosystems, and social systems, such as families and organizations. However, current approaches to estimating potential outcomes and causal effects typically treat such systems as single units, represent them with a fixed set of variables, and assume a homogeneous data-generating process. In this work, we study a compositional approach for estimating individual-level potential outcomes and causal effects in structured systems, where each unit is represented by an instance-specific composition of multiple heterogeneous components. The compositional approach decomposes unit-level causal queries into more fine-grained queries, explicitly modeling how unit-level interventions affect component-level outcomes to generate a unit's outcome. We demonstrate this approach using modular neural network architectures and show that it provides benefits for causal effect estimation from observational data, such as accurate causal effect estimation for structured units, increased sample efficiency, improved overlap between treatment and control groups, and compositional generalization to units with unseen combinations of components. Remarkably, our results show that compositional modeling can improve the accuracy of causal estimation even when component-level outcomes are unobserved. We also create and use a set of real-world evaluation environments for the empirical evaluation of compositional approaches for causal effect estimation and demonstrate the role of composition structure, varying amounts of component-level data access, and component heterogeneity in the performance of compositional models as compared to the non-compositional approaches.         ",
    "url": "https://arxiv.org/abs/2406.17714",
    "authors": [
      "Purva Pruthi",
      "David Jensen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2407.07755",
    "title": "Neural Geometry Processing via Spherical Neural Surfaces",
    "abstract": "           Neural surfaces (e.g., neural map encoding, deep implicits and neural radiance fields) have recently gained popularity because of their generic structure (e.g., multi-layer perceptron) and easy integration with modern learning-based setups. Traditionally, we have a rich toolbox of geometry processing algorithms designed for polygonal meshes to analyze and operate on surface geometry. In the absence of an analogous toolbox, neural representations are typically discretized and converted into a mesh, before applying any geometry processing algorithm. This is unsatisfactory and, as we demonstrate, unnecessary. In this work, we propose a spherical neural surface representation for genus-0 surfaces and demonstrate how to compute core geometric operators directly on this representation. Namely, we estimate surface normals and first and second fundamental forms of the surface, as well as compute surface gradient, surface divergence and Laplace-Beltrami operator on scalar/vector fields defined on the surface. Our representation is fully seamless, overcoming a key limitation of similar explicit representations such as Neural Surface Maps [Morreale et al. 2021]. These operators, in turn, enable geometry processing directly on the neural representations without any unnecessary meshing. We demonstrate illustrative applications in (neural) spectral analysis, heat flow and mean curvature flow, and evaluate robustness to isometric shape variations. We propose theoretical formulations and validate their numerical estimates, against analytical estimates, mesh-based baselines, and neural alternatives, where available. By systematically linking neural surface representations with classical geometry processing algorithms, we believe that this work can become a key ingredient in enabling neural geometry processing. Code is accessible from the project webpage.         ",
    "url": "https://arxiv.org/abs/2407.07755",
    "authors": [
      "Romy Williamson",
      "Niloy J. Mitra"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.08227",
    "title": "DALL-M: Context-Aware Clinical Data Augmentation with LLMs",
    "abstract": "           X-ray images are vital in medical diagnostics, but their effectiveness is limited without clinical context. Radiologists often find chest X-rays insufficient for diagnosing underlying diseases, necessitating the integration of structured clinical features with radiology reports. To address this, we introduce DALL-M, a novel framework that enhances clinical datasets by generating contextual synthetic data. DALL-M augments structured patient data, including vital signs (e.g., heart rate, oxygen saturation), radiology findings (e.g., lesion presence), and demographic factors. It integrates this tabular data with contextual knowledge extracted from radiology reports and domain-specific resources (e.g., Radiopaedia, Wikipedia), ensuring clinical consistency and reliability. DALL-M follows a three-phase process: (i) clinical context storage, (ii) expert query generation, and (iii) context-aware feature augmentation. Using large language models (LLMs), it generates both contextual synthetic values for existing clinical features and entirely new, clinically relevant features. Applied to 799 cases from the MIMIC-IV dataset, DALL-M expanded the original 9 clinical features to 91. Empirical validation with machine learning models (including Decision Trees, Random Forests, XGBoost, and TabNET) demonstrated a 16.5% improvement in F1 score and a 25% increase in Precision and Recall. DALL-M bridges an important gap in clinical data augmentation by preserving data integrity while enhancing predictive modeling in healthcare. Our results show that integrating LLM-generated synthetic features significantly improves model performance, making DALL-M a scalable and practical approach for AI-driven medical diagnostics.         ",
    "url": "https://arxiv.org/abs/2407.08227",
    "authors": [
      "Chihcheng Hsieh",
      "Catarina Moreira",
      "Isabel Blanco Nobre",
      "Sandra Costa Sousa",
      "Chun Ouyang",
      "Margot Brereton",
      "Joaquim Jorge",
      "Jacinto C. Nascimento"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.09295",
    "title": "Systematic Categorization, Construction and Evaluation of New Attacks against Multi-modal Mobile GUI Agents",
    "abstract": "           The integration of Large Language Models (LLMs) and Multi-modal Large Language Models (MLLMs) into mobile GUI agents has significantly enhanced user efficiency and experience. However, this advancement also introduces potential security vulnerabilities that have yet to be thoroughly explored. In this paper, we present a systematic security investigation of multi-modal mobile GUI agents, addressing this critical gap in the existing literature. Our contributions are twofold: (1) we propose a novel threat modeling methodology, leading to the discovery and feasibility analysis of 34 previously unreported attacks, and (2) we design an attack framework to systematically construct and evaluate these threats. Through a combination of real-world case studies and extensive dataset-driven experiments, we validate the severity and practicality of those attacks, highlighting the pressing need for robust security measures in mobile GUI systems.         ",
    "url": "https://arxiv.org/abs/2407.09295",
    "authors": [
      "Yulong Yang",
      "Xinshan Yang",
      "Shuaidong Li",
      "Chenhao Lin",
      "Zhengyu Zhao",
      "Chao Shen",
      "Tianwei Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.09364",
    "title": "Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text",
    "abstract": "           The significant progress in the development of Large Language Models has contributed to blurring the distinction between human and AI-generated text. The increasing pervasiveness of AI-generated text and the difficulty in detecting it poses new challenges for our society. In this paper, we tackle the problem of detecting and attributing AI-generated text by proposing WhosAI, a triplet-network contrastive learning framework designed to predict whether a given input text has been generated by humans or AI and to unveil the authorship of the text. Unlike most existing approaches, our proposed framework is conceived to learn semantic similarity representations from multiple generators at once, thus equally handling both detection and attribution tasks. Furthermore, WhosAI is model-agnostic and scalable to the release of new AI text-generation models by incorporating their generated instances into the embedding space learned by our framework. Experimental results on the TuringBench benchmark of 200K news articles show that our proposed framework achieves outstanding results in both the Turing Test and Authorship Attribution tasks, outperforming all the methods listed in the TuringBench benchmark leaderboards.         ",
    "url": "https://arxiv.org/abs/2407.09364",
    "authors": [
      "Lucio La Cava",
      "Davide Costa",
      "Andrea Tagarelli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2407.20361",
    "title": "From ML to LLM: Evaluating the Robustness of Phishing Webpage Detection Models against Adversarial Attacks",
    "abstract": "           Phishing attacks attempt to deceive users into stealing sensitive information, posing a significant cybersecurity threat. Advances in machine learning (ML) and deep learning (DL) have led to the development of numerous phishing webpage detection solutions, but these models remain vulnerable to adversarial attacks. Evaluating their robustness against adversarial phishing webpages is essential. Existing tools contain datasets of pre-designed phishing webpages for a limited number of brands, and lack diversity in phishing features. To address these challenges, we develop PhishOracle, a tool that generates adversarial phishing webpages by embedding diverse phishing features into legitimate webpages. We evaluate the robustness of three existing task-specific models -- Stack model, VisualPhishNet, and Phishpedia -- against PhishOracle-generated adversarial phishing webpages and observe a significant drop in their detection rates. In contrast, a multimodal large language model (MLLM)-based phishing detector demonstrates stronger robustness against these adversarial attacks but still is prone to evasion. Our findings highlight the vulnerability of phishing detection models to adversarial attacks, emphasizing the need for more robust detection approaches. Furthermore, we conduct a user study to evaluate whether PhishOracle-generated adversarial phishing webpages can deceive users. The results show that many of these phishing webpages evade not only existing detection models but also users. We also develop the PhishOracle web app, allowing users to input a legitimate URL, select relevant phishing features and generate a corresponding phishing webpage. All resources will be made publicly available on GitHub.         ",
    "url": "https://arxiv.org/abs/2407.20361",
    "authors": [
      "Aditya Kulkarni",
      "Vivek Balachandran",
      "Dinil Mon Divakaran",
      "Tamal Das"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.20640",
    "title": "Improved Bounds for Pure Private Agnostic Learning: Item-Level and User-Level Privacy",
    "abstract": "           Machine Learning has made remarkable progress in a wide range of fields. In many scenarios, learning is performed on datasets involving sensitive information, in which privacy protection is essential for learning algorithms. In this work, we study pure private learning in the agnostic model -- a framework reflecting the learning process in practice. We examine the number of users required under item-level (where each user contributes one example) and user-level (where each user contributes multiple examples) privacy and derive several improved upper bounds. For item-level privacy, our algorithm achieves a near optimal bound for general concept classes. We extend this to the user-level setting, rendering a tighter upper bound than the one proved by Ghazi et al. (2023). Lastly, we consider the problem of learning thresholds under user-level privacy and present an algorithm with a nearly tight user complexity.         ",
    "url": "https://arxiv.org/abs/2407.20640",
    "authors": [
      "Bo Li",
      "Wei Wang",
      "Peng Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.04315",
    "title": "Federated Cubic Regularized Newton Learning with Sparsification-amplified Differential Privacy",
    "abstract": "           This paper investigates the use of the cubic-regularized Newton method within a federated learning framework while addressing two major concerns that commonly arise in federated learning: privacy leakage and communication bottleneck. We introduce a federated learning algorithm called Differentially Private Federated Cubic Regularized Newton (DP-FCRN). By leveraging second-order techniques, our algorithm achieves lower iteration complexity compared to first-order methods. We also incorporate noise perturbation during local computations to ensure privacy. Furthermore, we employ sparsification in uplink transmission, which not only reduces the communication costs but also amplifies the privacy guarantee. Specifically, this approach reduces the necessary noise intensity without compromising privacy protection. We analyze the convergence properties of our algorithm and establish the privacy guarantee. Finally, we validate the effectiveness of the proposed algorithm through experiments on a benchmark dataset.         ",
    "url": "https://arxiv.org/abs/2408.04315",
    "authors": [
      "Wei Huo",
      "Changxin Liu",
      "Kemi Ding",
      "Karl Henrik Johansson",
      "Ling Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2408.04811",
    "title": "h4rm3l: A language for Composable Jailbreak Attack Synthesis",
    "abstract": "           Despite their demonstrated valuable capabilities, state-of-the-art (SOTA) widely deployed large language models (LLMs) still have the potential to cause harm to society due to the ineffectiveness of their safety filters, which can be bypassed by prompt transformations called jailbreak attacks. Current approaches to LLM safety assessment, which employ datasets of templated prompts and benchmarking pipelines, fail to cover sufficiently large and diverse sets of jailbreak attacks, leading to the widespread deployment of unsafe LLMs. Recent research showed that novel jailbreak attacks could be derived by composition; however, a formal composable representation for jailbreak attacks, which, among other benefits, could enable the exploration of a large compositional space of jailbreak attacks through program synthesis methods, has not been previously proposed. We introduce h4rm3l, a novel approach that addresses this gap with a human-readable domain-specific language (DSL). Our framework comprises: (1) The h4rm3l DSL, which formally expresses jailbreak attacks as compositions of parameterized string transformation primitives. (2) A synthesizer with bandit algorithms that efficiently generates jailbreak attacks optimized for a target black box LLM. (3) The h4rm3l red-teaming software toolkit that employs the previous two components and an automated harmful LLM behavior classifier that is strongly aligned with human judgment. We demonstrate h4rm3l's efficacy by synthesizing a dataset of 2656 successful novel jailbreak attacks targeting 6 SOTA open-source and proprietary LLMs, and by benchmarking those models against a subset of these synthesized attacks. Our results show that h4rm3l's synthesized attacks are diverse and more successful than existing jailbreak attacks in literature, with success rates exceeding 90% on SOTA LLMs.         ",
    "url": "https://arxiv.org/abs/2408.04811",
    "authors": [
      "Moussa Koulako Bala Doumbouya",
      "Ananjan Nandi",
      "Gabriel Poesia",
      "Davide Ghilardi",
      "Anna Goldie",
      "Federico Bianchi",
      "Dan Jurafsky",
      "Christopher D. Manning"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.05131",
    "title": "Range Membership Inference Attacks",
    "abstract": "           Machine learning models can leak private information about their training data. The standard methods to measure this privacy risk, based on membership inference attacks (MIAs), only check if a given data point \\textit{exactly} matches a training point, neglecting the potential of similar or partially overlapping memorized data revealing the same private information. To address this issue, we introduce the class of range membership inference attacks (RaMIAs), testing if the model was trained on any data in a specified range (defined based on the semantics of privacy). We formulate the RaMIAs game and design a principled statistical test for its composite hypotheses. We show that RaMIAs can capture privacy loss more accurately and comprehensively than MIAs on various types of data, such as tabular, image, and language. RaMIA paves the way for more comprehensive and meaningful privacy auditing of machine learning algorithms.         ",
    "url": "https://arxiv.org/abs/2408.05131",
    "authors": [
      "Jiashu Tao",
      "Reza Shokri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06621",
    "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
    "abstract": "           Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose two novel techniques for robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in this https URL.         ",
    "url": "https://arxiv.org/abs/2408.06621",
    "authors": [
      "Sungmin Cha",
      "Sungjun Cho",
      "Dasol Hwang",
      "Moontae Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.07490",
    "title": "Attention-Guided Perturbation for Unsupervised Image Anomaly Detection",
    "abstract": "           Reconstruction-based methods have significantly advanced unsupervised image anomaly detection involving only normal training images. However, it has been proven that modern neural networks generally have a strong reconstruction capacity and often reconstruct both normal and abnormal samples well, thereby failing to spot anomaly regions by checking the reconstruction quality. To prevent well-reconstructed anomalies, one simple but effective strategy is to perturb normal samples and then map perturbed versions to normal ones. Yet it treats each spatial position equally, disregarding the fact that the foreground locations are inherently more important for reconstruction. Motivated by this, we present a simple yet effective reconstruction framework named Attention-Guided Perturbation Network (AGPNet), which learns to add perturbations guided with an attention mask during training. Specifically, it consists of two branches, \\ie, a reconstruction branch and an auxiliary attention-based perturbation branch. The reconstruction branch learns to reconstruct normal samples, while the auxiliary one aims to produce attention masks to guide the noise perturbation process for normal samples. By doing so, we are expecting to synthesize hard yet more informative anomalies for training, which enable the reconstruction branch to learn important inherent normal patterns both comprehensively and efficiently. Extensive experiments are conducted on several popular benchmarks covering MVTec-AD, VisA, and MVTec-3D, and show that AGPNet obtains leading anomaly detection results under few-shot, one-class, and multi-class setups.         ",
    "url": "https://arxiv.org/abs/2408.07490",
    "authors": [
      "Yuxuan Cheng",
      "Tingfeng Huang",
      "Yuxuan Cai",
      "Jingbo Xia",
      "Rui Yu",
      "Jinhai Xiang",
      "Xinwei He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.15185",
    "title": "Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer",
    "abstract": "           Video Anomaly Detection (VAD) presents a significant challenge in computer vision, particularly due to the unpredictable and infrequent nature of anomalous events, coupled with the diverse and dynamic environments in which they occur. Human-centric VAD, a specialized area within this domain, faces additional complexities, including variations in human behavior, potential biases in data, and substantial privacy concerns related to human subjects. These issues complicate the development of models that are both robust and generalizable. To address these challenges, recent advancements have focused on pose-based VAD, which leverages human pose as a high-level feature to mitigate privacy concerns, reduce appearance biases, and minimize background interference. In this paper, we introduce SPARTA, a novel transformer-based architecture designed specifically for human-centric pose-based VAD. SPARTA introduces an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP) tokenization method that produces an enriched representation of human motion over time. This approach ensures that the transformer's attention mechanism captures both spatial and temporal patterns simultaneously, rather than focusing on only one aspect. The addition of the relative pose further emphasizes subtle deviations from normal human movements. The architecture's core, a novel Unified Encoder Twin Decoders (UETD) transformer, significantly improves the detection of anomalous behaviors in video data. Extensive evaluations across multiple benchmark datasets demonstrate that SPARTA consistently outperforms existing methods, establishing a new state-of-the-art in pose-based VAD.         ",
    "url": "https://arxiv.org/abs/2408.15185",
    "authors": [
      "Ghazal Alinezhad Noghre",
      "Armin Danesh Pazho",
      "Hamed Tabkhi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.01022",
    "title": "SINET: Sparsity-driven Interpretable Neural Network for Underwater Image Enhancement",
    "abstract": "           Improving the quality of underwater images is essential for advancing marine research and technology. This work introduces a sparsity-driven interpretable neural network (SINET) for the underwater image enhancement (UIE) task. Unlike pure deep learning methods, our network architecture is based on a novel channel-specific convolutional sparse coding (CCSC) model, ensuring good interpretability of the underlying image enhancement process. The key feature of SINET is that it estimates the salient features from the three color channels using three sparse feature estimation blocks (SFEBs). The architecture of SFEB is designed by unrolling an iterative algorithm for solving the $\\ell_1$ regularized convolutional sparse coding (CSC) problem. Our experiments show that SINET surpasses state-of-the-art PSNR value by $1.05$ dB with $3873$ times lower computational complexity. Code can be found at: this https URL.         ",
    "url": "https://arxiv.org/abs/2409.01022",
    "authors": [
      "Gargi Panda",
      "Soumitra Kundu",
      "Saumik Bhattacharya",
      "Aurobinda Routray"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2409.02606",
    "title": "Real-time design of architectural structures with differentiable mechanics and neural networks",
    "abstract": "           Designing mechanically efficient geometry for architectural structures like shells, towers, and bridges, is an expensive iterative process. Existing techniques for solving such inverse problems rely on traditional optimization methods, which are slow and computationally expensive, limiting iteration speed and design exploration. Neural networks would seem to offer a solution via data-driven amortized optimization, but they often require extensive fine-tuning and cannot ensure that important design criteria, such as mechanical integrity, are met. In this work, we combine neural networks with a differentiable mechanics simulator to develop a model that accelerates the solution of shape approximation problems for architectural structures represented as bar systems. This model explicitly guarantees compliance with mechanical constraints while generating designs that closely match target geometries. We validate our approach in two tasks, the design of masonry shells and cable-net towers. Our model achieves better accuracy and generalization than fully neural alternatives, and comparable accuracy to direct optimization but in real time, enabling fast and reliable design exploration. We further demonstrate its advantages by integrating it into 3D modeling software and fabricating a physical prototype. Our work opens up new opportunities for accelerated mechanical design enhanced by neural networks for the built environment.         ",
    "url": "https://arxiv.org/abs/2409.02606",
    "authors": [
      "Rafael Pastrana",
      "Eder Medina",
      "Isabel M. de Oliveira",
      "Sigrid Adriaenssens",
      "Ryan P. Adams"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2409.09021",
    "title": "INN-PAR: Invertible Neural Network for PPG to ABP Reconstruction",
    "abstract": "           Non-invasive and continuous blood pressure (BP) monitoring is essential for the early prevention of many cardiovascular diseases. Estimating arterial blood pressure (ABP) from photoplethysmography (PPG) has emerged as a promising solution. However, existing deep learning approaches for PPG-to-ABP reconstruction (PAR) encounter certain information loss, impacting the precision of the reconstructed signal. To overcome this limitation, we introduce an invertible neural network for PPG to ABP reconstruction (INN-PAR), which employs a series of invertible blocks to jointly learn the mapping between PPG and its gradient with the ABP signal and its gradient. INN-PAR efficiently captures both forward and inverse mappings simultaneously, thereby preventing information loss. By integrating signal gradients into the learning process, INN-PAR enhances the network's ability to capture essential high-frequency details, leading to more accurate signal reconstruction. Moreover, we propose a multi-scale convolution module (MSCM) within the invertible block, enabling the model to learn features across multiple scales effectively. We have experimented on two benchmark datasets, which show that INN-PAR significantly outperforms the state-of-the-art methods in both waveform reconstruction and BP measurement accuracy. Codes can be found at: this https URL.         ",
    "url": "https://arxiv.org/abs/2409.09021",
    "authors": [
      "Soumitra Kundu",
      "Gargi Panda",
      "Saumik Bhattacharya",
      "Aurobinda Routray",
      "Rajlakshmi Guha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2409.15159",
    "title": "DRAPER: Towards a Robust Robot Deployment and Reliable Evaluation for Quasi-Static Pick-and-Place Cloth-Shaping Neural Controllers",
    "abstract": "           Comparing robotic cloth-manipulation systems in a real-world setup is challenging. The fidelity gap between simulation-trained cloth neural controllers and real-world operation hinders the reliable deployment of these methods in physical trials. Inconsistent experimental setups and hardware limitations among different approaches obstruct objective evaluations. This study demonstrates a reliable real-world comparison of different simulation-trained neural controllers on both flattening and folding tasks with different types of fabrics varying in material, size, and colour. We introduce the DRAPER framework to enable this comprehensive study, which reliably reflects the true capabilities of these neural controllers. It specifically addresses real-world grasping errors, such as misgrasping and multilayer grasping, through real-world adaptations of the simulation environment to provide data trajectories that closely reflect real-world grasping scenarios. It also employs a special set of vision processing techniques to close the simulation-to-reality gap in the perception. Furthermore, it achieves robust grasping by adopting a tweezer-extended gripper and a grasping procedure. We demonstrate DRAPER's generalisability across different deep-learning methods and robotic platforms, offering valuable insights to the cloth manipulation research community.         ",
    "url": "https://arxiv.org/abs/2409.15159",
    "authors": [
      "Halid Abdulrahim Kadi",
      "Jose Alex Chandy",
      "Luis Figueredo",
      "Kasim Terzi\u0107",
      "Praminda Caleb-Solly"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.16073",
    "title": "OW-Rep: Open World Object Detection with Instance Representation Learning",
    "abstract": "           Open World Object Detection(OWOD) addresses realistic scenarios where unseen object classes emerge, enabling detectors trained on known classes to detect unknown objects and incrementally incorporate the knowledge they provide. While existing OWOD methods primarily focus on detecting unknown objects, they often overlook the rich semantic relationships between detected objects, which are essential for scene understanding and applications in open-world environments (e.g., open-world tracking and novel class discovery). In this paper, we extend the OWOD framework to jointly detect unknown objects and learn semantically rich instance embeddings, enabling the detector to capture fine-grained semantic relationships between instances. To this end, we propose two modules that leverage the rich and generalizable knowledge of Vision Foundation Models(VFM). First, the Unknown Box Refine Module uses instance masks from the Segment Anything Model to accurately localize unknown objects. The Embedding Transfer Module then distills instance-wise semantic similarities from VFM features to the detector's embeddings via a relaxed contrastive loss, enabling the detector to learn a semantically meaningful and generalizable instance feature. Extensive experiments show that our method significantly improves both unknown object detection and instance embedding quality, while also enhancing performance in downstream tasks such as open-world tracking.         ",
    "url": "https://arxiv.org/abs/2409.16073",
    "authors": [
      "Sunoh Lee",
      "Minsik Jeon",
      "Jihong Min",
      "Junwon Seo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.19801",
    "title": "CRScore: Grounding Automated Evaluation of Code Review Comments in Code Claims and Smells",
    "abstract": "           The task of automated code review has recently gained a lot of attention from the machine learning community. However, current review comment evaluation metrics rely on comparisons with a human-written reference for a given code change (also called a diff). Furthermore, code review is a one-to-many problem, like generation and summarization, with many \"valid reviews\" for a diff. Thus, we develop CRScore - a reference-free metric to measure dimensions of review quality like conciseness, comprehensiveness, and relevance. We design CRScore to evaluate reviews in a way that is grounded in claims and potential issues detected in the code by LLMs and static analyzers. We demonstrate that CRScore can produce valid, fine-grained scores of review quality that have the greatest alignment with human judgment among open source metrics (0.54 Spearman correlation) and are more sensitive than reference-based metrics. We also release a corpus of 2.9k human-annotated review quality scores for machine-generated and GitHub review comments to support the development of automated metrics.         ",
    "url": "https://arxiv.org/abs/2409.19801",
    "authors": [
      "Atharva Naik",
      "Marcus Alenius",
      "Daniel Fried",
      "Carolyn Rose"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2410.03801",
    "title": "P1-KAN: an effective Kolmogorov-Arnold network with application to hydraulic valley optimization",
    "abstract": "           A new Kolmogorov-Arnold network (KAN) is proposed to approximate potentially irregular functions in high dimensions. We provide error bounds for this approximation, assuming that the Kolmogorov-Arnold expansion functions are sufficiently smooth. When the function is only continuous, we also provide universal approximation theorems. We show that it outperforms multilayer perceptrons in terms of accuracy and convergence speed. We also compare it with several proposed KAN networks: it outperforms all networks for irregular functions and achieves similar accuracy to the original spline-based KAN network for smooth functions. Finally, we compare some of the KAN networks in optimizing a French hydraulic valley.         ",
    "url": "https://arxiv.org/abs/2410.03801",
    "authors": [
      "Xavier Warin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2410.04054",
    "title": "Large Language Models can Achieve Social Balance",
    "abstract": "           Social balance is a well-established concept in sociology which dictates how individual interactions can lead a population to become one faction of positive interactions or be divided in two or more antagonistic factions. In this paper, we consider a group of large language models (LLMs) and study how, after continuous interactions, they can achieve social balance. Across three different LLM models, we find that achieving social balance depends on (i) the type of interaction; (ii) whether agents consider homophily or influence from their peers; and (iii) the population size. We characterize how each model achieves social balance with different frequency, diversity of positive or negative interactions, and interaction stability across conditions (i) to (iii). We show that models achieve different notions of social balance and justify their social dynamics differently. Remarkably, the largest model is not necessarily more likely to achieve social balance with more frequency, stability, and diversity than the smaller ones.         ",
    "url": "https://arxiv.org/abs/2410.04054",
    "authors": [
      "Pedro Cisneros-Velarde"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2410.05270",
    "title": "CLIP's Visual Embedding Projector is a Few-shot Cornucopia",
    "abstract": "           We consider the problem of adapting a contrastively pretrained vision-language model like CLIP (Radford et al., 2021) for few-shot classification. The literature addresses this problem by learning a linear classifier of the frozen visual features, optimizing word embeddings, or learning external feature adapters. We introduce an alternative way for few-shot CLIP adaptation without adding ''external'' parameters to optimize. We find that simply fine-tuning the embedding projection matrix of the vision encoder leads to better performance than all baselines. Furthermore, we show that regularizing training with the distance between the fine-tuned and pretrained matrices adds reliability for adapting CLIP, making the results stable across different learning rates in the ''validation-free'' setting. This simple approach, coined ProLIP, yields state-of-the-art performance on 11 few-shot classification benchmarks, few-shot cross-dataset transfer, domain generalization, and base-to-new class generalization. We also show that ProLIP significantly outperforms prompt tuning when extended to another task of test-time adaptation, while being one order of magnitude faster to train. Code will be made available at: this https URL .         ",
    "url": "https://arxiv.org/abs/2410.05270",
    "authors": [
      "Mohammad Fahes",
      "Tuan-Hung Vu",
      "Andrei Bursuc",
      "Patrick P\u00e9rez",
      "Raoul de Charette"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.05869",
    "title": "Believing is Seeing: Unobserved Object Detection using Generative Models",
    "abstract": "           Can objects that are not visible in an image -- but are in the vicinity of the camera -- be detected? This study introduces the novel tasks of 2D, 2.5D and 3D unobserved object detection for predicting the location of nearby objects that are occluded or lie outside the image frame. We adapt several state-of-the-art pre-trained generative models to address this task, including 2D and 3D diffusion models and vision-language models, and show that they can be used to infer the presence of objects that are not directly observed. To benchmark this task, we propose a suite of metrics that capture different aspects of performance. Our empirical evaluation on indoor scenes from the RealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the use of generative models for the unobserved object detection task.         ",
    "url": "https://arxiv.org/abs/2410.05869",
    "authors": [
      "Subhransu S. Bhattacharjee",
      "Dylan Campbell",
      "Rahul Shome"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2410.06074",
    "title": "Scalable Mechanistic Neural Networks for Differential Equations and Machine Learning",
    "abstract": "           We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural network framework designed for scientific machine learning applications involving long temporal sequences. By reformulating the original Mechanistic Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time and space complexities from cubic and quadratic with respect to the sequence length, respectively, to linear. This significant improvement enables efficient modeling of long-term dynamics without sacrificing accuracy or interpretability. Extensive experiments demonstrate that S-MNN matches the original MNN in precision while substantially reducing computational resources. Consequently, S-MNN can drop-in replace the original MNN in applications, providing a practical and efficient tool for integrating mechanistic bottlenecks into neural network models of complex dynamical systems. Source code is available at this https URL .         ",
    "url": "https://arxiv.org/abs/2410.06074",
    "authors": [
      "Jiale Chen",
      "Dingling Yao",
      "Adeel Pervez",
      "Dan Alistarh",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.06418",
    "title": "MIRACLE3D: Memory-efficient Integrated Robust Approach for Continual Learning on Point Clouds via Shape Model Construction",
    "abstract": "           In this paper, we introduce a novel framework for memory-efficient and privacy-preserving continual learning in 3D object classification. Unlike conventional memory-based approaches in continual learning that require storing numerous exemplars, our method constructs a compact shape model for each class, retaining only the mean shape along with a few key modes of variation. This strategy not only enables the generation of diverse training samples while drastically reducing memory usage but also enhances privacy by eliminating the need to store original data. To further improve model robustness against input variations, an issue common in 3D domains due to the absence of strong backbones and limited training data, we incorporate Gradient Mode Regularization. This technique enhances model stability and broadens classification margins, resulting in accuracy improvements. We validate our approach through extensive experiments on the ModelNet40, ShapeNet, and ScanNet datasets, where we achieve state-of-the-art performance. Notably, our method consumes only 15% of the memory required by competing methods on the ModelNet40 and ShapeNet, while achieving comparable performance on the challenging ScanNet dataset with just 8.5% of the memory. These results underscore the scalability, effectiveness, and privacy-preserving strengths of our framework for 3D object classification.         ",
    "url": "https://arxiv.org/abs/2410.06418",
    "authors": [
      "Hossein Resani",
      "Behrooz Nasihatkon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.07921",
    "title": "Boosting Hierarchical Reinforcement Learning with Meta-Learning for Complex Task Adaptation",
    "abstract": "           Hierarchical Reinforcement Learning (HRL) is well-suitedd for solving complex tasks by breaking them down into structured policies. However, HRL agents often struggle with efficient exploration and quick adaptation. To overcome these limitations, we propose integrating meta-learning into HRL to enable agents to learn and adapt hierarchical policies more effectively. Our method leverages meta-learning to facilitate rapid task adaptation using prior experience, while intrinsic motivation mechanisms drive efficient exploration by rewarding the discovery of novel states. Specifically, our agent employs a high-level policy to choose among multiple low-level policies within custom-designed grid environments. By incorporating gradient-based meta-learning with differentiable inner-loop updates, we optimize performance across a curriculum of progressively challenging tasks. Experimental results highlight that our metalearning-enhanced hierarchical agent significantly outperforms standard HRL approaches lacking meta-learning and intrinsic motivation. The agent demonstrates faster learning, greater cumulative rewards, and higher success rates in complex grid-based scenarios. These Findings underscore the effectiveness of combining meta-learning, curriculum learning, and intrinsic motivation to enhance the capability of HRL agents in tackling complex tasks.         ",
    "url": "https://arxiv.org/abs/2410.07921",
    "authors": [
      "Arash Khajooeinejad",
      "Fatemeh Sadat Masoumi",
      "Masoumeh Chapariniya"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.09878",
    "title": "Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning",
    "abstract": "           Conformal prediction provides model-agnostic and distribution-free uncertainty quantification through prediction sets that are guaranteed to include the ground truth with any user-specified probability. Yet, conformal prediction is not reliable under poisoning attacks where adversaries manipulate both training and calibration data, which can significantly alter prediction sets in practice. As a solution, we propose reliable prediction sets (RPS): the first efficient method for constructing conformal prediction sets with provable reliability guarantees under poisoning. To ensure reliability under training poisoning, we introduce smoothed score functions that reliably aggregate predictions of classifiers trained on distinct partitions of the training data. To ensure reliability under calibration poisoning, we construct multiple prediction sets, each calibrated on distinct subsets of the calibration data. We then aggregate them into a majority prediction set, which includes a class only if it appears in a majority of the individual sets. Both proposed aggregations mitigate the influence of datapoints in the training and calibration data on the final prediction set. We experimentally validate our approach on image classification tasks, achieving strong reliability while maintaining utility and preserving coverage on clean data. Overall, our approach represents an important step towards more trustworthy uncertainty quantification in the presence of data poisoning.         ",
    "url": "https://arxiv.org/abs/2410.09878",
    "authors": [
      "Yan Scholten",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.11698",
    "title": "AI Rules? Characterizing Reddit Community Policies Towards AI-Generated Content",
    "abstract": "           How are Reddit communities responding to AI-generated content? We explored this question through a large-scale analysis of subreddit community rules and their change over time. We collected the metadata and community rules for over $300,000$ public subreddits and measured the prevalence of rules governing AI. We labeled subreddits and AI rules according to existing taxonomies from the HCI literature and a new taxonomy we developed specific to AI rules. While rules about AI are still relatively uncommon, the number of subreddits with these rules more than doubled over the course of a year. AI rules are more common in larger subreddits and communities focused on art or celebrity topics, and less common in those focused on social support. These rules often focus on AI images and evoke, as justification, concerns about quality and authenticity. Overall, our findings illustrate the emergence of varied concerns about AI, in different community contexts. Platform designers and HCI researchers should heed these concerns if they hope to encourage community self-determination in the age of generative AI. We make our datasets public to enable future large-scale studies of community self-governance.         ",
    "url": "https://arxiv.org/abs/2410.11698",
    "authors": [
      "Travis Lloyd",
      "Jennah Gosciak",
      "Tung Nguyen",
      "Mor Naaman"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2410.12782",
    "title": "In-Context Learning Enables Robot Action Prediction in LLMs",
    "abstract": "           Recently, Large Language Models (LLMs) have achieved remarkable success using in-context learning (ICL) in the language domain. However, leveraging the ICL capabilities within LLMs to directly predict robot actions remains largely unexplored. In this paper, we introduce RoboPrompt, a framework that enables off-the-shelf text-only LLMs to directly predict robot actions through ICL without training. Our approach first heuristically identifies keyframes that capture important moments from an episode. Next, we extract end-effector actions from these keyframes as well as the estimated initial object poses, and both are converted into textual descriptions. Finally, we construct a structured template to form ICL demonstrations from these textual descriptions and a task instruction. This enables an LLM to directly predict robot actions at test time. Through extensive experiments and analysis, RoboPrompt shows stronger performance over zero-shot and ICL baselines in simulated and real-world settings. Our project page is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.12782",
    "authors": [
      "Yida Yin",
      "Zekai Wang",
      "Yuvan Sharma",
      "Dantong Niu",
      "Trevor Darrell",
      "Roei Herzig"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2410.15154",
    "title": "MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation and Rigorous Verification",
    "abstract": "           Large Language Models (LLMs) have demonstrated significant potential in code generation. However, in the factory automation sector, particularly motion control, manual programming, alongside inefficient and unsafe debugging practices, remains prevalent. This stems from the complex interplay of mechanical and electrical systems and stringent safety requirements. Moreover, most current AI-assisted motion control programming efforts focus on PLCs, with little attention given to high-level languages and function libraries. To address these challenges, we introduce MCCoder, an LLM-powered system tailored for generating motion control code, integrated with a soft-motion controller. MCCoder improves code generation through a structured workflow that combines multitask decomposition, hybrid retrieval-augmented generation (RAG), and iterative self-correction, utilizing a well-established motion library. Additionally, it integrates a 3D simulator for intuitive motion validation and logs of full motion trajectories for data verification, significantly enhancing accuracy and safety. In the absence of benchmark datasets and metrics tailored for evaluating motion control code generation, we propose MCEVAL, a dataset spanning motion tasks of varying complexity. Experiments show that MCCoder outperforms baseline models using Advanced RAG, achieving an overall performance gain of 33.09% and a 131.77% improvement on complex tasks in the MCEVAL dataset.         ",
    "url": "https://arxiv.org/abs/2410.15154",
    "authors": [
      "Yin Li",
      "Liangwei Wang",
      "Shiyuan Piao",
      "Boo-Ho Yang",
      "Ziyue Li",
      "Wei Zeng",
      "Fugee Tsung"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.18775",
    "title": "Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances",
    "abstract": "           Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first comprehensive benchmark designed to evaluate the robustness of watermarking methods against a wide range of image editing techniques, including image regeneration, global editing, local editing, and image-to-video generation. Through extensive evaluations of eleven representative watermarking methods against prevalent editing techniques, we demonstrate that most methods fail to detect watermarks after such edits. To address this limitation, we propose VINE, a watermarking method that significantly enhances robustness against various image editing techniques while maintaining high image quality. Our approach involves two key innovations: (1) we analyze the frequency characteristics of image editing and identify that blurring distortions exhibit similar frequency properties, which allows us to use them as surrogate attacks during training to bolster watermark robustness; (2) we leverage a large-scale pretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to achieve more imperceptible and robust watermark embedding. Experimental results show that our method achieves outstanding watermarking performance under various image editing techniques, outperforming existing methods in both image quality and robustness. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.18775",
    "authors": [
      "Shilin Lu",
      "Zihan Zhou",
      "Jiayou Lu",
      "Yuanzhi Zhu",
      "Adams Wai-Kin Kong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2410.19723",
    "title": "Sparse Decomposition of Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNN) exhibit superior performance in graph representation learning, but their inference cost can be high, due to an aggregation operation that can require a memory fetch for a very large number of nodes. This inference cost is the major obstacle to deploying GNN models with \\emph{online prediction} to reflect the potentially dynamic node features. To address this, we propose an approach to reduce the number of nodes that are included during aggregation. We achieve this through a sparse decomposition, learning to approximate node representations using a weighted sum of linearly transformed features of a carefully selected subset of nodes within the extended neighbourhood. The approach achieves linear complexity with respect to the average node degree and the number of layers in the graph neural network. We introduce an algorithm to compute the optimal parameters for the sparse decomposition, ensuring an accurate approximation of the original GNN model, and present effective strategies to reduce the training time and improve the learning process. We demonstrate via extensive experiments that our method outperforms other baselines designed for inference speedup, achieving significant accuracy gains with comparable inference times for both node classification and spatio-temporal forecasting tasks.         ",
    "url": "https://arxiv.org/abs/2410.19723",
    "authors": [
      "Yaochen Hu",
      "Mai Zeng",
      "Ge Zhang",
      "Pavel Rumiantsev",
      "Liheng Ma",
      "Yingxue Zhang",
      "Mark Coates"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.22311",
    "title": "Convex Formulations for Training Two-Layer ReLU Neural Networks",
    "abstract": "           Solving non-convex, NP-hard optimization problems is crucial for training machine learning models, including neural networks. However, non-convexity often leads to black-box machine learning models with unclear inner workings. While convex formulations have been used for verifying neural network robustness, their application to training neural networks remains less explored. In response to this challenge, we reformulate the problem of training infinite-width two-layer ReLU networks as a convex completely positive program in a finite-dimensional (lifted) space. Despite the convexity, solving this problem remains NP-hard due to the complete positivity constraint. To overcome this challenge, we introduce a semidefinite relaxation that can be solved in polynomial time. We then experimentally evaluate the tightness of this relaxation, demonstrating its competitive performance in test accuracy across a range of classification tasks.         ",
    "url": "https://arxiv.org/abs/2410.22311",
    "authors": [
      "Karthik Prakhya",
      "Tolga Birdal",
      "Alp Yurtsever"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2410.23517",
    "title": "HETRI: Heterogeneous Ising Multiprocessing",
    "abstract": "           Ising machines are effective solvers for complex combinatorial optimization problems. The idea is mapping the optimal solution(s) to a combinatorial optimization problem to the minimum energy state(s) of a physical system, which naturally converges to a minimum energy state upon perturbance. The underlying mathematical abstraction, the Ising model, can capture the dynamic behavior of different physical systems by mapping each problem variable to a spin which can interact with other spins. Ising model as a mathematical abstraction can be mapped to hardware using traditional devices. In this paper we instead focus on Ising machines which represent a network of physical spins directly implemented in hardware using, e.g., quantum bits or electronic oscillators. To eliminate the scalability bottleneck due to the mismatch in problem vs. Ising machine size and connectivity, in this paper we make the case for HETRI: Heterogeneous Ising Multiprocessing. HETRI organizes the maximum number of physical spins that the underlying technology supports in Ising cores; and multiple independent Ising cores, in Ising chips. Ising cores in a chip feature different inter-spin connectivity or spin counts to match the problem characteristics. We provide a detailed design space exploration and quantify the performance in terms of time or energy to solution and solution accuracy with respect to homogeneous alternatives under the very same hardware budget and considering the very same spin technology.         ",
    "url": "https://arxiv.org/abs/2410.23517",
    "authors": [
      "H\u00fcsrev C\u0131lasun",
      "Abhimanyu Kumar",
      "Ziqing Zeng",
      "Nafisa Sadaf Prova",
      "Sachin S. Sapatnekar",
      "Ulya R. Karpuzcu"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2410.24119",
    "title": "Leveraging Large Language Models for Code Translation and Software Development in Scientific Computing",
    "abstract": "           The emergence of foundational models and generative artificial intelligence (GenAI) is poised to transform productivity in scientific computing, especially in code development, refactoring, and translating from one programming language to another. However, because the output of GenAI cannot be guaranteed to be correct, manual intervention remains necessary. Some of this intervention can be automated through task-specific tools, alongside additional methodologies for correctness verification and effective prompt development. We explored the application of GenAI in assisting with code translation, language interoperability, and codebase inspection within a legacy Fortran codebase used to simulate particle interactions at the Large Hadron Collider (LHC). In the process, we developed a tool, CodeScribe, which combines prompt engineering with user supervision to establish an efficient process for code conversion. In this paper, we demonstrate how CodeScribe assists in converting Fortran code to C++, generating Fortran-C APIs for integrating legacy systems with modern C++ libraries, and providing developer support for code organization and algorithm implementation. We also address the challenges of AI-driven code translation and highlight its benefits for enhancing productivity in scientific computing workflows.         ",
    "url": "https://arxiv.org/abs/2410.24119",
    "authors": [
      "Akash Dhruv",
      "Anshu Dubey"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.05483",
    "title": "The Limits of Differential Privacy in Online Learning",
    "abstract": "           Differential privacy (DP) is a formal notion that restricts the privacy leakage of an algorithm when running on sensitive data, in which privacy-utility trade-off is one of the central problems in private data analysis. In this work, we investigate the fundamental limits of differential privacy in online learning algorithms and present evidence that separates three types of constraints: no DP, pure DP, and approximate DP. We first describe a hypothesis class that is online learnable under approximate DP but not online learnable under pure DP under the adaptive adversarial setting. This indicates that approximate DP must be adopted when dealing with adaptive adversaries. We then prove that any private online learner must make an infinite number of mistakes for almost all hypothesis classes. This essentially generalizes previous results and shows a strong separation between private and non-private settings since a finite mistake bound is always attainable (as long as the class is online learnable) when there is no privacy requirement.         ",
    "url": "https://arxiv.org/abs/2411.05483",
    "authors": [
      "Bo Li",
      "Wei Wang",
      "Peng Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.06518",
    "title": "Causal Representation Learning from Multimodal Biomedical Observations",
    "abstract": "           Prevalent in biomedical applications (e.g., human phenotype research), multimodal datasets can provide valuable insights into the underlying physiological mechanisms. However, current machine learning (ML) models designed to analyze these datasets often lack interpretability and identifiability guarantees, which are essential for biomedical research. Recent advances in causal representation learning have shown promise in identifying interpretable latent causal variables with formal theoretical guarantees. Unfortunately, most current work on multimodal distributions either relies on restrictive parametric assumptions or yields only coarse identification results, limiting their applicability to biomedical research that favors a detailed understanding of the mechanisms. In this work, we aim to develop flexible identification conditions for multimodal data and principled methods to facilitate the understanding of biomedical datasets. Theoretically, we consider a nonparametric latent distribution (c.f., parametric assumptions in previous work) that allows for causal relationships across potentially different modalities. We establish identifiability guarantees for each latent component, extending the subspace identification results from previous work. Our key theoretical contribution is the structural sparsity of causal connections between modalities, which, as we will discuss, is natural for a large collection of biomedical systems. Empirically, we present a practical framework to instantiate our theoretical insights. We demonstrate the effectiveness of our approach through extensive experiments on both numerical and synthetic datasets. Results on a real-world human phenotype dataset are consistent with established biomedical research, validating our theoretical and methodological framework.         ",
    "url": "https://arxiv.org/abs/2411.06518",
    "authors": [
      "Yuewen Sun",
      "Lingjing Kong",
      "Guangyi Chen",
      "Loka Li",
      "Gongxu Luo",
      "Zijian Li",
      "Yixuan Zhang",
      "Yujia Zheng",
      "Mengyue Yang",
      "Petar Stojanov",
      "Eran Segal",
      "Eric P. Xing",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2411.06742",
    "title": "Loss-tolerant neural video codec aware congestion control for real time video communication",
    "abstract": "           Because of reinforcement learning's (RL) ability to automatically create more adaptive controlling logics beyond the hand-crafted heuristics, numerous effort has been made to apply RL to congestion control (CC) design for real time video communication (RTC) applications and has successfully shown promising benefits over the rule-based RTC CCs. Online reinforcement learning is often adopted to train the RL models so the models can directly adapt to real network environments. However, its trail-and-error manner can also cause catastrophic degradation of the quality of experience (QoE) of RTC application at run time. Thus, safeguard strategies such as falling back to hand-crafted heuristics can be used to run along with RL models to guarantee the actions explored in the training sensible, despite that these safeguard strategies interrupt the learning process and make it more challenging to discover optimal RL policies. The recent emergence of loss-tolerant neural video codecs (NVC) naturally provides a layer of protection for the online learning of RL-based congestion control because of its resilience to packet losses, but such packet loss resilience have not been fully exploited in prior works yet. In this paper, we present a reinforcement learning (RL) based congestion control which can be aware of and takes advantage of packet loss tolerance characteristic of NVCs via reward in online RL learning. Through extensive evaluation on various videos and network traces in a simulated environment, we demonstrate that our NVC-aware CC running with the loss-tolerant NVC reduces the training time by 41\\% compared to other prior RL-based CCs. It also boosts the mean video quality by 0.3 to 1.6dB, lower the tail frame delay by 3 to 200ms, and reduces the video stalls by 20\\% to 77\\% in comparison with other baseline RTC CCs.         ",
    "url": "https://arxiv.org/abs/2411.06742",
    "authors": [
      "Zhengxu Xia",
      "Hanchen Li",
      "Junchen Jiang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2411.07107",
    "title": "Training Neural Networks as Recognizers of Formal Languages",
    "abstract": "           Characterizing the computational power of neural network architectures in terms of formal language theory remains a crucial line of research, as it describes lower and upper bounds on the reasoning capabilities of modern AI. However, when empirically testing these bounds, existing work often leaves a discrepancy between experiments and the formal claims they are meant to support. The problem is that formal language theory pertains specifically to recognizers: machines that receive a string as input and classify whether it belongs to a language. On the other hand, it is common instead to evaluate language models on proxy tasks, e.g., language modeling or sequence-to-sequence transduction, that are similar in only an informal sense to the underlying theory. We correct this mismatch by training and evaluating neural networks directly as binary classifiers of strings, using a general method that can be applied to a wide variety of languages. As part of this, we extend an algorithm recently proposed by Sn\u00e6bjarnarson et al. (2025) for efficient length-controlled sampling of strings from regular languages. We provide results on a variety of languages across the Chomsky hierarchy for three neural architectures: a simple RNN, an LSTM, and a causally-masked transformer. We find that the RNN and LSTM often outperform the transformer, and that auxiliary training objectives such as language modeling can help, although no single objective uniformly improves performance across languages and architectures. Our contributions will facilitate theoretically sound empirical testing of language recognition claims in future work. We have released our datasets as a benchmark called FLaRe (Formal Language Recognition), along with our code.         ",
    "url": "https://arxiv.org/abs/2411.07107",
    "authors": [
      "Alexandra Butoi",
      "Ghazal Khalighinejad",
      "Anej Svete",
      "Josef Valvoda",
      "Ryan Cotterell",
      "Brian DuSell"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.07758",
    "title": "AdaSemiCD: An Adaptive Semi-Supervised Change Detection Method Based on Pseudo-Label Evaluation",
    "abstract": "           Change Detection (CD) is an essential field in remote sensing, with a primary focus on identifying areas of change in bi-temporal image pairs captured at varying intervals of the same region by a satellite. The data annotation process for the CD task is both time-consuming and labor-intensive. To make better use of the scarce labeled data and abundant unlabeled data, we present an adaptive dynamic semi-supervised learning method, AdaSemiCD, to improve the use of pseudo-labels and optimize the training process. Initially, due to the extreme class imbalance inherent in CD, the model is more inclined to focus on the background class, and it is easy to confuse the boundary of the target object. Considering these two points, we develop a measurable evaluation metric for pseudo-labels that enhances the representation of information entropy by class rebalancing and amplification of confusing areas to give a larger weight to prospects change objects. Subsequently, to enhance the reliability of sample-wise pseudo-labels, we introduce the AdaFusion module, which is capable of dynamically identifying the most uncertain region and substituting it with more trustworthy content. Lastly, to ensure better training stability, we introduce the AdaEMA module, which updates the teacher model using only batches of trusted samples. Experimental results from LEVIR-CD, WHU-CD, and CDD datasets validate the efficacy and universality of our proposed adaptive training framework.         ",
    "url": "https://arxiv.org/abs/2411.07758",
    "authors": [
      "Ran Lingyan",
      "Wen Dongcheng",
      "Zhuo Tao",
      "Zhang Shizhou",
      "Zhang Xiuwei",
      "Zhang Yanning"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.09145",
    "title": "Self-Supervised Monocular 4D Scene Reconstruction for Egocentric Videos",
    "abstract": "           Egocentric videos provide valuable insights into human interactions with the physical world, which has sparked growing interest in the computer vision and robotics communities. A critical challenge in fully understanding the geometry and dynamics of egocentric videos is dense scene reconstruction. However, the lack of high-quality labeled datasets in this field has hindered the effectiveness of current supervised learning methods. In this work, we aim to address this issue by exploring an self-supervised dynamic scene reconstruction approach. We introduce EgoMono4D, a novel model that unifies the estimation of multiple variables necessary for Egocentric Monocular 4D reconstruction, including camera intrinsic, camera poses, and video depth, all within a fast feed-forward framework. Starting from pretrained single-frame depth and intrinsic estimation model, we extend it with camera poses estimation and align multi-frame results on large-scale unlabeled egocentric videos. We evaluate EgoMono4D in both in-domain and zero-shot generalization settings, achieving superior performance in dense pointclouds sequence reconstruction compared to all baselines. EgoMono4D represents the first attempt to apply self-supervised learning for pointclouds sequence reconstruction to the label-scarce egocentric field, enabling fast, dense, and generalizable reconstruction. The interactable visualization, code and trained models are released this https URL ",
    "url": "https://arxiv.org/abs/2411.09145",
    "authors": [
      "Chengbo Yuan",
      "Geng Chen",
      "Li Yi",
      "Yang Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2411.09957",
    "title": "Sublinear-time Collision Detection with a Polynomial Number of States in Population Protocols",
    "abstract": "           This paper addresses the collision detection problem in population protocols. The network consists of state machines called agents. At each time step, exactly one pair of agents is chosen uniformly at random to have an interaction, changing the states of the two agents. The collision detection problem involves each agent starting with an input integer between $1$ and $n$, where $n$ is the number of agents, and requires those agents to determine whether there are any duplicate input values among all agents. Specifically, the goal is for all agents to output false if all input values are distinct, and true otherwise. In this paper, we present an algorithm that requires a polynomial number of states per agent and solves the collision detection problem with probability one in sub-linear parallel time, both with high probability and in expectation. To the best of our knowledge, this algorithm is the first to solve the collision detection problem using a polynomial number of states within sublinear parallel time, affirmatively answering the question raised by Burman, Chen, Chen, Doty, Nowak, Severson, and Xu [PODC 2021] for the first time.         ",
    "url": "https://arxiv.org/abs/2411.09957",
    "authors": [
      "Takumi Araya",
      "Yuichi Sudo"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2411.16446",
    "title": "VQ-SGen: A Vector Quantized Stroke Representation for Creative Sketch Generation",
    "abstract": "           This paper presents VQ-SGen, a novel algorithm for high-quality creative sketch generation. Recent approaches have framed the task as pixel-based generation either as a whole or part-by-part, neglecting the intrinsic and contextual relationships among individual strokes, such as the shape and spatial positioning of both proximal and distant strokes. To overcome these limitations, we propose treating each stroke within a sketch as an entity and introducing a vector-quantized (VQ) stroke representation for fine-grained sketch generation. Our method follows a two-stage framework - in stage one, we decouple each stroke's shape and location information to ensure the VQ representation prioritizes stroke shape learning. In stage two, we feed the precise and compact representation into an auto-decoding Transformer to incorporate stroke semantics, positions, and shapes into the generation process. By utilizing tokenized stroke representation, our approach generates strokes with high fidelity and facilitates novel applications, such as text or class label conditioned generation and sketch completion. Comprehensive experiments demonstrate our method surpasses existing state-of-the-art techniques on the CreativeSketch dataset, underscoring its effectiveness.         ",
    "url": "https://arxiv.org/abs/2411.16446",
    "authors": [
      "Jiawei Wang",
      "Zhiming Cui",
      "Changjian Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2411.16832",
    "title": "Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing",
    "abstract": "           Recent advancements in diffusion models have made generative image editing more accessible, enabling creative edits but raising ethical concerns, particularly regarding malicious edits to human portraits that threaten privacy and identity security. Existing protection methods primarily rely on adversarial perturbations to nullify edits but often fail against diverse editing requests. We propose FaceLock, a novel approach to portrait protection that optimizes adversarial perturbations to destroy or significantly alter biometric information, rendering edited outputs biometrically unrecognizable. FaceLock integrates facial recognition and visual perception into perturbation optimization to provide robust protection against various editing attempts. We also highlight flaws in commonly used evaluation metrics and reveal how they can be manipulated, emphasizing the need for reliable assessments of protection. Experiments show FaceLock outperforms baselines in defending against malicious edits and is robust against purification techniques. Ablation studies confirm its stability and broad applicability across diffusion-based editing algorithms. Our work advances biometric defense and sets the foundation for privacy-preserving practices in image editing. The code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2411.16832",
    "authors": [
      "Hanhui Wang",
      "Yihua Zhang",
      "Ruizheng Bai",
      "Yue Zhao",
      "Sijia Liu",
      "Zhengzhong Tu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.17150",
    "title": "Distilling Spectral Graph for Object-Context Aware Open-Vocabulary Semantic Segmentation",
    "abstract": "           Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent vision-language models (VLMs), enabling segmentation beyond predefined categories through various learning schemes. Notably, training-free methods offer scalable, easily deployable solutions for handling unseen data, a key goal of OVSS. Yet, a critical issue persists: lack of object-level context consideration when segmenting complex objects in the challenging environment of OVSS based on arbitrary query prompts. This oversight limits models' ability to group semantically consistent elements within object and map them precisely to user-defined arbitrary classes. In this work, we introduce a novel approach that overcomes this limitation by incorporating object-level contextual knowledge within images. Specifically, our model enhances intra-object consistency by distilling spectral-driven features from vision foundation models into the attention mechanism of the visual encoder, enabling semantically coherent components to form a single object mask. Additionally, we refine the text embeddings with zero-shot object presence likelihood to ensure accurate alignment with the specific objects represented in the images. By leveraging object-level contextual knowledge, our proposed approach achieves state-of-the-art performance with strong generalizability across diverse datasets.         ",
    "url": "https://arxiv.org/abs/2411.17150",
    "authors": [
      "Chanyoung Kim",
      "Dayun Ju",
      "Woojung Han",
      "Ming-Hsuan Yang",
      "Seong Jae Hwang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.17388",
    "title": "Can LLMs be Good Graph Judger for Knowledge Graph Construction?",
    "abstract": "           In real-world scenarios, most of the data obtained from information retrieval (IR) system is unstructured. Converting natural language sentences into structured Knowledge Graphs (KGs) remains a critical challenge. The quality of constructed KGs may also impact the performance of some KG-dependent domains like GraphRAG systems and recommendation systems. Recently, Large Language Models (LLMs) have demonstrated impressive capabilities in addressing a wide range of natural language processing tasks. However, there are still challenges when utilizing LLMs to address the task of generating structured KGs. And we have identified three limitations with respect to existing KG construction methods. (1)There is a large amount of information and excessive noise in real-world documents, which could result in extracting messy information. (2)Native LLMs struggle to effectively extract accuracy knowledge from some domain-specific documents. (3)Hallucinations phenomenon cannot be overlooked when utilizing LLMs directly as an unsupervised method for constructing KGs. In this paper, we propose GraphJudger, a knowledge graph construction framework to address the aforementioned challenges. We introduce three innovative modules in our method, which are entity-centric iterative text denoising, knowledge aware instruction tuning and graph judgement, respectively. We seek to utilize the capacity of LLMs to function as a graph judger, a capability superior to their role only as a predictor for KG construction problems. Experiments conducted on two general text-graph pair datasets and one domain-specific text-graph pair dataset show superior performances compared to baseline methods. The code of our proposed method is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.17388",
    "authors": [
      "Haoyu Huang",
      "Chong Chen",
      "Conghui He",
      "Yang Li",
      "Jiawei Jiang",
      "Wentao Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.19895",
    "title": "GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting",
    "abstract": "           3D Gaussian Splatting (3DGS) has recently created impressive 3D assets for various applications. However, considering security, capacity, invisibility, and training efficiency, the copyright of 3DGS assets is not well protected as existing watermarking methods are unsuited for its rendering pipeline. In this paper, we propose GuardSplat, an innovative and efficient framework for watermarking 3DGS assets. Specifically, 1) We propose a CLIP-guided pipeline for optimizing the message decoder with minimal costs. The key objective is to achieve high-accuracy extraction by leveraging CLIP's aligning capability and rich representations, demonstrating exceptional capacity and efficiency. 2) We tailor a Spherical-Harmonic-aware (SH-aware) Message Embedding module for 3DGS, seamlessly embedding messages into the SH features of each 3D Gaussian while preserving the original 3D structure. This enables watermarking 3DGS assets with minimal fidelity trade-offs and prevents malicious users from removing the watermarks from the model files, meeting the demands for invisibility and security. 3) We present an Anti-distortion Message Extraction module to improve robustness against various distortions. Experiments demonstrate that GuardSplat outperforms state-of-the-art and achieves fast optimization speed. Project page is at this https URL, and Code is at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.19895",
    "authors": [
      "Zixuan Chen",
      "Guangcong Wang",
      "Jiahao Zhu",
      "Jianhuang Lai",
      "Xiaohua Xie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.19951",
    "title": "Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation",
    "abstract": "           Recent years have witnessed the success of Multimodal Large Language Models (MLLMs) in the vision understanding domain. The success of these models can largely be attributed to the dominant scaling law, which states that larger parameter sizes and data volumes contribute to better performance. Notably, data scaling has mainly been powered by automatic data pipelines, which center around the self-instruction of LLMs. The paradigm has been taken for granted for quite some time, but the study of the effectiveness of scaling with these data has been neglected for a long time. In this context, this work revisits scaling with synthetic data and focuses on developing video-LLMs from a data-centric perspective. Our main study approach is fine-tuning pre-trained image-LLMs with video data and investigating learning efficiency through data scaling. Results from our preliminary experiments reveal a low learning efficiency phenomenon when simply scaling up video data samples, which, through our probing, can be ascribed to a lack of instruction diversity. Aiming at this issue, we propose a data augmentation method called Sparrow, which synthesizes video-like samples from pure text instruction data. Mixing these synthetic samples with the video data enables a more efficient training scheme. Through comprehensive experiments, we demonstrate that our proposed method achieves performance comparable to or even superior to baselines trained with many more samples. Meanwhile, we find that incorporating these synthetic samples can boost the performance of long video understanding without training with long video data. The code and data examples are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.19951",
    "authors": [
      "Shukang Yin",
      "Chaoyou Fu",
      "Sirui Zhao",
      "Yunhang Shen",
      "Chunjiang Ge",
      "Yan Yang",
      "Zuwei Long",
      "Yuhan Dai",
      "Yongdong Luo",
      "Haoyu Cao",
      "Tong Xu",
      "Xing Sun",
      "Caifeng Shan",
      "Ran He",
      "Enhong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.00678",
    "title": "2DMamba: Efficient State Space Model for Image Representation with Applications on Giga-Pixel Whole Slide Image Classification",
    "abstract": "           Efficiently modeling large 2D contexts is essential for various fields including Giga-Pixel Whole Slide Imaging (WSI) and remote sensing. Transformer-based models offer high parallelism but face challenges due to their quadratic complexity for handling long sequences. Recently, Mamba introduced a selective State Space Model (SSM) with linear complexity and high parallelism, enabling effective and efficient modeling of wide context in 1D sequences. However, extending Mamba to vision tasks, which inherently involve 2D structures, results in spatial discrepancies due to the limitations of 1D sequence processing. On the other hand, current 2D SSMs inherently model 2D structures but they suffer from prohibitively slow computation due to the lack of efficient parallel algorithms. In this work, we propose 2DMamba, a novel 2D selective SSM framework that incorporates the 2D spatial structure of images into Mamba, with a highly optimized hardware-aware operator, adopting both spatial continuity and computational efficiency. We validate the versatility of our approach on both WSIs and natural images. Extensive experiments on 10 public datasets for WSI classification and survival analysis show that 2DMamba improves up to 2.48% in AUC, 3.11% in F1 score, 2.47% in accuracy and 5.52% in C-index. Additionally, integrating our method with VMamba for natural imaging yields 0.5 to 0.7 improvements in mIoU on the ADE20k semantic segmentation dataset, and 0.2% accuracy improvement on ImageNet-1K classification dataset. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.00678",
    "authors": [
      "Jingwei Zhang",
      "Anh Tien Nguyen",
      "Xi Han",
      "Vincent Quoc-Huy Trinh",
      "Hong Qin",
      "Dimitris Samaras",
      "Mahdi S. Hosseini"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.01999",
    "title": "AVA: Fault-tolerant Reconfigurable Geo-Replication on Heterogeneous Clusters",
    "abstract": "           Fault-tolerant replicated database systems consume less energy than the compute-intensive proof-of-work blockchain. Thus, they are promising technologies for the building blocks that assemble global financial infrastructure. To facilitate global scaling, clustered replication protocols are essential in orchestrating nodes into clusters based on proximity. However, the existing approaches often assume a homogeneous and fixed model in which the number of nodes across clusters is the same and fixed, and often limited to a fail-stop fault model. This paper presents heterogeneous and reconfigurable clustered replication for the general environment with arbitrary failures. In particular, we present AVA, a fault-tolerant reconfigurable geo-replication that allows dynamic membership: replicas are allowed to join and leave clusters. We formally state and prove the safety and liveness properties of the protocol. Furthermore, our replication protocol is consensus-agnostic, meaning each cluster can utilize any local replication mechanism. In our comprehensive evaluation, we instantiate our replication with both HotStuff and BFT-SMaRt. Experiments on geo-distributed deployments on Google Cloud demonstrates that members of clusters can be reconfigured without considerably affecting transaction processing, and that heterogeneity of clusters may significantly improve throughput.         ",
    "url": "https://arxiv.org/abs/2412.01999",
    "authors": [
      "Tejas Mane",
      "Xiao Li",
      "Mohammad Sadoghi",
      "Mohsen Lesani"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2412.04626",
    "title": "BigDocs: An Open Dataset for Training Multimodal Models on Document and Code Tasks",
    "abstract": "           Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at this https URL .         ",
    "url": "https://arxiv.org/abs/2412.04626",
    "authors": [
      "Juan Rodriguez",
      "Xiangru Jian",
      "Siba Smarak Panigrahi",
      "Tianyu Zhang",
      "Aarash Feizi",
      "Abhay Puri",
      "Akshay Kalkunte",
      "Fran\u00e7ois Savard",
      "Ahmed Masry",
      "Shravan Nayak",
      "Rabiul Awal",
      "Mahsa Massoud",
      "Amirhossein Abaskohi",
      "Zichao Li",
      "Suyuchen Wang",
      "Pierre-Andr\u00e9 No\u00ebl",
      "Mats Leon Richter",
      "Saverio Vadacchino",
      "Shubham Agarwal",
      "Sanket Biswas",
      "Sara Shanian",
      "Ying Zhang",
      "Noah Bolger",
      "Kurt MacDonald",
      "Simon Fauvel",
      "Sathwik Tejaswi",
      "Srinivas Sunkara",
      "Joao Monteiro",
      "Krishnamurthy DJ Dvijotham",
      "Torsten Scholak",
      "Nicolas Chapados",
      "Sepideh Kharagani",
      "Sean Hughes",
      "M. \u00d6zsu",
      "Siva Reddy",
      "Marco Pedersoli",
      "Yoshua Bengio",
      "Christopher Pal",
      "Issam Laradji",
      "Spandana Gella",
      "Perouz Taslakian",
      "David Vazquez",
      "Sai Rajeswar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2412.06819",
    "title": "A Physics-Constrained Neural Differential Equation Framework for Data-Driven Snowpack Simulation",
    "abstract": "           This paper presents a physics-constrained neural differential equation framework for parameterization, and employs it to model the time evolution of seasonal snow depth given hydrometeorological forcings. When trained on data from multiple SNOTEL sites, the parameterization predicts daily snow depth with under 9% median error and Nash Sutcliffe Efficiencies over 0.94 across a wide variety of snow climates. The parameterization also generalizes to new sites not seen during training, which is not often true for calibrated snow models. Requiring the parameterization to predict snow water equivalent in addition to snow depth only increases error to ~12%. The structure of the approach guarantees the satisfaction of physical constraints, enables these constraints during model training, and allows modeling at different temporal resolutions without additional retraining of the parameterization. These benefits hold potential in climate modeling, and could extend to other dynamical systems with physical constraints.         ",
    "url": "https://arxiv.org/abs/2412.06819",
    "authors": [
      "Andrew Charbonneau",
      "Katherine Deck",
      "Tapio Schneider"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2412.06858",
    "title": "Taming Sensitive Weights : Noise Perturbation Fine-tuning for Robust LLM Quantization",
    "abstract": "           Quantization is a critical step to enable efficient LLM serving under limited resource. However, previous research observes that certain weights in the LLM, known as outliers, are significantly sensitive to quantization noises. Existing quantization methods leave these outliers as floating points or higher precisions to retain performance, posting challenges on the efficient hardware deployment of the mixed-precision model. This work investigates an alternative way to tame the sensitive weights' impact on the quantization error, by reducing the loss Hessian trace with respect to outliers through an efficient fine-tuning process. We propose Noise Perturbation Fine-tuning (NPFT), which identifies outlier weights and add random weight perturbations on the outliers as the model going through a PEFT optimization. NPFT tames the sensitivity of outlier weights so that the quantized model performance can be improved without special treatment to the outliers. When applied to OPT and LLaMA models, our NPFT method achieves stable performance improvements for both uniform and non-uniform quantizers, while also offering better inference efficiency. Notably, the simplest RTN can achieve performance on par with GPTQ using our NPFT on LLaMA2-7B-4bits benchmark.         ",
    "url": "https://arxiv.org/abs/2412.06858",
    "authors": [
      "Dongwei Wang",
      "Huanrui Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2412.08755",
    "title": "Proactive Adversarial Defense: Harnessing Prompt Tuning in Vision-Language Models to Detect Unseen Backdoored Images",
    "abstract": "           Backdoor attacks pose a critical threat by embedding hidden triggers into inputs, causing models to misclassify them into target labels. While extensive research has focused on mitigating these attacks in object recognition models through weight fine-tuning, much less attention has been given to detecting backdoored samples directly. Given the vast datasets used in training, manual inspection for backdoor triggers is impractical, and even state-of-the-art defense mechanisms fail to fully neutralize their impact. To address this gap, we introduce a groundbreaking method to detect unseen backdoored images during both training and inference. Leveraging the transformative success of prompt tuning in Vision Language Models (VLMs), our approach trains learnable text prompts to differentiate clean images from those with hidden backdoor triggers. Experiments demonstrate the exceptional efficacy of this method, achieving an impressive average accuracy of 86% across two renowned datasets for detecting unseen backdoor triggers, establishing a new standard in backdoor defense.         ",
    "url": "https://arxiv.org/abs/2412.08755",
    "authors": [
      "Kyle Stein",
      "Andrew Arash Mahyari",
      "Guillermo Francia",
      "Eman El-Sheikh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.09584",
    "title": "BaB-ND: Long-Horizon Motion Planning with Branch-and-Bound and Neural Dynamics",
    "abstract": "           Neural-network-based dynamics models learned from observational data have shown strong predictive capabilities for scene dynamics in robotic manipulation tasks. However, their inherent non-linearity presents significant challenges for effective planning. Current planning methods, often dependent on extensive sampling or local gradient descent, struggle with long-horizon motion planning tasks involving complex contact events. In this paper, we present a GPU-accelerated branch-and-bound (BaB) framework for motion planning in manipulation tasks that require trajectory optimization over neural dynamics models. Our approach employs a specialized branching heuristics to divide the search space into subdomains, and applies a modified bound propagation method, inspired by the state-of-the-art neural network verifier alpha-beta-CROWN, to efficiently estimate objective bounds within these subdomains. The branching process guides planning effectively, while the bounding process strategically reduces the search space. Our framework achieves superior planning performance, generating high-quality state-action trajectories and surpassing existing methods in challenging, contact-rich manipulation tasks such as non-prehensile planar pushing with obstacles, object sorting, and rope routing in both simulated and real-world settings. Furthermore, our framework supports various neural network architectures, ranging from simple multilayer perceptrons to advanced graph neural dynamics models, and scales efficiently with different model sizes.         ",
    "url": "https://arxiv.org/abs/2412.09584",
    "authors": [
      "Keyi Shen",
      "Jiangwei Yu",
      "Jose Barreiros",
      "Huan Zhang",
      "Yunzhu Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2412.11390",
    "title": "A3E: Aligned and Augmented Adversarial Ensemble for Accurate, Robust and Privacy-Preserving EEG Decoding",
    "abstract": "           An electroencephalogram (EEG) based brain-computer interface (BCI) enables direct communication between the brain and external devices. However, EEG-based BCIs face at least three major challenges in real-world applications: data scarcity and individual differences, adversarial vulnerability, and data privacy. While previous studies have addressed one or two of these issues, simultaneous accommodation of all three challenges remains challenging and unexplored. This paper fills this gap, by proposing an Aligned and Augmented Adversarial Ensemble (A3E) algorithm and integrating it into three privacy protection scenarios (centralized source-free transfer, federated source-free transfer, and source data perturbation), achieving simultaneously accurate decoding, adversarial robustness, and privacy protection of EEG-based BCIs. Experiments on three public EEG datasets demonstrated that our proposed approach outperformed over 10 classic and state-of-the-art approaches in both accuracy and robustness in all three privacy-preserving scenarios, even outperforming state-of-the-art transfer learning approaches that do not consider privacy protection at all. This is the first time that three major challenges in EEG-based BCIs can be addressed simultaneously, significantly improving the practicalness of EEG decoding in real-world BCIs.         ",
    "url": "https://arxiv.org/abs/2412.11390",
    "authors": [
      "Xiaoqing Chen",
      "Tianwang Jia",
      "Dongrui Wu"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2412.14477",
    "title": "Graph Topic Modeling for Documents with Spatial or Covariate Dependencies",
    "abstract": "           We address the challenge of incorporating document-level metadata into topic modeling to improve topic mixture estimation. To overcome the computational complexity and lack of theoretical guarantees in existing Bayesian methods, we extend probabilistic latent semantic indexing (pLSI), a frequentist framework for topic modeling, by incorporating document-level covariates or known similarities between documents through a graph formalism. Modeling documents as nodes and edges denoting similarities, we propose a new estimator based on a fast graph-regularized iterative singular value decomposition (SVD) that encourages similar documents to share similar topic mixture proportions. We characterize the estimation error of our proposed method by deriving high-probability bounds and develop a specialized cross-validation method to optimize our regularization parameters. We validate our model through comprehensive experiments on synthetic datasets and three real-world corpora, demonstrating improved performance and faster inference compared to existing Bayesian methods.         ",
    "url": "https://arxiv.org/abs/2412.14477",
    "authors": [
      "Yeo Jin Jung",
      "Claire Donnat"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2412.18573",
    "title": "Top General Performance = Top Domain Performance? DomainCodeBench: A Multi-domain Code Generation Benchmark",
    "abstract": "           With the rapid advancement of large language models (LLMs), extensive research has been conducted to investigate the code generation capabilities of LLMs. However, existing efforts primarily focus on general-domain tasks, leaving LLMs' code generation performance in real-world application domains underexplored. This raises a critical question: can a model's general-domain coding ability reliably represent its ability in specialized domains? In this paper, we introduce DomainCodeBench, a multi-domain code generation benchmark designed to systematically evaluate LLMs across 12 software application domains and 15 programming languages. DomainCodeBench contains 2,400 manually verified tasks with ground truth, human-annotated docstrings, and fine-grained dependency information to ensure more coverage of domain-specific challenges. Specifically, we first identify the most popular application domains by topic mining. Then, we curate coding tasks based on commonly used frameworks and platforms in each domain. We obtain several findings through extensive experiments on DomainCodeBench with ten mainstream LLMs. (1) Performance decoupling: experiments reveal that top general-domain models do not consistently excel in specific application domains; (2) Domain-specific weaknesses: LLMs often fail due to domain knowledge gaps and third-party library misusage; (3) Contextual enhancement: we show that augmenting prompts with domain-specific knowledge improves performance by around 38.17%, providing actionable insights for performance optimization. Our replication package, including the benchmark, source code, and experimental results, is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.18573",
    "authors": [
      "Dewu Zheng",
      "Yanlin Wang",
      "Ensheng Shi",
      "Xilin Liu",
      "Yuchi Ma",
      "Hongyu Zhang",
      "Zibin Zheng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2412.18869",
    "title": "Enhancing Robustness in Manipulability Assessment: The Pseudo-Ellipsoid Approach",
    "abstract": "           Manipulability analysis is a methodology employed to assess the capacity of an articulated system, at a specific configuration, to produce motion or exert force in diverse directions. The conventional method entails generating a virtual ellipsoid using the system's configuration and model. Yet, this approach poses challenges when applied to systems such as the human body, where direct access to such information is limited, necessitating reliance on estimations. Any inaccuracies in these estimations can distort the ellipsoid's configuration, potentially compromising the accuracy of the manipulability assessment. To address this issue, this article extends the standard approach by introducing the concept of the manipulability pseudo-ellipsoid. Through a series of theoretical analyses, simulations, and experiments, the article demonstrates that the proposed method exhibits reduced sensitivity to noise in sensory information, consequently enhancing the robustness of the approach.         ",
    "url": "https://arxiv.org/abs/2412.18869",
    "authors": [
      "Erfan Shahriari",
      "Kim Kirstin Peper",
      "Matej Hoffmann",
      "Sami Haddadin"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2412.21016",
    "title": "Assessing the Robustness of LLM-based NLP Software via Automated Testing",
    "abstract": "           Benefiting from the advancements in LLMs, NLP software has undergone rapid development. Such software is widely employed in various safety-critical tasks, such as financial sentiment analysis, toxic content moderation, and log generation. Unlike traditional software, LLM-based NLP software relies on prompts and examples as inputs. Given the complexity of LLMs and the unpredictability of real-world inputs, quantitatively assessing the robustness of such software is crucial. However, to the best of our knowledge, no automated robustness testing methods have been specifically designed to evaluate the overall inputs of LLM-based NLP software. To this end, this paper introduces the first AutOmated Robustness Testing frAmework, AORTA, which reconceptualizes the testing process into a combinatorial optimization problem. Existing testing methods designed for DNN-based software can be applied to LLM-based software by AORTA, but their effectiveness is limited. To address this, we propose a novel testing method for LLM-based software within AORTA called Adaptive Beam Search. ABS is tailored for the expansive feature space of LLMs and improves testing effectiveness through an adaptive beam width and the capability for backtracking. We successfully embed 18 test methods in the designed framework AORTA and compared the test validity of ABS with three datasets and five threat models. ABS facilitates a more comprehensive and accurate robustness assessment before software deployment, with an average test success rate of 86.138%. Compared to the currently best-performing baseline PWWS, ABS significantly reduces the computational overhead by up to 3441.895 seconds per successful test case and decreases the number of queries by 218.762 times on average. Furthermore, test cases generated by ABS exhibit greater naturalness and transferability.         ",
    "url": "https://arxiv.org/abs/2412.21016",
    "authors": [
      "Mingxuan Xiao",
      "Yan Xiao",
      "Shunhui Ji",
      "Hanbo Cai",
      "Lei Xue",
      "Pengcheng Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2501.00880",
    "title": "Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction",
    "abstract": "           Employing LLMs for visual generation has recently become a research focus. However, the existing methods primarily transfer the LLM architecture to visual generation but rarely investigate the fundamental differences between language and vision. This oversight may lead to suboptimal utilization of visual generation capabilities within the LLM framework. In this paper, we explore the characteristics of visual embedding space under the LLM framework and discover that the correlation between visual embeddings can help achieve more stable and robust generation results. We present IAR, an Improved AutoRegressive Visual Generation Method that enhances the training efficiency and generation quality of LLM-based visual generation models. Firstly, we propose a Codebook Rearrangement strategy that uses balanced k-means clustering algorithm to rearrange the visual codebook into clusters, ensuring high similarity among visual features within each cluster. Leveraging the rearranged codebook, we propose a Cluster-oriented Cross-entropy Loss that guides the model to correctly predict the cluster where the token is located. This approach ensures that even if the model predicts the wrong token index, there is a high probability the predicted token is located in the correct cluster, which significantly enhances the generation quality and robustness. Extensive experiments demonstrate that our method consistently enhances the model training efficiency and performance from 100M to 1.4B, reducing the training time by half while achieving the same FID. Additionally, our approach can be applied to various LLM-based visual generation models and adheres to the scaling law, providing a promising direction for future research in LLM-based visual generation. The code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2501.00880",
    "authors": [
      "Teng Hu",
      "Jiangning Zhang",
      "Ran Yi",
      "Jieyu Weng",
      "Yabiao Wang",
      "Xianfang Zeng",
      "Zhucun Xue",
      "Lizhuang Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.01131",
    "title": "Privacy Bills of Materials: A Transparent Privacy Information Inventory for Collaborative Privacy Notice Generation in Mobile App Development",
    "abstract": "           Privacy regulations mandate that developers must provide authentic and comprehensive privacy notices, e.g., privacy policies or labels, to inform users of their apps' privacy practices. However, due to a lack of knowledge of privacy requirements, developers often struggle to create accurate privacy notices, especially for sophisticated mobile apps with complex features and in crowded development teams. To address these challenges, we introduce Privacy Bills of Materials (PriBOM), a systematic software engineering approach that leverages different development team roles to better capture and coordinate mobile app privacy information. PriBOM facilitates transparency-centric privacy documentation and specific privacy notice creation, enabling traceability and trackability of privacy practices. We present a pre-fill of PriBOM based on static analysis and privacy notice analysis techniques. We demonstrate the perceived usefulness of PriBOM through a human evaluation with 150 diverse participants. Our findings suggest that PriBOM could serve as a significant solution for providing privacy support in DevOps for mobile apps.         ",
    "url": "https://arxiv.org/abs/2501.01131",
    "authors": [
      "Zhen Tao",
      "Shidong Pan",
      "Zhenchang Xing",
      "Xiaoyu Sun",
      "Omar Haggag",
      "John Grundy",
      "Jingjie Li",
      "Liming Zhu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2501.01908",
    "title": "Training-Free Mitigation of Adversarial Attacks on Deep Learning-Based MRI Reconstruction",
    "abstract": "           Deep learning (DL) methods, especially those based on physics-driven DL, have become the state-of-the-art for reconstructing sub-sampled magnetic resonance imaging (MRI) data. However, studies have shown that these methods are susceptible to small adversarial input perturbations, or attacks, resulting in major distortions in the output images. Various strategies have been proposed to reduce the effects of these attacks, but they require retraining and may lower reconstruction quality for non-perturbed/clean inputs. In this work, we propose a novel approach for mitigating adversarial attacks on MRI reconstruction models without any retraining. Our framework is based on the idea of cyclic measurement consistency. The output of the model is mapped to another set of MRI measurements for a different sub-sampling pattern, and this synthesized data is reconstructed with the same model. Intuitively, without an attack, the second reconstruction is expected to be consistent with the first, while with an attack, disruptions are present. A novel objective function is devised based on this idea, which is minimized within a small ball around the attack input for mitigation. Experimental results show that our method substantially reduces the impact of adversarial perturbations across different datasets, attack types/strengths and PD-DL networks, and qualitatively and quantitatively outperforms conventional mitigation methods that involve retraining. Finally, we extend our mitigation method to two important practical scenarios: a blind setup, where the attack strength or algorithm is not known to the end user; and an adaptive attack setup, where the attacker has full knowledge of the defense strategy. Our approach remains effective in both cases.         ",
    "url": "https://arxiv.org/abs/2501.01908",
    "authors": [
      "Mahdi Saberi",
      "Chi Zhang",
      "Mehmet Akcakaya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2501.09292",
    "title": "To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic Retrieval Augmented Generation",
    "abstract": "           Retrieval-Augmented Generation equips large language models with the capability to retrieve external knowledge, thereby mitigating hallucinations by incorporating information beyond the model's intrinsic abilities. However, most prior works have focused on invoking retrieval deterministically, which makes it unsuitable for tasks such as long-form question answering. Instead, dynamically performing retrieval by invoking it only when the underlying LLM lacks the required knowledge can be more efficient. In this context, we delve deeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring multiple uncertainty detection methods. We evaluate these methods for the task of long-form question answering, employing dynamic retrieval, and present our comparisons. Our findings suggest that uncertainty detection metrics, such as Degree Matrix Jaccard and Eccentricity, can reduce the number of retrieval calls by almost half, with only a slight reduction in question-answering accuracy.         ",
    "url": "https://arxiv.org/abs/2501.09292",
    "authors": [
      "Kaustubh D. Dhole"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2501.10240",
    "title": "Challenges and recommendations for Electronic Health Records data extraction and preparation for dynamic prediction modelling in hospitalized patients -- a practical guide",
    "abstract": "           Dynamic predictive modelling using electronic health record (EHR) data has gained significant attention in recent years. The reliability and trustworthiness of such models depend heavily on the quality of the underlying data, which is, in part, determined by the stages preceding the model development: data extraction from EHR systems and data preparation. In this article, we identified over forty challenges encountered during these stages and provide actionable recommendations for addressing them. These challenges are organized into four categories: cohort definition, outcome definition, feature engineering, and data cleaning. This comprehensive list serves as a practical guide for data extraction engineers and researchers, promoting best practices and improving the quality and real-world applicability of dynamic prediction models in clinical settings.         ",
    "url": "https://arxiv.org/abs/2501.10240",
    "authors": [
      "Elena Albu",
      "Shan Gao",
      "Pieter Stijnen",
      "Frank E. Rademakers",
      "Bas C T van Bussel",
      "Taya Collyer",
      "Tina Hernandez-Boussard",
      "Laure Wynants",
      "Ben Van Calster"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.11039",
    "title": "Beyond Any-Shot Adaptation: Predicting Optimization Outcome for Robustness Gains without Extra Pay",
    "abstract": "           Foundation models have revolutionized general-purpose problem-solving, offering rapid task adaptation through pretraining, meta-training, and finetuning. Recent crucial advances in these paradigms reveal the importance of challenging task prioritized sampling to enhance adaptation robustness under distribution shifts. However, ranking task difficulties over iteration as a preliminary step typically requires exhaustive task evaluation, which is practically unaffordable in computation and data-annotation. This study provides a novel perspective to illuminate the possibility of leveraging the dual importance of adaptation robustness and learning efficiency, particularly in scenarios where task evaluation is risky or costly, such as iterative agent-environment interactions for robotic policy evaluation or computationally intensive inference steps for finetuning foundation models. Firstly, we introduce Model Predictive Task Sampling (MPTS), a framework that bridges the task space and adaptation risk landscape, providing a theoretical foundation for robust active task sampling. MPTS employs a generative model to characterize the episodic optimization process and predicts task-specific adaptation risk via posterior inference. The resulting risk learner amortizes the costly evaluation of task adaptation performance and provably approximates task difficulty rankings. MPTS seamlessly integrates into zero-shot, few-shot, and supervised finetuning settings. Empirically, we conduct extensive experiments in pattern recognition using foundation models and sequential decision-making. Our results demonstrate that MPTS significantly enhances adaptation robustness for tail or out-of-distribution (OOD) tasks and improves learning efficiency compared to state-of-the-art (SOTA) methods. The code is available at the project site this https URL.         ",
    "url": "https://arxiv.org/abs/2501.11039",
    "authors": [
      "Qi Cheems Wang",
      "Zehao Xiao",
      "Yixiu Mao",
      "Yun Qu",
      "Jiayi Shen",
      "Yiqin Lv",
      "Xiangyang Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.12547",
    "title": "Human-like conceptual representations emerge from language prediction",
    "abstract": "           Recent advances in large language models (LLMs) provide a new opportunity to address the long-standing question of how concepts are represented and organized in the mind, which is central to unravelling the nature of human cognition. Here, we reframed the classic reverse dictionary task to simulate human concept inference in context and investigated the emergence of human-like conceptual representations within LLMs. We found that LLMs were able to infer concepts from definitional descriptions and construct representation spaces that converge towards a shared, context-independent structure. These representations effectively predicted human behavioural judgments and aligned well with neural activity patterns in the human brain, offering evidence for biological plausibility. These findings demonstrate that human-like conceptual representations and organization can naturally emerge from language prediction, even without real-world grounding. Our work supports the view that LLMs serve as valuable tools for understanding complex human cognition and paves the way for better alignment between artificial and human intelligence.         ",
    "url": "https://arxiv.org/abs/2501.12547",
    "authors": [
      "Ningyu Xu",
      "Qi Zhang",
      "Chao Du",
      "Qiang Luo",
      "Xipeng Qiu",
      "Xuanjing Huang",
      "Menghan Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.13125",
    "title": "Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction",
    "abstract": "           In designing multiple-choice questions (MCQs) in education, creating plausible distractors is crucial for identifying students' misconceptions and gaps in knowledge and accurately assessing their understanding. However, prior studies on distractor generation have not paid sufficient attention to enhancing the difficulty of distractors, resulting in reduced effectiveness of MCQs. This study presents a pipeline for training a model to generate distractors that are more likely to be selected by students. First, we train a pairwise ranker to reason about students' misconceptions and assess the relative plausibility of two distractors. Using this model, we create a dataset of pairwise distractor ranks and then train a distractor generator via Direct Preference Optimization (DPO) to generate more plausible distractors. Experiments on computer science subjects (Python, DB, MLDL) demonstrate that our pairwise ranker effectively identifies students' potential misunderstandings and achieves ranking accuracy comparable to human experts. Furthermore, our distractor generator outperforms several baselines in generating plausible distractors and produces questions with a higher item discrimination index (DI).         ",
    "url": "https://arxiv.org/abs/2501.13125",
    "authors": [
      "Yooseop Lee",
      "Suin Kim",
      "Yohan Jo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.14211",
    "title": "When GNNs meet symmetry in ILPs: an orbit-based feature augmentation approach",
    "abstract": "           A common characteristic in integer linear programs (ILPs) is symmetry, allowing variables to be permuted without altering the underlying problem structure. Recently, GNNs have emerged as a promising approach for solving ILPs. However, a significant challenge arises when applying GNNs to ILPs with symmetry: classic GNN architectures struggle to differentiate between symmetric variables, which limits their predictive accuracy. In this work, we investigate the properties of permutation equivariance and invariance in GNNs, particularly in relation to the inherent symmetry of ILP formulations. We reveal that the interaction between these two factors contributes to the difficulty of distinguishing between symmetric variables. To address this challenge, we explore the potential of feature augmentation and propose several guiding principles for constructing augmented features. Building on these principles, we develop an orbit-based augmentation scheme that first groups symmetric variables and then samples augmented features for each group from a discrete uniform distribution. Empirical results demonstrate that our proposed approach significantly enhances both training efficiency and predictive performance.         ",
    "url": "https://arxiv.org/abs/2501.14211",
    "authors": [
      "Qian Chen",
      "Lei Li",
      "Qian Li",
      "Jianghua Wu",
      "Akang Wang",
      "Ruoyu Sun",
      "Xiaodong Luo",
      "Tsung-Hui Chang",
      "Qingjiang Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2501.14892",
    "title": "Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in Graph-Augmented LLMs",
    "abstract": "           In knowledge-intensive tasks, especially in high-stakes domains like medicine and law, it is critical not only to retrieve relevant information but also to provide causal reasoning and explainability. Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, integrating knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has emerged as an effective solution. Traditional Graph RAG methods often rely on simple graph traversal or semantic similarity, which do not capture causal relationships or align well with the model's internal reasoning steps. This paper proposes a novel pipeline that filters large knowledge graphs to emphasize cause-effect edges, aligns the retrieval process with the model's chain-of-thought (CoT), and enhances reasoning through multi-stage path improvements. Experiments on medical question-answering tasks show consistent gains, with up to a 10\\% absolute improvement across multiple large language models (LLMs). This approach demonstrates the value of combining causal reasoning with stepwise retrieval, leading to more interpretable and logically grounded solutions for complex queries.         ",
    "url": "https://arxiv.org/abs/2501.14892",
    "authors": [
      "Hang Luo",
      "Jian Zhang",
      "Chujun Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2501.15035",
    "title": "Semi-supervised Anomaly Detection with Extremely Limited Labels in Dynamic Graphs",
    "abstract": "           Semi-supervised graph anomaly detection (GAD) has recently received increasing attention, which aims to distinguish anomalous patterns from graphs under the guidance of a moderate amount of labeled data and a large volume of unlabeled data. Although these proposed semi-supervised GAD methods have achieved great success, their superior performance will be seriously degraded when the provided labels are extremely limited due to some unpredictable factors. Besides, the existing methods primarily focus on anomaly detection in static graphs, and little effort was paid to consider the continuous evolution characteristic of graphs over time (dynamic graphs). To address these challenges, we propose a novel GAD framework (EL$^{2}$-DGAD) to tackle anomaly detection problem in dynamic graphs with extremely limited labels. Specifically, a transformer-based graph encoder model is designed to more effectively preserve evolving graph structures beyond the local neighborhood. Then, we incorporate an ego-context hypersphere classification loss to classify temporal interactions according to their structure and temporal neighborhoods while ensuring the normal samples are mapped compactly against anomalous data. Finally, the above loss is further augmented with an ego-context contrasting module which utilizes unlabeled data to enhance model generalization. Extensive experiments on four datasets and three label rates demonstrate the effectiveness of the proposed method in comparison to the existing GAD methods.         ",
    "url": "https://arxiv.org/abs/2501.15035",
    "authors": [
      "Jiazhen Chen",
      "Sichao Fu",
      "Zheng Ma",
      "Mingbin Feng",
      "Tony S. Wirjanto",
      "Qinmu Peng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.16944",
    "title": "Exact Computation of Any-Order Shapley Interactions for Graph Neural Networks",
    "abstract": "           Albeit the ubiquitous use of Graph Neural Networks (GNNs) in machine learning (ML) prediction tasks involving graph-structured data, their interpretability remains challenging. In explainable artificial intelligence (XAI), the Shapley Value (SV) is the predominant method to quantify contributions of individual features to a ML model's output. Addressing the limitations of SVs in complex prediction models, Shapley Interactions (SIs) extend the SV to groups of features. In this work, we explain single graph predictions of GNNs with SIs that quantify node contributions and interactions among multiple nodes. By exploiting the GNN architecture, we show that the structure of interactions in node embeddings are preserved for graph prediction. As a result, the exponential complexity of SIs depends only on the receptive fields, i.e. the message-passing ranges determined by the connectivity of the graph and the number of convolutional layers. Based on our theoretical results, we introduce GraphSHAP-IQ, an efficient approach to compute any-order SIs exactly. GraphSHAP-IQ is applicable to popular message passing techniques in conjunction with a linear global pooling and output layer. We showcase that GraphSHAP-IQ substantially reduces the exponential complexity of computing exact SIs on multiple benchmark datasets. Beyond exact computation, we evaluate GraphSHAP-IQ's approximation of SIs on popular GNN architectures and compare with existing baselines. Lastly, we visualize SIs of real-world water distribution networks and molecule structures using a SI-Graph.         ",
    "url": "https://arxiv.org/abs/2501.16944",
    "authors": [
      "Maximilian Muschalik",
      "Fabian Fumagalli",
      "Paolo Frazzetto",
      "Janine Strotherm",
      "Luca Hermes",
      "Alessandro Sperduti",
      "Eyke H\u00fcllermeier",
      "Barbara Hammer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.01684",
    "title": "Leveraging Joint Predictive Embedding and Bayesian Inference in Graph Self Supervised Learning",
    "abstract": "           Graph representation learning has emerged as a cornerstone for tasks like node classification and link prediction, yet prevailing self-supervised learning (SSL) methods face challenges such as computational inefficiency, reliance on contrastive objectives, and representation collapse. Existing approaches often depend on feature reconstruction, negative sampling, or complex decoders, which introduce training overhead and hinder generalization. Further, current techniques which address such limitations fail to account for the contribution of node embeddings to a certain prediction in the absence of labeled nodes. To address these limitations, we propose a novel joint embedding predictive framework for graph SSL that eliminates contrastive objectives and negative sampling while preserving semantic and structural information. Additionally, we introduce a semantic-aware objective term that incorporates pseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node discriminability by evaluating latent feature contributions. Extensive experiments demonstrate that our framework outperforms state-of-the-art graph SSL methods across benchmarks, achieving superior performance without contrastive loss or complex decoders. Key innovations include (1) a non-contrastive, view-invariant joint embedding predictive architecture, (2) Leveraging single context and multiple targets relationship between subgraphs, and (3) GMM-based pseudo-label scoring to capture semantic contributions. This work advances graph SSL by offering a computationally efficient, collapse-resistant paradigm that bridges spatial and semantic graph features for downstream tasks. The code for our paper can be found at this https URL ",
    "url": "https://arxiv.org/abs/2502.01684",
    "authors": [
      "Srinitish Srinivasan",
      "Omkumar CU"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2502.02027",
    "title": "From Fog to Failure: The Unintended Consequences of Dehazing on Object Detection in Clear Images",
    "abstract": "           This study explores the challenges of integrating human visual cue-based dehazing into object detection, given the selective nature of human perception. While human vision adapts dynamically to environmental conditions, computational dehazing does not always enhance detection uniformly. We propose a multi-stage framework where a lightweight detector identifies regions of interest (RoIs), which are then improved via spatial attention-based dehazing before final detection by a heavier model. Though effective in foggy conditions, this approach unexpectedly degrades the performance on clear images. We analyze this phenomenon, investigate possible causes, and offer insights for designing hybrid pipelines that balance enhancement and detection. Our findings highlight the need for selective preprocessing and challenge assumptions about universal benefits from cascading transformations.         ",
    "url": "https://arxiv.org/abs/2502.02027",
    "authors": [
      "Ashutosh Kumar",
      "Aman Chadha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.02454",
    "title": "IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View Automated Prompt Learning",
    "abstract": "           Using extensive training data from SA-1B, the Segment Anything Model (SAM) has demonstrated exceptional generalization and zero-shot capabilities, attracting widespread attention in areas such as medical image segmentation and remote sensing image segmentation. However, its performance in the field of image manipulation detection remains largely unexplored and unconfirmed. There are two main challenges in applying SAM to image manipulation detection: a) reliance on manual prompts, and b) the difficulty of single-view information in supporting cross-dataset generalization. To address these challenges, we develops a cross-view prompt learning paradigm called IMDPrompter based on SAM. Benefiting from the design of automated prompts, IMDPrompter no longer relies on manual guidance, enabling automated detection and localization. Additionally, we propose components such as Cross-view Feature Perception, Optimal Prompt Selection, and Cross-View Prompt Consistency, which facilitate cross-view perceptual learning and guide SAM to generate accurate masks. Extensive experimental results from five datasets (CASIA, Columbia, Coverage, IMD2020, and NIST16) validate the effectiveness of our proposed method.         ",
    "url": "https://arxiv.org/abs/2502.02454",
    "authors": [
      "Quan Zhang",
      "Yuxin Qi",
      "Xi Tang",
      "Jinwei Fang",
      "Xi Lin",
      "Ke Zhang",
      "Chun Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.02975",
    "title": "TGB-Seq Benchmark: Challenging Temporal GNNs with Complex Sequential Dynamics",
    "abstract": "           Future link prediction is a fundamental challenge in various real-world dynamic systems. To address this, numerous temporal graph neural networks (temporal GNNs) and benchmark datasets have been developed. However, these datasets often feature excessive repeated edges and lack complex sequential dynamics, a key characteristic inherent in many real-world applications such as recommender systems and ``Who-To-Follow'' on social networks. This oversight has led existing methods to inadvertently downplay the importance of learning sequential dynamics, focusing primarily on predicting repeated edges. In this study, we demonstrate that existing methods, such as GraphMixer and DyGFormer, are inherently incapable of learning simple sequential dynamics, such as ``a user who has followed OpenAI and Anthropic is more likely to follow AI at Meta next.'' Motivated by this issue, we introduce the Temporal Graph Benchmark with Sequential Dynamics (TGB-Seq), a new benchmark carefully curated to minimize repeated edges, challenging models to learn sequential dynamics and generalize to unseen edges. TGB-Seq comprises large real-world datasets spanning diverse domains, including e-commerce interactions, movie ratings, business reviews, social networks, citation networks and web link networks. Benchmarking experiments reveal that current methods usually suffer significant performance degradation and incur substantial training costs on TGB-Seq, posing new challenges and opportunities for future research. TGB-Seq datasets, leaderboards, and example codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.02975",
    "authors": [
      "Lu Yi",
      "Jie Peng",
      "Yanping Zheng",
      "Fengran Mo",
      "Zhewei Wei",
      "Yuhang Ye",
      "Yue Zixuan",
      "Zengfeng Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.07601",
    "title": "Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models",
    "abstract": "           Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the traditional unsupervised AD setting that requires a large number of normal samples to train a model, ZSAD is more practical for handling data-restricted real-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have shown revolutionary reasoning capabilities in various vision tasks. However, the reasoning of image abnormalities remains underexplored due to the lack of corresponding datasets and benchmarks. To facilitate research in AD & reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through investigation with our benchmark, we reveal that current MLLMs like GPT-4o cannot accurately detect and describe fine-grained anomalous details in images. To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning. Inspired by human behavior in visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM) mechanism to adaptively select and emphasize abnormal visual tokens. Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning. Extensions to medical and 3D AD are provided for future study. The link to our project page: this https URL ",
    "url": "https://arxiv.org/abs/2502.07601",
    "authors": [
      "Jiacong Xu",
      "Shao-Yuan Lo",
      "Bardia Safaei",
      "Vishal M. Patel",
      "Isht Dwivedi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.13191",
    "title": "On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis",
    "abstract": "           Spiking Neural Networks (SNNs) are increasingly explored for their energy efficiency and robustness in real-world applications, yet their privacy risks remain largely unexamined. In this work, we investigate the susceptibility of SNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an adversary attempts to determine whether a given sample was part of the training dataset. While prior work suggests that SNNs may offer inherent robustness due to their discrete, event-driven nature, we find that its resilience diminishes as latency (T) increases. Furthermore, we introduce an input dropout strategy under black box setting, that significantly enhances membership inference in SNNs. Our findings challenge the assumption that SNNs are inherently more secure, and even though they are expected to be better, our results reveal that SNNs exhibit privacy vulnerabilities that are equally comparable to Artificial Neural Networks (ANNs). Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.13191",
    "authors": [
      "Junyi Guan",
      "Abhijith Sharma",
      "Chong Tian",
      "Salem Lahlou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.13257",
    "title": "Random Forest Autoencoders for Guided Representation Learning",
    "abstract": "           Decades of research have produced robust methods for unsupervised data visualization, yet supervised visualization$\\unicode{x2013}$where expert labels guide representations$\\unicode{x2013}$remains underexplored, as most supervised approaches prioritize classification over visualization. Recently, RF-PHATE, a diffusion-based manifold learning method leveraging random forests and information geometry, marked significant progress in supervised visualization. However, its lack of an explicit mapping function limits scalability and prevents application to unseen data, posing challenges for large datasets and label-scarce scenarios. To overcome these limitations, we introduce Random Forest Autoencoders (RF-AE), a neural network-based framework for out-of-sample kernel extension that combines the flexibility of autoencoders with the supervised learning strengths of random forests and the geometry captured by RF-PHATE. RF-AE enables efficient out-of-sample supervised visualization and outperforms existing methods, including RF-PHATE's standard kernel extension, in both accuracy and interpretability. Additionally, RF-AE is robust to the choice of hyper-parameters and generalizes to any kernel-based dimensionality reduction method.         ",
    "url": "https://arxiv.org/abs/2502.13257",
    "authors": [
      "Adrien Aumon",
      "Shuang Ni",
      "Myriam Lizotte",
      "Guy Wolf",
      "Kevin R. Moon",
      "Jake S. Rhodes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.13459",
    "title": "Poisoned Source Code Detection in Code Models",
    "abstract": "           Deep learning models have gained popularity for conducting various tasks involving source code. However, their black-box nature raises concerns about potential risks. One such risk is a poisoning attack, where an attacker intentionally contaminates the training set with malicious samples to mislead the model's predictions in specific scenarios. To protect source code models from poisoning attacks, we introduce CodeGarrison (CG), a hybrid deep-learning model that relies on code embeddings to identify poisoned code samples. We evaluated CG against the state-of-the-art technique ONION for detecting poisoned samples generated by DAMP, MHM, ALERT, as well as a novel poisoning technique named CodeFooler. Results showed that CG significantly outperformed ONION with an accuracy of 93.5%. We also tested CG's robustness against unknown attacks, achieving an average accuracy of 85.6% in identifying poisoned samples across the four attacks mentioned above.         ",
    "url": "https://arxiv.org/abs/2502.13459",
    "authors": [
      "Ehab Ghannoum",
      "Mohammad Ghafari"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15483",
    "title": "MoMa: A Modular Deep Learning Framework for Material Property Prediction",
    "abstract": "           Deep learning methods for material property prediction have been widely explored to advance materials discovery. However, the prevailing pre-train then fine-tune paradigm often fails to address the inherent diversity and disparity of material tasks. To overcome these challenges, we introduce MoMa, a Modular framework for Materials that first trains specialized modules across a wide range of tasks and then adaptively composes synergistic modules tailored to each downstream scenario. Evaluation across 17 datasets demonstrates the superiority of MoMa, with a substantial 14% average improvement over the strongest baseline. Few-shot and continual learning experiments further highlight MoMa's potential for real-world applications. Pioneering a new paradigm of modular material learning, MoMa will be open-sourced to foster broader community collaboration.         ",
    "url": "https://arxiv.org/abs/2502.15483",
    "authors": [
      "Botian Wang",
      "Yawen Ouyang",
      "Yaohui Li",
      "Yiqun Wang",
      "Haorui Cui",
      "Jianbing Zhang",
      "Xiaonan Wang",
      "Wei-Ying Ma",
      "Hao Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)"
    ]
  },
  {
    "id": "arXiv:2502.15654",
    "title": "Machine-generated text detection prevents language model collapse",
    "abstract": "           As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since online data is the primary resource for LLM pre-training, subsequent models could be trained on an unknown portion of synthetic samples. This will lead to model collapse, a degenerative process whereby LLMs reinforce their own errors, and ultimately yield a declining performance. In this study, we investigate the impact of decoding strategy on model collapse, analysing the characteristics of text at each model generation, the similarity to human references, and the resulting model performance. Using the decoding strategies that lead to the most significant degradation, we evaluate model collapse in more realistic scenarios where the origin of the data (human or synthetic) is unknown. We train a machine-generated text detector and propose an importance sampling approach to alleviate model collapse. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on the open-ended text generation task. We demonstrate that it can not only prevent model collapse but also improve performance when sufficient human-authored samples are present.         ",
    "url": "https://arxiv.org/abs/2502.15654",
    "authors": [
      "George Drayson",
      "Vasileios Lampos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.16794",
    "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding",
    "abstract": "           Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available: this https URL.         ",
    "url": "https://arxiv.org/abs/2502.16794",
    "authors": [
      "Xilin Jiang",
      "Sukru Samet Dindar",
      "Vishal Choudhari",
      "Stephan Bickel",
      "Ashesh Mehta",
      "Guy M McKhann",
      "Daniel Friedman",
      "Adeen Flinker",
      "Nima Mesgarani"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2502.17912",
    "title": "Decoupled Graph Energy-based Model for Node Out-of-Distribution Detection on Heterophilic Graphs",
    "abstract": "           Despite extensive research efforts focused on OOD detection on images, OOD detection on nodes in graph learning remains underexplored. The dependence among graph nodes hinders the trivial adaptation of existing approaches on images that assume inputs to be i.i.d. sampled, since many unique features and challenges specific to graphs are not considered, such as the heterophily issue. Recently, GNNSafe, which considers node dependence, adapted energy-based detection to the graph domain with state-of-the-art performance, however, it has two serious issues: 1) it derives node energy from classification logits without specifically tailored training for modeling data distribution, making it less effective at recognizing OOD data; 2) it highly relies on energy propagation, which is based on homophily assumption and will cause significant performance degradation on heterophilic graphs, where the node tends to have dissimilar distribution with its neighbors. To address the above issues, we suggest training EBMs by MLE to enhance data distribution modeling and remove energy propagation to overcome the heterophily issues. However, training EBMs via MLE requires performing MCMC sampling on both node feature and node neighbors, which is challenging due to the node interdependence and discrete graph topology. To tackle the sampling challenge, we introduce DeGEM, which decomposes the learning process into two parts: a graph encoder that leverages topology information for node representations and an energy head that operates in latent space. Extensive experiments validate that DeGEM, without OOD exposure during training, surpasses previous state-of-the-art methods, achieving an average AUROC improvement of 6.71% on homophilic graphs and 20.29% on heterophilic graphs, and even outperform methods trained with OOD exposure. Our code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2502.17912",
    "authors": [
      "Yuhan Chen",
      "Yihong Luo",
      "Yifan Song",
      "Pengwen Dai",
      "Jing Tang",
      "Xiaochun Cao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.20580",
    "title": "Training Large Neural Networks With Low-Dimensional Error Feedback",
    "abstract": "           Training deep neural networks typically relies on backpropagating high dimensional error signals a computationally intensive process with little evidence supporting its implementation in the brain. However, since most tasks involve low-dimensional outputs, we propose that low-dimensional error signals may suffice for effective learning. To test this hypothesis, we introduce a novel local learning rule based on Feedback Alignment that leverages indirect, low-dimensional error feedback to train large networks. Our method decouples the backward pass from the forward pass, enabling precise control over error signal dimensionality while maintaining high-dimensional representations. We begin with a detailed theoretical derivation for linear networks, which forms the foundation of our learning framework, and extend our approach to nonlinear, convolutional, and transformer architectures. Remarkably, we demonstrate that even minimal error dimensionality on the order of the task dimensionality can achieve performance matching that of traditional backpropagation. Furthermore, our rule enables efficient training of convolutional networks, which have previously been resistant to Feedback Alignment methods, with minimal error. This breakthrough not only paves the way toward more biologically accurate models of learning but also challenges the conventional reliance on high-dimensional gradient signals in neural network training. Our findings suggest that low-dimensional error signals can be as effective as high-dimensional ones, prompting a reevaluation of gradient-based learning in high-dimensional systems. Ultimately, our work offers a fresh perspective on neural network optimization and contributes to understanding learning mechanisms in both artificial and biological systems.         ",
    "url": "https://arxiv.org/abs/2502.20580",
    "authors": [
      "Maher Hanut",
      "Jonathan Kadmon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2503.01001",
    "title": "Towards An Efficient LLM Training Paradigm for CTR Prediction",
    "abstract": "           Large Language Models (LLMs) have demonstrated tremendous potential as the next-generation ranking-based recommendation system. Many recent works have shown that LLMs can significantly outperform conventional click-through-rate (CTR) prediction approaches. Despite such promising results, the computational inefficiency inherent in the current training paradigm makes it particularly challenging to train LLMs for ranking-based recommendation tasks on large datasets. To train LLMs for CTR prediction, most existing studies adopt the prevalent ''sliding-window'' paradigm. Given a sequence of $m$ user interactions, a unique training prompt is constructed for each interaction by designating it as the prediction target along with its preceding $n$ interactions serving as context. In turn, the sliding-window paradigm results in an overall complexity of $O(mn^2)$ that scales linearly with the length of user interactions. Consequently, a direct adoption to train LLMs with such strategy can result in prohibitively high training costs as the length of interactions grows. To alleviate the computational inefficiency, we propose a novel training paradigm, namely Dynamic Target Isolation (DTI), that structurally parallelizes the training of $k$ (where $k >> 1$) target interactions. Furthermore, we identify two major bottlenecks - hidden-state leakage and positional bias overfitting - that limit DTI to only scale up to a small value of $k$ (e.g., 5) then propose a computationally light solution to effectively tackle each. Through extensive experiments on three widely adopted public CTR datasets, we empirically show that DTI reduces training time by an average of $\\textbf{92%}$ (e.g., from $70.5$ hrs to $5.31$ hrs), without compromising CTR prediction performance.         ",
    "url": "https://arxiv.org/abs/2503.01001",
    "authors": [
      "Allen Lin",
      "Renqin Cai",
      "Yun He",
      "Hanchao Yu",
      "Jing Qian",
      "Rui Li",
      "Qifan Wang",
      "James Caverlee"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.02041",
    "title": "Interpolation Neural Network-Tensor Decomposition (INN-TD): a scalable and interpretable approach for large-scale physics-based problems",
    "abstract": "           Deep learning has been extensively employed as a powerful function approximator for modeling physics-based problems described by partial differential equations (PDEs). Despite their popularity, standard deep learning models often demand prohibitively large computational resources and yield limited accuracy when scaling to large-scale, high-dimensional physical problems. Their black-box nature further hinders the application in industrial problems where interpretability and high precision are critical. To overcome these challenges, this paper introduces Interpolation Neural Network-Tensor Decomposition (INN-TD), a scalable and interpretable framework that has the merits of both machine learning and finite element methods for modeling large-scale physical systems. By integrating locally supported interpolation functions from finite element into the network architecture, INN-TD achieves a sparse learning structure with enhanced accuracy, faster training/solving speed, and reduced memory footprint. This makes it particularly effective for tackling large-scale high-dimensional parametric PDEs in training, solving, and inverse optimization tasks in physical problems where high precision is required.         ",
    "url": "https://arxiv.org/abs/2503.02041",
    "authors": [
      "Jiachen Guo",
      "Xiaoyu Xie",
      "Chanwook Park",
      "Hantao Zhang",
      "Matthew Politis",
      "Gino Domel",
      "Thomas J.R. Hughes",
      "Wing Kam Liu"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2503.02986",
    "title": "Mind the Gap: Detecting Black-box Adversarial Attacks in the Making through Query Update Analysis",
    "abstract": "           Adversarial attacks remain a significant threat that can jeopardize the integrity of Machine Learning (ML) models. In particular, query-based black-box attacks can generate malicious noise without having access to the victim model's architecture, making them practical in real-world contexts. The community has proposed several defenses against adversarial attacks, only to be broken by more advanced and adaptive attack strategies. In this paper, we propose a framework that detects if an adversarial noise instance is being generated. Unlike existing stateful defenses that detect adversarial noise generation by monitoring the input space, our approach learns adversarial patterns in the input update similarity space. In fact, we propose to observe a new metric called Delta Similarity (DS), which we show it captures more efficiently the adversarial behavior. We evaluate our approach against 8 state-of-the-art attacks, including adaptive attacks, where the adversary is aware of the defense and tries to evade detection. We find that our approach is significantly more robust than existing defenses both in terms of specificity and sensitivity.         ",
    "url": "https://arxiv.org/abs/2503.02986",
    "authors": [
      "Jeonghwan Park",
      "Niall McLaughlin",
      "Ihsen Alouani"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.03799",
    "title": "DeepGrav: Anomalous Gravitational-Wave Detection Through Deep Latent Features",
    "abstract": "           This work introduces a novel deep learning-based approach for gravitational wave anomaly detection, aiming to overcome the limitations of traditional matched filtering techniques in identifying unknown waveform gravitational wave signals. We introduce a modified convolutional neural network architecture inspired by ResNet that leverages residual blocks to extract high-dimensional features, effectively capturing subtle differences between background noise and gravitational wave signals. This network architecture learns a high-dimensional projection while preserving discrepancies with the original input, facilitating precise identification of gravitational wave signals. In our experiments, we implement an innovative data augmentation strategy that generates new data by computing the arithmetic mean of multiple signal samples while retaining the key features of the original signals. In the NSF HDR A3D3: Detecting Anomalous Gravitational Wave Signals competition, it is honorable for us (group name: easonyan123) to get to the first place at the end with our model achieving a true negative rate (TNR) of 0.9708 during development/validation phase and 0.9832 on an unseen challenge dataset during final/testing phase, the highest among all competitors. These results demonstrate that our method not only achieves excellent generalization performance but also maintains robust adaptability in addressing the complex uncertainties inherent in gravitational wave anomaly detection.         ",
    "url": "https://arxiv.org/abs/2503.03799",
    "authors": [
      "Jianqi Yan",
      "Alex P. Leung",
      "Zhiyuan Pei",
      "David C. Y. Hui",
      "Sangin Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "High Energy Astrophysical Phenomena (astro-ph.HE)",
      "General Relativity and Quantum Cosmology (gr-qc)"
    ]
  },
  {
    "id": "arXiv:2503.04556",
    "title": "Compositional Causal Reasoning Evaluation in Language Models",
    "abstract": "           Causal reasoning and compositional reasoning are two core aspirations in generative AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors simultaneously, termed compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate through graphs. We instantiate a framework for the systematic evaluation of CCR for the average treatment effect and the probability of necessity and sufficiency. As proof of concept, we demonstrate the design of CCR tasks for language models in the LLama, Phi, and GPT families. On a math word problem, our framework revealed a range of taxonomically distinct error patterns. Additionally, CCR errors increased with the complexity of causal paths for all models except o1.         ",
    "url": "https://arxiv.org/abs/2503.04556",
    "authors": [
      "Jacqueline R. M. A. Maasch",
      "Alihan H\u00fcy\u00fck",
      "Xinnuo Xu",
      "Aditya V. Nori",
      "Javier Gonzalez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.05515",
    "title": "Fluid Antenna-Aided Robust Secure Transmission for RSMA-ISAC Systems",
    "abstract": "           This paper leverages fluid antenna (FA) and rate-splitting multiple access (RSMA) to enhance the physical layer security (PLS) of an integrated sensing and communication (ISAC) system. We consider a practical multi-user multi-input single-output (MU-MISO) system, where a base station (BS) equipped with fixed position antennas (FPAs) employs RSMA to communicate with multiple single-FA users, while an eavesdropping target may potentially wiretap the signals. The system adopts a novel rate splitting (RS) scheme, where the common layer stream serves a dual purpose: it conveys valid data to legitimate users (LUs) while simultaneously generating jamming signals to confuse potential eavesdroppers. We establish the problem and propose the optimization algorithm under two conditions: perfect and imperfect channel state information (CSI) conditions. Specifically, under perfect the CSI condition, we address the non-convex optimization problem by proposing an alternating optimization (AO) algorithm, which decomposes the problem into two subproblems: beamforming matrix optimization and the adjustment of FA positions. For beamforming optimization, we utilize semidefinite programming (SDP) and successive convex approximation (SCA) to convert the problem into a more tractable convex form. Given a fixed beamforming matrix, SCA is applied to handle the surrogate upper bound of the constraints. In the case of imperfect CSI, the continuous nature of CSI errors leads to an infinite number of constraints. To overcome this challenge, we propose an AO-based algorithm that incorporates the S-Procedure and SCA to obtain a high-quality beamforming matrix and effective FA positions. Extensive simulation results demonstrate that the proposed FA-aided RSMA-ISAC system significantly enhances security compared to traditional FPA-based and SDMA-based systems.         ",
    "url": "https://arxiv.org/abs/2503.05515",
    "authors": [
      "Cixiao Zhang",
      "Yin Xu",
      "Size Peng",
      "Xinghao Guo",
      "Xiaowu Ou",
      "Hanjiang Hong",
      "Dazhi He",
      "Wenjun Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2503.05792",
    "title": "Ro-To-Go! Robust Reactive Control with Signal Temporal Logic",
    "abstract": "           Signal Temporal Logic (STL) robustness is a common objective for optimal robot control, but its dependence on history limits the robot's decision-making capabilities when used in Model Predictive Control (MPC) approaches. In this work, we introduce Signal Temporal Logic robustness-to-go (Ro-To-Go), a new quantitative semantics for the logic that isolates the contributions of suffix trajectories. We prove its relationship to formula progression for Metric Temporal Logic, and show that the robustness-to-go depends only on the suffix trajectory and progressed formula. We implement robustness-to-go as the objective in an MPC algorithm and use formula progression to efficiently evaluate it online. We test the algorithm in simulation and compare it to MPC using traditional STL robustness. Our experiments show that using robustness-to-go results in a higher success rate.         ",
    "url": "https://arxiv.org/abs/2503.05792",
    "authors": [
      "Roland Ilyes",
      "Lara Bruderm\u00fcller",
      "Nick Hawes",
      "Bruno Lacerda"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.05934",
    "title": "Improving Merge Sort and Quick Sort Performance by Utilizing Alphadev's Sorting Networks as Base Cases",
    "abstract": "           Recent work by Google DeepMind introduced assembly-optimized sorting networks that achieve faster performance for small fixed-size arrays (3-8). In this research, we investigate the integration of these networks as base cases in classical divide-and-conquer sorting algorithms, specifically Merge Sort and Quick Sort, to leverage these efficient sorting networks for small subarrays generated during the recursive process. We conducted benchmarks with 11 different optimization configurations and compared them to classical Merge Sort and Quick Sort. We tested the configurations with random, sorted and nearly sorted arrays. Our optimized Merge Sort, using a configuration of three sorting networks (sizes 6, 7, and 8), achieves at least 1.5x speedup for random and nearly sorted arrays, and at least 2x speedup for sorted arrays, in comparison to classical Merge Sort. This optimized Merge Sort surpasses both classical Quick Sort and similarly optimized Quick Sort variants when sorting random arrays of size 10,000 and larger. When comparing our optimized Quick Sort to classical Quick Sort, we observe a 1.5x speedup using the 3-to-5 configuration on sorted arrays of size 10,000. The 6-to-8 configuration maintains a consistent 1.5x improvement across sorted arrays from 25,000 to 1 million elements. Our findings demonstrate the potential of integrating AI-optimized sorting networks to enhance the performance of classical sorting algorithms.         ",
    "url": "https://arxiv.org/abs/2503.05934",
    "authors": [
      "Anas Gamal Aly",
      "Anders E. Jensen",
      "Hala ElAarag"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2503.06166",
    "title": "Secure On-Device Video OOD Detection Without Backpropagation",
    "abstract": "           Out-of-Distribution (OOD) detection is critical for ensuring the reliability of machine learning models in safety-critical applications such as autonomous driving and medical diagnosis. While deploying personalized OOD detection directly on edge devices is desirable, it remains challenging due to large model sizes and the computational infeasibility of on-device training. Federated learning partially addresses this but still requires gradient computation and backpropagation, exceeding the capabilities of many edge devices. To overcome these challenges, we propose SecDOOD, a secure cloud-device collaboration framework for efficient on-device OOD detection without requiring device-side backpropagation. SecDOOD utilizes cloud resources for model training while ensuring user data privacy by retaining sensitive information on-device. Central to SecDOOD is a HyperNetwork-based personalized parameter generation module, which adapts cloud-trained models to device-specific distributions by dynamically generating local weight adjustments, effectively combining central and local information without local fine-tuning. Additionally, our dynamic feature sampling and encryption strategy selectively encrypts only the most informative feature channels, largely reducing encryption overhead without compromising detection performance. Extensive experiments across multiple datasets and OOD scenarios demonstrate that SecDOOD achieves performance comparable to fully fine-tuned models, enabling secure, efficient, and personalized OOD detection on resource-limited edge devices. To enhance accessibility and reproducibility, our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.06166",
    "authors": [
      "Shawn Li",
      "Peilin Cai",
      "Yuxiao Zhou",
      "Zhiyu Ni",
      "Renjie Liang",
      "You Qin",
      "Yi Nian",
      "Zhengzhong Tu",
      "Xiyang Hu",
      "Yue Zhao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.06169",
    "title": "Treble Counterfactual VLMs: A Causal Approach to Hallucination",
    "abstract": "           Vision-Language Models (VLMs) have advanced multi-modal tasks like image captioning, visual question answering, and reasoning. However, they often generate hallucinated outputs inconsistent with the visual context or prompt, limiting reliability in critical applications like autonomous driving and medical imaging. Existing studies link hallucination to statistical biases, language priors, and biased feature learning but lack a structured causal understanding. In this work, we introduce a causal perspective to analyze and mitigate hallucination in VLMs. We hypothesize that hallucination arises from unintended direct influences of either the vision or text modality, bypassing proper multi-modal fusion. To address this, we construct a causal graph for VLMs and employ counterfactual analysis to estimate the Natural Direct Effect (NDE) of vision, text, and their cross-modal interaction on the output. We systematically identify and mitigate these unintended direct effects to ensure that responses are primarily driven by genuine multi-modal fusion. Our approach consists of three steps: (1) designing structural causal graphs to distinguish correct fusion pathways from spurious modality shortcuts, (2) estimating modality-specific and cross-modal NDE using perturbed image representations, hallucinated text embeddings, and degraded visual inputs, and (3) implementing a test-time intervention module to dynamically adjust the model's dependence on each modality. Experimental results demonstrate that our method significantly reduces hallucination while preserving task performance, providing a robust and interpretable framework for improving VLM reliability. To enhance accessibility and reproducibility, our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.06169",
    "authors": [
      "Shawn Li",
      "Jiashu Qu",
      "Yuxiao Zhou",
      "Yuehan Qin",
      "Tiankai Yang",
      "Yue Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.06246",
    "title": "AmazonNetLink: Enabling Education Access in Remote Amazonian Regions through Delay-Tolerant Networks",
    "abstract": "           Access to educational materials in remote Amazonian communities is challenged by limited communication infrastructure. This paper proposes a novel delay-tolerant network (DTN) approach for content distribution and compares the Epidemic, MaxProp, and PRoPHETv2 routing protocols using the ONE simulator under dynamically changing educational file sizes. Results show that while Epidemic routing achieves higher delivery rates due to extensive message replication, it also leads to increased resource usage. MaxProp offers a balance between delivery efficiency and resource utilization by prioritizing message delivery based on predefined heuristics but struggles under high congestion and resource constraints. PRoPHETv2, with its probability-based forwarding, uses resources more efficiently but is less effective in dynamic, dense networks. This analysis highlights trade-offs between delivery performance and resource efficiency, guiding protocol selection for specific community needs. In our future work, we aim to explore adaptive buffer management and congestion-aware DTN protocols.         ",
    "url": "https://arxiv.org/abs/2503.06246",
    "authors": [
      "Andr\u00e9s Fernando Bar\u00f3n Sandoval",
      "Milena Radenkovic"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.07638",
    "title": "Leveraging Taxonomy Similarity for Next Activity Prediction in Patient Treatment",
    "abstract": "           The rapid progress in modern medicine presents physicians with complex challenges when planning patient treatment. Techniques from the field of Predictive Business Process Monitoring, like Next-activity-prediction (NAP) can be used as a promising technique to support physicians in treatment planning, by proposing a possible next treatment step. Existing patient data, often in the form of electronic health records, can be analyzed to recommend the next suitable step in the treatment process. However, the use of patient data poses many challenges due to its knowledge-intensive character, high variability and scarcity of medical data. To overcome these challenges, this article examines the use of the knowledge encoded in taxonomies to improve and explain the prediction of the next activity in the treatment process. This study proposes the TS4NAP approach, which uses medical taxonomies (ICD-10-CM and ICD-10-PCS) in combination with graph matching to assess the similarities of medical codes to predict the next treatment step. The effectiveness of the proposed approach will be evaluated using event logs that are derived from the MIMIC-IV dataset. The results highlight the potential of using domain-specific knowledge held in taxonomies to improve the prediction of the next activity, and thus can improve treatment planning and decision-making by making the predictions more explainable.         ",
    "url": "https://arxiv.org/abs/2503.07638",
    "authors": [
      "Martin Kuhn",
      "Joscha Gr\u00fcger",
      "Tobias Geyer",
      "Ralph Bergmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.07862",
    "title": "cantnlp@DravidianLangTech2025: A Bag-of-Sounds Approach to Multimodal Hate Speech Detection",
    "abstract": "           This paper presents the systems and results for the Multimodal Social Media Data Analysis in Dravidian Languages (MSMDA-DL) shared task at the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages (DravidianLangTech-2025). We took a `bag-of-sounds' approach by training our hate speech detection system on the speech (audio) data using transformed Mel spectrogram measures. While our candidate model performed poorly on the test set, our approach offered promising results during training and development for Malayalam and Tamil. With sufficient and well-balanced training data, our results show that it is feasible to use both text and speech (audio) data in the development of multimodal hate speech detection systems.         ",
    "url": "https://arxiv.org/abs/2503.07862",
    "authors": [
      "Sidney Wong",
      "Andrew Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.08354",
    "title": "Robust Latent Matters: Boosting Image Generation with Sampling Error Synthesis",
    "abstract": "           Recent image generation schemes typically capture image distribution in a pre-constructed latent space relying on a frozen image tokenizer. Though the performance of tokenizer plays an essential role to the successful generation, its current evaluation metrics (e.g. rFID) fail to precisely assess the tokenizer and correlate its performance to the generation quality (e.g. gFID). In this paper, we comprehensively analyze the reason for the discrepancy of reconstruction and generation qualities in a discrete latent space, and, from which, we propose a novel plug-and-play tokenizer training scheme to facilitate latent space construction. Specifically, a latent perturbation approach is proposed to simulate sampling noises, i.e., the unexpected tokens sampled, from the generative process. With the latent perturbation, we further propose (1) a novel tokenizer evaluation metric, i.e., pFID, which successfully correlates the tokenizer performance to generation quality and (2) a plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer thus boosting the generation quality and convergence speed. Extensive benchmarking are conducted with 11 advanced discrete image tokenizers with 2 autoregressive generation models to validate our approach. The tokenizer trained with our proposed latent perturbation achieve a notable 1.60 gFID with classifier-free guidance (CFG) and 3.45 gFID without CFG with a $\\sim$400M generator. Code: this https URL.         ",
    "url": "https://arxiv.org/abs/2503.08354",
    "authors": [
      "Kai Qiu",
      "Xiang Li",
      "Jason Kuen",
      "Hao Chen",
      "Xiaohao Xu",
      "Jiuxiang Gu",
      "Yinyi Luo",
      "Bhiksha Raj",
      "Zhe Lin",
      "Marios Savvides"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08600",
    "title": "NSF-SciFy: Mining the NSF Awards Database for Scientific Claims",
    "abstract": "           We present NSF-SciFy, a large-scale dataset for scientific claim extraction derived from the National Science Foundation (NSF) awards database, comprising over 400K grant abstracts spanning five decades. While previous datasets relied on published literature, we leverage grant abstracts which offer a unique advantage: they capture claims at an earlier stage in the research lifecycle before publication takes effect. We also introduce a new task to distinguish between existing scientific claims and aspirational research intentions in proposals. Using zero-shot prompting with frontier large language models, we jointly extract 114K scientific claims and 145K investigation proposals from 16K grant abstracts in the materials science domain to create a focused subset called NSF-SciFy-MatSci. We use this dataset to evaluate 3 three key tasks: (1) technical to non-technical abstract generation, where models achieve high BERTScore (0.85+ F1); (2) scientific claim extraction, where fine-tuned models outperform base models by 100% relative improvement; and (3) investigation proposal extraction, showing 90%+ improvement with fine-tuning. We introduce novel LLM-based evaluation metrics for robust assessment of claim/proposal extraction quality. As the largest scientific claim dataset to date -- with an estimated 2.8 million claims across all STEM disciplines funded by the NSF -- NSF-SciFy enables new opportunities for claim verification and meta-scientific research. We publicly release all datasets, trained models, and evaluation code to facilitate further research.         ",
    "url": "https://arxiv.org/abs/2503.08600",
    "authors": [
      "Delip Rao",
      "Weiqiu You",
      "Eric Wong",
      "Chris Callison-Burch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.08992",
    "title": "Dual-Domain Homogeneous Fusion with Cross-Modal Mamba and Progressive Decoder for 3D Object Detection",
    "abstract": "           Fusing LiDAR and image features in a homogeneous BEV domain has become popular for 3D object detection in autonomous driving. However, this paradigm is constrained by the excessive feature compression. While some works explore dense voxel fusion to enable better feature interaction, they face high computational costs and challenges in query generation. Additionally, feature misalignment in both domains results in suboptimal detection accuracy. To address these limitations, we propose a Dual-Domain Homogeneous Fusion network (DDHFusion), which leverages the complementarily of both BEV and voxel domains while mitigating their drawbacks. Specifically, we first transform image features into BEV and sparse voxel representations using lift-splat-shot and our proposed Semantic-Aware Feature Sampling (SAFS) module. The latter significantly reduces computational overhead by discarding unimportant voxels. Next, we introduce Homogeneous Voxel and BEV Fusion (HVF and HBF) networks for multi-modal fusion within respective domains. They are equipped with novel cross-modal Mamba blocks to resolve feature misalignment and enable comprehensive scene perception. The output voxel features are injected into the BEV space to compensate for the information loss brought by direct height compression. During query selection, the Progressive Query Generation (PQG) mechanism is implemented in the BEV domain to reduce false negatives caused by feature compression. Furthermore, we propose a Progressive Decoder (QD) that sequentially aggregates not only context-rich BEV features but also geometry-aware voxel features with deformable attention and the Multi-Modal Voxel Feature Mixing (MMVFM) block for precise classification and box regression.         ",
    "url": "https://arxiv.org/abs/2503.08992",
    "authors": [
      "Xuzhong Hu",
      "Zaipeng Duan",
      "Pei An",
      "Jun zhang",
      "Jie Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.09313",
    "title": "xVLM2Vec: Adapting LVLM-based embedding models to multilinguality using Self-Knowledge Distillation",
    "abstract": "           In the current literature, most embedding models are based on the encoder-only transformer architecture to extract a dense and meaningful representation of the given input, which can be a text, an image, and more. With the recent advances in language modeling thanks to the introduction of Large Language Models, the possibility of extracting embeddings from these large and extensively trained models has been explored. However, current studies focus on textual embeddings in English, which is also the main language on which these models have been trained. Furthermore, there are very few models that consider multimodal and multilingual input. In light of this, we propose an adaptation methodology for Large Vision-Language Models trained on English language data to improve their performance in extracting multilingual and multimodal embeddings. Finally, we design and introduce a benchmark to evaluate the effectiveness of multilingual and multimodal embedding models.         ",
    "url": "https://arxiv.org/abs/2503.09313",
    "authors": [
      "Elio Musacchio",
      "Lucia Siciliani",
      "Pierpaolo Basile",
      "Giovanni Semeraro"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.09483",
    "title": "Learning Spatially Adaptive $\\ell_1$-Norms Weights for Convolutional Synthesis Regularization",
    "abstract": "           We propose an unrolled algorithm approach for learning spatially adaptive parameter maps in the framework of convolutional synthesis-based $\\ell_1$ regularization. More precisely, we consider a family of pre-trained convolutional filters and estimate deeply parametrized spatially varying parameters applied to the sparse feature maps by means of unrolling a FISTA algorithm to solve the underlying sparse estimation problem. The proposed approach is evaluated for image reconstruction of low-field MRI and compared to spatially adaptive and non-adaptive analysis-type procedures relying on Total Variation regularization and to a well-established model-based deep learning approach. We show that the proposed approach produces visually and quantitatively comparable results with the latter approaches and at the same time remains highly interpretable. In particular, the inferred parameter maps quantify the local contribution of each filter in the reconstruction, which provides valuable insight into the algorithm mechanism and could potentially be used to discard unsuited filters.         ",
    "url": "https://arxiv.org/abs/2503.09483",
    "authors": [
      "Andreas Kofler",
      "Luca Calatroni",
      "Christoph Kolbitsch",
      "Kostas Papafitsoros"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2503.09712",
    "title": "Revisiting Backdoor Attacks on Time Series Classification in the Frequency Domain",
    "abstract": "           Time series classification (TSC) is a cornerstone of modern web applications, powering tasks such as financial data analysis, network traffic monitoring, and user behavior analysis. In recent years, deep neural networks (DNNs) have greatly enhanced the performance of TSC models in these critical domains. However, DNNs are vulnerable to backdoor attacks, where attackers can covertly implant triggers into models to induce malicious outcomes. Existing backdoor attacks targeting DNN-based TSC models remain elementary. In particular, early methods borrow trigger designs from computer vision, which are ineffective for time series data. More recent approaches utilize generative models for trigger generation, but at the cost of significant computational complexity. In this work, we analyze the limitations of existing attacks and introduce an enhanced method, FreqBack. Drawing inspiration from the fact that DNN models inherently capture frequency domain features in time series data, we identify that improper perturbations in the frequency domain are the root cause of ineffective attacks. To address this, we propose to generate triggers both effectively and efficiently, guided by frequency analysis. FreqBack exhibits substantial performance across five models and eight datasets, achieving an impressive attack success rate of over 90%, while maintaining less than a 3% drop in model accuracy on clean data.         ",
    "url": "https://arxiv.org/abs/2503.09712",
    "authors": [
      "Yuanmin Huang",
      "Mi Zhang",
      "Zhaoxiang Wang",
      "Wenxuan Li",
      "Min Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.09853",
    "title": "Who Are You Behind the Screen? Implicit MBTI and Gender Detection Using Artificial Intelligence",
    "abstract": "           In personalized technology and psychological research, precisely detecting demographic features and personality traits from digital interactions becomes ever more important. This work investigates implicit categorization, inferring personality and gender variables directly from linguistic patterns in Telegram conversation data, while conventional personality prediction techniques mostly depend on explicitly self-reported labels. We refine a Transformer-based language model (RoBERTa) to capture complex linguistic cues indicative of personality traits and gender differences using a dataset comprising 138,866 messages from 1,602 users annotated with MBTI types and 195,016 messages from 2,598 users annotated with gender. Confidence levels help to greatly raise model accuracy to 86.16\\%, hence proving RoBERTa's capacity to consistently identify implicit personality types from conversational text data. Our results highlight the usefulness of Transformer topologies for implicit personality and gender classification, hence stressing their efficiency and stressing important trade-offs between accuracy and coverage in realistic conversational environments. With regard to gender classification, the model obtained an accuracy of 74.4\\%, therefore capturing gender-specific language patterns. Personality dimension analysis showed that people with introverted and intuitive preferences are especially more active in text-based interactions. This study emphasizes practical issues in balancing accuracy and data coverage as Transformer-based models show their efficiency in implicit personality and gender prediction tasks from conversational texts.         ",
    "url": "https://arxiv.org/abs/2503.09853",
    "authors": [
      "Kourosh Shahnazari",
      "Seyed Moein Ayyoubzadeh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.09867",
    "title": "Object-Aware DINO (Oh-A-Dino): Enhancing Self-Supervised Representations for Multi-Object Instance Retrieval",
    "abstract": "           Object-centric learning is fundamental to human vision and crucial for models requiring complex reasoning. Traditional approaches rely on slot-based bottlenecks to learn object properties explicitly, while recent self-supervised vision models like DINO have shown emergent object understanding. However, DINO representations primarily capture global scene features, often confounding individual object attributes. We investigate the effectiveness of DINO representations and slot-based methods for multi-object instance retrieval. Our findings reveal that DINO representations excel at capturing global object attributes such as object shape and size, but struggle with object-level details like colour, whereas slot-based representations struggle at both global and object-level understanding. To address this, we propose a method that combines global and local features by augmenting DINO representations with object-centric latent vectors from a Variational Autoencoder trained on segmented image patches that are extracted from the DINO features. This approach improves multi-object instance retrieval performance, bridging the gap between global scene understanding and fine-grained object representation without requiring full model retraining.         ",
    "url": "https://arxiv.org/abs/2503.09867",
    "authors": [
      "Stefan Sylvius Wagner",
      "Stefan Harmeling"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.10068",
    "title": "AI-assisted Early Detection of Pancreatic Ductal Adenocarcinoma on Contrast-enhanced CT",
    "abstract": "           Pancreatic ductal adenocarcinoma (PDAC) is one of the most common and aggressive types of pancreatic cancer. However, due to the lack of early and disease-specific symptoms, most patients with PDAC are diagnosed at an advanced disease stage. Consequently, early PDAC detection is crucial for improving patients' quality of life and expanding treatment options. In this work, we develop a coarse-to-fine approach to detect PDAC on contrast-enhanced CT scans. First, we localize and crop the region of interest from the low-resolution images, and then segment the PDAC-related structures at a finer scale. Additionally, we introduce two strategies to further boost detection performance: (1) a data-splitting strategy for model ensembling, and (2) a customized post-processing function. We participated in the PANORAMA challenge and ranked 1st place for PDAC detection with an AUROC of 0.9263 and an AP of 0.7243. Our code and models are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.10068",
    "authors": [
      "Han Liu",
      "Riqiang Gao",
      "Sasa Grbic"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.10345",
    "title": "Mirror Online Conformal Prediction with Intermittent Feedback",
    "abstract": "           Online conformal prediction enables the runtime calibration of a pre-trained artificial intelligence model using feedback on its performance. Calibration is achieved through set predictions that are updated via online rules so as to ensure long-term coverage guarantees. While recent research has demonstrated the benefits of incorporating prior knowledge into the calibration process, this has come at the cost of replacing coverage guarantees with less tangible regret guarantees based on the quantile loss. This work introduces intermittent mirror online conformal prediction (IM-OCP), a novel runtime calibration framework that integrates prior knowledge, while maintaining long-term coverage and achieving sub-linear regret. IM-OCP features closed-form updates with minimal memory complexity, and is designed to operate under potentially intermittent feedback.         ",
    "url": "https://arxiv.org/abs/2503.10345",
    "authors": [
      "Bowen Wang",
      "Matteo Zecchin",
      "Osvaldo Simeone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.10508",
    "title": "Hoi2Anomaly: An Explainable Anomaly Detection Approach Guided by Human-Object Interaction",
    "abstract": "           In the domain of Image Anomaly Detection (IAD), Existing methods frequently exhibit a paucity of fine-grained, interpretable semantic information, resulting in the detection of anomalous entities or activities that are susceptible to machine illusions. This deficiency often leads to the detection of anomalous entities or actions that are susceptible to machine illusions and lack sufficient explanation. In this thesis, we propose a novel approach to anomaly detection, termed Hoi2Anomaly, which aims to achieve precise discrimination and localization of anomalies. The proposed methodology involves the construction of a multi-modal instruction tuning dataset comprising human-object interaction (HOI) pairs in anomalous scenarios. Second, we have trained an HOI extractor in threat scenarios to localize and match anomalous actions and entities. Finally, explanatory content is generated for the detected anomalous HOI by fine-tuning the visual language pretraining (VLP) framework. The experimental results demonstrate that Hoi2Anomaly surpasses existing generative approaches in terms of precision and explainability. We will release Hoi2Anomaly for the advancement of the field of anomaly detection.         ",
    "url": "https://arxiv.org/abs/2503.10508",
    "authors": [
      "Yuhan Wang",
      "Cheng Liu",
      "Daou Zhang",
      "Weichao Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.10579",
    "title": "Semantic-Supervised Spatial-Temporal Fusion for LiDAR-based 3D Object Detection",
    "abstract": "           LiDAR-based 3D object detection presents significant challenges due to the inherent sparsity of LiDAR points. A common solution involves long-term temporal LiDAR data to densify the inputs. However, efficiently leveraging spatial-temporal information remains an open problem. In this paper, we propose a novel Semantic-Supervised Spatial-Temporal Fusion (ST-Fusion) method, which introduces a novel fusion module to relieve the spatial misalignment caused by the object motion over time and a feature-level semantic supervision to sufficiently unlock the capacity of the proposed fusion module. Specifically, the ST-Fusion consists of a Spatial Aggregation (SA) module and a Temporal Merging (TM) module. The SA module employs a convolutional layer with progressively expanding receptive fields to aggregate the object features from the local regions to alleviate the spatial misalignment, the TM module dynamically extracts object features from the preceding frames based on the attention mechanism for a comprehensive sequential presentation. Besides, in the semantic supervision, we propose a Semantic Injection method to enrich the sparse LiDAR data via injecting the point-wise semantic labels, using it for training a teacher model and providing a reconstruction target at the feature level supervised by the proposed object-aware loss. Extensive experiments on various LiDAR-based detectors demonstrate the effectiveness and universality of our proposal, yielding an improvement of approximately +2.8% in NDS based on the nuScenes benchmark.         ",
    "url": "https://arxiv.org/abs/2503.10579",
    "authors": [
      "Chaoqun Wang",
      "Xiaobin Hong",
      "Wenzhong Li",
      "Ruimao Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.10617",
    "title": "Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models",
    "abstract": "           Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another. While methods such as LoRA impose orthogonality constraints at the weight level, they do not fully address interference in hidden-state representations. We propose Compositional Subspace Representation Fine-tuning (CS-ReFT), a novel representation-based approach that learns multiple orthonormal subspace transformations, each specializing in a distinct skill, and composes them via a lightweight router. By isolating these subspace edits in the hidden state, rather than weight matrices, CS-ReFT prevents cross-task conflicts more effectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B achieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring only 0.0098% of model parameters. These findings show that specialized representation edits, composed via a simple router, significantly enhance multi-task instruction following with minimal overhead.         ",
    "url": "https://arxiv.org/abs/2503.10617",
    "authors": [
      "Andy Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.11008",
    "title": "Comparative Analysis of Advanced AI-based Object Detection Models for Pavement Marking Quality Assessment during Daytime",
    "abstract": "           Visual object detection utilizing deep learning plays a vital role in computer vision and has extensive applications in transportation engineering. This paper focuses on detecting pavement marking quality during daytime using the You Only Look Once (YOLO) model, leveraging its advanced architectural features to enhance road safety through precise and real-time assessments. Utilizing image data from New Jersey, this study employed three YOLOv8 variants: YOLOv8m, YOLOv8n, and YOLOv8x. The models were evaluated based on their prediction accuracy for classifying pavement markings into good, moderate, and poor visibility categories. The results demonstrated that YOLOv8n provides the best balance between accuracy and computational efficiency, achieving the highest mean Average Precision (mAP) for objects with good visibility and demonstrating robust performance across various Intersections over Union (IoU) thresholds. This research enhances transportation safety by offering an automated and accurate method for evaluating the quality of pavement markings.         ",
    "url": "https://arxiv.org/abs/2503.11008",
    "authors": [
      "Gian Antariksa",
      "Rohit Chakraborty",
      "Shriyank Somvanshi",
      "Subasish Das",
      "Mohammad Jalayer",
      "Deep Rameshkumar Patel",
      "David Mills"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.11227",
    "title": "GKG-LLM: A Unified Framework for Generalized Knowledge Graph Construction",
    "abstract": "           The construction of Generalized Knowledge Graph (GKG), including knowledge graph, event knowledge graph and commonsense knowledge graph, is fundamental for various natural language processing tasks. Current studies typically construct these types of graph separately, overlooking holistic insights and potential unification that could be beneficial in computing resources and usage perspectives. However, a key challenge in developing a unified framework for GKG is obstacles arising from task-specific differences. In this study, we propose a unified framework for constructing generalized knowledge graphs to address this challenge. First, we collect data from 15 sub-tasks in 29 datasets across the three types of graphs, categorizing them into in-sample, counter-task, and out-of-distribution (OOD) data. Then, we propose a three-stage curriculum learning fine-tuning framework, by iteratively injecting knowledge from the three types of graphs into the Large Language Models. Extensive experiments show that our proposed model improves the construction of all three graph types across in-domain, OOD and counter-task data.         ",
    "url": "https://arxiv.org/abs/2503.11227",
    "authors": [
      "Jian Zhang",
      "Bifan Wei",
      "Shihao Qi",
      "haiping Zhu",
      "Jun Liu",
      "Qika Lin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2111.03950",
    "title": "Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves",
    "abstract": "           We propose simple nonparametric estimators for mediated and time-varying dose response curves based on kernel ridge regression. By embedding Pearl's mediation formula and Robins' g-formula with kernels, we allow treatments, mediators, and covariates to be continuous in general spaces, and also allow for nonlinear treatment-confounder feedback. Our key innovation is a reproducing kernel Hilbert space technique called sequential kernel embedding, which we use to construct simple estimators that account for complex feedback. Our estimators preserve the generality of classic identification while also achieving nonasymptotic uniform rates. In nonlinear simulations with many covariates, we demonstrate strong performance. We estimate mediated and time-varying dose response curves of the US Job Corps, and clean data that may serve as a benchmark in future work. We extend our results to mediated and time-varying treatment effects and counterfactual distributions, verifying semiparametric efficiency and weak convergence.         ",
    "url": "https://arxiv.org/abs/2111.03950",
    "authors": [
      "Rahul Singh",
      "Liyuan Xu",
      "Arthur Gretton"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2312.10295",
    "title": "On a Generalization of Wasserstein Distance and the Beckmann Problem to Connection Graphs",
    "abstract": "           We propose a model of optimal parallel transport between vector fields on a connection graph, which consists of a weighted graph along with a map from its edges to an orthogonal group. Inspired by the well-known equivalence of 1-Wasserstein distance and minimum cost flows on standard graphs, we consider two versions of this problem: a minimum norm vector-valued flow problem with divergence constraints reflective of the connection structure of the graph; and a modified version which incorporates both quadratic regularization and a relaxation of the divergence constraint. Our theoretical contributions include: conditions for feasibility and computation of the Lagrangian dual problem for both problems, and duality correspondence for the relaxed-regularized version. Example applications of the model including transport between color images, vector field interpolation, and unsupervised clustering of vector field-valued data (in this case hurricane trajectory data) are also considered.         ",
    "url": "https://arxiv.org/abs/2312.10295",
    "authors": [
      "Sawyer Robertson",
      "Dhruv Kohli",
      "Gal Mishne",
      "Alexander Cloninger"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2401.11679",
    "title": "Simulating Nighttime Visible Satellite Imagery of Tropical Cyclones Using Conditional Generative Adversarial Networks",
    "abstract": "           Visible (VIS) imagery is important for monitoring Tropical Cyclones (TCs) but is unavailable at night. This study presents a Conditional Generative Adversarial Networks (CGAN) model to generate nighttime VIS imagery with significantly enhanced accuracy and spatial resolution. Our method offers three key improvements compared to existing models. First, we replaced the L1 loss in the pix2pix framework with the Structural Similarity Index Measure (SSIM) loss, which significantly reduced image blurriness. Second, we selected multispectral infrared (IR) bands as input based on a thorough examination of their spectral properties, providing essential physical information for accurate simulation. Third, we incorporated the direction parameters of the sun and the satellite, which addressed the dependence of VIS images on sunlight directions and enabled a much larger training set from continuous daytime data. The model was trained and validated using data from the Advanced Himawari Imager (AHI) in the daytime, achieving statistical results of SSIM = 0.923 and Root Mean Square Error (RMSE) = 0.0299, which significantly surpasses existing models. We also performed a cross-satellite nighttime model validation using the Day/Night Band (DNB) of the Visible/Infrared Imager Radiometer Suite (VIIRS), which yields outstanding results compared to existing models. Our model is operationally applied to generate accurate VIS imagery with arbitrary virtual sunlight directions, significantly contributing to the nighttime monitoring of various meteorological phenomena.         ",
    "url": "https://arxiv.org/abs/2401.11679",
    "authors": [
      "Jinghuai Yao",
      "Puyuan Du",
      "Yucheng Zhao",
      "Yubo Wang"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.02041",
    "title": "Alpha-divergence loss function for neural density ratio estimation",
    "abstract": "           Density ratio estimation (DRE) is a fundamental machine learning technique for capturing relationships between two probability distributions. State-of-the-art DRE methods estimate the density ratio using neural networks trained with loss functions derived from variational representations of $f$-divergences. However, existing methods face optimization challenges, such as overfitting due to lower-unbounded loss functions, biased mini-batch gradients, vanishing training loss gradients, and high sample requirements for Kullback--Leibler (KL) divergence loss functions. To address these issues, we focus on $\\alpha$-divergence, which provides a suitable variational representation of $f$-divergence. Subsequently, a novel loss function for DRE, the $\\alpha$-divergence loss function ($\\alpha$-Div), is derived. $\\alpha$-Div is concise but offers stable and effective optimization for DRE. The boundedness of $\\alpha$-divergence provides the potential for successful DRE with data exhibiting high KL-divergence. Our numerical experiments demonstrate the effectiveness of $\\alpha$-Div in optimization. However, the experiments also show that the proposed loss function offers no significant advantage over the KL-divergence loss function in terms of RMSE for DRE. This indicates that the accuracy of DRE is primarily determined by the amount of KL-divergence in the data and is less dependent on $\\alpha$-divergence.         ",
    "url": "https://arxiv.org/abs/2402.02041",
    "authors": [
      "Yoshiaki Kitazawa"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.01947",
    "title": "Characterization of Chordal Circular-arc Graphs: I. Split Graphs",
    "abstract": "           The most elusive problem around the class of circular-arc graphs is identifying all minimal graphs that are not in this class. The main obstacle is the lack of a systematic way of enumerating these minimal graphs. McConnell [FOCS 2001] presented a transformation from circular-arc graphs to interval graphs with certain patterns of representations. We fully characterize these interval patterns for circular-arc graphs that are split graphs, thereby building a connection between minimal split graphs that are not circular-arc graphs and minimal non-interval graphs. This connection enables us to identify all minimal split graphs that are not circular-arc graphs. As a byproduct, we develop a linear-time certifying recognition algorithm for circular-arc graphs when the input is a split graph.         ",
    "url": "https://arxiv.org/abs/2403.01947",
    "authors": [
      "Yixin Cao",
      "Jan Derbisz",
      "Tomasz Krawczyk"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2406.06306",
    "title": "Unified Fourier bases for signals on random graphs with group symmetries",
    "abstract": "           We consider a recently proposed approach to graph signal processing (GSP) based on graphons. We show how the graphon-based approach to GSP applies to graphs sampled from a stochastic block model derived from a weighted Cayley graph. When SBM block sizes are equal, a nice Fourier basis can be derived from the representation theory of the underlying group. We explore how the SBM Fourier basis is affected when block sizes are not uniform. When block sizes are nearly uniform, we demonstrate that the group Fourier basis closely approximates the SBM Fourier basis. More specifically, we quantify the approximation error using matrix perturbation theory. When block sizes are highly non-uniform, the group-based Fourier basis can no longer be used. However, we show that partial information regarding the SBM Fourier basis can still be obtained from the underlying group.         ",
    "url": "https://arxiv.org/abs/2406.06306",
    "authors": [
      "Mahya Ghandehari",
      "Jeannette Janssen",
      "Silo Murphy"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2411.02198",
    "title": "Metric properties of partial and robust Gromov-Wasserstein distances",
    "abstract": "           The Gromov-Wasserstein (GW) distances define a family of metrics, based on ideas from optimal transport, which enable comparisons between probability measures defined on distinct metric spaces. They are particularly useful in areas such as network analysis and geometry processing, as computation of a GW distance involves solving for registration between the objects which minimizes geometric distortion. Although GW distances have proven useful for various applications in the recent machine learning literature, it has been observed that they are inherently sensitive to outlier noise and cannot accommodate partial matching. This has been addressed by various constructions building on the GW framework; in this article, we focus specifically on a natural relaxation of the GW optimization problem, introduced by Chapel et al., which is aimed at addressing exactly these shortcomings. Our goal is to understand the theoretical properties of this relaxed optimization problem, from the viewpoint of metric geometry. While the relaxed problem fails to induce a metric, we derive precise characterizations of how it fails the axioms of non-degeneracy and triangle inequality. These observations lead us to define a novel family of distances, whose construction is inspired by the Prokhorov and Ky Fan distances, as well as by the recent work of Raghvendra et al.\\ on robust versions of classical Wasserstein distance. We show that our new distances define true metrics, that they induce the same topology as the GW distances, and that they enjoy additional robustness to perturbations. These results provide a mathematically rigorous basis for using our robust partial GW distances in applications where outliers and partial matching are concerns.         ",
    "url": "https://arxiv.org/abs/2411.02198",
    "authors": [
      "Jannatul Chhoa",
      "Michael Ivanitskiy",
      "Fushuai Jiang",
      "Shiying Li",
      "Daniel McBride",
      "Tom Needham",
      "Kaiying O'Hare"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.06802",
    "title": "Identifying the impact of local connectivity patterns on dynamics in excitatory-inhibitory networks",
    "abstract": "           Networks of excitatory and inhibitory (EI) neurons form a canonical circuit in the brain. Seminal theoretical results on dynamics of such networks are based on the assumption that synaptic strengths depend on the type of neurons they connect, but are otherwise statistically independent. Recent synaptic physiology datasets however highlight the prominence of specific connectivity patterns that go well beyond what is expected from independent connections. While decades of influential research have demonstrated the strong role of the basic EI cell type structure, to which extent additional connectivity features influence dynamics remains to be fully determined. Here we examine the effects of pairwise connectivity motifs on the linear dynamics in EI networks using an analytical framework that approximates the connectivity in terms of low-rank structures. This low-rank approximation is based on a mathematical derivation of the dominant eigenvalues of the connectivity matrix and predicts the impact on responses to external inputs of connectivity motifs and their interactions with cell-type structure. Our results reveal that a particular pattern of connectivity, chain motifs, have a much stronger impact on dominant eigenmodes than other pairwise motifs. An overrepresentation of chain motifs induces a strong positive eigenvalue in inhibition-dominated networks and generates a potential instability that requires revisiting the classical excitation-inhibition balance criteria. Examining effects of external inputs, we show that chain motifs can on their own induce paradoxical responses where an increased input to inhibitory neurons leads to a decrease in their activity due to the recurrent feedback. These findings have direct implications for the interpretation of experiments in which responses to optogenetic perturbations are measured and used to infer the dynamical regime of cortical circuits.         ",
    "url": "https://arxiv.org/abs/2411.06802",
    "authors": [
      "Yuxiu Shao",
      "David Dahmen",
      "Stefano Recanatesi",
      "Eric Shea-Brown",
      "Srdjan Ostojic"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2411.08926",
    "title": "DG-PPU: Dynamical Graphs based Post-processing of Point Clouds extracted from Knee Ultrasounds",
    "abstract": "           Patients undergoing total knee arthroplasty (TKA) often experience non-specific anterior knee pain, arising from abnormal patellofemoral joint (PFJ) instability. Tracking PFJ motion is challenging since static imaging modalities like CT and MRI are limited by field of view and metal artefact interference. Ultrasounds offer an alternative modality for dynamic musculoskeletal imaging. We aim to achieve accurate visualisation of patellar tracking and PFJ motion, using 3D registration of point clouds extracted from ultrasound scans across different angles of joint flexion. Ultrasound images containing soft tissue are often mislabeled as bone during segmentation, resulting in noisy 3D point clouds that hinder accurate registration of the bony joint anatomy. Machine learning the intrinsic geometry of the knee bone may help us eliminate these false positives. As the intrinsic geometry of the knee does not change during PFJ motion, one may expect this to be robust across multiple angles of joint flexion. Our dynamical graphs-based post-processing algorithm (DG-PPU) is able to achieve this, creating smoother point clouds that accurately represent bony knee anatomy across different angles. After inverting these point clouds back to their original ultrasound images, we evaluated that DG-PPU outperformed manual data cleaning done by our lab technician, deleting false positives and noise with 98.2% precision across three different angles of joint flexion. DG-PPU is the first algorithm to automate the post-processing of 3D point clouds extracted from ultrasound scans. With DG-PPU, we contribute towards the development of a novel patellar mal-tracking assessment system with ultrasound, which currently does not exist.         ",
    "url": "https://arxiv.org/abs/2411.08926",
    "authors": [
      "Injune Hwang",
      "Karthik Saravanan",
      "Caterina V Coralli",
      "S Jack Tu",
      "Stephen J Mellon"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.02894",
    "title": "On H-Intersecting Graph Families and Counting of Homomorphisms",
    "abstract": "           This work derives an upper bound on the maximum cardinality of a family of graphs on a fixed number of vertices, in which the intersection of every two graphs in that family contains a subgraph that is isomorphic to a specified graph H. Such families are referred to as H-intersecting graph families. The bound is derived using the combinatorial version of Shearer's lemma, and it forms a nontrivial extension of the bound derived by Chung, Graham, Frankl, and Shearer (1986), where H is specialized to a triangle. The derived bound is expressed in terms of the chromatic number of H, while a relaxed version, formulated using the Lov\u00e1sz $\\vartheta$-function of the complement of H, offers reduced computational complexity. Additionally, a probabilistic version of Shearer's lemma, combined with properties of the Shannon entropy, are employed to establish bounds related to the enumeration of graph homomorphisms, providing further insights into the interplay between combinatorial structures and information-theoretic principles.         ",
    "url": "https://arxiv.org/abs/2501.02894",
    "authors": [
      "Igal Sason"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2501.12470",
    "title": "Efficient Compilation for Shuttling Trapped-Ion Machines via the Position Graph Architectural Abstraction",
    "abstract": "           With the growth of quantum platforms for gate-based quantum computation, compilation holds a crucial factor in deciding the success of the implementation. There has been rich research and development in compilation techniques for the superconducting-qubit regime. In contrast, the trapped-ion architectures, currently leading in robust quantum computations due to their reliable operations, do not have many competitive compilation strategies. This work presents a novel unifying abstraction, called the position graph, for different types of hardware architectures. Using this abstraction, we model trapped-ion Quantum Charge-Coupled Device (QCCD) architectures and enable high-quality, scalable superconducting compilation methods. In particular, we devise a scheduling algorithm called SHuttling-Aware PERmutative heuristic search algorithm (SHAPER) to tackle the complex constraints and dynamics of trapped-ion QCCD with the cooperation of state-of-the-art permutation-aware mapping. This approach generates native, executable circuits and ion instructions on the hardware that adheres to the physical constraints of shuttling-based quantum computers. Using the position graph abstraction, we evaluate our algorithm on theorized and actual architectures. Our algorithm can successfully compile programs for these architectures where other state-of-the-art algorithms fail. In the cases when other algorithms complete, our algorithm produces a schedule that is $14\\%$ faster on average, up to $69\\%$ in the best case.\\\\ {\\bf Reproducibility:} source code and computational results are available at $[$will be added upon acceptance$]$         ",
    "url": "https://arxiv.org/abs/2501.12470",
    "authors": [
      "Bao Bach",
      "Ilya Safro",
      "Ed Younis"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2503.00856",
    "title": "Asymptotic Analysis of Two-Layer Neural Networks after One Gradient Step under Gaussian Mixtures Data with Structure",
    "abstract": "           In this work, we study the training and generalization performance of two-layer neural networks (NNs) after one gradient descent step under structured data modeled by Gaussian mixtures. While previous research has extensively analyzed this model under isotropic data assumption, such simplifications overlook the complexities inherent in real-world datasets. Our work addresses this limitation by analyzing two-layer NNs under Gaussian mixture data assumption in the asymptotically proportional limit, where the input dimension, number of hidden neurons, and sample size grow with finite ratios. We characterize the training and generalization errors by leveraging recent advancements in Gaussian universality. Specifically, we prove that a high-order polynomial model performs equivalent to the nonlinear neural networks under certain conditions. The degree of the equivalent model is intricately linked to both the \"data spread\" and the learning rate employed during one gradient step. Through extensive simulations, we demonstrate the equivalence between the original model and its polynomial counterpart across various regression and classification tasks. Additionally, we explore how different properties of Gaussian mixtures affect learning outcomes. Finally, we illustrate experimental results on Fashion-MNIST classification, indicating that our findings can translate to realistic data.         ",
    "url": "https://arxiv.org/abs/2503.00856",
    "authors": [
      "Samet Demir",
      "Zafer Dogan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  }
]