[
  {
    "id": "arXiv:2503.07622",
    "title": "Real-Time Detection of Robot Failures Using Gaze Dynamics in Collaborative Tasks",
    "abstract": "           Detecting robot failures during collaborative tasks is crucial for maintaining trust in human-robot interactions. This study investigates user gaze behaviour as an indicator of robot failures, utilising machine learning models to distinguish between non-failure and two types of failures: executional and decisional. Eye-tracking data were collected from 26 participants collaborating with a robot on Tangram puzzle-solving tasks. Gaze metrics, such as average gaze shift rates and the probability of gazing at specific areas of interest, were used to train machine learning classifiers, including Random Forest, AdaBoost, XGBoost, SVM, and CatBoost. The results show that Random Forest achieved 90% accuracy for detecting executional failures and 80% for decisional failures using the first 5 seconds of failure data. Real-time failure detection was evaluated by segmenting gaze data into intervals of 3, 5, and 10 seconds. These findings highlight the potential of gaze dynamics for real-time error detection in human-robot collaboration.         ",
    "url": "https://arxiv.org/abs/2503.07622",
    "authors": [
      "Ramtin Tabatabaei",
      "Vassilis Kostakos",
      "Wafa Johal"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.07633",
    "title": "A Quantum Neural Network Transfer-Learning Model for Forecasting Problems with Continuous and Discrete Variables",
    "abstract": "           This study introduces a continuous-variable quantum neural network (CV-QNN) model designed as a transfer-learning approach for forecasting problems. The proposed quantum technique features a simple structure with only eight trainable parameters, a single quantum layer with two wires to create entanglement, and ten quantum gates, hence the name QNNet10, effectively mimicking the functionality of classical neural networks. A notable aspect is that the quantum network achieves high accuracy with random initialization after a single iteration. This pretrained model is innovative as it requires no training or parameter tuning when applied to new datasets, allowing for parameter freezing while enabling the addition of a final layer for fine-tuning. Additionally, an equivalent discrete-variable quantum neural network (DV-QNN) is presented, structured similarly to the CV model. However, analysis shows that the two-wire DV model does not significantly enhance performance. As a result, a four-wire DV model is proposed, achieving comparable results but requiring a larger and more complex structure with additional gates. The pretrained model is applied to five forecasting problems of varying sizes, demonstrating its effectiveness.         ",
    "url": "https://arxiv.org/abs/2503.07633",
    "authors": [
      "Ismael Abdulrahman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2503.07635",
    "title": "Cross-modal Causal Relation Alignment for Video Question Grounding",
    "abstract": "           Video question grounding (VideoQG) requires models to answer the questions and simultaneously infer the relevant video segments to support the answers. However, existing VideoQG methods usually suffer from spurious cross-modal correlations, leading to a failure to identify the dominant visual scenes that align with the intended question. Moreover, vision-language models exhibit unfaithful generalization performance and lack robustness on challenging downstream tasks such as VideoQG. In this work, we propose a novel VideoQG framework named Cross-modal Causal Relation Alignment (CRA), to eliminate spurious correlations and improve the causal consistency between question-answering and video temporal grounding. Our CRA involves three essential components: i) Gaussian Smoothing Grounding (GSG) module for estimating the time interval via cross-modal attention, which is de-noised by an adaptive Gaussian filter, ii) Cross-Modal Alignment (CMA) enhances the performance of weakly supervised VideoQG by leveraging bidirectional contrastive learning between estimated video segments and QA features, iii) Explicit Causal Intervention (ECI) module for multimodal deconfounding, which involves front-door intervention for vision and back-door intervention for language. Extensive experiments on two VideoQG datasets demonstrate the superiority of our CRA in discovering visually grounded content and achieving robust question reasoning. Codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.07635",
    "authors": [
      "Weixing Chen",
      "Yang Liu",
      "Binglin Chen",
      "Jiandong Su",
      "Yongsen Zheng",
      "Liang Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.07638",
    "title": "Leveraging Taxonomy Similarity for Next Activity Prediction in Patient Treatment",
    "abstract": "           The rapid progress in modern medicine presents physicians with complex challenges when planning patient treatment. Techniques from the field of Predictive Business Process Monitoring, like Next-activity-prediction (NAP) can be used as a promising technique to support physicians in treatment planning, by proposing a possible next treatment step. Existing patient data, often in the form of electronic health records, can be analyzed to recommend the next suitable step in the treatment process. However, the use of patient data poses many challenges due to its knowledge-intensive character, high variability and scarcity of medical data. To overcome these challenges, this article examines the use of the knowledge encoded in taxonomies to improve and explain the prediction of the next activity in the treatment process. This study proposes the TS4NAP approach, which uses medical taxonomies (ICD-10-CM and ICD-10-PCS) in combination with graph matching to assess the similarities of medical codes to predict the next treatment step. The effectiveness of the proposed approach will be evaluated using event logs that are derived from the MIMIC-IV dataset. The results highlight the potential of using domain-specific knowledge held in taxonomies to improve the prediction of the next activity, and thus can improve treatment planning and decision-making by making the predictions more explainable.         ",
    "url": "https://arxiv.org/abs/2503.07638",
    "authors": [
      "Martin Kuhn",
      "Joscha Gr\u00fcger",
      "Tobias Geyer",
      "Ralph Bergmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.07642",
    "title": "dnamite: A Python Package for Neural Additive Models",
    "abstract": "           Additive models offer accurate and interpretable predictions for tabular data, a critical tool for statistical modeling. Recent advances in Neural Additive Models (NAMs) allow these models to handle complex machine learning tasks, including feature selection and survival analysis, on large-scale data. This paper introduces dnamite, a Python package that implements NAMs for these advanced applications. dnamite provides a scikit-learn style interface to train regression, classification, and survival analysis NAMs, with built-in support for feature selection. We describe the methodology underlying dnamite, its design principles, and its implementation. Through an application to the MIMIC III clinical dataset, we demonstrate the utility of dnamite in a real-world setting where feature selection and survival analysis are both important. The package is publicly available via pip and documented at this http URL.         ",
    "url": "https://arxiv.org/abs/2503.07642",
    "authors": [
      "Mike Van Ness",
      "Madeleine Udell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.07645",
    "title": "BicliqueEncoder: An Efficient Method for Link Prediction in Bipartite Networks using Formal Concept Analysis and Transformer Encoder",
    "abstract": "           We propose a novel and efficient method for link prediction in bipartite networks, using \\textit{formal concept analysis} (FCA) and the Transformer encoder. Link prediction in bipartite networks finds practical applications in various domains such as product recommendation in online sales, and prediction of chemical-disease interaction in medical science. Since for link prediction, the topological structure of a network contains valuable information, many approaches focus on extracting structural features and then utilizing them for link prediction. Bi-cliques, as a type of structural feature of bipartite graphs, can be utilized for link prediction. Although several link prediction methods utilizing bi-cliques have been proposed and perform well in rather small datasets, all of them face challenges with scalability when dealing with large datasets since they demand substantial computational resources. This limits the practical utility of these approaches in real-world applications. To overcome the limitation, we introduce a novel approach employing iceberg concept lattices and the Transformer encoder. Our method requires fewer computational resources, making it suitable for large-scale datasets while maintaining high prediction performance. We conduct experiments on five large real-world datasets that exceed the capacity of previous bi-clique-based approaches to demonstrate the efficacy of our method. Additionally, we perform supplementary experiments on five small datasets to compare with the previous bi-clique-based methods for bipartite link prediction and demonstrate that our method is more efficient than the previous ones.         ",
    "url": "https://arxiv.org/abs/2503.07645",
    "authors": [
      "Hongyuan Yang",
      "Siqi Peng",
      "Akihiro Yamamoto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.07653",
    "title": "Early Detection of Mental Health Issues Using Social Media Posts",
    "abstract": "           The increasing prevalence of mental health disorders, such as depression, anxiety, and bipolar disorder, calls for immediate need in developing tools for early detection and intervention. Social media platforms, like Reddit, represent a rich source of user-generated content, reflecting emotional and behavioral patterns. In this work, we propose a multi-modal deep learning framework that integrates linguistic and temporal features for early detection of mental health crises. Our approach is based on the method that utilizes a BiLSTM network both for text and temporal feature analysis, modeling sequential dependencies in a different manner, capturing contextual patterns quite well. This work includes a cross-modal attention approach that allows fusion of such outputs into context-aware classification of mental health conditions. The model was then trained and evaluated on a dataset of labeled Reddit posts preprocessed using text preprocessing, scaling of temporal features, and encoding of labels. Experimental results indicate that the proposed architecture performs better compared to traditional models with a validation accuracy of 74.55% and F1-Score of 0.7376. This study presents the importance of multi-modal learning for mental health detection and provides a baseline for further improvements by using more advanced attention mechanisms and other data modalities.         ",
    "url": "https://arxiv.org/abs/2503.07653",
    "authors": [
      "Qasim Bin Saeed",
      "Ijaz Ahmed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.07661",
    "title": "Disrupting Model Merging: A Parameter-Level Defense Without Sacrificing Accuracy",
    "abstract": "           Model merging is a technique that combines multiple finetuned models into a single model without additional training, allowing a free-rider to cheaply inherit specialized capabilities. This study investigates methodologies to suppress unwanted model merging by free-riders. Existing methods such as model watermarking or fingerprinting can only detect merging in hindsight. In contrast, we propose a first proactive defense against model merging. Specifically, our defense method modifies the model parameters so that the model is disrupted if the model is merged with any other model, while its functionality is kept unchanged if not merged with others. Our approach consists of two modules, rearranging MLP parameters and scaling attention heads, which push the model out of the shared basin in parameter space, causing the merging performance with other models to degrade significantly. We conduct extensive experiments on image classification, image generation, and text classification to demonstrate that our defense severely disrupts merging while retaining the functionality of the post-protect model. Moreover, we analyze potential adaptive attacks and further propose a dropout-based pruning to improve our proposal's robustness.         ",
    "url": "https://arxiv.org/abs/2503.07661",
    "authors": [
      "Wei Junhao",
      "Yu Zhe",
      "Sakuma Jun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.07665",
    "title": "The Computational Complexity of Positive Non-Clashing Teaching in Graphs",
    "abstract": "           We study the classical and parameterized complexity of computing the positive non-clashing teaching dimension of a set of concepts, that is, the smallest number of examples per concept required to successfully teach an intelligent learner under the considered, previously established model. For any class of concepts, it is known that this problem can be effortlessly transferred to the setting of balls in a graph G. We establish (1) the NP-hardness of the problem even when restricted to instances with positive non-clashing teaching dimension k=2 and where all balls in the graph are present, (2) near-tight running time upper and lower bounds for the problem on general graphs, (3) fixed-parameter tractability when parameterized by the vertex integrity of G, and (4) a lower bound excluding fixed-parameter tractability when parameterized by the feedback vertex number and pathwidth of G, even when combined with k. Our results provide a nearly complete understanding of the complexity landscape of computing the positive non-clashing teaching dimension and answer open questions from the literature.         ",
    "url": "https://arxiv.org/abs/2503.07665",
    "authors": [
      "Robert Ganian",
      "Liana Khazaliya",
      "Fionn Mc Inerney",
      "Mathis Rocton"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.07681",
    "title": "Transforming Traditional Neural Networks into Neuromorphic Quantum-Cognitive Models: A Tutorial with Applications",
    "abstract": "           Quantum technologies are increasingly pervasive, underpinning the operation of numerous electronic, optical and medical devices. Today, we are also witnessing rapid advancements in quantum computing and communication. However, access to quantum technologies in computation remains largely limited to professionals in research organisations and high-tech industries. This paper demonstrates how traditional neural networks can be transformed into neuromorphic quantum models, enabling anyone with a basic understanding of undergraduate-level machine learning to create quantum-inspired models that mimic the functioning of the human brain -- all using a standard laptop. We present several examples of these quantum machine learning transformations and explore their potential applications, aiming to make quantum technology more accessible and practical for broader use. The examples discussed in this paper include quantum-inspired analogues of feedforward neural networks, recurrent neural networks, Echo State Network reservoir computing and Bayesian neural networks, demonstrating that a quantum approach can both optimise the training process and equip the models with certain human-like cognitive characteristics.         ",
    "url": "https://arxiv.org/abs/2503.07681",
    "authors": [
      "Milan Maksimovic",
      "Ivan S. Maksymov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2503.07682",
    "title": "A Time Series Multitask Framework Integrating a Large Language Model, Pre-Trained Time Series Model, and Knowledge Graph",
    "abstract": "           Time series analysis is crucial in fields like finance, transportation, and industry. However, traditional models often focus solely on temporal features, limiting their ability to capture underlying information. This paper proposes a novel time series multitask framework, called LTM, which integrates temporal features with textual descriptions to enhance analytical and predictive capabilities. LTM combines pre-trained time series model, large language model (LLM), and knowledge graph to tackle time series tasks, including forecasting, imputation, and anomaly detection. LTM achieves improved performance with a few trainable parameters. It is very efficient and practical. LTM encodes time series data into patches and enriches user-provided prompts using knowledge graphs to generate enhanced prompts. A novel feature fusion method embeds prompts into each patch encoding, which is processed by a frozen LLM, followed by a feature enhancement module and a time decoder module. During fine-tuning stage, cosine similarity between prompts and temporal patches is integrated into the loss function to boost performance. Experiments on benchmark datasets show that LTM significantly outperforms existing methods. It provides a robust and versatile solution for time series tasks.         ",
    "url": "https://arxiv.org/abs/2503.07682",
    "authors": [
      "Shule Hao",
      "Junpeng Bao",
      "Chuncheng Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.07697",
    "title": "PoisonedParrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models",
    "abstract": "           As the capabilities of large language models (LLMs) continue to expand, their usage has become increasingly prevalent. However, as reflected in numerous ongoing lawsuits regarding LLM-generated content, addressing copyright infringement remains a significant challenge. In this paper, we introduce PoisonedParrot: the first stealthy data poisoning attack that induces an LLM to generate copyrighted content even when the model has not been directly trained on the specific copyrighted material. PoisonedParrot integrates small fragments of copyrighted text into the poison samples using an off-the-shelf LLM. Despite its simplicity, evaluated in a wide range of experiments, PoisonedParrot is surprisingly effective at priming the model to generate copyrighted content with no discernible side effects. Moreover, we discover that existing defenses are largely ineffective against our attack. Finally, we make the first attempt at mitigating copyright-infringement poisoning attacks by proposing a defense: ParrotTrap. We encourage the community to explore this emerging threat model further.         ",
    "url": "https://arxiv.org/abs/2503.07697",
    "authors": [
      "Michael-Andrei Panaitescu-Liess",
      "Pankayaraj Pathmanathan",
      "Yigitcan Kaya",
      "Zora Che",
      "Bang An",
      "Sicheng Zhu",
      "Aakriti Agrawal",
      "Furong Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.07700",
    "title": "A Task and Motion Planning Framework Using Iteratively Deepened AND/OR Graph Networks",
    "abstract": "           In this paper, we present an approach for integrated task and motion planning based on an AND/OR graph network, which is used to represent task-level states and actions, and we leverage it to implement different classes of task and motion planning problems (TAMP). Several problems that fall under task and motion planning do not have a predetermined number of sub-tasks to achieve a goal. For example, while retrieving a target object from a cluttered workspace, in principle the number of object re-arrangements required to finally grasp it cannot be known ahead of time. To address this challenge, and in contrast to traditional planners, also those based on AND/OR graphs, we grow the AND/OR graph at run-time by progressively adding sub-graphs until grasping the target object becomes feasible, which yields a network of AND/OR graphs. The approach is extended to enable multi-robot task and motion planning, and (i) it allows us to perform task allocation while coordinating the activity of a given number of robots, and (ii) can handle multi-robot tasks involving an a priori unknown number of sub-tasks. The approach is evaluated and validated both in simulation and with a real dual-arm robot manipulator, that is, Baxter from Rethink Robotics. In particular, for the single-robot task and motion planning, we validated our approach in three different TAMP domains. Furthermore, we also use three different robots for simulation, namely, Baxter, Franka Emika Panda manipulators, and a PR2 robot. Experiments show that our approach can be readily scaled to scenarios with many objects and robots, and is capable of handling different classes of TAMP problems.         ",
    "url": "https://arxiv.org/abs/2503.07700",
    "authors": [
      "Hossein Karami",
      "Antony Thomas",
      "Fulvio Mastrogiovanni"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.07702",
    "title": "A Reliable Self-Organized Distributed Complex Network for Communication of Smart Agents",
    "abstract": "           Collaboration is a fundamental and essential characteristic of many complex systems, ranging from ant colonies to human societies. Each component within a complex system interacts with others, even at a distance, to accomplish a given task. A network of collaboration can be defined to study the collective behavior of such systems within the framework of complex networks. The nodes in these networks may represent simple organisms or more sophisticated intelligent agents, such as humans. In this study, we utilize intelligent agents (nodes) trained through reinforcement learning techniques to establish connections with their neighbors, ultimately leading to the emergence of a large-scale communication cluster. Notably, there is no centralized administrator; instead, agents must adjust their connections based on information obtained from local observations. The connection strategy is formulated using a physical Hamiltonian, thereby categorizing this intelligent system under the paradigm of \"Physics-Guided Machine Learning\".         ",
    "url": "https://arxiv.org/abs/2503.07702",
    "authors": [
      "Mehdi Bakhshipoor",
      "Yousef Azizi",
      "Seyed Ehsan Nedaaee Oskoee"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2503.07743",
    "title": "SANDRO: a Robust Solver with a Splitting Strategy for Point Cloud Registration",
    "abstract": "           Point cloud registration is a critical problem in computer vision and robotics, especially in the field of navigation. Current methods often fail when faced with high outlier rates or take a long time to converge to a suitable solution. In this work, we introduce a novel algorithm for point cloud registration called SANDRO (Splitting strategy for point cloud Alignment using Non-convex anD Robust Optimization), which combines an Iteratively Reweighted Least Squares (IRLS) framework with a robust loss function with graduated non-convexity. This approach is further enhanced by a splitting strategy designed to handle high outlier rates and skewed distributions of outliers. SANDRO is capable of addressing important limitations of existing methods, as in challenging scenarios where the presence of high outlier rates and point cloud symmetries significantly hinder convergence. SANDRO achieves superior performance in terms of success rate when compared to the state-of-the-art methods, demonstrating a 20% improvement from the current state of the art when tested on the Redwood real dataset and 60% improvement when tested on synthetic data.         ",
    "url": "https://arxiv.org/abs/2503.07743",
    "authors": [
      "Michael Adlerstein",
      "Jo\u00e3o Carlos Virgolino Soares",
      "Angelo Bratta",
      "Claudio Semini"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.07767",
    "title": "Better Pose Initialization for Fast and Robust 2D/3D Pelvis Registration",
    "abstract": "           This paper presents an approach for improving 2D/3D pelvis registration in optimization-based pose estimators using a learned initialization function. Current methods often fail to converge to the optimal solution when initialized naively. We find that even a coarse initializer greatly improves pose estimator accuracy, and improves overall computational efficiency. This approach proves to be effective also in challenging cases under more extreme pose variation. Experimental validation demonstrates that our method consistently achieves robust and accurate registration, enhancing the reliability of 2D/3D registration for clinical applications.         ",
    "url": "https://arxiv.org/abs/2503.07767",
    "authors": [
      "Yehyun Suh",
      "J. Ryan Martin",
      "Daniel Moyer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.07769",
    "title": "Algorithms for Distance Problems in Continuous Graphs",
    "abstract": "           We study the problem of computing the diameter and the mean distance of a continuous graph, i.e., a connected graph where all points along the edges, instead of only the vertices, must be taken into account. It is known that for continuous graphs with $m$ edges these values can be computed in roughly $O(m^2)$ time. In this paper, we use geometric techniques to obtain subquadratic time algorithms to compute the diameter and the mean distance of a continuous graph for two well-established classes of sparse graphs. We show that the diameter and the mean distance of a continuous graph of treewidth at most $k$ can be computed in $O(n\\log^{O(k)} n)$ time, where $n$ is the number of vertices in the graph. We also show that computing the diameter and mean distance of a continuous planar graph with $n$ vertices and $F$ faces takes $O(n F \\log n)$ time.         ",
    "url": "https://arxiv.org/abs/2503.07769",
    "authors": [
      "Sergio Cabello",
      "Delia Garijo",
      "Antonia Kalb",
      "Fabian Klute",
      "Irene Parada",
      "Rodrigo I. Silveira"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2503.07770",
    "title": "Evaluating LLaMA 3.2 for Software Vulnerability Detection",
    "abstract": "           Deep Learning (DL) has emerged as a powerful tool for vulnerability detection, often outperforming traditional solutions. However, developing effective DL models requires large amounts of real-world data, which can be difficult to obtain in sufficient quantities. To address this challenge, DiverseVul dataset has been curated as the largest dataset of vulnerable and non-vulnerable C/C++ functions extracted exclusively from real-world projects. Its goal is to provide high-quality, large-scale samples for training DL models. However, during our study several inconsistencies were identified in the raw dataset while applying pre-processing techniques, highlighting the need for a refined version. In this work, we present a refined version of DiverseVul dataset, which is used to fine-tune a large language model, LLaMA 3.2, for vulnerability detection. Experimental results show that the use of pre-processing techniques led to an improvement in performance, with the model achieving an F1-Score of 66%, a competitive result when compared to our baseline, which achieved a 47% F1-Score in software vulnerability detection.         ",
    "url": "https://arxiv.org/abs/2503.07770",
    "authors": [
      "Jos\u00e9 Gon\u00e7alves",
      "Miguel Silva",
      "Bernardo Cabral",
      "Tiago Dias",
      "Eva Maia",
      "Isabel Pra\u00e7a",
      "Ricardo Severino",
      "Lu\u00eds Lino Ferreira"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.07775",
    "title": "Sublinear Algorithms for Wasserstein and Total Variation Distances: Applications to Fairness and Privacy Auditing",
    "abstract": "           Resource-efficiently computing representations of probability distributions and the distances between them while only having access to the samples is a fundamental and useful problem across mathematical sciences. In this paper, we propose a generic algorithmic framework to estimate the PDF and CDF of any sub-Gaussian distribution while the samples from them arrive in a stream. We compute mergeable summaries of distributions from the stream of samples that require sublinear space w.r.t. the number of observed samples. This allows us to estimate Wasserstein and Total Variation (TV) distances between any two sub-Gaussian distributions while samples arrive in streams and from multiple sources (e.g. federated learning). Our algorithms significantly improves on the existing methods for distance estimation incurring super-linear time and linear space complexities. In addition, we use the proposed estimators of Wasserstein and TV distances to audit the fairness and privacy of the ML algorithms. We empirically demonstrate the efficiency of the algorithms for estimating these distances and auditing using both synthetic and real-world datasets.         ",
    "url": "https://arxiv.org/abs/2503.07775",
    "authors": [
      "Debabrota Basu",
      "Debarshi Chanda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Data Structures and Algorithms (cs.DS)",
      "Computation (stat.CO)"
    ]
  },
  {
    "id": "arXiv:2503.07792",
    "title": "Efficient Neural Clause-Selection Reinforcement",
    "abstract": "           Clause selection is arguably the most important choice point in saturation-based theorem proving. Framing it as a reinforcement learning (RL) task is a way to challenge the human-designed heuristics of state-of-the-art provers and to instead automatically evolve -- just from prover experiences -- their potentially optimal replacement. In this work, we present a neural network architecture for scoring clauses for clause selection that is powerful yet efficient to evaluate. Following RL principles to make design decisions, we integrate the network into the Vampire theorem prover and train it from successful proof attempts. An experiment on the diverse TPTP benchmark finds the neurally guided prover improve over a baseline strategy, from which it initially learns--in terms of the number of in-training-unseen problems solved under a practically relevant, short CPU instruction limit--by 20%.         ",
    "url": "https://arxiv.org/abs/2503.07792",
    "authors": [
      "Martin Suda"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2503.07799",
    "title": "Self-supervised Normality Learning and Divergence Vector-guided Model Merging for Zero-shot Congenital Heart Disease Detection in Fetal Ultrasound Videos",
    "abstract": "           Congenital Heart Disease (CHD) is one of the leading causes of fetal mortality, yet the scarcity of labeled CHD data and strict privacy regulations surrounding fetal ultrasound (US) imaging present significant challenges for the development of deep learning-based models for CHD detection. Centralised collection of large real-world datasets for rare conditions, such as CHD, from large populations requires significant co-ordination and resource. In addition, data governance rules increasingly prevent data sharing between sites. To address these challenges, we introduce, for the first time, a novel privacy-preserving, zero-shot CHD detection framework that formulates CHD detection as a normality modeling problem integrated with model merging. In our framework dubbed Sparse Tube Ultrasound Distillation (STUD), each hospital site first trains a sparse video tube-based self-supervised video anomaly detection (VAD) model on normal fetal heart US clips with self-distillation loss. This enables site-specific models to independently learn the distribution of healthy cases. To aggregate knowledge across the decentralized models while maintaining privacy, we propose a Divergence Vector-Guided Model Merging approach, DivMerge, that combines site-specific models into a single VAD model without data exchange. Our approach preserves domain-agnostic rich spatio-temporal representations, ensuring generalization to unseen CHD cases. We evaluated our approach on real-world fetal US data collected from 5 hospital sites. Our merged model outperformed site-specific models by 23.77% and 30.13% in accuracy and F1-score respectively on external test sets.         ",
    "url": "https://arxiv.org/abs/2503.07799",
    "authors": [
      "Pramit Saha",
      "Divyanshu Mishra",
      "Netzahualcoyotl Hernandez-Cruz",
      "Olga Patey",
      "Aris Papageorghiou",
      "Yuki M. Asano",
      "J. Alison Noble"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.07818",
    "title": "Strengthening the Internal Adversarial Robustness in Lifted Neural Networks",
    "abstract": "           Lifted neural networks (i.e. neural architectures explicitly optimizing over respective network potentials to determine the neural activities) can be combined with a type of adversarial training to gain robustness for internal as well as input layers, in addition to improved generalization performance. In this work we first investigate how adversarial robustness in this framework can be further strengthened by solely modifying the training loss. In a second step we fix some remaining limitations and arrive at a novel training loss for lifted neural networks, that combines targeted and untargeted adversarial perturbations.         ",
    "url": "https://arxiv.org/abs/2503.07818",
    "authors": [
      "Christopher Zach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.07826",
    "title": "Magnet: Multi-turn Tool-use Data Synthesis and Distillation via Graph Translation",
    "abstract": "           Large language models (LLMs) have exhibited the ability to effectively utilize external tools to address user queries. However, their performance may be limited in complex, multi-turn interactions involving users and multiple tools. To address this, we propose Magnet, a principled framework for synthesizing high-quality training trajectories to enhance the function calling capability of large language model agents in multi-turn conversations with humans. The framework is based on automatic and iterative translations from a function signature path to a sequence of queries and executable function calls. We model the complicated function interactions in multi-turn cases with graph and design novel node operations to build reliable signature paths. Motivated by context distillation, when guiding the generation of positive and negative trajectories using a teacher model, we provide reference function call sequences as positive hints in context and contrastive, incorrect function calls as negative hints. Experiments show that training with the positive trajectories with supervised fine-tuning and preference optimization against negative trajectories, our 14B model, Magnet-14B-mDPO, obtains 68.01 on BFCL-v3 and 73.30 on ToolQuery, surpassing the performance of the teacher model Gemini-1.5-pro-002 by a large margin in function calling.         ",
    "url": "https://arxiv.org/abs/2503.07826",
    "authors": [
      "Fan Yin",
      "Zifeng Wang",
      "I-Hung Hsu",
      "Jun Yan",
      "Ke Jiang",
      "Yanfei Chen",
      "Jindong Gu",
      "Long T. Le",
      "Kai-Wei Chang",
      "Chen-Yu Lee",
      "Hamid Palangi",
      "Tomas Pfister"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.07828",
    "title": "Neural Radiance and Gaze Fields for Visual Attention Modeling in 3D Environments",
    "abstract": "           We introduce Neural Radiance and Gaze Fields (NeRGs) as a novel approach for representing visual attention patterns in 3D scenes. Our system renders a 2D view of a 3D scene with a pre-trained Neural Radiance Field (NeRF) and visualizes the gaze field for arbitrary observer positions, which may be decoupled from the render camera perspective. We achieve this by augmenting a standard NeRF with an additional neural network that models the gaze probability distribution. The output of a NeRG is a rendered image of the scene viewed from the camera perspective and a pixel-wise salience map representing conditional probability that an observer fixates on a given surface within the 3D scene as visible in the rendered image. Much like how NeRFs perform novel view synthesis, NeRGs enable the reconstruction of gaze patterns from arbitrary perspectives within complex 3D scenes. To ensure consistent gaze reconstructions, we constrain gaze prediction on the 3D structure of the scene and model gaze occlusion due to intervening surfaces when the observer's viewpoint is decoupled from the rendering camera. For training, we leverage ground truth head pose data from skeleton tracking data or predictions from 2D salience models. We demonstrate the effectiveness of NeRGs in a real-world convenience store setting, where head pose tracking data is available.         ",
    "url": "https://arxiv.org/abs/2503.07828",
    "authors": [
      "Andrei Chubarau",
      "Yinan Wang",
      "James J. Clark"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.07832",
    "title": "RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code",
    "abstract": "           Recent advances in language model (LM) agents and function calling have enabled autonomous, feedback-driven systems to solve problems across various digital domains. To better understand the unique limitations of LM agents, we introduce RefactorBench, a benchmark consisting of 100 large handcrafted multi-file refactoring tasks in popular open-source repositories. Solving tasks within RefactorBench requires thorough exploration of dependencies across multiple files and strong adherence to relevant instructions. Every task is defined by 3 natural language instructions of varying specificity and is mutually exclusive, allowing for the creation of longer combined tasks on the same repository. Baselines on RefactorBench reveal that current LM agents struggle with simple compositional tasks, solving only 22% of tasks with base instructions, in contrast to a human developer with short time constraints solving 87%. Through trajectory analysis, we identify various unique failure modes of LM agents, and further explore the failure mode of tracking past actions. By adapting a baseline agent to condition on representations of state, we achieve a 43.9% improvement in solving RefactorBench tasks. We further extend our state-aware approach to encompass entire digital environments and outline potential directions for future research. RefactorBench aims to support the study of LM agents by providing a set of real-world, multi-hop tasks within the realm of code.         ",
    "url": "https://arxiv.org/abs/2503.07832",
    "authors": [
      "Dhruv Gautam",
      "Spandan Garg",
      "Jinu Jang",
      "Neel Sundaresan",
      "Roshanak Zilouchian Moghaddam"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.07834",
    "title": "Network Analysis of Uniswap: Centralization and Fragility in the Decentralized Exchange Market",
    "abstract": "           The Uniswap is a Decentralized Exchange (DEX) protocol that facilitates automatic token exchange without the need for traditional order books. Every pair of tokens forms a liquidity pool on Uniswap, and each token can be paired with any other token to create liquidity pools. This characteristic motivates us to employ a complex network approach to analyze the features of the Uniswap market. This research presents a comprehensive analysis of the Uniswap network using complex network methods. The network on October 31, 2023, is built to observe its recent features, showcasing both scale-free and core-periphery properties. By employing node and edge-betweenness metrics, we detect the most important tokens and liquidity pools. Additionally, we construct daily networks spanning from the beginning of Uniswap V2 on May 5, 2020, until October 31, 2023, and our findings demonstrate that the network becomes increasingly fragile over time. Furthermore, we conduct a robustness analysis by simulating the deletion of nodes to estimate the impact of some extreme events such as the Terra collapse. The results indicate that the Uniswap network exhibits robustness, yet it is notably fragile when deleting tokens with high betweenness centrality. This finding highlights that, despite being a decentralized exchange, Uniswap exhibits significant centralization tendencies in terms of token network connectivity and the distribution of TVL across nodes (tokens) and edges (liquidity pools).         ",
    "url": "https://arxiv.org/abs/2503.07834",
    "authors": [
      "Tao Yan",
      "Claudio J.Tessone"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2503.07849",
    "title": "Actual Causation and Nondeterministic Causal Models",
    "abstract": "           In (Beckers, 2025) I introduced nondeterministic causal models as a generalization of Pearl's standard deterministic causal models. I here take advantage of the increased expressivity offered by these models to offer a novel definition of actual causation (that also applies to deterministic models). Instead of motivating the definition by way of (often subjective) intuitions about examples, I proceed by developing it based entirely on the unique function that it can fulfil in communicating and learning a causal model. First I generalize the more basic notion of counterfactual dependence, second I show how this notion has a vital role to play in the logic of causal discovery, third I introduce the notion of a structural simplification of a causal model, and lastly I bring both notions together in my definition of actual causation. Although novel, the resulting definition arrives at verdicts that are almost identical to those of my previous definition (Beckers, 2021, 2022).         ",
    "url": "https://arxiv.org/abs/2503.07849",
    "authors": [
      "Sander Beckers"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.07852",
    "title": "CIMAGE: Exploiting the Conditional Independence in Masked Graph Auto-encoders",
    "abstract": "           Recent Self-Supervised Learning (SSL) methods encapsulating relational information via masking in Graph Neural Networks (GNNs) have shown promising performance. However, most existing approaches rely on random masking strategies in either feature or graph space, which may fail to capture task-relevant information fully. We posit that this limitation stems from an inability to achieve minimum redundancy between masked and unmasked components while ensuring maximum relevance of both to potential downstream tasks. Conditional Independence (CI) inherently satisfies the minimum redundancy and maximum relevance criteria, but its application typically requires access to downstream labels. To address this challenge, we introduce CIMAGE, a novel approach that leverages Conditional Independence to guide an effective masking strategy within the latent space. CIMAGE utilizes CI-aware latent factor decomposition to generate two distinct contexts, leveraging high-confidence pseudo-labels derived from unsupervised graph clustering. In this framework, the pretext task involves reconstructing the masked second context solely from the information provided by the first context. Our theoretical analysis further supports the superiority of CIMAGE's novel CI-aware masking method by demonstrating that the learned embedding exhibits approximate linear separability, which enables accurate predictions for the downstream task. Comprehensive evaluations across diverse graph benchmarks illustrate the advantage of CIMAGE, with notably higher average rankings on node classification and link prediction tasks. Notably, our proposed model highlights the under-explored potential of CI in enhancing graph SSL methodologies and offers enriched insights for effective graph representation learning.         ",
    "url": "https://arxiv.org/abs/2503.07852",
    "authors": [
      "Jongwon Park",
      "Heesoo Jung",
      "Hogun Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.07858",
    "title": "Real-Time Line Parameter Estimation Method for Multi-Phase Unbalanced Distribution Networks",
    "abstract": "           An accurate distribution network model is crucial for monitoring, state estimation and energy management. However, existing data-driven methods often struggle with scalability or impose a heavy computational burden on large distribution networks. In this paper, leveraging natural load dynamics, we propose a two-stage line estimation method for multiphase unbalanced distribution networks. Simulation results using real-life load and PV data show that the proposed method reduces computational time by one to two orders of magnitude compared to existing methods.         ",
    "url": "https://arxiv.org/abs/2503.07858",
    "authors": [
      "Sakirat Wolly",
      "Xiaozhe Wang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.07862",
    "title": "cantnlp@DravidianLangTech-2025: A Bag-of-Sounds Approach to Multimodal Hate Speech Detection",
    "abstract": "           This paper presents the systems and results for the Multimodal Social Media Data Analysis in Dravidian Languages (MSMDA-DL) shared task at the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages (DravidianLangTech-2025). We took a `bag-of-sounds' approach by training our hate speech detection system on the speech (audio) data using transformed Mel spectrogram measures. While our candidate model performed poorly on the test set, our approach offered promising results during training and development for Malayalam and Tamil. With sufficient and well-balanced training data, our results show that it is feasible to use both text and speech (audio) data in the development of multimodal hate speech detection systems.         ",
    "url": "https://arxiv.org/abs/2503.07862",
    "authors": [
      "Sidney Wong",
      "Andrew Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.07870",
    "title": "Personalized Code Readability Assessment: Are We There Yet?",
    "abstract": "           Unreadable code could be a breeding ground for errors. Thus, previous work defined approaches based on machine learning to automatically assess code readability that can warn developers when some code artifacts (e.g., classes) become unreadable. Given datasets of code snippets manually evaluated by several developers in terms of their perceived readability, such approaches (i) establish a snippet-level ground truth, and (ii) train a binary (readable/unreadable) or a ternary (readable/neutral/unreadable) code readability classifier. Given this procedure, all existing approaches neglect the subjectiveness of code readability, i.e., the possible different developer-specific nuances in the code readability perception. In this paper, we aim to understand to what extent it is possible to assess code readability as subjectively perceived by developers through a personalized code readability assessment approach. This problem is significantly more challenging than the snippet-level classification problem: We assume that, in a realistic scenario, a given developer is keen to provide only a few code readability evaluations, thus less data is available. For this reason, we adopt an LLM with few-shot learning to achieve our goal. Our results, however, show that such an approach achieves worse results than a state-of-the-art feature-based model that is trained to work at the snippet-level. We tried to understand why this happens by looking more closely at the quality of the available code readability datasets and assessed the consistency of the inter-developer evaluations. We observed that up to a third of the evaluations are self-contradictory. Our negative results call for new and more reliable code readability datasets.         ",
    "url": "https://arxiv.org/abs/2503.07870",
    "authors": [
      "Antonio Vitale",
      "Emanuela Guglielmi",
      "Rocco Oliveto",
      "Simone Scalabrino"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.07882",
    "title": "ReLATE: Resilient Learner Selection for Multivariate Time-Series Classification Against Adversarial Attacks",
    "abstract": "           Minimizing computational overhead in time-series classification, particularly in deep learning models, presents a significant challenge. This challenge is further compounded by adversarial attacks, emphasizing the need for resilient methods that ensure robust performance and efficient model selection. We introduce ReLATE, a framework that identifies robust learners based on dataset similarity, reduces computational overhead, and enhances resilience. ReLATE maintains multiple deep learning models in well-known adversarial attack scenarios, capturing model performance. ReLATE identifies the most analogous dataset to a given target using a similarity metric, then applies the optimal model from the most similar dataset. ReLATE reduces computational overhead by an average of 81.2%, enhancing adversarial resilience and streamlining robust model selection, all without sacrificing performance, within 4.2% of Oracle.         ",
    "url": "https://arxiv.org/abs/2503.07882",
    "authors": [
      "Cagla Ipek Kocal",
      "Onat Gungor",
      "Aaron Tartz",
      "Tajana Rosing",
      "Baris Aksanli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.07883",
    "title": "Cross-platform Prediction of Depression Treatment Outcome Using Location Sensory Data on Smartphones",
    "abstract": "           Currently, depression treatment relies on closely monitoring patients response to treatment and adjusting the treatment as needed. Using self-reported or physician-administrated questionnaires to monitor treatment response is, however, burdensome, costly and suffers from recall bias. In this paper, we explore using location sensory data collected passively on smartphones to predict treatment outcome. To address heterogeneous data collection on Android and iOS phones, the two predominant smartphone platforms, we explore using domain adaptation techniques to map their data to a common feature space, and then use the data jointly to train machine learning models. Our results show that this domain adaptation approach can lead to significantly better prediction than that with no domain adaptation. In addition, our results show that using location features and baseline self-reported questionnaire score can lead to F1 score up to 0.67, comparable to that obtained using periodic self-reported questionnaires, indicating that using location data is a promising direction for predicting depression treatment outcome.         ",
    "url": "https://arxiv.org/abs/2503.07883",
    "authors": [
      "Soumyashree Sahoo",
      "Chinmaey Shende",
      "Md. Zakir Hossain",
      "Parit Patel",
      "Yushuo Niu",
      "Xinyu Wang",
      "Shweta Ware",
      "Jinbo Bi",
      "Jayesh Kamath",
      "Alexander Russel",
      "Dongjin Song",
      "Qian Yang",
      "Bing Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.07892",
    "title": "\"We're losing our neighborhoods. We're losing our community\": A comparative analysis of community discourse in online and offline public spheres",
    "abstract": "           Recovering from crises, such as hurricanes or wildfires, is a complex process that can take weeks, months, or even decades to overcome. Crises have both acute (immediate) and chronic (long-term) effects on communities. Crisis informatics research often focuses on the immediate response phase of disasters, thereby overlooking the long-term recovery phase, which is critical for understanding the information needs of users undergoing challenges like climate gentrification and housing inequity. We fill this gap by investigating community discourse over eight months following Hurricane Ida in an online neighborhood Facebook group and Town Hall Meetings of a borough in the New York Metropolitan region. Using a mixed methods approach, we examined the use of social media to manage long-term disaster recovery. The findings revealed a significant overlap in topics, underscoring the interconnected nature of online and offline community discourse, and illuminated themes related to the long-term consequences of disasters. We conclude with recommendations aimed at helping designers and government leaders enhance participation across community forums and support recovery in the aftermath of disasters.         ",
    "url": "https://arxiv.org/abs/2503.07892",
    "authors": [
      "Casey Randazzo",
      "Minkyung Kim",
      "Melanie Kwestel",
      "Marya L Doerfel",
      "Tawfiq Ammari"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.07902",
    "title": "LTLCodeGen: Code Generation of Syntactically Correct Temporal Logic for Robot Task Planning",
    "abstract": "           This paper focuses on planning robot navigation tasks from natural language specifications. We develop a modular approach, where a large language model (LLM) translates the natural language instructions into a linear temporal logic (LTL) formula with propositions defined by object classes in a semantic occupancy map. The LTL formula and the semantic occupancy map are provided to a motion planning algorithm to generate a collision-free robot path that satisfies the natural language instructions. Our main contribution is LTLCodeGen, a method to translate natural language to syntactically correct LTL using code generation. We demonstrate the complete task planning method in real-world experiments involving human speech to provide navigation instructions to a mobile robot. We also thoroughly evaluate our approach in simulated and real-world experiments in comparison to end-to-end LLM task planning and state-of-the-art LLM-to-LTL translation methods.         ",
    "url": "https://arxiv.org/abs/2503.07902",
    "authors": [
      "Behrad Rabiei",
      "Mahesh Kumar A.R.",
      "Zhirui Dai",
      "Surya L.S.R. Pilla",
      "Qiyue Dong",
      "Nikolay Atanasov"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.07909",
    "title": "FunGraph: Functionality Aware 3D Scene Graphs for Language-Prompted Scene Interaction",
    "abstract": "           The concept of 3D scene graphs is increasingly recognized as a powerful semantic and hierarchical representation of the environment. Current approaches often address this at a coarse, object-level resolution. In contrast, our goal is to develop a representation that enables robots to directly interact with their environment by identifying both the location of functional interactive elements and how these can be used. To achieve this, we focus on detecting and storing objects at a finer resolution, focusing on affordance-relevant parts. The primary challenge lies in the scarcity of data that extends beyond instance-level detection and the inherent difficulty of capturing detailed object features using robotic sensors. We leverage currently available 3D resources to generate 2D data and train a detector, which is then used to augment the standard 3D scene graph generation pipeline. Through our experiments, we demonstrate that our approach achieves functional element segmentation comparable to state-of-the-art 3D models and that our augmentation enables task-driven affordance grounding with higher accuracy than the current solutions.         ",
    "url": "https://arxiv.org/abs/2503.07909",
    "authors": [
      "Dennis Rotondi",
      "Fabio Scaparro",
      "Hermann Blum",
      "Kai O. Arras"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.07933",
    "title": "From Slices to Sequences: Autoregressive Tracking Transformer for Cohesive and Consistent 3D Lymph Node Detection in CT Scans",
    "abstract": "           Lymph node (LN) assessment is an essential task in the routine radiology workflow, providing valuable insights for cancer staging, treatment planning and beyond. Identifying scatteredly-distributed and low-contrast LNs in 3D CT scans is highly challenging, even for experienced clinicians. Previous lesion and LN detection methods demonstrate effectiveness of 2.5D approaches (i.e, using 2D network with multi-slice inputs), leveraging pretrained 2D model weights and showing improved accuracy as compared to separate 2D or 3D detectors. However, slice-based 2.5D detectors do not explicitly model inter-slice consistency for LN as a 3D object, requiring heuristic post-merging steps to generate final 3D LN instances, which can involve tuning a set of parameters for each dataset. In this work, we formulate 3D LN detection as a tracking task and propose LN-Tracker, a novel LN tracking transformer, for joint end-to-end detection and 3D instance association. Built upon DETR-based detector, LN-Tracker decouples transformer decoder's query into the track and detection groups, where the track query autoregressively follows previously tracked LN instances along the z-axis of a CT scan. We design a new transformer decoder with masked attention module to align track query's content to the context of current slice, meanwhile preserving detection query's high accuracy in current slice. An inter-slice similarity loss is introduced to encourage cohesive LN association between slices. Extensive evaluation on four lymph node datasets shows LN-Tracker's superior performance, with at least 2.7% gain in average sensitivity when compared to other top 3D/2.5D detectors. Further validation on public lung nodule and prostate tumor detection tasks confirms the generalizability of LN-Tracker as it achieves top performance on both tasks. Datasets will be released upon acceptance.         ",
    "url": "https://arxiv.org/abs/2503.07933",
    "authors": [
      "Qinji Yu",
      "Yirui Wang",
      "Ke Yan",
      "Dandan Zheng",
      "Dashan Ai",
      "Dazhou Guo",
      "Zhanghexuan Ji",
      "Yanzhou Su",
      "Yun Bian",
      "Na Shen",
      "Xiaowei Ding",
      "Le Lu",
      "Xianghua Ye",
      "Dakai Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.07935",
    "title": "Accelerating Development in UAV Network Digital Twins with a Flexible Simulation Framework",
    "abstract": "           Unmanned aerial vehicles (UAVs) enhance coverage and provide flexible deployment in 5G and next-generation wireless networks. The performance of such wireless networks can be improved by developing new navigation and wireless adaptation approaches in digital twins (DTs). However, challenges such as complex propagation conditions and hardware complexities in real-world scenarios introduce a realism gap with the DTs. Moreover, while using real-time full-stack protocols in DTs enables subsequent deployment and testing in a real-world environment, development in DTs requires high computational complexity and involves a long development time. In this paper, to accelerate the development cycle, we develop a measurement-calibrated Matlab-based simulation framework to replicate performance in a full-stack UAV wireless network DT. In particular, we use the DT from the NSF AERPAW platform and compare its reports with those generated by our developed simulation framework in wireless networks with similar settings. In both environments, we observe comparable results in terms of RSRP measurement, hence motivating iterative use of the developed simulation environment with the DT.         ",
    "url": "https://arxiv.org/abs/2503.07935",
    "authors": [
      "Md Sharif Hossen",
      "Anil Gurses",
      "Mihail Sichitiu",
      "Ismail Guvenc"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.07942",
    "title": "STEAD: Spatio-Temporal Efficient Anomaly Detection for Time and Compute Sensitive Applications",
    "abstract": "           This paper presents a new method for anomaly detection in automated systems with time and compute sensitive requirements, such as autonomous driving, with unparalleled efficiency. As systems like autonomous driving become increasingly popular, ensuring their safety has become more important than ever. Therefore, this paper focuses on how to quickly and effectively detect various anomalies in the aforementioned systems, with the goal of making them safer and more effective. Many detection systems have been developed with great success under spatial contexts; however, there is still significant room for improvement when it comes to temporal context. While there is substantial work regarding this task, there is minimal work done regarding the efficiency of models and their ability to be applied to scenarios that require real-time inference, i.e., autonomous driving where anomalies need to be detected the moment they are within view. To address this gap, we propose STEAD (Spatio-Temporal Efficient Anomaly Detection), whose backbone is developed using (2+1)D Convolutions and Performer Linear Attention, which ensures computational efficiency without sacrificing performance. When tested on the UCF-Crime benchmark, our base model achieves an AUC of 91.34%, outperforming the previous state-of-the-art, and our fast version achieves an AUC of 88.87%, while having 99.70% less parameters and outperforming the previous state-of-the-art as well. The code and pretrained models are made publicly available at this https URL ",
    "url": "https://arxiv.org/abs/2503.07942",
    "authors": [
      "Andrew Gao",
      "Jun Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.07952",
    "title": "NeRF-VIO: Map-Based Visual-Inertial Odometry with Initialization Leveraging Neural Radiance Fields",
    "abstract": "           A prior map serves as a foundational reference for localization in context-aware applications such as augmented reality (AR). Providing valuable contextual information about the environment, the prior map is a vital tool for mitigating drift. In this paper, we propose a map-based visual-inertial localization algorithm (NeRF-VIO) with initialization using neural radiance fields (NeRF). Our algorithm utilizes a multilayer perceptron model and redefines the loss function as the geodesic distance on \\(SE(3)\\), ensuring the invariance of the initialization model under a frame change within \\(\\mathfrak{se}(3)\\). The evaluation demonstrates that our model outperforms existing NeRF-based initialization solution in both accuracy and efficiency. By integrating a two-stage update mechanism within a multi-state constraint Kalman filter (MSCKF) framework, the state of NeRF-VIO is constrained by both captured images from an onboard camera and rendered images from a pre-trained NeRF model. The proposed algorithm is validated using a real-world AR dataset, the results indicate that our two-stage update pipeline outperforms MSCKF across all data sequences.         ",
    "url": "https://arxiv.org/abs/2503.07952",
    "authors": [
      "Yanyu Zhang",
      "Dongming Wang",
      "Jie Xu",
      "Mengyuan Liu",
      "Pengxiang Zhu",
      "Wei Ren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.07958",
    "title": "Pre-trained Models Succeed in Medical Imaging with Representation Similarity Degradation",
    "abstract": "           This paper investigates the critical problem of representation similarity evolution during cross-domain transfer learning, with particular focus on understanding why pre-trained models maintain effectiveness when adapted to medical imaging tasks despite significant domain gaps. The study establishes a rigorous problem definition centered on quantifying and analyzing representation similarity trajectories throughout the fine-tuning process, while carefully delineating the scope to encompass both medical image analysis and broader cross-domain adaptation scenarios. Our empirical findings reveal three critical discoveries: the potential existence of high-performance models that preserve both task accuracy and representation similarity to their pre-trained origins; a robust linear correlation between layer-wise similarity metrics and representation quality indicators; and distinct adaptation patterns that differentiate supervised versus self-supervised pre-training paradigms. The proposed similarity space framework not only provides mechanistic insights into knowledge transfer dynamics but also raises fundamental questions about optimal utilization of pre-trained models. These results advance our understanding of neural network adaptation processes while offering practical implications for transfer learning strategies that extend beyond medical imaging applications. The code will be available once accepted.         ",
    "url": "https://arxiv.org/abs/2503.07958",
    "authors": [
      "Wenqiang Zu",
      "Shenghao Xie",
      "Hao Chen",
      "Lei Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.07959",
    "title": "Recent Advances in Hypergraph Neural Networks",
    "abstract": "           The growing interest in hypergraph neural networks (HGNNs) is driven by their capacity to capture the complex relationships and patterns within hypergraph structured data across various domains, including computer vision, complex networks, and natural language processing. This paper comprehensively reviews recent advances in HGNNs and presents a taxonomy of mainstream models based on their architectures: hypergraph convolutional networks (HGCNs), hypergraph attention networks (HGATs), hypergraph autoencoders (HGAEs), hypergraph recurrent networks (HGRNs), and deep hypergraph generative models (DHGGMs). For each category, we delve into its practical applications, mathematical mechanisms, literature contributions, and open problems. Finally, we discuss some common challenges and promising research this http URL paper aspires to be a helpful resource that provides guidance for future research and applications of HGNNs.         ",
    "url": "https://arxiv.org/abs/2503.07959",
    "authors": [
      "Murong Yang",
      "Xin-Jian Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.07961",
    "title": "Overlap-aware meta-learning attention to enhance hypergraph neural networks for node classification",
    "abstract": "           Although hypergraph neural networks (HGNNs) have emerged as a powerful framework for analyzing complex datasets, their practical performance often remains limited. On one hand, existing networks typically employ a single type of attention mechanism, focusing on either structural or feature similarities during message passing. On the other hand, assuming that all nodes in current hypergraph models have the same level of overlap may lead to suboptimal generalization. To overcome these limitations, we propose a novel framework, overlap-aware meta-learning attention for hypergraph neural networks (OMA-HGNN). First, we introduce a hypergraph attention mechanism that integrates both structural and feature similarities. Specifically, we linearly combine their respective losses with weighted factors for the HGNN model. Second, we partition nodes into different tasks based on their diverse overlap levels and develop a multi-task Meta-Weight-Net (MWN) to determine the corresponding weighted factors. Third, we jointly train the internal MWN model with the losses from the external HGNN model and train the external model with the weighted factors from the internal model. To evaluate the effectiveness of OMA-HGNN, we conducted experiments on six real-world datasets and benchmarked its perfor-mance against nine state-of-the-art methods for node classification. The results demonstrate that OMA-HGNN excels in learning superior node representations and outperforms these baselines.         ",
    "url": "https://arxiv.org/abs/2503.07961",
    "authors": [
      "Murong Yang",
      "Shihui Ying",
      "Xin-Jian Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.07967",
    "title": "Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Maintenance",
    "abstract": "           While large language models (LLMs) have demonstrated promise in software engineering tasks like code completion and generation, their support for the maintenance of complex software systems remains limited. These models often struggle with understanding the tacit knowledge embedded in systems, such as responsibility allocation and collaboration across different modules. To address this gap, we introduce the concept and framework of \\textbf{Code Digital Twin}, a conceptual representation of tacit knowledge that captures the concepts, functionalities, and design rationales behind code elements, co-evolving with the software. A code digital twin is constructed using a methodology that combines knowledge extraction from both structured and unstructured sources--such as source code, documentation, and change histories--leveraging LLMs, static analysis tools, and human expertise. This framework can empower LLMs for software maintenance tasks such as issue localization and repository-level code generation by providing tacit knowledge as contexts. Based on the proposed methodology, we explore the key challenges and opportunities involved in the continuous construction and refinement of code digital twin.         ",
    "url": "https://arxiv.org/abs/2503.07967",
    "authors": [
      "Xin Peng",
      "Chong Wang",
      "Mingwei Liu",
      "Yiling Lou",
      "Yijian Wu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.07973",
    "title": "ALCS: An Adaptive Latency Compensation Scheduler for Multipath TCP in Satellite-Terrestrial Integrated Networks",
    "abstract": "           The Satellite-Terrestrial Integrated Network (STIN) enhances end-to-end transmission by simultaneously utilizing terrestrial and satellite networks, offering significant benefits in scenarios like emergency response and cross-continental communication. Low Earth Orbit (LEO) satellite networks offer reduced Round Trip Time (RTT) for long-distance data transmission and serve as a crucial backup during terrestrial network failures. Meanwhile, terrestrial networks are characterized by ample bandwidth resources and generally more stable link conditions. Therefore, integrating Multipath TCP (MPTCP) into STIN is vital for optimizing resource utilization and ensuring efficient data transfer by exploiting the complementary strengths of both networks. However, the inherent challenges of STIN, such as heterogeneity, instability, and handovers, pose difficulties for traditional multipath schedulers, which are typically designed for terrestrial networks. We propose a novel multipath data scheduling approach for STIN, Adaptive Latency Compensation Scheduler (ALCS), to address these issues. ALCS refines transmission latency estimates by incorporating RTT, congestion window size, inflight and queuing packets, and satellite trajectory information. It further employs adaptive mechanisms for latency compensation and proactive handover management. It further employs adaptive mechanisms for latency compensation and proactive handover management. Implemented in the MPTCP Linux Kernel and evaluated in a simulated STIN testbed, ALCS outperforms existing multipath schedulers, delivering faster data transmission and achieving throughput gains of 9.8% to 44.0% compared to benchmark algorithms.         ",
    "url": "https://arxiv.org/abs/2503.07973",
    "authors": [
      "Lin Wang",
      "Ze Wang",
      "Zeyi Deng",
      "Jingjing Zhang",
      "Yue Gao"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.07977",
    "title": "Boundary Regression for Leitmotif Detection in Music Audio",
    "abstract": "           Leitmotifs are musical phrases that are reprised in various forms throughout a piece. Due to diverse variations and instrumentation, detecting the occurrence of leitmotifs from audio recordings is a highly challenging task. Leitmotif detection may be handled as a subcategory of audio event detection, where leitmotif activity is predicted at the frame level. However, as leitmotifs embody distinct, coherent musical structures, a more holistic approach akin to bounding box regression in visual object detection can be helpful. This method captures the entirety of a motif rather than fragmenting it into individual frames, thereby preserving its musical integrity and producing more useful predictions. We present our experimental results on tackling leitmotif detection as a boundary regression task.         ",
    "url": "https://arxiv.org/abs/2503.07977",
    "authors": [
      "Sihun Lee",
      "Dasaem Jeong"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2503.07978",
    "title": "Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection",
    "abstract": "           The distributed nature of training makes Federated Learning (FL) vulnerable to backdoor attacks, where malicious model updates aim to compromise the global model's performance on specific tasks. Existing defense methods show limited efficacy as they overlook the inconsistency between benign and malicious model updates regarding both general and fine-grained directions. To fill this gap, we introduce AlignIns, a novel defense method designed to safeguard FL systems against backdoor attacks. AlignIns looks into the direction of each model update through a direction alignment inspection process. Specifically, it examines the alignment of model updates with the overall update direction and analyzes the distribution of the signs of their significant parameters, comparing them with the principle sign across all model updates. Model updates that exhibit an unusual degree of alignment are considered malicious and thus be filtered out. We provide the theoretical analysis of the robustness of AlignIns and its propagation error in FL. Our empirical results on both independent and identically distributed (IID) and non-IID datasets demonstrate that AlignIns achieves higher robustness compared to the state-of-the-art defense methods. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.07978",
    "authors": [
      "Jiahao Xu",
      "Zikai Zhang",
      "Rui Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2503.07989",
    "title": "Bio-Skin: A Cost-Effective Thermostatic Tactile Sensor with Multi-Modal Force and Temperature Detection",
    "abstract": "           Tactile sensors can significantly enhance the perception of humanoid robotics systems by providing contact information that facilitates human-like interactions. However, existing commercial tactile sensors focus on improving the resolution and sensitivity of single-modal detection with high-cost components and densely integrated design, incurring complex manufacturing processes and unaffordable prices. In this work, we present Bio-Skin, a cost-effective multi-modal tactile sensor that utilizes single-axis Hall-effect sensors for planar normal force measurement and bar-shape piezo resistors for 2D shear force measurement. A thermistor coupling with a heating wire is integrated into a silicone body to achieve temperature sensation and thermostatic function analogous to human skin. We also present a cross-reference framework to validate the two modalities of the force sensing signal, improving the sensing fidelity in a complex electromagnetic environment. Bio-Skin has a multi-layer design, and each layer is manufactured sequentially and subsequently integrated, thereby offering a fast production pathway. After calibration, Bio-Skin demonstrates performance metrics-including signal-to-range ratio, sampling rate, and measurement range-comparable to current commercial products, with one-tenth of the cost. The sensor's real-world performance is evaluated using an Allegro hand in object grasping tasks, while its temperature regulation functionality was assessed in a material detection task.         ",
    "url": "https://arxiv.org/abs/2503.07989",
    "authors": [
      "Haoran Guo",
      "Haoyang Wang",
      "Zhengxiong Li",
      "Lingfeng Tao"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.07991",
    "title": "Boundary Prompting: Elastic Urban Region Representation via Graph-based Spatial Tokenization",
    "abstract": "           Urban region representation is essential for various applications such as urban planning, resource allocation, and policy development. Traditional methods rely on fixed, predefined region boundaries, which fail to capture the dynamic and complex nature of real-world urban areas. In this paper, we propose the Boundary Prompting Urban Region Representation Framework (BPURF), a novel approach that allows for elastic urban region definitions. BPURF comprises two key components: (1) A spatial token dictionary, where urban entities are treated as tokens and integrated into a unified token graph, and (2) a region token set representation model which utilize token aggregation and a multi-channel model to embed token sets corresponding to region boundaries. Additionally, we propose fast token set extraction strategy to enable online token set extraction during training and prompting. This framework enables the definition of urban regions through boundary prompting, supporting varying region boundaries and adapting to different tasks. Extensive experiments demonstrate the effectiveness of BPURF in capturing the complex characteristics of urban regions.         ",
    "url": "https://arxiv.org/abs/2503.07991",
    "authors": [
      "Haojia Zhu",
      "Jiahui Jin",
      "Dong Kan",
      "Rouxi Shen",
      "Ruize Wang",
      "Xiangguo Sun",
      "Jinghui Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.07993",
    "title": "LLM-Powered Knowledge Graphs for Enterprise Intelligence and Analytics",
    "abstract": "           Disconnected data silos within enterprises obstruct the extraction of actionable insights, diminishing efficiency in areas such as product development, client engagement, meeting preparation, and analytics-driven decision-making. This paper introduces a framework that uses large language models (LLMs) to unify various data sources into a comprehensive, activity-centric knowledge graph. The framework automates tasks such as entity extraction, relationship inference, and semantic enrichment, enabling advanced querying, reasoning, and analytics across data types like emails, calendars, chats, documents, and logs. Designed for enterprise flexibility, it supports applications such as contextual search, task prioritization, expertise discovery, personalized recommendations, and advanced analytics to identify trends and actionable insights. Experimental results demonstrate its success in the discovery of expertise, task management, and data-driven decision making. By integrating LLMs with knowledge graphs, this solution bridges disconnected systems and delivers intelligent analytics-powered enterprise tools.         ",
    "url": "https://arxiv.org/abs/2503.07993",
    "authors": [
      "Rajeev Kumar",
      "Kumar Ishan",
      "Harishankar Kumar",
      "Abhinandan Singla"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08009",
    "title": "Integrated Energy Management for Operational Cost Optimization in Community Microgrids",
    "abstract": "           This study presents an integrated energy management strategy for cost optimization in multi-energy community microgrids (MGs). The proposed approach combines storage-based peak shaving, economic dispatch of diesel generators, and efficient utilization of renewable energy sources to enhance energy management in community MGs. The efficacy of the energy management system (EMS) was validated through a simulation case study for a rural Australian community. The results demonstrate that the proposed EMS effectively reduces the peak energy demand by up to 43%, lowers operational costs by 84.63% (from $189,939/year to $29,188/year), and achieves a renewable energy utilization of 92.3%, up from 47.8% in the base system. Furthermore, the levelized cost of energy was reduced by 14.21% to $0.163/kWh. The strategy ensures an uninterrupted power supply during grid outages by utilizing DGs and battery energy storage systems. The environmental benefits included a 196.4% reduction in CO2 emissions and 100% reductions in CO, unburned hydrocarbons, and particulate matter. These findings validate the feasibility of the proposed EMS in achieving cost-effective, reliable, and sustainable energy management in community MGs. These findings contribute to the field by introducing a novel approach and demonstrating the practical feasibility of multi-energy MGs.         ",
    "url": "https://arxiv.org/abs/2503.08009",
    "authors": [
      "Moslem Uddin",
      "Huadong Mo",
      "Daoyi Dong"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.08016",
    "title": "SGNetPose+: Stepwise Goal-Driven Networks with Pose Information for Trajectory Prediction in Autonomous Driving",
    "abstract": "           Predicting pedestrian trajectories is essential for autonomous driving systems, as it significantly enhances safety and supports informed decision-making. Accurate predictions enable the prevention of collisions, anticipation of crossing intent, and improved overall system efficiency. In this study, we present SGNetPose+, an enhancement of the SGNet architecture designed to integrate skeleton information or body segment angles with bounding boxes to predict pedestrian trajectories from video data to avoid hazards in autonomous driving. Skeleton information was extracted using a pose estimation model, and joint angles were computed based on the extracted joint data. We also apply temporal data augmentation by horizontally flipping video frames to increase the dataset size and improve performance. Our approach achieves state-of-the-art results on the JAAD and PIE datasets using pose data with the bounding boxes, outperforming the SGNet model. Code is available on Github: SGNetPose+.         ",
    "url": "https://arxiv.org/abs/2503.08016",
    "authors": [
      "Akshat Ghiya",
      "Ali K. AlShami",
      "Jugal Kalita"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08023",
    "title": "AdaSCALE: Adaptive Scaling for OOD Detection",
    "abstract": "           The ability of the deep learning model to recognize when a sample falls outside its learned distribution is critical for safe and reliable deployment. Recent state-of-the-art out-of-distribution (OOD) detection methods leverage activation shaping to improve the separation between in-distribution (ID) and OOD inputs. These approaches resort to sample-specific scaling but apply a static percentile threshold across all samples regardless of their nature, resulting in suboptimal ID-OOD separability. In this work, we propose \\textbf{AdaSCALE}, an adaptive scaling procedure that dynamically adjusts the percentile threshold based on a sample's estimated OOD likelihood. This estimation leverages our key observation: OOD samples exhibit significantly more pronounced activation shifts at high-magnitude activations under minor perturbation compared to ID samples. AdaSCALE enables stronger scaling for likely ID samples and weaker scaling for likely OOD samples, yielding highly separable energy scores. Our approach achieves state-of-the-art OOD detection performance, outperforming the latest rival OptFS by 14.94 in near-OOD and 21.67 in far-OOD datasets in average FPR@95 metric on the ImageNet-1k benchmark across eight diverse architectures. The code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2503.08023",
    "authors": [
      "Sudarshan Regmi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08029",
    "title": "Elastic Motion Policy: An Adaptive Dynamical System for Robust and Efficient One-Shot Imitation Learning",
    "abstract": "           Behavior cloning (BC) has become a staple imitation learning paradigm in robotics due to its ease of teaching robots complex skills directly from expert demonstrations. However, BC suffers from an inherent generalization issue. To solve this, the status quo solution is to gather more data. Yet, regardless of how much training data is available, out-of-distribution performance is still sub-par, lacks any formal guarantee of convergence and success, and is incapable of allowing and recovering from physical interactions with humans. These are critical flaws when robots are deployed in ever-changing human-centric environments. Thus, we propose Elastic Motion Policy (EMP), a one-shot imitation learning framework that allows robots to adjust their behavior based on the scene change while respecting the task specification. Trained from a single demonstration, EMP follows the dynamical systems paradigm where motion planning and control are governed by first-order differential equations with convergence guarantees. We leverage Laplacian editing in full end-effector space, $\\mathbb{R}^3\\times SO(3)$, and online convex learning of Lyapunov functions, to adapt EMP online to new contexts, avoiding the need to collect new demonstrations. We extensively validate our framework in real robot experiments, demonstrating its robust and efficient performance in dynamic environments, with obstacle avoidance and multi-step task capabilities. Project Website: this https URL ",
    "url": "https://arxiv.org/abs/2503.08029",
    "authors": [
      "Tianyu Li",
      "Sunan Sun",
      "Shubhodeep Shiv Aditya",
      "Nadia Figueroa"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.08031",
    "title": "Empirical Error Estimates for Graph Sparsification",
    "abstract": "           Graph sparsification is a well-established technique for accelerating graph-based learning algorithms, which uses edge sampling to approximate dense graphs with sparse ones. Because the sparsification error is random and unknown, users must contend with uncertainty about the reliability of downstream computations. Although it is possible for users to obtain conceptual guidance from theoretical error bounds in the literature, such results are typically impractical at a numerical level. Taking an alternative approach, we propose to address these issues from a data-driven perspective by computing empirical error estimates. The proposed error estimates are highly versatile, and we demonstrate this in four use cases: Laplacian matrix approximation, graph cut queries, graph-structured regression, and spectral clustering. Moreover, we provide two theoretical guarantees for the error estimates, and explain why the cost of computing them is manageable in comparison to the overall cost of a typical graph sparsification workflow.         ",
    "url": "https://arxiv.org/abs/2503.08031",
    "authors": [
      "Siyao Wang",
      "Miles E. Lopes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.08032",
    "title": "HOFAR: High-Order Augmentation of Flow Autoregressive Transformers",
    "abstract": "           Flow Matching and Transformer architectures have demonstrated remarkable performance in image generation tasks, with recent work FlowAR [Ren et al., 2024] synergistically integrating both paradigms to advance synthesis fidelity. However, current FlowAR implementations remain constrained by first-order trajectory modeling during the generation process. This paper introduces a novel framework that systematically enhances flow autoregressive transformers through high-order supervision. We provide theoretical analysis and empirical evaluation showing that our High-Order FlowAR (HOFAR) demonstrates measurable improvements in generation quality compared to baseline models. The proposed approach advances the understanding of flow-based autoregressive modeling by introducing a systematic framework for analyzing trajectory dynamics through high-order expansion.         ",
    "url": "https://arxiv.org/abs/2503.08032",
    "authors": [
      "Yingyu Liang",
      "Zhizhou Sha",
      "Zhenmei Shi",
      "Zhao Song",
      "Mingda Wan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08045",
    "title": "Adapting Large Language Models for Parameter-Efficient Log Anomaly Detection",
    "abstract": "           Log Anomaly Detection (LAD) seeks to identify atypical patterns in log data that are crucial to assessing the security and condition of systems. Although Large Language Models (LLMs) have shown tremendous success in various fields, the use of LLMs in enabling the detection of log anomalies is largely unexplored. This work aims to fill this gap. Due to the prohibitive costs involved in fully fine-tuning LLMs, we explore the use of parameter-efficient fine-tuning techniques (PEFTs) for adapting LLMs to LAD. To have an in-depth exploration of the potential of LLM-driven LAD, we present a comprehensive investigation of leveraging two of the most popular PEFTs -- Low-Rank Adaptation (LoRA) and Representation Fine-tuning (ReFT) -- to tap into three prominent LLMs of varying size, including RoBERTa, GPT-2, and Llama-3, for parameter-efficient LAD. Comprehensive experiments on four public log datasets are performed to reveal important insights into effective LLM-driven LAD in several key perspectives, including the efficacy of these PEFT-based LLM-driven LAD methods, their stability, sample efficiency, robustness w.r.t. unstable logs, and cross-dataset generalization. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.08045",
    "authors": [
      "Ying Fu Lim",
      "Jiawen Zhu",
      "Guansong Pang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.08049",
    "title": "SphOR: A Representation Learning Perspective on Open-set Recognition for Identifying Unknown Classes in Deep Learning Models",
    "abstract": "           The widespread use of deep learning classifiers necessitates Open-set recognition (OSR), which enables the identification of input data not only from classes known during training but also from unknown classes that might be present in test data. Many existing OSR methods are computationally expensive due to the reliance on complex generative models or suffer from high training costs. We investigate OSR from a representation-learning perspective, specifically through spherical embeddings. We introduce SphOR, a computationally efficient representation learning method that models the feature space as a mixture of von Mises-Fisher distributions. This approach enables the use of semantically ambiguous samples during training, to improve the detection of samples from unknown classes. We further explore the relationship between OSR performance and key representation learning properties which influence how well features are structured in high-dimensional space. Extensive experiments on multiple OSR benchmarks demonstrate the effectiveness of our method, producing state-of-the-art results, with improvements up-to 6% that validate its performance.         ",
    "url": "https://arxiv.org/abs/2503.08049",
    "authors": [
      "Nadarasar Bahavan",
      "Sachith Seneviratne",
      "Saman Halgamuge"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08055",
    "title": "Unmasking the Unknown: Facial Deepfake Detection in the Open-Set Paradigm",
    "abstract": "           Facial forgery methods such as deepfakes can be misused for identity manipulation and spreading misinformation. They have evolved alongside advancements in generative AI, leading to new and more sophisticated forgery techniques that diverge from existing 'known' methods. Conventional deepfake detection methods use the closedset paradigm, thus limiting their applicability to detecting forgeries created using methods that are not part of the training dataset. In this paper, we propose a shift from the closed-set paradigm for deepfake detection. In the open-set paradigm, models are designed not only to identify images created by known facial forgery methods but also to identify and flag those produced by previously unknown methods as 'unknown' and not as unforged/real/unmanipulated. In this paper, we propose an open-set deepfake classification algorithm based on supervised contrastive learning. The open-set paradigm used in our model allows it to function as a more robust tool capable of handling emerging and unseen deepfake techniques, enhancing reliability and confidence, and complementing forensic analysis. In open-set paradigm, we identify three groups including the \"unknown group that is neither considered known deepfake nor real. We investigate deepfake open-set classification across three scenarios, classifying deepfakes from unknown methods not as real, distinguishing real images from deepfakes, and classifying deepfakes from known methods, using the FaceForensics++ dataset as a benchmark. Our method achieves state of the art results in the first two tasks and competitive results in the third task.         ",
    "url": "https://arxiv.org/abs/2503.08055",
    "authors": [
      "Nadarasar Bahavan",
      "Sanjay Saha",
      "Ken Chen",
      "Sachith Seneviratne",
      "Sanka Rasnayaka",
      "Saman Halgamuge"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08056",
    "title": "DDO-IN: Dual Domains Optimization for Implicit Neural Network to Eliminate Motion Artifact in Magnetic Resonance Imaging",
    "abstract": "           Magnetic resonance imaging (MRI) motion artifacts can seriously affect clinical diagnostics, making it challenging to interpret images accurately. Existing methods for eliminating motion artifacts struggle to retain fine structural details and simultaneously lack the necessary vividness and sharpness. In this study, we present a novel dual-domain optimization (DDO) approach that integrates information from the pixel and frequency domains guiding the recovery of clean magnetic resonance images through implicit neural representations(INRs). Specifically, our approach leverages the low-frequency components in the k-space as a reference to capture accurate tissue textures, while high-frequency and pixel information contribute to recover details. Furthermore, we design complementary masks and dynamic loss weighting transitioning from global to local attention that effectively suppress artifacts while retaining useful details for reconstruction. Experimental results on the NYU fastMRI dataset demonstrate that our method outperforms existing approaches in multiple evaluation metrics. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.08056",
    "authors": [
      "Zhongyu Mai",
      "Zewei Zhan",
      "Hanyu Guo",
      "Yulang Huang",
      "Weifeng Su"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08059",
    "title": "Symbolic Neural Ordinary Differential Equations",
    "abstract": "           Differential equations are widely used to describe complex dynamical systems with evolving parameters in nature and engineering. Effectively learning a family of maps from the parameter function to the system dynamics is of great significance. In this study, we propose a novel learning framework of symbolic continuous-depth neural networks, termed Symbolic Neural Ordinary Differential Equations (SNODEs), to effectively and accurately learn the underlying dynamics of complex systems. Specifically, our learning framework comprises three stages: initially, pre-training a predefined symbolic neural network via a gradient flow matching strategy; subsequently, fine-tuning this network using Neural ODEs; and finally, constructing a general neural network to capture residuals. In this process, we apply the SNODEs framework to partial differential equation systems through Fourier analysis, achieving resolution-invariant modeling. Moreover, this framework integrates the strengths of symbolism and connectionism, boasting a universal approximation theorem while significantly enhancing interpretability and extrapolation capabilities relative to state-of-the-art baseline methods. We demonstrate this through experiments on several representative complex systems. Therefore, our framework can be further applied to a wide range of scientific problems, such as system bifurcation and control, reconstruction and forecasting, as well as the discovery of new equations.         ",
    "url": "https://arxiv.org/abs/2503.08059",
    "authors": [
      "Xin Li",
      "Chengli Zhao",
      "Xue Zhang",
      "Xiaojun Duan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Mathematical Physics (math-ph)"
    ]
  },
  {
    "id": "arXiv:2503.08065",
    "title": "STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion Probabilistic Model",
    "abstract": "           Vessel trajectory prediction is a critical component for ensuring maritime traffic safety and avoiding collisions. Due to the inherent uncertainty in vessel behavior, trajectory prediction systems must adopt a multimodal approach to accurately model potential future motion states. However, existing vessel trajectory prediction methods lack the ability to comprehensively model behavioral multi-modality. To better capture multimodal behavior in interactive scenarios, we propose modeling interactions as dynamic graphs, replacing traditional aggregation-based techniques that rely on vessel states. By leveraging the natural multimodal capabilities of diffusion models, we frame the trajectory prediction task as an inverse process of motion uncertainty diffusion, wherein uncertainties across potential navigational areas are progressively eliminated until the desired trajectories is produced. In summary, we pioneer the integration of Spatio-Temporal Graph (STG) with diffusion models in ship trajectory prediction. Extensive experiments on real Automatic Identification System (AIS) data validate the superiority of our approach.         ",
    "url": "https://arxiv.org/abs/2503.08065",
    "authors": [
      "Jin Wenzhe",
      "Tang Haina",
      "Zhang Xudong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08066",
    "title": "From Data to Global Asymptotic Stability of Unknown Large-Scale Networks with Provable Guarantees",
    "abstract": "           We offer a compositional data-driven scheme for synthesizing controllers that ensure global asymptotic stability (GAS) across large-scale interconnected networks, characterized by unknown mathematical models. In light of each network's configuration composed of numerous subsystems with smaller dimensions, our proposed framework gathers data from each subsystem's trajectory, enabling the design of local controllers that ensure input-to-state stability (ISS) properties over subsystems, signified by ISS Lyapunov functions. To accomplish this, we require only a single input-state trajectory from each unknown subsystem up to a specified time horizon, fulfilling certain rank conditions. Subsequently, under small-gain compositional reasoning, we leverage ISS Lyapunov functions derived from data to offer a control Lyapunov function (CLF) for the interconnected network, ensuring GAS certificate over the network. We demonstrate that while the computational complexity for designing a CLF increases polynomially with the network dimension using sum-of-squares (SOS) optimization, our compositional data-driven approach significantly mitigates it to \\emph{linear} with respect to the number of subsystems. We showcase the efficacy of our data-driven approach over a set of benchmarks, involving physical networks with diverse interconnection topologies.         ",
    "url": "https://arxiv.org/abs/2503.08066",
    "authors": [
      "Mahdieh Zaker",
      "Amy Nejati",
      "Abolfazl Lavaei"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.08075",
    "title": "MuCoS: Efficient Drug Target Discovery via Multi Context Aware Sampling in Knowledge Graphs",
    "abstract": "           Accurate prediction of drug target interactions is critical for accelerating drug discovery and elucidating complex biological mechanisms. In this work, we frame drug target prediction as a link prediction task on heterogeneous biomedical knowledge graphs (KG) that integrate drugs, proteins, diseases, pathways, and other relevant entities. Conventional KG embedding methods such as TransE and ComplEx SE are hindered by their reliance on computationally intensive negative sampling and their limited generalization to unseen drug target pairs. To address these challenges, we propose Multi Context Aware Sampling (MuCoS), a novel framework that prioritizes high-density neighbours to capture salient structural patterns and integrates these with contextual embeddings derived from BERT. By unifying structural and textual modalities and selectively sampling highly informative patterns, MuCoS circumvents the need for negative sampling, significantly reducing computational overhead while enhancing predictive accuracy for novel drug target associations and drug targets. Extensive experiments on the KEGG50k dataset demonstrate that MuCoS outperforms state-of-the-art baselines, achieving up to a 13\\% improvement in mean reciprocal rank (MRR) in predicting any relation in the dataset and a 6\\% improvement in dedicated drug target relation prediction.         ",
    "url": "https://arxiv.org/abs/2503.08075",
    "authors": [
      "Haji Gul",
      "Abdul Ghani Naim",
      "Ajaz Ahmad Bhat"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.08076",
    "title": "Efficient Trajectory Generation Based on Traversable Planes in 3D Complex Architectural Spaces",
    "abstract": "           With the increasing integration of robots into human life, their role in architectural spaces where people spend most of their time has become more prominent. While motion capabilities and accurate localization for automated robots have rapidly developed, the challenge remains to generate efficient, smooth, comprehensive, and high-quality trajectories in these areas. In this paper, we propose a novel efficient planner for ground robots to autonomously navigate in large complex multi-layered architectural spaces. Considering that traversable regions typically include ground, slopes, and stairs, which are planar or nearly planar structures, we simplify the problem to navigation within and between complex intersecting planes. We first extract traversable planes from 3D point clouds through segmenting, merging, classifying, and connecting to build a plane-graph, which is lightweight but fully represents the traversable regions. We then build a trajectory optimization based on motion state trajectory and fully consider special constraints when crossing multi-layer planes to maximize the robot's maneuverability. We conduct experiments in simulated environments and test on a CubeTrack robot in real-world scenarios, validating the method's effectiveness and practicality.         ",
    "url": "https://arxiv.org/abs/2503.08076",
    "authors": [
      "Mengke Zhang",
      "Zhihao Tian",
      "Yaoguang Xia",
      "Chao Xu",
      "Fei Gao",
      "Yanjun Cao"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.08082",
    "title": "Interference Graph Estimation for Resource Allocation in Multi-Cell Multi-Numerology Networks: A Power-Domain Approach",
    "abstract": "           The interference graph, depicting the intra- and inter-cell interference channel gains, is indispensable for resource allocation in multi-cell this http URL, there lacks viable methods of interference graph estimation (IGE) for multi-cell multi-numerology (MN) networks. To fill this gap, we propose an efficient power-domain approach to IGE for the resource allocation in multi-cell MN networks. Unlike traditional reference signal-based approaches that consume frequency-time resources, our approach uses power as a new dimension for the estimation of channel gains. By carefully controlling the transmit powers of base stations, our approach is capable of estimating both intra- and inter-cell interference channel gains. As a power-domain approach, it can be seamlessly integrated with the resource allocation such that IGE and resource allocation can be conducted simultaneously using the same frequency-time resources. We derive the necessary conditions for the power-domain IGE and design a practical power control scheme. We formulate a multi-objective joint optimization problem of IGE and resource allocation, propose iterative solutions with proven convergence, and analyze the computational complexity. Our simulation results show that power-domain IGE can accurately estimate strong interference channel gains with low power overhead and is robust to carrier frequency and timing offsets.         ",
    "url": "https://arxiv.org/abs/2503.08082",
    "authors": [
      "Daqian Ding",
      "Haorui Li",
      "Yibo Pi",
      "Xudong Wang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.08083",
    "title": "Degradation Self-Supervised Learning for Lithium-ion Battery Health Diagnostics",
    "abstract": "           Health evaluation for lithium-ion batteries (LIBs) typically relies on constant charging/discharging protocols, often neglecting scenarios involving dynamic current profiles prevalent in electric vehicles. Conventional health indicators for LIBs also depend on the uniformity of measured data, restricting their adaptability to non-uniform conditions. In this study, a novel training strategy for estimating LIB health based on the paradigm of self-supervised learning is proposed. A multiresolution analysis technique, empirical wavelet transform, is utilized to decompose non-stationary voltage signals in the frequency domain. This allows the removal of ineffective components for the health evaluation model. The transformer neural network serves as the model backbone, and a loss function is designed to describe the capacity degradation behavior with the assumption that the degradation in LIBs across most operating conditions is inevitable and irreversible. The results show that the model can learn the aging characteristics by analyzing sequences of voltage and current profiles obtained at various time intervals from the same LIB cell. The proposed method is successfully applied to the Stanford University LIB aging dataset, derived from electric vehicle real driving profiles. Notably, this approach achieves an average correlation coefficient of 0.9 between the evaluated health index and the degradation of actual capacity, demonstrating its efficacy in capturing LIB health degradation. This research highlights the feasibility of training deep neural networks using unlabeled LIB data, offering cost-efficient means and unleashing the potential of the measured information.         ",
    "url": "https://arxiv.org/abs/2503.08083",
    "authors": [
      "J. C. Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08084",
    "title": "Instruction-Augmented Long-Horizon Planning: Embedding Grounding Mechanisms in Embodied Mobile Manipulation",
    "abstract": "           Enabling humanoid robots to perform long-horizon mobile manipulation planning in real-world environments based on embodied perception and comprehension abilities has been a longstanding challenge. With the recent rise of large language models (LLMs), there has been a notable increase in the development of LLM-based planners. These approaches either utilize human-provided textual representations of the real world or heavily depend on prompt engineering to extract such representations, lacking the capability to quantitatively understand the environment, such as determining the feasibility of manipulating objects. To address these limitations, we present the Instruction-Augmented Long-Horizon Planning (IALP) system, a novel framework that employs LLMs to generate feasible and optimal actions based on real-time sensor feedback, including grounded knowledge of the environment, in a closed-loop interaction. Distinct from prior works, our approach augments user instructions into PDDL problems by leveraging both the abstract reasoning capabilities of LLMs and grounding mechanisms. By conducting various real-world long-horizon tasks, each consisting of seven distinct manipulatory skills, our results demonstrate that the IALP system can efficiently solve these tasks with an average success rate exceeding 80%. Our proposed method can operate as a high-level planner, equipping robots with substantial autonomy in unstructured environments through the utilization of multi-modal sensor inputs.         ",
    "url": "https://arxiv.org/abs/2503.08084",
    "authors": [
      "Fangyuan Wang",
      "Shipeng Lyu",
      "Peng Zhou",
      "Anqing Duan",
      "Guodong Guo",
      "David Navarro-Alarcon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08092",
    "title": "SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection",
    "abstract": "           Most previous 3D object detection methods that leverage the multi-modality of LiDAR and cameras utilize the Bird's Eye View (BEV) space for intermediate feature representation. However, this space uses a low x, y-resolution and sacrifices z-axis information to reduce the overall feature resolution, which may result in declined accuracy. To tackle the problem of using low-resolution features, this paper focuses on the sparse nature of LiDAR point cloud data. From our observation, the number of occupied cells in the 3D voxels constructed from a LiDAR data can be even fewer than the number of total cells in the BEV map, despite the voxels' significantly higher resolution. Based on this, we introduce a novel sparse voxel-based transformer network for 3D object detection, dubbed as SparseVoxFormer. Instead of performing BEV feature extraction, we directly leverage sparse voxel features as the input for a transformer-based detector. Moreover, with regard to the camera modality, we introduce an explicit modality fusion approach that involves projecting 3D voxel coordinates onto 2D images and collecting the corresponding image features. Thanks to these components, our approach can leverage geometrically richer multi-modal features while even reducing the computational cost. Beyond the proof-of-concept level, we further focus on facilitating better multi-modal fusion and flexible control over the number of sparse features. Finally, thorough experimental results demonstrate that utilizing a significantly smaller number of sparse features drastically reduces computational costs in a 3D object detector while enhancing both overall and long-range performance.         ",
    "url": "https://arxiv.org/abs/2503.08092",
    "authors": [
      "Hyeongseok Son",
      "Jia He",
      "Seung-In Park",
      "Ying Min",
      "Yunhao Zhang",
      "ByungIn Yoo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08093",
    "title": "MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface Reconstruction",
    "abstract": "           3D Gaussian Splatting (3DGS) has gained significant attention for its high-quality rendering capabilities, ultra-fast training, and inference speeds. However, when we apply 3DGS to surface reconstruction tasks, especially in environments with dynamic objects and distractors, the method suffers from floating artifacts and color errors due to inconsistency from different viewpoints. To address this challenge, we propose Multi-View Consistency Gaussian Splatting for the domain of Robust Surface Reconstruction (\\textbf{MVGSR}), which takes advantage of lightweight Gaussian models and a {heuristics-guided distractor masking} strategy for robust surface reconstruction in non-static environments. Compared to existing methods that rely on MLPs for distractor segmentation strategies, our approach separates distractors from static scene elements by comparing multi-view feature consistency, allowing us to obtain precise distractor masks early in training. Furthermore, we introduce a pruning measure based on multi-view contributions to reset transmittance, effectively reducing floating artifacts. Finally, a multi-view consistency loss is applied to achieve high-quality performance in surface reconstruction tasks. Experimental results demonstrate that MVGSR achieves competitive geometric accuracy and rendering fidelity compared to the state-of-the-art surface reconstruction algorithms. More information is available on our project page (\\href{this https URL}{this url})         ",
    "url": "https://arxiv.org/abs/2503.08093",
    "authors": [
      "Chenfeng Hou",
      "Qi Xun Yeo",
      "Mengqi Guo",
      "Yongxin Su",
      "Yanyan Li",
      "Gim Hee Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08096",
    "title": "MegaSR: Mining Customized Semantics and Expressive Guidance for Image Super-Resolution",
    "abstract": "           Pioneering text-to-image (T2I) diffusion models have ushered in a new era of real-world image super-resolution (Real-ISR), significantly enhancing the visual perception of reconstructed images. However, existing methods typically integrate uniform abstract textual semantics across all blocks, overlooking the distinct semantic requirements at different depths and the fine-grained, concrete semantics inherently present in the images themselves. Moreover, relying solely on a single type of guidance further disrupts the consistency of reconstruction. To address these issues, we propose MegaSR, a novel framework that mines customized block-wise semantics and expressive guidance for diffusion-based ISR. Compared to uniform textual semantics, MegaSR enables flexible adaptation to multi-granularity semantic awareness by dynamically incorporating image attributes at each block. Furthermore, we experimentally identify HED edge maps, depth maps, and segmentation maps as the most expressive guidance, and propose a multi-stage aggregation strategy to modulate them into the T2I models. Extensive experiments demonstrate the superiority of MegaSR in terms of semantic richness and structural consistency.         ",
    "url": "https://arxiv.org/abs/2503.08096",
    "authors": [
      "Xinrui Li",
      "Jianlong Wu",
      "Xinchuan Huang",
      "Chong Chen",
      "Weili Guan",
      "Xian-Sheng Hua",
      "Liqiang Nie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08097",
    "title": "Evidential Uncertainty Probes for Graph Neural Networks",
    "abstract": "           Accurate quantification of both aleatoric and epistemic uncertainties is essential when deploying Graph Neural Networks (GNNs) in high-stakes applications such as drug discovery and financial fraud detection, where reliable predictions are critical. Although Evidential Deep Learning (EDL) efficiently quantifies uncertainty using a Dirichlet distribution over predictive probabilities, existing EDL-based GNN (EGNN) models require modifications to the network architecture and retraining, failing to take advantage of pre-trained models. We propose a plug-and-play framework for uncertainty quantification in GNNs that works with pre-trained models without the need for retraining. Our Evidential Probing Network (EPN) uses a lightweight Multi-Layer-Perceptron (MLP) head to extract evidence from learned representations, allowing efficient integration with various GNN architectures. We further introduce evidence-based regularization techniques, referred to as EPN-reg, to enhance the estimation of epistemic uncertainty with theoretical justifications. Extensive experiments demonstrate that the proposed EPN-reg achieves state-of-the-art performance in accurate and efficient uncertainty quantification, making it suitable for real-world deployment.         ",
    "url": "https://arxiv.org/abs/2503.08097",
    "authors": [
      "Linlin Yu",
      "Kangshuo Li",
      "Pritom Kumar Saha",
      "Yifei Lou",
      "Feng Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08101",
    "title": "Accelerate 3D Object Detection Models via Zero-Shot Attention Key Pruning",
    "abstract": "           Query-based methods with dense features have demonstrated remarkable success in 3D object detection tasks. However, the computational demands of these models, particularly with large image sizes and multiple transformer layers, pose significant challenges for efficient running on edge devices. Existing pruning and distillation methods either need retraining or are designed for ViT models, which are hard to migrate to 3D detectors. To address this issue, we propose a zero-shot runtime pruning method for transformer decoders in 3D object detection models. The method, termed tgGBC (trim keys gradually Guided By Classification scores), systematically trims keys in transformer modules based on their importance. We expand the classification score to multiply it with the attention map to get the importance score of each key and then prune certain keys after each transformer layer according to their importance scores. Our method achieves a 1.99x speedup in the transformer decoder of the latest ToC3D model, with only a minimal performance loss of less than 1%. Interestingly, for certain models, our method even enhances their performance. Moreover, we deploy 3D detectors with tgGBC on an edge device, further validating the effectiveness of our method. The code can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.08101",
    "authors": [
      "Lizhen Xu",
      "Xiuxiu Bai",
      "Xiaojun Jia",
      "Jianwu Fang",
      "Shanmin Pang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08128",
    "title": "Permanent of bipartite graphs in terms of determinants",
    "abstract": "           Computing the permanent of a $(0,1)$-matrix is a well-known $\\#P$-complete problem. In this paper, we present an expression for the permanent of a bipartite graph in terms of the determinant of the graph and its subgraphs, obtained by successively removing rows and columns corresponding to vertices involved in vertex-disjoint $4k$-cycles. Our formula establishes a general relationship between the permanent and the determinant for any bipartite graph. Since computing the permanent of a biadjacency matrix is equivalent to counting the number of its perfect matchings, this approach also provides a more efficient method for counting perfect matchings in certain types of bipartite graphs. s in certain types of bipartite graphs.         ",
    "url": "https://arxiv.org/abs/2503.08128",
    "authors": [
      "Surabhi Chakrabartty",
      "Ranveer Singh"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2503.08129",
    "title": "Coordinated Path Following of UAVs using Event-Triggered Communication over Networks with Digraph Topologies",
    "abstract": "           This article presents a novel time-coordination algorithm based on event-triggered communication to ensure multiple UAVs progress along their desired paths in coordination with one another. In the proposed algorithm, a UAV transmits its progression information to its neighbor UAVs only when a decentralized trigger condition is satisfied. Consequently, it significantly reduces the volume of inter-vehicle communications required to achieve the goal compared with the existing algorithms based on continuous communication. With such intermittent communications, it is shown that a decentralized coordination controller guarantees exponential convergence of the coordination error to a neighborhood of zero. Furthermore, a lower bound on the difference between two consecutive event-triggered times is provided showing that the Zeno behavior is excluded with the proposed algorithm. Lastly, simulation results validate the efficacy of the proposed algorithm.         ",
    "url": "https://arxiv.org/abs/2503.08129",
    "authors": [
      "Hyungsoo Kang",
      "Isaac Kaminer",
      "Venanzio Cichella",
      "Naira Hovakimyan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.08135",
    "title": "ArticulatedGS: Self-supervised Digital Twin Modeling of Articulated Objects using 3D Gaussian Splatting",
    "abstract": "           We tackle the challenge of concurrent reconstruction at the part level with the RGB appearance and estimation of motion parameters for building digital twins of articulated objects using the 3D Gaussian Splatting (3D-GS) method. With two distinct sets of multi-view imagery, each depicting an object in separate static articulation configurations, we reconstruct the articulated object in 3D Gaussian representations with both appearance and geometry information at the same time. Our approach decoupled multiple highly interdependent parameters through a multi-step optimization process, thereby achieving a stable optimization procedure and high-quality outcomes. We introduce ArticulatedGS, a self-supervised, comprehensive framework that autonomously learns to model shapes and appearances at the part level and synchronizes the optimization of motion parameters, all without reliance on 3D supervision, motion cues, or semantic labels. Our experimental results demonstrate that, among comparable methodologies, our approach has achieved optimal outcomes in terms of part segmentation accuracy, motion estimation accuracy, and visual quality.         ",
    "url": "https://arxiv.org/abs/2503.08135",
    "authors": [
      "Junfu Guo",
      "Yu Xin",
      "Gaoyi Liu",
      "Kai Xu",
      "Ligang Liu",
      "Ruizhen Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08148",
    "title": "Few-Shot Class-Incremental Model Attribution Using Learnable Representation From CLIP-ViT Features",
    "abstract": "           Recently, images that distort or fabricate facts using generative models have become a social concern. To cope with continuous evolution of generative artificial intelligence (AI) models, model attribution (MA) is necessary beyond just detection of synthetic images. However, current deep learning-based MA methods must be trained from scratch with new data to recognize unseen models, which is time-consuming and data-intensive. This work proposes a new strategy to deal with persistently emerging generative models. We adapt few-shot class-incremental learning (FSCIL) mechanisms for MA problem to uncover novel generative AI models. Unlike existing FSCIL approaches that focus on object classification using high-level information, MA requires analyzing low-level details like color and texture in synthetic images. Thus, we utilize a learnable representation from different levels of CLIP-ViT features. To learn an effective representation, we propose Adaptive Integration Module (AIM) to calculate a weighted sum of CLIP-ViT block features for each image, enhancing the ability to identify generative models. Extensive experiments show our method effectively extends from prior generative models to recent ones.         ",
    "url": "https://arxiv.org/abs/2503.08148",
    "authors": [
      "Hanbyul Lee",
      "Juneho Yi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08152",
    "title": "Depth-Assisted Network for Indiscernible Marine Object Counting with Adaptive Motion-Differentiated Feature Encoding",
    "abstract": "           Indiscernible marine object counting encounters numerous challenges, including limited visibility in underwater scenes, mutual occlusion and overlap among objects, and the dynamic similarity in appearance, color, and texture between the background and foreground. These factors significantly complicate the counting process. To address the scarcity of video-based indiscernible object counting datasets, we have developed a novel dataset comprising 50 videos, from which approximately 800 frames have been extracted and annotated with around 40,800 point-wise object labels. This dataset accurately represents real underwater environments where indiscernible marine objects are intricately integrated with their surroundings, thereby comprehensively illustrating the aforementioned challenges in object counting. To address these challenges, we propose a depth-assisted network with adaptive motion-differentiated feature encoding. The network consists of a backbone encoding module and three branches: a depth-assisting branch, a density estimation branch, and a motion weight generation branch. Depth-aware features extracted by the depth-assisting branch are enhanced via a depth-enhanced encoder to improve object representation. Meanwhile, weights from the motion weight generation branch refine multi-scale perception features in the adaptive flow estimation module. Experimental results demonstrate that our method not only achieves state-of-the-art performance on the proposed dataset but also yields competitive results on three additional video-based crowd counting datasets. The pre-trained model, code, and dataset are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.08152",
    "authors": [
      "Chengzhi Ma",
      "Kunqian Li",
      "Shuaixin Liu",
      "Han Mei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08161",
    "title": "OASIS: Order-Augmented Strategy for Improved Code Search",
    "abstract": "           Code embeddings capture the semantic representations of code and are crucial for various code-related large language model (LLM) applications, such as code search. Previous training primarily relies on optimizing the InfoNCE loss by comparing positive natural language (NL)-code pairs with in-batch negatives. However, due to the sparse nature of code contexts, training solely by comparing the major differences between positive and negative pairs may fail to capture deeper semantic nuances. To address this issue, we propose a novel order-augmented strategy for improved code search (OASIS). It leverages order-based similarity labels to train models to capture subtle differences in similarity among negative pairs. Extensive benchmark evaluations demonstrate that our OASIS model significantly outperforms previous state-of-the-art models focusing solely on major positive-negative differences. It underscores the value of exploiting subtle differences among negative pairs with order labels for effective code embedding training.         ",
    "url": "https://arxiv.org/abs/2503.08161",
    "authors": [
      "Zuchen Gao",
      "Zizheng Zhan",
      "Xianming Li",
      "Erxin Yu",
      "Haotian Zhang",
      "Yuqun Zhang",
      "Jing Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.08180",
    "title": "Towards Synthesized and Editable Motion In-Betweening Through Part-Wise Phase Representation",
    "abstract": "           Styled motion in-betweening is crucial for computer animation and gaming. However, existing methods typically encode motion styles by modeling whole-body motions, often overlooking the representation of individual body parts. This limitation reduces the flexibility of infilled motion, particularly in adjusting the motion styles of specific limbs independently. To overcome this challenge, we propose a novel framework that models motion styles at the body-part level, enhancing both the diversity and controllability of infilled motions. Our approach enables more nuanced and expressive animations by allowing precise modifications to individual limb motions while maintaining overall motion coherence. Leveraging phase-related insights, our framework employs periodic autoencoders to automatically extract the phase of each body part, capturing distinctive local style features. Additionally, we effectively decouple the motion source from synthesis control by integrating motion manifold learning and conditional generation techniques from both image and motion domains. This allows the motion source to generate high-quality motions across various styles, with extracted motion and style features readily available for controlled synthesis in subsequent tasks. Comprehensive evaluations demonstrate that our method achieves superior speed, robust generalization, and effective generation of extended motion sequences.         ",
    "url": "https://arxiv.org/abs/2503.08180",
    "authors": [
      "Minyue Dai",
      "Jingbo Wang",
      "Ke Fan",
      "Bin Ji",
      "Haoyu Zhao",
      "Junting Dong",
      "Bo Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08189",
    "title": "SoTCKGE:Continual Knowledge Graph Embedding Based on Spatial Offset Transformation",
    "abstract": "           Current Continual Knowledge Graph Embedding (CKGE) methods primarily rely on translation-based embedding methods, leveraging previously acquired knowledge to initialize new facts. To enhance learning efficiency, these methods often integrate fine-tuning or continual learning strategies. However, this compromises the model's prediction accuracy and the translation-based methods lack support for complex relational structures (multi-hop relations). To tackle this challenge, we propose a novel CKGE framework SoTCKGE grounded in Spatial Offset Transformation. Within this framework, entity positions are defined as being jointly determined by base position vectors and offset vectors. This not only enhances the model's ability to represent complex relational structures but also allows for the embedding update of both new and old knowledge through simple spatial offset transformations, without the need for continuous learning methods. Furthermore, we introduce a hierarchical update strategy and a balanced embedding method to refine the parameter update process, effectively minimizing training costs and augmenting model accuracy. To comprehensively assess the performance of our model, we have conducted extensive experimlents on four publicly accessible datasets and a new dataset constructed by us. Experimental results demonstrate the advantage of our model in enhancing multi-hop relationship learning and further improving prediction accuracy.         ",
    "url": "https://arxiv.org/abs/2503.08189",
    "authors": [
      "Xinyan Wang",
      "Jinshuo Liu",
      "Cheng Bi",
      "Kaijian Xie",
      "Meng Wang",
      "Juan Deng",
      "Jeff Pan"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.08192",
    "title": "Automating Violence Detection and Categorization from Ancient Texts",
    "abstract": "           Violence descriptions in literature offer valuable insights for a wide range of research in the humanities. For historians, depictions of violence are of special interest for analyzing the societal dynamics surrounding large wars and individual conflicts of influential people. Harvesting data for violence research manually is laborious and time-consuming. This study is the first one to evaluate the effectiveness of large language models (LLMs) in identifying violence in ancient texts and categorizing it across multiple dimensions. Our experiments identify LLMs as a valuable tool to scale up the accurate analysis of historical texts and show the effect of fine-tuning and data augmentation, yielding an F1-score of up to 0.93 for violence detection and 0.86 for fine-grained violence categorization.         ",
    "url": "https://arxiv.org/abs/2503.08192",
    "authors": [
      "Alhassan Abdelhalim",
      "Michaela Regneri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Digital Libraries (cs.DL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08213",
    "title": "DeepRAG: Building a Custom Hindi Embedding Model for Retrieval Augmented Generation from Scratch",
    "abstract": "           In this paper, I present our work on DeepRAG, a specialized embedding model we built specifically for Hindi language in RAG systems. While LLMs have gotten really good at generating text, their performance in retrieval tasks still depends heavily on having quality embeddings - something that's been lacking for Hindi despite being one of the world's most spoken languages. We tackled this by creating embeddings from the ground up rather than just fine-tuning existing models. Our process involved collecting diverse Hindi texts (over 2.7M samples), training a custom SentencePiece tokenizer that actually understands Hindi morphology, designing transformer architecture with Hindi-specific attention mechanisms, and optimizing with contrastive learning. Results were honestly better than I expected - we saw a 23% improvement in retrieval precision compared to the multilingual models everyone's been using. The paper details our methodology, which I think could help others working with low-resource languages where the one-size-fits-all multilingual models fall short. We've also integrated our embeddings with LangChain to build complete Hindi RAG systems, which might be useful for practitioners. While there's still tons more to explore, I believe this work addresses a critical gap for Hindi NLP and demonstrates why language-specific approaches matter.         ",
    "url": "https://arxiv.org/abs/2503.08213",
    "authors": [
      "Nandakishor M"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08226",
    "title": "A Grey-box Text Attack Framework using Explainable AI",
    "abstract": "           Explainable AI is a strong strategy implemented to understand complex black-box model predictions in a human interpretable language. It provides the evidence required to execute the use of trustworthy and reliable AI systems. On the other hand, however, it also opens the door to locating possible vulnerabilities in an AI model. Traditional adversarial text attack uses word substitution, data augmentation techniques and gradient-based attacks on powerful pre-trained Bidirectional Encoder Representations from Transformers (BERT) variants to generate adversarial sentences. These attacks are generally whitebox in nature and not practical as they can be easily detected by humans E.g. Changing the word from \"Poor\" to \"Rich\". We proposed a simple yet effective Grey-box cum Black-box approach that does not require the knowledge of the model while using a set of surrogate Transformer/BERT models to perform the attack using Explainable AI techniques. As Transformers are the current state-of-the-art models for almost all Natural Language Processing (NLP) tasks, an attack generated from BERT1 is transferable to BERT2. This transferability is made possible due to the attention mechanism in the transformer that allows the model to capture long-range dependencies in a sequence. Using the power of BERT generalisation via attention, we attempt to exploit how transformers learn by attacking a few surrogate transformer variants which are all based on a different architecture. We demonstrate that this approach is highly effective to generate semantically good sentences by changing as little as one word that is not detectable by humans while still fooling other BERT models.         ",
    "url": "https://arxiv.org/abs/2503.08226",
    "authors": [
      "Esther Chiramal",
      "Kelvin Soh Boon Kai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08228",
    "title": "Investigating Execution-Aware Language Models for Code Optimization",
    "abstract": "           Code optimization is the process of enhancing code efficiency, while preserving its intended functionality. This process often requires a deep understanding of the code execution behavior at run-time to identify and address inefficiencies effectively. Recent studies have shown that language models can play a significant role in automating code optimization. However, these models may have insufficient knowledge of how code execute at run-time. To address this limitation, researchers have developed strategies that integrate code execution information into language models. These strategies have shown promise, enhancing the effectiveness of language models in various software engineering tasks. However, despite the close relationship between code execution behavior and efficiency, the specific impact of these strategies on code optimization remains largely unexplored. This study investigates how incorporating code execution information into language models affects their ability to optimize code. Specifically, we apply three different training strategies to incorporate four code execution aspects -- line executions, line coverage, branch coverage, and variable states -- into CodeT5+, a well-known language model for code. Our results indicate that execution-aware models provide limited benefits compared to the standard CodeT5+ model in optimizing code.         ",
    "url": "https://arxiv.org/abs/2503.08228",
    "authors": [
      "Federico Di Menna",
      "Luca Traini",
      "Gabriele Bavota",
      "Vittorio Cortellessa"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2503.08232",
    "title": "Generation and Balancing Capacity in Future Electric Power Systems -- Scenario Analysis Using Bayesian Networks",
    "abstract": "           This paper examines the evolution of the Finnish electric energy system up to 2035, focusing on the likelihood of different development paths. The primary contribution of this paper is the development of an extensive Bayesian Network, designed to model and analyse the evolution of power generation capacity mix, assess the likelihood of different grid management scenarios, and understand the causal relationships underlying these scenarios. A target optimisation was carried out using the constructed Bayesian Network to explore possibilities to minimise grid management complexity. The results of the optimisation reveal that the authorities and stakeholders should prioritise increasing demand response, gas power, and battery storage capacities. These mature technologies are well-suited to guarantee energy adequacy during peak consumption periods, which in Finland typically occur during consecutive cold, dark and windless winter weeks. Although this study focuses on the evolution of the Finnish power grid, the constructed Bayesian Network approach is broadly applicable and can be utilised to explore causal relationships in other countries by employing the designed questionnaire and engaging a panel of experts specific to the country's energy infrastructure.         ",
    "url": "https://arxiv.org/abs/2503.08232",
    "authors": [
      "Seppo Borenius",
      "Pekka Kekolahti",
      "Petri M\u00e4h\u00f6nen",
      "Matti Lehtonen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2503.08239",
    "title": "EnergyFormer: Energy Attention with Fourier Embedding for Hyperspectral Image Classification",
    "abstract": "           Hyperspectral imaging (HSI) provides rich spectral-spatial information across hundreds of contiguous bands, enabling precise material discrimination in applications such as environmental monitoring, agriculture, and urban analysis. However, the high dimensionality and spectral variability of HSI data pose significant challenges for feature extraction and classification. This paper presents EnergyFormer, a transformer-based framework designed to address these challenges through three key innovations: (1) Multi-Head Energy Attention (MHEA), which optimizes an energy function to selectively enhance critical spectral-spatial features, improving feature discrimination; (2) Fourier Position Embedding (FoPE), which adaptively encodes spectral and spatial dependencies to reinforce long-range interactions; and (3) Enhanced Convolutional Block Attention Module (ECBAM), which selectively amplifies informative wavelength bands and spatial structures, enhancing representation learning. Extensive experiments on the WHU-Hi-HanChuan, Salinas, and Pavia University datasets demonstrate that EnergyFormer achieves exceptional overall accuracies of 99.28\\%, 98.63\\%, and 98.72\\%, respectively, outperforming state-of-the-art CNN, transformer, and Mamba-based models. The source code will be made available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.08239",
    "authors": [
      "Saad Sohail",
      "Muhammad Usama",
      "Usman Ghous",
      "Manuel Mazzara",
      "Salvatore Distefano",
      "Muhammad Ahmad"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08245",
    "title": "ExMAG: Learning of Maximally Ancestral Graphs",
    "abstract": "           As one transitions from statistical to causal learning, one is seeking the most appropriate causal model. Dynamic Bayesian networks are a popular model, where a weighted directed acyclic graph represents the causal relationships. Stochastic processes are represented by its vertices, and weighted oriented edges suggest the strength of the causal relationships. When there are confounders, one would like to utilize both oriented edges (when the direction of causality is clear) and edges that are not oriented (when there is a confounder), yielding mixed graphs. A little-studied extension of acyclicity to this mixed-graph setting is known as maximally ancestral graphs. We propose a score-based learning algorithm for learning maximally ancestral graphs. A mixed-integer quadratic program is formulated, and an algorithmic approach is proposed, in which the pre-generation of exponentially many constraints is avoided by generating only violated constraints in the so-called branch-and-cut (``lazy constraint'') method. Comparing the novel approach to the state-of-the-art, we show that the proposed approach turns out to produce more accurate results when applied to small and medium-sized synthetic instances containing up to 25 variables.         ",
    "url": "https://arxiv.org/abs/2503.08245",
    "authors": [
      "Petr Ry\u0161av\u00fd",
      "Pavel Ryt\u00ed\u0159",
      "Xiaoyu He",
      "Jakub Mare\u010dek",
      "Georgios Korpas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08253",
    "title": "SARA: Structural and Adversarial Representation Alignment for Training-efficient Diffusion Models",
    "abstract": "           Modern diffusion models encounter a fundamental trade-off between training efficiency and generation quality. While existing representation alignment methods, such as REPA, accelerate convergence through patch-wise alignment, they often fail to capture structural relationships within visual representations and ensure global distribution consistency between pretrained encoders and denoising networks. To address these limitations, we introduce SARA, a hierarchical alignment framework that enforces multi-level representation constraints: (1) patch-wise alignment to preserve local semantic details, (2) autocorrelation matrix alignment to maintain structural consistency within representations, and (3) adversarial distribution alignment to mitigate global representation discrepancies. Unlike previous approaches, SARA explicitly models both intra-representation correlations via self-similarity matrices and inter-distribution coherence via adversarial alignment, enabling comprehensive alignment across local and global scales. Experiments on ImageNet-256 show that SARA achieves an FID of 1.36 while converging twice as fast as REPA, surpassing recent state-of-the-art image generation methods. This work establishes a systematic paradigm for optimizing diffusion training through hierarchical representation alignment.         ",
    "url": "https://arxiv.org/abs/2503.08253",
    "authors": [
      "Hesen Chen",
      "Junyan Wang",
      "Zhiyu Tan",
      "Hao Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08259",
    "title": "Multitask Reinforcement Learning for Quadcopter Attitude Stabilization and Tracking using Graph Policy",
    "abstract": "           Quadcopter attitude control involves two tasks: smooth attitude tracking and aggressive stabilization from arbitrary states. Although both can be formulated as tracking problems, their distinct state spaces and control strategies complicate a unified reward function. We propose a multitask deep reinforcement learning framework that leverages parallel simulation with IsaacGym and a Graph Convolutional Network (GCN) policy to address both tasks effectively. Our multitask Soft Actor-Critic (SAC) approach achieves faster, more reliable learning and higher sample efficiency than single-task methods. We validate its real-world applicability by deploying the learned policy - a compact two-layer network with 24 neurons per layer - on a Pixhawk flight controller, achieving 400 Hz control without extra computational resources. We provide our code at this https URL\\_UAV/.         ",
    "url": "https://arxiv.org/abs/2503.08259",
    "authors": [
      "Yu Tang Liu",
      "Afonso Vale",
      "Aamir Ahmad",
      "Rodrigo Ventura",
      "Meysam Basiri"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.08269",
    "title": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks",
    "abstract": "           Recent Customized Portrait Generation (CPG) methods, taking a facial image and a textual prompt as inputs, have attracted substantial attention. Although these methods generate high-fidelity portraits, they fail to prevent the generated portraits from being tracked and misused by malicious face recognition systems. To address this, this paper proposes a Customized Portrait Generation framework with facial Adversarial attacks (Adv-CPG). Specifically, to achieve facial privacy protection, we devise a lightweight local ID encryptor and an encryption enhancer. They implement progressive double-layer encryption protection by directly injecting the target identity and adding additional identity guidance, respectively. Furthermore, to accomplish fine-grained and personalized portrait generation, we develop a multi-modal image customizer capable of generating controlled fine-grained facial features. To the best of our knowledge, Adv-CPG is the first study that introduces facial adversarial attacks into CPG. Extensive experiments demonstrate the superiority of Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is 28.1% and 2.86% higher compared to the SOTA noise-based attack methods and unconstrained attack methods, respectively.         ",
    "url": "https://arxiv.org/abs/2503.08269",
    "authors": [
      "Junying Wang",
      "Hongyuan Zhang",
      "Yuan Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08275",
    "title": "Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models",
    "abstract": "           Long-form writing agents require flexible integration and interaction across information retrieval, reasoning, and composition. Current approaches rely on predetermined workflows and rigid thinking patterns to generate outlines before writing, resulting in constrained adaptability during writing. In this paper we propose a general agent framework that achieves human-like adaptive writing through recursive task decomposition and dynamic integration of three fundamental task types, i.e. retrieval, reasoning, and composition. Our methodology features: 1) a planning mechanism that interleaves recursive task decomposition and execution, eliminating artificial restrictions on writing workflow; and 2) integration of task types that facilitates heterogeneous task decomposition. Evaluations on both fiction writing and technical report generation show that our method consistently outperforms state-of-the-art approaches across all automatic evaluation metrics, which demonstrate the effectiveness and broad applicability of our proposed framework.         ",
    "url": "https://arxiv.org/abs/2503.08275",
    "authors": [
      "Ruibin Xiong",
      "Yimeng Chen",
      "Dmitrii Khizbullin",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.08284",
    "title": "Neural cyberattacks applied to the vision under realistic visual stimuli",
    "abstract": "           Brain-Computer Interfaces (BCIs) are systems traditionally used in medicine and designed to interact with the brain to record or stimulate neurons. Despite their benefits, the literature has demonstrated that invasive BCIs focused on neurostimulation present vulnerabilities allowing attackers to gain control. In this context, neural cyberattacks emerged as threats able to disrupt spontaneous neural activity by performing neural overstimulation or inhibition. Previous work validated these attacks in small-scale simulations with a reduced number of neurons, lacking real-world complexity. Thus, this work tackles this limitation by analyzing the impact of two existing neural attacks, Neuronal Flooding (FLO) and Neuronal Jamming (JAM), on a complex neuronal topology of the primary visual cortex of mice consisting of approximately 230,000 neurons, tested on three realistic visual stimuli: flash effect, movie, and drifting gratings. Each attack was evaluated over three relevant events per stimulus, also testing the impact of attacking 25% and 50% of the neurons. The results, based on the number of spikes and shift percentages metrics, showed that the attacks caused the greatest impact on the movie, while dark and fixed events are the most robust. Although both attacks can significantly affect neural activity, JAM was generally more damaging, producing longer temporal delays, and had a larger prevalence. Finally, JAM did not require to alter many neurons to significantly affect neural activity, while the impact in FLO increased with the number of neurons attacked.         ",
    "url": "https://arxiv.org/abs/2503.08284",
    "authors": [
      "Victoria Magdalena L\u00f3pez Madejska",
      "Sergio L\u00f3pez Bernal",
      "Gregorio Mart\u00ednez P\u00e9rez",
      "Alberto Huertas Celdr\u00e1n"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.08293",
    "title": "A systematic literature review of unsupervised learning algorithms for anomalous traffic detection based on flows",
    "abstract": "           The constant increase of devices connected to the Internet, and therefore of cyber-attacks, makes it necessary to analyze network traffic in order to recognize malicious activity. Traditional packet-based analysis methods are insufficient because in large networks the amount of traffic is so high that it is unfeasible to review all communications. For this reason, flows is a suitable approach for this situation, which in future 5G networks will have to be used, as the number of packets will increase dramatically. If this is also combined with unsupervised learning models, it can detect new threats for which it has not been trained. This paper presents a systematic review of the literature on unsupervised learning algorithms for detecting anomalies in network flows, following the PRISMA guideline. A total of 63 scientific articles have been reviewed, analyzing 13 of them in depth. The results obtained show that autoencoder is the most used option, followed by SVM, ALAD, or SOM. On the other hand, all the datasets used for anomaly detection have been collected, including some specialised in IoT or with real data collected from honeypots.         ",
    "url": "https://arxiv.org/abs/2503.08293",
    "authors": [
      "Alberto Miguel-Diez",
      "Adri\u00e1n Campazas-Vega",
      "Claudia \u00c1lvarez-Aparicio",
      "Gonzalo Esteban-Costales",
      "\u00c1ngel Manuel Guerrero-Higueras"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.08297",
    "title": "Privacy for Free: Leveraging Local Differential Privacy Perturbed Data from Multiple Services",
    "abstract": "           Local Differential Privacy (LDP) has emerged as a widely adopted privacy-preserving technique in modern data analytics, enabling users to share statistical insights while maintaining robust privacy guarantees. However, current LDP applications assume a single service gathering perturbed information from users. In reality, multiple services may be interested in collecting users' data, which poses privacy burdens to users as more such services emerge. To address this issue, this paper proposes a framework for collecting and aggregating data based on perturbed information from multiple services, regardless of their estimated statistics (e.g., mean or distribution) and perturbation mechanisms. Then for mean estimation, we introduce the Unbiased Averaging (UA) method and its optimized version, User-level Weighted Averaging (UWA). The former utilizes biased perturbed data, while the latter assigns weights to different perturbed results based on perturbation information, thereby achieving minimal variance. For distribution estimation, we propose the User-level Likelihood Estimation (ULE), which treats all perturbed results from a user as a whole for maximum likelihood estimation. Experimental results demonstrate that our framework and constituting methods significantly improve the accuracy of both mean and distribution estimation.         ",
    "url": "https://arxiv.org/abs/2503.08297",
    "authors": [
      "Rong Du",
      "Qingqing Ye",
      "Yue Fu",
      "Haibo Hu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.08305",
    "title": "ELECTRA: A Symmetry-breaking Cartesian Network for Charge Density Prediction with Floating Orbitals",
    "abstract": "           We present the Electronic Tensor Reconstruction Algorithm (ELECTRA) - an equivariant model for predicting electronic charge densities using \"floating\" orbitals. Floating orbitals are a long-standing idea in the quantum chemistry community that promises more compact and accurate representations by placing orbitals freely in space, as opposed to centering all orbitals at the position of atoms. Finding ideal placements of these orbitals requires extensive domain knowledge though, which thus far has prevented widespread adoption. We solve this in a data-driven manner by training a Cartesian tensor network to predict orbital positions along with orbital coefficients. This is made possible through a symmetry-breaking mechanism that is used to learn position displacements with lower symmetry than the input molecule while preserving the rotation equivariance of the charge density itself. Inspired by recent successes of Gaussian Splatting in representing densities in space, we are using Gaussians as our orbitals and predict their weights and covariance matrices. Our method achieves a state-of-the-art balance between computational efficiency and predictive accuracy on established benchmarks.         ",
    "url": "https://arxiv.org/abs/2503.08305",
    "authors": [
      "Jonas Elsborg",
      "Luca Thiede",
      "Al\u00e1n Aspuru-Guzik",
      "Tejs Vegge",
      "Arghya Bhowmik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2503.08321",
    "title": "i-WiViG: Interpretable Window Vision GNN",
    "abstract": "           Deep learning models based on graph neural networks have emerged as a popular approach for solving computer vision problems. They encode the image into a graph structure and can be beneficial for efficiently capturing the long-range dependencies typically present in remote sensing imagery. However, an important drawback of these methods is their black-box nature which may hamper their wider usage in critical applications. In this work, we tackle the self-interpretability of the graph-based vision models by proposing our Interpretable Window Vision GNN (i-WiViG) approach, which provides explanations by automatically identifying the relevant subgraphs for the model prediction. This is achieved with window-based image graph processing that constrains the node receptive field to a local image region and by using a self-interpretable graph bottleneck that ranks the importance of the long-range relations between the image regions. We evaluate our approach to remote sensing classification and regression tasks, showing it achieves competitive performance while providing inherent and faithful explanations through the identified relations. Further, the quantitative evaluation reveals that our model reduces the infidelity of post-hoc explanations compared to other Vision GNN models, without sacrificing explanation sparsity.         ",
    "url": "https://arxiv.org/abs/2503.08321",
    "authors": [
      "Ivica Obadic",
      "Dmitry Kangin",
      "Dario Oliveira",
      "Plamen P Angelov",
      "Xiao Xiang Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08325",
    "title": "Prototype-based Heterogeneous Federated Learning for Blade Icing Detection in Wind Turbines with Class Imbalanced Data",
    "abstract": "           Wind farms, typically in high-latitude regions, face a high risk of blade icing. Traditional centralized training methods raise serious privacy concerns. To enhance data privacy in detecting wind turbine blade icing, traditional federated learning (FL) is employed. However, data heterogeneity, resulting from collections across wind farms in varying environmental conditions, impacts the model's optimization capabilities. Moreover, imbalances in wind turbine data lead to models that tend to favor recognizing majority classes, thus neglecting critical icing anomalies. To tackle these challenges, we propose a federated prototype learning model for class-imbalanced data in heterogeneous environments to detect wind turbine blade icing. We also propose a contrastive supervised loss function to address the class imbalance problem. Experiments on real data from 20 turbines across two wind farms show our method outperforms five FL models and five class imbalance methods, with an average improvement of 19.64\\% in \\( mF_{\\beta} \\) and 5.73\\% in \\( m \\)BA compared to the second-best method, BiFL.         ",
    "url": "https://arxiv.org/abs/2503.08325",
    "authors": [
      "Lele Qi",
      "Mengna Liu",
      "Xu Cheng",
      "Fan Shi",
      "Xiufeng Liu",
      "Shengyong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08348",
    "title": "Design and Implementation of FourCropNet: A CNN-Based System for Efficient Multi-Crop Disease Detection and Management",
    "abstract": "           Plant disease detection is a critical task in agriculture, directly impacting crop yield, food security, and sustainable farming practices. This study proposes FourCropNet, a novel deep learning model designed to detect diseases in multiple crops, including CottonLeaf, Grape, Soybean, and Corn. The model leverages an advanced architecture comprising residual blocks for efficient feature extraction, attention mechanisms to enhance focus on disease-relevant regions, and lightweight layers for computational efficiency. These components collectively enable FourCropNet to achieve superior performance across varying datasets and class complexities, from single-crop datasets to combined datasets with 15 classes. The proposed model was evaluated on diverse datasets, demonstrating high accuracy, specificity, sensitivity, and F1 scores. Notably, FourCropNet achieved the highest accuracy of 99.7% for Grape, 99.5% for Corn, and 95.3% for the combined dataset. Its scalability and ability to generalize across datasets underscore its robustness. Comparative analysis shows that FourCropNet consistently outperforms state-of-the-art models such as MobileNet, VGG16, and EfficientNet across various metrics. FourCropNet's innovative design and consistent performance make it a reliable solution for real-time disease detection in agriculture. This model has the potential to assist farmers in timely disease diagnosis, reducing economic losses and promoting sustainable agricultural practices.         ",
    "url": "https://arxiv.org/abs/2503.08348",
    "authors": [
      "H. P. Khandagale",
      "Sangram Patil",
      "V. S. Gavali",
      "S. V. Chavan",
      "P. P. Halkarnikar",
      "Prateek A. Meshram"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08354",
    "title": "Robust Latent Matters: Boosting Image Generation with Sampling Error",
    "abstract": "           Recent image generation schemes typically capture image distribution in a pre-constructed latent space relying on a frozen image tokenizer. Though the performance of tokenizer plays an essential role to the successful generation, its current evaluation metrics (e.g. rFID) fail to precisely assess the tokenizer and correlate its performance to the generation quality (e.g. gFID). In this paper, we comprehensively analyze the reason for the discrepancy of reconstruction and generation qualities in a discrete latent space, and, from which, we propose a novel plug-and-play tokenizer training scheme to facilitate latent space construction. Specifically, a latent perturbation approach is proposed to simulate sampling noises, i.e., the unexpected tokens sampled, from the generative process. With the latent perturbation, we further propose (1) a novel tokenizer evaluation metric, i.e., pFID, which successfully correlates the tokenizer performance to generation quality and (2) a plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer thus boosting the generation quality and convergence speed. Extensive benchmarking are conducted with 11 advanced discrete image tokenizers with 2 autoregressive generation models to validate our approach. The tokenizer trained with our proposed latent perturbation achieve a notable 1.60 gFID with classifier-free guidance (CFG) and 3.45 gFID without CFG with a $\\sim$400M generator. Code: this https URL.         ",
    "url": "https://arxiv.org/abs/2503.08354",
    "authors": [
      "Kai Qiu",
      "Xiang Li",
      "Jason Kuen",
      "Hao Chen",
      "Xiaohao Xu",
      "Jiuxiang Gu",
      "Yinyi Luo",
      "Bhiksha Raj",
      "Zhe Lin",
      "Marios Savvides"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08371",
    "title": "Density Ratio-based Proxy Causal Learning Without Density Ratios",
    "abstract": "           We address the setting of Proxy Causal Learning (PCL), which has the goal of estimating causal effects from observed data in the presence of hidden confounding. Proxy methods accomplish this task using two proxy variables related to the latent confounder: a treatment proxy (related to the treatment) and an outcome proxy (related to the outcome). Two approaches have been proposed to perform causal effect estimation given proxy variables; however only one of these has found mainstream acceptance, since the other was understood to require density ratio estimation - a challenging task in high dimensions. In the present work, we propose a practical and effective implementation of the second approach, which bypasses explicit density ratio estimation and is suitable for continuous and high-dimensional treatments. We employ kernel ridge regression to derive estimators, resulting in simple closed-form solutions for dose-response and conditional dose-response curves, along with consistency guarantees. Our methods empirically demonstrate superior or comparable performance to existing frameworks on synthetic and real-world datasets.         ",
    "url": "https://arxiv.org/abs/2503.08371",
    "authors": [
      "Bariscan Bozkurt",
      "Ben Deaner",
      "Dimitri Meunier",
      "Liyuan Xu",
      "Arthur Gretton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08381",
    "title": "InfluenceNet: AI Models for Banzhaf and Shapley Value Prediction",
    "abstract": "           Power indices are essential in assessing the contribution and influence of individual agents in multi-agent systems, providing crucial insights into collaborative dynamics and decision-making processes. While invaluable, traditional computational methods for exact or estimated power indices values require significant time and computational constraints, especially for large $(n\\ge10)$ coalitions. These constraints have historically limited researchers' ability to analyse complex multi-agent interactions comprehensively. To address this limitation, we introduce a novel Neural Networks-based approach that efficiently estimates power indices for voting games, demonstrating comparable and often superiour performance to existing tools in terms of both speed and accuracy. This method not only addresses existing computational bottlenecks, but also enables rapid analysis of large coalitions, opening new avenues for multi-agent system research by overcoming previous computational limitations and providing researchers with a more accessible, scalable analytical this http URL increased efficiency will allow for the analysis of more complex and realistic multi-agent scenarios.         ",
    "url": "https://arxiv.org/abs/2503.08381",
    "authors": [
      "Benjamin Kempinski",
      "Tal Kachman"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08410",
    "title": "A Deep-Learning Iterative Stacked Approach for Prediction of Reactive Dissolution in Porous Media",
    "abstract": "           Simulating reactive dissolution of solid minerals in porous media has many subsurface applications, including carbon capture and storage (CCS), geothermal systems and oil & gas recovery. As traditional direct numerical simulators are computationally expensive, it is of paramount importance to develop faster and more efficient alternatives. Deep-learning-based solutions, most of them built upon convolutional neural networks (CNNs), have been recently designed to tackle this problem. However, these solutions were limited to approximating one field over the domain (e.g. velocity field). In this manuscript, we present a novel deep learning approach that incorporates both temporal and spatial information to predict the future states of the dissolution process at a fixed time-step horizon, given a sequence of input states. The overall performance, in terms of speed and prediction accuracy, is demonstrated on a numerical simulation dataset, comparing its prediction results against state-of-the-art approaches, also achieving a speedup around $10^4$ over traditional numerical simulators.         ",
    "url": "https://arxiv.org/abs/2503.08410",
    "authors": [
      "Marcos Cirne",
      "Hannah Menke",
      "Alhasan Abdellatif",
      "Julien Maes",
      "Florian Doster",
      "Ahmed H. Elsheikh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08429",
    "title": "Using Powerful Prior Knowledge of Diffusion Model in Deep Unfolding Networks for Image Compressive Sensing",
    "abstract": "           Recently, Deep Unfolding Networks (DUNs) have achieved impressive reconstruction quality in the field of image Compressive Sensing (CS) by unfolding iterative optimization algorithms into neural networks. The reconstruction quality of DUNs depends on the learned prior knowledge, so introducing stronger prior knowledge can further improve reconstruction quality. On the other hand, pre-trained diffusion models contain powerful prior knowledge and have a solid theoretical foundation and strong scalability, but it requires a large number of iterative steps to achieve reconstruction. In this paper, we propose to use the powerful prior knowledge of pre-trained diffusion model in DUNs to achieve high-quality reconstruction with less steps for image CS. Specifically, we first design an iterative optimization algorithm named Diffusion Message Passing (DMP), which embeds a pre-trained diffusion model into each iteration process of DMP. Then, we deeply unfold the DMP algorithm into a neural network named DMP-DUN. The proposed DMP-DUN can use lightweight neural networks to achieve mapping from measurement data to the intermediate steps of the reverse diffusion process and directly approximate the divergence of the diffusion model, thereby further improving reconstruction efficiency. Extensive experiments show that our proposed DMP-DUN achieves state-of-the-art performance and requires at least only 2 steps to reconstruct the image. Codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.08429",
    "authors": [
      "Chen Liao",
      "Yan Shen",
      "Dan Li",
      "Zhongli Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08437",
    "title": "ICPR 2024 Competition on Rider Intention Prediction",
    "abstract": "           The recent surge in the vehicle market has led to an alarming increase in road accidents. This underscores the critical importance of enhancing road safety measures, particularly for vulnerable road users like motorcyclists. Hence, we introduce the rider intention prediction (RIP) competition that aims to address challenges in rider safety by proactively predicting maneuvers before they occur, thereby strengthening rider safety. This capability enables the riders to react to the potential incorrect maneuvers flagged by advanced driver assistance systems (ADAS). We collect a new dataset, namely, rider action anticipation dataset (RAAD) for the competition consisting of two tasks: single-view RIP and multi-view RIP. The dataset incorporates a spectrum of traffic conditions and challenging navigational maneuvers on roads with varying lighting conditions. For the competition, we received seventy-five registrations and five team submissions for inference of which we compared the methods of the top three performing teams on both the RIP tasks: one state-space model (Mamba2) and two learning-based approaches (SVM and CNN-LSTM). The results indicate that the state-space model outperformed the other methods across the entire dataset, providing a balanced performance across maneuver classes. The SVM-based RIP method showed the second-best performance when using random sampling and SMOTE. However, the CNN-LSTM method underperformed, primarily due to class imbalance issues, particularly struggling with minority classes. This paper details the proposed RAAD dataset and provides a summary of the submissions for the RIP 2024 competition.         ",
    "url": "https://arxiv.org/abs/2503.08437",
    "authors": [
      "Shankar Gangisetty",
      "Abdul Wasi",
      "Shyam Nandan Rai",
      "C. V. Jawahar",
      "Sajay Raj",
      "Manish Prajapati",
      "Ayesha Choudhary",
      "Aaryadev Chandra",
      "Dev Chandan",
      "Shireen Chand",
      "Suvaditya Mukherjee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.08456",
    "title": "Network Analysis of Global Banking Systems and Detection of Suspicious Transactions",
    "abstract": "           A novel network-based approach is introduced to analyze banking systems, focusing on two main themes: identifying influential nodes within global banking networks using Bank for International Settlements data and developing an algorithm to detect suspicious transactions for anti-money laundering. Leveraging the concept of adversarial networks, we examine Bank for International Settlements data to characterize low-key leaders and highly-exposed nodes in the context of financial contagion among countries. Low-key leaders are nodes with significant influence despite lower centrality, while highly-exposed nodes represent those most vulnerable to defaults. Separately, using anonymized transaction data from Rabobank, we design an anti-money laundering algorithm based on network partitioning via the Louvain method and cycle detection, identifying unreported transaction patterns indicative of potential money laundering. The findings provide insights into system-wide vulnerabilities and propose tools to address challenges in financial stability and regulatory compliance.         ",
    "url": "https://arxiv.org/abs/2503.08456",
    "authors": [
      "Anthony Bonato",
      "Juan Chavez Palan",
      "Adam Szava"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.08474",
    "title": "Collaborative Dynamic 3D Scene Graphs for Open-Vocabulary Urban Scene Understanding",
    "abstract": "           Mapping and scene representation are fundamental to reliable planning and navigation in mobile robots. While purely geometric maps using voxel grids allow for general navigation, obtaining up-to-date spatial and semantically rich representations that scale to dynamic large-scale environments remains challenging. In this work, we present CURB-OSG, an open-vocabulary dynamic 3D scene graph engine that generates hierarchical decompositions of urban driving scenes via multi-agent collaboration. By fusing the camera and LiDAR observations from multiple perceiving agents with unknown initial poses, our approach generates more accurate maps compared to a single agent while constructing a unified open-vocabulary semantic hierarchy of the scene. Unlike previous methods that rely on ground truth agent poses or are evaluated purely in simulation, CURB-OSG alleviates these constraints. We evaluate the capabilities of CURB-OSG on real-world multi-agent sensor data obtained from multiple sessions of the Oxford Radar RobotCar dataset. We demonstrate improved mapping and object prediction accuracy through multi-agent collaboration as well as evaluate the environment partitioning capabilities of the proposed approach. To foster further research, we release our code and supplementary material at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.08474",
    "authors": [
      "Tim Steinke",
      "Martin B\u00fcchner",
      "Niclas V\u00f6disch",
      "Abhinav Valada"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.08479",
    "title": "Soft Actor-Critic-based Control Barrier Adaptation for Robust Autonomous Navigation in Unknown Environments",
    "abstract": "           Motion planning failures during autonomous navigation often occur when safety constraints are either too conservative, leading to deadlocks, or too liberal, resulting in collisions. To improve robustness, a robot must dynamically adapt its safety constraints to ensure it reaches its goal while balancing safety and performance measures. To this end, we propose a Soft Actor-Critic (SAC)-based policy for adapting Control Barrier Function (CBF) constraint parameters at runtime, ensuring safe yet non-conservative motion. The proposed approach is designed for a general high-level motion planner, low-level controller, and target system model, and is trained in simulation only. Through extensive simulations and physical experiments, we demonstrate that our framework effectively adapts CBF constraints, enabling the robot to reach its final goal without compromising safety.         ",
    "url": "https://arxiv.org/abs/2503.08479",
    "authors": [
      "Nicholas Mohammad",
      "Nicola Bezzo"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08482",
    "title": "A Multimodal Physics-Informed Neural Network Approach for Mean Radiant Temperature Modeling",
    "abstract": "           Outdoor thermal comfort is a critical determinant of urban livability, particularly in hot desert climates where extreme heat poses challenges to public health, energy consumption, and urban planning. Mean Radiant Temperature ($T_{mrt}$) is a key parameter for evaluating outdoor thermal comfort, especially in urban environments where radiation dynamics significantly impact human thermal exposure. Traditional methods of estimating $T_{mrt}$ rely on field measurements and computational simulations, both of which are resource intensive. This study introduces a Physics-Informed Neural Network (PINN) approach that integrates shortwave and longwave radiation modeling with deep learning techniques. By leveraging a multimodal dataset that includes meteorological data, built environment characteristics, and fisheye image-derived shading information, our model enhances predictive accuracy while maintaining physical consistency. Our experimental results demonstrate that the proposed PINN framework outperforms conventional deep learning models, with the best-performing configurations achieving an RMSE of 3.50 and an $R^2$ of 0.88. This approach highlights the potential of physics-informed machine learning in bridging the gap between computational modeling and real-world applications, offering a scalable and interpretable solution for urban thermal comfort assessments.         ",
    "url": "https://arxiv.org/abs/2503.08482",
    "authors": [
      "Pouya Shaeri",
      "Saud AlKhaled",
      "Ariane Middel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.08484",
    "title": "Generalizable AI-Generated Image Detection Based on Fractal Self-Similarity in the Spectrum",
    "abstract": "           The generalization performance of AI-generated image detection remains a critical challenge. Although most existing methods perform well in detecting images from generative models included in the training set, their accuracy drops significantly when faced with images from unseen generators. To address this limitation, we propose a novel detection method based on the fractal self-similarity of the spectrum, a common feature among images generated by different models. Specifically, we demonstrate that AI-generated images exhibit fractal-like spectral growth through periodic extension and low-pass filtering. This observation motivates us to exploit the similarity among different fractal branches of the spectrum. Instead of directly analyzing the spectrum, our method mitigates the impact of varying spectral characteristics across different generators, improving detection performance for images from unseen models. Experiments on a public benchmark demonstrated the generalized detection performance across both GANs and diffusion models.         ",
    "url": "https://arxiv.org/abs/2503.08484",
    "authors": [
      "Shengpeng Xiao",
      "Yuanfang Guo",
      "Heqi Peng",
      "Zeming Liu",
      "Liang Yang",
      "Yunhong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08485",
    "title": "TT-GaussOcc: Test-Time Compute for Self-Supervised Occupancy Prediction via Spatio-Temporal Gaussian Splatting",
    "abstract": "           Self-supervised 3D occupancy prediction offers a promising solution for understanding complex driving scenes without requiring costly 3D annotations. However, training dense voxel decoders to capture fine-grained geometry and semantics can demand hundreds of GPU hours, and such models often fail to adapt to varying voxel resolutions or new classes without extensive retraining. To overcome these limitations, we propose a practical and flexible test-time occupancy prediction framework termed TT-GaussOcc. Our approach incrementally optimizes time-aware 3D Gaussians instantiated from raw sensor streams at runtime, enabling voxelization at arbitrary user-specified resolution. Specifically, TT-GaussOcc operates in a \"lift-move-voxel\" symphony: we first \"lift\" surrounding-view semantics obtained from 2D vision foundation models (VLMs) to instantiate Gaussians at non-empty 3D space; Next, we \"move\" dynamic Gaussians from previous frames along estimated Gaussian scene flow to complete appearance and eliminate trailing artifacts of fast-moving objects, while accumulating static Gaussians to enforce temporal consistency; Finally, we mitigate inherent noises in semantic predictions and scene flow vectors by periodically smoothing neighboring Gaussians during optimization, using proposed trilateral RBF kernels that jointly consider color, semantic, and spatial affinities. The historical static and current dynamic Gaussians are then combined and voxelized to generate occupancy prediction. Extensive experiments on Occ3D and nuCraft with varying voxel resolutions demonstrate that TT-GaussOcc surpasses self-supervised baselines by 46% on mIoU without any offline training, and supports finer voxel resolutions at 2.6 FPS inference speed.         ",
    "url": "https://arxiv.org/abs/2503.08485",
    "authors": [
      "Fengyi Zhang",
      "Huitong Yang",
      "Zheng Zhang",
      "Zi Huang",
      "Yadan Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08486",
    "title": "Inferring Input Grammars from Code with Symbolic Parsing",
    "abstract": "           Generating effective test inputs for a software system requires that these inputs be valid, as they will otherwise be rejected without reaching actual functionality. In the absence of a specification for the input language, common test generation techniques rely on sample inputs, which are abstracted into matching grammars and/or evolved guided by test coverage. However, if sample inputs miss features of the input language, the chances of generating these features randomly are slim. In this work, we present the first technique for symbolically and automatically mining input grammars from the code of recursive descent parsers. So far, the complexity of parsers has made such a symbolic analysis challenging to impossible. Our realization of the symbolic parsing technique overcomes these challenges by (1) associating each parser function parse_ELEM() with a nonterminal <ELEM>; (2) limiting recursive calls and loop iterations, such that a symbolic analysis of parse_ELEM() needs to consider only a finite number of paths; and (3) for each path, create an expansion alternative for <ELEM>. Being purely static, symbolic parsing does not require seed inputs; as it mitigates path explosion, it scales to complex parsers. Our evaluation promises symbolic parsing to be highly accurate. Applied on parsers for complex languages such as TINY-C or JSON, our STALAGMITE implementation extracts grammars with an accuracy of 99--100%, widely improving over the state of the art despite requiring only the program code and no input samples. The resulting grammars cover the entire input space, allowing for comprehensive and effective test generation, reverse engineering, and documentation.         ",
    "url": "https://arxiv.org/abs/2503.08486",
    "authors": [
      "Leon Bettscheider",
      "Andreas Zeller"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Formal Languages and Automata Theory (cs.FL)"
    ]
  },
  {
    "id": "arXiv:2503.08497",
    "title": "MMRL: Multi-Modal Representation Learning for Vision-Language Models",
    "abstract": "           Large-scale pre-trained Vision-Language Models (VLMs) have become essential for transfer learning across diverse tasks. However, adapting these models with limited few-shot data often leads to overfitting, diminishing their performance on new tasks. To tackle this issue, we propose a novel Multi-Modal Representation Learning (MMRL) framework that introduces a shared, learnable, and modality-agnostic representation space. MMRL projects the space tokens to text and image representation tokens, facilitating more effective multi-modal interactions. Unlike previous approaches that solely optimize class token features, MMRL integrates representation tokens at higher layers of the encoders--where dataset-specific features are more prominent--while preserving generalized knowledge in the lower layers. During training, both representation and class features are optimized, with trainable projection layer applied to the representation tokens, whereas the class token projection layer remains frozen to retain pre-trained knowledge. Furthermore, a regularization term is introduced to align the class features and text features with the zero-shot features from the frozen VLM, thereby safeguarding the model's generalization capacity. For inference, a decoupling strategy is employed, wherein both representation and class features are utilized for base classes, while only the class features, which retain more generalized knowledge, are used for new tasks. Extensive experiments across 15 datasets demonstrate that MMRL outperforms state-of-the-art methods, achieving a balanced trade-off between task-specific adaptation and generalization. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.08497",
    "authors": [
      "Yuncheng Guo",
      "Xiaodong Gu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08505",
    "title": "CFNet: Optimizing Remote Sensing Change Detection through Content-Aware Enhancement",
    "abstract": "           Change detection is a crucial and widely applied task in remote sensing, aimed at identifying and analyzing changes occurring in the same geographical area over time. Due to variability in acquisition conditions, bi-temporal remote sensing images often exhibit significant differences in image style. Even with the powerful generalization capabilities of DNNs, these unpredictable style variations between bi-temporal images inevitably affect model's ability to accurately detect changed areas. To address issue above, we propose the Content Focuser Network (CFNet), which takes content-aware strategy as a key insight. CFNet employs EfficientNet-B5 as the backbone for feature extraction. To enhance the model's focus on the content features of images while mitigating the misleading effects of style features, we develop a constraint strategy that prioritizes the content features of bi-temporal images, termed Content-Aware. Furthermore, to enable the model to flexibly focus on changed and unchanged areas according to the requirements of different stages, we design a reweighting module based on the cosine distance between bi-temporal image features, termed Focuser. CFNet achieve outstanding performance across three well-known change detection datasets: CLCD (F1: 81.41%, IoU: 68.65%), LEVIR-CD (F1: 92.18%, IoU: 85.49%), and SYSU-CD (F1: 82.89%, IoU: 70.78%). The code and pretrained models of CFNet are publicly released at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.08505",
    "authors": [
      "Fan Wu",
      "Sijun Dong",
      "Xiaoliang Meng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08529",
    "title": "SignRep: Enhancing Self-Supervised Sign Representations",
    "abstract": "           Sign language representation learning presents unique challenges due to the complex spatio-temporal nature of signs and the scarcity of labeled datasets. Existing methods often rely either on models pre-trained on general visual tasks, that lack sign-specific features, or use complex multimodal and multi-branch architectures. To bridge this gap, we introduce a scalable, self-supervised framework for sign representation learning. We leverage important inductive (sign) priors during the training of our RGB model. To do this, we leverage simple but important cues based on skeletons while pretraining a masked autoencoder. These sign specific priors alongside feature regularization and an adversarial style agnostic loss provide a powerful backbone. Notably, our model does not require skeletal keypoints during inference, avoiding the limitations of keypoint-based models during downstream tasks. When finetuned, we achieve state-of-the-art performance for sign recognition on the WLASL, ASL-Citizen and NMFs-CSL datasets, using a simpler architecture and with only a single-modality. Beyond recognition, our frozen model excels in sign dictionary retrieval and sign translation, surpassing standard MAE pretraining and skeletal-based representations in retrieval. It also reduces computational costs for training existing sign translation models while maintaining strong performance on Phoenix2014T, CSL-Daily and How2Sign.         ",
    "url": "https://arxiv.org/abs/2503.08529",
    "authors": [
      "Ryan Wong",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08531",
    "title": "Visual Attention Graph",
    "abstract": "           Visual attention plays a critical role when our visual system executes active visual tasks by interacting with the physical scene. However, how to encode the visual object relationship in the psychological world of our brain deserves to be explored. In the field of computer vision, predicting visual fixations or scanpaths is a usual way to explore the visual attention and behaviors of human observers when viewing a scene. Most existing methods encode visual attention using individual fixations or scanpaths based on the raw gaze shift data collected from human observers. This may not capture the common attention pattern well, because without considering the semantic information of the viewed scene, raw gaze shift data alone contain high inter- and intra-observer variability. To address this issue, we propose a new attention representation, called Attention Graph, to simultaneously code the visual saliency and scanpath in a graph-based representation and better reveal the common attention behavior of human observers. In the attention graph, the semantic-based scanpath is defined by the path on the graph, while saliency of objects can be obtained by computing fixation density on each node. Systemic experiments demonstrate that the proposed attention graph combined with our new evaluation metrics provides a better benchmark for evaluating attention prediction methods. Meanwhile, extra experiments demonstrate the promising potentials of the proposed attention graph in assessing human cognitive states, such as autism spectrum disorder screening and age classification.         ",
    "url": "https://arxiv.org/abs/2503.08531",
    "authors": [
      "Kai-Fu Yang",
      "Yong-Jie Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2503.08543",
    "title": "BoundarEase: Fostering Constructive Community Engagement to Inform More Equitable Student Assignment Policies",
    "abstract": "           School districts across the United States (US) play a pivotal role in shaping access to quality education through their student assignment policies -- most prominently, school attendance boundaries. Community engagement processes for changing such policies, however, are often opaque, cumbersome, and highly polarizing -- hampering equitable access to quality schools in ways that can perpetuate disparities in future life outcomes. In this paper, we describe a collaboration with a large US public school district serving nearly 150,000 students to design and evaluate a new sociotechnical system, \"BoundarEase\", for fostering more constructive community engagement around changing school attendance boundaries. Through a formative study with 16 community members, we first identify several frictions in existing community engagement processes, like individualistic over collective thinking; a failure to understand and empathize with the different ways policies might impact other community members; and challenges in understanding the impacts of boundary changes. These frictions inspire the design and development of BoundarEase, a web platform that allows community members to explore and offer feedback on potential boundaries. A user study with 12 community members reveals that BoundarEase prompts reflection among community members on how policies might impact families beyond their own, and increases transparency around the details of policy proposals. Our paper offers education researchers insights into the challenges and opportunities involved in community engagement for designing student assignment policies; human-computer interaction researchers a case study of how new sociotechnical systems might help mitigate polarization in local policymaking; and school districts a practical tool they might use to facilitate community engagement to foster more equitable student assignment policies.         ",
    "url": "https://arxiv.org/abs/2503.08543",
    "authors": [
      "Cassandra Overney",
      "Cassandra Moe",
      "Alvin Chang",
      "Nabeel Gillani"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2503.08549",
    "title": "Graph of AI Ideas: Leveraging Knowledge Graphs and LLMs for AI Research Idea Generation",
    "abstract": "           Reading relevant scientific papers and analyzing research development trends is a critical step in generating new scientific ideas. However, the rapid increase in the volume of research literature and the complex citation relationships make it difficult for researchers to quickly analyze and derive meaningful research trends. The development of large language models (LLMs) has provided a novel approach for automatically summarizing papers and generating innovative research ideas. However, existing paper-based idea generation methods either simply input papers into LLMs via prompts or form logical chains of creative development based on citation relationships, without fully exploiting the semantic information embedded in these citations. Inspired by knowledge graphs and human cognitive processes, we propose a framework called the Graph of AI Ideas (GoAI) for the AI research field, which is dominated by open-access papers. This framework organizes relevant literature into entities within a knowledge graph and summarizes the semantic information contained in citations into relations within the graph. This organization effectively reflects the relationships between two academic papers and the advancement of the AI research field. Such organization aids LLMs in capturing the current progress of research, thereby enhancing their creativity. Experimental results demonstrate the effectiveness of our approach in generating novel, clear, and effective research ideas.         ",
    "url": "https://arxiv.org/abs/2503.08549",
    "authors": [
      "Xian Gao",
      "Zongyun Zhang",
      "Mingye Xie",
      "Ting Liu",
      "Yuzhuo Fu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.08551",
    "title": "Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs",
    "abstract": "           The difficulty of multiple-choice questions (MCQs) is a crucial factor for educational assessments. Predicting MCQ difficulty is challenging since it requires understanding both the complexity of reaching the correct option and the plausibility of distractors, i.e., incorrect options. In this paper, we propose a novel, two-stage method to predict the difficulty of MCQs. First, to better estimate the complexity of each MCQ, we use large language models (LLMs) to augment the reasoning steps required to reach each option. We use not just the MCQ itself but also these reasoning steps as input to predict the difficulty. Second, to capture the plausibility of distractors, we sample knowledge levels from a distribution to account for variation among students responding to the MCQ. This setup, inspired by item response theory (IRT), enable us to estimate the likelihood of students selecting each (both correct and incorrect) option. We align these predictions with their ground truth values, using a Kullback-Leibler (KL) divergence-based regularization objective, and use estimated likelihoods to predict MCQ difficulty. We evaluate our method on two real-world \\emph{math} MCQ and response datasets with ground truth difficulty values estimated using IRT. Experimental results show that our method outperforms all baselines, up to a 28.3\\% reduction in mean squared error and a 34.6\\% improvement in the coefficient of determination. We also qualitatively discuss how our novel method results in higher accuracy in predicting MCQ difficulty.         ",
    "url": "https://arxiv.org/abs/2503.08551",
    "authors": [
      "Wanyong Feng",
      "Peter Tran",
      "Stephen Sireci",
      "Andrew Lan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08558",
    "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies",
    "abstract": "           Recent years have witnessed impressive robotic manipulation systems driven by advances in imitation learning and generative modeling, such as diffusion- and flow-based approaches. As robot policy performance increases, so does the complexity and time horizon of achievable tasks, inducing unexpected and diverse failure modes that are difficult to predict a priori. To enable trustworthy policy deployment in safety-critical human environments, reliable runtime failure detection becomes important during policy inference. However, most existing failure detection approaches rely on prior knowledge of failure modes and require failure data during training, which imposes a significant challenge in practicality and scalability. In response to these limitations, we present FAIL-Detect, a modular two-stage approach for failure detection in imitation learning-based robotic manipulation. To accurately identify failures from successful training data alone, we frame the problem as sequential out-of-distribution (OOD) detection. We first distill policy inputs and outputs into scalar signals that correlate with policy failures and capture epistemic uncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile framework for uncertainty quantification with statistical guarantees. Empirically, we thoroughly investigate both learned and post-hoc scalar signal candidates on diverse robotic manipulation tasks. Our experiments show learned signals to be mostly consistently effective, particularly when using our novel flow-based density estimator. Furthermore, our method detects failures more accurately and faster than state-of-the-art (SOTA) failure detection baselines. These results highlight the potential of FAIL-Detect to enhance the safety and reliability of imitation learning-based robotic systems as they progress toward real-world deployment.         ",
    "url": "https://arxiv.org/abs/2503.08558",
    "authors": [
      "Chen Xu",
      "Tony Khuong Nguyen",
      "Emma Dixon",
      "Christopher Rodriguez",
      "Patrick Miller",
      "Robert Lee",
      "Paarth Shah",
      "Rares Ambrus",
      "Haruki Nishimura",
      "Masha Itkina"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08568",
    "title": "Privacy Law Enforcement Under Centralized Governance: A Qualitative Analysis of Four Years' Special Privacy Rectification Campaigns",
    "abstract": "           In recent years, major privacy laws like the GDPR have brought about positive changes. However, challenges remain in enforcing the laws, particularly due to under-resourced regulators facing a large number of potential privacy-violating software applications (apps) and the high costs of investigating them. Since 2019, China has launched a series of privacy enforcement campaigns known as Special Privacy Rectification Campaigns (SPRCs) to address widespread privacy violations in its mobile application (app) ecosystem. Unlike the enforcement of the GDPR, SPRCs are characterized by large-scale privacy reviews and strict sanctions, under the strong control of central authorities. In SPRCs, central government authorities issue administrative orders to mobilize various resources for market-wide privacy reviews of mobile apps. They enforce strict sanctions by requiring privacy-violating apps to rectify issues within a short timeframe or face removal from app stores. While there are a few reports on SPRCs, the effectiveness and potential problems of this campaign-style privacy enforcement approach remain unclear to the community. In this study, we conducted 18 semi-structured interviews with app-related engineers involved in SPRCs to better understand the campaign-style privacy enforcement. Based on the interviews, we reported our findings on a variety of aspects of SPRCs, such as the processes that app engineers regularly follow to achieve privacy compliance in SPRCs, the challenges they encounter, the solutions they adopt to address these challenges, and the impacts of SPRCs, etc. We found that app engineers face a series of challenges in achieving privacy compliance in their apps...         ",
    "url": "https://arxiv.org/abs/2503.08568",
    "authors": [
      "Tao Jing",
      "Yao Li",
      "Jingzhou Ye",
      "Jie Wang",
      "Xueqiang Wang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.08581",
    "title": "MsaMIL-Net: An End-to-End Multi-Scale Aware Multiple Instance Learning Network for Efficient Whole Slide Image Classification",
    "abstract": "           Bag-based Multiple Instance Learning (MIL) approaches have emerged as the mainstream methodology for Whole Slide Image (WSI) classification. However, most existing methods adopt a segmented training strategy, which first extracts features using a pre-trained feature extractor and then aggregates these features through MIL. This segmented training approach leads to insufficient collaborative optimization between the feature extraction network and the MIL network, preventing end-to-end joint optimization and thereby limiting the overall performance of the model. Additionally, conventional methods typically extract features from all patches of fixed size, ignoring the multi-scale observation characteristics of pathologists. This not only results in significant computational resource waste when tumor regions represent a minimal proportion (as in the Camelyon16 dataset) but may also lead the model to suboptimal solutions. To address these limitations, this paper proposes an end-to-end multi-scale WSI classification framework that integrates multi-scale feature extraction with multiple instance learning. Specifically, our approach includes: (1) a semantic feature filtering module to reduce interference from non-lesion areas; (2) a multi-scale feature extraction module to capture pathological information at different levels; and (3) a multi-scale fusion MIL module for global modeling and feature integration. Through an end-to-end training strategy, we simultaneously optimize both the feature extractor and MIL network, ensuring maximum compatibility between them. Experiments were conducted on three cross-center datasets (DigestPath2019, BCNB, and UBC-OCEAN). Results demonstrate that our proposed method outperforms existing state-of-the-art approaches in terms of both accuracy (ACC) and AUC metrics.         ",
    "url": "https://arxiv.org/abs/2503.08581",
    "authors": [
      "Jiangping Wen",
      "Jinyu Wen",
      "Emei Fang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08596",
    "title": "X-Field: A Physically Grounded Representation for 3D X-ray Reconstruction",
    "abstract": "           X-ray imaging is indispensable in medical diagnostics, yet its use is tightly regulated due to potential health risks. To mitigate radiation exposure, recent research focuses on generating novel views from sparse inputs and reconstructing Computed Tomography (CT) volumes, borrowing representations from the 3D reconstruction area. However, these representations originally target visible light imaging that emphasizes reflection and scattering effects, while neglecting penetration and attenuation properties of X-ray imaging. In this paper, we introduce X-Field, the first 3D representation specifically designed for X-ray imaging, rooted in the energy absorption rates across different materials. To accurately model diverse materials within internal structures, we employ 3D ellipsoids with distinct attenuation coefficients. To estimate each material's energy absorption of X-rays, we devise an efficient path partitioning algorithm accounting for complex ellipsoid intersections. We further propose hybrid progressive initialization to refine the geometric accuracy of X-Filed and incorporate material-based optimization to enhance model fitting along material boundaries. Experiments show that X-Field achieves superior visual fidelity on both real-world human organ and synthetic object datasets, outperforming state-of-the-art methods in X-ray Novel View Synthesis and CT Reconstruction.         ",
    "url": "https://arxiv.org/abs/2503.08596",
    "authors": [
      "Feiran Wang",
      "Jiachen Tao",
      "Junyi Wu",
      "Haoxuan Wang",
      "Bin Duan",
      "Kai Wang",
      "Zongxin Yang",
      "Yan Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08600",
    "title": "NSF-SciFy: Mining the NSF Awards Database for Scientific Claims",
    "abstract": "           We present NSF-SciFy, a large-scale dataset for scientific claim extraction derived from the National Science Foundation (NSF) awards database, comprising over 400K grant abstracts spanning five decades. While previous datasets relied on published literature, we leverage grant abstracts which offer a unique advantage: they capture claims at an earlier stage in the research lifecycle before publication takes effect. We also introduce a new task to distinguish between existing scientific claims and aspirational research intentions in this http URL zero-shot prompting with frontier large language models, we jointly extract 114K scientific claims and 145K investigation proposals from 16K grant abstracts in the materials science domain to create a focused subset called NSF-SciFy-MatSci. We use this dataset to evaluate 3 three key tasks: (1) technical to non-technical abstract generation, where models achieve high BERTScore (0.85+ F1); (2) scientific claim extraction, where fine-tuned models outperform base models by 100% relative improvement; and (3) investigation proposal extraction, showing 90%+ improvement with fine-tuning. We introduce novel LLM-based evaluation metrics for robust assessment of claim/proposal extraction quality. As the largest scientific claim dataset to date -- with an estimated 2.8 million claims across all STEM disciplines funded by the NSF -- NSF-SciFy enables new opportunities for claim verification and meta-scientific research. We publicly release all datasets, trained models, and evaluation code to facilitate further research.         ",
    "url": "https://arxiv.org/abs/2503.08600",
    "authors": [
      "Delip Rao",
      "Weiqiu You",
      "Eric Wong",
      "Chris Callison-Burch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.08633",
    "title": "How Does Overparameterization Affect Machine Unlearning of Deep Neural Networks?",
    "abstract": "           Machine unlearning is the task of updating a trained model to forget specific training data without retraining from scratch. In this paper, we investigate how unlearning of deep neural networks (DNNs) is affected by the model parameterization level, which corresponds here to the DNN width. We define validation-based tuning for several unlearning methods from the recent literature, and show how these methods perform differently depending on (i) the DNN parameterization level, (ii) the unlearning goal (unlearned data privacy or bias removal), (iii) whether the unlearning method explicitly uses the unlearned examples. Our results show that unlearning excels on overparameterized models, in terms of balancing between generalization and achieving the unlearning goal; although for bias removal this requires the unlearning method to use the unlearned examples. We further elucidate our error-based analysis by measuring how much the unlearning changes the classification decision regions in the proximity of the unlearned examples, and avoids changing them elsewhere. By this we show that the unlearning success for overparameterized models stems from the ability to delicately change the model functionality in small regions in the input space while keeping much of the model functionality unchanged.         ",
    "url": "https://arxiv.org/abs/2503.08633",
    "authors": [
      "Gal Alon",
      "Yehuda Dar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08636",
    "title": "Birds look like cars: Adversarial analysis of intrinsically interpretable deep learning",
    "abstract": "           A common belief is that intrinsically interpretable deep learning models ensure a correct, intuitive understanding of their behavior and offer greater robustness against accidental errors or intentional manipulation. However, these beliefs have not been comprehensively verified, and growing evidence casts doubt on them. In this paper, we highlight the risks related to overreliance and susceptibility to adversarial manipulation of these so-called \"intrinsically (aka inherently) interpretable\" models by design. We introduce two strategies for adversarial analysis with prototype manipulation and backdoor attacks against prototype-based networks, and discuss how concept bottleneck models defend against these attacks. Fooling the model's reasoning by exploiting its use of latent prototypes manifests the inherent uninterpretability of deep neural networks, leading to a false sense of security reinforced by a visual confirmation bias. The reported limitations of prototype-based networks put their trustworthiness and applicability into question, motivating further work on the robustness and alignment of (deep) interpretable models.         ",
    "url": "https://arxiv.org/abs/2503.08636",
    "authors": [
      "Hubert Baniecki",
      "Przemyslaw Biecek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08639",
    "title": "GBlobs: Explicit Local Structure via Gaussian Blobs for Improved Cross-Domain LiDAR-based 3D Object Detection",
    "abstract": "           LiDAR-based 3D detectors need large datasets for training, yet they struggle to generalize to novel domains. Domain Generalization (DG) aims to mitigate this by training detectors that are invariant to such domain shifts. Current DG approaches exclusively rely on global geometric features (point cloud Cartesian coordinates) as input features. Over-reliance on these global geometric features can, however, cause 3D detectors to prioritize object location and absolute position, resulting in poor cross-domain performance. To mitigate this, we propose to exploit explicit local point cloud structure for DG, in particular by encoding point cloud neighborhoods with Gaussian blobs, GBlobs. Our proposed formulation is highly efficient and requires no additional parameters. Without any bells and whistles, simply by integrating GBlobs in existing detectors, we beat the current state-of-the-art in challenging single-source DG benchmarks by over 21 mAP (Waymo->KITTI), 13 mAP (KITTI->Waymo), and 12 mAP (nuScenes->KITTI), without sacrificing in-domain performance. Additionally, GBlobs demonstrate exceptional performance in multi-source DG, surpassing the current state-of-the-art by 17, 12, and 5 mAP on Waymo, KITTI, and ONCE, respectively.         ",
    "url": "https://arxiv.org/abs/2503.08639",
    "authors": [
      "Du\u0161an Mali\u0107",
      "Christian Fruhwirth-Reisinger",
      "Samuel Schulter",
      "Horst Possegger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08648",
    "title": "GraphSense: Graph Embedding Based Code Suggestion Framework",
    "abstract": "           Code suggestions have become an integral part of IDEs and developers use code suggestions generated by IDEs all the time. These code suggestions are mostly for calling a method of an object or for using a function of a library and not for possible next line of the code. GPT based models are too slow or resource intensive for real-time code suggestions in local environments. As a solution to this GraphSense was introduced which provide code suggestions with minimum amount of resource usage in real-time.         ",
    "url": "https://arxiv.org/abs/2503.08648",
    "authors": [
      "H.R Navod Thisura Peiris"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.08665",
    "title": "REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder",
    "abstract": "           We present a novel perspective on learning video embedders for generative modeling: rather than requiring an exact reproduction of an input video, an effective embedder should focus on synthesizing visually plausible reconstructions. This relaxed criterion enables substantial improvements in compression ratios without compromising the quality of downstream generative models. Specifically, we propose replacing the conventional encoder-decoder video embedder with an encoder-generator framework that employs a diffusion transformer (DiT) to synthesize missing details from a compact latent space. Therein, we develop a dedicated latent conditioning module to condition the DiT decoder on the encoded video latent embedding. Our experiments demonstrate that our approach enables superior encoding-decoding performance compared to state-of-the-art methods, particularly as the compression ratio increases. To demonstrate the efficacy of our approach, we report results from our video embedders achieving a temporal compression ratio of up to 32x (8x higher than leading video embedders) and validate the robustness of this ultra-compact latent space for text-to-video generation, providing a significant efficiency boost in latent diffusion model training and inference.         ",
    "url": "https://arxiv.org/abs/2503.08665",
    "authors": [
      "Yitian Zhang",
      "Long Mai",
      "Aniruddha Mahapatra",
      "David Bourgin",
      "Yicong Hong",
      "Jonah Casebeer",
      "Feng Liu",
      "Yun Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08673",
    "title": "Keypoint Detection and Description for Raw Bayer Images",
    "abstract": "           Keypoint detection and local feature description are fundamental tasks in robotic perception, critical for applications such as SLAM, robot localization, feature matching, pose estimation, and 3D mapping. While existing methods predominantly operate on RGB images, we propose a novel network that directly processes raw images, bypassing the need for the Image Signal Processor (ISP). This approach significantly reduces hardware requirements and memory consumption, which is crucial for robotic vision systems. Our method introduces two custom-designed convolutional kernels capable of performing convolutions directly on raw images, preserving inter-channel information without converting to RGB. Experimental results show that our network outperforms existing algorithms on raw images, achieving higher accuracy and stability under large rotations and scale variations. This work represents the first attempt to develop a keypoint detection and feature description network specifically for raw images, offering a more efficient solution for resource-constrained environments.         ",
    "url": "https://arxiv.org/abs/2503.08673",
    "authors": [
      "Jiakai Lin",
      "Jinchang Zhang",
      "Guoyu Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.07619",
    "title": "Physics- and data-driven Active Learning of neural network representations for free energy functions of materials from statistical mechanics",
    "abstract": "           Accurate free energy representations are crucial for understanding phase dynamics in materials. We employ a scale-bridging approach to incorporate atomistic information into our free energy model by training a neural network on DFT-informed Monte Carlo data. To optimize sampling in the high-dimensional Monte Carlo space, we present an Active Learning framework that integrates space-filling sampling, uncertainty-based sampling, and physics-informed sampling. Additionally, our approach includes methods such as hyperparameter tuning, dynamic sampling, and novelty enforcement. These strategies can be combined to reduce MSE,either globally or in targeted regions of interest,while minimizing the number of required data points. The framework introduced here is broadly applicable to Monte Carlo sampling of a range of materials systems.         ",
    "url": "https://arxiv.org/abs/2503.07619",
    "authors": [
      "Jamie Holber",
      "Krishna Garikipati"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.07687",
    "title": "Personalized Convolutional Dictionary Learning of Physiological Time Series",
    "abstract": "           Human physiological signals tend to exhibit both global and local structures: the former are shared across a population, while the latter reflect inter-individual variability. For instance, kinetic measurements of the gait cycle during locomotion present common characteristics, although idiosyncrasies may be observed due to biomechanical disposition or pathology. To better represent datasets with local-global structure, this work extends Convolutional Dictionary Learning (CDL), a popular method for learning interpretable representations, or dictionaries, of time-series data. In particular, we propose Personalized CDL (PerCDL), in which a local dictionary models local information as a personalized spatiotemporal transformation of a global dictionary. The transformation is learnable and can combine operations such as time warping and rotation. Formal computational and statistical guarantees for PerCDL are provided and its effectiveness on synthetic and real human locomotion data is demonstrated.         ",
    "url": "https://arxiv.org/abs/2503.07687",
    "authors": [
      "Axel Roques",
      "Samuel Gruffaz",
      "Kyurae Kim",
      "Alain Oliviero-Durmus",
      "Laurent Oudre"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2503.07736",
    "title": "Uncertainty quantification and posterior sampling for network reconstruction",
    "abstract": "           Network reconstruction is the task of inferring the unseen interactions between elements of a system, based only on their behavior or dynamics. This inverse problem is in general ill-posed, and admits many solutions for the same observation. Nevertheless, the vast majority of statistical methods proposed for this task -- formulated as the inference of a graphical generative model -- can only produce a ``point estimate,'' i.e. a single network considered the most likely. In general, this can give only a limited characterization of the reconstruction, since uncertainties and competing answers cannot be conveyed, even if their probabilities are comparable, while being structurally different. In this work we present an efficient MCMC algorithm for sampling from posterior distributions of reconstructed networks, which is able to reveal the full population of answers for a given reconstruction problem, weighted according to their plausibilities. Our algorithm is general, since it does not rely on specific properties of particular generative models, and is specially suited for the inference of large and sparse networks, since in this case an iteration can be performed in time $O(N\\log^2 N)$ for a network of $N$ nodes, instead of $O(N^2)$, as would be the case for a more naive approach. We demonstrate the suitability of our method in providing uncertainties and consensus of solutions (which provably increases the reconstruction accuracy) in a variety of synthetic and empirical cases.         ",
    "url": "https://arxiv.org/abs/2503.07736",
    "authors": [
      "Tiago P. Peixoto"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2503.07811",
    "title": "A primer on optimal transport for causal inference with observational data",
    "abstract": "           The theory of optimal transportation has developed into a powerful and elegant framework for comparing probability distributions, with wide-ranging applications in all areas of science. The fundamental idea of analyzing probabilities by comparing their underlying state space naturally aligns with the core idea of causal inference, where understanding and quantifying counterfactual states is paramount. Despite this intuitive connection, explicit research at the intersection of optimal transport and causal inference is only beginning to develop. Yet, many foundational models in causal inference have implicitly relied on optimal transport principles for decades, without recognizing the underlying connection. Therefore, the goal of this review is to offer an introduction to the surprisingly deep existing connections between optimal transport and the identification of causal effects with observational data -- where optimal transport is not just a set of potential tools, but actually builds the foundation of model assumptions. As a result, this review is intended to unify the language and notation between different areas of statistics, mathematics, and econometrics, by pointing out these existing connections, and to explore novel problems and directions for future work in both areas derived from this realization.         ",
    "url": "https://arxiv.org/abs/2503.07811",
    "authors": [
      "Florian F Gunsilius"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)",
      "Econometrics (econ.EM)"
    ]
  },
  {
    "id": "arXiv:2503.07824",
    "title": "Pure Exploration with Feedback Graphs",
    "abstract": "           We study the sample complexity of pure exploration in an online learning problem with a feedback graph. This graph dictates the feedback available to the learner, covering scenarios between full-information, pure bandit feedback, and settings with no feedback on the chosen action. While variants of this problem have been investigated for regret minimization, no prior work has addressed the pure exploration setting, which is the focus of our study. We derive an instance-specific lower bound on the sample complexity of learning the best action with fixed confidence, even when the feedback graph is unknown and stochastic, and present unidentifiability results for Bernoulli rewards. Additionally, our findings reveal how the sample complexity scales with key graph-dependent quantities. Lastly, we introduce TaS-FG (Track and Stop for Feedback Graphs), an asymptotically optimal algorithm, and demonstrate its efficiency across different graph configurations.         ",
    "url": "https://arxiv.org/abs/2503.07824",
    "authors": [
      "Alessio Russo",
      "Yichen Song",
      "Aldo Pacchiano"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.07924",
    "title": "Multi-Objective Routing Optimization Using Coherent Ising Machine in Wireless Multihop Networks",
    "abstract": "           Multi-objective combinatorial optimization in wireless communication networks is a challenging task, particularly for large-scale and diverse topologies. Recent advances in quantum computing offer promising solutions for such problems. Coherent Ising Machines (CIM), a quantum-inspired algorithm, leverages the quantum properties of coherent light, enabling faster convergence to the ground state. This paper applies CIM to multi-objective routing optimization in wireless multi-hop networks. We formulate the routing problem as a Quadratic Unconstrained Binary Optimization (QUBO) problem and map it onto an Ising model, allowing CIM to solve it. CIM demonstrates strong scalability across diverse network topologies without requiring topology-specific adjustments, overcoming the limitations of traditional quantum algorithms like Quantum Approximate Optimization Algorithm (QAOA) and Variational Quantum Eigensolver (VQE). Our results show that CIM provides feasible and near-optimal solutions for networks containing hundreds of nodes and thousands of edges. Additionally, a complexity analysis highlights CIM's increasing efficiency as network size grows         ",
    "url": "https://arxiv.org/abs/2503.07924",
    "authors": [
      "Yu-Xuan Lin",
      "Chu-Yao Xu",
      "Chuan Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2503.07994",
    "title": "A Neural Symbolic Model for Space Physics",
    "abstract": "           In this study, we unveil a new AI model, termed PhyE2E, to discover physical formulas through symbolic regression. PhyE2E simplifies symbolic regression by decomposing it into sub-problems using the second-order derivatives of an oracle neural network, and employs a transformer model to translate data into symbolic formulas in an end-to-end manner. The resulting formulas are refined through Monte-Carlo Tree Search and Genetic Programming. We leverage a large language model to synthesize extensive symbolic expressions resembling real physics, and train the model to recover these formulas directly from data. A comprehensive evaluation reveals that PhyE2E outperforms existing state-of-the-art approaches, delivering superior symbolic accuracy, precision in data fitting, and consistency in physical units. We deployed PhyE2E to five applications in space physics, including the prediction of sunspot numbers, solar rotational angular velocity, emission line contribution functions, near-Earth plasma pressure, and lunar-tide plasma signals. The physical formulas generated by AI demonstrate a high degree of accuracy in fitting the experimental data from satellites and astronomical telescopes. We have successfully upgraded the formula proposed by NASA in 1993 regarding solar activity, and for the first time, provided the explanations for the long cycle of solar activity in an explicit form. We also found that the decay of near-Earth plasma pressure is proportional to r^2 to Earth, where subsequent mathematical derivations are consistent with satellite data from another independent study. Moreover, we found physical formulas that can describe the relationships between emission lines in the extreme ultraviolet spectrum of the Sun, temperatures, electron densities, and magnetic fields. The formula obtained is consistent with the properties that physicists had previously hypothesized it should possess.         ",
    "url": "https://arxiv.org/abs/2503.07994",
    "authors": [
      "Jie Ying",
      "Haowei Lin",
      "Chao Yue",
      "Yajie Chen",
      "Chao Xiao",
      "Quanqi Shi",
      "Yitao Liang",
      "Shing-Tung Yau",
      "Yuan Zhou",
      "Jianzhu Ma"
    ],
    "subjectives": [
      "Solar and Stellar Astrophysics (astro-ph.SR)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Artificial Intelligence (cs.AI)",
      "Space Physics (physics.space-ph)"
    ]
  },
  {
    "id": "arXiv:2503.08088",
    "title": "Secure domination in $P_5$-free graphs",
    "abstract": "           A dominating set of a graph $G$ is a set $S \\subseteq V(G)$ such that every vertex in $V(G) \\setminus S$ has a neighbor in $S$, where two vertices are neighbors if they are adjacent. A secure dominating set of $G$ is a dominating set $S$ of $G$ with the additional property that for every vertex $v \\in V(G) \\setminus S$, there exists a neighbor $u$ of $v$ in $S$ such that $(S \\setminus \\{u\\}) \\cup \\{v\\}$ is a dominating set of $G$. The secure domination number of $G$, denoted by $\\gamma_s(G)$, is the minimum cardinality of a secure dominating set of $G$. We prove that if $G$ is a $P_5$-free graph, then $\\gamma_s(G) \\le \\frac{3}{2}\\alpha(G)$, where $\\alpha(G)$ denotes the independence number of $G$. We further show that if $G$ is a connected $(P_5, H)$-free graph for some $H \\in \\{ P_3 \\cup P_1, K_2 \\cup 2K_1, ~\\text{paw},~ C_4\\}$, then $\\gamma_s(G)\\le \\max\\{3,\\alpha(G)\\}$. We also show that if $G$ is a $(P_3 \\cup P_2)$-free graph, then $\\gamma_s(G)\\le \\alpha(G)+1$.         ",
    "url": "https://arxiv.org/abs/2503.08088",
    "authors": [
      "Paras Vinubhai Maniya",
      "Uttam K. Gupta",
      "Michael A. Henning",
      "Dinabandhu Pradhan"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2503.08103",
    "title": "Median Consensus Embedding for Dimensionality Reduction",
    "abstract": "           This study proposes median consensus embedding (MCE) to address variability in low-dimensional embeddings caused by random initialization in dimensionality reduction techniques such as t-distributed stochastic neighbor embedding. MCE is defined as the geometric median of multiple embeddings. By assuming multiple embeddings as independent and identically distributed random samples and applying large deviation theory, we prove that MCE achieves consistency at an exponential rate. Furthermore, we develop a practical algorithm to implement MCE by constructing a distance function between embeddings based on the Frobenius norm of the pairwise distance matrix of data points. Application to real-world data demonstrates that MCE converges rapidly and significantly reduces instability. These results confirm that MCE effectively mitigates random initialization issues in embedding methods.         ",
    "url": "https://arxiv.org/abs/2503.08103",
    "authors": [
      "Yui Tomo",
      "Daisuke Yoneoka"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08251",
    "title": "MT-NAM: An Efficient and Adaptive Model for Epileptic Seizure Detection",
    "abstract": "           Enhancing the accuracy and efficiency of machine learning algorithms employed in neural interface systems is crucial for advancing next-generation intelligent therapeutic devices. However, current systems often utilize basic machine learning models that do not fully exploit the natural structure of brain signals. Additionally, existing learning models used for neural signal processing often demonstrate low speed and efficiency during inference. To address these challenges, this study introduces Micro Tree-based NAM (MT-NAM), a distilled model based on the recently proposed Neural Additive Models (NAM). The MT-NAM achieves a remarkable 100$\\times$ improvement in inference speed compared to standard NAM, without compromising accuracy. We evaluate our approach on the CHB-MIT scalp EEG dataset, which includes recordings from 24 patients with varying numbers of sessions and seizures. NAM achieves an 85.3\\% window-based sensitivity and 95\\% specificity. Interestingly, our proposed MT-NAM shows only a 2\\% reduction in sensitivity compared to the original NAM. To regain this sensitivity, we utilize a test-time template adjuster (T3A) as an update mechanism, enabling our model to achieve higher sensitivity during test time by accommodating transient shifts in neural signals. With this online update approach, MT-NAM achieves the same sensitivity as the standard NAM while achieving approximately 50$\\times$ acceleration in inference speed.         ",
    "url": "https://arxiv.org/abs/2503.08251",
    "authors": [
      "Arshia Afzal",
      "Volkan Cevher",
      "Mahsa Shoaran"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08579",
    "title": "Sparsity-Induced Global Matrix Autoregressive Model with Auxiliary Network Data",
    "abstract": "           Jointly modeling and forecasting economic and financial variables across a large set of countries has long been a significant challenge. Two primary approaches have been utilized to address this issue: the vector autoregressive model with exogenous variables (VARX) and the matrix autoregression (MAR). The VARX model captures domestic dependencies, but treats variables exogenous to represent global factors driven by international trade. In contrast, the MAR model simultaneously considers variables from multiple countries but ignores the trade network. In this paper, we propose an extension of the MAR model that achieves these two aims at once, i.e., studying both international dependencies and the impact of the trade network on the global economy. Additionally, we introduce a sparse component to the model to differentiate between systematic and idiosyncratic cross-predictability. To estimate the model parameters, we propose both a likelihood estimation method and a bias-corrected alternating minimization version. We provide theoretical and empirical analyses of the model's properties, alongside presenting intriguing economic insights derived from our findings.         ",
    "url": "https://arxiv.org/abs/2503.08579",
    "authors": [
      "Sanyou Wu",
      "Dan Yang",
      "Yan Xu",
      "Long Feng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2503.08610",
    "title": "Hierarchical autoregressive neural networks in three-dimensional statistical system",
    "abstract": "           Autoregressive Neural Networks (ANN) have been recently proposed as a mechanism to improve the efficiency of Monte Carlo algorithms for several spin systems. The idea relies on the fact that the total probability of a configuration can be factorized into conditional probabilities of each spin, which in turn can be approximated by a neural network. Once trained, the ANNs can be used to sample configurations from the approximated probability distribution and to evaluate explicitly this probability for a given configuration. It has also been observed that such conditional probabilities give access to information-theoretic observables such as mutual information or entanglement entropy. So far, these methods have been applied to two-dimensional statistical systems or one-dimensional quantum systems. In this paper, we describe a generalization of the hierarchical algorithm to three spatial dimensions and study its performance on the example of the Ising model. We discuss the efficiency of the training and also describe the scaling with the system's dimensionality by comparing results for two- and three-dimensional Ising models with the same number of spins. Finally, we provide estimates of thermodynamical observables for the three-dimensional Ising model, such as the entropy and free energy in a range of temperatures across the phase transition.         ",
    "url": "https://arxiv.org/abs/2503.08610",
    "authors": [
      "Piotr Bia\u0142as",
      "Vaibhav Chahar",
      "Piotr Korcyl",
      "Tomasz Stebel",
      "Mateusz Winiarski",
      "Dawid Zapolski"
    ],
    "subjectives": [
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Lattice (hep-lat)"
    ]
  },
  {
    "id": "arXiv:2301.01918",
    "title": "Plant species richness prediction from DESIS hyperspectral data: A comparison study on feature extraction procedures and regression models",
    "abstract": "           The diversity of terrestrial vascular plants plays a key role in maintaining the stability and productivity of ecosystems. Airborne hyperspectral imaging has shown promise for measuring plant diversity remotely, but to operationalise these efforts over large regions we need to advance satellite-based alternatives. The advanced spectral and spatial specification of the recently launched DESIS (the DLR Earth Sensing Imaging Spectrometer) instrument provides a unique opportunity to test the potential for monitoring plant species diversity with spaceborne hyperspectral data. This study provides a quantitative assessment on the ability of DESIS hyperspectral data for predicting plant species richness in two different habitat types in southeast Australia. Spectral features were first extracted from the DESIS spectra, then regressed against on-ground estimates of plant species richness, with a two-fold cross validation scheme to assess the predictive performance. We tested and compared the effectiveness of Principal Component Analysis (PCA), Canonical Correlation Analysis (CCA), and Partial Least Squares analysis (PLS) for feature extraction, and Kernel Ridge Regression (KRR), Gaussian Process Regression (GPR), and Random Forest Regression (RFR) for species richness prediction. The best prediction results were $r=0.76$ and $\\text{RMSE}=5.89$ for the Southern Tablelands region, and $r=0.68$ and $\\text{RMSE}=5.95$ for the Snowy Mountains region. Relative importance analysis for the DESIS spectral bands showed that the red-edge, red, and blue spectral regions were more important for predicting plant species richness than the green bands and the near-infrared bands beyond red-edge. We also found that the DESIS hyperspectral data performed better than Sentinel-2 multispectral data in the prediction of plant species richness.         ",
    "url": "https://arxiv.org/abs/2301.01918",
    "authors": [
      "Yiqing Guo",
      "Karel Mokany",
      "Cindy Ong",
      "Peyman Moghadam",
      "Simon Ferrier",
      "Shaun R. Levick"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Populations and Evolution (q-bio.PE)"
    ]
  },
  {
    "id": "arXiv:2302.05104",
    "title": "Monte Carlo Neural PDE Solver for Learning PDEs via Probabilistic Representation",
    "abstract": "           In scenarios with limited available data, training the function-to-function neural PDE solver in an unsupervised manner is essential. However, the efficiency and accuracy of existing methods are constrained by the properties of numerical algorithms, such as finite difference and pseudo-spectral methods, integrated during the training stage. These methods necessitate careful spatiotemporal discretization to achieve reasonable accuracy, leading to significant computational challenges and inaccurate simulations, particularly in cases with substantial spatiotemporal variations. To address these limitations, we propose the Monte Carlo Neural PDE Solver (MCNP Solver) for training unsupervised neural solvers via the PDEs' probabilistic representation, which regards macroscopic phenomena as ensembles of random particles. Compared to other unsupervised methods, MCNP Solver naturally inherits the advantages of the Monte Carlo method, which is robust against spatiotemporal variations and can tolerate coarse step size. In simulating the trajectories of particles, we employ Heun's method for the convection process and calculate the expectation via the probability density function of neighbouring grid points during the diffusion process. These techniques enhance accuracy and circumvent the computational issues associated with Monte Carlo sampling. Our numerical experiments on convection-diffusion, Allen-Cahn, and Navier-Stokes equations demonstrate significant improvements in accuracy and efficiency compared to other unsupervised baselines. The source code will be publicly available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2302.05104",
    "authors": [
      "Rui Zhang",
      "Qi Meng",
      "Rongchan Zhu",
      "Yue Wang",
      "Wenlei Shi",
      "Shihua Zhang",
      "Zhi-Ming Ma",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2302.10469",
    "title": "ApproxABFT: Approximate Algorithm-Based Fault Tolerance for Neural Network Processing",
    "abstract": "           With the growing adoption of neural network models in safety-critical applications such as autonomous driving and robotics, reliability has become a critical metric alongside performance and energy efficiency. Algorithm-based fault tolerance (ABFT) strategies, designed atop standard chip architectures, are both cost-effective and adaptable to different architectures, making them particularly attractive. However, traditional ABFT relies on precise error metrics, triggering error recovery procedures even for minor computational deviations. These minor errors often do not impact the accuracy of the neural network model due to the inherent fault tolerance. To address this inefficiency, we propose an approximate ABFT approach, called ApproxABFT, which initiates error recovery only when computational errors are significant. This approach avoids unnecessary recovery procedures, streamlines the error recovery process, and focuses on correcting impactful errors, ultimately enhancing recovery quality. Additionally, ApproxABFT incorporates a fine-grained blocking strategy to smooth error sensitivity across layers within neural network models. Experimental results demonstrate that ApproxABFT reduces the computing overhead by 67.83\\% and improves the tolerable bit error rate by an order of magnitude on average compared to classical accurate ABFT.         ",
    "url": "https://arxiv.org/abs/2302.10469",
    "authors": [
      "Xinghua Xue",
      "Cheng Liu",
      "Feng Min",
      "Tao Luo",
      "Yinhe Han"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2305.18226",
    "title": "HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis",
    "abstract": "           As the use of Large Language Models (LLMs) in text generation tasks proliferates, concerns arise over their potential to compromise academic integrity. The education sector currently tussles with distinguishing student-authored homework assignments from AI-generated ones. This paper addresses the challenge by introducing HowkGPT, designed to identify homework assignments generated by AI. HowkGPT is built upon a dataset of academic assignments and accompanying metadata [17] and employs a pretrained LLM to compute perplexity scores for student-authored and ChatGPT-generated responses. These scores then assist in establishing a threshold for discerning the origin of a submitted assignment. Given the specificity and contextual nature of academic work, HowkGPT further refines its analysis by defining category-specific thresholds derived from the metadata, enhancing the precision of the detection. This study emphasizes the critical need for effective strategies to uphold academic integrity amidst the growing influence of LLMs and provides an approach to ensuring fair and accurate grading in educational institutions.         ",
    "url": "https://arxiv.org/abs/2305.18226",
    "authors": [
      "Christoforos Vasilatos",
      "Manaar Alam",
      "Talal Rahwan",
      "Yasir Zaki",
      "Michail Maniatakos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.03617",
    "title": "RouteKG: A knowledge graph-based framework for route prediction on road networks",
    "abstract": "           Short-term route prediction on road networks allows us to anticipate the future trajectories of road users, enabling various applications ranging from dynamic traffic control to personalized navigation. Despite recent advances in this area, existing methods focus primarily on learning sequential transition patterns, neglecting the inherent spatial relations in road networks that can affect human routing decisions. To fill this gap, this paper introduces RouteKG, a novel Knowledge Graph-based framework for route prediction. Specifically, we construct a Knowledge Graph on the road network to encode spatial relations, especially moving directions that are crucial for human navigation. Moreover, an n-ary tree-based algorithm is introduced to efficiently generate top-K routes in a batch mode, enhancing computational efficiency. To further optimize the prediction performance, a rank refinement module is incorporated to fine-tune the candidate route rankings. The model performance is evaluated using two real-world vehicle trajectory datasets from two Chinese cities under various practical scenarios. The results demonstrate a significant improvement in accuracy over baseline methods. We further validate the proposed method by utilizing the pre-trained model as a simulator for real-time traffic flow estimation at the link level. RouteKG holds great potential for transforming vehicle navigation, traffic management, and a variety of intelligent transportation tasks, playing a crucial role in advancing the core foundation of intelligent and connected urban systems.         ",
    "url": "https://arxiv.org/abs/2310.03617",
    "authors": [
      "Yihong Tang",
      "Zhan Zhao",
      "Weipeng Deng",
      "Shuyu Lei",
      "Yuebing Liang",
      "Zhenliang Ma"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2311.10248",
    "title": "Identifying the Truth of Global Model: A Generic Solution to Defend Against Byzantine and Backdoor Attacks in Federated Learning (full version)",
    "abstract": "           Federated Learning (FL) enables multiple parties to train machine learning models collaboratively without sharing the raw training data. However, the federated nature of FL enables malicious clients to influence a trained model by injecting error model updates via Byzantine or backdoor attacks. To detect malicious model updates, a typical approach is to measure the distance between each model update and a \\textit{ground-truth model update}. To find such \\textit{ground-truth model updates}, existing defenses either require a benign root dataset on the server (e.g., FLTrust) or simply use trimmed mean or median as the threshold for clipping (e.g., FLAME). However, such benign root datasets are impractical, and the trimmed mean or median may also eliminate contributions from these underrepresented datasets. In this paper, we propose a generic solution, namely FedTruth, to defend against model poisoning attacks in FL, where the \\textit{ground-truth model update} (i.e., the global model update) will be estimated among all the model updates with dynamic aggregation weights. Specifically, FedTruth does not have specific assumptions on the benign or malicious data distribution or access to a benign root dataset. Moreover, FedTruth considers the potential contributions from all benign clients. Our empirical results show that FedTruth can reduce the impacts of poisoned model updates against both Byzantine and backdoor attacks, and is also efficient in large-scale FL systems.         ",
    "url": "https://arxiv.org/abs/2311.10248",
    "authors": [
      "Sheldon C. Ebron",
      "Meiying Zhang",
      "Kan Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2311.11091",
    "title": "Deep Tensor Network",
    "abstract": "           We introduce the Deep Tensor Network, a novel framework that integrates tensor-based operations into the attention mechanism, thereby enhancing both the expressivity and computational efficiency of deep neural networks. Our approach leverages the algebraic structure of tensor products to generalize the conventional dot-product attention and to formulate new operators, namely, Tensor Attention and Tensor Interaction, which capture higher-order token dependencies. Through rigorous theoretical analysis based on the universal properties of tensor products, we demonstrate that our framework not only improves efficiency by reducing computational complexity but also offers a principled method for modeling complex interactions in sequential data. Empirical evaluations further substantiate that the proposed deep tensor network can serve as a robust building block for advancing state-of-the-art performance in various deep learning tasks.         ",
    "url": "https://arxiv.org/abs/2311.11091",
    "authors": [
      "Xuantao Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2312.08177",
    "title": "Advanced Image Segmentation Techniques for Neural Activity Detection via C-fos Immediate Early Gene Expression",
    "abstract": "           This paper investigates the application of advanced image segmentation techniques to analyze C-fos immediate early gene expression, a crucial marker for neural activity. Due to the complexity and high variability of neural circuits, accurate segmentation of C-fos images is paramount for the development of new insights into neural function. Amidst this backdrop, this research aims to improve accuracy and minimize manual intervention in C-fos image segmentation by leveraging the capabilities of CNNs and the Unet model. We describe the development of a novel workflow for the segmentation process involving Convolutional Neural Networks (CNNs) and the Unet model, demonstrating their efficiency in various image segmentation tasks. Our workflow incorporates pre-processing steps such as cropping, image feature extraction, and clustering for the training dataset selection. We used an AutoEncoder model to extract features and implement constrained clustering to identify similarities and differences in image types. Additionally, we utilized manual and automatic labeling approaches to enhance the performance of our model. We demonstrated the effectiveness of our method in distinguishing areas with significant C-fos expression from normal tissue areas. Lastly, we implemented a modified Unet network for the detection of C-fos expressions. This research contributes to the development of more efficient and automated image segmentation methods, advancing the understanding of neural function in neuroscience research.         ",
    "url": "https://arxiv.org/abs/2312.08177",
    "authors": [
      "Peilin Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.01974",
    "title": "Hypergraph-Transformer (HGT) for Interactive Event Prediction in Laparoscopic and Robotic Surgery",
    "abstract": "           Understanding and anticipating intraoperative events and actions is critical for intraoperative assistance and decision-making during minimally invasive surgery. Automated prediction of events, actions, and the following consequences is addressed through various computational approaches with the objective of augmenting surgeons' perception and decision-making capabilities. We propose a predictive neural network that is capable of understanding and predicting critical interactive aspects of surgical workflow from intra-abdominal video, while flexibly leveraging surgical knowledge graphs. The approach incorporates a hypergraph-transformer (HGT) structure that encodes expert knowledge into the network design and predicts the hidden embedding of the graph. We verify our approach on established surgical datasets and applications, including the detection and prediction of action triplets, and the achievement of the Critical View of Safety (CVS). Moreover, we address specific, safety-related tasks, such as predicting the clipping of cystic duct or artery without prior achievement of the CVS. Our results demonstrate the superiority of our approach compared to unstructured alternatives.         ",
    "url": "https://arxiv.org/abs/2402.01974",
    "authors": [
      "Lianhao Yin",
      "Yutong Ban",
      "Jennifer Eckhoff",
      "Ozanan Meireles",
      "Daniela Rus",
      "Guy Rosman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.07340",
    "title": "Perfect Recovery for Random Geometric Graph Matching with Shallow Graph Neural Networks",
    "abstract": "           We study the graph matching problem in the presence of vertex feature information using shallow graph neural networks. Specifically, given two graphs that are independent perturbations of a single random geometric graph with sparse binary features, the task is to recover an unknown one-to-one mapping between the vertices of the two graphs. We show under certain conditions on the sparsity and noise level of the feature vectors, a carefully designed two-layer graph neural network can, with high probability, recover the correct mapping between the vertices with the help of the graph structure. Additionally, we prove that our condition on the noise parameter is tight up to logarithmic factors. Finally, we compare the performance of the graph neural network to directly solving an assignment problem using the noisy vertex features and demonstrate that when the noise level is at least constant, this direct matching fails to achieve perfect recovery, whereas the graph neural network can tolerate noise levels growing as fast as a power of the size of the graph. Our theoretical findings are further supported by numerical studies as well as real-world data experiments.         ",
    "url": "https://arxiv.org/abs/2402.07340",
    "authors": [
      "Suqi Liu",
      "Morgane Austern"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Social and Information Networks (cs.SI)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2403.09905",
    "title": "Right Place, Right Time! Dynamizing Topological Graphs for Embodied Navigation",
    "abstract": "           Embodied Navigation tasks often involve constructing topological graphs of a scene during exploration to facilitate high-level planning and decision-making for execution in continuous environments. Prior literature makes the assumption of static graphs with stationary targets, which does not hold in many real-world environments with moving objects. To address this, we present a novel formulation generalizing navigation to dynamic environments by introducing structured object transitions to dynamize static topological graphs called Object Transition Graphs (OTGs). OTGs simulate portable targets following structured routes inspired by human habits. We apply this technique to Matterport3D (MP3D), a popular simulator for evaluating embodied tasks. On these dynamized OTGs, we establish a navigation benchmark by evaluating Oracle-based, Reinforcement Learning, and Large Language Model (LLM)-based approaches on a multi-object finding task. Further, we quantify agent adaptability, and make key inferences such as agents employing learned decision-making strategies generalize better than those relying on privileged oracle knowledge. To the best of our knowledge, ours is the first work to introduce structured temporal dynamism on topological graphs for studying generalist embodied navigation policies. The code and dataset for our OTGs will be made publicly available to foster research on embodied navigation in dynamic scenes.         ",
    "url": "https://arxiv.org/abs/2403.09905",
    "authors": [
      "Vishnu Sashank Dorbala",
      "Bhrij Patel",
      "Amrit Singh Bedi",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.16067",
    "title": "Adversarial Guided Diffusion Models for Adversarial Purification",
    "abstract": "           Diffusion model (DM) based adversarial purification (AP) has proven to be a powerful defense method that can remove adversarial perturbations and generate a purified example without threats. In principle, the pre-trained DMs can only ensure that purified examples conform to the same distribution of the training data, but it may inadvertently compromise the semantic information of input examples, leading to misclassification of purified examples. Recent advancements introduce guided diffusion techniques to preserve semantic information while removing the perturbations. However, these guidances often rely on distance measures between purified examples and diffused examples, which can also preserve perturbations in purified examples. To further unleash the robustness power of DM-based AP, we propose an adversarial guided diffusion model (AGDM) by introducing a novel adversarial guidance that contains sufficient semantic information but does not explicitly involve adversarial perturbations. The guidance is modeled by an auxiliary neural network obtained with adversarial training, considering the distance in the latent representations rather than at the pixel-level values. Extensive experiments are conducted on CIFAR-10, CIFAR-100 and ImageNet to demonstrate that our method is effective for simultaneously maintaining semantic information and removing the adversarial perturbations. In addition, comprehensive comparisons show that our method significantly enhances the robustness of existing DM-based AP, with an average robust accuracy improved by up to 7.30% on CIFAR-10.         ",
    "url": "https://arxiv.org/abs/2403.16067",
    "authors": [
      "Guang Lin",
      "Zerui Tao",
      "Jianhai Zhang",
      "Toshihisa Tanaka",
      "Qibin Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2403.16862",
    "title": "INPC: Implicit Neural Point Clouds for Radiance Field Rendering",
    "abstract": "           We introduce a new approach for reconstruction and novel view synthesis of unbounded real-world scenes. In contrast to previous methods using either volumetric fields, grid-based models, or discrete point cloud proxies, we propose a hybrid scene representation, which implicitly encodes the geometry in a continuous octree-based probability field and view-dependent appearance in a multi-resolution hash grid. This allows for extraction of arbitrary explicit point clouds, which can be rendered using rasterization. In doing so, we combine the benefits of both worlds and retain favorable behavior during optimization: Our novel implicit point cloud representation and differentiable bilinear rasterizer enable fast rendering while preserving the fine geometric detail captured by volumetric neural fields. Furthermore, this representation does not depend on priors like structure-from-motion point clouds. Our method achieves state-of-the-art image quality on common benchmarks. Furthermore, we achieve fast inference at interactive frame rates, and can convert our trained model into a large, explicit point cloud to further enhance performance.         ",
    "url": "https://arxiv.org/abs/2403.16862",
    "authors": [
      "Florian Hahlbohm",
      "Linus Franke",
      "Moritz Kappel",
      "Susana Castillo",
      "Martin Eisemann",
      "Marc Stamminger",
      "Marcus Magnor"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.02493",
    "title": "A Neural Multigrid Solver for Helmholtz Equations with High Wavenumber and Heterogeneous Media",
    "abstract": "           In this paper, we propose a deep learning-enhanced multigrid solver for high-frequency and heterogeneous Helmholtz equations. By applying spectral analysis, we categorize the iteration error into characteristic and non-characteristic components. We eliminate the non-characteristic components by a multigrid wave cycle, which employs carefully selected smoothers on each grid. We diminish the characteristic components by a learned phase function and the approximate solution of an advection-diffusion-reaction (ADR) equation, which is solved using another multigrid V-cycle on a coarser scale, referred to as the ADR cycle. The resulting solver, termed Wave-ADR-NS, enables the handling of error components with varying frequencies and overcomes constraints on the number of grid points per wavelength on coarse grids. Furthermore, we provide an efficient implementation using differentiable programming, making Wave-ADR-NS an end-to-end Helmholtz solver that incorporates parameters learned through a semi-supervised training. Wave-ADR-NS demonstrates robust generalization capabilities for both in-distribution and out-of-distribution velocity fields of varying difficulty. Comparative experiments with other multigrid methods validate its superior performance in solving heterogeneous 2D Helmholtz equations with wavenumbers exceeding 2000.         ",
    "url": "https://arxiv.org/abs/2404.02493",
    "authors": [
      "Chen Cui",
      "Kai Jiang",
      "Shi Shu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2404.11868",
    "title": "OTCXR: Rethinking Self-supervised Alignment using Optimal Transport for Chest X-ray Analysis",
    "abstract": "           Self-supervised learning (SSL) has emerged as a promising technique for analyzing medical modalities such as X-rays due to its ability to learn without annotations. However, conventional SSL methods face challenges in achieving semantic alignment and capturing subtle details, which limits their ability to accurately represent the underlying anatomical structures and pathological features. To address these limitations, we propose OTCXR, a novel SSL framework that leverages optimal transport (OT) to learn dense semantic invariance. By integrating OT with our innovative Cross-Viewpoint Semantics Infusion Module (CV-SIM), OTCXR enhances the model's ability to capture not only local spatial features but also global contextual dependencies across different viewpoints. This approach enriches the effectiveness of SSL in the context of chest radiographs. Furthermore, OTCXR incorporates variance and covariance regularizations within the OT framework to prioritize clinically relevant information while suppressing less informative features. This ensures that the learned representations are comprehensive and discriminative, particularly beneficial for tasks such as thoracic disease diagnosis. We validate OTCXR's efficacy through comprehensive experiments on three publicly available chest X-ray datasets. Our empirical results demonstrate the superiority of OTCXR over state-of-the-art methods across all evaluated tasks, confirming its capability to learn semantically rich representations.         ",
    "url": "https://arxiv.org/abs/2404.11868",
    "authors": [
      "Vandan Gorade",
      "Azad Singh",
      "Deepak Mishra"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.02986",
    "title": "Experiences with Sub-Arctic Sensor Network Deployment",
    "abstract": "           This paper discusses the experiences gained from designing, deploying, and maintaining low-power Wireless Sensor Networks (WSN) in three geothermally active remote locations in Iceland. The network was deployed for environmental monitoring and real-time data collection to assist in investigating the impact of global warming on the (sub)Arctic climate and the resulting carbon release from the region. Functional networks with more than 50 sensor nodes from three sites with extreme weather conditions and hard-to-access terrain have been collecting data since 2021. The networks employ primary cell-powered wireless sensor nodes equipped with DASH7 Alliance Protocol (D7A) for low-power data transmission and solar-powered D7A-cellular gateways for the backend connection. The WSNs have so far achieved over three years of uptime with minimal maintenance required throughout this period. We present a detailed discussion of different network components, their architecture, and the networks' overall performance and reliability.         ",
    "url": "https://arxiv.org/abs/2405.02986",
    "authors": [
      "Priyesh Pappinisseri Puluckul",
      "Maarten Weyn"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2405.08232",
    "title": "Robust Aggregation of Electric Vehicle Flexiblity",
    "abstract": "           We address the problem of characterizing the aggregate flexibility in populations of electric vehicles (EVs) with uncertain charging requirements. Extending upon prior results that provide exact characterizations of aggregate flexibility in populations of electric vehicle (EVs), we adapt the framework to encompass more general charging requirements. In doing so we give a characterization of the exact aggregate flexibility as a generalized polymatroid. Furthermore, this paper advances these aggregation methodologies to address the case in which charging requirements are uncertain. In this extended framework, requirements are instead sampled from a specified distribution. In particular, we construct robust aggregate flexibility sets, sets of aggregate charging profiles over which we can provide probabilistic guarantees that actual realized populations will be able to track. By leveraging measure concentration results that establish powerful finite sample guarantees, we are able to give tight bounds on these robust flexibility sets, even in low sample regimes that are well suited for aggregating small populations of EVs. We detail explicit methods of calculating these sets. Finally, we provide numerical results that validate our results and case studies that demonstrate the applicability of the theory developed herein.         ",
    "url": "https://arxiv.org/abs/2405.08232",
    "authors": [
      "Karan Mukhi",
      "Chengrui Qu",
      "Pengcheng You",
      "Alessandro Abate"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2405.14001",
    "title": "Nondeterministic Causal Models",
    "abstract": "           I generalize acyclic deterministic structural causal models to the nondeterministic case and argue that this offers an improved semantics for counterfactuals. The standard, deterministic, semantics developed by Halpern (and based on the initial proposal of Galles & Pearl) assumes that for each assignment of values to parent variables there is a unique assignment to their child variable, and it assumes that the actual world (an assignment of values to all variables of a model) specifies a unique counterfactual world for each intervention. Both assumptions are unrealistic, and therefore I drop both of them in my proposal. I do so by allowing multi-valued functions in the structural equations. In addition, I adjust the semantics so that the solutions to the equations that obtained in the actual world are preserved in any counterfactual world. I provide a sound and complete axiomatization of the resulting logic and compare it to the standard one by Halpern and to more recent proposals that are closer to mine. Finally, I extend these models to the probabilistic case and show that they open up the way to identifying counterfactuals even in Causal Bayesian Networks.         ",
    "url": "https://arxiv.org/abs/2405.14001",
    "authors": [
      "Sander Beckers"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2405.19653",
    "title": "SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems",
    "abstract": "           Surrogate models are used to predict the behavior of complex energy systems that are too expensive to simulate with traditional numerical methods. Our work introduces the use of language descriptions, which we call ``system captions'' or SysCaps, to interface with such surrogates. We argue that interacting with surrogates through text, particularly natural language, makes these models more accessible for both experts and non-experts. We introduce a lightweight multimodal text and timeseries regression model and a training pipeline that uses large language models (LLMs) to synthesize high-quality captions from simulation metadata. Our experiments on two real-world simulators of buildings and wind farms show that our SysCaps-augmented surrogates have better accuracy on held-out systems than traditional methods while enjoying new generalization abilities, such as handling semantically related descriptions of the same test system. Additional experiments also highlight the potential of SysCaps to unlock language-driven design space exploration and to regularize training through prompt augmentation.         ",
    "url": "https://arxiv.org/abs/2405.19653",
    "authors": [
      "Patrick Emami",
      "Zhaonan Li",
      "Saumya Sinha",
      "Truc Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2406.14746",
    "title": "Behavior-Inspired Neural Networks for Relational Inference",
    "abstract": "           From pedestrians to Kuramoto oscillators, interactions between agents govern how dynamical systems evolve in space and time. Discovering how these agents relate to each other has the potential to improve our understanding of the often complex dynamics that underlie these systems. Recent works learn to categorize relationships between agents based on observations of their physical behavior. These approaches model relationship categories as outcomes of a categorical distribution which is limiting and contrary to real-world systems, where relationship categories often intermingle and interact. In this work, we introduce a level of abstraction between the observable behavior of agents and the latent categories that determine their behavior. To do this, we learn a mapping from agent observations to agent preferences for a set of latent categories. The learned preferences and inter-agent proximity are integrated in a nonlinear opinion dynamics model, which allows us to naturally identify mutually exclusive categories, predict an agent's evolution in time, and control an agent's behavior. Through extensive experiments, we demonstrate the utility of our model for learning interpretable categories, and the efficacy of our model for long-horizon trajectory prediction.         ",
    "url": "https://arxiv.org/abs/2406.14746",
    "authors": [
      "Yulong Yang",
      "Bowen Feng",
      "Keqin Wang",
      "Naomi Ehrich Leonard",
      "Adji Bousso Dieng",
      "Christine Allen-Blanchette"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2406.17281",
    "title": "Adaptive Topology Reconstruction for Robust Graph Representation Learning",
    "abstract": "           Graph Neural Networks (GNNs) have become fundamental in semi-supervised learning for graph representation, leveraging their ability to capture complex node relationships. A recent trend in GNN research focuses on adaptive multi-hop structure learning, moving beyond fixed-hop aggregation to more flexible and dynamic neighborhood selection. While GAMLP \\citep{Zhang_2022} employs separate MLP layers for each multi-hop domain and ImprovingTE \\citep{Yao2023ImprovingTE} enhances this by injecting contextualized substructure information, these methods still rely heavily on predefined sampling strategies, which may limit their ability to generalize and maintain stable accuracy. To address these limitations, we propose an \\textbf{adaptive reconstruction framework} that dynamically refines multi-hop structure learning. Inspired by \"coreset selection\" \\citep{guo2022deepcore}, our approach adaptively \\textbf{reconstructs} node neighborhoods to optimize message passing, ensuring more \\textbf{effective and context-aware information flow} across the graph. To further enhance structural robustness, we introduce two key modules: the \\textbf{Distance Recomputator} and the \\textbf{Topology Reconstructor} (\\textcolor{blue}{DRTR}). The Distance Recomputator \\textbf{reassesses and recalibrates} node distances based on adaptive graph properties, leading to \\textbf{improved node embeddings} that better reflect latent relationships. Meanwhile, the Topology Reconstructor \\textbf{dynamically refines local graph structures}, enabling the model to \\textbf{adapt to evolving graph topologies} and mitigate the impact of noise and mislabeled data. Empirical evaluations demonstrate that our \\textbf{adaptive reconstruction framework} achieves \\textbf{significant improvements} over existing multi-hop-based models, providing more \\textbf{stable and accurate} performance in various graph learning benchmarks.         ",
    "url": "https://arxiv.org/abs/2406.17281",
    "authors": [
      "Dong Liu",
      "Yanxuan Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.08952",
    "title": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework for Few-shot Fake News Detection",
    "abstract": "           Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news from real ones in extremely low-resource scenarios. This task has garnered increased attention due to the widespread dissemination and harmful impact of fake news on social media. Large Language Models (LLMs) have demonstrated competitive performance with the help of their rich prior knowledge and excellent in-context learning abilities. However, existing methods face significant limitations, such as the Understanding Ambiguity and Information Scarcity, which significantly undermine the potential of LLMs. To address these shortcomings, we propose a Dual-perspective Knowledge-guided Fake News Detection (DKFND) model, designed to enhance LLMs from both inside and outside perspectives. Specifically, DKFND first identifies the knowledge concepts of each news article through a Detection Module. Subsequently, DKFND creatively designs an Investigation Module to retrieve inside and outside valuable information concerning to the current news, followed by another Judge Module to evaluate the relevance and confidence of them. Finally, a Determination Module further derives two respective predictions and obtain the final result. Extensive experiments on two public datasets show the efficacy of our proposed method, particularly in low-resource settings.         ",
    "url": "https://arxiv.org/abs/2407.08952",
    "authors": [
      "Ye Liu",
      "Jiajun Zhu",
      "Xukai Liu",
      "Haoyu Tang",
      "Yanghai Zhang",
      "Kai Zhang",
      "Xiaofang Zhou",
      "Enhong Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.10448",
    "title": "Spectral Representation for Causal Estimation with Hidden Confounders",
    "abstract": "           We address the problem of causal effect estimation where hidden confounders are present, with a focus on two settings: instrumental variable regression with additional observed confounders, and proxy causal learning. Our approach uses a singular value decomposition of a conditional expectation operator, followed by a saddle-point optimization problem, which, in the context of IV regression, can be thought of as a neural net generalization of the seminal approach due to Darolles et al. [2011]. Saddle-point formulations have gathered considerable attention recently, as they can avoid double sampling bias and are amenable to modern function approximation methods. We provide experimental validation in various settings, and show that our approach outperforms existing methods on common benchmarks.         ",
    "url": "https://arxiv.org/abs/2407.10448",
    "authors": [
      "Haotian Sun",
      "Antoine Moulin",
      "Tongzheng Ren",
      "Arthur Gretton",
      "Bo Dai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2408.02971",
    "title": "Parameter-Efficient Electromagnetic Surrogate Solver for Broadband Field Prediction using Discrete Wavelength Data",
    "abstract": "           Recently, electromagnetic surrogate solvers, trained on solutions of Maxwell's equations under specific simulation conditions, enabled real-time inference of computationally expensive simulations. However, conventional surrogate solvers often consider only a narrow range of simulation parameters and fail when encountering even slight variations in simulation conditions. To address this limitation, we propose a broadband surrogate solver capable of providing solutions across a continuous range of wavelengths, even when trained on discrete wavelength data. Our approach leverages a Wave-Informed element-wise Multiplicative Encoding and a Fourier Group Convolutional Shuffling operator to mitigate overfitting while capturing the fundamental characteristics of Maxwell's equations. Compared to the state-of-the-art surrogate solver, our model achieves a 74% reduction in parameters, an 80.5% improvement in prediction accuracy for untrained wavelengths, demonstrating superior generalization and efficiency.         ",
    "url": "https://arxiv.org/abs/2408.02971",
    "authors": [
      "Joonhyuk Seo",
      "Chanik Kang",
      "Dongjin Seo",
      "Haejun Chung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Emerging Technologies (cs.ET)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2408.07514",
    "title": "CNN-JEPA: Self-Supervised Pretraining Convolutional Neural Networks Using Joint Embedding Predictive Architecture",
    "abstract": "           Self-supervised learning (SSL) has become an important approach in pretraining large neural networks, enabling unprecedented scaling of model and dataset sizes. While recent advances like I-JEPA have shown promising results for Vision Transformers, adapting such methods to Convolutional Neural Networks (CNNs) presents unique challenges. In this paper, we introduce CNN-JEPA, a novel SSL method that successfully applies the joint embedding predictive architecture approach to CNNs. Our method incorporates a sparse CNN encoder to handle masked inputs, a fully convolutional predictor using depthwise separable convolutions, and an improved masking strategy. We demonstrate that CNN-JEPA outperforms I-JEPA with ViT architectures on ImageNet-100, achieving a 73.3% linear top-1 accuracy using a standard ResNet-50 encoder. Compared to other CNN-based SSL methods, CNN-JEPA requires 17-35% less training time for the same number of epochs and approaches the linear and k-NN top-1 accuracies of BYOL, SimCLR, and VICReg. Our approach offers a simpler, more efficient alternative to existing SSL methods for CNNs, requiring minimal augmentations and no separate projector network.         ",
    "url": "https://arxiv.org/abs/2408.07514",
    "authors": [
      "Andr\u00e1s Kalapos",
      "B\u00e1lint Gyires-T\u00f3th"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.09859",
    "title": "OccMamba: Semantic Occupancy Prediction with State Space Models",
    "abstract": "           Training deep learning models for semantic occupancy prediction is challenging due to factors such as a large number of occupancy cells, severe occlusion, limited visual cues, complicated driving scenarios, etc. Recent methods often adopt transformer-based architectures given their strong capability in learning input-conditioned weights and long-range relationships. However, transformer-based networks are notorious for their quadratic computation complexity, seriously undermining their efficacy and deployment in semantic occupancy prediction. Inspired by the global modeling and linear computation complexity of the Mamba architecture, we present the first Mamba-based network for semantic occupancy prediction, termed OccMamba. Specifically, we first design the hierarchical Mamba module and local context processor to better aggregate global and local contextual information, respectively. Besides, to relieve the inherent domain gap between the linguistic and 3D domains, we present a simple yet effective 3D-to-1D reordering scheme, i.e., height-prioritized 2D Hilbert expansion. It can maximally retain the spatial structure of 3D voxels as well as facilitate the processing of Mamba blocks. Endowed with the aforementioned designs, our OccMamba is capable of directly and efficiently processing large volumes of dense scene grids, achieving state-of-the-art performance across three prevalent occupancy prediction benchmarks, including OpenOccupancy, SemanticKITTI, and SemanticPOSS. Notably, on OpenOccupancy, our OccMamba outperforms the previous state-of-the-art Co-Occ by 5.1% IoU and 4.3% mIoU, respectively. Our implementation is open-sourced and available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2408.09859",
    "authors": [
      "Heng Li",
      "Yuenan Hou",
      "Xiaohan Xing",
      "Yuexin Ma",
      "Xiao Sun",
      "Yanyong Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.10436",
    "title": "Learning Regularization for Graph Inverse Problems",
    "abstract": "           In recent years, Graph Neural Networks (GNNs) have been utilized for various applications ranging from drug discovery to network design and social networks. In many applications, it is impossible to observe some properties of the graph directly; instead, noisy and indirect measurements of these properties are available. These scenarios are coined as Graph Inverse Problems (GRIP). In this work, we introduce a framework leveraging GNNs to solve GRIPs. The framework is based on a combination of likelihood and prior terms, which are used to find a solution that fits the data while adhering to learned prior information. Specifically, we propose to combine recent deep learning techniques that were developed for inverse problems, together with GNN architectures, to formulate and solve GRIP. We study our approach on a number of representative problems that demonstrate the effectiveness of the framework.         ",
    "url": "https://arxiv.org/abs/2408.10436",
    "authors": [
      "Moshe Eliasof",
      "Md Shahriar Rahim Siddiqui",
      "Carola-Bibiane Sch\u00f6nlieb",
      "Eldad Haber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.10883",
    "title": "Dynamic Analysis and Adaptive Discriminator for Fake News Detection",
    "abstract": "           In current web environment, fake news spreads rapidly across online social networks, posing serious threats to society. Existing multimodal fake news detection methods can generally be classified into knowledge-based and semantic-based approaches. However, these methods are heavily rely on human expertise and feedback, lacking flexibility. To address this challenge, we propose a Dynamic Analysis and Adaptive Discriminator (DAAD) approach for fake news detection. For knowledge-based methods, we introduce the Monte Carlo Tree Search algorithm to leverage the self-reflective capabilities of large language models (LLMs) for prompt optimization, providing richer, domain-specific details and guidance to the LLMs, while enabling more flexible integration of LLM comment on news content. For semantic-based methods, we define four typical deceit patterns: emotional exaggeration, logical inconsistency, image manipulation, and semantic inconsistency, to reveal the mechanisms behind fake news creation. To detect these patterns, we carefully design four discriminators and expand them in depth and breadth, using the soft-routing mechanism to explore optimal detection models. Experimental results on three real-world datasets demonstrate the superiority of our approach. The code will be available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2408.10883",
    "authors": [
      "Xinqi Su",
      "Zitong Yu",
      "Yawen Cui",
      "Ajian Liu",
      "Xun Lin",
      "Yuhao Wang",
      "Haochen Liang",
      "Wenhui Li",
      "Li Shen",
      "Xiaochun Cao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.15993",
    "title": "ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution",
    "abstract": "           Detecting and attributing temperature increases driven by climate change is crucial for understanding global warming and informing adaptation strategies. However, distinguishing human-induced climate signals from natural variability remains challenging for traditional detection and attribution (D&A) methods, which rely on identifying specific \"fingerprints\" -- spatial patterns expected to emerge from external forcings such as greenhouse gas emissions. Deep learning offers promise in discerning these complex patterns within expansive spatial datasets, yet the lack of standardized protocols has hindered consistent comparisons across studies. To address this gap, we introduce ClimDetect, a standardized dataset comprising 1.17M daily climate snapshots paired with target climate change indicator variables. The dataset is curated from both CMIP6 climate model simulations and real-world observation-assimilated reanalysis datasets (ERA5, JRA-3Q, and MERRA-2), and is designed to enhance model accuracy in detecting climate change signals. ClimDetect integrates various input and target variables used in previous research, ensuring comparability and consistency across studies. We also explore the application of vision transformers (ViT) to climate data -- a novel approach that, to our knowledge, has not been attempted before for climate change detection tasks. Our open-access data serve as a benchmark for advancing climate science by enabling end-to-end model development and evaluation. ClimDetect is publicly accessible via Hugging Face dataset repository at: this https URL.         ",
    "url": "https://arxiv.org/abs/2408.15993",
    "authors": [
      "Sungduk Yu",
      "Brian L. White",
      "Anahita Bhiwandiwalla",
      "Musashi Hinck",
      "Matthew Lyle Olson",
      "Yaniv Gurwicz",
      "Raanan Y. Rohekar",
      "Tung Nguyen",
      "Vasudev Lal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2409.05780",
    "title": "Breaking Neural Network Scaling Laws with Modularity",
    "abstract": "           Modular neural networks outperform nonmodular neural networks on tasks ranging from visual question answering to robotics. These performance improvements are thought to be due to modular networks' superior ability to model the compositional and combinatorial structure of real-world problems. However, a theoretical explanation of how modularity improves generalizability, and how to leverage task modularity while training networks remains elusive. Using recent theoretical progress in explaining neural network generalization, we investigate how the amount of training data required to generalize on a task varies with the intrinsic dimensionality of a task's input. We show theoretically that when applied to modularly structured tasks, while nonmodular networks require an exponential number of samples with task dimensionality, modular networks' sample complexity is independent of task dimensionality: modular networks can generalize in high dimensions. We then develop a novel learning rule for modular networks to exploit this advantage and empirically show the improved generalization of the rule, both in- and out-of-distribution, on high-dimensional, modular tasks.         ",
    "url": "https://arxiv.org/abs/2409.05780",
    "authors": [
      "Akhilan Boopathy",
      "Sunshine Jiang",
      "William Yue",
      "Jaedong Hwang",
      "Abhiram Iyer",
      "Ila Fiete"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2409.13822",
    "title": "Personalization in Human-Robot Interaction through Preference-based Action Representation Learning",
    "abstract": "           Preference-based reinforcement learning (PbRL) has shown significant promise for personalization in human-robot interaction (HRI) by explicitly integrating human preferences into the robot learning process. However, existing practices often require training a personalized robot policy from scratch, resulting in inefficient use of human feedback. In this paper, we propose preference-based action representation learning (PbARL), an efficient fine-tuning method that decouples common task structure from preference by leveraging pre-trained robot policies. Instead of directly fine-tuning the pre-trained policy with human preference, PbARL uses it as a reference for an action representation learning task that maximizes the mutual information between the pre-trained source domain and the target user preference-aligned domain. This approach allows the robot to personalize its behaviors while preserving original task performance and eliminates the need for extensive prior information from the source domain, thereby enhancing efficiency and practicality in real-world HRI scenarios. Empirical results on the Assistive Gym benchmark and a real-world user study (N=8) demonstrate the benefits of our method compared to state-of-the-art approaches.         ",
    "url": "https://arxiv.org/abs/2409.13822",
    "authors": [
      "Ruiqi Wang",
      "Dezhong Zhao",
      "Dayoon Suh",
      "Ziqin Yuan",
      "Guohua Chen",
      "Byung-Cheol Min"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.19338",
    "title": "Decoding Echo Chambers: LLM-Powered Simulations Revealing Polarization in Social Networks",
    "abstract": "           The impact of social media on critical issues such as echo chambers needs to be addressed, as these phenomena can have disruptive consequences for our society. Traditional research often oversimplifies emotional tendencies and opinion evolution into numbers and formulas, neglecting that news and communication are conveyed through text, which limits these approaches. Hence, in this work, we propose an LLM-based simulation for the social opinion network to evaluate and counter polarization phenomena. We first construct three typical network structures to simulate different characteristics of social interactions. Then, agents interact based on recommendation algorithms and update their strategies through reasoning and analysis. By comparing these interactions with the classic Bounded Confidence Model (BCM), the Friedkin Johnsen (FJ) model, and using echo chamber-related indices, we demonstrate the effectiveness of our framework in simulating opinion dynamics and reproducing phenomena such as opinion polarization and echo chambers. We propose two mitigation methods, active and passive nudges, that can help reduce echo chambers, specifically within language-based simulations. We hope our work will offer valuable insights and guidance for social polarization mitigation.         ",
    "url": "https://arxiv.org/abs/2409.19338",
    "authors": [
      "Chenxi Wang",
      "Zongfang Liu",
      "Dequan Yang",
      "Xiuying Chen"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2410.01306",
    "title": "Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation",
    "abstract": "           Empathetic and coherent responses are critical in auto-mated chatbot-facilitated psychotherapy. This study addresses the challenge of enhancing the emotional and contextual understanding of large language models (LLMs) in psychiatric applications. We introduce Emotion-Aware Embedding Fusion, a novel framework integrating hierarchical fusion and attention mechanisms to prioritize semantic and emotional features in therapy transcripts. Our approach combines multiple emotion lexicons, including NRC Emotion Lexicon, VADER, WordNet, and SentiWordNet, with state-of-the-art LLMs such as Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4. Therapy session transcripts, comprising over 2,000 samples are segmented into hierarchical levels (word, sentence, and session) using neural networks, while hierarchical fusion combines these features with pooling techniques to refine emotional representations. Atten-tion mechanisms, including multi-head self-attention and cross-attention, further prioritize emotional and contextual features, enabling temporal modeling of emotion-al shifts across sessions. The processed embeddings, computed using BERT, GPT-3, and RoBERTa are stored in the Facebook AI similarity search vector database, which enables efficient similarity search and clustering across dense vector spaces. Upon user queries, relevant segments are retrieved and provided as context to LLMs, enhancing their ability to generate empathetic and con-textually relevant responses. The proposed framework is evaluated across multiple practical use cases to demonstrate real-world applicability, including AI-driven therapy chatbots. The system can be integrated into existing mental health platforms to generate personalized responses based on retrieved therapy session data.         ",
    "url": "https://arxiv.org/abs/2410.01306",
    "authors": [
      "Abdur Rasool",
      "Muhammad Irfan Shahzad",
      "Hafsa Aslam",
      "Vincent Chan",
      "Muhammad Ali Arshad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2410.01351",
    "title": "Learning and teaching biological data science in the Bioconductor community",
    "abstract": "           Modern biological research is increasingly data-intensive, leading to a growing demand for effective training in biological data science. In this article, we provide an overview of key resources and best practices available within the Bioconductor project - an open-source software community focused on omics data analysis. This guide serves as a valuable reference for both learners and educators in the field.         ",
    "url": "https://arxiv.org/abs/2410.01351",
    "authors": [
      "Jenny Drnevich",
      "Frederick J. Tan",
      "Fabricio Almeida-Silva",
      "Robert Castelo",
      "Aedin C. Culhane",
      "Sean Davis",
      "Maria A. Doyle",
      "Ludwig Geistlinger",
      "Andrew R. Ghazi",
      "Susan Holmes",
      "Leo Lahti",
      "Alexandru Mahmoud",
      "Kozo Nishida",
      "Marcel Ramos",
      "Kevin Rue-Albrecht",
      "David J.H. Shih",
      "Laurent Gatto",
      "Charlotte Soneson"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Other Quantitative Biology (q-bio.OT)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2410.11352",
    "title": "Modeling advection on distance-weighted directed networks",
    "abstract": "           In this paper we propose a model for describing advection dynamics on distance-weighted directed graphs. To this end we establish a set of key properties, or axioms, that a discrete advection operator should satisfy, and prove that there exists an essentially unique operator satisfying all such properties. Both infinite and finite networks are considered, as well as possible variants and extensions. We illustrate the proposed model through examples, both analytical and numerical, and we describe an application to the simulation of a traffic network.         ",
    "url": "https://arxiv.org/abs/2410.11352",
    "authors": [
      "Michele Benzi",
      "Fabio Durastante",
      "Francesco Zigliotto"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2410.16888",
    "title": "Unsupervised Time Series Anomaly Prediction with Importance-based Generative Contrastive Learning",
    "abstract": "           Time series anomaly prediction plays an essential role in many real-world scenarios, such as environmental prevention and prompt maintenance of cyber-physical systems. However, existing time series anomaly prediction methods mainly require supervised training with plenty of manually labeled data, which are difficult to obtain in practice. Besides, unseen anomalies can occur during inference, which could differ from the labeled training data and make these models fail to predict such new anomalies. In this paper, we study a novel problem of unsupervised time series anomaly prediction. We provide a theoretical analysis and propose Importance-based Generative Contrastive Learning (IGCL) to address the aforementioned problems. IGCL distinguishes between normal and anomaly precursors, which are generated by our anomaly precursor pattern generation module. To address the efficiency issues caused by the potential complex anomaly precursor combinations, we propose a memory bank with importance-based scores to adaptively store representative anomaly precursors and generate more complicated anomaly precursors. Extensive experiments on seven benchmark datasets show our method outperforms state-of-the-art baselines on unsupervised time series anomaly prediction problems.         ",
    "url": "https://arxiv.org/abs/2410.16888",
    "authors": [
      "Kai Zhao",
      "Zhihao Zhuang",
      "Chenjuan Guo",
      "Hao Miao",
      "Yunyao Cheng",
      "Bin Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.19256",
    "title": "Spatioformer: A Geo-encoded Transformer for Large-Scale Plant Species Richness Prediction",
    "abstract": "           Earth observation data have shown promise in predicting species richness of vascular plants ($\\alpha$-diversity), but extending this approach to large spatial scales is challenging because geographically distant regions may exhibit different compositions of plant species ($\\beta$-diversity), resulting in a location-dependent relationship between richness and spectral measurements. In order to handle such geolocation dependency, we propose \\textit{Spatioformer}, where a novel geolocation encoder is coupled with the transformer model to encode geolocation context into remote sensing imagery. The Spatioformer model compares favourably to state-of-the-art models in richness predictions on a large-scale ground-truth richness dataset (HAVPlot) that consists of 68,170 in-situ richness samples covering diverse landscapes across Australia. The results demonstrate that geolocational information is advantageous in predicting species richness from satellite observations over large spatial scales. With Spatioformer, plant species richness maps over Australia are compiled from Landsat archive for the years from 2015 to 2023. The richness maps produced in this study reveal the spatiotemporal dynamics of plant species richness in Australia, providing supporting evidence to inform effective planning and policy development for plant diversity conservation. Regions of high richness prediction uncertainties are identified, highlighting the need for future in-situ surveys to be conducted in these areas to enhance the prediction accuracy.         ",
    "url": "https://arxiv.org/abs/2410.19256",
    "authors": [
      "Yiqing Guo",
      "Karel Mokany",
      "Shaun R. Levick",
      "Jinyan Yang",
      "Peyman Moghadam"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.22269",
    "title": "Fourier Head: Helping Large Language Models Learn Complex Probability Distributions",
    "abstract": "           As the quality of large language models has improved, there has been increased interest in using them to model non-linguistic tokens. For example, the Decision Transformer recasts agentic decision making as a sequence modeling problem, using a decoder-only LLM to model the distribution over the discrete action space for an Atari agent. However, when adapting LLMs to non-linguistic domains, it remains unclear if softmax over discrete bins captures the continuous structure of the tokens and the potentially complex distributions needed for high quality token generation. We introduce a neural network layer, constructed using Fourier series, which we can easily substitute for any linear layer if we want the outputs to have a more continuous structure. We perform extensive analysis on synthetic datasets, as well as on large-scale decision making and time series forecasting tasks. We also provide theoretical evidence that this layer can better learn signal from data while ignoring high-frequency noise. All of our results support the effectiveness of our proposed Fourier head in scenarios where the underlying data distribution has a natural continuous structure. For example, the Fourier head improves a Decision Transformer agent's returns across four benchmark Atari games by as much as 377%, and increases a state-of-the-art times series foundation model's forecasting performance by 3.5% across 20 benchmarks unseen during training.         ",
    "url": "https://arxiv.org/abs/2410.22269",
    "authors": [
      "Nate Gillman",
      "Daksh Aggarwal",
      "Michael Freeman",
      "Saurabh Singh",
      "Chen Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.02114",
    "title": "Semiparametric conformal prediction",
    "abstract": "           Many risk-sensitive applications require well-calibrated prediction sets over multiple, potentially correlated target variables, for which the prediction algorithm may report correlated errors. In this work, we aim to construct the conformal prediction set accounting for the joint correlation structure of the vector-valued non-conformity scores. Drawing from the rich literature on multivariate quantiles and semiparametric statistics, we propose an algorithm to estimate the $1-\\alpha$ quantile of the scores, where $\\alpha$ is the user-specified miscoverage rate. In particular, we flexibly estimate the joint cumulative distribution function (CDF) of the scores using nonparametric vine copulas and improve the asymptotic efficiency of the quantile estimate using its influence function. The vine decomposition allows our method to scale well to a large number of targets. As well as guaranteeing asymptotically exact coverage, our method yields desired coverage and competitive efficiency on a range of real-world regression problems, including those with missing-at-random labels in the calibration set.         ",
    "url": "https://arxiv.org/abs/2411.02114",
    "authors": [
      "Ji Won Park",
      "Robert Tibshirani",
      "Kyunghyun Cho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.05837",
    "title": "Gaussian Smoothing in Saliency Maps: The Stability-Fidelity Trade-Off in Neural Network Interpretability",
    "abstract": "           Saliency maps have been widely used to interpret the decisions of neural network classifiers and discover phenomena from their learned functions. However, standard gradient-based maps are frequently observed to be highly sensitive to the randomness of training data and the stochasticity in the training process. In this work, we study the role of Gaussian smoothing in the well-known Smooth-Grad algorithm in the stability of the gradient-based maps to the randomness of training samples. We extend the algorithmic stability framework to gradient-based interpretation maps and prove bounds on the stability error of standard Simple-Grad, Integrated-Gradients, and Smooth-Grad saliency maps. Our theoretical results suggest the role of Gaussian smoothing in boosting the stability of gradient-based maps to the randomness of training settings. On the other hand, we analyze the faithfulness of the Smooth-Grad maps to the original Simple-Grad and show the lower fidelity under a more intense Gaussian smoothing. We support our theoretical results by performing several numerical experiments on standard image datasets. Our empirical results confirm our hypothesis on the fidelity-stability trade-off in the application of Gaussian smoothing to gradient-based interpretation maps.         ",
    "url": "https://arxiv.org/abs/2411.05837",
    "authors": [
      "Zhuorui Ye",
      "Farzan Farnia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.05979",
    "title": "Variance-Aware Linear UCB with Deep Representation for Neural Contextual Bandits",
    "abstract": "           By leveraging the representation power of deep neural networks, neural upper confidence bound (UCB) algorithms have shown success in contextual bandits. To further balance the exploration and exploitation, we propose Neural-$\\sigma^2$-LinearUCB, a variance-aware algorithm that utilizes $\\sigma^2_t$, i.e., an upper bound of the reward noise variance at round $t$, to enhance the uncertainty quantification quality of the UCB, resulting in a regret performance improvement. We provide an oracle version for our algorithm characterized by an oracle variance upper bound $\\sigma^2_t$ and a practical version with a novel estimation for this variance bound. Theoretically, we provide rigorous regret analysis for both versions and prove that our oracle algorithm achieves a better regret guarantee than other neural-UCB algorithms in the neural contextual bandits setting. Empirically, our practical method enjoys a similar computational efficiency, while outperforming state-of-the-art techniques by having a better calibration and lower regret across multiple standard settings, including on the synthetic, UCI, MNIST, and CIFAR-10 datasets.         ",
    "url": "https://arxiv.org/abs/2411.05979",
    "authors": [
      "Ha Manh Bui",
      "Enrique Mallada",
      "Anqi Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.10794",
    "title": "Going Beyond Conventional OOD Detection",
    "abstract": "           Out-of-distribution (OOD) detection is critical to ensure the safe deployment of deep learning models in critical applications. Deep learning models can often misidentify OOD samples as in-distribution (ID) samples. This vulnerability worsens in the presence of spurious correlation in the training set. Likewise, in fine-grained classification settings, detection of fine-grained OOD samples becomes inherently challenging due to their high similarity to ID samples. However, current research on OOD detection has largely ignored these challenging scenarios, focusing instead on relatively easier (conventional) cases. In this work, we present a unified Approach to Spurious, fine-grained, and Conventional OOD Detection (ASCOOD). First, we propose synthesizing virtual outliers from ID data by approximating the destruction of invariant features. To this end, we identify invariant features with the pixel attribution method using the model being learned. This approach eliminates the burden of curating external OOD datasets. Then, we simultaneously incentivize ID classification and predictive uncertainty towards virtual outliers leveraging standardized feature representation. Our approach effectively mitigates the impact of spurious correlations and encourages capturing fine-grained attributes. Extensive experiments across seven datasets demonstrate the merit of ASCOOD in spurious, fine-grained, and conventional settings. The code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2411.10794",
    "authors": [
      "Sudarshan Regmi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.15210",
    "title": "Towards Million-Scale Adversarial Robustness Evaluation With Stronger Individual Attacks",
    "abstract": "           As deep learning models are increasingly deployed in safety-critical applications, evaluating their vulnerabilities to adversarial perturbations is essential for ensuring their reliability and trustworthiness. Over the past decade, a large number of white-box adversarial robustness evaluation methods (i.e., attacks) have been proposed, ranging from single-step to multi-step methods and from individual to ensemble methods. Despite these advances, challenges remain in conducting meaningful and comprehensive robustness evaluations, particularly when it comes to large-scale testing and ensuring evaluations reflect real-world adversarial risks. In this work, we focus on image classification models and propose a novel individual attack method, Probability Margin Attack (PMA), which defines the adversarial margin in the probability space rather than the logits space. We analyze the relationship between PMA and existing cross-entropy or logits-margin-based attacks, and show that PMA can outperform the current state-of-the-art individual methods. Building on PMA, we propose two types of ensemble attacks that balance effectiveness and efficiency. Furthermore, we create a million-scale dataset, CC1M, derived from the existing CC3M dataset, and use it to conduct the first million-scale white-box adversarial robustness evaluation of adversarially-trained ImageNet models. Our findings provide valuable insights into the robustness gaps between individual versus ensemble attacks and small-scale versus million-scale evaluations.         ",
    "url": "https://arxiv.org/abs/2411.15210",
    "authors": [
      "Yong Xie",
      "Weijie Zheng",
      "Hanxun Huang",
      "Guangnan Ye",
      "Xingjun Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.17384",
    "title": "Assessing Electricity Network Capacity Requirements for Industrial Decarbonisation in Great Britain",
    "abstract": "           Decarbonising the industrial sector is vital to reach net zero targets. The deployment of industrial decarbonisation technologies is expected to increase industrial electricity demand in many countries and this may require upgrades to the existing electricity network or new network investment. While the infrastructure requirements to support the introduction of new fuels and technologies in industry, such as hydrogen and carbon capture, utilisation and storage are often discussed, the need for investment to increase the capacity of the electricity network to meet increasing industrial electricity demands is often overlooked in the literature. This paper addresses this gap by quantifying the requirements for additional electricity network capacity to support the decarbonisation of industrial sectors across Great Britain (GB). The Net Zero Industrial Pathways model is used to predict the future electricity demand from industrial sites to 2050 which is then compared spatially to the available headroom across the distribution network in GB. The results show that network headroom is sufficient to meet extra capacity demands from industrial sites over the period to 2030 in nearly all GB regions and network scenarios. However, as electricity demand rises due to increased electrification across all sectors and industrial decarbonisation accelerates towards 2050, the network will need significant new capacity (71 GW + by 2050) particularly in the central, south, and north-west regions of England, and Wales. Without solving these network constraints, around 65% of industrial sites that are large point sources of emissions would be constrained in terms of electric capacity by 2040. These sites are responsible for 69% of industrial point source emissions.         ",
    "url": "https://arxiv.org/abs/2411.17384",
    "authors": [
      "Ahmed Gailani",
      "Peter Taylor"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.18078",
    "title": "Enhancing Prohibited Item Detection through X-ray-Specific Augmentation and Contextual Feature Integration",
    "abstract": "           X-ray prohibited item detection faces challenges due to the long-tail distribution and unique characteristics of X-ray imaging. Traditional data augmentation strategies, such as copy-paste and mixup, are ineffective at improving the detection of rare items due to the complex interactions between overlapping objects. Furthermore, X-ray imaging removes easily distinguishable features like color and texture, making it difficult to differentiate between visually similar categories. To address these challenges, in this work, we propose the X-ray Imaging-driven Detection Network (XIDNet). Inspired by the unique characteristics of X-ray imaging, this network introduces two key innovations: a novel X-ray-specific augmentation strategy that generates more realistic training samples for rare items, thereby improving detection performance for categories with insufficient samples, and an contextual feature integration algorithm that captures the spatial and semantic interactions between objects and surroundings under X-ray imaging, enhancing the model's ability to distinguish between similar categories. Extensive experimental results show that XIDNet effectively leverages X-ray imaging characteristics to significantly improve detection performance, outperforming popular SoTA methods by up to 17.2% in tail categories.         ",
    "url": "https://arxiv.org/abs/2411.18078",
    "authors": [
      "Renshuai Tao",
      "Haoyu Wang",
      "Wei Wang",
      "Yunchao Wei",
      "Yao Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.19162",
    "title": "Lost & Found: Tracking Changes from Egocentric Observations in 3D Dynamic Scene Graphs",
    "abstract": "           Recent approaches have successfully focused on the segmentation of static reconstructions, thereby equipping downstream applications with semantic 3D understanding. However, the world in which we live is dynamic, characterized by numerous interactions between the environment and humans or robotic agents. Static semantic maps are unable to capture this information, and the naive solution of rescanning the environment after every change is both costly and ineffective in tracking e.g. objects being stored away in drawers. With Lost & Found we present an approach that addresses this limitation. Based solely on egocentric recordings with corresponding hand position and camera pose estimates, we are able to track the 6DoF poses of the moving object within the detected interaction interval. These changes are applied online to a transformable scene graph that captures object-level relations. Compared to state-of-the-art object pose trackers, our approach is more reliable in handling the challenging egocentric viewpoint and the lack of depth information. It outperforms the second-best approach by 34% and 56% for translational and orientational error, respectively, and produces visibly smoother 6DoF object trajectories. In addition, we illustrate how the acquired interaction information in the dynamic scene graph can be employed in the context of robotic applications that would otherwise be unfeasible: We show how our method allows to command a mobile manipulator through teach & repeat, and how information about prior interaction allows a mobile manipulator to retrieve an object hidden in a drawer. Code, videos and corresponding data are accessible at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.19162",
    "authors": [
      "Tjark Behrens",
      "Ren\u00e9 Zurbr\u00fcgg",
      "Marc Pollefeys",
      "Zuria Bauer",
      "Hermann Blum"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.19517",
    "title": "RL-MILP Solver: A Reinforcement Learning Approach for Solving Mixed-Integer Linear Programs with Graph Neural Networks",
    "abstract": "           Mixed-integer linear programming (MILP) is a widely used optimization technique across various fields. Existing $\\textit{end-to-end learning}$ methods for MILP generate values for a subset of decision variables and delegate the remaining problem to traditional MILP solvers. However, this approach often fails to guarantee solution feasibility (i.e., satisfying all constraints) due to inaccurate predictions and primarily focuses on binary decision variables. Satisfying all constraints is a prerequisite for obtaining the optimal solution, and the feasibility issue becomes even more critical with non-binary integer (integer, for short) variables. Thus, addressing the feasibility of MILP involving integer variables is crucial. To address these challenges, we propose a novel reinforcement learning (RL)-based solver that not only finds the first feasible solution but also incrementally discovers better feasible solutions without delegating the remainder to off-the-shelf solvers. Our experimental results demonstrate that the proposed method achieves (near-)optimal solutions.         ",
    "url": "https://arxiv.org/abs/2411.19517",
    "authors": [
      "Tae-Hoon Lee",
      "Min-Soo Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.06545",
    "title": "On How Iterative Magnitude Pruning Discovers Local Receptive Fields in Fully Connected Neural Networks",
    "abstract": "           Since its use in the Lottery Ticket Hypothesis, iterative magnitude pruning (IMP) has become a popular method for extracting sparse subnetworks that can be trained to high performance. Despite its success, the mechanism that drives the success of IMP remains unclear. One possibility is that IMP is capable of extracting subnetworks with good inductive biases that facilitate performance. Supporting this idea, recent work showed that applying IMP to fully connected neural networks (FCNs) leads to the emergence of local receptive fields (RFs), a feature of mammalian visual cortex and convolutional neural networks that facilitates image processing. However, it remains unclear why IMP would uncover localized features in the first place. Inspired by results showing that training on synthetic images with highly non-Gaussian statistics (e.g., sharp edges) is sufficient to drive the emergence of local RFs in FCNs, we hypothesize that IMP iteratively increases the non-Gaussian statistics of FCN representations, creating a feedback loop that enhances localization. Here, we demonstrate first that non-Gaussian input statistics are indeed necessary for IMP to discover localized RFs. We then develop a new method for measuring the effect of individual weights on the statistics of the FCN representations (\"cavity method\"), which allows us to show that IMP systematically increases the non-Gaussianity of pre-activations, leading to the formation of localized RFs. Our work, which is the first to study the effect of IMP on the statistics of the representations of neural networks, sheds parsimonious light on one way in which IMP can drive the formation of strong inductive biases.         ",
    "url": "https://arxiv.org/abs/2412.06545",
    "authors": [
      "William T. Redman",
      "Zhangyang Wang",
      "Alessandro Ingrosso",
      "Sebastian Goldt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.08014",
    "title": "MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents",
    "abstract": "           Physical adversarial attacks in driving scenarios can expose critical vulnerabilities in visual perception models. However, developing such attacks remains challenging due to diverse real-world environments and the requirement for maintaining visual naturality. Building upon this challenge, we reformulate physical adversarial attacks as a one-shot patch generation problem. Our approach generates adversarial patches through a deep generative model that considers the specific scene context, enabling direct physical deployment in matching environments. The primary challenge lies in simultaneously achieving two objectives: generating adversarial patches that effectively mislead object detection systems while determining contextually appropriate deployment within the scene. We propose MAGIC (Mastering Physical Adversarial Generation In Context), a novel framework powered by multi-modal LLM agents to address these challenges. MAGIC automatically understands scene context and generates adversarial patch through the synergistic interaction of language and vision capabilities. In particular, MAGIC orchestrates three specialized LLM agents: The adv-patch generation agent (GAgent) masters the creation of deceptive patches through strategic prompt engineering for text-to-image models. The adv-patch deployment agent (DAgent) ensures contextual coherence by determining optimal deployment strategies based on scene understanding. The self-examination agent (EAgent) completes this trilogy by providing critical oversight and iterative refinement of both processes. We validate our method on both digital and physical levels, i.e., nuImage and manually captured real-world scenes, where both statistical and visual results prove that our MAGIC is powerful and effective for attacking widely applied object detection systems, i.e., YOLO and DETR series.         ",
    "url": "https://arxiv.org/abs/2412.08014",
    "authors": [
      "Yun Xing",
      "Nhat Chung",
      "Jie Zhang",
      "Yue Cao",
      "Ivor Tsang",
      "Yang Liu",
      "Lei Ma",
      "Qing Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.12478",
    "title": "Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script",
    "abstract": "           DNN-based language models perform excellently on various tasks, but even SOTA LLMs are susceptible to textual adversarial attacks. Adversarial texts play crucial roles in multiple subfields of NLP. However, current research has the following issues. (1) Most textual adversarial attack methods target rich-resourced languages. How do we generate adversarial texts for less-studied languages? (2) Most textual adversarial attack methods are prone to generating invalid or ambiguous adversarial texts. How do we construct high-quality adversarial robustness benchmarks? (3) New language models may be immune to part of previously generated adversarial texts. How do we update adversarial robustness benchmarks? To address the above issues, we introduce HITL-GAT, a system based on a general approach to human-in-the-loop generation of adversarial texts. HITL-GAT contains four stages in one pipeline: victim model construction, adversarial example generation, high-quality benchmark construction, and adversarial robustness evaluation. Additionally, we utilize HITL-GAT to make a case study on Tibetan script which can be a reference for the adversarial research of other less-studied languages.         ",
    "url": "https://arxiv.org/abs/2412.12478",
    "authors": [
      "Xi Cao",
      "Yuan Sun",
      "Jiajun Li",
      "Quzong Gesang",
      "Nuo Qun",
      "Tashi Nyima"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2412.14042",
    "title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds",
    "abstract": "           Computer-Aided Design (CAD) models are typically constructed by sequentially drawing parametric sketches and applying CAD operations to obtain a 3D model. The problem of 3D CAD reverse engineering consists of reconstructing the sketch and CAD operation sequences from 3D representations such as point clouds. In this paper, we address this challenge through novel contributions across three levels: CAD sequence representation, network design, and training dataset. In particular, we represent CAD sketch-extrude sequences as Python code. The proposed CAD-Recode translates a point cloud into Python code that, when executed, reconstructs the CAD model. Taking advantage of the exposure of pre-trained Large Language Models (LLMs) to Python code, we leverage a relatively small LLM as a decoder for CAD-Recode and combine it with a lightweight point cloud projector. CAD-Recode is trained on a procedurally generated dataset of one million CAD sequences. CAD-Recode significantly outperforms existing methods across the DeepCAD, Fusion360 and real-world CC3D datasets. Furthermore, we show that our CAD Python code output is interpretable by off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering from point clouds.         ",
    "url": "https://arxiv.org/abs/2412.14042",
    "authors": [
      "Danila Rukhovich",
      "Elona Dupont",
      "Dimitrios Mallis",
      "Kseniya Cherenkova",
      "Anis Kacem",
      "Djamila Aouada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.15312",
    "title": "PCA-Featured Transformer for Jamming Detection in 5G UAV Networks",
    "abstract": "           Unmanned Aerial Vehicles (UAVs) face significant security risks from jamming attacks, which can compromise network functionality. Traditional detection methods often fall short when confronting AI-powered jamming that dynamically modifies its behavior, while contemporary machine learning approaches frequently demand substantial feature engineering and struggle with temporal patterns in attack signatures. The vulnerability extends to 5G networks employing Time Division Duplex (TDD) or Frequency Division Duplex (FDD), where service quality may deteriorate due to deliberate interference. We introduce a novel U-shaped transformer architecture that leverages Principal Component Analysis (PCA) to refine feature representations for improved wireless security. The training process is regularized by incorporating the output entropy uncertainty into the loss function, a mechanism inspired by the Soft Actor-Critic (SAC) algorithm in Reinforcement Learning (RL) to enable robust jamming detection techniques. The architecture features a modified transformer encoder specially designed to process critical wireless signal features, including Received Signal Strength Indicator (RSSI) and Signal-to- Interference-plus-Noise Ratio (SINR) measurements. We complement this with a custom positional encoding mechanism that specifically accounts for the inherent periodicity of wireless signals,enabling a more accurate representation of temporal signal patterns. In addition, we propose a batch size scheduler and implement chunking techniques to optimize convergence for time series data. These advancements contribute to up to a ten times improvement in training speed within the advanced U-shaped encoder-decoder transformer model introduced in this study. Experimental evaluations demonstrate the effectiveness of our entropy-based approach, achieving detection rates of 85.06% in NLoS scenarios.         ",
    "url": "https://arxiv.org/abs/2412.15312",
    "authors": [
      "Joseanne Viana",
      "Hamed Farkhari",
      "Pedro Sebastiao",
      "Victor P Gil Jimenez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.19496",
    "title": "Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for Large Vision-Language Models",
    "abstract": "           Large Vision-Language Models (LVLMs) exhibit impressive potential across various tasks but also face significant privacy risks, limiting their practical applications. Current researches on privacy assessment for LVLMs is limited in scope, with gaps in both assessment dimensions and privacy categories. To bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for evaluating the privacy preservation capabilities of LVLMs in terms of privacy awareness and leakage. Privacy awareness measures the model's ability to recognize the privacy sensitivity of input data, while privacy leakage assesses the risk of the model unintentionally disclosing privacy information in its output. We design a range of sub-tasks to thoroughly evaluate the model's privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of personal privacy, 15 categories of trade secrets, and 18 categories of state secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs. Our results reveal that current LVLMs generally pose a high risk of facilitating privacy breaches, with vulnerabilities varying across personal privacy, trade secret, and state secret.         ",
    "url": "https://arxiv.org/abs/2412.19496",
    "authors": [
      "Jie Zhang",
      "Xiangkui Cao",
      "Zhouyu Han",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.08896",
    "title": "Parallel Query Processing with Heterogeneous Machines",
    "abstract": "           We study the problem of computing a full Conjunctive Query in parallel using $p$ heterogeneous machines. Our computational model is similar to the MPC model, but each machine has its own cost function mapping from the number of bits it receives to a cost. An optimal algorithm should minimize the maximum cost across all machines. We consider algorithms over a single communication round and give a lower bound and matching upper bound for databases where each relation has the same cardinality. We do this for both linear cost functions like in previous work, but also for more general cost functions. For databases with relations of different cardinalities, we also find a lower bound, and give matching upper bounds for specific queries like the cartesian product, the join, the star query, and the triangle query. Our approach is inspired by the HyperCube algorithm, but there are additional challenges involved when machines have heterogeneous cost functions.         ",
    "url": "https://arxiv.org/abs/2501.08896",
    "authors": [
      "Simon Frisk",
      "Paraschos Koutris"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2501.10459",
    "title": "Efficient Traffic Prediction Through Spatio-Temporal Distillation",
    "abstract": "           Graph neural networks (GNNs) have gained considerable attention in recent years for traffic flow prediction due to their ability to learn spatio-temporal pattern representations through a graph-based message-passing framework. Although GNNs have shown great promise in handling traffic datasets, their deployment in real-life applications has been hindered by scalability constraints arising from high-order message passing. Additionally, the over-smoothing problem of GNNs may lead to indistinguishable region representations as the number of layers increases, resulting in performance degradation. To address these challenges, we propose a new knowledge distillation paradigm termed LightST that transfers spatial and temporal knowledge from a high-capacity teacher to a lightweight student. Specifically, we introduce a spatio-temporal knowledge distillation framework that helps student MLPs capture graph-structured global spatio-temporal patterns while alleviating the over-smoothing effect with adaptive knowledge distillation. Extensive experiments verify that LightST significantly speeds up traffic flow predictions by 5X to 40X compared to state-of-the-art spatio-temporal GNNs, all while maintaining superior accuracy.         ",
    "url": "https://arxiv.org/abs/2501.10459",
    "authors": [
      "Qianru Zhang",
      "Xinyi Gao",
      "Haixin Wang",
      "Siu-Ming Yiu",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2501.13456",
    "title": "KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks",
    "abstract": "           Graph neural networks (GNNs) with attention mechanisms, often referred to as attentive GNNs, have emerged as a prominent paradigm in advanced GNN models in recent years. However, our understanding of the critical process of scoring neighbor nodes remains limited, leading to the underperformance of many existing attentive GNNs. In this paper, we unify the scoring functions of current attentive GNNs and propose Kolmogorov-Arnold Attention (KAA), which integrates the Kolmogorov-Arnold Network (KAN) architecture into the scoring process. KAA enhances the performance of scoring functions across the board and can be applied to nearly all existing attentive GNNs. To compare the expressive power of KAA with other scoring functions, we introduce Maximum Ranking Distance (MRD) to quantitatively estimate their upper bounds in ranking errors for node importance. Our analysis reveals that, under limited parameters and constraints on width and depth, both linear transformation-based and MLP-based scoring functions exhibit finite expressive power. In contrast, our proposed KAA, even with a single-layer KAN parameterized by zero-order B-spline functions, demonstrates nearly infinite expressive power. Extensive experiments on both node-level and graph-level tasks using various backbone models show that KAA-enhanced scoring functions consistently outperform their original counterparts, achieving performance improvements of over 20% in some cases.         ",
    "url": "https://arxiv.org/abs/2501.13456",
    "authors": [
      "Taoran Fang",
      "Tianhong Gao",
      "Chunping Wang",
      "Yihao Shang",
      "Wei Chow",
      "Lei Chen",
      "Yang Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.16663",
    "title": "Data Duplication: A Novel Multi-Purpose Attack Paradigm in Machine Unlearning",
    "abstract": "           Duplication is a prevalent issue within datasets. Existing research has demonstrated that the presence of duplicated data in training datasets can significantly influence both model performance and data privacy. However, the impact of data duplication on the unlearning process remains largely unexplored. This paper addresses this gap by pioneering a comprehensive investigation into the role of data duplication, not only in standard machine unlearning but also in federated and reinforcement unlearning paradigms. Specifically, we propose an adversary who duplicates a subset of the target model's training set and incorporates it into the training set. After training, the adversary requests the model owner to unlearn this duplicated subset, and analyzes the impact on the unlearned model. For example, the adversary can challenge the model owner by revealing that, despite efforts to unlearn it, the influence of the duplicated subset remains in the model. Moreover, to circumvent detection by de-duplication techniques, we propose three novel near-duplication methods for the adversary, each tailored to a specific unlearning paradigm. We then examine their impacts on the unlearning process when de-duplication techniques are applied. Our findings reveal several crucial insights: 1) the gold standard unlearning method, retraining from scratch, fails to effectively conduct unlearning under certain conditions; 2) unlearning duplicated data can lead to significant model degradation in specific scenarios; and 3) meticulously crafted duplicates can evade detection by de-duplication methods.         ",
    "url": "https://arxiv.org/abs/2501.16663",
    "authors": [
      "Dayong Ye",
      "Tianqing Zhu",
      "Jiayang Li",
      "Kun Gao",
      "Bo Liu",
      "Leo Yu Zhang",
      "Wanlei Zhou",
      "Yang Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.17328",
    "title": "SIC: Similarity-Based Interpretable Image Classification with Neural Networks",
    "abstract": "           The deployment of deep learning models in critical domains necessitates a balance between high accuracy and interpretability. We introduce SIC, an inherently interpretable neural network that provides local and global explanations of its decision-making process. Leveraging the concept of case-based reasoning, SIC extracts class-representative support vectors from training images, ensuring they capture relevant features while suppressing irrelevant ones. Classification decisions are made by calculating and aggregating similarity scores between these support vectors and the input's latent feature vector. We employ B-Cos transformations, which align model weights with inputs, to yield coherent pixel-level explanations in addition to global explanations of case-based reasoning. We evaluate SIC on three tasks: fine-grained classification on Stanford Dogs and FunnyBirds, multi-label classification on Pascal VOC, and pathology detection on the RSNA dataset. Results indicate that SIC not only achieves competitive accuracy compared to state-of-the-art black-box and inherently interpretable models but also offers insightful explanations verified through practical evaluation on the FunnyBirds benchmark. Our theoretical analysis proves that these explanations fulfill established axioms for explanations. Our findings underscore SIC's potential for applications where understanding model decisions is as critical as the decisions themselves.         ",
    "url": "https://arxiv.org/abs/2501.17328",
    "authors": [
      "Tom Nuno Wolf",
      "Emre Kavak",
      "Fabian Bongratz",
      "Christian Wachinger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.18509",
    "title": "Reframing Dense Action Detection (RefDense): A Paradigm Shift in Problem Solving & a Novel Optimization Strategy",
    "abstract": "           Dense action detection involves detecting multiple co-occurring actions while action classes are often ambiguous and represent overlapping concepts. We argue that handling the dual challenge of temporal and class overlaps is too complex to effectively be tackled by a single network. To address this, we propose to decompose the task of detecting dense ambiguous actions into detecting dense, unambiguous sub-concepts that form the action classes (i.e., action entities and action motions), and assigning these sub-tasks to distinct sub-networks. By isolating these unambiguous concepts, the sub-networks can focus exclusively on resolving a single challenge, dense temporal overlaps. Furthermore, simultaneous actions in a video often exhibit interrelationships, and exploiting these relationships can improve the method performance. However, current dense action detection networks fail to effectively learn these relationships due to their reliance on binary cross-entropy optimization, which treats each class independently. To address this limitation, we propose providing explicit supervision on co-occurring concepts during network optimization through a novel language-guided contrastive learning loss. Our extensive experiments demonstrate the superiority of our approach over state-of-the-art methods, achieving substantial improvements of 3.8% and 1.7% on average across all metrics on the challenging benchmark datasets, Charades and MultiTHUMOS.         ",
    "url": "https://arxiv.org/abs/2501.18509",
    "authors": [
      "Faegheh Sardari",
      "Armin Mustafa",
      "Philip J. B. Jackson",
      "Adrian Hilton"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.03424",
    "title": "Prediction of the Most Fire-Sensitive Point in Building Structures with Differentiable Agents for Thermal Simulators",
    "abstract": "           Fire safety is crucial for ensuring the stability of building structures, yet evaluating whether a structure meets fire safety requirement is challenging. Fires can originate at any point within a structure, and simulating every potential fire scenario is both expensive and time-consuming. To address this challenge, we propose the concept of the Most Fire-Sensitive Point (MFSP) and an efficient machine learning framework for its identification. The MFSP is defined as the location at which a fire, if initiated, would cause the most severe detrimental impact on the building's stability, effectively representing the worst-case fire scenario. In our framework, a Graph Neural Network (GNN) serves as an efficient and differentiable agent for conventional Finite Element Analysis (FEA) simulators by predicting the Maximum Interstory Drift Ratio (MIDR) under fire, which then guides the training and evaluation of the MFSP predictor. Additionally, we enhance our framework with a novel edge update mechanism and a transfer learning-based training scheme. Evaluations on a large-scale simulation dataset demonstrate the good performance of the proposed framework in identifying the MFSP, offering a transformative tool for optimizing fire safety assessments in structural design. All developed datasets and codes are open-sourced online.         ",
    "url": "https://arxiv.org/abs/2502.03424",
    "authors": [
      "Yuan Xinjie",
      "Khalid M. Mosalam"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.08209",
    "title": "Equivariant Masked Position Prediction for Efficient Molecular Representation",
    "abstract": "           Graph neural networks (GNNs) have shown considerable promise in computational chemistry. However, the limited availability of molecular data raises concerns regarding GNNs' ability to effectively capture the fundamental principles of physics and chemistry, which constrains their generalization capabilities. To address this challenge, we introduce a novel self-supervised approach termed Equivariant Masked Position Prediction (EMPP), grounded in intramolecular potential and force theory. Unlike conventional attribute masking techniques, EMPP formulates a nuanced position prediction task that is more well-defined and enhances the learning of quantum mechanical features. EMPP also bypasses the approximation of the Gaussian mixture distribution commonly used in denoising methods, allowing for more accurate acquisition of physical properties. Experimental results indicate that EMPP significantly enhances performance of advanced molecular architectures, surpassing state-of-the-art self-supervised approaches. Our code is released in this https URL ",
    "url": "https://arxiv.org/abs/2502.08209",
    "authors": [
      "Junyi An",
      "Chao Qu",
      "Yun-Fei Shi",
      "XinHao Liu",
      "Qianwei Tang",
      "Fenglei Cao",
      "Yuan Qi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.11054",
    "title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models",
    "abstract": "           Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation, a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMs' strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at this https URL to facilitate further research in this critical domain.         ",
    "url": "https://arxiv.org/abs/2502.11054",
    "authors": [
      "Zonghao Ying",
      "Deyue Zhang",
      "Zonglei Jing",
      "Yisong Xiao",
      "Quanchen Zou",
      "Aishan Liu",
      "Siyuan Liang",
      "Xiangzheng Zhang",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.15488",
    "title": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection",
    "abstract": "           Camera-based multi-view 3D detection has emerged as an attractive solution for autonomous driving due to its low cost and broad applicability. However, despite the strong performance of PETR-based methods in 3D perception benchmarks, their direct INT8 quantization for onboard deployment leads to drastic accuracy drops-up to 58.2% in mAP and 36.9% in NDS on the NuScenes dataset. In this work, we propose Q-PETR, a quantization-aware position embedding transformation that re-engineers key components of the PETR framework to reconcile the discrepancy between the dynamic ranges of positional encodings and image features, and to adapt the cross-attention mechanism for low-bit inference. By redesigning the positional encoding module and introducing an adaptive quantization strategy, Q-PETR maintains floating-point performance with a performance degradation of less than 1% under standard 8-bit per-tensor post-training quantization. Moreover, compared to its FP32 counterpart, Q-PETR achieves a two-fold speedup and reduces memory usage by three times, thereby offering a deployment-friendly solution for resource-constrained onboard devices. Extensive experiments across various PETR-series models validate the strong generalization and practical benefits of our approach.         ",
    "url": "https://arxiv.org/abs/2502.15488",
    "authors": [
      "Jiangyong Yu",
      "Changyong Shu",
      "Dawei Yang",
      "Sifan Zhou",
      "Zichen Yu",
      "Xing Hu",
      "Yan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.17591",
    "title": "Proactive Privacy Amnesia for Large Language Models: Safeguarding PII with Negligible Impact on Model Utility",
    "abstract": "           With the rise of large language models (LLMs), increasing research has recognized their risk of leaking personally identifiable information (PII) under malicious attacks. Although efforts have been made to protect PII in LLMs, existing methods struggle to balance privacy protection with maintaining model utility. In this paper, inspired by studies of amnesia in cognitive science, we propose a novel approach, Proactive Privacy Amnesia (PPA), to safeguard PII in LLMs while preserving their utility. This mechanism works by actively identifying and forgetting key memories most closely associated with PII in sequences, followed by a memory implanting using suitable substitute memories to maintain the LLM's functionality. We conduct evaluations across multiple models to protect common PII, such as phone numbers and physical addresses, against prevalent PII-targeted attacks, demonstrating the superiority of our method compared with other existing defensive techniques. The results show that our PPA method completely eliminates the risk of phone number exposure by 100% and significantly reduces the risk of physical address exposure by 9.8% - 87.6%, all while maintaining comparable model utility performance.         ",
    "url": "https://arxiv.org/abs/2502.17591",
    "authors": [
      "Martin Kuo",
      "Jingyang Zhang",
      "Jianyi Zhang",
      "Minxue Tang",
      "Louis DiValentin",
      "Aolin Ding",
      "Jingwei Sun",
      "William Chen",
      "Amin Hass",
      "Tianlong Chen",
      "Yiran Chen",
      "Hai Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.00852",
    "title": "A Transfer Framework for Enhancing Temporal Graph Learning in Data-Scarce Settings",
    "abstract": "           Dynamic interactions between entities are prevalent in domains like social platforms, financial systems, healthcare, and e-commerce. These interactions can be effectively represented as time-evolving graphs, where predicting future connections is a key task in applications such as recommendation systems. Temporal Graph Neural Networks (TGNNs) have achieved strong results for such predictive tasks but typically require extensive training data, which is often limited in real-world scenarios. One approach to mitigating data scarcity is leveraging pre-trained models from related datasets. However, direct knowledge transfer between TGNNs is challenging due to their reliance on node-specific memory structures, making them inherently difficult to adapt across datasets. To address this, we introduce a novel transfer approach that disentangles node representations from their associated features through a structured bipartite encoding mechanism. This decoupling enables more effective transfer of memory components and other learned inductive patterns from one dataset to another. Empirical evaluations on real-world benchmarks demonstrate that our method significantly enhances TGNN performance in low-data regimes, outperforming non-transfer baselines by up to 56\\% and surpassing existing transfer strategies by 36\\%         ",
    "url": "https://arxiv.org/abs/2503.00852",
    "authors": [
      "Sidharth Agarwal",
      "Tanishq Dubey",
      "Shubham Gupta",
      "Srikanta Bedathur"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2503.00860",
    "title": "Hierarchical graph sampling based minibatch learning with chain preservation and variance reduction",
    "abstract": "           Graph sampling based Graph Convolutional Networks (GCNs) decouple the sampling from the forward and backward propagation during minibatch training, which exhibit good scalability in terms of layer depth and graph size. We propose HIS_GCNs, a hierarchical importance graph sampling based learning method. By constructing minibatches using sampled subgraphs, HIS_GCNs gives attention to the importance of both core and periphery nodes/edges in a scale-free training graph. Specifically, it preserves the centrum of the core to most minibatches, which maintains connectivity between periphery nodes, and samples periphery edges without core node interference, in order to keep more long chains composed entirely of low-degree nodes in the same minibatch. HIS_GCNs can maximize the discrete Ricci curvature (i.e., Ollivier-Ricci curvatures) of the edges in a subgraph that enables the preservation of important chains for information propagation, and can achieve a low node embedding variance and a high convergence speed. Diverse experiments on Graph Neural Networks (GNNs) with node classification tasks confirm superior performance of HIS_GCNs in both accuracy and training time. Open sourced code (this https URL).         ",
    "url": "https://arxiv.org/abs/2503.00860",
    "authors": [
      "Qia Hu",
      "Bo Jiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.02162",
    "title": "X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning",
    "abstract": "           Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible and safer, existing CXR foundation models focus primarily on detecting diseases that are readily visible on the CXR. Recently, works have explored training disease classification models on simulated CXRs, but they remain limited to recognizing a single disease type from CT. CT foundation models have also emerged with significantly improved detection of pathologies in CT. However, the generalized application of CT-derived labels on CXR has remained illusive. In this study, we propose X2CT-CLIP, a tri-modal knowledge transfer learning framework that bridges the modality gap between CT and CXR while reducing the computational burden of model training. Our approach is the first work to enable multi-abnormality classification in CT, using CXR, by transferring knowledge from 3D CT volumes and associated radiology reports to a CXR encoder via a carefully designed tri-modal alignment mechanism in latent space. Extensive evaluations on three multi-label CT datasets demonstrate that our method outperforms state-of-the-art baselines in cross-modal retrieval, few-shot adaptation, and external validation. These results highlight the potential of CXR, enriched with knowledge derived from CT, as a viable efficient alternative for disease detection in resource-limited settings.         ",
    "url": "https://arxiv.org/abs/2503.02162",
    "authors": [
      "Jianzhong You",
      "Yuan Gao",
      "Sangwook Kim",
      "Chris Mcintosh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.02783",
    "title": "IterPref: Focal Preference Learning for Code Generation via Iterative Debugging",
    "abstract": "           Preference learning enhances Code LLMs beyond supervised fine-tuning by leveraging relative quality comparisons. Existing methods construct preference pairs from candidates based on test case success, treating the higher pass rate sample as positive and the lower as negative. However, this approach does not pinpoint specific errors in the code, which prevents the model from learning more informative error correction patterns, as aligning failing code as a whole lacks the granularity needed to capture meaningful error-resolution relationships. To address these issues, we propose IterPref, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. IterPref explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To generate informative pairs, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with IterPref achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our code and data will be made publicaly available.         ",
    "url": "https://arxiv.org/abs/2503.02783",
    "authors": [
      "Jie Wu",
      "Haoling Li",
      "Xin Zhang",
      "Jianwen Luo",
      "Yangyu Huang",
      "Ruihang Chu",
      "Yujiu Yang",
      "Scarlett Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.02800",
    "title": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration",
    "abstract": "           Anomaly detection in complex industrial environments poses unique challenges, particularly in contexts characterized by data sparsity and evolving operational conditions. Predictive maintenance (PdM) in such settings demands methodologies that are adaptive, transferable, and capable of integrating domain-specific knowledge. In this paper, we present RAAD-LLM, a novel framework for adaptive anomaly detection, leveraging large language models (LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach addresses the aforementioned PdM challenges. By effectively utilizing domain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time series data without requiring fine-tuning on specific datasets. The framework's adaptability mechanism enables it to adjust its understanding of normal operating conditions dynamically, thus increasing detection accuracy. We validate this methodology through a real-world application for a plastics manufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show significant improvements over our previous model with an accuracy increase from 70.7% to 88.6% on the real-world dataset. By allowing for the enriching of input series data with semantics, RAAD-LLM incorporates multimodal capabilities that facilitate more collaborative decision-making between the model and plant operators. Overall, our findings support RAAD-LLM's ability to revolutionize anomaly detection methodologies in PdM, potentially leading to a paradigm shift in how anomaly detection is implemented across various industries.         ",
    "url": "https://arxiv.org/abs/2503.02800",
    "authors": [
      "Alicia Russell-Gilbert",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Maria Seale",
      "Joseph Jabour",
      "Thomas Arnold",
      "Joshua Church"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2503.04036",
    "title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge",
    "abstract": "           Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques primarily focus on effective memorization after pretraining, while overlooking challenges that arise in other stages of the LLM pipeline, such as the risk of watermark filtering during data preprocessing, or potential forgetting through post-training, or verification difficulties due to API-only access. We propose a novel data watermarking approach that injects coherent and plausible yet fictitious knowledge into training data using generated passages describing a fictitious entity and its associated attributes. Our watermarks are designed to be memorized by the LLM through seamlessly integrating in its training data, making them harder to detect lexically during preprocessing. We demonstrate that our watermarks can be effectively memorized by LLMs, and that increasing our watermarks' density, length, and diversity of attributes strengthens their memorization. We further show that our watermarks remain robust throughout LLM development, maintaining their effectiveness after continual pretraining and supervised finetuning. Finally, we show that our data watermarks can be evaluated even under API-only access via question answering.         ",
    "url": "https://arxiv.org/abs/2503.04036",
    "authors": [
      "Xinyue Cui",
      "Johnny Tian-Zheng Wei",
      "Swabha Swayamdipta",
      "Robin Jia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.04784",
    "title": "KunlunBaize: LLM with Multi-Scale Convolution and Multi-Token Prediction Under TransformerX Framework",
    "abstract": "           Large language models have demonstrated remarkable performance across various tasks, yet they face challenges such as low computational efficiency, gradient vanishing, and difficulties in capturing complex feature interactions. To address these limitations, a novel framework has been proposed. This framework incorporates a learnable dense residual skip connection mechanism, a TransformerX module a transformer based component integrating multiscale convolution and adaptive activation functions and a multitoken prediction interaction module. The learnable dense residual connections enhance information flow and feature capture across layers. Within the TransformerX module, large convolutional kernels aggregate semantic information from extensive text segments, while smaller convolutions focus on local word order and syntactic structures. The adaptive activation function dynamically adjusts its parameters based on the semantic features of the input text, improving the model's ability to handle diverse semantic expressions and complex relationships. The multitoken prediction module boosts data utilization and accelerates inference by predicting multiple future tokens. These components significantly enhance the performance and efficiency of large language models.         ",
    "url": "https://arxiv.org/abs/2503.04784",
    "authors": [
      "Cheng Li",
      "Jiexiong Liu",
      "Yixuan Chen",
      "Yanqin Jia",
      "Zhepeng Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.04838",
    "title": "Combined Physics and Event Camera Simulator for Slip Detection",
    "abstract": "           Robot manipulation is a common task in fields like industrial manufacturing. Detecting when objects slip from a robot's grasp is crucial for safe and reliable operation. Event cameras, which register pixel-level brightness changes at high temporal resolution (called ``events''), offer an elegant feature when mounted on a robot's end effector: since they only detect motion relative to their viewpoint, a properly grasped object produces no events, while a slipping object immediately triggers them. To research this feature, representative datasets are essential, both for analytic approaches and for training machine learning models. The majority of current research on slip detection with event-based data is done on real-world scenarios and manual data collection, as well as additional setups for data labeling. This can result in a significant increase in the time required for data collection, a lack of flexibility in scene setups, and a high level of complexity in the repetition of experiments. This paper presents a simulation pipeline for generating slip data using the described camera-gripper configuration in a robot arm, and demonstrates its effectiveness through initial data-driven experiments. The use of a simulator, once it is set up, has the potential to reduce the time spent on data collection, provide the ability to alter the setup at any time, simplify the process of repetition and the generation of arbitrarily large data sets. Two distinct datasets were created and validated through visual inspection and artificial neural networks (ANNs). Visual inspection confirmed photorealistic frame generation and accurate slip modeling, while three ANNs trained on this data achieved high validation accuracy and demonstrated good generalization capabilities on a separate test set, along with initial applicability to real-world data. Project page: this https URL ",
    "url": "https://arxiv.org/abs/2503.04838",
    "authors": [
      "Thilo Reinold",
      "Suman Ghosh",
      "Guillermo Gallego"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.05794",
    "title": "CBW: Towards Dataset Ownership Verification for Speaker Verification via Clustering-based Backdoor Watermarking",
    "abstract": "           With the increasing adoption of deep learning in speaker verification, large-scale speech datasets have become valuable intellectual property. To audit and prevent the unauthorized usage of these valuable released datasets, especially in commercial or open-source scenarios, we propose a novel dataset ownership verification method. Our approach introduces a clustering-based backdoor watermark (CBW), enabling dataset owners to determine whether a suspicious third-party model has been trained on a protected dataset under a black-box setting. The CBW method consists of two key stages: dataset watermarking and ownership verification. During watermarking, we implant multiple trigger patterns in the dataset to make similar samples (measured by their feature similarities) close to the same trigger while dissimilar samples are near different triggers. This ensures that any model trained on the watermarked dataset exhibits specific misclassification behaviors when exposed to trigger-embedded inputs. To verify dataset ownership, we design a hypothesis-test-based framework that statistically evaluates whether a suspicious model exhibits the expected backdoor behavior. We conduct extensive experiments on benchmark datasets, verifying the effectiveness and robustness of our method against potential adaptive attacks. The code for reproducing main experiments is available at this https URL ",
    "url": "https://arxiv.org/abs/2503.05794",
    "authors": [
      "Yiming Li",
      "Kaiying Yan",
      "Shuo Shao",
      "Tongqing Zhai",
      "Shu-Tao Xia",
      "Zhan Qin",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2503.06443",
    "title": "Mobility-Aware Decentralized Federated Learning with Joint Optimization of Local Iteration and Leader Selection for Vehicular Networks",
    "abstract": "           Federated learning (FL) emerges as a promising approach to empower vehicular networks, composed by intelligent connected vehicles equipped with advanced sensing, computing, and communication capabilities. While previous studies have explored the application of FL in vehicular networks, they have largely overlooked the intricate challenges arising from the mobility of vehicles and resource constraints. In this paper, we propose a framework of mobility-aware decentralized federated learning (MDFL) for vehicular networks. In this framework, nearby vehicles train an FL model collaboratively, yet in a decentralized manner. We formulate a local iteration and leader selection joint optimization problem (LSOP) to improve the training efficiency of MDFL. For problem solving, we first reformulate LSOP as a decentralized partially observable Markov decision process (Dec-POMDP), and then develop an effective optimization algorithm based on multi-agent proximal policy optimization (MAPPO) to solve Dec-POMDP. Finally, we verify the performance of the proposed algorithm by comparing it with other algorithms.         ",
    "url": "https://arxiv.org/abs/2503.06443",
    "authors": [
      "Dongyu Chen",
      "Tao Deng",
      "Juncheng Jia",
      "Siwei Feng",
      "Di Yuan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.06966",
    "title": "MIGA: Mutual Information-Guided Attack on Denoising Models for Semantic Manipulation",
    "abstract": "           Deep learning-based denoising models have been widely employed in vision tasks, functioning as filters to eliminate noise while retaining crucial semantic information. Additionally, they play a vital role in defending against adversarial perturbations that threaten downstream tasks. However, these models can be intrinsically susceptible to adversarial attacks due to their dependence on specific noise assumptions. Existing attacks on denoising models mainly aim at deteriorating visual clarity while neglecting semantic manipulation, rendering them either easily detectable or limited in effectiveness. In this paper, we propose Mutual Information-Guided Attack (MIGA), the first method designed to directly attack deep denoising models by strategically disrupting their ability to preserve semantic content via adversarial perturbations. By minimizing the mutual information between the original and denoised images, a measure of semantic similarity. MIGA forces the denoiser to produce perceptually clean yet semantically altered outputs. While these images appear visually plausible, they encode systematically distorted semantics, revealing a fundamental vulnerability in denoising models. These distortions persist in denoised outputs and can be quantitatively assessed through downstream task performance. We propose new evaluation metrics and systematically assess MIGA on four denoising models across five datasets, demonstrating its consistent effectiveness in disrupting semantic fidelity. Our findings suggest that denoising models are not always robust and can introduce security risks in real-world applications.         ",
    "url": "https://arxiv.org/abs/2503.06966",
    "authors": [
      "Guanghao Li",
      "Mingzhi Chen",
      "Hao Yu",
      "Shuting Dong",
      "Wenhao Jiang",
      "Ming Tang",
      "Chun Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.06990",
    "title": "TiGer: Self-Supervised Purification for Time-evolving Graphs",
    "abstract": "           Time-evolving graphs, such as social and citation networks, often contain noise that distorts structural and temporal patterns, adversely affecting downstream tasks, such as node classification. Existing purification methods focus on static graphs, limiting their ability to account for critical temporal dependencies in dynamic graphs. In this work, we propose TiGer (Time-evolving Graph purifier), a self-supervised method explicitly designed for time-evolving graphs. TiGer assigns two different sub-scores to edges using (1) self-attention for capturing long-term contextual patterns shaped by both adjacent and distant past events of varying significance and (2) statistical distance measures for detecting inconsistency over a short-term period. These sub-scores are used to identify and filter out suspicious (i.e., noise-like) edges through an ensemble strategy, ensuring robustness without requiring noise labels. Our experiments on five real-world datasets show TiGer filters out noise with up to 10.2% higher accuracy and improves node classification performance by up to 5.3%, compared to state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2503.06990",
    "authors": [
      "Hyeonsoo Jo",
      "Jongha Lee",
      "Fanchen Bu",
      "Kijung Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.07216",
    "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates",
    "abstract": "           Federated Learning (FL) is a widely used framework for training models in a decentralized manner, ensuring that the central server does not have direct access to data from local clients. However, this approach may still fail to fully preserve data privacy, as models from local clients are exposed to the central server during the aggregation process. This issue becomes even more critical when training vision-language models (VLMs) with FL, as VLMs can easily memorize training data instances, making them vulnerable to membership inference attacks (MIAs). To address this challenge, we propose the FedRand framework, which avoids disclosing the full set of client parameters. In this framework, each client randomly selects subparameters of Low-Rank Adaptation (LoRA) from the server and keeps the remaining counterparts of the LoRA weights as private parameters. After training both parameters on the client's private dataset, only the non-private client parameters are sent back to the server for aggregation. This approach mitigates the risk of exposing client-side VLM parameters, thereby enhancing data privacy. We empirically validate that FedRand improves robustness against MIAs compared to relevant baselines while achieving accuracy comparable to methods that communicate full LoRA parameters across several benchmark datasets.         ",
    "url": "https://arxiv.org/abs/2503.07216",
    "authors": [
      "Sangwoo Park",
      "Seanie Lee",
      "Byungjoo Kim",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.07347",
    "title": "DaD: Distilled Reinforcement Learning for Diverse Keypoint Detection",
    "abstract": "           Keypoints are what enable Structure-from-Motion (SfM) systems to scale to thousands of images. However, designing a keypoint detection objective is a non-trivial task, as SfM is non-differentiable. Typically, an auxiliary objective involving a descriptor is optimized. This however induces a dependency on the descriptor, which is undesirable. In this paper we propose a fully self-supervised and descriptor-free objective for keypoint detection, through reinforcement learning. To ensure training does not degenerate, we leverage a balanced top-K sampling strategy. While this already produces competitive models, we find that two qualitatively different types of detectors emerge, which are only able to detect light and dark keypoints respectively. To remedy this, we train a third detector, DaD, that optimizes the Kullback-Leibler divergence of the pointwise maximum of both light and dark detectors. Our approach significantly improve upon SotA across a range of benchmarks. Code and model weights are publicly available at this https URL ",
    "url": "https://arxiv.org/abs/2503.07347",
    "authors": [
      "Johan Edstedt",
      "Georg B\u00f6kman",
      "M\u00e5rten Wadenb\u00e4ck",
      "Michael Felsberg"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.07385",
    "title": "Stochastic Tube-based Model Predictive Control for Cyber-Physical Systems under False Data Injection Attacks with Bounded Probability",
    "abstract": "           This paper addresses the challenge of amplitude-unbounded false data injection (FDI) attacks targeting the sensor-to-controller (S-C) channel in cyber-physical systems (CPSs). We introduce a resilient tube-based model predictive control (MPC) scheme. This scheme incorporates a threshold-based attack detector and a control sequence buffer to enhance system security. We mathematically model the common FDI attacks and derive the maximum duration of such attacks based on the hypothesis testing principle. Following this, the minimum feasible sequence length of the control sequence buffer is obtained. The system is proven to remain input-to-state stable (ISS) under bounded external disturbances and amplitude-unbounded FDI attacks. Moreover, the feasible region under this scenario is provided in this paper. Finally, the proposed algorithm is validated by numerical simulations and shows superior control performance compared to the existing methods.         ",
    "url": "https://arxiv.org/abs/2503.07385",
    "authors": [
      "Yuzhou Xiao",
      "Senchun Chai",
      "Li Dai",
      "Yuanqing Xia",
      "Runqi Chai"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2305.11857",
    "title": "Computing high-dimensional optimal transport by flow neural networks",
    "abstract": "           Computing optimal transport (OT) for general high-dimensional data has been a long-standing challenge. Despite much progress, most of the efforts including neural network methods have been focused on the static formulation of the OT problem. The current work proposes to compute the dynamic OT between two arbitrary distributions $P$ and $Q$ by optimizing a flow model, where both distributions are only accessible via finite samples. Our method learns the dynamic OT by finding an invertible flow that minimizes the transport cost. The trained optimal transport flow subsequently allows for performing many downstream tasks, including infinitesimal density ratio estimation (DRE) and domain adaptation by interpolating distributions in the latent space. The effectiveness of the proposed model on high-dimensional data is demonstrated by strong empirical performance on OT baselines, image-to-image translation, and high-dimensional DRE.         ",
    "url": "https://arxiv.org/abs/2305.11857",
    "authors": [
      "Chen Xu",
      "Xiuyuan Cheng",
      "Yao Xie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2403.01865",
    "title": "Out-of-distribution robustness for multivariate analysis via causal regularisation",
    "abstract": "           We propose a regularisation strategy of classical machine learning algorithms rooted in causality that ensures robustness against distribution shifts. Building upon the anchor regression framework, we demonstrate how incorporating a straightforward regularisation term into the loss function of classical multivariate analysis algorithms, such as (orthonormalized) partial least squares, reduced-rank regression, and multiple linear regression, enables out-of-distribution generalisation. Our framework allows users to efficiently verify the compatibility of a loss function with the regularisation strategy. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with multivariate analysis approaches and its role in enhancing replicability while guarding against distribution shifts. The extended anchor framework advances causal inference methodologies, addressing the need for reliable out-of-distribution generalisation.         ",
    "url": "https://arxiv.org/abs/2403.01865",
    "authors": [
      "Homer Durand",
      "Gherardo Varando",
      "Nathan Mankovich",
      "Gustau Camps-Valls"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2408.03833",
    "title": "Multilinear Extensions in Submodular Optimization for Optimal Sensor Scheduling in Nonlinear Networks",
    "abstract": "           Optimal sensing nodes selection (SNS) in dynamic systems is a combinatorial optimization problem that has been thoroughly studied in the recent literature. This problem can be formulated within the context of set optimization. For high-dimensional nonlinear systems, the problem is extremely difficult to solve. It scales poorly too. Current literature poses combinatorial submodular set optimization problems via maximizing observability performance metrics subject to matroid constraints. Such an approach is typically solved using greedy algorithms that require lower computational effort yet often yield sub-optimal solutions. In this paper, we address the SNS problem for nonlinear dynamical networks using a variational form of the system dynamics, that basically perturb the system physics. As a result, we show that the observability performance metrics under such system representation are indeed submodular. The optimal problem is then solved using the multilinear continuous extension. This extension offers a computationally scalable and approximate continuous relaxation with a performance guarantee. The effectiveness of the extended submodular program is studied and compared to greedy algorithms. We demonstrate the proposed set optimization formulation for SNS on nonlinear natural gas combustion networks.         ",
    "url": "https://arxiv.org/abs/2408.03833",
    "authors": [
      "Mohamad H. Kazma",
      "Ahmad F. Taha"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2410.19105",
    "title": "Conditional diffusions for neural posterior estimation",
    "abstract": "           Neural posterior estimation (NPE), a simulation-based computational approach for Bayesian inference, has shown great success in approximating complex posterior distributions. Existing NPE methods typically rely on normalizing flows, which approximate a distribution by composing many simple, invertible transformations. But flow-based models, while state of the art for NPE, are known to suffer from several limitations, including training instability and sharp trade-offs between representational power and computational cost. In this work, we demonstrate the effectiveness of conditional diffusions coupled with high-capacity summary networks for amortized NPE. Conditional diffusions address many of the challenges faced by flow-based methods. Our results show that, across a highly varied suite of benchmarking problems for NPE architectures, diffusions offer improved stability, superior accuracy, and faster training times, even with simpler, shallower models. Building on prior work on diffusions for NPE, we show that these gains persist across a variety of different summary network architectures. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.19105",
    "authors": [
      "Tianyu Chen",
      "Vansh Bansal",
      "James G. Scott"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2410.19147",
    "title": "Functional Brain Network Identification in Opioid Use Disorder Using Machine Learning Analysis of Resting-State fMRI BOLD Signals",
    "abstract": "           Understanding the neurobiology of opioid use disorder (OUD) using resting-state functional magnetic resonance imaging (rs-fMRI) may help inform treatment strategies to improve patient outcomes. Recent literature suggests time-frequency characteristics of rs-fMRI blood oxygenation level-dependent (BOLD) signals may offer complementary information to traditional analysis techniques. However, existing studies of OUD analyze BOLD signals using measures computed across all time points. This study, for the first time in the literature, employs data-driven machine learning (ML) for time-frequency analysis of local neural activity within key functional networks to differentiate OUD subjects from healthy controls (HC). We obtain time-frequency features based on rs-fMRI BOLD signals from the default mode network (DMN), salience network (SN), and executive control network (ECN) for 31 OUD and 45 HC subjects. Then, we perform 5-fold cross-validation classification (OUD vs. HC) experiments to study the discriminative power of functional network features while taking into consideration significant demographic features. The DMN and SN show the most discriminative power, significantly (p < 0.05) outperforming chance baselines with mean F1 scores of 0.7097 and 0.7018, respectively, and mean AUCs of 0.8378 and 0.8755, respectively. Follow-up Boruta ML analysis of selected time-frequency (wavelet) features reveals significant (p < 0.05) detail coefficients for all three functional networks, underscoring the need for ML and time-frequency analysis of rs-fMRI BOLD signals in the study of OUD.         ",
    "url": "https://arxiv.org/abs/2410.19147",
    "authors": [
      "Ahmed Temtam",
      "Megan A. Witherow",
      "Liangsuo Ma",
      "M. Shibly Sadique",
      "F. Gerard Moeller",
      "Khan M. Iftekharuddin"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.19780",
    "title": "Sampling from Bayesian Neural Network Posteriors with Symmetric Minibatch Splitting Langevin Dynamics",
    "abstract": "           We propose a scalable kinetic Langevin dynamics algorithm for sampling parameter spaces of big data and AI applications. Our scheme combines a symmetric forward/backward sweep over minibatches with a symmetric discretization of Langevin dynamics. For a particular Langevin splitting method (UBU), we show that the resulting Symmetric Minibatch Splitting-UBU (SMS-UBU) integrator has bias $O(h^2 d^{1/2})$ in dimension $d>0$ with stepsize $h>0$, despite only using one minibatch per iteration, thus providing excellent control of the sampling bias as a function of the stepsize. We apply the algorithm to explore local modes of the posterior distribution of Bayesian neural networks (BNNs) and evaluate the calibration performance of the posterior predictive probabilities for neural networks with convolutional neural network architectures for classification problems on three different datasets (Fashion-MNIST, Celeb-A and chest X-ray). Our results indicate that BNNs sampled with SMS-UBU can offer significantly better calibration performance compared to standard methods of training and stochastic weight averaging.         ",
    "url": "https://arxiv.org/abs/2410.19780",
    "authors": [
      "Daniel Paulin",
      "Peter A. Whalley",
      "Neil K. Chada",
      "Benedict Leimkuhler"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2411.02177",
    "title": "Physics-informed neural networks viewpoint for solving the Dyson-Schwinger equations of quantum electrodynamics",
    "abstract": "           Physics-informed neural networks (PINNs) are employed to solve the Dyson--Schwinger equations of quantum electrodynamics (QED) in Euclidean space, with a focus on the non-perturbative generation of the fermion's dynamical mass function in the Landau gauge. By inserting the integral equation directly into the loss function, our PINN framework enables a single neural network to learn a continuous and differentiable representation of the mass function over a spectrum of momenta. Also, we benchmark our approach against a traditional numerical algorithm showing the main differences among them. Our novel strategy, which can be extended to other quantum field theories, paves the way for forefront applications of machine learning in high-level theoretical physics.         ",
    "url": "https://arxiv.org/abs/2411.02177",
    "authors": [
      "Rodrigo Carmo Terin"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Theory (hep-th)"
    ]
  },
  {
    "id": "arXiv:2411.12159",
    "title": "Sensor-fusion based Prognostics Framework for Complex Engineering Systems Exhibiting Multiple Failure Modes",
    "abstract": "           Complex engineering systems are often subject to multiple failure modes. Developing a remaining useful life (RUL) prediction model that does not consider the failure mode causing degradation is likely to result in inaccurate predictions. However, distinguishing between causes of failure without manually inspecting the system is nontrivial. This challenge is increased when the causes of historically observed failures are unknown. Sensors, which are useful for monitoring the state-of-health of systems, can also be used for distinguishing between multiple failure modes as the presence of multiple failure modes results in discriminatory behavior of the sensor signals. When systems are equipped with multiple sensors, some sensors may exhibit behavior correlated with degradation, while other sensors do not. Furthermore, which sensors exhibit this behavior may differ for each failure mode. In this paper, we present a simultaneous clustering and sensor selection approach for unlabeled training datasets of systems exhibiting multiple failure modes. The cluster assignments and the selected sensors are then utilized in real-time to first diagnose the active failure mode and then to predict the system RUL. We validate the methodology using a simulated dataset of systems exhibiting two failure modes and on NASA turbofan degradation dataset.         ",
    "url": "https://arxiv.org/abs/2411.12159",
    "authors": [
      "Benjamin Peters",
      "Ayush Mohanty",
      "Xiaolei Fang",
      "Stephen K. Robinson",
      "Nagi Gebraeel"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2411.13364",
    "title": "Effective dimensional reduction of complex systems based on tensor networks",
    "abstract": "           The exact treatment of Markovian models of complex systems requires knowledge of probability distributions exponentially large in the number of components $n$. Mean-field approximations provide an effective reduction in complexity of the models, requiring only a number of phase space variables polynomial in system size. However, this comes at the cost of losing accuracy close to critical points in the systems dynamics and an inability to capture correlations in the system. In this work, we introduce a tunable approximation scheme for Markovian spreading models on networks based on Matrix Product States (MPS). By controlling the bond dimensions of the MPS, we can investigate the effective dimensionality needed to accurately represent the exact $2^n$ dimensional steady-state distribution. We introduce the entanglement entropy as a measure of the compressibility of the system and find that it peaks just after the phase transition on the disordered side, in line with the intuition that more complex states are at the 'edge of chaos'. We compare the accuracy of the MPS with exact methods on different types of small random networks and with Markov Chain Monte Carlo methods for a simplified version of the railway network of the Netherlands with 55 nodes. The MPS provides a systematic way to tune the accuracy of the approximation by reducing the dimensionality of the systems state vector, leading to an improvement over second-order mean-field approximations for sufficiently large bond dimensions.         ",
    "url": "https://arxiv.org/abs/2411.13364",
    "authors": [
      "Wout Merbis",
      "Madelon Geurts",
      "Cl\u00e9lia de Mulatier",
      "Philippe Corboz"
    ],
    "subjectives": [
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2502.14009",
    "title": "Benchmarking Self-Supervised Methods for Accelerated MRI Reconstruction",
    "abstract": "           Reconstructing MRI from highly undersampled measurements is crucial for accelerating medical imaging, but is challenging due to the ill-posedness of the inverse problem. While supervised deep learning approaches have shown remarkable success, they rely on fully-sampled ground truth data, which is often impractical or impossible to obtain. Recently, numerous self-supervised methods have emerged that do not require ground truth, however, the lack of systematic comparison and standard experimental setups have hindered research. We present the first comprehensive review of loss functions from all feedforward self-supervised methods and the first benchmark on accelerated MRI reconstruction without ground truth, showing that there is a wide range in performance across methods. In addition, we propose Multi-Operator Equivariant Imaging (MO-EI), a novel framework that builds on the imaging model considered in existing methods to outperform all state-of-the-art and approaches supervised performance. Finally, to facilitate reproducible benchmarking, we provide implementations of all methods in the DeepInverse library (this https URL) and easy-to-use demo code at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.14009",
    "authors": [
      "Andrew Wang",
      "Mike Davies"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.04966",
    "title": "Prediction of Frozen Region Growth in Kidney Cryoablation Intervention Using a 3D Flow-Matching Model",
    "abstract": "           This study presents a 3D flow-matching model designed to predict the progression of the frozen region (iceball) during kidney cryoablation. Precise intraoperative guidance is critical in cryoablation to ensure complete tumor eradication while preserving adjacent healthy tissue. However, conventional methods, typically based on physics driven or diffusion based simulations, are computationally demanding and often struggle to represent complex anatomical structures accurately. To address these limitations, our approach leverages intraoperative CT imaging to inform the model. The proposed 3D flow matching model is trained to learn a continuous deformation field that maps early-stage CT scans to future predictions. This transformation not only estimates the volumetric expansion of the iceball but also generates corresponding segmentation masks, effectively capturing spatial and morphological changes over time. Quantitative analysis highlights the model robustness, demonstrating strong agreement between predictions and ground-truth segmentations. The model achieves an Intersection over Union (IoU) score of 0.61 and a Dice coefficient of 0.75. By integrating real time CT imaging with advanced deep learning techniques, this approach has the potential to enhance intraoperative guidance in kidney cryoablation, improving procedural outcomes and advancing the field of minimally invasive surgery.         ",
    "url": "https://arxiv.org/abs/2503.04966",
    "authors": [
      "Siyeop Yoon",
      "Yujin Oh",
      "Matthew Tivnan",
      "Sifan Song",
      "Pengfei Jin",
      "Sekeun Kim",
      "Hyun Jin Cho",
      "Dufan Wu",
      "Raul Uppot",
      "Quanzheng Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  }
]