[
  {
    "id": "arXiv:2503.00037",
    "title": "Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment in LVLMs",
    "abstract": "           Large Vision-Language Models (LVLMs) have made significant strides in multimodal comprehension, thanks to extensive pre-training and fine-tuning on large-scale visual datasets. However, despite their robust textual safety mechanisms, they remain vulnerable to harmful visual inputs. Existing safeguards-typically relying on pre-filtering or fine-tuning-incur high costs and diminish overall utility. To address this critical vulnerability, we introduce SafeCLIP, a lightweight method that leverages LVLMs inherent multimodal alignment for zero-shot toxic image detection. By projecting CLIPs discarded CLS token into its text space and matching it with toxic descriptors, SafeCLIP detects harmful content without any architectural changes-adding minimal latency and enabling dynamic safety corrections during inference and this http URL show that SafeCLIP achieves a 66.9% defense success rate with only 3.2% false positive rate and 7.2% overhead. In contrast, state-of-the-art methods achieve 52.9% success but have a 10.7% false positive rate and 210% overhead. Our work demonstrates that leveraging inherent multimodal alignment can yield efficient, low-cost LVLM safety. Code is available at this http URL.         ",
    "url": "https://arxiv.org/abs/2503.00037",
    "authors": [
      "Wei Zhao",
      "Zhe Li",
      "Yige Li",
      "Jun Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00038",
    "title": "from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors",
    "abstract": "           Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our study, we introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR) to induce the LLM to calibrate malicious metaphors for jailbreaking. Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed. Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content. Experimental results demonstrate that AVATAR can effectively and transferable jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs.         ",
    "url": "https://arxiv.org/abs/2503.00038",
    "authors": [
      "Yu Yan",
      "Sheng Sun",
      "Zenghao Duan",
      "Teli Liu",
      "Min Liu",
      "Zhiyi Yin",
      "Qi Li",
      "Jiangyu Lei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.00040",
    "title": "Memory-Free and Parallel Computation for Quantized Spiking Neural Networks",
    "abstract": "           Quantized Spiking Neural Networks (QSNNs) offer superior energy efficiency and are well-suited for deployment on resource-limited edge devices. However, limited bit-width weight and membrane potential result in a notable performance decline. In this study, we first identify a new underlying cause for this decline: the loss of historical information due to the quantized membrane potential. To tackle this issue, we introduce a memory-free quantization method that captures all historical information without directly storing membrane potentials, resulting in better performance with less memory requirements. To further improve the computational efficiency, we propose a parallel training and asynchronous inference framework that greatly increases training speed and energy efficiency. We combine the proposed memory-free quantization and parallel computation methods to develop a high-performance and efficient QSNN, named MFP-QSNN. Extensive experiments show that our MFP-QSNN achieves state-of-the-art performance on various static and neuromorphic image datasets, requiring less memory and faster training speeds. The efficiency and efficacy of the MFP-QSNN highlight its potential for energy-efficient neuromorphic computing.         ",
    "url": "https://arxiv.org/abs/2503.00040",
    "authors": [
      "Dehao Zhang",
      "Shuai Wang",
      "Yichen Xiao",
      "Wenjie Wei",
      "Yimeng Shan",
      "Malu Zhang",
      "Yang Yang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00044",
    "title": "Advanced YOLO-based Real-time Power Line Detection for Vegetation Management",
    "abstract": "           Power line infrastructure is a key component of the power system, and it is rapidly expanding to meet growing energy demands. Vegetation encroachment is a significant threat to the safe operation of power lines, requiring reliable and timely management to enhance the resilience and reliability of the power network. Integrating smart grid technology, especially Unmanned Aerial Vehicles (UAVs), provides substantial potential to revolutionize the management of extensive power line networks with advanced imaging techniques. However, processing the vast quantity of images captured by UAV patrols remains a significant challenge. This paper introduces an intelligent real-time monitoring framework for detecting power lines and adjacent vegetation. It is developed based on the deep-learning Convolutional Neural Network (CNN), You Only Look Once (YOLO), renowned for its high-speed object detection capabilities. Unlike existing deep learning-based methods, this framework enhances accuracy by integrating YOLOv8 with directional filters. They can extract directional features and textures of power lines and their vicinity, generating Oriented Bounding Boxes (OBB) for more precise localization. Additionally, a post-processing algorithm is developed to create a vegetation encroachment metric for power lines, allowing for a quantitative assessment of the surrounding vegetation distribution. The effectiveness of the proposed framework is demonstrated using a widely used power line dataset.         ",
    "url": "https://arxiv.org/abs/2503.00044",
    "authors": [
      "Shuaiang Rong",
      "Lina He",
      "Salih Furkan Atici",
      "Ahmet Enis Cetin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2503.00057",
    "title": "Improved YOLOv12 with LLM-Generated Synthetic Data for Enhanced Apple Detection and Benchmarking Against YOLOv11 and YOLOv10",
    "abstract": "           This study evaluated the performance of the YOLOv12 object detection model, and compared against YOLOv11 and YOLOv10 for apple detection in commercial orchards using synthetic images generated by Large Language Models (LLMs). The YOLOv12n configuration excelled, achieving the highest precision at 0.916, the highest recall at 0.969, and the highest mean Average Precision (mAP@50) at 0.978. In comparison, the YOLOv11 series was led by YOLO11x, which recorded the highest precision at 0.857, recall at 0.85, and mAP@50 at 0.91. For the YOLOv10 series, YOLOv10b and YOLOv10l tied for the highest precision at 0.85, with YOLOv10n achieving the highest recall at 0.8 and mAP@50 at 0.89. The study also highlighted efficiency in processing speeds, where YOLOv11n reported the lowest inference time at 4.7 ms, compared to YOLOv12n's 5.6 ms and YOLOv10n's 5.9 ms. Although YOLOv12 is new in more accurate than YOLOv11, and YOLOv10, the YOLO11n still stays the fastest YOLO algorithm among YOLOv10, YOLOv11 and YOLOv12. These findings demonstrated that YOLOv12, when trained on high-quality LLM-generated datasets, not only surpassed its predecessors in key performance metrics but also offered a cost-effective solution by reducing the need for extensive manual data collection in the field.         ",
    "url": "https://arxiv.org/abs/2503.00057",
    "authors": [
      "Ranjan Sapkota",
      "Manoj Karkee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.00061",
    "title": "Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents",
    "abstract": "           Large Language Model (LLM) agents exhibit remarkable performance across diverse applications by using external tools to interact with environments. However, integrating external tools introduces security risks, such as indirect prompt injection (IPI) attacks. Despite defenses designed for IPI attacks, their robustness remains questionable due to insufficient testing against adaptive attacks. In this paper, we evaluate eight different defenses and bypass all of them using adaptive attacks, consistently achieving an attack success rate of over 50%. This reveals critical vulnerabilities in current defenses. Our research underscores the need for adaptive attack evaluation when designing defenses to ensure robustness and reliability. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00061",
    "authors": [
      "Qiusi Zhan",
      "Richard Fang",
      "Henil Shalin Panchal",
      "Daniel Kang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00062",
    "title": "CRFU: Compressive Representation Forgetting Against Privacy Leakage on Machine Unlearning",
    "abstract": "           Machine unlearning allows data owners to erase the impact of their specified data from trained models. Unfortunately, recent studies have shown that adversaries can recover the erased data, posing serious threats to user privacy. An effective unlearning method removes the information of the specified data from the trained model, resulting in different outputs for the same input before and after unlearning. Adversaries can exploit these output differences to conduct privacy leakage attacks, such as reconstruction and membership inference attacks. However, directly applying traditional defenses to unlearning leads to significant model utility degradation. In this paper, we introduce a Compressive Representation Forgetting Unlearning scheme (CRFU), designed to safeguard against privacy leakage on unlearning. CRFU achieves data erasure by minimizing the mutual information between the trained compressive representation (learned through information bottleneck theory) and the erased data, thereby maximizing the distortion of data. This ensures that the model's output contains less information that adversaries can exploit. Furthermore, we introduce a remembering constraint and an unlearning rate to balance the forgetting of erased data with the preservation of previously learned knowledge, thereby reducing accuracy degradation. Theoretical analysis demonstrates that CRFU can effectively defend against privacy leakage attacks. Our experimental results show that CRFU significantly increases the reconstruction mean square error (MSE), achieving a defense effect improvement of approximately $200\\%$ against privacy reconstruction attacks with only $1.5\\%$ accuracy degradation on MNIST.         ",
    "url": "https://arxiv.org/abs/2503.00062",
    "authors": [
      "Weiqi Wang",
      "Chenhan Zhang",
      "Zhiyi Tian",
      "Shushu Liu",
      "Shui Yu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.00063",
    "title": "NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary",
    "abstract": "           Adversarial attacks exploit the vulnerability of deep models against adversarial samples. Existing point cloud attackers are tailored to specific models, iteratively optimizing perturbations based on gradients in either a white-box or black-box setting. Despite their promising attack performance, they often struggle to produce transferable adversarial samples due to overfitting the specific parameters of surrogate models. To overcome this issue, we shift our focus to the data distribution itself and introduce a novel approach named \\textbf{NoPain}, which employs optimal transport (OT) to identify the inherent singular boundaries of the data manifold for cross-network point cloud attacks. Specifically, we first calculate the OT mapping from noise to the target feature space, then identify singular boundaries by locating non-differentiable positions. Finally, we sample along singular boundaries to generate adversarial point clouds. Once the singular boundaries are determined, NoPain can efficiently produce adversarial samples without the need of iterative updates or guidance from the surrogate classifiers. Extensive experiments demonstrate that the proposed end-to-end method outperforms baseline approaches in terms of both transferability and efficiency, while also maintaining notable advantages even against defense strategies. The source code will be publicly available.         ",
    "url": "https://arxiv.org/abs/2503.00063",
    "authors": [
      "Zezeng Li",
      "Xiaoyu Du",
      "Na Lei",
      "Liming Chen",
      "Weimin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00065",
    "title": "ADAGE: Active Defenses Against GNN Extraction",
    "abstract": "           Graph Neural Networks (GNNs) achieve high performance in various real-world applications, such as drug discovery, traffic states prediction, and recommendation systems. The fact that building powerful GNNs requires a large amount of training data, powerful computing resources, and human expertise turns the models into lucrative targets for model stealing attacks. Prior work has revealed that the threat vector of stealing attacks against GNNs is large and diverse, as an attacker can leverage various heterogeneous signals ranging from node labels to high-dimensional node embeddings to create a local copy of the target GNN at a fraction of the original training costs. This diversity in the threat vector renders the design of effective and general defenses challenging and existing defenses usually focus on one particular stealing setup. Additionally, they solely provide means to identify stolen model copies rather than preventing the attack. To close this gap, we propose the first and general Active Defense Against GNN Extraction (ADAGE). By analyzing the queries to the GNN, tracking their diversity in terms of proximity to different communities identified in the underlying graph, and increasing the defense strength with the growing fraction of communities that have been queried, ADAGE can prevent stealing in all common attack setups. Our extensive experimental evaluation using six benchmark datasets, four GNN models, and three types of adaptive attackers shows that ADAGE penalizes attackers to the degree of rendering stealing impossible, whilst not harming predictive performance for legitimate users. ADAGE, thereby, contributes towards securely sharing valuable GNNs in the future.         ",
    "url": "https://arxiv.org/abs/2503.00065",
    "authors": [
      "Jing Xu",
      "Franziska Boenisch",
      "Adam Dziedzic"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00068",
    "title": "PI-HMR: Towards Robust In-bed Temporal Human Shape Reconstruction with Contact Pressure Sensing",
    "abstract": "           Long-term in-bed monitoring benefits automatic and real-time health management within healthcare, and the advancement of human shape reconstruction technologies further enhances the representation and visualization of users' activity patterns. However, existing technologies are primarily based on visual cues, facing serious challenges in non-light-of-sight and privacy-sensitive in-bed scenes. Pressure-sensing bedsheets offer a promising solution for real-time motion reconstruction. Yet, limited exploration in model designs and data have hindered its further development. To tackle these issues, we propose a general framework that bridges gaps in data annotation and model design. Firstly, we introduce SMPLify-IB, an optimization method that overcomes the depth ambiguity issue in top-view scenarios through gravity constraints, enabling generating high-quality 3D human shape annotations for in-bed datasets. Then we present PI-HMR, a temporal-based human shape estimator to regress meshes from pressure sequences. By integrating multi-scale feature fusion with high-pressure distribution and spatial position priors, PI-HMR outperforms SOTA methods with 17.01mm Mean-Per-Joint-Error decrease. This work provides a whole         ",
    "url": "https://arxiv.org/abs/2503.00068",
    "authors": [
      "Ziyu Wu",
      "Yufan Xiong",
      "Mengting Niu",
      "Fangting Xie",
      "Quan Wan",
      "Qijun Ying",
      "Boyan Liu",
      "Xiaohui Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2503.00074",
    "title": "CAMETA: Conflict-Aware Multi-Agent Estimated Time of Arrival Prediction for Mobile Robots",
    "abstract": "           This study presents the conflict-aware multi-agent estimated time of arrival (CAMETA) framework, a novel approach for predicting the arrival times of multiple agents in unstructured environments without predefined road infrastructure. The CAMETA framework consists of three components: a path planning layer generating potential path suggestions, a multi-agent ETA prediction layer predicting the arrival times for all agents based on the paths, and lastly, a path selection layer that calculates the accumulated cost and selects the best path. The novelty of the CAMETA framework lies in the heterogeneous map representation and the heterogeneous graph neural network architecture. As a result of the proposed novel structure, CAMETA improves the generalization capability compared to the state-of-the-art methods that rely on structured road infrastructure and historical data. The simulation results demonstrate the efficiency and efficacy of the multi-agent ETA prediction layer, with a mean average percentage error improvement of 29.5% and 44% when compared to a traditional path planning method (A *) which does not consider conflicts. The performance of the CAMETA framework shows significant improvements in terms of robustness to noise and conflicts as well as determining proficient routes compared to state-of-the-art multi-agent path planners.         ",
    "url": "https://arxiv.org/abs/2503.00074",
    "authors": [
      "Jonas le Fevre Sejersen",
      "Erdal Kayacan"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.00078",
    "title": "Equilibrium and Selfish Behavior in Network Contagion",
    "abstract": "           In this paper we consider non-atomic games in populations that are provided with a choice of preventive policies to act against a contagion spreading amongst interacting populations, be it biological organisms or connected computing devices. The spreading model of the contagion is the standard SIR model. Each participant of the population has a choice from amongst a set of precautionary policies with each policy presenting a payoff or utility, which we assume is the same within each group, the risk being the possibility of infection. The policy groups interact with each other. We also define a network model to model interactions between different population sets. The population sets reside at nodes of the network and follow policies available at that node. We define game-theoretic models and study the inefficiency of allowing for individual decision making, as opposed to centralized control. We study the computational aspects as well.         ",
    "url": "https://arxiv.org/abs/2503.00078",
    "authors": [
      "Yi Zhang",
      "Sanjiv Kapoor"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2503.00093",
    "title": "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
    "abstract": "           The proliferation of LLM bias probes introduces three significant challenges: (1) we lack principled criteria for choosing appropriate probes, (2) we lack a system for reconciling conflicting results across probes, and (3) we lack formal frameworks for reasoning about when (and why) probe results will generalize to real user behavior. We address these challenges by systematizing LLM social bias probing using actionable insights from social sciences. We then introduce EcoLevels - a framework that helps (a) determine appropriate bias probes, (b) reconcile conflicting findings across probes, and (c) generate predictions about bias generalization. Overall, we ground our analysis in social science research because many LLM probes are direct applications of human probes, and these fields have faced similar challenges when studying social bias in humans. Based on our work, we suggest how the next generation of LLM bias probing can (and should) benefit from decades of social science research.         ",
    "url": "https://arxiv.org/abs/2503.00093",
    "authors": [
      "Kirsten N. Morehouse",
      "Siddharth Swaroop",
      "Weiwei Pan"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00128",
    "title": "AnnoCaseLaw: A Richly-Annotated Dataset For Benchmarking Explainable Legal Judgment Prediction",
    "abstract": "           Legal systems worldwide continue to struggle with overwhelming caseloads, limited judicial resources, and growing complexities in legal proceedings. Artificial intelligence (AI) offers a promising solution, with Legal Judgment Prediction (LJP) -- the practice of predicting a court's decision from the case facts -- emerging as a key research area. However, existing datasets often formulate the task of LJP unrealistically, not reflecting its true difficulty. They also lack high-quality annotation essential for legal reasoning and explainability. To address these shortcomings, we introduce AnnoCaseLaw, a first-of-its-kind dataset of 471 meticulously annotated U.S. Appeals Court negligence cases. Each case is enriched with comprehensive, expert-labeled annotations that highlight key components of judicial decision making, along with relevant legal concepts. Our dataset lays the groundwork for more human-aligned, explainable LJP models. We define three legally relevant tasks: (1) judgment prediction; (2) concept identification; and (3) automated case annotation, and establish a performance baseline using industry-leading large language models (LLMs). Our results demonstrate that LJP remains a formidable task, with application of legal precedent proving particularly difficult. Code and data are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00128",
    "authors": [
      "Magnus Sesodia",
      "Alina Petrova",
      "John Armour",
      "Thomas Lukasiewicz",
      "Oana-Maria Camburu",
      "Puneet K. Dokania",
      "Philip Torr",
      "Christian Schroeder de Witt"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00132",
    "title": "CNSv2: Probabilistic Correspondence Encoded Neural Image Servo",
    "abstract": "           Visual servo based on traditional image matching methods often requires accurate keypoint correspondence for high precision control. However, keypoint detection or matching tends to fail in challenging scenarios with inconsistent illuminations or textureless objects, resulting significant performance degradation. Previous approaches, including our proposed Correspondence encoded Neural image Servo policy (CNS), attempted to alleviate these issues by integrating neural control strategies. While CNS shows certain improvement against error correspondence over conventional image-based controllers, it could not fully resolve the limitations arising from poor keypoint detection and matching. In this paper, we continue to address this problem and propose a new solution: Probabilistic Correspondence Encoded Neural Image Servo (CNSv2). CNSv2 leverages probabilistic feature matching to improve robustness in challenging scenarios. By redesigning the architecture to condition on multimodal feature matching, CNSv2 achieves high precision, improved robustness across diverse scenes and runs in real-time. We validate CNSv2 with simulations and real-world experiments, demonstrating its effectiveness in overcoming the limitations of detector-based methods in visual servo tasks.         ",
    "url": "https://arxiv.org/abs/2503.00132",
    "authors": [
      "Anzhe Chen",
      "Hongxiang Yu",
      "Shuxin Li",
      "Yuxi Chen",
      "Zhongxiang Zhou",
      "Wentao Sun",
      "Rong Xiong",
      "Yue Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.00134",
    "title": "Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary Recommendations",
    "abstract": "           Large Language Models (LLMs) effectively leverage common-sense knowledge for general reasoning, yet they struggle with personalized reasoning when tasked with interpreting multifactor personal data. This limitation restricts their applicability in domains that require context-aware decision-making tailored to individuals. This paper introduces Personalized Causal Graph Reasoning as an agentic framework that enhances LLM reasoning by incorporating personal causal graphs derived from data of individuals. These graphs provide a foundation that guides the LLM's reasoning process. We evaluate it on a case study on nutrient-oriented dietary recommendations, which requires personal reasoning due to the implicit unique dietary effects. We propose a counterfactual evaluation to estimate the efficiency of LLM-recommended foods for glucose management. Results demonstrate that the proposed method efficiently provides personalized dietary recommendations to reduce average glucose iAUC across three time windows, which outperforms the previous approach. LLM-as-a-judge evaluation results indicate that our proposed method enhances personalization in the reasoning process.         ",
    "url": "https://arxiv.org/abs/2503.00134",
    "authors": [
      "Zhongqi Yang",
      "Amir Rahmani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.00137",
    "title": "SCORE: Systematic COnsistency and Robustness Evaluation for Large Language Models",
    "abstract": "           Typical evaluations of Large Language Models (LLMs) report a single metric per dataset, often representing the model's best-case performance under carefully selected settings. Unfortunately, this approach overlooks model robustness and reliability in real-world applications. For instance, simple paraphrasing of prompts on the MMLU-Pro dataset causes accuracy fluctuations of up to 10\\%, while reordering answer choices in the AGIEval dataset results in accuracy differences of up to 6.1\\%. While some studies discuss issues with LLM robustness, there is no unified or centralized framework for evaluating the robustness of language models. To address this gap and consolidate existing research on model robustness, we present SCORE ($\\mathbf{S}$ystematic $\\mathbf{CO}$nsistency and $\\mathbf{R}$obustness $\\mathbf{E}$valuation), a comprehensive framework for non-adversarial evaluation of LLMs. The SCORE framework evaluates models by repeatedly testing them on the same benchmarks in various setups to give a realistic estimate of their accuracy and consistency. We release the code publicly and start an LLM robustness leaderboard to facilitate further development and research.         ",
    "url": "https://arxiv.org/abs/2503.00137",
    "authors": [
      "Grigor Nalbandyan",
      "Rima Shahbazyan",
      "Evelina Bakhturina"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.00154",
    "title": "Fed-KAN: Federated Learning with Kolmogorov-Arnold Networks for Traffic Prediction",
    "abstract": "           Non-Terrestrial Networks (NTNs) are becoming a critical component of modern communication infrastructures, especially with the advent of Low Earth Orbit (LEO) satellite systems. Traditional centralized learning approaches face major challenges in such networks due to high latency, intermittent connectivity and limited bandwidth. Federated Learning (FL) is a promising alternative as it enables decentralized training while maintaining data privacy. However, existing FL models, such as Federated Learning with Multi-Layer Perceptrons (Fed-MLP), can struggle with high computational complexity and poor adaptability to dynamic NTN environments. This paper provides a detailed analysis for Federated Learning with Kolmogorov-Arnold Networks (Fed-KAN), its implementation and performance improvements over traditional FL models in NTN environments for traffic forecasting. The proposed Fed-KAN is a novel approach that utilises the functional approximation capabilities of KANs in a FL framework. We evaluate Fed-KAN compared to Fed-MLP on a traffic dataset of real satellite operator and show a significant reduction in training and test loss. Our results show that Fed-KAN can achieve a 77.39% reduction in average test loss compared to Fed-MLP, highlighting its improved performance and better generalization ability. At the end of the paper, we also discuss some potential applications of Fed-KAN within O-RAN and Fed-KAN usage for split functionalities in NTN architecture.         ",
    "url": "https://arxiv.org/abs/2503.00154",
    "authors": [
      "Engin Zeydan",
      "Cristian J. Vaca-Rubio",
      "Luis Blanco",
      "Roberto Pereira",
      "Marius Caus",
      "Kapal Dev"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00170",
    "title": "Elastic Restaking Networks",
    "abstract": "           Decentralized services for blockchains often require their validators (operators) to deposit stake (collateral), which is forfeited (slashed) if they misbehave. Restaking networks let validators secure multiple services by reusing stake, giving rise to a strategic game: Validators can coordinate to misbehave across multiple services, extracting digital assets while forfeiting their stake only once. Previous work focused either on preventing coordinated misbehavior or on protecting services if all other services are Byzantine and might unjustly cause slashing due to bugs or malice. The first model overlooks how a single Byzantine service can collapse the network, while the second ignores shared-stake benefits. To bridge the gap, we model the strategic game of coordinated misbehavior when a given fraction of services are Byzantine. We introduce elastic restaking networks, where validators can allocate portions of their stake that may cumulatively exceed their total stake, and when allocations are lost, the remaining stake stretches to cover remaining allocations. We show that elastic networks exhibit superior robustness compared to previous approaches, and demonstrate a synergistic effect where an elastic restaking network enhances its blockchain's security, contrary to community concerns of an opposite effect in existing networks. We then design incentives for tuning validators' allocations. Our elastic restaking system and incentive design have immediate practical implications for deployed restaking networks, which have billions of dollars in stake.         ",
    "url": "https://arxiv.org/abs/2503.00170",
    "authors": [
      "Roi Bar-Zur",
      "Ittay Eyal"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2503.00179",
    "title": "Zero-Shot and Efficient Clarification Need Prediction in Conversational Search",
    "abstract": "           Clarification need prediction (CNP) is a key task in conversational search, aiming to predict whether to ask a clarifying question or give an answer to the current user query. However, current research on CNP suffers from the issues of limited CNP training data and low efficiency. In this paper, we propose a zero-shot and efficient CNP framework (Zef-CNP), in which we first prompt large language models (LLMs) in a zero-shot manner to generate two sets of synthetic queries: ambiguous and specific (unambiguous) queries. We then use the generated queries to train efficient CNP models. Zef-CNP eliminates the need for human-annotated clarification-need labels during training and avoids the use of LLMs with high query latency at query time. To further improve the generation quality of synthetic queries, we devise a topic-, information-need-, and query-aware chain-of-thought (CoT) prompting strategy (TIQ-CoT). Moreover, we enhance TIQ-CoT with counterfactual query generation (CoQu), which guides LLMs first to generate a specific/ambiguous query and then sequentially generate its corresponding ambiguous/specific query. Experimental results show that Zef-CNP achieves superior CNP effectiveness and efficiency compared with zero- and few-shot LLM-based CNP predictors.         ",
    "url": "https://arxiv.org/abs/2503.00179",
    "authors": [
      "Lili Lu",
      "Chuan Meng",
      "Federico Ravenda",
      "Mohammad Aliannejadi",
      "Fabio Crestani"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00184",
    "title": "Robust Evidence for Declining Disruptiveness: Assessing the Role of Zero-Backward-Citation Works",
    "abstract": "           We respond to Holst et al.'s (HATWG) critique that the observed decline in scientific disruptiveness demonstrated in Park et al. (PLF) stems from including works with zero backward citations (0-bcites). Applying their own advocated dataset, metric, and exclusion criteria, we demonstrate statistically and practically significant declines in disruptiveness that equal major benchmark transformations in science. Notably, we show that HATWG's own regression model -- designed specifically to address their concerns about 0-bcite works -- reveals highly significant declines for both papers (p<0.001) and patents (p<0.001), a finding they neither acknowledge nor interpret. Their critique is undermined by methodological deficiencies, including reliance on visual inspection without statistical assessment, and severe data quality issues in their SciSciNet dataset, which contains nearly three times more 0-bcite papers than our original data. HATWG's departure from established scientometric practices -- notably their inclusion of document types and fields known for poor metadata quality -- invalidates their conclusions. Monte Carlo simulations and additional analyses using multiple disruptiveness measures across datasets further validate the robustness of the declining trend. Our findings collectively demonstrate that the observed decline in disruptiveness is not an artifact of 0-bcite works but represents a substantive change in scientific and technological innovation patterns.         ",
    "url": "https://arxiv.org/abs/2503.00184",
    "authors": [
      "Michael Park",
      "Erin Leahey",
      "Russell J. Funk"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Digital Libraries (cs.DL)"
    ]
  },
  {
    "id": "arXiv:2503.00187",
    "title": "Steering Dialogue Dynamics for Robustness against Multi-turn Jailbreaking Attacks",
    "abstract": "           Large language models (LLMs) are highly vulnerable to jailbreaking attacks, wherein adversarial prompts are designed to elicit harmful responses. While existing defenses effectively mitigate single-turn attacks by detecting and filtering unsafe inputs, they fail against multi-turn jailbreaks that exploit contextual drift over multiple interactions, gradually leading LLMs away from safe behavior. To address this challenge, we propose a safety steering framework grounded in safe control theory, ensuring invariant safety in multi-turn dialogues. Our approach models the dialogue with LLMs using state-space representations and introduces a novel neural barrier function (NBF) to detect and filter harmful queries emerging from evolving contexts proactively. Our method achieves invariant safety at each turn of dialogue by learning a safety predictor that accounts for adversarial queries, preventing potential context drift toward jailbreaks. Extensive experiments under multiple LLMs show that our NBF-based safety steering outperforms safety alignment baselines, offering stronger defenses against multi-turn jailbreaks while maintaining a better trade-off between safety and helpfulness under different multi-turn jailbreak methods. Our code is available at this https URL .         ",
    "url": "https://arxiv.org/abs/2503.00187",
    "authors": [
      "Hanjiang Hu",
      "Alexander Robey",
      "Changliu Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.00191",
    "title": "Learning Vision-Based Neural Network Controllers with Semi-Probabilistic Safety Guarantees",
    "abstract": "           Ensuring safety in autonomous systems with vision-based control remains a critical challenge due to the high dimensionality of image inputs and the fact that the relationship between true system state and its visual manifestation is unknown. Existing methods for learning-based control in such settings typically lack formal safety guarantees. To address this challenge, we introduce a novel semi-probabilistic verification framework that integrates reachability analysis with conditional generative adversarial networks and distribution-free tail bounds to enable efficient and scalable verification of vision-based neural network controllers. Next, we develop a gradient-based training approach that employs a novel safety loss function, safety-aware data-sampling strategy to efficiently select and store critical training examples, and curriculum learning, to efficiently synthesize safe controllers in the semi-probabilistic framework. Empirical evaluations in X-Plane 11 airplane landing simulation, CARLA-simulated autonomous lane following, and F1Tenth lane following in a physical visually-rich miniature environment demonstrate the effectiveness of our method in achieving formal safety guarantees while maintaining strong nominal performance. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00191",
    "authors": [
      "Xinhang Ma",
      "Junlin Wu",
      "Hussein Sibai",
      "Yiannis Kantaros",
      "Yevgeniy Vorobeychik"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00206",
    "title": "Quantifying First-Order Markov Violations in Noisy Reinforcement Learning: A Causal Discovery Approach",
    "abstract": "           Reinforcement learning (RL) methods frequently assume that each new observation completely reflects the environment's state, thereby guaranteeing Markovian (one-step) transitions. In practice, partial observability or sensor/actuator noise often invalidates this assumption. This paper proposes a systematic methodology for detecting such violations, combining a partial correlation-based causal discovery process (PCMCI) with a novel Markov Violation score (MVS). The MVS measures multi-step dependencies that emerge when noise or incomplete state information disrupts the Markov property. Classic control tasks (CartPole, Pendulum, Acrobot) serve as examples to illustrate how targeted noise and dimension omissions affect both RL performance and measured Markov consistency. Surprisingly, even substantial observation noise sometimes fails to induce strong multi-lag dependencies in certain domains (e.g., Acrobot). In contrast, dimension-dropping investigations show that excluding some state variables (e.g., angular velocities in CartPole and Pendulum) significantly reduces returns and increases MVS, while removing other dimensions has minimal impact. These findings emphasize the importance of locating and safeguarding the most causally essential dimensions in order to preserve effective single-step learning. By integrating partial correlation tests with RL performance outcomes, the proposed approach precisely identifies when and where the Markov assumption is violated. This framework offers a principled mechanism for developing robust policies, informing representation learning, and addressing partial observability in real-world RL scenarios. All code and experimental logs are accessible for reproducibility (this https URL).         ",
    "url": "https://arxiv.org/abs/2503.00206",
    "authors": [
      "Naveen Mysore"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.00210",
    "title": "Foundation-Model-Boosted Multimodal Learning for fMRI-based Neuropathic Pain Drug Response Prediction",
    "abstract": "           Neuropathic pain, affecting up to 10% of adults, remains difficult to treat due to limited therapeutic efficacy and tolerability. Although resting-state functional MRI (rs-fMRI) is a promising non-invasive measurement of brain biomarkers to predict drug response in therapeutic development, the complexity of fMRI demands machine learning models with substantial capacity. However, extreme data scarcity in neuropathic pain research limits the application of high-capacity models. To address the challenge of data scarcity, we propose FMM$_{TC}$, a Foundation-Model-boosted Multimodal learning framework for fMRI-based neuropathic pain drug response prediction, which leverages both internal multimodal information in pain-specific data and external knowledge from large pain-agnostic data. Specifically, to maximize the value of limited pain-specific data, FMM$_{TC}$ integrates complementary information from two rs-fMRI modalities: Time series and functional Connectivity. FMM$_{TC}$ is further boosted by an fMRI foundation model with its external knowledge from extensive pain-agnostic fMRI datasets enriching limited pain-specific information. Evaluations with an in-house dataset and a public dataset from OpenNeuro demonstrate FMM$_{TC}$'s superior representation ability, generalizability, and cross-dataset adaptability over existing unimodal fMRI models that only consider one of the rs-fMRI modalities. The ablation study validates the effectiveness of multimodal learning and foundation-model-powered external knowledge transfer in FMM$_{TC}$. An integrated gradient-based interpretation study explains how FMM$_{TC}$'s cross-dataset dynamic behaviors enhance its adaptability. In conclusion, FMM$_{TC}$ boosts clinical trials in neuropathic pain therapeutic development by accurately predicting drug responses to improve the participant stratification efficiency.         ",
    "url": "https://arxiv.org/abs/2503.00210",
    "authors": [
      "Wenrui Fan",
      "L. M. Riza Rizky",
      "Jiayang Zhang",
      "Chen Chen",
      "Haiping Lu",
      "Kevin Teh",
      "Dinesh Selvarajah",
      "Shuo Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.00234",
    "title": "Towards Fairness for the Right Reasons: Using Saliency Maps to Evaluate Bias Removal in Neural Networks",
    "abstract": "           The widespread adoption of machine learning systems has raised critical concerns about fairness and bias, making mitigating harmful biases essential for AI development. In this paper, we investigate the relationship between fairness improvement and the removal of harmful biases in neural networks applied to computer vision tasks. First, we introduce a set of novel XAI-based metrics that analyze saliency maps to assess shifts in a model's decision-making process. Then, we demonstrate that successful debiasing methods systematically redirect model focus away from protected attributes. Additionally, we show that techniques originally developed for artifact removal can be effectively repurposed for fairness. These findings underscore the importance of ensuring that models are fair for the right reasons, contributing to the development of more ethical and trustworthy AI systems.         ",
    "url": "https://arxiv.org/abs/2503.00234",
    "authors": [
      "Lukasz Sztukiewicz",
      "Ignacy St\u0119pka",
      "Micha\u0142 Wili\u0144ski",
      "Jerzy Stefanowski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2503.00240",
    "title": "1-Lipschitz Network Initialization for Certifiably Robust Classification Applications: A Decay Problem",
    "abstract": "           This paper discusses the weight parametrization of two standard 1-Lipschitz network structure methodologies, the Almost-Orthogonal-Layers (AOL) and the SDP-based Lipschitz Layers (SLL), and derives their impact on the initialization for deep 1-Lipschitz feedforward networks in addition to discussing underlying issues surrounding this initialization. These networks are mainly used in certifiably robust classification applications to combat adversarial attacks by limiting the effects of perturbations on the output classification result. An exact and an upper bound for the parameterized weight variance was calculated assuming a standard Normal distribution initialization; additionally, an upper bound was computed assuming a Generalized Normal Distribution, generalizing the proof for Uniform, Laplace, and Normal distribution weight initializations. It is demonstrated that the weight variance holds no bearing on the output variance distribution and that only the dimension of the weight matrices matters. Additionally, this paper demonstrates that the weight initialization always causes deep 1-Lipschitz networks to decay to zero.         ",
    "url": "https://arxiv.org/abs/2503.00240",
    "authors": [
      "Marius F. R. Juston",
      "William R. Norris",
      "Dustin Nottage",
      "Ahmet Soylemezoglu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.00258",
    "title": "Decoupling Content and Expression: Two-Dimensional Detection of AI-Generated Text",
    "abstract": "           The wide usage of LLMs raises critical requirements on detecting AI participation in texts. Existing studies investigate these detections in scattered contexts, leaving a systematic and unified approach unexplored. In this paper, we present HART, a hierarchical framework of AI risk levels, each corresponding to a detection task. To address these tasks, we propose a novel 2D Detection Method, decoupling a text into content and language expression. Our findings show that content is resistant to surface-level changes, which can serve as a key feature for detection. Experiments demonstrate that 2D method significantly outperforms existing detectors, achieving an AUROC improvement from 0.705 to 0.849 for level-2 detection and from 0.807 to 0.886 for RAID. We release our data and code at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00258",
    "authors": [
      "Guangsheng Bao",
      "Lihua Rong",
      "Yanbin Zhao",
      "Qiji Zhou",
      "Yue Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2503.00263",
    "title": "On the time complexity of finding a well-spread perfect matching in bridgeless cubic graphs",
    "abstract": "           We present an algorithm for finding a perfect matching in a $3$-edge-connected cubic graph that intersects every $3$-edge cut in exactly one edge. Specifically, we propose an algorithm with a time complexity of $O(n \\log^4 n)$, which significantly improves upon the previously known $O(n^3)$-time algorithms for the same problem. The technique we use for the improvement is efficient use of cactus model of 3-edge cuts. As an application, we use our algorithm to compute embeddings of $3$-edge-connected cubic graphs with limited number of singular edges (i.e., edges that are twice in the boundary of one face) in $O(n \\log^4 n)$ time; this application contributes to the study of the well-known Cycle Double Cover conjecture.         ",
    "url": "https://arxiv.org/abs/2503.00263",
    "authors": [
      "Babak Ghanbari",
      "Robert \u0160\u00e1mal"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2503.00268",
    "title": "Input Specific Neural Networks",
    "abstract": "           The black-box nature of neural networks limits the ability to encode or impose specific structural relationships between inputs and outputs. While various studies have introduced architectures that ensure the network's output adheres to a particular form in relation to certain inputs, the majority of these approaches impose constraints on only a single set of inputs. This paper introduces a novel neural network architecture, termed the Input Specific Neural Network (ISNN), which extends this concept by allowing scalar-valued outputs to be subject to multiple constraints. Specifically, the ISNN can enforce convexity in some inputs, non-decreasing monotonicity combined with convexity with respect to others, and simple non-decreasing monotonicity or arbitrary relationships with additional inputs. The paper presents two distinct ISNN architectures, along with equations for the first and second derivatives of the output with respect to the inputs. These networks are broadly applicable. In this work, we restrict their usage to solving problems in computational mechanics. In particular, we show how they can be effectively applied to fitting data-driven constitutive models. We then embed our trained data-driven constitutive laws into a finite element solver where significant time savings can be achieved by using explicit manual differentiation using the derived equations as opposed to automatic differentiation. We also show how ISNNs can be used to learn structural relationships between inputs and outputs via a binary gating mechanism. Particularly, ISNNs are employed to model an anisotropic free energy potential to get the homogenized macroscopic response in a decoupled multiscale setting, where the network learns whether or not the potential should be modeled as polyconvex, and retains only the relevant layers while using the minimum number of inputs.         ",
    "url": "https://arxiv.org/abs/2503.00268",
    "authors": [
      "Asghar A. Jadoon",
      "D. Thomas Seidl",
      "Reese E. Jones",
      "Jan N. Fuhg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.00286",
    "title": "A Unified Framework for Heterogeneous Semi-supervised Learning",
    "abstract": "           In this work, we introduce a novel problem setup termed as Heterogeneous Semi-Supervised Learning (HSSL), which presents unique challenges by bridging the semi-supervised learning (SSL) task and the unsupervised domain adaptation (UDA) task, and expanding standard semi-supervised learning to cope with heterogeneous training data. At its core, HSSL aims to learn a prediction model using a combination of labeled and unlabeled training data drawn separately from heterogeneous domains that share a common set of semantic categories; this model is intended to differentiate the semantic categories of test instances sampled from both the labeled and unlabeled domains. In particular, the labeled and unlabeled domains have dissimilar label distributions and class feature distributions. This heterogeneity, coupled with the assorted sources of the test data, introduces significant challenges to standard SSL and UDA methods. Therefore, we propose a novel method, Unified Framework for Heterogeneous Semi-supervised Learning (Uni-HSSL), to address HSSL by directly learning a fine-grained classifier from the heterogeneous data, which adaptively handles the inter-domain heterogeneity while leveraging both the unlabeled data and the inter-domain semantic class relationships for cross-domain knowledge transfer and adaptation. We conduct comprehensive experiments and the experimental results validate the efficacy and superior performance of the proposed Uni-HSSL over state-of-the-art semi-supervised learning and unsupervised domain adaptation methods.         ",
    "url": "https://arxiv.org/abs/2503.00286",
    "authors": [
      "Marzi Heidari",
      "Abdullah Alchihabi",
      "Hao Yan",
      "Yuhong Guo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00295",
    "title": "Robust Multi-Objective Preference Alignment with Online DPO",
    "abstract": "           Multi-objective preference alignment of large language models (LLMs) is critical for developing AI systems that are more configurable, personalizable, helpful, and safe. However, optimizing model outputs to satisfy diverse objectives with variable weights at inference time for truly personalized models presents a significant challenge. Existing approaches are either computationally expensive to train or do not sufficiently steer model behaviors. This paper introduces the Multi-Objective Online DPO (MO-ODPO) algorithm, designed to robustly and efficiently align model behaviors with multiple, potentially conflicting human preferences. Our approach incorporates a prompt conditioning mechanism, allowing us to train a single preference-conditional policy, that can adapt to new preference combinations at inference. Experiments on two popular benchmarks show that MO-ODPO Pareto-dominates existing baselines while providing excellent inference-time steerability between diverse objectives.         ",
    "url": "https://arxiv.org/abs/2503.00295",
    "authors": [
      "Raghav Gupta",
      "Ryan Sullivan",
      "Yunxuan Li",
      "Samrat Phatale",
      "Abhinav Rastogi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00296",
    "title": "Synthetic data enables context-aware bioacoustic sound event detection",
    "abstract": "           We propose a methodology for training foundation models that enhances their in-context learning capabilities within the domain of bioacoustic signal processing. We use synthetically generated training data, introducing a domain-randomization-based pipeline that constructs diverse acoustic scenes with temporally strong labels. We generate over 8.8 thousand hours of strongly-labeled audio and train a query-by-example, transformer-based model to perform few-shot bioacoustic sound event detection. Our second contribution is a public benchmark of 13 diverse few-shot bioacoustics tasks. Our model outperforms previously published methods by 49%, and we demonstrate that this is due to both model design and data scale. We make our trained model available via an API, to provide ecologists and ethologists with a training-free tool for bioacoustic sound event detection.         ",
    "url": "https://arxiv.org/abs/2503.00296",
    "authors": [
      "Benjamin Hoffman",
      "David Robinson",
      "Marius Miron",
      "Vittorio Baglione",
      "Daniela Canestrari",
      "Damian Elias",
      "Eva Trapote",
      "Olivier Pietquin"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00298",
    "title": "Energy-Efficient Edge Inference in Integrated Sensing, Communication, and Computation Networks",
    "abstract": "           Task-oriented integrated sensing, communication, and computation (ISCC) is a key technology for achieving low-latency edge inference and enabling efficient implementation of artificial intelligence (AI) in industrial cyber-physical systems (ICPS). However, the constrained energy supply at edge devices has emerged as a critical bottleneck. In this paper, we propose a novel energy-efficient ISCC framework for AI inference at resource-constrained edge devices, where adjustable split inference, model pruning, and feature quantization are jointly designed to adapt to diverse task requirements. A joint resource allocation design problem for the proposed ISCC framework is formulated to minimize the energy consumption under stringent inference accuracy and latency constraints. To address the challenge of characterizing inference accuracy, we derive an explicit approximation for it by analyzing the impact of sensing, communication, and computation processes on the inference performance. Building upon the analytical results, we propose an iterative algorithm employing alternating optimization to solve the resource allocation problem. In each subproblem, the optimal solutions are available by respectively applying a golden section search method and checking the Karush-Kuhn-Tucker (KKT) conditions, thereby ensuring the convergence to a local optimum of the original problem. Numerical results demonstrate the effectiveness of the proposed ISCC design, showing a significant reduction in energy consumption of up to 40% compared to existing methods, particularly in low-latency scenarios.         ",
    "url": "https://arxiv.org/abs/2503.00298",
    "authors": [
      "Jiacheng Yao",
      "Wei Xu",
      "Guangxu Zhu",
      "Kaibin Huang",
      "Shuguang Cui"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.00306",
    "title": "Unlocking Efficient, Scalable, and Continual Knowledge Editing with Basis-Level Representation Fine-Tuning",
    "abstract": "           Large language models (LLMs) have achieved remarkable performance on various natural language tasks. However, they are trained on static corpora and their knowledge can become outdated quickly in the fast-changing world. This motivates the development of knowledge editing methods designed to update certain knowledge in LLMs without changing unrelated others. To make selective edits, previous efforts often sought to update a small amount of parameters in some specific layer(s) of a LLM. Nonetheless, in challenging scenarios, they still fall short in making successful edits while preserving knowledge irrelevant to the updates simultaneously, resulting in a notable editing-locality trade-off. In this work, we question if the trade-offs are caused by the fact that parameter-based updates have a global effect, i.e., edited parameters affect all inputs indiscriminately. In light of this, we explore the feasibility of representation fine-tuning, which applied some linear update to a few representations in a learned subspace, for knowledge editing. While being effective to enhance an LLM's general ability as demonstrated in the previous work, we theoretically show that this linear update imposes a tension in editing-locality trade-off. Subsequently, BaFT is proposed to break the linearity. BaFT computes a weight for each basis that spans a dimension of the subspace based on the input representation. This input-dependent weighting mechanism allows BaFT to manage different types of knowledge in an adaptive way, thereby achieving a better editing-locality trade-off. Experiments on three LLMs with five editing benchmarks in diverse scenarios show the superiority of our method.         ",
    "url": "https://arxiv.org/abs/2503.00306",
    "authors": [
      "Tianci Liu",
      "Ruirui Li",
      "Yunzhe Qi",
      "Hui Liu",
      "Xianfeng Tang",
      "Tianqi Zheng",
      "Qingyu Yin",
      "Monica Xiao Cheng",
      "Jun Huan",
      "Haoyu Wang",
      "Jing Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00317",
    "title": "DeepONet Augmented by Randomized Neural Networks for Efficient Operator Learning in PDEs",
    "abstract": "           Deep operator networks (DeepONets) represent a powerful class of data-driven methods for operator learning, demonstrating strong approximation capabilities for a wide range of linear and nonlinear operators. They have shown promising performance in learning operators that govern partial differential equations (PDEs), including diffusion-reaction systems and Burgers' equations. However, the accuracy of DeepONets is often constrained by computational limitations and optimization challenges inherent in training deep neural networks. Furthermore, the computational cost associated with training these networks is typically very high. To address these challenges, we leverage randomized neural networks (RaNNs), in which the parameters of the hidden layers remain fixed following random initialization. RaNNs compute the output layer parameters using the least-squares method, significantly reducing training time and mitigating optimization errors. In this work, we integrate DeepONets with RaNNs to propose RaNN-DeepONets, a hybrid architecture designed to balance accuracy and efficiency. Furthermore, to mitigate the need for extensive data preparation, we introduce the concept of physics-informed RaNN-DeepONets. Instead of relying on data generated through other time-consuming numerical methods, we incorporate PDE information directly into the training process. We evaluate the proposed model on three benchmark PDE problems: diffusion-reaction dynamics, Burgers' equation, and the Darcy flow problem. Through these tests, we assess its ability to learn nonlinear operators with varying input types. When compared to the standard DeepONet framework, RaNN-DeepONets achieves comparable accuracy while reducing computational costs by orders of magnitude. These results highlight the potential of RaNN-DeepONets as an efficient alternative for operator learning in PDE-based systems.         ",
    "url": "https://arxiv.org/abs/2503.00317",
    "authors": [
      "Zhaoxi Jiang",
      "Fei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2503.00325",
    "title": "CADRef: Robust Out-of-Distribution Detection via Class-Aware Decoupled Relative Feature Leveraging",
    "abstract": "           Deep neural networks (DNNs) have been widely criticized for their overconfidence when dealing with out-of-distribution (OOD) samples, highlighting the critical need for effective OOD detection to ensure the safe deployment of DNNs in real-world settings. Existing post-hoc OOD detection methods primarily enhance the discriminative power of logit-based approaches by reshaping sample features, yet they often neglect critical information inherent in the features themselves. In this paper, we propose the Class-Aware Relative Feature-based method (CARef), which utilizes the error between a sample's feature and its class-aware average feature as a discriminative criterion. To further refine this approach, we introduce the Class-Aware Decoupled Relative Feature-based method (CADRef), which decouples sample features based on the alignment of signs between the relative feature and corresponding model weights, enhancing the discriminative capabilities of CARef. Extensive experimental results across multiple datasets and models demonstrate that both proposed methods exhibit effectiveness and robustness in OOD detection compared to state-of-the-art methods. Specifically, our two methods outperform the best baseline by 2.82% and 3.27% in AUROC, with improvements of 4.03% and 6.32% in FPR95, respectively.         ",
    "url": "https://arxiv.org/abs/2503.00325",
    "authors": [
      "Zhiwei Ling",
      "Yachen Chang",
      "Hailiang Zhao",
      "Xinkui Zhao",
      "Kingsum Chow",
      "Shuiguang Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00331",
    "title": "PINN-DT: Optimizing Energy Consumption in Smart Building Using Hybrid Physics-Informed Neural Networks and Digital Twin Framework with Blockchain Security",
    "abstract": "           The advancement of smart grid technologies necessitates the integration of cutting-edge computational methods to enhance predictive energy optimization. This study proposes a multi-faceted approach by incorporating (1) Deep Reinforcement Learning (DRL) agents trained using data from Digital Twins (DTs) to optimize energy consumption in real time, (2) Physics-Informed Neural Networks (PINNs) to seamlessly embed physical laws within the optimization process, ensuring model accuracy and interpretability, and (3) Blockchain (BC) technology to facilitate secure and transparent communication across the smart grid infrastructure. The model was trained and validated using comprehensive datasets, including smart meter energy consumption data, renewable energy outputs, dynamic pricing, and user preferences collected from IoT devices. The proposed framework achieved superior predictive performance with a Mean Absolute Error (MAE) of 0.237 kWh, Root Mean Square Error (RMSE) of 0.298 kWh, and an R-squared (R2) value of 0.978, indicating a 97.8% explanation of data variance. Classification metrics further demonstrated the model's robustness, achieving 97.7% accuracy, 97.8% precision, 97.6% recall, and an F1 Score of 97.7%. Comparative analysis with traditional models like Linear Regression, Random Forest, SVM, LSTM, and XGBoost revealed the superior accuracy and real-time adaptability of the proposed method. In addition to enhancing energy efficiency, the model reduced energy costs by 35%, maintained a 96% user comfort index, and increased renewable energy utilization to 40%. This study demonstrates the transformative potential of integrating PINNs, DT, and Blockchain technologies to optimize energy consumption in smart grids, paving the way for sustainable, secure, and efficient energy management systems.         ",
    "url": "https://arxiv.org/abs/2503.00331",
    "authors": [
      "Hajar Kazemi Naeini",
      "Roya Shomali",
      "Abolhassan Pishahang",
      "Hamidreza Hasanzadeh",
      "Mahdieh Mohammadi",
      "Saeid Asadi",
      "Ahmad Gholizadeh Lonbar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00333",
    "title": "More of the Same: Persistent Representational Harms Under Increased Representation",
    "abstract": "           To recognize and mitigate the harms of generative AI systems, it is crucial to consider who is represented in the outputs of generative AI systems and how people are represented. A critical gap emerges when naively improving who is represented, as this does not imply bias mitigation efforts have been applied to address how people are represented. We critically examined this by investigating gender representation in occupation across state-of-the-art large language models. We first show evidence suggesting that over time there have been interventions to models altering the resulting gender distribution, and we find that women are more represented than men when models are prompted to generate biographies or personas. We then demonstrate that representational biases persist in how different genders are represented by examining statistically significant word differences across genders. This results in a proliferation of representational harms, stereotypes, and neoliberalism ideals that, despite existing interventions to increase female representation, reinforce existing systems of oppression.         ",
    "url": "https://arxiv.org/abs/2503.00333",
    "authors": [
      "Jennifer Mickel",
      "Maria De-Arteaga",
      "Leqi Liu",
      "Kevin Tian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00334",
    "title": "MCNet: Monotonic Calibration Networks for Expressive Uncertainty Calibration in Online Advertising",
    "abstract": "           In online advertising, uncertainty calibration aims to adjust a ranking model's probability predictions to better approximate the true likelihood of an event, e.g., a click or a conversion. However, existing calibration approaches may lack the ability to effectively model complex nonlinear relations, consider context features, and achieve balanced performance across different data subsets. To tackle these challenges, we introduce a novel model called Monotonic Calibration Networks, featuring three key designs: a monotonic calibration function (MCF), an order-preserving regularizer, and a field-balance regularizer. The nonlinear MCF is capable of naturally modeling and universally approximating the intricate relations between uncalibrated predictions and the posterior probabilities, thus being much more expressive than existing methods. MCF can also integrate context features using a flexible model architecture, thereby achieving context awareness. The order-preserving and field-balance regularizers promote the monotonic relationship between adjacent bins and the balanced calibration performance on data subsets, respectively. Experimental results on both public and industrial datasets demonstrate the superior performance of our method in generating well-calibrated probability predictions.         ",
    "url": "https://arxiv.org/abs/2503.00334",
    "authors": [
      "Quanyu Dai",
      "Jiaren Xiao",
      "Zhaocheng Du",
      "Jieming Zhu",
      "Chengxiao Luo",
      "Xiao-Ming Wu",
      "Zhenhua Dong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.00342",
    "title": "Hierarchical Multi-Stage BERT Fusion Framework with Dual Attention for Enhanced Cyberbullying Detection in Social Media",
    "abstract": "           Detecting and classifying cyberbullying in social media is hard because of the complex nature of online language and the changing nature of content. This study presents a multi-stage BERT fusion framework. It uses hierarchical embeddings, dual attention mechanisms, and extra features to improve detection of cyberbullying content. The framework combines BERT embeddings with features like sentiment and topic information. It uses self-attention and cross-attention to align features and has a hierarchical classification head for multi-category classification. A dynamic loss balancing strategy helps optimize learning and improves accuracy, precision, recall, and F1-score. These results show the model's strong performance and potential for broader use in analyzing social media content.         ",
    "url": "https://arxiv.org/abs/2503.00342",
    "authors": [
      "Jiani Wang",
      "Xiaochuan Xu",
      "Peiyang Yu",
      "Zeqiu Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.00344",
    "title": "Legged Robot State Estimation Using Invariant Neural-Augmented Kalman Filter with a Neural Compensator",
    "abstract": "           This paper presents an algorithm to improve state estimation for legged robots. Among existing model-based state estimation methods for legged robots, the contact-aided invariant extended Kalman filter defines the state on a Lie group to preserve invariance, thereby significantly accelerating convergence. It achieves more accurate state estimation by leveraging contact information as measurements for the update step. However, when the model exhibits strong nonlinearity, the estimation accuracy decreases. Such nonlinearities can cause initial errors to accumulate and lead to large drifts over time. To address this issue, we propose compensating for errors by augmenting the Kalman filter with an artificial neural network serving as a nonlinear function approximator. Furthermore, we design this neural network to respect the Lie group structure to ensure invariance, resulting in our proposed Invariant Neural-Augmented Kalman Filter (InNKF). The proposed algorithm offers improved state estimation performance by combining the strengths of model-based and learning-based approaches. Supplementary Video: this https URL ",
    "url": "https://arxiv.org/abs/2503.00344",
    "authors": [
      "Seokju Lee",
      "Hyun-Bin Kim",
      "Kyung-Soo Kim"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.00345",
    "title": "Towards Understanding the Benefit of Multitask Representation Learning in Decision Process",
    "abstract": "           Multitask Representation Learning (MRL) has emerged as a prevalent technique to improve sample efficiency in Reinforcement Learning (RL). Empirical studies have found that training agents on multiple tasks simultaneously within online and transfer learning environments can greatly improve efficiency. Despite its popularity, a comprehensive theoretical framework that elucidates its operational efficacy remains incomplete. Prior analyses have predominantly assumed that agents either possess a pre-known representation function or utilize functions from a linear class, where both are impractical. The complexity of real-world applications typically requires the use of sophisticated, non-linear functions such as neural networks as representation function, which are not pre-existing but must be learned. Our work tries to fill the gap by extending the analysis to \\textit{unknown non-linear} representations, giving a comprehensive analysis for its mechanism in online and transfer learning setting. We consider the setting that an agent simultaneously playing $M$ contextual bandits (or MDPs), developing a shared representation function $\\phi$ from a non-linear function class $\\Phi$ using our novel Generalized Functional Upper Confidence Bound algorithm (GFUCB). We formally prove that this approach yields a regret upper bound that outperforms the lower bound associated with learning $M$ separate tasks, marking the first demonstration of MRL's efficacy in a general function class. This framework also explains the contribution of representations to transfer learning when faced with new, yet related tasks, and identifies key conditions for successful transfer. Empirical experiments further corroborate our theoretical findings.         ",
    "url": "https://arxiv.org/abs/2503.00345",
    "authors": [
      "Rui Lu",
      "Yang Yue",
      "Andrew Zhao",
      "Simon Du",
      "Gao Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00348",
    "title": "SHAZAM: Self-Supervised Change Monitoring for Hazard Detection and Mapping",
    "abstract": "           The increasing frequency of environmental hazards due to climate change underscores the urgent need for effective monitoring systems. Current approaches either rely on expensive labelled datasets, struggle with seasonal variations, or require multiple observations for confirmation (which delays detection). To address these challenges, this work presents SHAZAM - Self-Supervised Change Monitoring for Hazard Detection and Mapping. SHAZAM uses a lightweight conditional UNet to generate expected images of a region of interest (ROI) for any day of the year, allowing for the direct modelling of normal seasonal changes and the ability to distinguish potential hazards. A modified structural similarity measure compares the generated images with actual satellite observations to compute region-level anomaly scores and pixel-level hazard maps. Additionally, a theoretically grounded seasonal threshold eliminates the need for dataset-specific optimisation. Evaluated on four diverse datasets that contain bushfires (wildfires), burned regions, extreme and out-of-season snowfall, floods, droughts, algal blooms, and deforestation, SHAZAM achieved F1 score improvements of between 0.066 and 0.234 over existing methods. This was achieved primarily through more effective hazard detection (higher recall) while using only 473K parameters. SHAZAM demonstrated superior mapping capabilities through higher spatial resolution and improved ability to suppress background features while accentuating both immediate and gradual hazards. SHAZAM has been established as an effective and generalisable solution for hazard detection and mapping across different geographical regions and a diverse range of hazards. The Python code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2503.00348",
    "authors": [
      "Samuel Garske",
      "Konrad Heidler",
      "Bradley Evans",
      "KC Wong",
      "Xiao Xiang Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2503.00355",
    "title": "Structured Reasoning for Fairness: A Multi-Agent Approach to Bias Detection in Textual Data",
    "abstract": "           From disinformation spread by AI chatbots to AI recommendations that inadvertently reinforce stereotypes, textual bias poses a significant challenge to the trustworthiness of large language models (LLMs). In this paper, we propose a multi-agent framework that systematically identifies biases by disentangling each statement as fact or opinion, assigning a bias intensity score, and providing concise, factual justifications. Evaluated on 1,500 samples from the WikiNPOV dataset, the framework achieves 84.9% accuracy$\\unicode{x2014}$an improvement of 13.0% over the zero-shot baseline$\\unicode{x2014}$demonstrating the efficacy of explicitly modeling fact versus opinion prior to quantifying bias intensity. By combining enhanced detection accuracy with interpretable explanations, this approach sets a foundation for promoting fairness and accountability in modern language models.         ",
    "url": "https://arxiv.org/abs/2503.00355",
    "authors": [
      "Tianyi Huang",
      "Elsa Fan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00358",
    "title": "CRUPL: A Semi-Supervised Cyber Attack Detection with Consistency Regularization and Uncertainty-aware Pseudo-Labeling in Smart Grid",
    "abstract": "           The modern power grids are integrated with digital technologies and automation systems. The inclusion of digital technologies has made the smart grids vulnerable to cyber-attacks. Cyberattacks on smart grids can compromise data integrity and jeopardize the reliability of the power supply. Traditional intrusion detection systems often need help to effectively detect novel and sophisticated attacks due to their reliance on labeled training data, which may only encompass part of the spectrum of potential threats. This work proposes a semi-supervised method for cyber-attack detection in smart grids by leveraging the labeled and unlabeled measurement data. We implement consistency regularization and pseudo-labeling to identify deviations from expected behavior and predict the attack classes. We use a curriculum learning approach to improve pseudo-labeling performance, capturing the model uncertainty. We demonstrate the efficiency of the proposed method in detecting different types of cyberattacks, minimizing the false positives by implementing them on publicly available datasets. The method proposes a promising solution by improving the detection accuracy to 99% in the presence of unknown samples and significantly reducing false positives.         ",
    "url": "https://arxiv.org/abs/2503.00358",
    "authors": [
      "Smruti P. Dash",
      "Kedar V. Khandeparkar",
      "Nipun Agrawal"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00359",
    "title": "Solving Instance Detection from an Open-World Perspective",
    "abstract": "           Instance detection (InsDet) aims to localize specific object instances within a novel scene imagery based on given visual references. Technically, it requires proposal detection to identify all possible object instances, followed by instance-level matching to pinpoint the ones of interest. Its open-world nature supports its wide-ranging applications from robotics to AR/VR, but also presents significant challenges: methods must generalize to unknown testing data distributions because (1) the testing scene imagery is unseen during training, and (2) there are domain gaps between visual references and detected proposals. Existing methods attempt to tackle these challenges by synthesizing diverse training examples or utilizing off-the-shelf foundation models (FMs). However, they only partially capitalize the available open-world information. In this paper, we approach InsDet from an Open-World perspective, introducing our method IDOW. We find that, while pretrained FMs yield high recall in instance detection, they are not specifically optimized for instance-level feature matching. To address this, we adapt pretrained FMs for improved instance-level matching using open-world data. Our approach incorporates metric learning along with novel data augmentations, which sample distractors as negative examples and synthesize novel-view instances to enrich the visual references. Extensive experiments demonstrate that our method significantly outperforms prior works, achieving >10 AP over previous results on two recently released challenging benchmark datasets in both conventional and novel instance detection settings.         ",
    "url": "https://arxiv.org/abs/2503.00359",
    "authors": [
      "Qianqian Shen",
      "Yunhan Zhao",
      "Nahyun Kwon",
      "Jeeeun Kim",
      "Yanan Li",
      "Shu Kong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00374",
    "title": "MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning via Modality Alignment and Retention",
    "abstract": "           Histopathology and transcriptomics are fundamental modalities in oncology, encapsulating the morphological and molecular aspects of the disease. Multi-modal self-supervised learning has demonstrated remarkable potential in learning pathological representations by integrating diverse data sources. Conventional multi-modal integration methods primarily emphasize modality alignment, while paying insufficient attention to retaining the modality-specific structures. However, unlike conventional scenarios where multi-modal inputs share highly overlapping features, histopathology and transcriptomics exhibit pronounced heterogeneity, offering orthogonal yet complementary insights. Histopathology provides morphological and spatial context, elucidating tissue architecture and cellular topology, whereas transcriptomics delineates molecular signatures through gene expression patterns. This inherent disparity introduces a major challenge in aligning them while maintaining modality-specific fidelity. To address these challenges, we present MIRROR, a novel multi-modal representation learning method designed to foster both modality alignment and retention. MIRROR employs dedicated encoders to extract comprehensive features for each modality, which is further complemented by a modality alignment module to achieve seamless integration between phenotype patterns and molecular profiles. Furthermore, a modality retention module safeguards unique attributes from each modality, while a style clustering module mitigates redundancy and enhances disease-relevant information by modeling and aligning consistent pathological signatures within a clustering space. Extensive evaluations on TCGA cohorts for cancer subtyping and survival analysis highlight MIRROR's superior performance, demonstrating its effectiveness in constructing comprehensive oncological feature representations and benefiting the cancer diagnosis.         ",
    "url": "https://arxiv.org/abs/2503.00374",
    "authors": [
      "Tianyi Wang",
      "Jianan Fan",
      "Dingxin Zhang",
      "Dongnan Liu",
      "Yong Xia",
      "Heng Huang",
      "Weidong Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00377",
    "title": "Adversarial Attacks on Event-Based Pedestrian Detectors: A Physical Approach",
    "abstract": "           Event cameras, known for their low latency and high dynamic range, show great potential in pedestrian detection applications. However, while recent research has primarily focused on improving detection accuracy, the robustness of event-based visual models against physical adversarial attacks has received limited attention. For example, adversarial physical objects, such as specific clothing patterns or accessories, can exploit inherent vulnerabilities in these systems, leading to misdetections or misclassifications. This study is the first to explore physical adversarial attacks on event-driven pedestrian detectors, specifically investigating whether certain clothing patterns worn by pedestrians can cause these detectors to fail, effectively rendering them unable to detect the person. To address this, we developed an end-to-end adversarial framework in the digital domain, framing the design of adversarial clothing textures as a 2D texture optimization problem. By crafting an effective adversarial loss function, the framework iteratively generates optimal textures through backpropagation. Our results demonstrate that the textures identified in the digital domain possess strong adversarial properties. Furthermore, we translated these digitally optimized textures into physical clothing and tested them in real-world scenarios, successfully demonstrating that the designed textures significantly degrade the performance of event-based pedestrian detection models. This work highlights the vulnerability of such models to physical adversarial attacks.         ",
    "url": "https://arxiv.org/abs/2503.00377",
    "authors": [
      "Guixu Lin",
      "Muyao Niu",
      "Qingtian Zhu",
      "Zhengwei Yin",
      "Zhuoxiao Li",
      "Shengfeng He",
      "Yinqiang Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00378",
    "title": "Conditioning on Local Statistics for Scalable Heterogeneous Federated Learning",
    "abstract": "           Federated learning is a distributed machine learning approach where multiple clients collaboratively train a model without sharing their local data, which contributes to preserving privacy. A challenge in federated learning is managing heterogeneous data distributions across clients, which can hinder model convergence and performance due to the need for the global model to generalize well across diverse local datasets. We propose to use local characteristic statistics, by which we mean some statistical properties calculated independently by each client using only their local training dataset. These statistics, such as means, covariances, and higher moments, are used to capture the characteristics of the local data distribution. They are not shared with other clients or a central node. During training, these local statistics help the model learn how to condition on the local data distribution, and during inference, they guide the client's predictions. Our experiments show that this approach allows for efficient handling of heterogeneous data across the federation, has favorable scaling compared to approaches that directly try to identify peer nodes that share distribution characteristics, and maintains privacy as no additional information needs to be communicated.         ",
    "url": "https://arxiv.org/abs/2503.00378",
    "authors": [
      "Rickard Br\u00e4nnvall"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2503.00380",
    "title": "Adaptive Wall-Following Control for Unmanned Ground Vehicles Using Spiking Neural Networks",
    "abstract": "           Unmanned ground vehicles operating in complex environments must adaptively adjust to modeling uncertainties and external disturbances to perform tasks such as wall following and obstacle avoidance. This paper introduces an adaptive control approach based on spiking neural networks for wall fitting and tracking, which learns and adapts to unforeseen disturbances. We propose real-time wall-fitting algorithms to model unknown wall shapes and generate corresponding trajectories for the vehicle to follow. A discretized linear quadratic regulator is developed to provide a baseline control signal based on an ideal vehicle model. Point matching algorithms then identify the nearest matching point on the trajectory to generate feedforward control inputs. Finally, an adaptive spiking neural network controller, which adjusts its connection weights online based on error signals, is integrated with the aforementioned control algorithms. Numerical simulations demonstrate that this adaptive control framework outperforms the traditional linear quadratic regulator in tracking complex trajectories and following irregular walls, even in the presence of partial actuator failures and state estimation errors.         ",
    "url": "https://arxiv.org/abs/2503.00380",
    "authors": [
      "Hengye Yang",
      "Yanxiao Chen",
      "Zexuan Fan",
      "Lin Shao",
      "Tao Sun"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.00383",
    "title": "Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems",
    "abstract": "           By locally encoding raw data into intermediate features, collaborative inference enables end users to leverage powerful deep learning models without exposure of sensitive raw data to cloud servers. However, recent studies have revealed that these intermediate features may not sufficiently preserve privacy, as information can be leaked and raw data can be reconstructed via model inversion attacks (MIAs). Obfuscation-based methods, such as noise corruption, adversarial representation learning, and information filters, enhance the inversion robustness by obfuscating the task-irrelevant redundancy empirically. However, methods for quantifying such redundancy remain elusive, and the explicit mathematical relation between this redundancy minimization and inversion robustness enhancement has not yet been established. To address that, this work first theoretically proves that the conditional entropy of inputs given intermediate features provides a guaranteed lower bound on the reconstruction mean square error (MSE) under any MIA. Then, we derive a differentiable and solvable measure for bounding this conditional entropy based on the Gaussian mixture estimation and propose a conditional entropy maximization (CEM) algorithm to enhance the inversion robustness. Experimental results on four datasets demonstrate the effectiveness and adaptability of our proposed CEM; without compromising feature utility and computing efficiency, plugging the proposed CEM into obfuscation-based defense mechanisms consistently boosts their inversion robustness, achieving average gains ranging from 12.9\\% to 48.2\\%. Code is available at \\href{this https URL}{this https URL}.         ",
    "url": "https://arxiv.org/abs/2503.00383",
    "authors": [
      "Song Xia",
      "Yi Yu",
      "Wenhan Yang",
      "Meiwen Ding",
      "Zhuo Chen",
      "Lingyu Duan",
      "Alex C. Kot",
      "Xudong Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.00384",
    "title": "A Survey of Adversarial Defenses in Vision-based Systems: Categorization, Methods and Challenges",
    "abstract": "           Adversarial attacks have emerged as a major challenge to the trustworthy deployment of machine learning models, particularly in computer vision applications. These attacks have a varied level of potency and can be implemented in both white box and black box approaches. Practical attacks include methods to manipulate the physical world and enforce adversarial behaviour by the corresponding target neural network models. Multiple different approaches to mitigate different kinds of such attacks are available in the literature, each with their own advantages and limitations. In this survey, we present a comprehensive systematization of knowledge on adversarial defenses, focusing on two key computer vision tasks: image classification and object detection. We review the state-of-the-art adversarial defense techniques and categorize them for easier comparison. In addition, we provide a schematic representation of these categories within the context of the overall machine learning pipeline, facilitating clearer understanding and benchmarking of defenses. Furthermore, we map these defenses to the types of adversarial attacks and datasets where they are most effective, offering practical insights for researchers and practitioners. This study is necessary for understanding the scope of how the available defenses are able to address the adversarial threats, and their shortcomings as well, which is necessary for driving the research in this area in the most appropriate direction, with the aim of building trustworthy AI systems for regular practical use-cases.         ",
    "url": "https://arxiv.org/abs/2503.00384",
    "authors": [
      "Nandish Chattopadhyay",
      "Abdul Basit",
      "Bassem Ouni",
      "Muhammad Shafique"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00393",
    "title": "Reservoir Network with Structural Plasticity for Human Activity Recognition",
    "abstract": "           The unprecedented dissemination of edge devices is accompanied by a growing demand for neuromorphic chips that can process time-series data natively without cloud support. Echo state network (ESN) is a class of recurrent neural networks that can be used to identify unique patterns in time-series data and predict future events. It is known for minimal computing resource requirements and fast training, owing to the use of linear optimization solely at the readout stage. In this work, a custom-design neuromorphic chip based on ESN targeting edge devices is proposed. The proposed system supports various learning mechanisms, including structural plasticity and synaptic plasticity, locally on-chip. This provides the network with an additional degree of freedom to continuously learn, adapt, and alter its structure and sparsity level, ensuring high performance and continuous stability. We demonstrate the performance of the proposed system as well as its robustness to noise against real-world time-series datasets while considering various topologies of data movement. An average accuracy of 95.95% and 85.24% are achieved on human activity recognition and prosthetic finger control, respectively. We also illustrate that the proposed system offers a throughput of 6x10^4 samples/sec with a power consumption of 47.7mW on a 65nm IBM process.         ",
    "url": "https://arxiv.org/abs/2503.00393",
    "authors": [
      "Abdullah M. Zyarah",
      "Alaa M. Abdul-Hadi",
      "Dhireesha Kudithipudi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00400",
    "title": "Inteval Analysis for two spherical functions arising from robust Perspective-n-Lines problem",
    "abstract": "           This report presents a comprehensive interval analysis of two spherical functions derived from the robust Perspective-n-Lines (PnL) problem. The study is motivated by the application of a dimension-reduction technique to achieve global solutions for the robust PnL problem. We establish rigorous theoretical results, supported by detailed proofs, and validate our findings through extensive numerical simulations.         ",
    "url": "https://arxiv.org/abs/2503.00400",
    "authors": [
      "Xiang Zheng",
      "Haodong Jiang",
      "Junfeng Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.00404",
    "title": "SecRef*: Securely Sharing Mutable References Between Verified and Unverified Code in F*",
    "abstract": "           We introduce SecRef*, a secure compilation framework protecting stateful programs verified in F* against linked unverified code, with which the program dynamically shares ML-style mutable references. To ease program verification in this setting, we propose a way of tracking which references are shareable with the unverified code, and which ones are not shareable and whose contents are thus guaranteed to be unchanged after calling into unverified code. This universal property of non-shareable references is exposed in the interface on which the verified program can rely when calling into unverified code. The remaining refinement types and pre- and post-conditions that the verified code expects from the unverified code are converted into dynamic checks about the shared references by using higher-order contracts. We prove formally in F* that this strategy ensures sound and secure interoperability with unverified code. Since SecRef* is built on top of the Monotonic State effect of F*, these proofs rely on the first monadic representation for this effect, which is a contribution of our work that can be of independent interest. Finally, we use SecRef* to build a simple cooperative multi-threading scheduler that is verified and that securely interacts with unverified threads.         ",
    "url": "https://arxiv.org/abs/2503.00404",
    "authors": [
      "Cezar-Constantin Andrici",
      "Danel Ahman",
      "Catalin Hritcu",
      "Ruxandra Icleanu",
      "Guido Mart\u00ednez",
      "Exequiel Rivas",
      "Th\u00e9o Winterhalter"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.00414",
    "title": "SGC-Net: Stratified Granular Comparison Network for Open-Vocabulary HOI Detection",
    "abstract": "           Recent open-vocabulary human-object interaction (OV-HOI) detection methods primarily rely on large language model (LLM) for generating auxiliary descriptions and leverage knowledge distilled from CLIP to detect unseen interaction categories. Despite their effectiveness, these methods face two challenges: (1) feature granularity deficiency, due to reliance on last layer visual features for text alignment, leading to the neglect of crucial object-level details from intermediate layers; (2) semantic similarity confusion, resulting from CLIP's inherent biases toward certain classes, while LLM-generated descriptions based solely on labels fail to adequately capture inter-class similarities. To address these challenges, we propose a stratified granular comparison network. First, we introduce a granularity sensing alignment module that aggregates global semantic features with local details, refining interaction representations and ensuring robust alignment between intermediate visual features and text embeddings. Second, we develop a hierarchical group comparison module that recursively compares and groups classes using LLMs, generating fine-grained and discriminative descriptions for each interaction category. Experimental results on two widely-used benchmark datasets, SWIG-HOI and HICO-DET, demonstrate that our method achieves state-of-the-art results in OV-HOI detection. Codes will be released on this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00414",
    "authors": [
      "Xin Lin",
      "Chong Shi",
      "Zuopeng Yang",
      "Haojin Tang",
      "Zhili Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00436",
    "title": "HalCECE: A Framework for Explainable Hallucination Detection through Conceptual Counterfactuals in Image Captioning",
    "abstract": "           In the dynamic landscape of artificial intelligence, the exploration of hallucinations within vision-language (VL) models emerges as a critical frontier. This work delves into the intricacies of hallucinatory phenomena exhibited by widely used image captioners, unraveling interesting patterns. Specifically, we step upon previously introduced techniques of conceptual counterfactual explanations to address VL hallucinations. The deterministic and efficient nature of the employed conceptual counterfactuals backbone is able to suggest semantically minimal edits driven by hierarchical knowledge, so that the transition from a hallucinated caption to a non-hallucinated one is performed in a black-box manner. HalCECE, our proposed hallucination detection framework is highly interpretable, by providing semantically meaningful edits apart from standalone numbers, while the hierarchical decomposition of hallucinated concepts leads to a thorough hallucination analysis. Another novelty tied to the current work is the investigation of role hallucinations, being one of the first works to involve interconnections between visual concepts in hallucination detection. Overall, HalCECE recommends an explainable direction to the crucial field of VL hallucination detection, thus fostering trustworthy evaluation of current and future VL systems.         ",
    "url": "https://arxiv.org/abs/2503.00436",
    "authors": [
      "Maria Lymperaiou",
      "Giorgos FIlandrianos",
      "Angeliki Dimitriou",
      "Athanasios Voulodimos",
      "Giorgos Stamou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00442",
    "title": "Detection of Customer Interested Garments in Surveillance Video using Computer Vision",
    "abstract": "           One of the basic requirements of humans is clothing and this approach aims to identify the garments selected by customer during shopping, from surveillance video. The existing approaches to detect garments were developed on western wear using datasets of western clothing. They do not address Indian garments due to the increased complexity. In this work, we propose a computer vision based framework to address this problem through video surveillance. The proposed framework uses the Mixture of Gaussians background subtraction algorithm to identify the foreground present in a video frame. The visual information present in this foreground is analysed using computer vision techniques such as image segmentation to detect the various garments, the customer is interested in. The framework was tested on a dataset, that comprises of CCTV videos from a garments store. When presented with raw surveillance footage, the proposed framework demonstrated its effectiveness in detecting the interest of customer in choosing their garments by achieving a high precision and recall.         ",
    "url": "https://arxiv.org/abs/2503.00442",
    "authors": [
      "Earnest Paul Ijjina",
      "Aniruddha Srinivas Joshi",
      "Goutham Kanahasabai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00464",
    "title": "Unstable Grounds for Beautiful Trees? Testing the Robustness of Concept Translations in the Compilation of Multilingual Wordlists",
    "abstract": "           Multilingual wordlists play a crucial role in comparative linguistics. While many studies have been carried out to test the power of computational methods for language subgrouping or divergence time estimation, few studies have put the data upon which these studies are based to a rigorous test. Here, we conduct a first experiment that tests the robustness of concept translation as an integral part of the compilation of multilingual wordlists. Investigating the variation in concept translations in independently compiled wordlists from 10 dataset pairs covering 9 different language families, we find that on average, only 83% of all translations yield the same word form, while identical forms in terms of phonetic transcriptions can only be found in 23% of all cases. Our findings can prove important when trying to assess the uncertainty of phylogenetic studies and the conclusions derived from them.         ",
    "url": "https://arxiv.org/abs/2503.00464",
    "authors": [
      "David Snee",
      "Luca Ciucci",
      "Arne Rubehn",
      "Kellen Parker van Dam",
      "Johann-Mattis List"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.00476",
    "title": "G-OSR: A Comprehensive Benchmark for Graph Open-Set Recognition",
    "abstract": "           Graph Neural Networks (GNNs) have achieved significant success in machine learning, with wide applications in social networks, bioinformatics, knowledge graphs, and other fields. Most research assumes ideal closed-set environments. However, in real-world open-set environments, graph learning models face challenges in robustness and reliability due to unseen classes. This highlights the need for Graph Open-Set Recognition (GOSR) methods to address these issues and ensure effective GNN application in practical scenarios. Research in GOSR is in its early stages, with a lack of a comprehensive benchmark spanning diverse tasks and datasets to evaluate methods. Moreover, traditional methods, Graph Out-of-Distribution Detection (GOODD), GOSR, and Graph Anomaly Detection (GAD) have mostly evolved in isolation, with little exploration of their interconnections or potential applications to GOSR. To fill these gaps, we introduce \\textbf{G-OSR}, a comprehensive benchmark for evaluating GOSR methods at both the node and graph levels, using datasets from multiple domains to ensure fair and standardized comparisons of effectiveness and efficiency across traditional, GOODD, GOSR, and GAD methods. The results offer critical insights into the generalizability and limitations of current GOSR methods and provide valuable resources for advancing research in this field through systematic analysis of diverse approaches.         ",
    "url": "https://arxiv.org/abs/2503.00476",
    "authors": [
      "Yicong Dong",
      "Rundong He",
      "Guangyao Chen",
      "Wentao Zhang",
      "Zhongyi Han",
      "Jieming Shi",
      "Yilong Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00477",
    "title": "TSDW: A Tri-Stream Dynamic Weight Network for Cloth-Changing Person Re-Identification",
    "abstract": "           Cloth-Changing Person Re-identification (CC-ReID) aims to solve the challenge of identifying individuals across different temporal-spatial scenarios, viewpoints, and clothing variations. This field is gaining increasing attention in big data research and public security domains. Existing ReID research primarily relies on face recognition, gait semantic recognition, and clothing-irrelevant feature identification, which perform relatively well in scenarios with high-quality clothing change videos and images. However, these approaches depend on either single features or simple combinations of multiple features, making further performance improvements difficult. Additionally, limitations such as missing facial information, challenges in gait extraction, and inconsistent camera parameters restrict the broader application of CC-ReID. To address the above limitations, we innovatively propose a Tri-Stream Dynamic Weight Network (TSDW) that requires only images. This dynamic weighting network consists of three parallel feature streams: facial features, head-limb features, and global features. Each stream specializes in extracting its designated features, after which a gating network dynamically fuses confidence levels. The three parallel feature streams enhance recognition performance and reduce the impact of any single feature failure, thereby improving model robustness. Extensive experiments on benchmark datasets (e.g., PRCC, Celeb-reID, VC-Clothes) demonstrate that our method significantly outperforms existing state-of-the-art approaches.         ",
    "url": "https://arxiv.org/abs/2503.00477",
    "authors": [
      "Ruiqi He",
      "Zihan Wang",
      "Xiang Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00485",
    "title": "Homomorphism Expressivity of Spectral Invariant Graph Neural Networks",
    "abstract": "           Graph spectra are an important class of structural features on graphs that have shown promising results in enhancing Graph Neural Networks (GNNs). Despite their widespread practical use, the theoretical understanding of the power of spectral invariants -- particularly their contribution to GNNs -- remains incomplete. In this paper, we address this fundamental question through the lens of homomorphism expressivity, providing a comprehensive and quantitative analysis of the expressive power of spectral invariants. Specifically, we prove that spectral invariant GNNs can homomorphism-count exactly a class of specific tree-like graphs which we refer to as parallel trees. We highlight the significance of this result in various contexts, including establishing a quantitative expressiveness hierarchy across different architectural variants, offering insights into the impact of GNN depth, and understanding the subgraph counting capabilities of spectral invariant GNNs. In particular, our results significantly extend Arvind et al. (2024) and settle their open questions. Finally, we generalize our analysis to higher-order GNNs and answer an open question raised by Zhang et al. (2024).         ",
    "url": "https://arxiv.org/abs/2503.00485",
    "authors": [
      "Jingchu Gai",
      "Yiheng Du",
      "Bohang Zhang",
      "Haggai Maron",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00516",
    "title": "Two-stream Beats One-stream: Asymmetric Siamese Network for Efficient Visual Tracking",
    "abstract": "           Efficient tracking has garnered attention for its ability to operate on resource-constrained platforms for real-world deployment beyond desktop GPUs. Current efficient trackers mainly follow precision-oriented trackers, adopting a one-stream framework with lightweight modules. However, blindly adhering to the one-stream paradigm may not be optimal, as incorporating template computation in every frame leads to redundancy, and pervasive semantic interaction between template and search region places stress on edge devices. In this work, we propose a novel asymmetric Siamese tracker named \\textbf{AsymTrack} for efficient tracking. AsymTrack disentangles template and search streams into separate branches, with template computing only once during initialization to generate modulation signals. Building on this architecture, we devise an efficient template modulation mechanism to unidirectional inject crucial cues into the search features, and design an object perception enhancement module that integrates abstract semantics and local details to overcome the limited representation in lightweight tracker. Extensive experiments demonstrate that AsymTrack offers superior speed-precision trade-offs across different platforms compared to the current state-of-the-arts. For instance, AsymTrack-T achieves 60.8\\% AUC on LaSOT and 224/81/84 FPS on GPU/CPU/AGX, surpassing HiT-Tiny by 6.0\\% AUC with higher speeds. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00516",
    "authors": [
      "Jiawen Zhu",
      "Huayi Tang",
      "Xin Chen",
      "Xinying Wang",
      "Dong Wang",
      "Huchuan Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00521",
    "title": "2DMCG:2DMambawith Change Flow Guidance for Change Detection in Remote Sensing",
    "abstract": "           Remote sensing change detection (CD) has made significant advancements with the adoption of Convolutional Neural Networks (CNNs) and Transformers. While CNNs offer powerful feature extraction, they are constrained by receptive field limitations, and Transformers suffer from quadratic complexity when processing long sequences, restricting scalability. The Mamba architecture provides an appealing alternative, offering linear complexity and high parallelism. However, its inherent 1D processing structure causes a loss of spatial information in 2D vision tasks. This paper addresses this limitation by proposing an efficient framework based on a Vision Mamba variant that enhances its ability to capture 2D spatial information while maintaining the linear complexity characteristic of Mamba. The framework employs a 2DMamba encoder to effectively learn global spatial contextual information from multi-temporal images. For feature fusion, we introduce a 2D scan-based, channel-parallel scanning strategy combined with a spatio-temporal feature fusion method, which adeptly captures both local and global change information, alleviating spatial discontinuity issues during fusion. In the decoding stage, we present a feature change flow-based decoding method that improves the mapping of feature change information from low-resolution to high-resolution feature maps, mitigating feature shift and misalignment. Extensive experiments on benchmark datasets such as LEVIR-CD+ and WHU-CD demonstrate the superior performance of our framework compared to state-of-the-art methods, showcasing the potential of Vision Mamba for efficient and accurate remote sensing change detection.         ",
    "url": "https://arxiv.org/abs/2503.00521",
    "authors": [
      "JunYao Kaung",
      "HongWei Ge"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00529",
    "title": "Co-state Neural Network for Real-time Nonlinear Optimal Control with Input Constraints",
    "abstract": "           In this paper, we propose a method to solve nonlinear optimal control problems (OCPs) with constrained control input in real-time using neural networks (NNs). We introduce what we have termed co-state Neural Network (CoNN) that learns the mapping from any given state value to its corresponding optimal co-state trajectory based on the Pontryagin's Minimum (Maximum) Principle (PMP). In essence, the CoNN parameterizes the Two-Point Boundary Value Problem (TPBVP) that results from the PMP for various initial states. The CoNN is trained using data generated from numerical solutions of TPBVPs for unconstrained OCPs to learn the mapping from a state to its corresponding optimal co-state trajectory. For better generalizability, the CoNN is also trained to respect the first-order optimality conditions (system dynamics). The control input constraints are satisfied by solving a quadratic program (QP) given the predicted optimal co-states. We demonstrate the effectiveness of our CoNN-based controller in a feedback scheme for numerical examples with both unconstrained and constrained control input. We also verify that the controller can handle unknown disturbances effectively.         ",
    "url": "https://arxiv.org/abs/2503.00529",
    "authors": [
      "Lihan Lian",
      "Uduak Inyang-Udoh"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.00539",
    "title": "Distributionally Robust Reinforcement Learning with Human Feedback",
    "abstract": "           Reinforcement learning from human feedback (RLHF) has evolved to be one of the main methods for fine-tuning large language models (LLMs). However, existing RLHF methods are non-robust, and their performance deteriorates if the downstream task differs significantly from the preference dataset used in fine-tuning. In order to mitigate this problem, we introduce a distributionally robust RLHF for fine-tuning LLMs. In particular, our goal is to ensure that a fine-tuned model retains its performance even when the distribution of prompts significantly differs from the distribution encountered during fine-tuning. We formulate distributionally robust optimization (DRO) version of two popular fine-tuning methods -- (1) reward-based RLHF and (2) reward-free DPO (direct preference optimization). We propose a minibatch gradient descent based algorithms for both of them, and theoretically prove convergence guarantees for the algorithms. Subsequently, we evaluate our algorithms on an out-of-distribution (OOD) task by first training the model on the Unified-Feedback dataset and evaluating its performance on two different datasets. The experimental results show that our robust training improves the accuracy of the learned reward models on average, and markedly on some tasks, such as reasoning. Furthermore, we show that the robust versions of policy optimization methods, similarly improve performance on OOD tasks.         ",
    "url": "https://arxiv.org/abs/2503.00539",
    "authors": [
      "Debmalya Mandal",
      "Paulius Sasnauskas",
      "Goran Radanovic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00547",
    "title": "Performance Heterogeneity in Graph Neural Networks: Lessons for Architecture Design and Preprocessing",
    "abstract": "           Graph Neural Networks have emerged as the most popular architecture for graph-level learning, including graph classification and regression tasks, which frequently arise in areas such as biochemistry and drug discovery. Achieving good performance in practice requires careful model design. Due to gaps in our understanding of the relationship between model and data characteristics, this often requires manual architecture and hyperparameter tuning. This is particularly pronounced in graph-level tasks, due to much higher variation in the input data than in node-level tasks. To work towards closing these gaps, we begin with a systematic analysis of individual performance in graph-level tasks. Our results establish significant performance heterogeneity in both message-passing and transformer-based architectures. We then investigate the interplay of model and data characteristics as drivers of the observed heterogeneity. Our results suggest that graph topology alone cannot explain heterogeneity. Using the Tree Mover's Distance, which jointly evaluates topological and feature information, we establish a link between class-distance ratios and performance heterogeneity in graph classification. These insights motivate model and data preprocessing choices that account for heterogeneity between graphs. We propose a selective rewiring approach, which only targets graphs whose individual performance benefits from rewiring. We further show that the optimal network depth depends on the graph's spectrum, which motivates a heuristic for choosing the number of GNN layers. Our experiments demonstrate the utility of both design choices in practice.         ",
    "url": "https://arxiv.org/abs/2503.00547",
    "authors": [
      "Lukas Fesser",
      "Melanie Weber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00548",
    "title": "Unbiased Video Scene Graph Generation via Visual and Semantic Dual Debiasing",
    "abstract": "           Video Scene Graph Generation (VidSGG) aims to capture dynamic relationships among entities by sequentially analyzing video frames and integrating visual and semantic information. However, VidSGG is challenged by significant biases that skew predictions. To mitigate these biases, we propose a VIsual and Semantic Awareness (VISA) framework for unbiased VidSGG. VISA addresses visual bias through memory-enhanced temporal integration that enhances object representations and concurrently reduces semantic bias by iteratively integrating object features with comprehensive semantic information derived from triplet relationships. This visual-semantics dual debiasing approach results in more unbiased representations of complex scene dynamics. Extensive experiments demonstrate the effectiveness of our method, where VISA outperforms existing unbiased VidSGG approaches by a substantial margin (e.g., +13.1% improvement in mR@20 and mR@50 for the SGCLS task under Semi Constraint).         ",
    "url": "https://arxiv.org/abs/2503.00548",
    "authors": [
      "Yanjun Li",
      "Zhaoyang Li",
      "Honghui Chen",
      "Lizhi Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2503.00551",
    "title": "PL-VIWO: A Lightweight and Robust Point-Line Monocular Visual Inertial Wheel Odometry",
    "abstract": "           This paper presents a novel tightly coupled Filter-based monocular visual-inertial-wheel odometry (VIWO) system for ground robots, designed to deliver accurate and robust localization in long-term complex outdoor navigation scenarios. As an external sensor, the camera enhances localization performance by introducing visual constraints. However, obtaining a sufficient number of effective visual features is often challenging, particularly in dynamic or low-texture environments. To address this issue, we incorporate the line features for additional geometric constraints. Unlike traditional approaches that treat point and line features independently, our method exploits the geometric relationships between points and lines in 2D images, enabling fast and robust line matching and triangulation. Additionally, we introduce Motion Consistency Check (MCC) to filter out potential dynamic points, ensuring the effectiveness of point feature updates. The proposed system was evaluated on publicly available datasets and benchmarked against state-of-the-art methods. Experimental results demonstrate superior performance in terms of accuracy, robustness, and efficiency. The source code is publicly available at: this https URL ",
    "url": "https://arxiv.org/abs/2503.00551",
    "authors": [
      "Zhixin Zhang",
      "Wenzhi Bai",
      "Liang Zhao",
      "Pawel Ladosz"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.00563",
    "title": "A Guide to Failure in Machine Learning: Reliability and Robustness from Foundations to Practice",
    "abstract": "           One of the main barriers to adoption of Machine Learning (ML) is that ML models can fail unexpectedly. In this work, we aim to provide practitioners a guide to better understand why ML models fail and equip them with techniques they can use to reason about failure. Specifically, we discuss failure as either being caused by lack of reliability or lack of robustness. Differentiating the causes of failure in this way allows us to formally define why models fail from first principles and tie these definitions to engineering concepts and real-world deployment settings. Throughout the document we provide 1) a summary of important theoretic concepts in reliability and robustness, 2) a sampling current techniques that practitioners can utilize to reason about ML model reliability and robustness, and 3) examples that show how these concepts and techniques can apply to real-world settings.         ",
    "url": "https://arxiv.org/abs/2503.00563",
    "authors": [
      "Eric Heim",
      "Oren Wright",
      "David Shriver"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00568",
    "title": "Logica-TGD: Transforming Graph Databases Logically",
    "abstract": "           Graph transformations are a powerful computational model for manipulating complex networks, but handling temporal aspects and scalability remain significant challenges. We present a novel approach to implementing these transformations using Logica, an open-source logic programming language and system that operates on parallel databases like DuckDB and BigQuery. Leveraging the parallelism of these engines, our method enhances performance and accessibility, while also offering a practical way to handle time-varying graphs. We illustrate Logica's graph querying and transformation capabilities with several examples, including the computation of the well-founded solution to the classic \"Win-Move\" game, a declarative program for pathfinding in a dynamic graph, and the application of Logica to the collection of all current facts of Wikidata for taxonomic relations analysis. We argue that clear declarative syntax, built-in visualization and powerful supported engines make Logica a convenient tool for graph transformations.         ",
    "url": "https://arxiv.org/abs/2503.00568",
    "authors": [
      "Evgeny Skvortsov",
      "Yilin Xia",
      "Bertram Lud\u00e4scher",
      "Shawn Bowers"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2503.00575",
    "title": "Polyconvex Physics-Augmented Neural Network Constitutive Models in Principal Stretches",
    "abstract": "           Accurate constitutive models of soft materials are crucial for understanding their mechanical behavior and ensuring reliable predictions in the design process. To this end, scientific machine learning research has produced flexible and general material model architectures that can capture the behavior of a wide range of materials, reducing the need for expert-constructed closed-form models. The focus has gradually shifted towards embedding physical constraints in the network architecture to regularize these over-parameterized models. Two popular approaches are input convex neural networks (ICNN) and neural ordinary differential equations (NODE). A related alternative has been the generalization of closed-form models, such as sparse regression from a large library. Remarkably, all prior work using ICNN or NODE uses the invariants of the Cauchy-Green tensor and none uses the principal stretches. In this work, we construct general polyconvex functions of the principal stretches in a physics-aware deep-learning framework and offer insights and comparisons to invariant-based formulations. The framework is based on recent developments to characterize polyconvex functions in terms of convex functions of the right stretch tensor $\\mathbf{U}$, its cofactor $\\text{cof}\\mathbf{U}$, and its determinant $J$. Any convex function of a symmetric second-order tensor can be described with a convex and symmetric function of its eigenvalues. Thus, we first describe convex functions of $\\mathbf{U}$ and $\\text{cof}\\mathbf{U}$ in terms of their respective eigenvalues using deep Holder sets composed with ICNN functions. A third ICNN takes as input $J$ and the two convex functions of $\\mathbf{U}$ and $\\text{cof}\\mathbf{U}$, and returns the strain energy as output. The ability of the model to capture arbitrary materials is demonstrated using synthetic and experimental data.         ",
    "url": "https://arxiv.org/abs/2503.00575",
    "authors": [
      "Adrian Buganza Tepole",
      "Asghar Jadoon",
      "Manuel Rausch",
      "Jan N. Fuhg"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00576",
    "title": "Enhancing Context-Aware Human Motion Prediction for Efficient Robot Handovers",
    "abstract": "           Accurate human motion prediction (HMP) is critical for seamless human-robot collaboration, particularly in handover tasks that require real-time adaptability. Despite the high accuracy of state-of-the-art models, their computational complexity limits practical deployment in real-world robotic applications. In this work, we enhance human motion forecasting for handover tasks by leveraging siMLPe [1], a lightweight yet powerful architecture, and introducing key improvements. Our approach, named IntentMotion incorporates intention-aware conditioning, task-specific loss functions, and a novel intention classifier, significantly improving motion prediction accuracy while maintaining efficiency. Experimental results demonstrate that our method reduces body loss error by over 50%, achieves 200x faster inference, and requires only 3% of the parameters compared to existing state-of-the-art HMP models. These advancements establish our framework as a highly efficient and scalable solution for real-time human-robot interaction.         ",
    "url": "https://arxiv.org/abs/2503.00576",
    "authors": [
      "Gerard G\u00f3mez-Izquierdo",
      "Javier Laplaza",
      "Alberto Sanfeliu",
      "Ana\u00eds Garrell"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.00578",
    "title": "Channel-Attentive Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNNs) set the state-of-the-art in representation learning for graph-structured data. They are used in many domains, from online social networks to complex molecules. Most GNNs leverage the message-passing paradigm and achieve strong performances on various tasks. However, the message-passing mechanism used in most models suffers from over-smoothing as a GNN's depth increases. The over-smoothing degrades GNN's performance due to the increased similarity between the representations of unrelated nodes. This study proposes an adaptive channel-wise message-passing approach to alleviate the over-smoothing. The proposed model, Channel-Attentive GNN, learns how to attend to neighboring nodes and their feature channels. Thus, much diverse information can be transferred between nodes during message-passing. Experiments with widely used benchmark datasets show that the proposed model is more resistant to over-smoothing than baselines and achieves state-of-the-art performances for various graphs with strong heterophily. Our code is at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00578",
    "authors": [
      "Tu\u011frul Hasan Karabulut",
      "\u0130nci M. Bayta\u015f"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00580",
    "title": "Brain Foundation Models: A Survey on Advancements in Neural Signal Processing and Brain Discovery",
    "abstract": "           Brain foundation models (BFMs) have emerged as a transformative paradigm in computational neuroscience, offering a revolutionary framework for processing diverse neural signals across different brain-related tasks. These models leverage large-scale pre-training techniques, allowing them to generalize effectively across multiple scenarios, tasks, and modalities, thus overcoming the traditional limitations faced by conventional artificial intelligence (AI) approaches in understanding complex brain data. By tapping into the power of pretrained models, BFMs provide a means to process neural data in a more unified manner, enabling advanced analysis and discovery in the field of neuroscience. In this survey, we define BFMs for the first time, providing a clear and concise framework for constructing and utilizing these models in various applications. We also examine the key principles and methodologies for developing these models, shedding light on how they transform the landscape of neural signal processing. This survey presents a comprehensive review of the latest advancements in BFMs, covering the most recent methodological innovations, novel views of application areas, and challenges in the field. Notably, we highlight the future directions and key challenges that need to be addressed to fully realize the potential of BFMs. These challenges include improving the quality of brain data, optimizing model architecture for better generalization, increasing training efficiency, and enhancing the interpretability and robustness of BFMs in real-world applications.         ",
    "url": "https://arxiv.org/abs/2503.00580",
    "authors": [
      "Xinliang Zhou",
      "Chenyu Liu",
      "Zhisheng Chen",
      "Kun Wang",
      "Yi Ding",
      "Ziyu Jia",
      "Qingsong Wen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.00583",
    "title": "Space-Time Graphs of Convex Sets for Multi-Robot Motion Planning",
    "abstract": "           We address the Multi-Robot Motion Planning (MRMP) problem of computing collision-free trajectories for multiple robots in shared continuous environments. While existing frameworks effectively decompose MRMP into single-robot subproblems, spatiotemporal motion planning with dynamic obstacles remains challenging, particularly in cluttered or narrow-corridor settings. We propose Space-Time Graphs of Convex Sets (ST-GCS), a novel planner that systematically covers the collision-free space-time domain with convex sets instead of relying on random sampling. By extending Graphs of Convex Sets (GCS) into the time dimension, ST-GCS formulates time-optimal trajectories in a unified convex optimization that naturally accommodates velocity bounds and flexible arrival times. We also propose Exact Convex Decomposition (ECD) to \"reserve\" trajectories as spatiotemporal obstacles, maintaining a collision-free space-time graph of convex sets for subsequent planning. Integrated into two prioritized-planning frameworks, ST-GCS consistently achieves higher success rates and better solution quality than state-of-the-art sampling-based planners -- often at orders-of-magnitude faster runtimes -- underscoring its benefits for MRMP in challenging settings.         ",
    "url": "https://arxiv.org/abs/2503.00583",
    "authors": [
      "Jingtao Tang",
      "Zining Mao",
      "Lufan Yang",
      "Hang Ma"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00594",
    "title": "Estimation of total body fat using symbolic regression and evolutionary algorithms",
    "abstract": "           Body fat percentage is an increasingly popular alternative to Body Mass Index to measure overweight and obesity, offering a more accurate representation of body composition. In this work, we evaluate three evolutionary computation techniques, Grammatical Evolution, Context-Free Grammar Genetic Programming, and Dynamic Structured Grammatical Evolution, to derive an interpretable mathematical expression to estimate the percentage of body fat that are also accurate. Our primary objective is to obtain a model that balances accuracy with explainability, making it useful for clinical and health applications. We compare the performance of the three variants on a public anthropometric dataset and compare the results obtained with the QLattice framework. Experimental results show that grammatical evolution techniques can obtain competitive results in performance and interpretability.         ",
    "url": "https://arxiv.org/abs/2503.00594",
    "authors": [
      "Jose-Manuel Mu\u00f1oz",
      "Odin Mor\u00f3n-Garc\u00eda",
      "J. Ignacio Hidalgo",
      "Omar Costilla-Reyes"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.00596",
    "title": "BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge",
    "abstract": "           This paper proposes a novel backdoor threat attacking the LLM-as-a-Judge evaluation regime, where the adversary controls both the candidate and evaluator model. The backdoored evaluator victimizes benign users by unfairly assigning inflated scores to adversary. A trivial single token backdoor poisoning 1% of the evaluator training data triples the adversary's score with respect to their legitimate score. We systematically categorize levels of data access corresponding to three real-world settings, (1) web poisoning, (2) malicious annotator, and (3) weight poisoning. These regimes reflect a weak to strong escalation of data access that highly correlates with attack severity. Under the weakest assumptions - web poisoning (1), the adversary still induces a 20% score inflation. Likewise, in the (3) weight poisoning regime, the stronger assumptions enable the adversary to inflate their scores from 1.5/5 to 4.9/5. The backdoor threat generalizes across different evaluator architectures, trigger designs, evaluation tasks, and poisoning rates. By poisoning 10% of the evaluator training data, we control toxicity judges (Guardrails) to misclassify toxic prompts as non-toxic 89% of the time, and document reranker judges in RAG to rank the poisoned document first 97% of the time. LLM-as-a-Judge is uniquely positioned at the intersection of ethics and technology, where social implications of mislead model selection and evaluation constrain the available defensive tools. Amidst these challenges, model merging emerges as a principled tool to offset the backdoor, reducing ASR to near 0% whilst maintaining SOTA performance. Model merging's low computational cost and convenient integration into the current LLM Judge training pipeline position it as a promising avenue for backdoor mitigation in the LLM-as-a-Judge setting.         ",
    "url": "https://arxiv.org/abs/2503.00596",
    "authors": [
      "Terry Tong",
      "Fei Wang",
      "Zhe Zhao",
      "Muhao Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.00599",
    "title": "Large Engagement Networks for Classifying Coordinated Campaigns and Organic Twitter Trends",
    "abstract": "           Social media users and inauthentic accounts, such as bots, may coordinate in promoting their topics. Such topics may give the impression that they are organically popular among the public, even though they are astroturfing campaigns that are centrally managed. It is challenging to predict if a topic is organic or a coordinated campaign due to the lack of reliable ground truth. In this paper, we create such ground truth by detecting the campaigns promoted by ephemeral astroturfing attacks. These attacks push any topic to Twitter's (X) trends list by employing bots that tweet in a coordinated manner in a short period and then immediately delete their tweets. We manually curate a dataset of organic Twitter trends. We then create engagement networks out of these datasets which can serve as a challenging testbed for graph classification task to distinguish between campaigns and organic trends. Engagement networks consist of users as nodes and engagements as edges (retweets, replies, and quotes) between users. We release the engagement networks for 179 campaigns and 135 non-campaigns, and also provide finer-grain labels to characterize the type of the campaigns and non-campaigns. Our dataset, LEN (Large Engagement Networks), is available in the URL below. In comparison to traditional graph classification datasets, which are small with tens of nodes and hundreds of edges at most, graphs in LEN are larger. The average graph in LEN has ~11K nodes and ~23K edges. We show that state-of-the-art GNN methods give only mediocre results for campaign vs. non-campaign and campaign type classification on LEN. LEN offers a unique and challenging playfield for the graph classification problem. We believe that LEN will help advance the frontiers of graph classification techniques on large networks and also provide an interesting use case in terms of distinguishing coordinated campaigns and organic trends.         ",
    "url": "https://arxiv.org/abs/2503.00599",
    "authors": [
      "Atul Anand Gopalakrishnan",
      "Jakir Hossain",
      "Tugrulcan Elmas",
      "Ahmet Erdem Sariyuce"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00615",
    "title": "xIDS-EnsembleGuard: An Explainable Ensemble Learning-based Intrusion Detection System",
    "abstract": "           In this paper, we focus on addressing the challenges of detecting malicious attacks in networks by designing an advanced Explainable Intrusion Detection System (xIDS). The existing machine learning and deep learning approaches have invisible limitations, such as potential biases in predictions, a lack of interpretability, and the risk of overfitting to training data. These issues can create doubt about their usefulness, transparency, and a decrease in trust among stakeholders. To overcome these challenges, we propose an ensemble learning technique called \"EnsembleGuard.\" This approach uses the predicted outputs of multiple models, including tree-based methods (LightGBM, GBM, Bagging, XGBoost, CatBoost) and deep learning models such as LSTM (long short-term memory) and GRU (gated recurrent unit), to maintain a balance and achieve trustworthy results. Our work is unique because it combines both tree-based and deep learning models to design an interpretable and explainable meta-model through model distillation. By considering the predictions of all individual models, our meta-model effectively addresses key challenges and ensures both explainable and reliable results. We evaluate our model using well-known datasets, including UNSW-NB15, NSL-KDD, and CIC-IDS-2017, to assess its reliability against various types of attacks. During analysis, we found that our model outperforms both tree-based models and other comparative approaches in different attack scenarios.         ",
    "url": "https://arxiv.org/abs/2503.00615",
    "authors": [
      "Muhammad Adil",
      "Mian Ahmad Jan",
      "Safayat Bin Hakim",
      "Houbing Herbert Song",
      "Zhanpeng Jin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.00639",
    "title": "Synergy Between Sufficient Changes and Sparse Mixing Procedure for Disentangled Representation Learning",
    "abstract": "           Disentangled representation learning aims to uncover latent variables underlying the observed data, and generally speaking, rather strong assumptions are needed to ensure identifiability. Some approaches rely on sufficient changes on the distribution of latent variables indicated by auxiliary variables such as domain indices, but acquiring enough domains is often challenging. Alternative approaches exploit structural sparsity assumptions on the mixing procedure, but such constraints are usually (partially) violated in practice. Interestingly, we find that these two seemingly unrelated assumptions can actually complement each other to achieve identifiability. Specifically, when conditioned on auxiliary variables, the sparse mixing procedure assumption provides structural constraints on the mapping from estimated to true latent variables and hence compensates for potentially insufficient distribution changes. Building on this insight, we propose an identifiability theory with less restrictive constraints regarding distribution changes and the sparse mixing procedure, enhancing applicability to real-world scenarios. Additionally, we develop an estimation framework incorporating a domain encoding network and a sparse mixing constraint and provide two implementations based on variational autoencoders and generative adversarial networks, respectively. Experiment results on synthetic and real-world datasets support our theoretical results.         ",
    "url": "https://arxiv.org/abs/2503.00639",
    "authors": [
      "Zijian Li",
      "Shunxing Fan",
      "Yujia Zheng",
      "Ignavier Ng",
      "Shaoan Xie",
      "Guangyi Chen",
      "Xinshuai Dong",
      "Ruichu Cai",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.00643",
    "title": "Deep Change Monitoring: A Hyperbolic Representative Learning Framework and a Dataset for Long-term Fine-grained Tree Change Detection",
    "abstract": "           In environmental protection, tree monitoring plays an essential role in maintaining and improving ecosystem health. However, precise monitoring is challenging because existing datasets fail to capture continuous fine-grained changes in trees due to low-resolution images and high acquisition costs. In this paper, we introduce UAVTC, a large-scale, long-term, high-resolution dataset collected using UAVs equipped with cameras, specifically designed to detect individual Tree Changes (TCs). UAVTC includes rich annotations and statistics based on biological knowledge, offering a fine-grained view for tree monitoring. To address environmental influences and effectively model the hierarchical diversity of physiological TCs, we propose a novel Hyperbolic Siamese Network (HSN) for TC detection, enabling compact and hierarchical representations of dynamic tree changes. Extensive experiments show that HSN can effectively capture complex hierarchical changes and provide a robust solution for fine-grained TC detection. In addition, HSN generalizes well to cross-domain face anti-spoofing task, highlighting its broader significance in AI. We believe our work, combining ecological insights and interdisciplinary expertise, will benefit the community by offering a new benchmark and innovative AI technologies.         ",
    "url": "https://arxiv.org/abs/2503.00643",
    "authors": [
      "Yante Li",
      "Hanwen Qi",
      "Haoyu Chen",
      "Xinlian Liang",
      "Guoying Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00647",
    "title": "CAP: A Connectivity-Aware Hierarchical Coverage Path Planning Algorithm for Unknown Environments using Coverage Guidance Graph",
    "abstract": "           Efficient coverage of unknown environments requires robots to adapt their paths in real time based on on-board sensor data. In this paper, we introduce CAP, a connectivity-aware hierarchical coverage path planning algorithm for efficient coverage of unknown environments. During online operation, CAP incrementally constructs a coverage guidance graph to capture essential information about the environment. Based on the updated graph, the hierarchical planner determines an efficient path to maximize global coverage efficiency and minimize local coverage time. The performance of CAP is evaluated and compared with five baseline algorithms through high-fidelity simulations as well as robot experiments. Our results show that CAP yields significant improvements in coverage time, path length, and path overlap ratio.         ",
    "url": "https://arxiv.org/abs/2503.00647",
    "authors": [
      "Zongyuan Shen",
      "Burhanuddin Shirose",
      "Prasanna Sriganesh",
      "Matthew Travers"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.00665",
    "title": "Development of an Unpaired Deep Neural Network for Synthesizing X-ray Fluoroscopic Images from Digitally Reconstructed Tomography in Image Guided Radiotherapy",
    "abstract": "           Purpose The purpose of this study was to develop and evaluate a deep neural network (DNN) capable of generating flat-panel detector (FPD) images from digitally reconstructed radiography (DRR) images in lung cancer treatment, with the aim of improving clinical workflows in image-guided radiotherapy. Methods A modified CycleGAN architecture was trained on paired DRR-FPD image data obtained from patients with lung tumors. The training dataset consisted of over 400 DRR-FPD image pairs, and the final model was evaluated on an independent set of 100 FPD images. Mean absolute error (MAE), peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and Kernel Inception Distance (KID) were used to quantify the similarity between synthetic and ground-truth FPD images. Computation time for generating synthetic images was also measured. Results Despite some positional mismatches in the DRR-FPD pairs, the synthetic FPD images closely resembled the ground-truth FPD images. The proposed DNN achieved notable improvements over both input DRR images and a U-Net-based method in terms of MAE, PSNR, SSIM, and KID. The average image generation time was on the order of milliseconds per image, indicating its potential for real-time application. Qualitative evaluations showed that the DNN successfully reproduced image noise patterns akin to real FPD images, reducing the need for manual noise adjustments. Conclusions The proposed DNN effectively converted DRR images into realistic FPD images for thoracic cases, offering a fast and practical method that could streamline patient setup verification and enhance overall clinical workflow. Future work should validate the model across different imaging systems and address remaining challenges in marker visualization, thereby fostering broader clinical adoption.         ",
    "url": "https://arxiv.org/abs/2503.00665",
    "authors": [
      "Chisako Hayashi",
      "Shinichiro Mori",
      "Yasukuni Mori",
      "Lim Taehyeung",
      "Hiroki Suyari",
      "Hitoshi Ishikawa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2503.00670",
    "title": "Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly Detection in Videos",
    "abstract": "           Anomaly detection in videos is a challenging task as anomalies in different videos are of different kinds. Therefore, a promising way to approach video anomaly detection is by learning the non-anomalous nature of the video at hand. To this end, we propose a one-class few-shot learning driven transformer based approach for anomaly detection in videos that is self-context aware. Features from the first few consecutive non-anomalous frames in a video are used to train the transformer in predicting the non-anomalous feature of the subsequent frame. This takes place under the attention of a self-context learned from the input features themselves. After the learning, given a few previous frames, the video-specific transformer is used to infer if a frame is anomalous or not by comparing the feature predicted by it with the actual. The effectiveness of the proposed method with respect to the state-of-the-art is demonstrated through qualitative and quantitative results on different standard datasets. We also study the positive effect of the self-context used in our approach.         ",
    "url": "https://arxiv.org/abs/2503.00670",
    "authors": [
      "Gargi V. Pillai",
      "Ashish Verma",
      "Debashis Sen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00691",
    "title": "How Diversely Can Language Models Solve Problems? Exploring the Algorithmic Diversity of Model-Generated Code",
    "abstract": "           Language models (LMs) have exhibited impressive abilities in generating code from natural language requirements. In this work, we highlight the diversity of code generated by LMs as a critical criterion for evaluating their code generation capabilities. There is a lack of studies focused on assessing the diversity of generated code, which overlooks its importance in code LMs. Therefore, we propose a systematic approach to evaluate code diversity, introducing various metrics with inter-code similarity. Specifically, we introduce code clustering methods that leverages LMs' capabilities in code understanding and reasoning, resulting in a set of metrics that represent the number of algorithms in model-generated solutions. We extensively investigate the property of model-generated solutions by contrasting them with human-written ones and quantifying the impact of various factors on code diversity: model size, temperature, instruction tuning, and problem complexity. Our analysis demonstrates that model-generated solutions exhibit low algorithmic diversity, which was neglected by the research community. Moreover, we explore methods to increase code diversity by combining solutions from different models and increasing sampling temperatures. Our findings highlight that code diversity can be enhanced with the help of heterogeneous models and setting temperature beyond 1.0 that has not been fully explored due to the functional correctness degradation. To facilitate our research direction, we publicly share our code and datasets through open-source repositories.         ",
    "url": "https://arxiv.org/abs/2503.00691",
    "authors": [
      "Seonghyeon Lee",
      "Heejae Chon",
      "Joonwon Jang",
      "Dongha Lee",
      "Hwanjo Yu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.00703",
    "title": "Towards hyperparameter-free optimization with differential privacy",
    "abstract": "           Differential privacy (DP) is a privacy-preserving paradigm that protects the training data when training deep learning models. Critically, the performance of models is determined by the training hyperparameters, especially those of the learning rate schedule, thus requiring fine-grained hyperparameter tuning on the data. In practice, it is common to tune the learning rate hyperparameters through the grid search that (1) is computationally expensive as multiple runs are needed, and (2) increases the risk of data leakage as the selection of hyperparameters is data-dependent. In this work, we adapt the automatic learning rate schedule to DP optimization for any models and optimizers, so as to significantly mitigate or even eliminate the cost of hyperparameter tuning when applied together with automatic per-sample gradient clipping. Our hyperparameter-free DP optimization is almost as computationally efficient as the standard non-DP optimization, and achieves state-of-the-art DP performance on various language and vision tasks.         ",
    "url": "https://arxiv.org/abs/2503.00703",
    "authors": [
      "Zhiqi Bu",
      "Ruixuan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00709",
    "title": "ICanC: Improving Camera-based Object Detection and Energy Consumption in Low-Illumination Environments",
    "abstract": "           This paper introduces ICanC (pronounced \"I Can See\"), a novel system designed to enhance object detection and optimize energy efficiency in autonomous vehicles (AVs) operating in low-illumination environments. By leveraging the complementary capabilities of LiDAR and camera sensors, ICanC improves detection accuracy under conditions where camera performance typically declines, while significantly reducing unnecessary headlight usage. This approach aligns with the broader objective of promoting sustainable transportation. ICanC comprises three primary nodes: the Obstacle Detector, which processes LiDAR point cloud data to fit bounding boxes onto detected objects and estimate their position, velocity, and orientation; the Danger Detector, which evaluates potential threats using the information provided by the Obstacle Detector; and the Light Controller, which dynamically activates headlights to enhance camera visibility solely when a threat is detected. Experiments conducted in physical and simulated environments demonstrate ICanC's robust performance, even in the presence of significant noise interference. The system consistently achieves high accuracy in camera-based object detection when headlights are engaged, while significantly reducing overall headlight energy consumption. These results position ICanC as a promising advancement in autonomous vehicle research, achieving a balance between energy efficiency and reliable object detection.         ",
    "url": "https://arxiv.org/abs/2503.00709",
    "authors": [
      "Daniel Ma",
      "Ren Zhong",
      "Weisong Shi"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.00712",
    "title": "Streaming Algorithms for Network Design",
    "abstract": "           We consider the Survivable Network Design problem (SNDP) in the single-pass insertion-only streaming model. The input to SNDP is an edge-weighted graph $G = (V, E)$ and an integer connectivity requirement $r(uv)$ for each $u, v \\in V$. The objective is to find a min-weight subgraph $H \\subseteq G$ s.t., for every $u, v \\in V$, $u$ and $v$ are $r(uv)$-edge/vertex-connected. Recent work by [JKMV24] obtained approximation algorithms for edge-connectivity augmentation, and via that, also derived algorithms for edge-connectivity SNDP (EC-SNDP). We consider vertex-connectivity setting (VC-SNDP) and obtain several results for it as well as improved bounds for EC-SNDP. * We provide a general framework for solving connectivity problems in streaming; this is based on a connection to fault-tolerant spanners. For VC-SNDP we provide an $O(tk)$-approximation in $\\tilde O(k^{1-1/t}n^{1 + 1/t})$ space, where $k$ is the maximum connectivity requirement, assuming an exact algorithm at the end of the stream. Using a refined LP-based analysis, we provide an $O(\\beta t)$-approximation in polynomial time, where $\\beta$ is the best polytime approximation w.r.t. the optimal fractional solution to a natural LP relaxation. When applied to EC-SNDP, our framework provides an $O(t)$-approximation in $\\tilde O(k^{1-1/t}n^{1 + 1/t})$ space, improving the $O(t \\log k)$-approximation of [JKMV24]; this also extends to element-connectivity SNDP. * We consider vertex connectivity-augmentation in the link-arrival model. The input is a $k$-vertex-connected subgraph $G$, and the weighted links $L$ arrive in the stream; the goal is to store the min-weight set of links s.t. $G \\cup L$ is $(k+1)$-vertex-connected. We obtain $O(1)$ approximations in near-linear space for $k = 1, 2$. Our result for $k=2$ is based on SPQR tree, a novel application for this well-known representation of $2$-connected graphs.         ",
    "url": "https://arxiv.org/abs/2503.00712",
    "authors": [
      "Chandra chekuri",
      "Rhea Jain",
      "Sepideh Mahabadi",
      "Ali Vakilian"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2503.00717",
    "title": "LLMDR: LLM-Driven Deadlock Detection and Resolution in Multi-Agent Pathfinding",
    "abstract": "           Multi-Agent Pathfinding (MAPF) is a core challenge in multi-agent systems. Existing learning-based MAPF methods often struggle with scalability, particularly when addressing complex scenarios that are prone to deadlocks. To address these challenges, we introduce LLMDR (LLM-Driven Deadlock Detection and Resolution), an approach designed to resolve deadlocks and improve the performance of learnt MAPF models. LLMDR integrates the inference capabilities of large language models (LLMs) with learnt MAPF models and prioritized planning, enabling it to detect deadlocks and provide customized resolution strategies. We evaluate LLMDR on standard MAPF benchmark maps with varying agent numbers, measuring its performance when combined with several base models. The results demonstrate that LLMDR improves the performance of learnt MAPF models, particularly in deadlock-prone scenarios, with notable improvements in success rates. These findings show the potential of integrating LLMs to improve the scalability of learning-based MAPF methods. The source code for LLMDR is available at: this https URL ",
    "url": "https://arxiv.org/abs/2503.00717",
    "authors": [
      "Seungbae Seo",
      "Junghwan Kim",
      "Minjeong Shin",
      "Bongwon Suh"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00723",
    "title": "Re-Imagining Multimodal Instruction Tuning: A Representation View",
    "abstract": "           Multimodal instruction tuning has proven to be an effective strategy for achieving zero-shot generalization by fine-tuning pre-trained Large Multimodal Models (LMMs) with instruction-following data. However, as the scale of LMMs continues to grow, fully fine-tuning these models has become highly parameter-intensive. Although Parameter-Efficient Fine-Tuning (PEFT) methods have been introduced to reduce the number of tunable parameters, a significant performance gap remains compared to full fine-tuning. Furthermore, existing PEFT approaches are often highly parameterized, making them difficult to interpret and control. In light of this, we introduce Multimodal Representation Tuning (MRT), a novel approach that focuses on directly editing semantically rich multimodal representations to achieve strong performance and provide intuitive control over LMMs. Empirical results show that our method surpasses current state-of-the-art baselines with significant performance gains (e.g., 1580.40 MME score) while requiring substantially fewer tunable parameters (e.g., 0.03% parameters). Additionally, we conduct experiments on editing instrumental tokens within multimodal representations, demonstrating that direct manipulation of these representations enables simple yet effective control over network behavior.         ",
    "url": "https://arxiv.org/abs/2503.00723",
    "authors": [
      "Yiyang Liu",
      "James Chenhao Liang",
      "Ruixiang Tang",
      "Yugyung Lee",
      "Majid Rabbani",
      "Sohail Dianat",
      "Raghuveer Rao",
      "Lifu Huang",
      "Dongfang Liu",
      "Qifan Wang",
      "Cheng Han"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00724",
    "title": "Unmasking Digital Falsehoods: A Comparative Analysis of LLM-Based Misinformation Detection Strategies",
    "abstract": "           The proliferation of misinformation on social media has raised significant societal concerns, necessitating robust detection mechanisms. Large Language Models such as GPT-4 and LLaMA2 have been envisioned as possible tools for detecting misinformation based on their advanced natural language understanding and reasoning capabilities. This paper conducts a comparison of LLM-based approaches to detecting misinformation between text-based, multimodal, and agentic approaches. We evaluate the effectiveness of fine-tuned models, zero-shot learning, and systematic fact-checking mechanisms in detecting misinformation across different topic domains like public health, politics, and finance. We also discuss scalability, generalizability, and explainability of the models and recognize key challenges such as hallucination, adversarial attacks on misinformation, and computational resources. Our findings point towards the importance of hybrid approaches that pair structured verification protocols with adaptive learning techniques to enhance detection accuracy and explainability. The paper closes by suggesting potential avenues of future work, including real-time tracking of misinformation, federated learning, and cross-platform detection models.         ",
    "url": "https://arxiv.org/abs/2503.00724",
    "authors": [
      "Tianyi Huang",
      "Jingyuan Yi",
      "Peiyang Yu",
      "Xiaochuan Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.00750",
    "title": "Edge Prompt Tuning for Graph Neural Networks",
    "abstract": "           Pre-training powerful Graph Neural Networks (GNNs) with unlabeled graph data in a self-supervised manner has emerged as a prominent technique in recent years. However, inevitable objective gaps often exist between pre-training and downstream tasks. To bridge this gap, graph prompt tuning techniques design and learn graph prompts by manipulating input graphs or reframing downstream tasks as pre-training tasks without fine-tuning the pre-trained GNN models. While recent graph prompt tuning methods have proven effective in adapting pre-trained GNN models for downstream tasks, they overlook the crucial role of edges in graph prompt design, which can significantly affect the quality of graph representations for downstream tasks. In this study, we propose EdgePrompt, a simple yet effective graph prompt tuning method from the perspective of edges. Unlike previous studies that design prompt vectors on node features, EdgePrompt manipulates input graphs by learning additional prompt vectors for edges and incorporates the edge prompts through message passing in the pre-trained GNN models to better embed graph structural information for downstream tasks. Our method is compatible with prevalent GNN architectures pre-trained under various pre-training strategies and is universal for different downstream tasks. We provide comprehensive theoretical analyses of our method regarding its capability of handling node classification and graph classification as downstream tasks. Extensive experiments on ten graph datasets under four pre-training strategies demonstrate the superiority of our proposed method against six baselines. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00750",
    "authors": [
      "Xingbo Fu",
      "Yinhan He",
      "Jundong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00755",
    "title": "Riemann Tensor Neural Networks: Learning Conservative Systems with Physics-Constrained Networks",
    "abstract": "           Divergence-free symmetric tensors (DFSTs) are fundamental in continuum mechanics, encoding conservation laws such as mass and momentum conservation. We introduce Riemann Tensor Neural Networks (RTNNs), a novel neural architecture that inherently satisfies the DFST condition to machine precision, providing a strong inductive bias for enforcing these conservation laws. We prove that RTNNs can approximate any sufficiently smooth DFST with arbitrary precision and demonstrate their effectiveness as surrogates for conservative PDEs, achieving improved accuracy across benchmarks. This work is the first to use DFSTs as an inductive bias in neural PDE surrogates and to explicitly enforce the conservation of both mass and momentum within a physics-constrained neural architecture.         ",
    "url": "https://arxiv.org/abs/2503.00755",
    "authors": [
      "Anas Jnini",
      "Lorenzo Breschi",
      "Flavio Vella"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00762",
    "title": "MR-EIT: Multi-Resolution Reconstruction for Electrical Impedance Tomography via Data-Driven and Unsupervised Dual-Mode Neural Networks",
    "abstract": "           This paper presents a multi-resolution reconstruction method for Electrical Impedance Tomography (EIT), referred to as MR-EIT, which is capable of operating in both supervised and unsupervised learning modes. MR-EIT integrates an ordered feature extraction module and an unordered coordinate feature expression module. The former achieves the mapping from voltage to two-dimensional conductivity features through pre-training, while the latter realizes multi-resolution reconstruction independent of the order and size of the input sequence by utilizing symmetric functions and local feature extraction mechanisms. In the data-driven mode, MR-EIT reconstructs high-resolution images from low-resolution data of finite element meshes through two stages of pre-training and joint training, and demonstrates excellent performance in simulation experiments. In the unsupervised learning mode, MR-EIT does not require pre-training data and performs iterative optimization solely based on measured voltages to rapidly achieve image reconstruction from low to high resolution. It shows robustness to noise and efficient super-resolution reconstruction capabilities in both simulation and real water tank experiments. Experimental results indicate that MR-EIT outperforms the comparison methods in terms of Structural Similarity (SSIM) and Relative Image Error (RIE), especially in the unsupervised learning mode, where it can significantly reduce the number of iterations and improve image reconstruction quality.         ",
    "url": "https://arxiv.org/abs/2503.00762",
    "authors": [
      "Fangming Shi",
      "Jinzhen Liu",
      "Xiangqian Meng",
      "Yapeng Zhou",
      "Hui Xiong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2503.00763",
    "title": "Optimal Bilinear Equalizer Beamforming Design for Cell-Free Massive MIMO Networks with Arbitrary Channel Estimators",
    "abstract": "           This paper studies the distributed optimal bilinear equalizer (OBE) beamforming design for both the uplink and downlink cell-free massive multiple-input multiple-output networks. We consider arbitrary statistics-based channel estimators over spatially correlated Rician fading channels. In the uplink, we derive the achievable spectral efficiency (SE) performance and OBE combining schemes with arbitrary statistics-based channel estimators and compute their respective closed-form expressions. It is insightful to explore that the achievable SE performance is not dependent on the choice of channel estimator when OBE combining schemes are applied over Rayleigh channels. In the downlink, we derive the achievable SE performance expressions with BE precoding schemes and arbitrary statistics-based channel estimators utilized and compute them in closed form. Then, we obtain the OBE precoding scheme leveraging insights from uplink OBE combining schemes.         ",
    "url": "https://arxiv.org/abs/2503.00763",
    "authors": [
      "Zhe Wang",
      "Jiayi Zhang",
      "Hao Lei",
      "Dusit Niyato",
      "Bo Ai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.00782",
    "title": "Wavelet-Driven Masked Image Modeling: A Path to Efficient Visual Representation",
    "abstract": "           Masked Image Modeling (MIM) has garnered significant attention in self-supervised learning, thanks to its impressive capacity to learn scalable visual representations tailored for downstream tasks. However, images inherently contain abundant redundant information, leading the pixel-based MIM reconstruction process to focus excessively on finer details such as textures, thus prolonging training times unnecessarily. Addressing this challenge requires a shift towards a compact representation of features during MIM reconstruction. Frequency domain analysis provides a promising avenue for achieving compact image feature representation. In contrast to the commonly used Fourier transform, wavelet transform not only offers frequency information but also preserves spatial characteristics and multi-level features of the image. Additionally, the multi-level decomposition process of wavelet transformation aligns well with the hierarchical architecture of modern neural networks. In this study, we leverage wavelet transform as a tool for efficient representation learning to expedite the training process of MIM. Specifically, we conduct multi-level decomposition of images using wavelet transform, utilizing wavelet coefficients from different levels to construct distinct reconstruction targets representing various frequencies and scales. These reconstruction targets are then integrated into the MIM process, with adjustable weights assigned to prioritize the most crucial information. Extensive experiments demonstrate that our method achieves comparable or superior performance across various downstream tasks while exhibiting higher training efficiency.         ",
    "url": "https://arxiv.org/abs/2503.00782",
    "authors": [
      "Wenzhao Xiang",
      "Chang Liu",
      "Hongyang Yu",
      "Xilin Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00784",
    "title": "DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting",
    "abstract": "           Large language models (LLMs) exhibit exceptional performance across a wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents a promising draft-then-verify framework that reduces generation latency while maintaining output distribution fidelity. Nevertheless, the draft model introduces additional computational overhead, becoming a performance bottleneck and increasing the time to first token (TTFT). Previous approaches to mitigate draft model overhead have primarily relied on heuristics and generally failed to match the quality of the draft language models. To address these challenges, we propose DuoDecoding, a novel approach that strategically deploys the draft and target models on the CPU and GPU respectively, enabling parallel decoding while preserving draft quality. Our method incorporates a hardware-aware optimal draft budget to minimize idle times and employs dynamic multi-sequence drafting to enhance draft quality. Extensive experiments across seven tasks show that DuoDecoding achieves up to 2.61x speedup in generation latency, while reducing TTFT to 83% of that in conventional speculative decoding. The Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00784",
    "authors": [
      "Kai Lv",
      "Honglin Guo",
      "Qipeng Guo",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.00786",
    "title": "Graph Attention Networks Unleashed: A Fast and Explainable Vulnerability Assessment Framework for Microgrids",
    "abstract": "           Independent microgrids are crucial for supplying electricity by combining distributed energy resources and loads in scenarios like isolated islands and field combat. Fast and accurate assessments of microgrid vulnerability against intentional attacks or natural disasters are essential for effective risk prevention and design optimization. However, conventional Monte Carlo simulation (MCS) methods are computationally expensive and time-consuming, while existing machine learning-based approaches often lack accuracy and explainability. To address these challenges, this study proposes a fast and explainable vulnerability assessment framework that integrates MCS with a graph attention network enhanced by self-attention pooling (GAT-S). MCS generates training data, while the GAT-S model learns the structural and electrical characteristics of the microgrid and further assesses its vulnerability intelligently. The GAT-S improves explainability and computational efficiency by dynamically assigning attention weights to critical nodes. Comprehensive experimental evaluations across various microgrid configurations demonstrate that the proposed framework provides accurate vulnerability assessments, achieving a mean squared error as low as 0.001, real-time responsiveness within 1 second, and delivering explainable results.         ",
    "url": "https://arxiv.org/abs/2503.00786",
    "authors": [
      "Wei Liu",
      "Tao Zhang",
      "Chenhui Lin",
      "Kaiwen Li",
      "Rui Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00790",
    "title": "Acoustic Anomaly Detection on UAM Propeller Defect with Acoustic dataset for Crack of drone Propeller (ADCP)",
    "abstract": "           The imminent commercialization of UAM requires stable, AI-based maintenance systems to ensure safety for both passengers and pedestrians. This paper presents a methodology for non-destructively detecting cracks in UAM propellers using drone propeller sound datasets. Normal operating sounds were recorded, and abnormal sounds (categorized as ripped and broken) were differentiated by varying the microphone-propeller angle and throttle power. Our novel approach integrates FFT and STFT preprocessing techniques to capture both global frequency patterns and local time-frequency variations, thereby enhancing anomaly detection performance. The constructed Acoustic Dataset for Crack of Drone Propeller (ADCP) demonstrates the potential for detecting propeller cracks and lays the groundwork for future UAM maintenance applications.         ",
    "url": "https://arxiv.org/abs/2503.00790",
    "authors": [
      "Juho Lee",
      "Donghyun Yoon",
      "Gumoon Jeong",
      "Hyeoncheol Kim"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Emerging Technologies (cs.ET)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2503.00801",
    "title": "STAR-Edge: Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds",
    "abstract": "           Extracting geometric edges from unstructured point clouds remains a significant challenge, particularly in thin-walled structures that are commonly found in everyday objects. Traditional geometric methods and recent learning-based approaches frequently struggle with these structures, as both rely heavily on sufficient contextual information from local point neighborhoods. However, 3D measurement data of thin-walled structures often lack the accurate, dense, and regular neighborhood sampling required for reliable edge extraction, resulting in degraded performance. In this work, we introduce STAR-Edge, a novel approach designed for detecting and refining edge points in thin-walled structures. Our method leverages a unique representation-the local spherical curve-to create structure-aware neighborhoods that emphasize co-planar points while reducing interference from close-by, non-co-planar surfaces. This representation is transformed into a rotation-invariant descriptor, which, combined with a lightweight multi-layer perceptron, enables robust edge point classification even in the presence of noise and sparse or irregular sampling. Besides, we also use the local spherical curve representation to estimate more precise normals and introduce an optimization function to project initially identified edge points exactly on the true edges. Experiments conducted on the ABC dataset and thin-walled structure-specific datasets demonstrate that STAR-Edge outperforms existing edge detection methods, showcasing better robustness under various challenging conditions.         ",
    "url": "https://arxiv.org/abs/2503.00801",
    "authors": [
      "Zikuan Li",
      "Honghua Chen",
      "Yuecheng Wang",
      "Sibo Wu",
      "Mingqiang Wei",
      "Jun Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00813",
    "title": "HLoRA: Efficient Federated Learning System for LLM Heterogeneous Fine-Tuning",
    "abstract": "           Federated learning systems have been identified as an efficient approach to scaling distributed model training with a large amount of participants or data owners while guaranteeing data privacy. To apply the current most popular pre-trained large language models to other domains with data privacy guarantee requirements, existing works propose fine-tuning the pre-trained large language models in federated learning environments across data owners using the parameter efficient fine-tuning approaches, LoRA. To address the resource and data heterogeneous issues for the participants, previous works adopted heterogeneous LoRA using different ranks for different clients and pending their rank, which brings bias for the parameter aggregation. To address this issue, we propose HLoRA, an efficient federated learning system utilizing a modified LoRA approach that incorporates rank heterogeneity to optimize communication and computational efficiency. Experimental results, conducted using the Microsoft Research Paraphrase Corpus (MRPC), Quora Question Pairs (QQP) and Recognizing Textual Entailment (RTE), within the Plato federated learning framework, demonstrate that our method not only reduces resource demands but also outperforms traditional LoRA applications in terms of convergence speed and final model accuracy. This study shows that our approach can significantly improve the practical deployment of federated LLM fine-tuning, particularly in environments with diverse client resources.         ",
    "url": "https://arxiv.org/abs/2503.00813",
    "authors": [
      "Qianli Liu",
      "Zhaorui Zhang",
      "Xin Yao",
      "Benben Liu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2503.00814",
    "title": "PINN-MG: A physics-informed neural network for mesh generation",
    "abstract": "           In numerical simulation, structured mesh generation often requires a lot of time and manpower investment. The general scheme for structured quad mesh generation is to find a mapping between the computational domain and the physical domain. This mapping can be obtained by solving partial differential equations. However, existing structured mesh generation methods are difficult to ensure both efficiency and mesh quality. In this paper, we propose a structured mesh generation method based on physics-informed neural network, PINN-MG. It takes boundary curves as input and then utilizes an attention network to capture the potential mapping between computational and physical domains, generating structured meshes for the input physical domain. PINN-MG introduces the Navier-Lam\u00e9 equation in linear elastic as a partial differential equation term in the loss function, ensuring that the neural network conforms to the law of elastic body deformation when optimizing the loss value. The training process of PINN-MG is completely unsupervised and does not require any prior knowledge or datasets, which greatly reduces the previous workload of producing structured mesh datasets. Experimental results show that PINN-MG can generate higher quality structured quad meshes than other methods, and has the advantages of traditional algebraic methods and differential methods.         ",
    "url": "https://arxiv.org/abs/2503.00814",
    "authors": [
      "Min Wang",
      "Haisheng Li",
      "Haoxuan Zhang",
      "Xiaoqun Wu",
      "Nan Li"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2503.00816",
    "title": "Random Walks in Self-supervised Learning for Triangular Meshes",
    "abstract": "           This study addresses the challenge of self-supervised learning for 3D mesh analysis. It presents an new approach that uses random walks as a form of data augmentation to generate diverse representations of mesh surfaces. Furthermore, it employs a combination of contrastive and clustering losses. The contrastive learning framework maximizes similarity between augmented instances of the same mesh while minimizing similarity between different meshes. We integrate this with a clustering loss, enhancing class distinction across training epochs and mitigating training variance. Our model's effectiveness is evaluated using mean Average Precision (mAP) scores and a supervised SVM linear classifier on extracted features, demonstrating its potential for various downstream tasks such as object classification and shape retrieval.         ",
    "url": "https://arxiv.org/abs/2503.00816",
    "authors": [
      "Gal Yefet",
      "Ayellet Tal"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00838",
    "title": "Foundation Models Secretly Understand Neural Network Weights: Enhancing Hypernetwork Architectures with Foundation Models",
    "abstract": "           Large pre-trained models, or foundation models, have shown impressive performance when adapted to a variety of downstream tasks, often out-performing specialized models. Hypernetworks, neural networks that generate some or all of the parameters of another neural network, have become an increasingly important technique for conditioning and generalizing implicit neural representations (INRs), which represent signals or objects such as audio or 3D shapes using a neural network. However, despite the potential benefits of incorporating foundation models in hypernetwork methods, this research direction has not been investigated, likely due to the dissimilarity of the weight generation task with other visual tasks. To address this gap, we (1) show how foundation models can improve hypernetworks with Transformer-based architectures, (2) provide an empirical analysis of the benefits of foundation models for hypernetworks through the lens of the generalizable INR task, showing that leveraging foundation models improves performance, generalizability, and data efficiency across a variety of algorithms and modalities. We also provide further analysis in examining the design space of foundation model-based hypernetworks, including examining the choice of foundation models, algorithms, and the effect of scaling foundation models.         ",
    "url": "https://arxiv.org/abs/2503.00838",
    "authors": [
      "Jeffrey Gu",
      "Serena Yeung-Levy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00844",
    "title": "Impact of Surrogate Model Accuracy on Performance and Model Management Strategy in Surrogate-Assisted Evolutionary Algorithms",
    "abstract": "           Surrogate-assisted evolutionary algorithms (SAEAs) have been proposed to solve expensive optimization problems. Although SAEAs use surrogate models that approximate the evaluations of solutions using machine learning techniques, prior research has not adequately investigated the impact of surrogate model accuracy on search performance and model management strategy in SAEAs. This study analyzes how surrogate model accuracy affects search performance and model management strategies. For this purpose, we construct a pseudo-surrogate model with adjustable prediction accuracy to ensure fair comparisons across different model management strategies. We compared three model management strategies: (1) pre-selection (PS), (2) individual-based (IB), and (3) generation-based (GB) on standard benchmark problems with a baseline model that does not use surrogates. The experimental results reveal that a higher surrogate model accuracy improves the search performance. However, the impact varies according to the strategy used. Specifically, PS demonstrates a clear trend of improved performance as the estimation accuracy increases, whereas IB and GB exhibit robust performance when the accuracy surpasses a certain threshold. In model strategy comparisons, GB exhibits superior performance across a broad range of prediction accuracies, IB outperforms it at lower accuracies, and PS outperforms it at higher accuracies. The findings of this study clarify guidelines for selecting appropriate model management strategies based on the surrogate model accuracy.         ",
    "url": "https://arxiv.org/abs/2503.00844",
    "authors": [
      "Yuki Hanawa",
      "Tomohiro Harada",
      "Yukiya Miura"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.00845",
    "title": "Rewarding Graph Reasoning Process makes LLMs more Generalized Reasoners",
    "abstract": "           Despite significant advancements in Large Language Models (LLMs), developing advanced reasoning capabilities in LLMs remains a key challenge. Process Reward Models (PRMs) have demonstrated exceptional promise in enhancing reasoning by providing step-wise feedback, particularly in the context of mathematical reasoning. However, their application to broader reasoning domains remains understudied, largely due to the high costs associated with manually creating step-level supervision. In this work, we explore the potential of PRMs in graph reasoning problems - a domain that demands sophisticated multi-step reasoning and offers opportunities for automated step-level data generation using established graph algorithms. We introduce GraphSILO, the largest dataset for graph reasoning problems with fine-grained step-wise labels, built using automated Task-oriented Trajectories and Monte Carlo Tree Search (MCTS) to generate detailed reasoning steps with step-wise labels. Building upon this dataset, we train GraphPRM, the first PRM designed for graph reasoning problems, and evaluate its effectiveness in two key settings: inference-time scaling and reinforcement learning via Direct Preference Optimization (DPO). Experimental results show that GraphPRM significantly improves LLM performance across 13 graph reasoning tasks, delivering a 9% gain for Qwen2.5-7B and demonstrating transferability to new graph reasoning datasets and new reasoning domains like mathematical problem-solving. Notably, GraphPRM enhances LLM performance on GSM8K and Math500, underscoring the cross-domain applicability of graph-based reasoning rewards. Our findings highlight the potential of PRMs in advancing reasoning across diverse domains, paving the way for more versatile and effective LLMs.         ",
    "url": "https://arxiv.org/abs/2503.00845",
    "authors": [
      "Miao Peng",
      "Nuo Chen",
      "Zongrui Suo",
      "Jia Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00852",
    "title": "A Transfer Framework for Enhancing Temporal Graph Learning in Data-Scarce Settings",
    "abstract": "           Dynamic interactions between entities are prevalent in domains like social platforms, financial systems, healthcare, and e-commerce. These interactions can be effectively represented as time-evolving graphs, where predicting future connections is a key task in applications such as recommendation systems. Temporal Graph Neural Networks (TGNNs) have achieved strong results for such predictive tasks but typically require extensive training data, which is often limited in real-world scenarios. One approach to mitigating data scarcity is leveraging pre-trained models from related datasets. However, direct knowledge transfer between TGNNs is challenging due to their reliance on node-specific memory structures, making them inherently difficult to adapt across datasets. To address this, we introduce a novel transfer approach that disentangles node representations from their associated features through a structured bipartite encoding mechanism. This decoupling enables more effective transfer of memory components and other learned inductive patterns from one dataset to another. Empirical evaluations on real-world benchmarks demonstrate that our method significantly enhances TGNN performance in low-data regimes, outperforming non-transfer baselines by up to 56\\% and surpassing existing transfer strategies by 36\\%         ",
    "url": "https://arxiv.org/abs/2503.00852",
    "authors": [
      "Shubham Gupta",
      "Srikanta Bedathur"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2503.00860",
    "title": "Hierarchical graph sampling based minibatch learning with chain preservation and variance reduction",
    "abstract": "           Graph sampling based Graph Convolutional Networks (GCNs) decouple the sampling from the forward and backward propagation during minibatch training, which exhibit good scalability in terms of layer depth and graph size. We propose HIS_GCNs, a hierarchical importance graph sampling based learning method. By constructing minibatches using sampled subgraphs, HIS_GCNs gives attention to the importance of both core and periphery nodes/edges. Specifically, it preserves the centrum of the core to most minibatches, which maintains connectivity between periphery nodes, and samples periphery edges without core node interference, in order to keep more long chains composed entirely of low-degree nodes in the same minibatch. In addition, we verify the effectiveness of HIS_GCNs in reducing node embedding variance and chain information loss. Experiments on GCNs and other Graph Neural Networks (GNNs) with node classification tasks on five large-scale graphs confirm superior performance of the proposed hierarchical importance sampling method in both accuracy and training time.         ",
    "url": "https://arxiv.org/abs/2503.00860",
    "authors": [
      "Qia Hu",
      "Bo Jiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00862",
    "title": "Efficient End-to-end Visual Localization for Autonomous Driving with Decoupled BEV Neural Matching",
    "abstract": "           Accurate localization plays an important role in high-level autonomous driving systems. Conventional map matching-based localization methods solve the poses by explicitly matching map elements with sensor observations, generally sensitive to perception noise, therefore requiring costly hyper-parameter tuning. In this paper, we propose an end-to-end localization neural network which directly estimates vehicle poses from surrounding images, without explicitly matching perception results with HD maps. To ensure efficiency and interpretability, a decoupled BEV neural matching-based pose solver is proposed, which estimates poses in a differentiable sampling-based matching module. Moreover, the sampling space is hugely reduced by decoupling the feature representation affected by each DoF of poses. The experimental results demonstrate that the proposed network is capable of performing decimeter level localization with mean absolute errors of 0.19m, 0.13m and 0.39 degree in longitudinal, lateral position and yaw angle while exhibiting a 68.8% reduction in inference memory usage.         ",
    "url": "https://arxiv.org/abs/2503.00862",
    "authors": [
      "Jinyu Miao",
      "Tuopu Wen",
      "Ziang Luo",
      "Kangan Qian",
      "Zheng Fu",
      "Yunlong Wang",
      "Kun Jiang",
      "Mengmeng Yang",
      "Jin Huang",
      "Zhihua Zhong",
      "Diange Yang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.00870",
    "title": "NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks In Open Domains",
    "abstract": "           We explore neuro-symbolic approaches to generalize actionable knowledge, enabling embodied agents to tackle complex tasks more effectively in open-domain environments. A key challenge for embodied agents is the generalization of knowledge across diverse environments and situations, as limited experiences often confine them to their prior knowledge. To address this issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual learner that emulates the hypothetico-deductive model by continually formulating and validating knowledge from limited experiences through the combined use of Large Language Models (LLMs) and symbolic tools. Specifically, we devise a contrastive generality improvement scheme within NeSyC, which iteratively generates hypotheses using LLMs and conducts contrastive validation via symbolic tools. This scheme reinforces the justification for admissible actions while minimizing the inference of inadmissible ones. Additionally, we incorporate a memory-based monitoring scheme that efficiently detects action errors and triggers the knowledge refinement process across domains. Experiments conducted on diverse embodied task benchmarks-including ALFWorld, VirtualHome, Minecraft, RLBench, and a real-world robotic scenario-demonstrate that NeSyC is highly effective in solving complex embodied tasks across a range of open-domain environments.         ",
    "url": "https://arxiv.org/abs/2503.00870",
    "authors": [
      "Wonje Choi",
      "Jinwoo Park",
      "Sanghyun Ahn",
      "Daehee Lee",
      "Honguk Woo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00871",
    "title": "CyberCScope: Mining Skewed Tensor Streams and Online Anomaly Detection in Cybersecurity Systems",
    "abstract": "           Cybersecurity systems are continuously producing a huge number of time-stamped events in the form of high-order tensors, such as {count; time, port, flow duration, packet size, . . . }, and so how can we detect anomalies/intrusions in real time? How can we identify multiple types of intrusions and capture their characteristic behaviors? The tensor data consists of categorical and continuous attributes and the data distributions of continuous attributes typically exhibit skew. These data properties require handling skewed infinite and finite dimensional spaces simultaneously. In this paper, we propose a novel streaming method, namely CyberCScope. The method effectively decomposes incoming tensors into major trends while explicitly distinguishing between categorical and skewed continuous attributes. To our knowledge, it is the first to compute hybrid skewed infinite and finite dimensional decomposition. Based on this decomposition, it streamingly finds distinct time-evolving patterns, enabling the detection of multiple types of anomalies. Extensive experiments on large-scale real datasets demonstrate that CyberCScope detects various intrusions with higher accuracy than state-of-the-art baselines while providing meaningful summaries for the intrusions that occur in practice.         ",
    "url": "https://arxiv.org/abs/2503.00871",
    "authors": [
      "Kota Nakamura",
      "Koki Kawabata",
      "Shungo Tanaka",
      "Yasuko Matsubara",
      "Yasushi Sakurai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.00876",
    "title": "Improve Representation for Imbalanced Regression through Geometric Constraints",
    "abstract": "           In representation learning, uniformity refers to the uniform feature distribution in the latent space (i.e., unit hypersphere). Previous work has shown that improving uniformity contributes to the learning of under-represented classes. However, most of the previous work focused on classification; the representation space of imbalanced regression remains unexplored. Classification-based methods are not suitable for regression tasks because they cluster features into distinct groups without considering the continuous and ordered nature essential for regression. In a geometric aspect, we uniquely focus on ensuring uniformity in the latent space for imbalanced regression through two key losses: enveloping and homogeneity. The enveloping loss encourages the induced trace to uniformly occupy the surface of a hypersphere, while the homogeneity loss ensures smoothness, with representations evenly spaced at consistent intervals. Our method integrates these geometric principles into the data representations via a Surrogate-driven Representation Learning (SRL) framework. Experiments with real-world regression and operator learning tasks highlight the importance of uniformity in imbalanced regression and validate the efficacy of our geometry-based loss functions.         ",
    "url": "https://arxiv.org/abs/2503.00876",
    "authors": [
      "Zijian Dong",
      "Yilei Wu",
      "Chongyao Chen",
      "Yingtian Zou",
      "Yichi Zhang",
      "Juan Helen Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00885",
    "title": "Social Welfare Maximization in Approval-Based Committee Voting under Uncertainty",
    "abstract": "           Approval voting is widely used for making multi-winner voting decisions. The canonical rule (also called Approval Voting) used in the setting aims to maximize social welfare by selecting candidates with the highest number of approvals. We revisit approval-based multi-winner voting in scenarios where the information regarding the voters' preferences is uncertain. We present several algorithmic results for problems related to social welfare maximization under uncertainty, including computing an outcome that is social welfare maximizing with the highest probability, computing the social welfare probability distribution of a given outcome, computing the probability that a given outcome is social welfare maximizing, and understanding how robust an outcome is with respect to social welfare maximizing.         ",
    "url": "https://arxiv.org/abs/2503.00885",
    "authors": [
      "Haris Aziz",
      "Yuhang Guo",
      "Venkateswara Rao Kagita",
      "Baharak Rastegari",
      "Mashbat Suzuki"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2503.00898",
    "title": "Range and Angle Estimation with Spiking Neural Resonators for FMCW Radar",
    "abstract": "           Automotive radar systems face the challenge of managing high sampling rates and large data bandwidth while complying with stringent real-time and energy efficiency requirements. The growing complexity of autonomous vehicles further intensifies these requirements. Neuromorphic computing offers promising solutions because of its inherent energy efficiency and parallel processing capacity. This research presents a novel spiking neuron model for signal processing of frequency-modulated continuous wave (FMCW) radars that outperforms the state-of-the-art spectrum analysis algorithms in latency and data bandwidth. These spiking neural resonators are based on the resonate-and-fire neuron model and optimized to dynamically process raw radar data while simultaneously emitting an output in the form of spikes. We designed the first neuromorphic neural network consisting of these spiking neural resonators that estimates range and angle from FMCW radar data. We evaluated the range-angle maps on simulated datasets covering multiple scenarios and compared the results with a state-of-the-art pipeline for radar processing. The proposed neuron model significantly reduces the processing latency compared to traditional frequency analysis algorithms, such as the Fourier transformation (FT), which needs to sample and store entire data frames before processing. The evaluations demonstrate that these spiking neural resonators achieve state-of-the-art detection accuracy while emitting spikes simultaneously to processing and transmitting only 0.02 % of the data compared to a float-32 FT. The results showcase the potential for neuromorphic signal processing for FMCW radar systems and pave the way for designing neuromorphic radar sensors.         ",
    "url": "https://arxiv.org/abs/2503.00898",
    "authors": [
      "Nico Reeb",
      "Javier Lopez-Randulfe",
      "Robin Dietrich",
      "Alois C. Knoll"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.00905",
    "title": "DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared Imaging",
    "abstract": "           Thermal imaging is often compromised by dynamic, complex degradations caused by hardware limitations and unpredictable environmental factors. The scarcity of high-quality infrared data, coupled with the challenges of dynamic, intricate degradations, makes it difficult to recover details using existing methods. In this paper, we introduce thermal degradation simulation integrated into the training process via a mini-max optimization, by modeling these degraded factors as adversarial attacks on thermal images. The simulation is dynamic to maximize objective functions, thus capturing a broad spectrum of degraded data distributions. This approach enables training with limited data, thereby improving model this http URL, we introduce a dual-interaction network that combines the benefits of spiking neural networks with scale transformation to capture degraded features with sharp spike signal intensities. This architecture ensures compact model parameters while preserving efficient feature representation. Extensive experiments demonstrate that our method not only achieves superior visual quality under diverse single and composited degradation, but also delivers a significant reduction in processing when trained on only fifty clear images, outperforming existing techniques in efficiency and accuracy. The source code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00905",
    "authors": [
      "Zhu Liu",
      "Zijun Wang",
      "Jinyuan Liu",
      "Fanqi Meng",
      "Long Ma",
      "Risheng Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00917",
    "title": "AMUN: Adversarial Machine UNlearning",
    "abstract": "           Machine unlearning, where users can request the deletion of a forget dataset, is becoming increasingly important because of numerous privacy regulations. Initial works on ``exact'' unlearning (e.g., retraining) incur large computational overheads. However, while computationally inexpensive, ``approximate'' methods have fallen short of reaching the effectiveness of exact unlearning: models produced fail to obtain comparable accuracy and prediction confidence on both the forget and test (i.e., unseen) dataset. Exploiting this observation, we propose a new unlearning method, Adversarial Machine UNlearning (AMUN), that outperforms prior state-of-the-art (SOTA) methods for image classification. AMUN lowers the confidence of the model on the forget samples by fine-tuning the model on their corresponding adversarial examples. Adversarial examples naturally belong to the distribution imposed by the model on the input space; fine-tuning the model on the adversarial examples closest to the corresponding forget samples (a) localizes the changes to the decision boundary of the model around each forget sample and (b) avoids drastic changes to the global behavior of the model, thereby preserving the model's accuracy on test samples. Using AMUN for unlearning a random $10\\%$ of CIFAR-10 samples, we observe that even SOTA membership inference attacks cannot do better than random guessing.         ",
    "url": "https://arxiv.org/abs/2503.00917",
    "authors": [
      "Ali Ebrahimpour-Boroojeny",
      "Hari Sundaram",
      "Varun Chandrasekaran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.00923",
    "title": "HWC-Loco: A Hierarchical Whole-Body Control Approach to Robust Humanoid Locomotion",
    "abstract": "           Humanoid robots, capable of assuming human roles in various workplaces, have become essential to the advancement of embodied intelligence. However, as robots with complex physical structures, learning a control model that can operate robustly across diverse environments remains inherently challenging, particularly under the discrepancies between training and deployment environments. In this study, we propose HWC-Loco, a robust whole-body control algorithm tailored for humanoid locomotion tasks. By reformulating policy learning as a robust optimization problem, HWC-Loco explicitly learns to recover from safety-critical scenarios. While prioritizing safety guarantees, overly conservative behavior can compromise the robot's ability to complete the given tasks. To tackle this challenge, HWC-Loco leverages a hierarchical policy for robust control. This policy can dynamically resolve the trade-off between goal-tracking and safety recovery, guided by human behavior norms and dynamic constraints. To evaluate the performance of HWC-Loco, we conduct extensive comparisons against state-of-the-art humanoid control models, demonstrating HWC-Loco's superior performance across diverse terrains, robot structures, and locomotion tasks under both simulated and real-world environments.         ",
    "url": "https://arxiv.org/abs/2503.00923",
    "authors": [
      "Sixu Lin",
      "Guanren Qiao",
      "Yunxin Tai",
      "Ang Li",
      "Kui Jia",
      "Guiliang Liu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.00925",
    "title": "Explainable Classifier for Malignant Lymphoma Subtyping via Cell Graph and Image Fusion",
    "abstract": "           Malignant lymphoma subtype classification directly impacts treatment strategies and patient outcomes, necessitating classification models that achieve both high accuracy and sufficient explainability. This study proposes a novel explainable Multi-Instance Learning (MIL) framework that identifies subtype-specific Regions of Interest (ROIs) from Whole Slide Images (WSIs) while integrating cell distribution characteristics and image information. Our framework simultaneously addresses three objectives: (1) indicating appropriate ROIs for each subtype, (2) explaining the frequency and spatial distribution of characteristic cell types, and (3) achieving high-accuracy subtyping by leveraging both image and cell-distribution modalities. The proposed method fuses cell graph and image features extracted from each patch in the WSI using a Mixture-of-Experts (MoE) approach and classifies subtypes within an MIL framework. Experiments on a dataset of 1,233 WSIs demonstrate that our approach achieves state-of-the-art accuracy among ten comparative methods and provides region-level and cell-level explanations that align with a pathologist's perspectives.         ",
    "url": "https://arxiv.org/abs/2503.00925",
    "authors": [
      "Daiki Nishiyama",
      "Hiroaki Miyoshi",
      "Noriaki Hashimoto",
      "Koichi Ohshima",
      "Hidekata Hontani",
      "Ichiro Takeuchi",
      "Jun Sakuma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00932",
    "title": "Improving the Transferability of Adversarial Attacks by an Input Transpose",
    "abstract": "           Deep neural networks (DNNs) are highly susceptible to adversarial examples--subtle perturbations applied to inputs that are often imperceptible to humans yet lead to incorrect model predictions. In black-box scenarios, however, existing adversarial examples exhibit limited transferability and struggle to effectively compromise multiple unseen DNN models. Previous strategies enhance the cross-model generalization of adversarial examples by introducing versatility into adversarial perturbations, thereby improving transferability. However, further refining perturbation versatility often demands intricate algorithm development and substantial computation consumption. In this work, we propose an input transpose method that requires almost no additional labor and computation costs but can significantly improve the transferability of existing adversarial strategies. Even without adding adversarial perturbations, our method demonstrates considerable effectiveness in cross-model attacks. Our exploration finds that on specific datasets, a mere $1^\\circ$ left or right rotation might be sufficient for most adversarial examples to deceive unseen models. Our further analysis suggests that this transferability improvement triggered by rotating only $1^\\circ$ may stem from visible pattern shifts in the DNN's low-level feature maps. Moreover, this transferability exhibits optimal angles that, when identified under unrestricted query conditions, could potentially yield even greater performance.         ",
    "url": "https://arxiv.org/abs/2503.00932",
    "authors": [
      "Qing Wan",
      "Shilong Deng",
      "Xun Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00943",
    "title": "A Fully Self-Synchronized Control for Hybrid Series-Parallel Electronized Power Networks",
    "abstract": "           The hybrid series-parallel system is the final form of the power electronics-enabled power system, which combines the advantages of both series and parallel connections. Although self-synchronization of parallel-type and series-type systems is well known, self-synchronization of hybrid systems remains unrevealed. To fill in this gap, a fully self-synchronized control for hybrid series-parallel system is proposed in this paper. Based on the self-synchronization mechanism of power angle in parallel-type system and power factor angle in series-type system, a decentralized control strategy by integration of power droop and power factor angle droop can realize self-synchronization and power balancing of each module in the hybrid system.         ",
    "url": "https://arxiv.org/abs/2503.00943",
    "authors": [
      "Zexiong Wei",
      "Yao Sun",
      "Xiaochao Hou",
      "Mei Su"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.00957",
    "title": "Exploiting Vulnerabilities in Speech Translation Systems through Targeted Adversarial Attacks",
    "abstract": "           As speech translation (ST) systems become increasingly prevalent, understanding their vulnerabilities is crucial for ensuring robust and reliable communication. However, limited work has explored this issue in depth. This paper explores methods of compromising these systems through imperceptible audio manipulations. Specifically, we present two innovative approaches: (1) the injection of perturbation into source audio, and (2) the generation of adversarial music designed to guide targeted translation, while also conducting more practical over-the-air attacks in the physical world. Our experiments reveal that carefully crafted audio perturbations can mislead translation models to produce targeted, harmful outputs, while adversarial music achieve this goal more covertly, exploiting the natural imperceptibility of music. These attacks prove effective across multiple languages and translation models, highlighting a systemic vulnerability in current ST architectures. The implications of this research extend beyond immediate security concerns, shedding light on the interpretability and robustness of neural speech processing systems. Our findings underscore the need for advanced defense mechanisms and more resilient architectures in the realm of audio systems. More details and samples can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00957",
    "authors": [
      "Chang Liu",
      "Haolin Wu",
      "Xi Yang",
      "Kui Zhang",
      "Cong Wu",
      "Weiming Zhang",
      "Nenghai Yu",
      "Tianwei Zhang",
      "Qing Guo",
      "Jie Zhang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2503.00961",
    "title": "CAGN-GAT Fusion: A Hybrid Contrastive Attentive Graph Neural Network for Network Intrusion Detection",
    "abstract": "           Cybersecurity threats are growing, making network intrusion detection essential. Traditional machine learning models remain effective in resource-limited environments due to their efficiency, requiring fewer parameters and less computational time. However, handling short and highly imbalanced datasets remains challenging. In this study, we propose the fusion of a Contrastive Attentive Graph Network and Graph Attention Network (CAGN-GAT Fusion) and benchmark it against 15 other models, including both Graph Neural Networks (GNNs) and traditional ML models. Our evaluation is conducted on four benchmark datasets (KDD-CUP-1999, NSL-KDD, UNSW-NB15, and CICIDS2017) using a short and proportionally imbalanced dataset with a constant size of 5000 samples to ensure fairness in comparison. Results show that CAGN-GAT Fusion demonstrates stable and competitive accuracy, recall, and F1-score, even though it does not achieve the highest performance in every dataset. Our analysis also highlights the impact of adaptive graph construction techniques, including small changes in connections (edge perturbation) and selective hiding of features (feature masking), improving detection performance. The findings confirm that GNNs, particularly CAGN-GAT Fusion, are robust and computationally efficient, making them well-suited for resource-constrained environments. Future work will explore GraphSAGE layers and multiview graph construction techniques to further enhance adaptability and detection accuracy.         ",
    "url": "https://arxiv.org/abs/2503.00961",
    "authors": [
      "Md Abrar Jahin",
      "Shahriar Soudeep",
      "M. F. Mridha",
      "Raihan Kabir",
      "Md Rashedul Islam",
      "Yutaka Watanobe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00986",
    "title": "Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning",
    "abstract": "           In egocentric video understanding, the motion of hands and objects as well as their interactions play a significant role by nature. However, existing egocentric video representation learning methods mainly focus on aligning video representation with high-level narrations, overlooking the intricate dynamics between hands and objects. In this work, we aim to integrate the modeling of fine-grained hand-object dynamics into the video representation learning process. Since no suitable data is available, we introduce HOD, a novel pipeline employing a hand-object detector and a large language model to generate high-quality narrations with detailed descriptions of hand-object dynamics. To learn these fine-grained dynamics, we propose EgoVideo, a model with a new lightweight motion adapter to capture fine-grained hand-object motion information. Through our co-training strategy, EgoVideo effectively and efficiently leverages the fine-grained hand-object dynamics in the HOD data. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple egocentric downstream tasks, including improvements of 6.3% in EK-100 multi-instance retrieval, 5.7% in EK-100 classification, and 16.3% in EGTEA classification in zero-shot settings. Furthermore, our model exhibits robust generalization capabilities in hand-object interaction and robot manipulation tasks. Code and data are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00986",
    "authors": [
      "Baoqi Pei",
      "Yifei Huang",
      "Jilan Xu",
      "Guo Chen",
      "Yuping He",
      "Lijin Yang",
      "Yali Wang",
      "Weidi Xie",
      "Yu Qiao",
      "Fei Wu",
      "Limin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01000",
    "title": "Privacy vs. Profit: The Impact of Google's Manifest Version 3 (MV3) Update on Ad Blocker Effectiveness",
    "abstract": "           Google's recent update to the manifest file for Chrome browser extensions-transitioning from manifest version 2 (MV2) to manifest version 3 (MV3)-has raised concerns among users and ad blocker providers, who worry that the new restrictions, notably the shift from the powerful WebRequest API to the more restrictive DeclarativeNetRequest API, might reduce ad blocker effectiveness. Because ad blockers play a vital role for millions of users seeking a more private and ad-free browsing experience, this study empirically investigates how the MV3 update affects their ability to block ads and trackers. Through a browser-based experiment conducted across multiple samples of ad-supported websites, we compare the MV3 to MV2 instances of four widely used ad blockers. Our results reveal no statistically significant reduction in ad-blocking or anti-tracking effectiveness for MV3 ad blockers compared to their MV2 counterparts, and in some cases, MV3 instances even exhibit slight improvements in blocking trackers. These findings are reassuring for users, indicating that the MV3 instances of popular ad blockers continue to provide effective protection against intrusive ads and privacy-infringing trackers. While some uncertainties remain, ad blocker providers appear to have successfully navigated the MV3 update, finding solutions that maintain the core functionality of their ad blockers.         ",
    "url": "https://arxiv.org/abs/2503.01000",
    "authors": [
      "Karlo Lukic",
      "Lazaros Papadopoulos"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "General Economics (econ.GN)"
    ]
  },
  {
    "id": "arXiv:2503.01001",
    "title": "Towards An Efficient LLM Training Paradigm for CTR Prediction",
    "abstract": "           Large Language Models (LLMs) have demonstrated tremendous potential as the next-generation ranking-based recommendation system. Many recent works have shown that LLMs can significantly outperform conventional click-through-rate (CTR) prediction approaches. Despite such promising results, the computational inefficiency inherent in the current training paradigm makes it particularly challenging to train LLMs for ranking-based recommendation tasks on large datasets. To train LLMs for CTR prediction, most existing studies adopt the prevalent ''sliding-window'' paradigm. Given a sequence of $m$ user interactions, a unique training prompt is constructed for each interaction by designating it as the prediction target along with its preceding $n$ interactions serving as context. In turn, the sliding-window paradigm results in an overall complexity of $O(mn^2)$ that scales linearly with the length of user interactions. Consequently, a direct adoption to train LLMs with such strategy can result in prohibitively high training costs as the length of interactions grows. To alleviate the computational inefficiency, we propose a novel training paradigm, namely Dynamic Target Isolation (DTI), that structurally parallelizes the training of $k$ (where $k >> 1$) target interactions. Furthermore, we identify two major bottlenecks - hidden-state leakage and positional bias overfitting - that limit DTI to only scale up to a small value of $k$ (e.g., 5) then propose a computationally light solution to effectively tackle each. Through extensive experiments on three widely adopted public CTR datasets, we empirically show that DTI reduces training time by an average of $\\textbf{92%}$ (e.g., from $70.5$ hrs to $5.31$ hrs), without compromising CTR prediction performance.         ",
    "url": "https://arxiv.org/abs/2503.01001",
    "authors": [
      "Allen Lin",
      "Renqin Cai",
      "Yun He",
      "Hanchao Yu",
      "Jing Qian",
      "Rui Li",
      "Qifan Wang",
      "James Caverlee"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.01013",
    "title": "Explainable Multi-modal Time Series Prediction with LLM-in-the-Loop",
    "abstract": "           Time series analysis provides essential insights for real-world system dynamics and informs downstream decision-making, yet most existing methods often overlook the rich contextual signals present in auxiliary modalities. To bridge this gap, we introduce TimeXL, a multi-modal prediction framework that integrates a prototype-based time series encoder with three collaborating Large Language Models (LLMs) to deliver more accurate predictions and interpretable explanations. First, a multi-modal prototype-based encoder processes both time series and textual inputs to generate preliminary forecasts alongside case-based rationales. These outputs then feed into a prediction LLM, which refines the forecasts by reasoning over the encoder's predictions and explanations. Next, a reflection LLM compares the predicted values against the ground truth, identifying textual inconsistencies or noise. Guided by this feedback, a refinement LLM iteratively enhances text quality and triggers encoder retraining. This closed-loop workflow -- prediction, critique (reflect), and refinement -- continuously boosts the framework's performance and interpretability. Empirical evaluations on four real-world datasets demonstrate that TimeXL achieves up to 8.9\\% improvement in AUC and produces human-centric, multi-modal explanations, highlighting the power of LLM-driven reasoning for time series prediction.         ",
    "url": "https://arxiv.org/abs/2503.01013",
    "authors": [
      "Yushan Jiang",
      "Wenchao Yu",
      "Geon Lee",
      "Dongjin Song",
      "Kijung Shin",
      "Wei Cheng",
      "Yanchi Liu",
      "Haifeng Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01020",
    "title": "Delving into Out-of-Distribution Detection with Medical Vision-Language Models",
    "abstract": "           Recent advances in medical vision-language models (VLMs) demonstrate impressive performance in image classification tasks, driven by their strong zero-shot generalization capabilities. However, given the high variability and complexity inherent in medical imaging data, the ability of these models to detect out-of-distribution (OOD) data in this domain remains underexplored. In this work, we conduct the first systematic investigation into the OOD detection potential of medical VLMs. We evaluate state-of-the-art VLM-based OOD detection methods across a diverse set of medical VLMs, including both general and domain-specific purposes. To accurately reflect real-world challenges, we introduce a cross-modality evaluation pipeline for benchmarking full-spectrum OOD detection, rigorously assessing model robustness against both semantic shifts and covariate shifts. Furthermore, we propose a novel hierarchical prompt-based method that significantly enhances OOD detection performance. Extensive experiments are conducted to validate the effectiveness of our approach. The codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.01020",
    "authors": [
      "Lie Ju",
      "Sijin Zhou",
      "Yukun Zhou",
      "Huimin Lu",
      "Zhuoting Zhu",
      "Pearse A. Keane",
      "Zongyuan Ge"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01030",
    "title": "Language Models Predict Empathy Gaps Between Social In-groups and Out-groups",
    "abstract": "           Studies of human psychology have demonstrated that people are more motivated to extend empathy to in-group members than out-group members (Cikara et al., 2011). In this study, we investigate how this aspect of intergroup relations in humans is replicated by LLMs in an emotion intensity prediction task. In this task, the LLM is given a short description of an experience a person had that caused them to feel a particular emotion; the LLM is then prompted to predict the intensity of the emotion the person experienced on a numerical scale. By manipulating the group identities assigned to the LLM's persona (the \"perceiver\") and the person in the narrative (the \"experiencer\"), we measure how predicted emotion intensities differ between in-group and out-group settings. We observe that LLMs assign higher emotion intensity scores to in-group members than out-group members. This pattern holds across all three types of social groupings we tested: race/ethnicity, nationality, and religion. We perform an in-depth analysis on Llama-3.1-8B, the model which exhibited strongest intergroup bias among those tested.         ",
    "url": "https://arxiv.org/abs/2503.01030",
    "authors": [
      "Yu Hou",
      "Hal Daum\u00e9 III",
      "Rachel Rudinger"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01031",
    "title": "Can We Find the Code? An Empirical Study of Google Scholar's Code Retrieval",
    "abstract": "           Academic codes associated with research papers are valuable resources for scholars. In specialized fields outside computer science, code availability is often limited, making effective code retrieval essential. Google Scholar is a crucial academic search tool. If a code published in the paper is not retrievable via Google Scholar, its accessibility and impact are significantly reduced. This study takes the term \"accelerated degradation\" combined with \"reliability\" as an example, and finds that, for papers published by Elsevier, only GitHub links included in abstracts are comprehensively retrieved by Google Scholar. When such links appear within the main body of a paper, even in the \"Data Availability\" section, they may be ignored and become unsearchable. These findings highlight the importance of strategically placing GitHub links in abstracts to enhance code discoverability on Google Scholar.         ",
    "url": "https://arxiv.org/abs/2503.01031",
    "authors": [
      "Shi-Shun Chen"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.01037",
    "title": "A Comparison of Object Detection and Phrase Grounding Models in Chest X-ray Abnormality Localization using Eye-tracking Data",
    "abstract": "           Chest diseases rank among the most prevalent and dangerous global health issues. Object detection and phrase grounding deep learning models interpret complex radiology data to assist healthcare professionals in diagnosis. Object detection locates abnormalities for classes, while phrase grounding locates abnormalities for textual descriptions. This paper investigates how text enhances abnormality localization in chest X-rays by comparing the performance and explainability of these two tasks. To establish an explainability baseline, we proposed an automatic pipeline to generate image regions for report sentences using radiologists' eye-tracking data. The better performance - mIoU = 0.36 vs. 0.20 - and explainability - Containment ratio 0.48 vs. 0.26 - of the phrase grounding model infers the effectiveness of text in enhancing chest X-ray abnormality localization.         ",
    "url": "https://arxiv.org/abs/2503.01037",
    "authors": [
      "Elham Ghelichkhan",
      "Tolga Tasdizen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01038",
    "title": "Analyzing Social Media Engagement of Computer Science Conferences",
    "abstract": "           Context: X, formerly known as Twitter, is one of the largest social media platforms and has been widely used for communication during research conferences. While previous studies have examined how users engage with X during these events, limited research has focused on analyzing the content posted by computer science conferences. Objective: This study investigates how conferences from different areas of computer science perform on social media by analyzing their activity, follower engagement, and the content posted on X. Method: We collect posts from 22 computer science conferences and conduct statistical experiments to identify variations in content. Additionally, we perform a manual analysis of the top five posts for each engagement metric. Results: Our findings indicate statistically significant differences in category, sentiment, and post length across computer science conference posts. Among all engagement metrics, likes were the most common way users interacted with conference content. Conclusion: This study provides insights into the social media presence of computer science conferences, highlighting key differences in content, sentiment, and engagement patterns across different venues.         ",
    "url": "https://arxiv.org/abs/2503.01038",
    "authors": [
      "Rey Ortiz",
      "Sharif Ahmed",
      "Priscilla Salas",
      "Nasir U Eisty"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.01079",
    "title": "Depth-Adaptive Graph Neural Networks via Learnable Bakry-'Emery Curvature",
    "abstract": "           Graph Neural Networks (GNNs) have demonstrated strong representation learning capabilities for graph-based tasks. Recent advances on GNNs leverage geometric properties, such as curvature, to enhance its representation capabilities by modeling complex connectivity patterns and information flow within graphs. However, most existing approaches focus solely on discrete graph topology, overlooking diffusion dynamics and task-specific dependencies essential for effective learning. To address this, we propose integrating Bakry-\u00c9mery curvature, which captures both structural and task-driven aspects of information propagation. We develop an efficient, learnable approximation strategy, making curvature computation scalable for large graphs. Furthermore, we introduce an adaptive depth mechanism that dynamically adjusts message-passing layers per vertex based on its curvature, ensuring efficient propagation. Our theoretical analysis establishes a link between curvature and feature distinctiveness, showing that high-curvature vertices require fewer layers, while low-curvature ones benefit from deeper propagation. Extensive experiments on benchmark datasets validate the effectiveness of our approach, showing consistent performance improvements across diverse graph learning tasks.         ",
    "url": "https://arxiv.org/abs/2503.01079",
    "authors": [
      "Asela Hevapathige",
      "Ahad N. Zehmakan",
      "Qing Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01082",
    "title": "Efficient or Powerful? Trade-offs Between Machine Learning and Deep Learning for Mental Illness Detection on Social Media",
    "abstract": "           Social media platforms provide valuable insights into mental health trends by capturing user-generated discussions on conditions such as depression, anxiety, and suicidal ideation. Machine learning (ML) and deep learning (DL) models have been increasingly applied to classify mental health conditions from textual data, but selecting the most effective model involves trade-offs in accuracy, interpretability, and computational efficiency. This study evaluates multiple ML models, including logistic regression, random forest, and LightGBM, alongside deep learning architectures such as ALBERT and Gated Recurrent Units (GRUs), for both binary and multi-class classification of mental health conditions. Our findings indicate that ML and DL models achieve comparable classification performance on medium-sized datasets, with ML models offering greater interpretability through variable importance scores, while DL models are more robust to complex linguistic patterns. Additionally, ML models require explicit feature engineering, whereas DL models learn hierarchical representations directly from text. Logistic regression provides the advantage of capturing both positive and negative associations between features and mental health conditions, whereas tree-based models prioritize decision-making power through split-based feature selection. This study offers empirical insights into the advantages and limitations of different modeling approaches and provides recommendations for selecting appropriate methods based on dataset size, interpretability needs, and computational constraints.         ",
    "url": "https://arxiv.org/abs/2503.01082",
    "authors": [
      "Zhanyi Ding",
      "Zhongyan Wang",
      "Yeyubei Zhang",
      "Yuchen Cao",
      "Yunchong Liu",
      "Xiaorui Shen",
      "Yexin Tian",
      "Jianglai Dai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.01085",
    "title": "Identity documents recognition and detection using semantic segmentation with convolutional neural network",
    "abstract": "           Object recognition and detection are well-studied problems with a developed set of almost standard solutions. Identity documents recognition, classification, detection, and localization are the tasks required in a number of applications, particularly, in physical access control security systems at critical infrastructure premises. In this paper, we propose the new original architecture of a model based on an artificial convolutional neural network and semantic segmentation approach for the recognition and detection of identity documents in images. The challenge with the processing of such images is the limited computational performance and the limited amount of memory when such an application is running on industrial oneboard microcomputer hardware. The aim of this research is to prove the feasibility of the proposed technique and to obtain quality metrics. The methodology of the research is to evaluate the deep learning detection model trained on the mobile identity document video dataset. The dataset contains five hundred video clips for fifty different identity document types. The numerical results from simulations are used to evaluate the quality metrics. We present the results as accuracy versus threshold of the intersection over union value. The paper reports an accuracy above 0.75 for the intersection over union (IoU) threshold value of 0.8. Besides, we assessed the size of the model and proved the feasibility of running the model on an industrial one-board microcomputer or smartphone hardware.         ",
    "url": "https://arxiv.org/abs/2503.01085",
    "authors": [
      "Mykola Kozlenko",
      "Volodymyr Sendetskyi",
      "Oleksiy Simkiv",
      "Nazar Savchenko",
      "Andy Bosyi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01098",
    "title": "SolBench: A Dataset and Benchmark for Evaluating Functional Correctness in Solidity Code Completion and Repair",
    "abstract": "           Smart contracts are crucial programs on blockchains, and their immutability post-deployment makes functional correctness vital. Despite progress in code completion models, benchmarks for Solidity, the primary smart contract language, are lacking. Existing metrics like BLEU do not adequately assess the functional correctness of generated smart contracts. To fill this gap, we introduce SolBench, a benchmark for evaluating the functional correctness of Solidity smart contracts generated by code completion models. SolBench includes 4,178 functions from 1,155 Ethereum-deployed contracts. Testing advanced models revealed challenges in generating correct code without context, as Solidity functions rely on context-defined variables and interfaces. To address this, we propose a Retrieval-Augmented Code Repair framework. In this framework, an executor verifies functional correctness, and if necessary, an LLM repairs the code using retrieved snippets informed by executor traces. We conduct a comprehensive evaluation of both closed-source and open-source LLMs across various model sizes and series to assess their performance in smart contract completion. The results show that code repair and retrieval techniques effectively enhance the correctness of smart contract completion while reducing computational costs.         ",
    "url": "https://arxiv.org/abs/2503.01098",
    "authors": [
      "Zaoyu Chen",
      "Haoran Qin",
      "Nuo Chen",
      "Xiangyu Zhao",
      "Lei Xue",
      "Xiapu Luo",
      "Xiao-Ming Wu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.01100",
    "title": "Fence Theorem: Preprocessing is Dual-Objective Semantic Structure Isolator in 3D Anomaly Detection",
    "abstract": "           3D anomaly detection (AD) is prominent but difficult due to lacking a unified theoretical foundation for preprocessing design. We establish the Fence Theorem, formalizing preprocessing as a dual-objective semantic isolator: (1) mitigating cross-semantic interference to the greatest extent feasible and (2) confining anomaly judgments to aligned semantic spaces wherever viable, thereby establishing intra-semantic comparability. Any preprocessing approach achieves this goal through a two-stage process of Emantic-Division and Spatial-Constraints stage. Through systematic deconstruction, we theoretically and experimentally subsume existing preprocessing methods under this theorem via tripartite evidence: qualitative analyses, quantitative studies, and mathematical proofs. Guided by the Fence Theorem, we implement Patch3D, consisting of Patch-Cutting and Patch-Matching modules, to segment semantic spaces and consolidate similar ones while independently modeling normal features within each space. Experiments on Anomaly-ShapeNet and Real3D-AD with different settings demonstrate that progressively finer-grained semantic alignment in preprocessing directly enhances point-level AD accuracy, providing inverse validation of the theorem's causal logic.         ",
    "url": "https://arxiv.org/abs/2503.01100",
    "authors": [
      "Hanzhe Liang",
      "Jie Zhou",
      "Xuanxin Chen",
      "Jinbao Wang",
      "Can Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01101",
    "title": "Graph-Based Dynamics and Network Control of a Single Articulated Robotic System",
    "abstract": "           Extensive research on graph-based dynamics and control of multi-agent systems has successfully demonstrated control of robotic swarms, where each robot is perceived as an independent agent virtually connected by a network topology. The strong advantage of the network control structure lies in the decentralized nature of the control action, which only requires the knowledge of virtually connected agents. In this paper, we seek to expand the ideas of virtual network constraints to physical constraints on a class of tree-structured robots which we denote as single articulated robotic (SAR) systems. In our proposed framework, each link can be viewed as an agent, and each holonomic constraint connecting links serves as an edge. By following the first principles of Lagrangian dynamics, we derive a consensus-like matrix-differential equation with weighted graph and edge Laplacians for the dynamics of a SAR system. The sufficient condition for the holonomic constraint forces becoming independent to the control inputs is derived. This condition leads to a decentralized leader-follower network control framework for regulating the relative configuration of the robot. Simulation results demonstrate the effectiveness of the proposed control method.         ",
    "url": "https://arxiv.org/abs/2503.01101",
    "authors": [
      "Jonathan Lane",
      "Nak-seung Patrick Hyun"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.01124",
    "title": "ViKANformer: Embedding Kolmogorov Arnold Networks in Vision Transformers for Pattern-Based Learning",
    "abstract": "           Vision Transformers (ViTs) have significantly advanced image classification by applying self-attention on patch embeddings. However, the standard MLP blocks in each Transformer layer may not capture complex nonlinear dependencies optimally. In this paper, we propose ViKANformer, a Vision Transformer where we replace the MLP sub-layers with Kolmogorov-Arnold Network (KAN) expansions, including Vanilla KAN, Efficient-KAN, Fast-KAN, SineKAN, and FourierKAN, while also examining a Flash Attention variant. By leveraging the Kolmogorov-Arnold theorem, which guarantees that multivariate continuous functions can be expressed via sums of univariate continuous functions, we aim to boost representational power. Experimental results on MNIST demonstrate that SineKAN, Fast-KAN, and a well-tuned Vanilla KAN can achieve over 97% accuracy, albeit with increased training overhead. This trade-off highlights that KAN expansions may be beneficial if computational cost is acceptable. We detail the expansions, present training/test accuracy and F1/ROC metrics, and provide pseudocode and hyperparameters for reproducibility. Finally, we compare ViKANformer to a simple MLP and a small CNN baseline on MNIST, illustrating the efficiency of Transformer-based methods even on a small-scale dataset.         ",
    "url": "https://arxiv.org/abs/2503.01124",
    "authors": [
      "Shreyas S",
      "Akshath M"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01129",
    "title": "Apollo-MILP: An Alternating Prediction-Correction Neural Solving Framework for Mixed-Integer Linear Programming",
    "abstract": "           Leveraging machine learning (ML) to predict an initial solution for mixed-integer linear programming (MILP) has gained considerable popularity in recent years. These methods predict a solution and fix a subset of variables to reduce the problem dimension. Then, they solve the reduced problem to obtain the final solutions. However, directly fixing variable values can lead to low-quality solutions or even infeasible reduced problems if the predicted solution is not accurate enough. To address this challenge, we propose an Alternating prediction-correction neural solving framework (Apollo-MILP) that can identify and select accurate and reliable predicted values to fix. In each iteration, Apollo-MILP conducts a prediction step for the unfixed variables, followed by a correction step to obtain an improved solution (called reference solution) through a trust-region search. By incorporating the predicted and reference solutions, we introduce a novel Uncertainty-based Error upper BOund (UEBO) to evaluate the uncertainty of the predicted values and fix those with high confidence. A notable feature of Apollo-MILP is the superior ability for problem reduction while preserving optimality, leading to high-quality final solutions. Experiments on commonly used benchmarks demonstrate that our proposed Apollo-MILP significantly outperforms other ML-based approaches in terms of solution quality, achieving over a 50% reduction in the solution gap.         ",
    "url": "https://arxiv.org/abs/2503.01129",
    "authors": [
      "Haoyang Liu",
      "Jie Wang",
      "Zijie Geng",
      "Xijun Li",
      "Yuxuan Zong",
      "Fangzhou Zhu",
      "Jianye Hao",
      "Feng Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01131",
    "title": "Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs",
    "abstract": "           This paper presents an extensive examination of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain specific facts into Large Language Models (LLMs), focusing on improving the fine-tuning process by categorizing question-answer (QA) pairs into Factual and Conceptual classes using a BERT-based classifier. Two distinct Llama-2 models are fine-tuned based on these classifications and evaluated using larger models like GPT-3.5 Turbo and Gemini. Our results indicate that models trained on conceptual datasets outperform those trained on factual datasets. Additionally, we compare the efficiency of two synthetic fine-tuning dataset generation techniques, D-RAG and D-Naive, with D-Naive demonstrating superior performance. Although PEFT has shown effectiveness, our research indicates that it may not be the most optimal method for embedding facts into LLMs. However, it has demonstrated exceptional performance in instruction-based tasks. Our findings are reinforced by a 1000-sample dataset in the data center domain, where the fine-tuned Llama-2 7B model significantly outperforms the baseline model in generating product recommendations. Our study highlights the importance of QA pair categorization and synthetic dataset generation techniques in enhancing the performance of LLMs in specific domains.         ",
    "url": "https://arxiv.org/abs/2503.01131",
    "authors": [
      "Shivam Ratnakar",
      "Abhiroop Talasila",
      "Raghav Chamadiya",
      "Nikhil Agarwal",
      "Vinayak K Doifode"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01136",
    "title": "Prior-guided Hierarchical Harmonization Network for Efficient Image Dehazing",
    "abstract": "           Image dehazing is a crucial task that involves the enhancement of degraded images to recover their sharpness and textures. While vision Transformers have exhibited impressive results in diverse dehazing tasks, their quadratic complexity and lack of dehazing priors pose significant drawbacks for real-world applications. In this paper, guided by triple priors, Bright Channel Prior (BCP), Dark Channel Prior (DCP), and Histogram Equalization (HE), we propose a \\textit{P}rior-\\textit{g}uided Hierarchical \\textit{H}armonization Network (PGH$^2$Net) for image dehazing. PGH$^2$Net is built upon the UNet-like architecture with an efficient encoder and decoder, consisting of two module types: (1) Prior aggregation module that injects B/DCP and selects diverse contexts with gating attention. (2) Feature harmonization modules that subtract low-frequency components from spatial and channel aspects and learn more informative feature distributions to equalize the feature maps.         ",
    "url": "https://arxiv.org/abs/2503.01136",
    "authors": [
      "Xiongfei Su",
      "Siyuan Li",
      "Yuning Cui",
      "Miao Cao",
      "Yulun Zhang",
      "Zheng Chen",
      "Zongliang Wu",
      "Zedong Wang",
      "Yuanlong Zhang",
      "Xin Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01139",
    "title": "Can Large Language Models Help Experimental Design for Causal Discovery?",
    "abstract": "           Designing proper experiments and selecting optimal intervention targets is a longstanding problem in scientific or causal discovery. Identifying the underlying causal structure from observational data alone is inherently this http URL interventional data, on the other hand, is crucial to causal discovery, yet it is usually expensive and time-consuming to gather sufficient interventional data to facilitate causal this http URL approaches commonly utilize uncertainty or gradient signals to determine the intervention targets. However, numerical-based approaches may yield suboptimal results due to the inaccurate estimation of the guiding signals at the beginning when with limited interventional data. In this work, we investigate a different approach, whether we can leverage Large Language Models (LLMs) to assist with the intervention targeting in causal discovery by making use of the rich world knowledge about the experimental design in this http URL, we present \\oursfull (\\ours) -- a robust framework that effectively incorporates LLMs to augment existing numerical approaches for the intervention targeting in causal discovery. Across $4$ realistic benchmark scales, \\ours demonstrates significant improvements and robustness over existing methods and even surpasses humans, which demonstrates the usefulness of LLMs in assisting with experimental design for scientific discovery.         ",
    "url": "https://arxiv.org/abs/2503.01139",
    "authors": [
      "Junyi Li",
      "Yongqiang Chen",
      "Chenxi Liu",
      "Qianyi Cai",
      "Tongliang Liu",
      "Bo Han",
      "Kun Zhang",
      "Hui Xiong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2503.01146",
    "title": "Enhancing Deep Reinforcement Learning-based Robot Navigation Generalization through Scenario Augmentation",
    "abstract": "           This work focuses on enhancing the generalization performance of deep reinforcement learning-based robot navigation in unseen environments. We present a novel data augmentation approach called scenario augmentation, which enables robots to navigate effectively across diverse settings without altering the training scenario. The method operates by mapping the robot's observation into an imagined space, generating an imagined action based on this transformed observation, and then remapping this action back to the real action executed in simulation. Through scenario augmentation, we conduct extensive comparative experiments to investigate the underlying causes of suboptimal navigation behaviors in unseen environments. Our analysis indicates that limited training scenarios represent the primary factor behind these undesired behaviors. Experimental results confirm that scenario augmentation substantially enhances the generalization capabilities of deep reinforcement learning-based navigation systems. The improved navigation framework demonstrates exceptional performance by producing near-optimal trajectories with significantly reduced navigation time in real-world applications.         ",
    "url": "https://arxiv.org/abs/2503.01146",
    "authors": [
      "Shanze Wang",
      "Mingao Tan",
      "Zhibo Yang",
      "Xianghui Wang",
      "Xiaoyu Shen",
      "Hailong Huang",
      "Wei Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.01152",
    "title": "STGAN: Spatial-temporal Graph Autoregression Network for Pavement Distress Deterioration Prediction",
    "abstract": "           Pavement distress significantly compromises road integrity and poses risks to drivers. Accurate prediction of pavement distress deterioration is essential for effective road management, cost reduction in maintenance, and improvement of traffic safety. However, real-world data on pavement distress is usually collected irregularly, resulting in uneven, asynchronous, and sparse spatial-temporal datasets. This hinders the application of existing spatial-temporal models, such as DCRNN, since they are only applicable to regularly and synchronously collected data. To overcome these challenges, we propose the Spatial-Temporal Graph Autoregression Network (STGAN), a novel graph neural network model designed for accurately predicting irregular pavement distress deterioration using complex spatial-temporal data. Specifically, STGAN integrates the temporal domain into the spatial domain, creating a larger graph where nodes are represented by spatial-temporal tuples and edges are formed based on a similarity-based connection mechanism. Furthermore, based on the constructed spatiotemporal graph, we formulate pavement distress deterioration prediction as a graph autoregression task, i.e., the graph size increases incrementally and the prediction is performed sequentially. This is accomplished by a novel spatial-temporal attention mechanism deployed by STGAN. Utilizing the ConTrack dataset, which contains pavement distress records collected from different locations in Shanghai, we demonstrate the superior performance of STGAN in capturing spatial-temporal correlations and addressing the aforementioned challenges. Experimental results further show that STGAN outperforms baseline models, and ablation studies confirm the effectiveness of its novel modules. Our findings contribute to promoting proactive road maintenance decision-making and ultimately enhancing road safety and resilience.         ",
    "url": "https://arxiv.org/abs/2503.01152",
    "authors": [
      "Shilin Tong",
      "Difei Wu",
      "Xiaona Liu",
      "Le Zheng",
      "Yuchuan Du",
      "Difan Zou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01158",
    "title": "EasyCraft: A Robust and Efficient Framework for Automatic Avatar Crafting",
    "abstract": "           Character customization, or 'face crafting,' is a vital feature in role-playing games (RPGs), enhancing player engagement by enabling the creation of personalized avatars. Existing automated methods often struggle with generalizability across diverse game engines due to their reliance on the intermediate constraints of specific image domain and typically support only one type of input, either text or image. To overcome these challenges, we introduce EasyCraft, an innovative end-to-end feedforward framework that automates character crafting by uniquely supporting both text and image inputs. Our approach employs a translator capable of converting facial images of any style into crafting parameters. We first establish a unified feature distribution in the translator's image encoder through self-supervised learning on a large-scale dataset, enabling photos of any style to be embedded into a unified feature representation. Subsequently, we map this unified feature distribution to crafting parameters specific to a game engine, a process that can be easily adapted to most game engines and thus enhances EasyCraft's generalizability. By integrating text-to-image techniques with our translator, EasyCraft also facilitates precise, text-based character crafting. EasyCraft's ability to integrate diverse inputs significantly enhances the versatility and accuracy of avatar creation. Extensive experiments on two RPG games demonstrate the effectiveness of our method, achieving state-of-the-art results and facilitating adaptability across various avatar engines.         ",
    "url": "https://arxiv.org/abs/2503.01158",
    "authors": [
      "Suzhen Wang",
      "Weijie Chen",
      "Wei Zhang",
      "Minda Zhao",
      "Lincheng Li",
      "Rongsheng Zhang",
      "Zhipeng Hu",
      "Xin Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01169",
    "title": "A Zero-Shot Learning Approach for Ephemeral Gully Detection from Remote Sensing using Vision Language Models",
    "abstract": "           Ephemeral gullies are a primary cause of soil erosion and their reliable, accurate, and early detection will facilitate significant improvements in the sustainability of global agricultural systems. In our view, prior research has not successfully addressed automated detection of ephemeral gullies from remotely sensed images, so for the first time, we present and evaluate three successful pipelines for ephemeral gully detection. Our pipelines utilize remotely sensed images, acquired from specific agricultural areas over a period of time. The pipelines were tested with various choices of Visual Language Models (VLMs), and they classified the images based on the presence of ephemeral gullies with accuracy higher than 70% and a F1-score close to 80% for positive gully detection. Additionally, we developed the first public dataset for ephemeral gully detection, labeled by a team of soil- and plant-science experts. To evaluate the proposed pipelines, we employed a variety of zero-shot classification methods based on State-of-the-Art (SOTA) open-source Vision-Language Models (VLMs). In addition to that, we compare the same pipelines with a transfer learning approach. Extensive experiments were conducted to validate the detection pipelines and to analyze the impact of hyperparameter changes in their performance. The experimental results demonstrate that the proposed zero-shot classification pipelines are highly effective in detecting ephemeral gullies in a scenario where classification datasets are scarce.         ",
    "url": "https://arxiv.org/abs/2503.01169",
    "authors": [
      "Seyed Mohamad Ali Tousi",
      "Ramy Farag",
      "Jacket Demby's",
      "Gbenga Omotara",
      "John A. Lory",
      "G. N. DeSouza"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01175",
    "title": "HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation",
    "abstract": "           Co-speech gestures are crucial non-verbal cues that enhance speech clarity and expressiveness in human communication, which have attracted increasing attention in multimodal research. While the existing methods have made strides in gesture accuracy, challenges remain in generating diverse and coherent gestures, as most approaches assume independence among multimodal inputs and lack explicit modeling of their interactions. In this work, we propose a novel multimodal learning method named HOP for co-speech gesture generation that captures the heterogeneous entanglement between gesture motion, audio rhythm, and text semantics, enabling the generation of coordinated gestures. By leveraging spatiotemporal graph modeling, we achieve the alignment of audio and action. Moreover, to enhance modality coherence, we build the audio-text semantic representation based on a reprogramming module, which is beneficial for cross-modality adaptation. Our approach enables the trimodal system to learn each other's features and represent them in the form of topological entanglement. Extensive experiments demonstrate that HOP achieves state-of-the-art performance, offering more natural and expressive co-speech gesture generation. More information, codes, and demos are available here: this https URL ",
    "url": "https://arxiv.org/abs/2503.01175",
    "authors": [
      "Hongye Cheng",
      "Tianyu Wang",
      "Guangsi Shi",
      "Zexing Zhao",
      "Yanwei Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2503.01184",
    "title": "Language-Assisted Feature Transformation for Anomaly Detection",
    "abstract": "           This paper introduces LAFT, a novel feature transformation method designed to incorporate user knowledge and preferences into anomaly detection using natural language. Accurately modeling the boundary of normality is crucial for distinguishing abnormal data, but this is often challenging due to limited data or the presence of nuisance attributes. While unsupervised methods that rely solely on data without user guidance are common, they may fail to detect anomalies of specific interest. To address this limitation, we propose Language-Assisted Feature Transformation (LAFT), which leverages the shared image-text embedding space of vision-language models to transform visual features according to user-defined requirements. Combined with anomaly detection methods, LAFT effectively aligns visual features with user preferences, allowing anomalies of interest to be detected. Extensive experiments on both toy and real-world datasets validate the effectiveness of our method.         ",
    "url": "https://arxiv.org/abs/2503.01184",
    "authors": [
      "EungGu Yun",
      "Heonjin Ha",
      "Yeongwoo Nam",
      "Bryan Dongik Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01195",
    "title": "PostHoc FREE Calibrating on Kolmogorov Arnold Networks",
    "abstract": "           Kolmogorov Arnold Networks (KANs) are neural architectures inspired by the Kolmogorov Arnold representation theorem that leverage B Spline parameterizations for flexible, locally adaptive function approximation. Although KANs can capture complex nonlinearities beyond those modeled by standard MultiLayer Perceptrons (MLPs), they frequently exhibit miscalibrated confidence estimates manifesting as overconfidence in dense data regions and underconfidence in sparse areas. In this work, we systematically examine the impact of four critical hyperparameters including Layer Width, Grid Order, Shortcut Function, and Grid Range on the calibration of KANs. Furthermore, we introduce a novel TemperatureScaled Loss (TSL) that integrates a temperature parameter directly into the training objective, dynamically adjusting the predictive distribution during learning. Both theoretical analysis and extensive empirical evaluations on standard benchmarks demonstrate that TSL significantly reduces calibration errors, thereby improving the reliability of probabilistic predictions. Overall, our study provides actionable insights into the design of spline based neural networks and establishes TSL as a robust loss solution for enhancing calibration.         ",
    "url": "https://arxiv.org/abs/2503.01195",
    "authors": [
      "Wenhao Liang",
      "Wei Emma Zhang",
      "Lin Yue",
      "Miao Xu",
      "Olaf Maennel",
      "Weitong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01208",
    "title": "Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models",
    "abstract": "           Multi-Modal Large Language Models (MLLMs) have exhibited remarkable performance on various vision-language tasks such as Visual Question Answering (VQA). Despite accumulating evidence of privacy concerns associated with task-relevant content, it remains unclear whether MLLMs inadvertently memorize private content that is entirely irrelevant to the training tasks. In this paper, we investigate how randomly generated task-irrelevant private content can become spuriously correlated with downstream objectives due to partial mini-batch training dynamics, thus causing inadvertent memorization. Concretely, we randomly generate task-irrelevant watermarks into VQA fine-tuning images at varying probabilities and propose a novel probing framework to determine whether MLLMs have inadvertently encoded such content. Our experiments reveal that MLLMs exhibit notably different training behaviors in partial mini-batch settings with task-irrelevant watermarks embedded. Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger distinct representational patterns when encountering previously seen task-irrelevant knowledge, even if this knowledge does not influence their output during prompting. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.01208",
    "authors": [
      "Tianjie Ju",
      "Yi Hua",
      "Hao Fei",
      "Zhenyu Shao",
      "Yubin Zheng",
      "Haodong Zhao",
      "Mong-Li Lee",
      "Wynne Hsu",
      "Zhuosheng Zhang",
      "Gongshen Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.01229",
    "title": "Enhancing Network Security Management in Water Systems using FM-based Attack Attribution",
    "abstract": "           Water systems are vital components of modern infrastructure, yet they are increasingly susceptible to sophisticated cyber attacks with potentially dire consequences on public health and safety. While state-of-the-art machine learning techniques effectively detect anomalies, contemporary model-agnostic attack attribution methods using LIME, SHAP, and LEMNA are deemed impractical for large-scale, interdependent water systems. This is due to the intricate interconnectivity and dynamic interactions that define these complex environments. Such methods primarily emphasize individual feature importance while falling short of addressing the crucial sensor-actuator interactions in water systems, which limits their effectiveness in identifying root cause attacks. To this end, we propose a novel model-agnostic Factorization Machines (FM)-based approach that capitalizes on water system sensor-actuator interactions to provide granular explanations and attributions for cyber attacks. For instance, an anomaly in an actuator pump activity can be attributed to a top root cause attack candidates, a list of water pressure sensors, which is derived from the underlying linear and quadratic effects captured by our approach. We validate our method using two real-world water system specific datasets, SWaT and WADI, demonstrating its superior performance over traditional attribution methods. In multi-feature cyber attack scenarios involving intricate sensor-actuator interactions, our FM-based attack attribution method effectively ranks attack root causes, achieving approximately 20% average improvement over SHAP and LEMNA.         ",
    "url": "https://arxiv.org/abs/2503.01229",
    "authors": [
      "Aleksandar Avdalovic",
      "Joseph Khoury",
      "Ahmad Taha",
      "Elias Bou-Harb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.01232",
    "title": "Learning Covariance-Based Multi-Scale Representation of Neuroimaging Measures for Alzheimer Classification",
    "abstract": "           Stacking excessive layers in DNN results in highly underdetermined system when training samples are limited, which is very common in medical applications. In this regard, we present a framework capable of deriving an efficient high-dimensional space with reasonable increase in model size. This is done by utilizing a transform (i.e., convolution) that leverages scale-space theory with covariance structure. The overall model trains on this transform together with a downstream classifier (i.e., Fully Connected layer) to capture the optimal multi-scale representation of the original data which corresponds to task-specific components in a dual space. Experiments on neuroimaging measures from Alzheimer's Disease Neuroimaging Initiative (ADNI) study show that our model performs better and converges faster than conventional models even when the model size is significantly reduced. The trained model is made interpretable using gradient information over the multi-scale transform to delineate personalized AD-specific regions in the brain.         ",
    "url": "https://arxiv.org/abs/2503.01232",
    "authors": [
      "Seunghun Baek",
      "Injun Choi",
      "Mustafa Dere",
      "Minjeong Kim",
      "Guorong Wu",
      "Won Hwa Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01234",
    "title": "Self-Adaptive Gamma Context-Aware SSM-based Model for Metal Defect Detection",
    "abstract": "           Metal defect detection is critical in industrial quality assurance, yet existing methods struggle with grayscale variations and complex defect states, limiting its robustness. To address these challenges, this paper proposes a Self-Adaptive Gamma Context-Aware SSM-based model(GCM-DET). This advanced detection framework integrating a Dynamic Gamma Correction (GC) module to enhance grayscale representation and optimize feature extraction for precise defect reconstruction. A State-Space Search Management (SSM) architecture captures robust multi-scale features, effectively handling defects of varying shapes and scales. Focal Loss is employed to mitigate class imbalance and refine detection accuracy. Additionally, the CD5-DET dataset is introduced, specifically designed for port container maintenance, featuring significant grayscale variations and intricate defect patterns. Experimental results demonstrate that the proposed model achieves substantial improvements, with mAP@0.5 gains of 27.6\\%, 6.6\\%, and 2.6\\% on the CD5-DET, NEU-DET, and GC10-DET datasets.         ",
    "url": "https://arxiv.org/abs/2503.01234",
    "authors": [
      "Sijin Sun",
      "Ming Deng",
      "Xingrui Yu",
      "Xinyu Xi",
      "Liangbin Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01245",
    "title": "A Survey On Large Language Models For Code Generation",
    "abstract": "           Large Language Models (LLMs) have demonstrated their remarkable capabilities in numerous fields. This survey focuses on how LLMs empower users, regardless of their technical background, to use human languages to automatically generate executable code. We begin with understanding LLMs' limitations and challenges in automated code generation. Subsequently, we review various fine-tuning techniques designed to enhance both the performance and adaptability of LLMs in code generation tasks. We then review the existing metrics and benchmarks for evaluations to assess model performance based on fine-tuning techniques. Finally, we explore the applications of LLMs (e.g. CodeLlama, GitHub Copilot, ToolGen) in code generation tasks to illustrate their roles and functionalities. This survey provides a comprehensive overview of LLMs for code generation, helps researchers in diverse fields better understand the current state-of-the-art technologies, and offers the potential of effectively leveraging LLMs for code generation tasks.         ",
    "url": "https://arxiv.org/abs/2503.01245",
    "authors": [
      "Nam Huynh",
      "Beiyu Lin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01256",
    "title": "Prior-Fitted Networks Scale to Larger Datasets When Treated as Weak Learners",
    "abstract": "           Prior-Fitted Networks (PFNs) have recently been proposed to efficiently perform tabular classification tasks. Although they achieve good performance on small datasets, they encounter limitations with larger datasets. These limitations include significant memory consumption and increased computational complexity, primarily due to the impracticality of incorporating all training samples as inputs within these networks. To address these challenges, we investigate the fitting assumption for PFNs and input samples. Building on this understanding, we propose \\textit{BoostPFN} designed to enhance the performance of these networks, especially for large-scale datasets. We also theoretically validate the convergence of BoostPFN and our empirical results demonstrate that the BoostPFN method can outperform standard PFNs with the same size of training samples in large datasets and achieve a significant acceleration in training times compared to other established baselines in the field, including widely-used Gradient Boosting Decision Trees (GBDTs), deep learning methods and AutoML systems. High performance is maintained for up to 50x of the pre-training size of PFNs, substantially extending the limit of training samples. Through this work, we address the challenges of efficiently handling large datasets via PFN-based models, paving the way for faster and more effective tabular data classification training and prediction process. Code is available at Github.         ",
    "url": "https://arxiv.org/abs/2503.01256",
    "authors": [
      "Yuxin Wang",
      "Botian Jiang",
      "Yiran Guo",
      "Quan Gan",
      "David Wipf",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01260",
    "title": "OIPR: Evaluation for Time-series Anomaly Detection Inspired by Operator Interest",
    "abstract": "           With the growing adoption of time-series anomaly detection (TAD) technology, numerous studies have employed deep learning-based detectors for analyzing time-series data in the fields of Internet services, industrial systems, and sensors. The selection and optimization of anomaly detectors strongly rely on the availability of an effective performance evaluation method for TAD. Since anomalies in time-series data often manifest as a sequence of points, conventional metrics that solely consider the detection of individual point are inadequate. Existing evaluation methods for TAD typically employ point-based or event-based metrics to capture the temporal context. However, point-based metrics tend to overestimate detectors that excel only in detecting long anomalies, while event-based metrics are susceptible to being misled by fragmented detection results. To address these limitations, we propose OIPR, a novel set of TAD evaluation metrics. It models the process of operators receiving detector alarms and handling faults, utilizing area under the operator interest curve to evaluate the performance of TAD algorithms. Furthermore, we build a special scenario dataset to compare the characteristics of different evaluation methods. Through experiments conducted on the special scenario dataset and five real-world datasets, we demonstrate the remarkable performance of OIPR in extreme and complex scenarios. It achieves a balance between point and event perspectives, overcoming their primary limitations and offering applicability to broader situations.         ",
    "url": "https://arxiv.org/abs/2503.01260",
    "authors": [
      "Yuhan Jing",
      "Jingyu Wang",
      "Lei Zhang",
      "Haifeng Sun",
      "Bo He",
      "Zirui Zhuang",
      "Chengsen Wang",
      "Qi Qi",
      "Jianxin Liao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01264",
    "title": "DCAMamba: Mamba-based Rapid Response DC Arc Fault Detection",
    "abstract": "           In electrical equipment, even minor contact issues can lead to arc faults. Traditional methods often struggle to balance the accuracy and rapid response required for effective arc fault detection. To address this challenge, we introduce DCAMamba, a novel framework for arc fault detection. Specifically, DCAMamba is built upon a state-space model (SSM) and utilizes a hardware-aware parallel algorithm, designed in a cyclic mode using the Mamba architecture. To meet the dual demands of high accuracy and fast response in arc fault detection, we have refined the original Mamba model and incorporated a Feature Amplification Strategy (FAS), a simple yet effective method that enhances the model's ability to interpret arc fault data. Experimental results show that DCAMamba, with FAS, achieves a 12$\\%$ improvement in accuracy over the original Mamba, while maintaining an inference time of only 1.87 milliseconds. These results highlight the significant potential of DCAMamba as a future backbone for signal processing. Our code will be made open-source after peer review.         ",
    "url": "https://arxiv.org/abs/2503.01264",
    "authors": [
      "Lukun Wang",
      "Ruxue Zhao",
      "Wancheng Feng",
      "Pu Sun",
      "Chunpeng Tian"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2503.01281",
    "title": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache Allocation GNN Inference Acceleration System",
    "abstract": "           Graph Neural Networks (GNNs) are powerful tools for processing graph-structured data, increasingly used for large-scale real-world graphs via sampling-based inference methods. However, inherent characteristics of neighbor sampling lead to redundant data loading during GNN inference, compounded by inefficient data transfers between host and GPU memory, resulting in slow inference and low resource utilization. Existing methods to accelerate GNN inference face several challenges: (1) low practical GPU memory utilization, (2) overlooking adjacency matrix locality, and (3) long preprocessing time. To address these challenges, we introduce DCI, an efficient workload-aware dual-cache allocation system for GNN inference acceleration. DCI allocates cache capacities for both node features and adjacency matrices based on workload patterns during the pre-sampling phase, leveraging a lightweight cache-filling algorithm to optimize data loading efficiency. Experimental results demonstrate that DCI accelerates sampling and node feature loading, achieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$ compared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing preprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms state-of-the-art single-cache inference systems by achieving speedup of 1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache population strategy. Our lightweight population algorithm allows DCI to achieve nearly the same inference speed while keeping preprocessing time to less than 20\\% of that required by DUCATI.         ",
    "url": "https://arxiv.org/abs/2503.01281",
    "authors": [
      "Yi Luo",
      "Yaobin Wang",
      "Qi Wang",
      "Yingchen Song",
      "Huan Wu",
      "Qingfeng Wang",
      "Jun Huang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2503.01284",
    "title": "Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating MobileNetV2 and GraphSAGE with Cross-Modal Attention",
    "abstract": "           Soybean leaf disease detection is critical for agricultural productivity but faces challenges due to visually similar symptoms and limited interpretability in conventional methods. While Convolutional Neural Networks (CNNs) excel in spatial feature extraction, they often neglect inter-image relational dependencies, leading to misclassifications. This paper proposes an interpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that synergizes MobileNetV2 for localized feature extraction and GraphSAGE for relational modeling. The framework constructs a graph where nodes represent leaf images, with edges defined by cosine similarity-based adjacency matrices and adaptive neighborhood sampling. This design captures fine-grained lesion features and global symptom patterns, addressing inter-class similarity challenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM visualizations, generating heatmaps to highlight disease-influential regions. Evaluated on a dataset of ten soybean leaf diseases, the model achieves $97.16\\%$ accuracy, surpassing standalone CNNs ($\\le95.04\\%$) and traditional machine learning models ($\\le77.05\\%$). Ablation studies validate the sequential architecture's superiority over parallel or single-model configurations. With only 2.3 million parameters, the lightweight MobileNetV2-GraphSAGE combination ensures computational efficiency, enabling real-time deployment in resource-constrained environments. The proposed approach bridges the gap between accurate classification and practical applicability, offering a robust, interpretable tool for agricultural diagnostics while advancing CNN-GNN integration in plant pathology research.         ",
    "url": "https://arxiv.org/abs/2503.01284",
    "authors": [
      "Md Abrar Jahin",
      "Soudeep Shahriar",
      "M. F. Mridha",
      "Nilanjan Dey"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01287",
    "title": "Robust Simulation-Based Inference under Missing Data via Neural Processes",
    "abstract": "           Simulation-based inference (SBI) methods typically require fully observed data to infer parameters of models with intractable likelihood functions. However, datasets often contain missing values due to incomplete observations, data corruptions (common in astrophysics), or instrument limitations (e.g., in high-energy physics applications). In such scenarios, missing data must be imputed before applying any SBI method. We formalize the problem of missing data in SBI and demonstrate that naive imputation methods can introduce bias in the estimation of SBI posterior. We also introduce a novel amortized method that addresses this issue by jointly learning the imputation model and the inference network within a neural posterior estimation (NPE) framework. Extensive empirical results on SBI benchmarks show that our approach provides robust inference outcomes compared to standard baselines for varying levels of missing data. Moreover, we demonstrate the merits of our imputation model on two real-world bioactivity datasets (Adrenergic and Kinase assays). Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.01287",
    "authors": [
      "Yogesh Verma",
      "Ayush Bharti",
      "Vikas Garg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.01290",
    "title": "ACTIVA: Amortized Causal Effect Estimation without Graphs via Transformer-based Variational Autoencoder",
    "abstract": "           Predicting the distribution of outcomes under hypothetical interventions is crucial in domains like healthcare, economics, and policy-making. Current methods often rely on strong assumptions, such as known causal graphs or parametric models, and lack amortization across problem instances, limiting their practicality. We propose a novel transformer-based conditional variational autoencoder architecture, named ACTIVA, that extends causal transformer encoders to predict causal effects as mixtures of Gaussians. Our method requires no causal graph and predicts interventional distributions given only observational data and a queried intervention. By amortizing over many simulated instances, it enables zero-shot generalization to novel datasets without retraining. Experiments demonstrate accurate predictions for synthetic and semi-synthetic data, showcasing the effectiveness of our graph-free, amortized causal inference approach.         ",
    "url": "https://arxiv.org/abs/2503.01290",
    "authors": [
      "Andreas Sauter",
      "Saber Salehkaleybar",
      "Aske Plaat",
      "Erman Acar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01292",
    "title": "PA-CLIP: Enhancing Zero-Shot Anomaly Detection through Pseudo-Anomaly Awareness",
    "abstract": "           In industrial anomaly detection (IAD), accurately identifying defects amidst diverse anomalies and under varying imaging conditions remains a significant challenge. Traditional approaches often struggle with high false-positive rates, frequently misclassifying normal shadows and surface deformations as defects, an issue that becomes particularly pronounced in products with complex and intricate surface features. To address these challenges, we introduce PA-CLIP, a zero-shot anomaly detection method that reduces background noise and enhances defect detection through a pseudo-anomaly-based framework. The proposed method integrates a multiscale feature aggregation strategy for capturing detailed global and local information, two memory banks for distinguishing background information, including normal patterns and pseudo-anomalies, from true anomaly features, and a decision-making module designed to minimize false positives caused by environmental variations while maintaining high defect sensitivity. Demonstrated on the MVTec AD and VisA datasets, PA-CLIP outperforms existing zero-shot methods, providing a robust solution for industrial defect detection.         ",
    "url": "https://arxiv.org/abs/2503.01292",
    "authors": [
      "Yurui Pan",
      "Lidong Wang",
      "Yuchao Chen",
      "Wenbing Zhu",
      "Bo Peng",
      "Mingmin Chi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01295",
    "title": "CodeArena: A Collective Evaluation Platform for LLM Code Generation",
    "abstract": "           Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and programming syntax, thereby substantially boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capabilities. However, persistent challenges, such as benchmark leakage, data dissipation, and limited system accessibility, continue to impede a timely and accurate assessment. To address these limitations, we introduce CodeArena, an online evaluation framework tailored for LLM code generation. The key innovation is a collective evaluation mechanism, which dynamically recalibrates individual model scores based on the holistic performance of all participating models, mitigating score biases caused by widespread benchmark leakage. In addition, CodeArena ensures open access to all submitted solutions and test cases and provides automation-friendly APIs to streamline the code evaluation workflow. Our main contributions are: (1) a collective evaluation system for unbiased assessment, (2) a public repository of solutions and test cases, and (3) automation-ready APIs for seamless integration.         ",
    "url": "https://arxiv.org/abs/2503.01295",
    "authors": [
      "Mingzhe Du",
      "Anh Tuan Luu",
      "Bin Ji",
      "Xiaobao Wu",
      "Dong Huang",
      "Terry Yue Zhuo",
      "Qian Liu",
      "See-Kiong Ng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.01302",
    "title": "Causal Tree Extraction from Medical Case Reports: A Novel Task for Experts-like Text Comprehension",
    "abstract": "           Extracting causal relationships from a medical case report is essential for comprehending the case, particularly its diagnostic process. Since the diagnostic process is regarded as a bottom-up inference, causal relationships in cases naturally form a multi-layered tree structure. The existing tasks, such as medical relation extraction, are insufficient for capturing the causal relationships of an entire case, as they treat all relations equally without considering the hierarchical structure inherent in the diagnostic process. Thus, we propose a novel task, Causal Tree Extraction (CTE), which receives a case report and generates a causal tree with the primary disease as the root, providing an intuitive understanding of a case's diagnostic process. Subsequently, we construct a Japanese case report CTE dataset, J-Casemap, propose a generation-based CTE method that outperforms the baseline by 20.2 points in the human evaluation, and introduce evaluation metrics that reflect clinician preferences. Further experiments also show that J-Casemap enhances the performance of solving other medical tasks, such as question answering.         ",
    "url": "https://arxiv.org/abs/2503.01302",
    "authors": [
      "Sakiko Yahata",
      "Zhen Wan",
      "Fei Cheng",
      "Sadao Kurohashi",
      "Hisahiko Sato",
      "Ryozo Nagai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.01311",
    "title": "ecg2o: A Seamless Extension of g2o for Equality-Constrained Factor Graph Optimization",
    "abstract": "           Factor graph optimization serves as a fundamental framework for robotic perception, enabling applications such as pose estimation, simultaneous localization and mapping (SLAM), structure-from-motion (SfM), and situational awareness. Traditionally, these methods solve unconstrained least squares problems using algorithms such as Gauss-Newton and Levenberg-Marquardt. However, extending factor graphs with native support for equality constraints can improve solution accuracy and broaden their applicability, particularly in optimal control. In this paper, we propose a novel extension of factor graphs that seamlessly incorporates equality constraints without requiring additional optimization algorithms. Our approach maintains the efficiency and flexibility of existing second-order optimization techniques while ensuring constraint feasibility. To validate our method, we apply it to an optimal control problem for velocity tracking in autonomous vehicles and benchmark our results against state-of-the-art constraint handling techniques. Additionally, we introduce ecg2o, a header-only C++ library that extends the widely used g2o factor graph library by adding full support for equality-constrained optimization. This library, along with demonstrative examples and the optimal control problem, is available as open source at this https URL ",
    "url": "https://arxiv.org/abs/2503.01311",
    "authors": [
      "Anas Abdelkarim",
      "Holger Voos",
      "Daniel G\u00f6rges"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.01315",
    "title": "Explainable Depression Detection in Clinical Interviews with Personalized Retrieval-Augmented Generation",
    "abstract": "           Depression is a widespread mental health disorder, and clinical interviews are the gold standard for assessment. However, their reliance on scarce professionals highlights the need for automated detection. Current systems mainly employ black-box neural networks, which lack interpretability, which is crucial in mental health contexts. Some attempts to improve interpretability use post-hoc LLM generation but suffer from hallucination. To address these limitations, we propose RED, a Retrieval-augmented generation framework for Explainable depression Detection. RED retrieves evidence from clinical interview transcripts, providing explanations for predictions. Traditional query-based retrieval systems use a one-size-fits-all approach, which may not be optimal for depression detection, as user backgrounds and situations vary. We introduce a personalized query generation module that combines standard queries with user-specific background inferred by LLMs, tailoring retrieval to individual contexts. Additionally, to enhance LLM performance in social intelligence, we augment LLMs by retrieving relevant knowledge from a social intelligence datastore using an event-centric retriever. Experimental results on the real-world benchmark demonstrate RED's effectiveness compared to neural networks and LLM-based baselines.         ",
    "url": "https://arxiv.org/abs/2503.01315",
    "authors": [
      "Linhai Zhang",
      "Ziyang Gao",
      "Deyu Zhou",
      "Yulan He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.01319",
    "title": "ABFS: Natural Robustness Testing for LLM-based NLP Software",
    "abstract": "           Owing to the exceptional performance of Large Language Models (LLMs) in Natural Language Processing (NLP) tasks, LLM-based NLP software has rapidly gained traction across various domains, such as financial analysis and content moderation. However, these applications frequently exhibit robustness deficiencies, where slight perturbations in input (prompt+example) may lead to erroneous outputs. Current robustness testing methods face two main limitations: (1) low testing effectiveness, limiting the applicability of LLM-based software in safety-critical scenarios, and (2) insufficient naturalness of test cases, reducing the practical value of testing outcomes. To address these issues, this paper proposes ABFS, a straightforward yet effective automated testing method that, for the first time, treats the input prompts and examples as a unified whole for robustness testing. Specifically, ABFS formulates the testing process as a combinatorial optimization problem, employing Best-First Search to identify successful test cases within the perturbation space and designing a novel Adaptive control strategy to enhance test case naturalness. We evaluate the robustness testing performance of ABFS on three datasets across five threat models. On Llama2-13b, the traditional StressTest achieves only a 13.273% success rate, while ABFS attains a success rate of 98.064%, supporting a more comprehensive robustness assessment before software deployment. Compared to baseline methods, ABFS introduces fewer modifications to the original input and consistently generates test cases with superior naturalness. Furthermore, test cases generated by ABFS exhibit stronger transferability and higher testing efficiency, significantly reducing testing costs.         ",
    "url": "https://arxiv.org/abs/2503.01319",
    "authors": [
      "Mingxuan Xiao",
      "Yan Xiao",
      "Shunhui Ji",
      "Yunhe Li",
      "Lei Xue",
      "Pengcheng Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.01327",
    "title": "Victim-Centred Abuse Investigations and Defenses for Social Media Platforms",
    "abstract": "           Online abuse, a persistent aspect of social platform interactions, impacts user well-being and exposes flaws in platform designs that include insufficient detection efforts and inadequate victim protection measures. Ensuring safety in platform interactions requires the integration of victim perspectives in the design of abuse detection and response systems. In this paper, we conduct surveys (n = 230) and semi-structured interviews (n = 15) with students at a minority-serving institution in the US, to explore their experiences with abuse on a variety of social platforms, their defense strategies, and their recommendations for social platforms to improve abuse responses. We build on study findings to propose design requirements for abuse defense systems and discuss the role of privacy, anonymity, and abuse attribution requirements in their implementation. We introduce ARI, a blueprint for a unified, transparent, and personalized abuse response system for social platforms that sustainably detects abuse by leveraging the expertise of platform users, incentivized with proceeds obtained from abusers.         ",
    "url": "https://arxiv.org/abs/2503.01327",
    "authors": [
      "Zaid Hakami",
      "Ashfaq Ali Shafin",
      "Peter J. Clarke",
      "Niki Pissinou",
      "Bogdan Carbunar"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.01329",
    "title": "Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive Fine-tuning",
    "abstract": "           Recent advancements in large language models (LLMs) based on transformer architectures have sparked significant interest in understanding their inner workings. In this paper, we introduce a novel approach to modeling transformer architectures using highly flexible non-autonomous neural ordinary differential equations (ODEs). Our proposed model parameterizes all weights of attention and feed-forward blocks through neural networks, expressing these weights as functions of a continuous layer index. Through spectral analysis of the model's dynamics, we uncover an increase in eigenvalue magnitude that challenges the weight-sharing assumption prevalent in existing theoretical studies. We also leverage the Lyapunov exponent to examine token-level sensitivity, enhancing model interpretability. Our neural ODE transformer demonstrates performance comparable to or better than vanilla transformers across various configurations and datasets, while offering flexible fine-tuning capabilities that can adapt to different architectural constraints.         ",
    "url": "https://arxiv.org/abs/2503.01329",
    "authors": [
      "Anh Tong",
      "Thanh Nguyen-Tang",
      "Dongeun Lee",
      "Duc Nguyen",
      "Toan Tran",
      "David Hall",
      "Cheongwoong Kang",
      "Jaesik Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01345",
    "title": "Same Question, Different Words: A Latent Adversarial Framework for Prompt Robustness",
    "abstract": "           Insensitivity to semantically-preserving variations of prompts (paraphrases) is crucial for reliable behavior and real-world deployment of large language models. However, language models exhibit significant performance degradation when faced with semantically equivalent but differently phrased prompts, and existing solutions either depend on trial-and-error prompt engineering or require computationally expensive inference-time algorithms. In this study, built on the key insight that worst-case prompts exhibit a drift in embedding space, we present Latent Adversarial Paraphrasing (LAP), a dual-loop adversarial framework: the inner loop trains a learnable perturbation to serve as a \"latent continuous paraphrase\" while preserving semantics through Lagrangian regulation, and the outer loop optimizes the language model parameters on these perturbations. We conduct extensive experiments to demonstrate the effectiveness of LAP across multiple LLM architectures on the RobustAlpaca benchmark with a 0.5%-4% absolution improvement on worst-case win-rate compared with vanilla supervised fine-tuning.         ",
    "url": "https://arxiv.org/abs/2503.01345",
    "authors": [
      "Tingchen Fu",
      "Fazl Barez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01346",
    "title": "SRAG: Structured Retrieval-Augmented Generation for Multi-Entity Question Answering over Wikipedia Graph",
    "abstract": "           Multi-entity question answering (MEQA) poses significant challenges for large language models (LLMs), which often struggle to consolidate scattered information across multiple documents. An example question might be \"What is the distribution of IEEE Fellows among various fields of study?\", which requires retrieving information from diverse sources e.g., Wikipedia pages. The effectiveness of current retrieval-augmented generation (RAG) methods is limited by the LLMs' capacity to aggregate insights from numerous pages. To address this gap, this paper introduces a structured RAG (SRAG) framework that systematically organizes extracted entities into relational tables (e.g., tabulating entities with schema columns like \"name\" and \"field of study\") and then apply table-based reasoning techniques. Our approach decouples retrieval and reasoning, enabling LLMs to focus on structured data analysis rather than raw text aggregation. Extensive experiments on Wikipedia-based multi-entity QA tasks demonstrate that SRAG significantly outperforms state-of-the-art long-context LLMs and RAG solutions, achieving a 29.6% improvement in accuracy. The results underscore the efficacy of structuring unstructured data to enhance LLMs' reasoning capabilities.         ",
    "url": "https://arxiv.org/abs/2503.01346",
    "authors": [
      "Teng Lin",
      "Yizhang Zhu",
      "Yuyu Luo",
      "Nan Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.01347",
    "title": "Spatial Transcriptomics Analysis of Spatially Dense Gene Expression Prediction",
    "abstract": "           Spatial transcriptomics (ST) measures gene expression at fine-grained spatial resolution, offering insights into tissue molecular landscapes. Previous methods for spatial gene expression prediction usually crop spots of interest from pathology tissue slide images, and learn a model that maps each spot to a single gene expression profile. However, it fundamentally loses spatial resolution of gene expression: 1) each spot often contains multiple cells with distinct gene expression; 2) spots are cropped at fixed resolutions, limiting the ability to predict gene expression at varying spatial scales. To address these limitations, this paper presents PixNet, a dense prediction network capable of predicting spatially resolved gene expression across spots of varying sizes and scales directly from pathology images. Different from previous methods that map individual spots to gene expression values, we generate a dense continuous gene expression map from the pathology image, and aggregate values within spots of interest to predict the gene expression. Our PixNet outperforms state-of-the-art methods on 3 common ST datasets, while showing superior performance in predicting gene expression across multiple spatial scales. The source code will be publicly available.         ",
    "url": "https://arxiv.org/abs/2503.01347",
    "authors": [
      "Ruikun Zhang",
      "Yan Yang",
      "Liyuan Pan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01362",
    "title": "Streaming Piano Transcription Based on Consistent Onset and Offset Decoding with Sustain Pedal Detection",
    "abstract": "           This paper describes a streaming audio-to-MIDI piano transcription approach that aims to sequentially translate a music signal into a sequence of note onset and offset events. The sequence-to-sequence nature of this task may call for the computationally-intensive transformer model for better performance, which has recently been used for offline transcription benchmarks and could be extended for streaming transcription with causal attention mechanisms. We assume that the performance limitation of this naive approach lies in the decoder. Although time-frequency features useful for onset detection are considerably different from those for offset detection, the single decoder is trained to output a mixed sequence of onset and offset events without guarantee of the correspondence between the onset and offset events of the same note. To overcome this limitation, we propose a streaming encoder-decoder model that uses a convolutional encoder aggregating local acoustic features, followed by an autoregressive Transformer decoder detecting a variable number of onset events and another decoder detecting the offset events for the active pitches with validation of the sustain pedal at each time frame. Experiments using the MAESTRO dataset showed that the proposed streaming method performed comparably with or even better than the state-of-the-art offline methods while significantly reducing the computational cost.         ",
    "url": "https://arxiv.org/abs/2503.01362",
    "authors": [
      "Weixing Wei",
      "Jiahao Zhao",
      "Yulun Wu",
      "Kazuyoshi Yoshii"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Information Retrieval (cs.IR)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2503.01385",
    "title": "Q-NL Verifier: Leveraging Synthetic Data for Robust Knowledge Graph Question Answering",
    "abstract": "           Question answering (QA) requires accurately aligning user questions with structured queries, a process often limited by the scarcity of high-quality query-natural language (Q-NL) pairs. To overcome this, we present Q-NL Verifier, an approach to generating high-quality synthetic pairs of queries and NL translations. Our approach relies on large language models (LLMs) to generate semantically precise natural language paraphrases of structured queries. Building on these synthetic Q-NL pairs, we introduce a learned verifier component that automatically determines whether a generated paraphrase is semantically equivalent to the original query. Our experiments with the well-known LC-QuAD 2.0 benchmark show that Q-NL Verifier generalizes well to paraphrases from other models and even human-authored translations. Our approach strongly aligns with human judgments across varying query complexities and outperforms existing NLP metrics in assessing semantic correctness. We also integrate the verifier into QA pipelines, showing that verifier-filtered synthetic data has significantly higher quality in terms of translation correctness and enhances NL to Q translation accuracy. Lastly, we release an updated version of the LC-QuAD 2.0 benchmark containing our synthetic Q-NL pairs and verifier scores, offering a new resource for robust and scalable QA.         ",
    "url": "https://arxiv.org/abs/2503.01385",
    "authors": [
      "Tim Schwabe",
      "Louisa Siebel",
      "Patrik Valach",
      "Maribel Acosta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01386",
    "title": "Geo-Semantic-Parsing: AI-powered geoparsing by traversing semantic knowledge graphs",
    "abstract": "           Online social networks convey rich information about geospatial facets of reality. However in most cases, geographic information is not explicit and structured, thus preventing its exploitation in real-time applications. We address this limitation by introducing a novel geoparsing and geotagging technique called Geo-Semantic-Parsing (GSP). GSP identifies location references in free text and extracts the corresponding geographic coordinates. To reach this goal, we employ a semantic annotator to identify relevant portions of the input text and to link them to the corresponding entity in a knowledge graph. Then, we devise and experiment with several efficient strategies for traversing the knowledge graph, thus expanding the available set of information for the geoparsing task. Finally, we exploit all available information for learning a regression model that selects the best entity with which to geotag the input text. We evaluate GSP on a well-known reference dataset including almost 10k event-related tweets, achieving $F1=0.66$. We extensively compare our results with those of 2 baselines and 3 state-of-the-art geoparsing techniques, achieving the best performance. On the same dataset, competitors obtain $F1 \\leq 0.55$. We conclude by providing in-depth analyses of our results, showing that the overall superior performance of GSP is mainly due to a large improvement in recall, with respect to existing techniques.         ",
    "url": "https://arxiv.org/abs/2503.01386",
    "authors": [
      "Leonardo Nizzoli",
      "Marco Avvenuti",
      "Maurizio Tesconi",
      "Stefano Cresci"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.01391",
    "title": "The Road Less Traveled: Investigating Robustness and Explainability in CNN Malware Detection",
    "abstract": "           Machine learning has become a key tool in cybersecurity, improving both attack strategies and defense mechanisms. Deep learning models, particularly Convolutional Neural Networks (CNNs), have demonstrated high accuracy in detecting malware images generated from binary data. However, the decision-making process of these black-box models remains difficult to interpret. This study addresses this challenge by integrating quantitative analysis with explainability tools such as Occlusion Maps, HiResCAM, and SHAP to better understand CNN behavior in malware classification. We further demonstrate that obfuscation techniques can reduce model accuracy by up to 50%, and propose a mitigation strategy to enhance robustness. Additionally, we analyze heatmaps from multiple tests and outline a methodology for identification of artifacts, aiding researchers in conducting detailed manual investigations. This work contributes to improving the interpretability and resilience of deep learning-based intrusion detection systems         ",
    "url": "https://arxiv.org/abs/2503.01391",
    "authors": [
      "Matteo Brosolo",
      "Vinod Puthuvath",
      "Mauro Conti"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.01394",
    "title": "Enhancing Social Media Rumor Detection: A Semantic and Graph Neural Network Approach for the 2024 Global Election",
    "abstract": "           The development of social media platforms has revolutionized the speed and manner in which information is disseminated, leading to both beneficial and detrimental effects on society. While these platforms facilitate rapid communication, they also accelerate the spread of rumors and extremist speech, impacting public perception and behavior significantly. This issue is particularly pronounced during election periods, where the influence of social media on election outcomes has become a matter of global concern. With the unprecedented number of elections in 2024, against this backdrop, the election ecosystem has encountered unprecedented challenges. This study addresses the urgent need for effective rumor detection on social media by proposing a novel method that combines semantic analysis with graph neural networks. We have meticulously collected a dataset from PolitiFact and Twitter, focusing on politically relevant rumors. Our approach involves semantic analysis using a fine-tuned BERT model to vectorize text content and construct a directed graph where tweets and comments are nodes, and interactions are edges. The core of our method is a graph neural network, SAGEWithEdgeAttention, which extends the GraphSAGE model by incorporating first-order differences as edge attributes and applying an attention mechanism to enhance feature aggregation. This innovative approach allows for the fine-grained analysis of the complex social network structure, improving rumor detection accuracy. The study concludes that our method significantly outperforms traditional content analysis and time-based models, offering a theoretically sound and practically efficient solution.         ",
    "url": "https://arxiv.org/abs/2503.01394",
    "authors": [
      "Liu Yan",
      "Liu Yunpeng",
      "Zhao Liang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01395",
    "title": "Jailbreaking Generative AI: Empowering Novices to Conduct Phishing Attacks",
    "abstract": "           The rapid advancements in generative AI models, such as ChatGPT, have introduced both significant benefits and new risks within the cybersecurity landscape. This paper investigates the potential misuse of the latest AI model, ChatGPT-4o Mini, in facilitating social engineering attacks, with a particular focus on phishing, one of the most pressing cybersecurity threats today. While existing literature primarily addresses the technical aspects, such as jailbreaking techniques, none have fully explored the free and straightforward execution of a comprehensive phishing campaign by novice users using ChatGPT-4o Mini. In this study, we examine the vulnerabilities of AI-driven chatbot services in 2025, specifically how methods like jailbreaking and reverse psychology can bypass ethical safeguards, allowing ChatGPT to generate phishing content, suggest hacking tools, and assist in carrying out phishing attacks. Our findings underscore the alarming ease with which even inexperienced users can execute sophisticated phishing campaigns, emphasizing the urgent need for stronger cybersecurity measures and heightened user awareness in the age of AI.         ",
    "url": "https://arxiv.org/abs/2503.01395",
    "authors": [
      "Rina Mishra",
      "Gaurav Varshney",
      "Shreya Singh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.01396",
    "title": "CorrNetDroid: Android Malware Detector leveraging a Correlation-based Feature Selection for Network Traffic features",
    "abstract": "           Copious mobile operating systems exist in the market, but Android remains the user's choice. Meanwhile, its growing popularity has also attracted malware developers. Researchers have proposed various static solutions for Android malware detection. However, stealthier malware evade static analysis. This raises the need for a robust Android malware detection system capable of dealing with advanced threats and overcoming the shortcomings of static analysis. Hence, this work proposes a dynamic analysis-based Android malware detection system, CorrNetDroid, that works over network traffic flows. Many traffic features exhibit overlapping ranges in normal and malware datasets. Therefore, we first rank the features using two statistical measures, crRelevance and Normalized Mean Residue Similarity (NMRS), to assess feature-class and feature-feature correlations. Thereafter, we introduce a novel correlation-based feature selection algorithm that applies NMRS on crRelevance rankings to identify the optimal feature subset for Android malware detection. Experimental results highlight that our model effectively reduces the feature set while detecting Android malware with 99.50 percent accuracy when considering only two network traffic features. Furthermore, our experiments demonstrate that the NMRS-based algorithm on crRelevance rankings outperforms statistical tests such as chi-square, ANOVA, Mann-Whitney U test, and Kruskal-Wallis test. In addition, our model surpasses various state-of-the-art Android malware detection techniques in terms of detection accuracy.         ",
    "url": "https://arxiv.org/abs/2503.01396",
    "authors": [
      "Yash Sharma",
      "Anshul Arora"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2503.01407",
    "title": "Divide and Conquer: Heterogeneous Noise Integration for Diffusion-based Adversarial Purification",
    "abstract": "           Existing diffusion-based purification methods aim to disrupt adversarial perturbations by introducing a certain amount of noise through a forward diffusion process, followed by a reverse process to recover clean examples. However, this approach is fundamentally flawed: the uniform operation of the forward process across all pixels compromises normal pixels while attempting to combat adversarial perturbations, resulting in the target model producing incorrect predictions. Simply relying on low-intensity noise is insufficient for effective defense. To address this critical issue, we implement a heterogeneous purification strategy grounded in the interpretability of neural networks. Our method decisively applies higher-intensity noise to specific pixels that the target model focuses on while the remaining pixels are subjected to only low-intensity noise. This requirement motivates us to redesign the sampling process of the diffusion model, allowing for the effective removal of varying noise levels. Furthermore, to evaluate our method against strong adaptative attack, our proposed method sharply reduces time cost and memory usage through a single-step resampling. The empirical evidence from extensive experiments across three datasets demonstrates that our method outperforms most current adversarial training and purification techniques by a substantial margin.         ",
    "url": "https://arxiv.org/abs/2503.01407",
    "authors": [
      "Gaozheng Pei",
      "Shaojie Lyu",
      "Gong Chen",
      "Ke Ma",
      "Qianqian Xu",
      "Yingfei Sun",
      "Qingming Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01434",
    "title": "RUSSO: Robust Underwater SLAM with Sonar Optimization against Visual Degradation",
    "abstract": "           Visual degradation in underwater environments poses unique and significant challenges, which distinguishes underwater SLAM from popular vision-based SLAM on the ground. In this paper, we propose RUSSO, a robust underwater SLAM system which fuses stereo camera, inertial measurement unit (IMU), and imaging sonar to achieve robust and accurate localization in challenging underwater environments for 6 degrees of freedom (DoF) estimation. During visual degradation, the system is reduced to a sonar-inertial system estimating 3-DoF poses. The sonar pose estimation serves as a strong prior for IMU propagation, thereby enhancing the reliability of pose estimation with IMU propagation. Additionally, we propose a SLAM initialization method that leverages the imaging sonar to counteract the lack of visual features during the initialization stage of SLAM. We extensively validate RUSSO through experiments in simulator, pool, and sea scenarios. The results demonstrate that RUSSO achieves better robustness and localization accuracy compared to the state-of-the-art visual-inertial SLAM systems, especially in visually challenging scenarios. To the best of our knowledge, this is the first time fusing stereo camera, IMU, and imaging sonar to realize robust underwater SLAM against visual degradation.         ",
    "url": "https://arxiv.org/abs/2503.01434",
    "authors": [
      "Shu Pan",
      "Ziyang Hong",
      "Zhangrui Hu",
      "Xiandong Xu",
      "Wenjie Lu",
      "Liang Hu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.01436",
    "title": "Fall Detection from Indoor Videos using MediaPipe and Handcrafted Feature",
    "abstract": "           Falls are a common cause of fatal injuries and hospitalization. However, having fall detection on person, in particular for senior citizens can prove to be critical. Presently,there are handheld, ambient detector and vision-based detection techniques being utilized for fall detection. However, the approaches have issues with accuracy and cost. In this regard, in this research, an approach is proposed to detect falls in indoor environments utilizing the handcrafted features extracted from human body skeleton. The human body skeleton is formed using MediaPipe framework. Results on UR Fall detection show the superiority of our model, capable of detecting falls correctly in a wide number of settings involving people belonging to different ages and genders. This proposed model using MediaPipe for fall classification in daily activities achieves significant accuracy compare to the present existing approaches.         ",
    "url": "https://arxiv.org/abs/2503.01436",
    "authors": [
      "Fatima Ahmed",
      "Parag Biswas",
      "Abdur Rashid",
      "Md. Khaliluzzaman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01437",
    "title": "Eau De $Q$-Network: Adaptive Distillation of Neural Networks in Deep Reinforcement Learning",
    "abstract": "           Recent works have successfully demonstrated that sparse deep reinforcement learning agents can be competitive against their dense counterparts. This opens up opportunities for reinforcement learning applications in fields where inference time and memory requirements are cost-sensitive or limited by hardware. Until now, dense-to-sparse methods have relied on hand-designed sparsity schedules that are not synchronized with the agent's learning pace. Crucially, the final sparsity level is chosen as a hyperparameter, which requires careful tuning as setting it too high might lead to poor performances. In this work, we address these shortcomings by crafting a dense-to-sparse algorithm that we name Eau De $Q$-Network (EauDeQN). To increase sparsity at the agent's learning pace, we consider multiple online networks with different sparsity levels, where each online network is trained from a shared target network. At each target update, the online network with the smallest loss is chosen as the next target network, while the other networks are replaced by a pruned version of the chosen network. We evaluate the proposed approach on the Atari $2600$ benchmark and the MuJoCo physics simulator, showing that EauDeQN reaches high sparsity levels while keeping performances high.         ",
    "url": "https://arxiv.org/abs/2503.01437",
    "authors": [
      "Th\u00e9o Vincent",
      "Tim Faust",
      "Yogesh Tripathi",
      "Jan Peters",
      "Carlo D'Eramo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01438",
    "title": "CAO-RONet: A Robust 4D Radar Odometry with Exploring More Information from Low-Quality Points",
    "abstract": "           Recently, 4D millimetre-wave radar exhibits more stable perception ability than LiDAR and camera under adverse conditions (e.g. rain and fog). However, low-quality radar points hinder its application, especially the odometry task that requires a dense and accurate matching. To fully explore the potential of 4D radar, we introduce a learning-based odometry framework, enabling robust ego-motion estimation from finite and uncertain geometry information. First, for sparse radar points, we propose a local completion to supplement missing structures and provide denser guideline for aligning two frames. Then, a context-aware association with a hierarchical structure flexibly matches points of different scales aided by feature similarity, and improves local matching consistency through correlation balancing. Finally, we present a window-based optimizer that uses historical priors to establish a coupling state estimation and correct errors of inter-frame matching. The superiority of our algorithm is confirmed on View-of-Delft dataset, achieving around a 50% performance improvement over previous approaches and delivering accuracy on par with LiDAR odometry. Our code will be available.         ",
    "url": "https://arxiv.org/abs/2503.01438",
    "authors": [
      "Zhiheng Li",
      "Yubo Cui",
      "Ningyuan Huang",
      "Chenglin Pang",
      "Zheng Fang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.01442",
    "title": "Leveraging LLMs for Mental Health: Detection and Recommendations from Social Discussions",
    "abstract": "           Textual data from social platforms captures various aspects of mental health through discussions around and across issues, while users reach out for help and others sympathize and offer support. We propose a comprehensive framework that leverages Natural Language Processing (NLP) and Generative AI techniques to identify and assess mental health disorders, detect their severity, and create recommendations for behavior change and therapeutic interventions based on users' posts on Reddit. To classify the disorders, we use rule-based labeling methods as well as advanced pre-trained NLP models to extract nuanced semantic features from the data. We fine-tune domain-adapted and generic pre-trained NLP models based on predictions from specialized Large Language Models (LLMs) to improve classification accuracy. Our hybrid approach combines the generalization capabilities of pre-trained models with the domain-specific insights captured by LLMs, providing an improved understanding of mental health discourse. Our findings highlight the strengths and limitations of each model, offering valuable insights into their practical applicability. This research potentially facilitates early detection and personalized care to aid practitioners and aims to facilitate timely interventions and improve overall well-being, thereby contributing to the broader field of mental health surveillance and digital health analytics.         ",
    "url": "https://arxiv.org/abs/2503.01442",
    "authors": [
      "Vaishali Aggarwal",
      "Sachin Thukral",
      "Krushil Patel",
      "Arnab Chatterjee"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.01449",
    "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
    "abstract": "           Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.         ",
    "url": "https://arxiv.org/abs/2503.01449",
    "authors": [
      "Ting Zhang",
      "Chengran Yang",
      "Yindu Su",
      "Martin Weyssow",
      "Hung Nguyen",
      "Tan Bui",
      "Hong Jin Kang",
      "Yikun Li",
      "Eng Lieh Ouh",
      "Lwin Khin Shar",
      "David Lo"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.01463",
    "title": "MI-DETR: An Object Detection Model with Multi-time Inquiries Mechanism",
    "abstract": "           Based on analyzing the character of cascaded decoder architecture commonly adopted in existing DETR-like models, this paper proposes a new decoder architecture. The cascaded decoder architecture constrains object queries to update in the cascaded direction, only enabling object queries to learn relatively-limited information from image features. However, the challenges for object detection in natural scenes (e.g., extremely-small, heavily-occluded, and confusingly mixed with the background) require an object detection model to fully utilize image features, which motivates us to propose a new decoder architecture with the parallel Multi-time Inquiries (MI) mechanism. MI enables object queries to learn more comprehensive information, and our MI based model, MI-DETR, outperforms all existing DETR-like models on COCO benchmark under different backbones and training epochs, achieving +2.3 AP and +0.6 AP improvements compared to the most representative model DINO and SOTA model Relation-DETR under ResNet-50 backbone. In addition, a series of diagnostic and visualization experiments demonstrate the effectiveness, rationality, and interpretability of MI.         ",
    "url": "https://arxiv.org/abs/2503.01463",
    "authors": [
      "Zhixiong Nan",
      "Xianghong Li",
      "Jifeng Dai",
      "Tao Xiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01469",
    "title": "Hierarchical Causal Transformer with Heterogeneous Information for Expandable Sequential Recommendation",
    "abstract": "           Sequential recommendation systems leveraging transformer architectures have demonstrated exceptional capabilities in capturing user behavior patterns. At the core of these systems lies the critical challenge of constructing effective item representations. Traditional approaches employ feature fusion through simple concatenation or basic neural architectures to create uniform representation sequences. However, these conventional methods fail to address the intrinsic diversity of item attributes, thereby constraining the transformer's capacity to discern fine-grained patterns and hindering model extensibility. Although recent research has begun incorporating user-related heterogeneous features into item sequences, the equally crucial item-side heterogeneous feature continue to be neglected. To bridge this methodological gap, we present HeterRec - an innovative framework featuring two novel components: the Heterogeneous Token Flattening Layer (HTFL) and Hierarchical Causal Transformer (HCT). HTFL pioneers a sophisticated tokenization mechanism that decomposes items into multi-dimensional token sets and structures them into heterogeneous sequences, enabling scalable performance enhancement through model expansion. The HCT architecture further enhances pattern discovery through token-level and item-level attention mechanisms. furthermore, we develop a Listwise Multi-step Prediction (LMP) objective function to optimize learning process. Rigorous validation, including real-world industrial platforms, confirms HeterRec's state-of-the-art performance in both effective and efficiency.         ",
    "url": "https://arxiv.org/abs/2503.01469",
    "authors": [
      "Hao Deng",
      "Haibo Xing",
      "Kanefumi Matsuyama",
      "Yulei Huang",
      "Jinxin Hu",
      "Hong Wen",
      "Jia Xu",
      "Zulong Chen",
      "Yu Zhang",
      "Xiaoyi Zeng",
      "Jing Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.01470",
    "title": "Position: Ensuring mutual privacy is necessary for effective external evaluation of proprietary AI systems",
    "abstract": "           The external evaluation of AI systems is increasingly recognised as a crucial approach for understanding their potential risks. However, facilitating external evaluation in practice faces significant challenges in balancing evaluators' need for system access with AI developers' privacy and security concerns. Additionally, evaluators have reason to protect their own privacy - for example, in order to maintain the integrity of held-out test sets. We refer to the challenge of ensuring both developers' and evaluators' privacy as one of providing mutual privacy. In this position paper, we argue that (i) addressing this mutual privacy challenge is essential for effective external evaluation of AI systems, and (ii) current methods for facilitating external evaluation inadequately address this challenge, particularly when it comes to preserving evaluators' privacy. In making these arguments, we formalise the mutual privacy problem; examine the privacy and access requirements of both model owners and evaluators; and explore potential solutions to this challenge, including through the application of cryptographic and hardware-based approaches.         ",
    "url": "https://arxiv.org/abs/2503.01470",
    "authors": [
      "Ben Bucknall",
      "Robert F. Trager",
      "Michael A. Osborne"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.01475",
    "title": "ProRCA: A Causal Python Package for Actionable Root Cause Analysis in Real-world Business Scenarios",
    "abstract": "           Root Cause Analysis (RCA) is becoming ever more critical as modern systems grow in complexity, volume of data, and interdependencies. While traditional RCA methods frequently rely on correlation-based or rule-based techniques, these approaches can prove inadequate in highly dynamic, multi-layered environments. In this paper, we present a pathway-tracing package built on the DoWhy causal inference library. Our method integrates conditional anomaly scoring, noise-based attribution, and depth-first path exploration to reveal multi-hop causal chains. By systematically tracing entire causal pathways from an observed anomaly back to the initial triggers, our approach provides a comprehensive, end-to-end RCA solution. Experimental evaluations with synthetic anomaly injections demonstrate the package's ability to accurately isolate triggers and rank root causes by their overall significance.         ",
    "url": "https://arxiv.org/abs/2503.01475",
    "authors": [
      "Ahmed Dawoud",
      "Shravan Talupula"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01482",
    "title": "Revisiting Locally Differentially Private Protocols: Towards Better Trade-offs in Privacy, Utility, and Attack Resistance",
    "abstract": "           Local Differential Privacy (LDP) offers strong privacy protection, especially in settings in which the server collecting the data is untrusted. However, designing LDP mechanisms that achieve an optimal trade-off between privacy, utility, and robustness to adversarial inference attacks remains challenging. In this work, we introduce a general multi-objective optimization framework for refining LDP protocols, enabling the joint optimization of privacy and utility under various adversarial settings. While our framework is flexible enough to accommodate multiple privacy and security attacks as well as utility metrics, in this paper we specifically optimize for Attacker Success Rate (ASR) under distinguishability attack as a measure of privacy and Mean Squared Error (MSE) as a measure of utility. We systematically revisit these trade-offs by analyzing eight state-of-the-art LDP protocols and proposing refined counterparts that leverage tailored optimization techniques. Experimental results demonstrate that our proposed adaptive mechanisms consistently outperform their non-adaptive counterparts, reducing ASR by up to five orders of magnitude while maintaining competitive utility. Analytical derivations also confirm the effectiveness of our mechanisms, moving them closer to the ASR-MSE Pareto frontier.         ",
    "url": "https://arxiv.org/abs/2503.01482",
    "authors": [
      "H\u00e9ber H. Arcolezi",
      "S\u00e9bastien Gambs"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.01488",
    "title": "InversionGNN: A Dual Path Network for Multi-Property Molecular Optimization",
    "abstract": "           Exploring chemical space to find novel molecules that simultaneously satisfy multiple properties is crucial in drug discovery. However, existing methods often struggle with trading off multiple properties due to the conflicting or correlated nature of chemical properties. To tackle this issue, we introduce InversionGNN framework, an effective yet sample-efficient dual-path graph neural network (GNN) for multi-objective drug discovery. In the direct prediction path of InversionGNN, we train the model for multi-property prediction to acquire knowledge of the optimal combination of functional groups. Then the learned chemical knowledge helps the inversion generation path to generate molecules with required properties. In order to decode the complex knowledge of multiple properties in the inversion path, we propose a gradient-based Pareto search method to balance conflicting properties and generate Pareto optimal molecules. Additionally, InversionGNN is able to search the full Pareto front approximately in discrete chemical space. Comprehensive experimental evaluations show that InversionGNN is both effective and sample-efficient in various discrete multi-objective settings including drug discovery.         ",
    "url": "https://arxiv.org/abs/2503.01488",
    "authors": [
      "Yifan Niu",
      "Ziqi Gao",
      "Tingyang Xu",
      "Yang Liu",
      "Yatao Bian",
      "Yu Rong",
      "Junzhou Huang",
      "Jia Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01532",
    "title": "Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in Power-Disparate Social Scenarios",
    "abstract": "           Large language models (LLMs) have demonstrated remarkable capabilities in simulating human behaviour and social intelligence. However, they risk perpetuating societal biases, especially when demographic information is involved. We introduce a novel framework using cosine distance to measure semantic shifts in responses and an LLM-judged Preference Win Rate (WR) to assess how demographic prompts affect response quality across power-disparate social scenarios. Evaluating five LLMs over 100 diverse social scenarios and nine demographic axes, our findings suggest a \"default persona\" bias toward middle-aged, able-bodied, native-born, Caucasian, atheistic males with centrist views. Moreover, interactions involving specific demographics are associated with lower-quality responses. Lastly, the presence of power disparities increases variability in response semantics and quality across demographic groups, suggesting that implicit biases may be heightened under power-imbalanced conditions. These insights expose the demographic biases inherent in LLMs and offer potential paths toward future bias mitigation efforts in LLMs.         ",
    "url": "https://arxiv.org/abs/2503.01532",
    "authors": [
      "Bryan Chen Zhengyu Tan",
      "Roy Ka-Wei Lee"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2503.01538",
    "title": "Formally Discovering and Reproducing Network Protocols Vulnerabilities",
    "abstract": "           The rapid evolution of cyber threats has increased the need for robust methods to discover vulnerabilities in increasingly complex and diverse network protocols. This paper introduces Network Attack-centric Compositional Testing (NACT), a novel methodology designed to discover new vulnerabilities in network protocols and create scenarios to reproduce these vulnerabilities through attacker models. NACT integrates composable attacker specifications, formal specification mutations, and randomized constraint-solving techniques to generate sophisticated attack scenarios and test cases. The methodology enables comprehensive testing of both single-protocol and multi-protocol interactions. Through case studies involving a custom minimalist protocol (MiniP) and five widely used QUIC implementations, NACT is shown to effectively identify, reproduce, and find new real-world vulnerabilities such as version negotiation abuse. Additionally, by comparing the current and older versions of these QUIC implementations, NACT demonstrates its ability to detect both persistent vulnerabilities and regressions. Finally, by supporting cross-protocol testing within a black-box testing framework, NACT provides a versatile approach to improve the security of network protocols.         ",
    "url": "https://arxiv.org/abs/2503.01538",
    "authors": [
      "Christophe Crochet",
      "John Aoga",
      "Axel Legay"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.01549",
    "title": "Patterning Silver Nanowire Network via the Gibbs-Thomson Effect",
    "abstract": "           As transparent electrodes, patterned silver nanowire (AgNW) networks suffer from noticeable pattern visibility, which is an unsettled issue for practical applications such as display. Here, we introduce a Gibbs-Thomson effect (GTE)-based patterning method to effectively reduce pattern visibility. Unlike conventional top-down and bottom-up strategies that rely on selective etching, removal, or deposition of AgNWs, our approach focuses on fragmenting nanowires primarily at the junctions through the GTE. This is realized by modifying AgNWs with a compound of diphenyliodonium nitrate and silver nitrate, which aggregates into nanoparticles at the junctions of AgNWs. These nanoparticles can boost the fragmentation of nanowires at the junctions under an ultralow temperature (75\u00b0C), allow pattern transfer through a photolithographic masking operation, and enhance plasmonic welding during UV exposure. The resultant patterned electrodes have trivial differences in transmittance ({\\Delta}T = 1.4%) and haze ({\\Delta}H = 0.3%) between conductive and insulative regions, with high-resolution patterning size down to 10 {\\mu}m. To demonstrate the practicality of this novel method, we constructed a highly transparent, optoelectrical interactive tactile e-skin using the patterned AgNW electrodes.         ",
    "url": "https://arxiv.org/abs/2503.01549",
    "authors": [
      "Hongteng Wang",
      "Haichuan Li",
      "Yijia Xin",
      "Weizhen Chen",
      "Haogen Liu",
      "Ying Chen",
      "Yaofei Chen",
      "Lei Chen",
      "Yunhan Luo",
      "Zhe Chen",
      "Gui-Shi Liu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.01556",
    "title": "Effective High-order Graph Representation Learning for Credit Card Fraud Detection",
    "abstract": "           Credit card fraud imposes significant costs on both cardholders and issuing banks. Fraudsters often disguise their crimes, such as using legitimate transactions through several benign users to bypass anti-fraud detection. Existing graph neural network (GNN) models struggle with learning features of camouflaged, indirect multi-hop transactions due to their inherent over-smoothing issues in deep multi-layer aggregation, presenting a major challenge in detecting disguised relationships. Therefore, in this paper, we propose a novel High-order Graph Representation Learning model (HOGRL) to avoid incorporating excessive noise during the multi-layer aggregation process. In particular, HOGRL learns different orders of \\emph{pure} representations directly from high-order transaction graphs. We realize this goal by effectively constructing high-order transaction graphs first and then learning the \\emph{pure} representations of each order so that the model could identify fraudsters' multi-hop indirect transactions via multi-layer \\emph{pure} feature learning. In addition, we introduce a mixture-of-expert attention mechanism to automatically determine the importance of different orders for jointly optimizing fraud detection performance. We conduct extensive experiments in both the open source and real-world datasets, the result demonstrates the significant improvements of our proposed HOGRL compared with state-of-the-art fraud detection baselines. HOGRL's superior performance also proves its effectiveness in addressing high-order fraud camouflage criminals.         ",
    "url": "https://arxiv.org/abs/2503.01556",
    "authors": [
      "Yao Zou",
      "Dawei Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01557",
    "title": "MoCFL: Mobile Cluster Federated Learning Framework for Highly Dynamic Network",
    "abstract": "           Frequent fluctuations of client nodes in highly dynamic mobile clusters can lead to significant changes in feature space distribution and data drift, posing substantial challenges to the robustness of existing federated learning (FL) strategies. To address these issues, we proposed a mobile cluster federated learning framework (MoCFL). MoCFL enhances feature aggregation by introducing an affinity matrix that quantifies the similarity between local feature extractors from different clients, addressing dynamic data distribution changes caused by frequent client churn and topology changes. Additionally, MoCFL integrates historical and current feature information when training the global classifier, effectively mitigating the catastrophic forgetting problem frequently encountered in mobile scenarios. This synergistic combination ensures that MoCFL maintains high performance and stability in dynamically changing mobile environments. Experimental results on the UNSW-NB15 dataset show that MoCFL excels in dynamic environments, demonstrating superior robustness and accuracy while maintaining reasonable training costs.         ",
    "url": "https://arxiv.org/abs/2503.01557",
    "authors": [
      "Kai Fang",
      "Jiangtao Deng",
      "Chengzu Dong",
      "Usman Naseem",
      "Tongcun Liu",
      "Hailin Feng",
      "Wei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01561",
    "title": "A Reconfigurable Stream-Based FPGA Accelerator for Bayesian Confidence Propagation Neural Networks",
    "abstract": "           Brain-inspired algorithms are attractive and emerging alternatives to classical deep learning methods for use in various machine learning applications. Brain-inspired systems can feature local learning rules, both unsupervised/semi-supervised learning and different types of plasticity (structural/synaptic), allowing them to potentially be faster and more energy-efficient than traditional machine learning alternatives. Among the more salient brain-inspired algorithms are Bayesian Confidence Propagation Neural Networks (BCPNNs). BCPNN is an important tool for both machine learning and computational neuroscience research, and recent work shows that BCPNN can reach state-of-the-art performance in tasks such as learning and memory recall compared to other models. Unfortunately, BCPNN is primarily executed on slow general-purpose processors (CPUs) or power-hungry graphics processing units (GPUs), reducing the applicability of using BCPNN in (among others) Edge systems. In this work, we design a custom stream-based accelerator for BCPNN using Field-Programmable Gate Arrays (FPGA) using Xilinx Vitis High-Level Synthesis (HLS) flow. Furthermore, we model our accelerator's performance using first principles, and we empirically show that our proposed accelerator is between 1.3x - 5.3x faster than an Nvidia A100 GPU while at the same time consuming between 2.62x - 3.19x less power and 5.8x - 16.5x less energy without any degradation in performance.         ",
    "url": "https://arxiv.org/abs/2503.01561",
    "authors": [
      "Muhammad Ihsan Al Hafiz",
      "Naresh Ravichandran",
      "Anders Lansner",
      "Pawel Herman",
      "Artur Podobas"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.01569",
    "title": "Meta Learning-Driven Iterative Refinement for Robust Anomaly Detection in Industrial Inspection",
    "abstract": "           This study investigates the performance of robust anomaly detection models in industrial inspection, focusing particularly on their ability to handle noisy data. We propose to leverage the adaptation ability of meta learning approaches to identify and reject noisy training data to improve the learning process. In our model, we employ Model Agnostic Meta Learning (MAML) and an iterative refinement process through an Inter-Quartile Range rejection scheme to enhance their adaptability and robustness. This approach significantly improves the models capability to distinguish between normal and defective conditions. Our results of experiments conducted on well known MVTec and KSDD2 datasets demonstrate that the proposed method not only excels in environments with substantial noise but can also contribute in case of a clear training set, isolating those samples that are relatively out of distribution, thus offering significant improvements over traditional models.         ",
    "url": "https://arxiv.org/abs/2503.01569",
    "authors": [
      "Muhammad Aqeel",
      "Shakiba Sharifi",
      "Marco Cristani",
      "Francesco Setti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01571",
    "title": "MLINE-VINS: Robust Monocular Visual-Inertial SLAM With Flow Manhattan and Line Features",
    "abstract": "           In this paper we introduce MLINE-VINS, a novel monocular visual-inertial odometry (VIO) system that leverages line features and Manhattan Word assumption. Specifically, for line matching process, we propose a novel geometric line optical flow algorithm that efficiently tracks line features with varying lengths, whitch is do not require detections and descriptors in every frame. To address the instability of Manhattan estimation from line features, we propose a tracking-by-detection module that consistently tracks and optimizes Manhattan framse in consecutive images. By aligning the Manhattan World with the VIO world frame, the tracking could restart using the latest pose from back-end, simplifying the coordinate transformations within the system. Furthermore, we implement a mechanism to validate Manhattan frames and a novel global structural constraints back-end optimization. Extensive experiments results on vairous datasets, including benchmark and self-collected datasets, show that the proposed approach outperforms existing methods in terms of accuracy and long-range robustness. The source code of our method is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2503.01571",
    "authors": [
      "Chao Ye",
      "Haoyuan Li",
      "Weiyang Lin",
      "Xianqiang Yang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.01580",
    "title": "A Selective Learning Method for Temporal Graph Continual Learning",
    "abstract": "           Node classification is a key task in temporal graph learning (TGL). Real-life temporal graphs often introduce new node classes over time, but existing TGL methods assume a fixed set of classes. This assumption brings limitations, as updating models with full data is costly, while focusing only on new classes results in forgetting old ones. Graph continual learning (GCL) methods mitigate forgetting using old-class subsets but fail to account for their evolution. We define this novel problem as temporal graph continual learning (TGCL), which focuses on efficiently maintaining up-to-date knowledge of old classes. To tackle TGCL, we propose a selective learning framework that substitutes the old-class data with its subsets, Learning Towards the Future (LTF). We derive an upper bound on the error caused by such replacement and transform it into objectives for selecting and learning subsets that minimize classification error while preserving the distribution of the full old-class data. Experiments on three real-world datasets validate the effectiveness of LTF on TGCL.         ",
    "url": "https://arxiv.org/abs/2503.01580",
    "authors": [
      "Hanmo Liu",
      "Shimin Di",
      "Haoyang Li",
      "Xun Jian",
      "Yue Wang",
      "Lei Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01601",
    "title": "Evaluating Stenosis Detection with Grounding DINO, YOLO, and DINO-DETR",
    "abstract": "           Detecting stenosis in coronary angiography is vital for diagnosing and managing cardiovascular diseases. This study evaluates the performance of state-of-the-art object detection models on the ARCADE dataset using the MMDetection framework. The models are assessed using COCO evaluation metrics, including Intersection over Union (IoU), Average Precision (AP), and Average Recall (AR). Results indicate variations in detection accuracy across different models, attributed to differences in algorithmic design, transformer-based vs. convolutional architectures. Additionally, several challenges were encountered during implementation, such as compatibility issues between PyTorch, CUDA, and MMDetection, as well as dataset inconsistencies in ARCADE. The findings provide insights into model selection for stenosis detection and highlight areas for further improvement in deep learning-based coronary artery disease diagnosis.         ",
    "url": "https://arxiv.org/abs/2503.01601",
    "authors": [
      "Muhammad Musab Ansari"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01605",
    "title": "A Leaf-Level Dataset for Soybean-Cotton Detection and Segmentation",
    "abstract": "           Soybean and cotton are major drivers of many countries' agricultural sectors, offering substantial economic returns but also facing persistent challenges from volunteer plants and weeds that hamper sustainable management. Effectively controlling volunteer plants and weeds demands advanced recognition strategies that can identify these amidst complex crop canopies. While deep learning methods have demonstrated promising results for leaf-level detection and segmentation, existing datasets often fail to capture the complexity of real-world agricultural fields. To address this, we collected 640 high-resolution images from a commercial farm spanning multiple growth stages, weed pressures, and lighting variations. Each image is annotated at the leaf-instance level, with 7,221 soybean and 5,190 cotton leaves labeled via bounding boxes and segmentation masks, capturing overlapping foliage, small leaf size, and morphological similarities. We validate this dataset using YOLOv11, demonstrating state-of-the-art performance in accurately identifying and segmenting overlapping foliage. Our publicly available dataset supports advanced applications such as selective herbicide spraying and pest monitoring and can foster more robust, data-driven strategies for soybean-cotton management.         ",
    "url": "https://arxiv.org/abs/2503.01605",
    "authors": [
      "Thiago H. Segreto",
      "Juliano Negri",
      "Paulo H. Polegato",
      "Jo\u00e3o Manoel Herrera Pinheiro",
      "Ricardo Godoy",
      "Marcelo Becker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01606",
    "title": "Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering",
    "abstract": "           Large language models have recently pushed open domain question answering (ODQA) to new frontiers. However, prevailing retriever-reader pipelines often depend on multiple rounds of prompt level instructions, leading to high computational overhead, instability, and suboptimal retrieval coverage. In this paper, we propose EmbQA, an embedding-level framework that alleviates these shortcomings by enhancing both the retriever and the reader. Specifically, we refine query representations via lightweight linear layers under an unsupervised contrastive learning objective, thereby reordering retrieved passages to highlight those most likely to contain correct answers. Additionally, we introduce an exploratory embedding that broadens the model's latent semantic space to diversify candidate generation and employs an entropy-based selection mechanism to choose the most confident answer automatically. Extensive experiments across three open-source LLMs, three retrieval methods, and four ODQA benchmarks demonstrate that EmbQA substantially outperforms recent baselines in both accuracy and efficiency.         ",
    "url": "https://arxiv.org/abs/2503.01606",
    "authors": [
      "Zhanghao Hu",
      "Hanqi Yan",
      "Qingling Zhu",
      "Zhenyi Shen",
      "Yulan He",
      "Lin Gui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01612",
    "title": "Robust Palm-Vein Recognition Using the MMD Filter: Improving SIFT-Based Feature Matching",
    "abstract": "           A major challenge with palm vein images is that slight movements of the fingers and thumb, or variations in hand posture, can stretch the skin in different areas and alter the vein patterns. This can result in an infinite number of variations in palm vein images for a given individual. This paper introduces a novel filtering technique for SIFT-based feature matching, known as the Mean and Median Distance (MMD) Filter. This method evaluates the differences in keypoint coordinates and computes the mean and median in each direction to eliminate incorrect matches. Experiments conducted on the 850nm subset of the CASIA dataset indicate that the proposed MMD filter effectively preserves correct points while reducing false positives detected by other filtering methods. A comparison with existing SIFT-based palm vein recognition systems demonstrates that the proposed MMD filter delivers outstanding performance, achieving lower Equal Error Rate (EER) values. This article presents an extended author's version based on our previous work, A Keypoint Filtering Method for SIFT based Palm-Vein Recognition.         ",
    "url": "https://arxiv.org/abs/2503.01612",
    "authors": [
      "Kaveen Perera",
      "Fouad Khelifi",
      "Ammar Belatreche"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01618",
    "title": "Energy-Dissipative Evolutionary Kolmogorov-Arnold Networks for Complex PDE Systems",
    "abstract": "           We introduce evolutionary Kolmogorov-Arnold Networks (EvoKAN), a novel framework for solving complex partial differential equations (PDEs). EvoKAN builds on Kolmogorov-Arnold Networks (KANs), where activation functions are spline based and trainable on each edge, offering localized flexibility across multiple scales. Rather than retraining the network repeatedly, EvoKAN encodes only the PDE's initial state during an initial learning phase. The network parameters then evolve numerically, governed by the same PDE, without any additional optimization. By treating these parameters as continuous functions in the relevant coordinates and updating them through time steps, EvoKAN can predict system trajectories over arbitrarily long horizons, a notable challenge for many conventional neural-network based methods. In addition, EvoKAN integrates the scalar auxiliary variable (SAV) method to guarantee unconditional energy stability and computational efficiency. At individual time step, SAV only needs to solve decoupled linear systems with constant coefficients, the implementation is significantly simplified. We test the proposed framework in several complex PDEs, including one dimensional and two dimensional Allen-Cahn equations and two dimensional Navier-Stokes equations. Numerical results show that EvoKAN solutions closely match analytical references and established numerical benchmarks, effectively capturing both phase-field phenomena (Allen-Cahn) and turbulent flows (Navier-Stokes).         ",
    "url": "https://arxiv.org/abs/2503.01618",
    "authors": [
      "Guang Lin",
      "Changhong Mou",
      "Jiahao Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2503.01642",
    "title": "Graph-Augmented Reasoning: Evolving Step-by-Step Knowledge Graph Retrieval for LLM Reasoning",
    "abstract": "           Recent large language model (LLM) reasoning, despite its success, suffers from limited domain knowledge, susceptibility to hallucinations, and constrained reasoning depth, particularly in small-scale models deployed in resource-constrained environments. This paper presents the first investigation into integrating step-wise knowledge graph retrieval with step-wise reasoning to address these challenges, introducing a novel paradigm termed as graph-augmented reasoning. Our goal is to enable frozen, small-scale LLMs to retrieve and process relevant mathematical knowledge in a step-wise manner, enhancing their problem-solving abilities without additional training. To this end, we propose KG-RAR, a framework centered on process-oriented knowledge graph construction, a hierarchical retrieval strategy, and a universal post-retrieval processing and reward model (PRP-RM) that refines retrieved information and evaluates each reasoning step. Experiments on the Math500 and GSM8K benchmarks across six models demonstrate that KG-RAR yields encouraging results, achieving a 20.73\\% relative improvement with Llama-3B on Math500.         ",
    "url": "https://arxiv.org/abs/2503.01642",
    "authors": [
      "Wenjie Wu",
      "Yongcheng Jing",
      "Yingjie Wang",
      "Wenbin Hu",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01643",
    "title": "Error estimates of asymptotic-preserving neural networks in approximating stochastic linearized Boltzmann equation",
    "abstract": "           In this paper, we construct an asymptotic-preserving neural networks (APNNs) [21] for the linearized Boltzmann equation in the acoustic scaling and with uncertain parameters. Utilizing the micro-macro decomposition, we design the loss function based on the stochastic-Galerkin system conducted from the micro-macro equations. Rigorous analysis is provided to show the capability of neural networks in approximating solutions near the global Maxwellian. By employing hypocoercivity techniques, we demonstrate two key results: the existence of APNNs when the loss function approaches zero, and the convergence of the APNN approximated solution as the loss tends to zero, with the error exhibiting an exponential decay in time.         ",
    "url": "https://arxiv.org/abs/2503.01643",
    "authors": [
      "Jiayu Wan",
      "Liu Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2503.01653",
    "title": "Distilled Prompt Learning for Incomplete Multimodal Survival Prediction",
    "abstract": "           The integration of multimodal data including pathology images and gene profiles is widely applied in precise survival prediction. Despite recent advances in multimodal survival models, collecting complete modalities for multimodal fusion still poses a significant challenge, hindering their application in clinical settings. Current approaches tackling incomplete modalities often fall short, as they typically compensate for only a limited part of the knowledge of missing modalities. To address this issue, we propose a Distilled Prompt Learning framework (DisPro) to utilize the strong robustness of Large Language Models (LLMs) to missing modalities, which employs two-stage prompting for compensation of comprehensive information for missing modalities. In the first stage, Unimodal Prompting (UniPro) distills the knowledge distribution of each modality, preparing for supplementing modality-specific knowledge of the missing modality in the subsequent stage. In the second stage, Multimodal Prompting (MultiPro) leverages available modalities as prompts for LLMs to infer the missing modality, which provides modality-common information. Simultaneously, the unimodal knowledge acquired in the first stage is injected into multimodal inference to compensate for the modality-specific knowledge of the missing modality. Extensive experiments covering various missing scenarios demonstrated the superiority of the proposed method. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.01653",
    "authors": [
      "Yingxue Xu",
      "Fengtao Zhou",
      "Chenyu Zhao",
      "Yihui Wang",
      "Can Yang",
      "Hao Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01654",
    "title": "A Shared Encoder Approach to Multimodal Representation Learning",
    "abstract": "           Multimodal representation learning has demonstrated remarkable potential in enabling models to process and integrate diverse data modalities, such as text and images, for improved understanding and performance. While the medical domain can benefit significantly from this paradigm, the scarcity of paired multimodal data and reliance on proprietary or pretrained encoders pose significant challenges. In this work, we present a shared encoder framework for multimodal representation learning tailored to the medical domain. Our approach employs a single set of encoder parameters shared across modalities, augmented with learnable modality features. Empirical results demonstrate that our shared encoder idea achieves superior performance compared to separate modality-specific encoders, demonstrating improved generalization in data-constrained settings. Notably, the performance gains are more pronounced with fewer training examples, underscoring the efficiency of our shared encoder framework for real-world medical applications with limited data. Our code and experiment setup are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.01654",
    "authors": [
      "Shuvendu Roy",
      "Franklin Ogidi",
      "Ali Etemad",
      "Elham Dolatabadi",
      "Arash Afkanpour"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01655",
    "title": "Enhancing Object Detection Accuracy in Underwater Sonar Images through Deep Learning-based Denoising",
    "abstract": "           Sonar image object detection is crucial for underwater robotics and other applications. However, various types of noise in sonar images can affect the accuracy of object detection. Denoising, as a critical preprocessing step, aims to remove noise while retaining useful information to improve detection accuracy. Although deep learning-based denoising algorithms perform well on optical images, their application to underwater sonar images remains underexplored. This paper systematically evaluates the effectiveness of several deep learning-based denoising algorithms, originally designed for optical images, in the context of underwater sonar image object detection. We apply nine trained denoising models to images from five open-source sonar datasets, each processing different types of noise. We then test the denoised images using four object detection algorithms. The results show that different denoising models have varying effects on detection performance. By combining the strengths of multiple denoising models, the detection results can be optimized, thus more effectively suppressing noise. Additionally, we adopt a multi-frame denoising technique, using different outputs generated by multiple denoising models as multiple frames of the same scene for further processing to enhance detection accuracy. This method, originally designed for optical images, leverages complementary noise-reduction effects. Experimental results show that denoised sonar images improve the performance of object detection algorithms compared to the original sonar images.         ",
    "url": "https://arxiv.org/abs/2503.01655",
    "authors": [
      "Ziyu Wang",
      "Tao Xue",
      "Yanbin Wang",
      "Jingyuan Li",
      "Haibin Zhang",
      "Zhiqiang Xu",
      "Gaofei Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01660",
    "title": "Non-convergence to the optimal risk for Adam and stochastic gradient descent optimization in the training of deep neural networks",
    "abstract": "           Despite the omnipresent use of stochastic gradient descent (SGD) optimization methods in the training of deep neural networks (DNNs), it remains, in basically all practically relevant scenarios, a fundamental open problem to provide a rigorous theoretical explanation for the success (and the limitations) of SGD optimization methods in deep learning. In particular, it remains an open question to prove or disprove convergence of the true risk of SGD optimization methods to the optimal true risk value in the training of DNNs. In one of the main results of this work we reveal for a general class of activations, loss functions, random initializations, and SGD optimization methods (including, for example, standard SGD, momentum SGD, Nesterov accelerated SGD, Adagrad, RMSprop, Adadelta, Adam, Adamax, Nadam, Nadamax, and AMSGrad) that in the training of any arbitrary fully-connected feedforward DNN it does not hold that the true risk of the considered optimizer converges in probability to the optimal true risk value. Nonetheless, the true risk of the considered SGD optimization method may very well converge to a strictly suboptimal true risk value.         ",
    "url": "https://arxiv.org/abs/2503.01660",
    "authors": [
      "Thang Do",
      "Arnulf Jentzen",
      "Adrian Riekert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2503.01661",
    "title": "MUSt3R: Multi-view Network for Stereo 3D Reconstruction",
    "abstract": "           DUSt3R introduced a novel paradigm in geometric computer vision by proposing a model that can provide dense and unconstrained Stereo 3D Reconstruction of arbitrary image collections with no prior information about camera calibration nor viewpoint poses. Under the hood, however, DUSt3R processes image pairs, regressing local 3D reconstructions that need to be aligned in a global coordinate system. The number of pairs, growing quadratically, is an inherent limitation that becomes especially concerning for robust and fast optimization in the case of large image collections. In this paper, we propose an extension of DUSt3R from pairs to multiple views, that addresses all aforementioned concerns. Indeed, we propose a Multi-view Network for Stereo 3D Reconstruction, or MUSt3R, that modifies the DUSt3R architecture by making it symmetric and extending it to directly predict 3D structure for all views in a common coordinate frame. Second, we entail the model with a multi-layer memory mechanism which allows to reduce the computational complexity and to scale the reconstruction to large collections, inferring thousands of 3D pointmaps at high frame-rates with limited added complexity. The framework is designed to perform 3D reconstruction both offline and online, and hence can be seamlessly applied to SfM and visual SLAM scenarios showing state-of-the-art performance on various 3D downstream tasks, including uncalibrated Visual Odometry, relative camera pose, scale and focal estimation, 3D reconstruction and multi-view depth estimation.         ",
    "url": "https://arxiv.org/abs/2503.01661",
    "authors": [
      "Yohann Cabon",
      "Lucas Stoffl",
      "Leonid Antsfeld",
      "Gabriela Csurka",
      "Boris Chidlovskii",
      "Jerome Revaud",
      "Vincent Leroy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01669",
    "title": "An Efficient Continual Learning Framework for Multivariate Time Series Prediction Tasks with Application to Vehicle State Estimation",
    "abstract": "           In continual time series analysis using neural networks, catastrophic forgetting (CF) of previously learned models when training on new data domains has always been a significant challenge. This problem is especially challenging in vehicle estimation and control, where new information is sequentially introduced to the model. Unfortunately, existing work on continual learning has not sufficiently addressed the adverse effects of catastrophic forgetting in time series analysis, particularly in multivariate output environments. In this paper, we present EM-ReSeleCT (Efficient Multivariate Representative Selection for Continual Learning in Time Series Tasks), an enhanced approach designed to handle continual learning in multivariate environments. Our approach strategically selects representative subsets from old and historical data and incorporates memory-based continual learning techniques with an improved optimization algorithm to adapt the pre-trained model on new information while preserving previously acquired information. Additionally, we develop a sequence-to-sequence transformer model (autoregressive model) specifically designed for vehicle state estimation. Moreover, we propose an uncertainty quantification framework using conformal prediction to assess the sensitivity of the memory size and to showcase the robustness of the proposed method. Experimental results from tests on an electric Equinox vehicle highlight the superiority of our method in continually learning new information while retaining prior knowledge, outperforming state-of-the-art continual learning methods. Furthermore, EM-ReSeleCT significantly reduces training time, a critical advantage in continual learning applications.         ",
    "url": "https://arxiv.org/abs/2503.01669",
    "authors": [
      "Arvin Hosseinzadeh",
      "Ladan Khoshnevisan",
      "Mohammad Pirani",
      "Shojaeddin Chenouri",
      "Amir Khajepour"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01672",
    "title": "Automated Annotation of Evolving Corpora for Augmenting Longitudinal Network Data: A Framework Integrating Large Language Models and Expert Knowledge",
    "abstract": "           Longitudinal network data are essential for analyzing political, economic, and social systems and processes. In political science, these datasets are often generated through human annotation or supervised machine learning applied to evolving corpora. However, as semantic contexts shift over time, inferring dynamic interaction types on emerging issues among a diverse set of entities poses significant challenges, particularly in maintaining timely and consistent annotations. This paper presents the Expert-Augmented LLM Annotation (EALA) approach, which leverages Large Language Models (LLMs) in combination with historically annotated data and expert-constructed codebooks to extrapolate and extend datasets into future periods. We evaluate the performance and reliability of EALA using a dataset of climate negotiations. Our findings demonstrate that EALA effectively predicts nuanced interactions between negotiation parties and captures the evolution of topics over time. At the same time, we identify several limitations inherent to LLM-based annotation, highlighting areas for further improvement. Given the wide availability of codebooks and annotated datasets, EALA holds substantial promise for advancing research in political science and beyond.         ",
    "url": "https://arxiv.org/abs/2503.01672",
    "authors": [
      "Xiao Liu",
      "Zirui Wu",
      "Jiayi Li",
      "Zhicheng Shao",
      "Xun Pang",
      "Yansong Feng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.01675",
    "title": "Using (Not so) Large Language Models for Generating Simulation Models in a Formal DSL -- A Study on Reaction Networks",
    "abstract": "           Formal languages are an integral part of modeling and simulation. They allow the distillation of knowledge into concise simulation models amenable to automatic execution, interpretation, and analysis. However, the arguably most humanly accessible means of expressing models is through natural language, which is not easily interpretable by computers. Here, we evaluate how a Large Language Model (LLM) might be used for formalizing natural language into simulation models. Existing studies only explored using very large LLMs, like the commercial GPT models, without fine-tuning model weights. To close this gap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned to translate natural language descriptions to reaction network models in a domain-specific language, offering a self-hostable, compute-, and memory efficient alternative. To this end, we develop a synthetic data generator to serve as the basis for fine-tuning and evaluation. Our quantitative evaluation shows that our fine-tuned Mistral model can recover the ground truth simulation model in up to 84.5% of cases. In addition, our small-scale user study demonstrates the model's practical potential for one-time generation as well as interactive modeling in various domains. While promising, in its current form, the fine-tuned small LLM cannot catch up with large LLMs. We conclude that higher-quality training data are required, and expect future small and open-source LLMs to offer new opportunities.         ",
    "url": "https://arxiv.org/abs/2503.01675",
    "authors": [
      "Justin N. Kreikemeyer",
      "Mi\u0142osz Jankowski",
      "Pia Wilsdorf",
      "Adelinde M. Uhrmacher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.01676",
    "title": "Perceptual Motor Learning with Active Inference Framework for Robust Lateral Control",
    "abstract": "           This paper presents a novel Perceptual Motor Learning (PML) framework integrated with Active Inference (AIF) to enhance lateral control in Highly Automated Vehicles (HAVs). PML, inspired by human motor learning, emphasizes the seamless integration of perception and action, enabling efficient decision-making in dynamic environments. Traditional autonomous driving approaches--including modular pipelines, imitation learning, and reinforcement learning--struggle with adaptability, generalization, and computational efficiency. In contrast, PML with AIF leverages a generative model to minimize prediction error (\"surprise\") and actively shape vehicle control based on learned perceptual-motor representations. Our approach unifies deep learning with active inference principles, allowing HAVs to perform lane-keeping maneuvers with minimal data and without extensive retraining across different environments. Extensive experiments in the CARLA simulator demonstrate that PML with AIF enhances adaptability without increasing computational overhead while achieving performance comparable to conventional methods. These findings highlight the potential of PML-driven active inference as a robust alternative for real-world autonomous driving applications.         ",
    "url": "https://arxiv.org/abs/2503.01676",
    "authors": [
      "Elahe Delavari",
      "John Moore",
      "Junho Hong",
      "Jaerock Kwon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.01682",
    "title": "GRNFormer: A Biologically-Guided Framework for Integrating Gene Regulatory Networks into RNA Foundation Models",
    "abstract": "           Foundation models for single-cell RNA sequencing (scRNA-seq) have shown promising capabilities in capturing gene expression patterns. However, current approaches face critical limitations: they ignore biological prior knowledge encoded in gene regulatory relationships and fail to leverage multi-omics signals that could provide complementary regulatory insights. In this paper, we propose GRNFormer, a new framework that systematically integrates multi-scale Gene Regulatory Networks (GRNs) inferred from multi-omics data into RNA foundation model training. Our framework introduces two key innovations. First, we introduce a pipeline for constructing hierarchical GRNs that capture regulatory relationships at both cell-type-specific and cell-specific resolutions. Second, we design a structure-aware integration framework that addresses the information asymmetry in GRNs through two technical advances: (1) A graph topological adapter using multi-head cross-attention to weight regulatory relationships dynamically, and (2) a novel edge perturbation strategy that perturb GRNs with biologically-informed co-expression links to augment graph neural network training. Comprehensive experiments have been conducted on three representative downstream tasks across multiple model architectures to demonstrate the effectiveness of GRNFormer. It achieves consistent improvements over state-of-the-art (SoTA) baselines: $3.6\\%$ increase in drug response prediction correlation, $9.6\\%$ improvement in single-cell drug classification AUC, and $1.1\\%$ average gain in gene perturbation prediction accuracy.         ",
    "url": "https://arxiv.org/abs/2503.01682",
    "authors": [
      "Mufan Qiu",
      "Xinyu Hu",
      "Fengwei Zhan",
      "Sukwon Yun",
      "Jie Peng",
      "Ruichen Zhang",
      "Bhavya Kailkhura",
      "Jiekun Yang",
      "Tianlong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01700",
    "title": "Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation",
    "abstract": "           Recent works have shown great potentials of Large Language Models (LLMs) in robot task and motion planning (TAMP). Current LLM approaches generate text- or code-based reasoning chains with sub-goals and action plans. However, they do not fully leverage LLMs' symbolic computing and code generation capabilities. Many robot TAMP tasks involve complex optimization under multiple constraints, where pure textual reasoning is insufficient. While augmenting LLMs with predefined solvers and planners improves performance, it lacks generalization across tasks. Given LLMs' growing coding proficiency, we enhance their TAMP capabilities by steering them to generate code as symbolic planners for optimization and constraint verification. Unlike prior work that uses code to interface with robot action modules, we steer LLMs to generate code as solvers, planners, and checkers for TAMP tasks requiring symbolic computing, while still leveraging textual reasoning to incorporate common sense. With a multi-round guidance and answer evolution framework, the proposed Code-as-Symbolic-Planner improves success rates by average 24.1\\% over best baseline methods across seven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner shows strong effectiveness and generalizability across discrete and continuous environments, 2D/3D simulations and real-world settings, as well as single- and multi-robot tasks with diverse requirements. See our project website this https URL for prompts, videos, and code.         ",
    "url": "https://arxiv.org/abs/2503.01700",
    "authors": [
      "Yongchao Chen",
      "Yilun Hao",
      "Yang Zhang",
      "Chuchu Fan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.01702",
    "title": "Relating Piecewise Linear Kolmogorov Arnold Networks to ReLU Networks",
    "abstract": "           Kolmogorov-Arnold Networks are a new family of neural network architectures which holds promise for overcoming the curse of dimensionality and has interpretability benefits (arXiv:2404.19756). In this paper, we explore the connection between Kolmogorov Arnold Networks (KANs) with piecewise linear (univariate real) functions and ReLU networks. We provide completely explicit constructions to convert a piecewise linear KAN into a ReLU network and vice versa.         ",
    "url": "https://arxiv.org/abs/2503.01702",
    "authors": [
      "Nandi Schoots",
      "Mattia Jacopo Villani",
      "Niels uit de Bos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01720",
    "title": "Quality Measures for Dynamic Graph Generative Models",
    "abstract": "           Deep generative models have recently achieved significant success in modeling graph data, including dynamic graphs, where topology and features evolve over time. However, unlike in vision and natural language domains, evaluating generative models for dynamic graphs is challenging due to the difficulty of visualizing their output, making quantitative metrics essential. In this work, we develop a new quality metric for evaluating generative models of dynamic graphs. Current metrics for dynamic graphs typically involve discretizing the continuous-evolution of graphs into static snapshots and then applying conventional graph similarity measures. This approach has several limitations: (a) it models temporally related events as i.i.d. samples, failing to capture the non-uniform evolution of dynamic graphs; (b) it lacks a unified measure that is sensitive to both features and topology; (c) it fails to provide a scalar metric, requiring multiple metrics without clear superiority; and (d) it requires explicitly instantiating each static snapshot, leading to impractical runtime demands that hinder evaluation at scale. We propose a novel metric based on the \\textit{Johnson-Lindenstrauss} lemma, applying random projections directly to dynamic graph data. This results in an expressive, scalar, and application-agnostic measure of dynamic graph similarity that overcomes the limitations of traditional methods. We also provide a comprehensive empirical evaluation of metrics for continuous-time dynamic graphs, demonstrating the effectiveness of our approach compared to existing methods. Our implementation is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.01720",
    "authors": [
      "Ryien Hosseini",
      "Filippo Simini",
      "Venkatram Vishwanath",
      "Rebecca Willett",
      "Henry Hoffmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01722",
    "title": "Learning Exposure Mapping Functions for Inferring Heterogeneous Peer Effects",
    "abstract": "           In causal inference, interference refers to the phenomenon in which the actions of peers in a network can influence an individual's outcome. Peer effect refers to the difference in counterfactual outcomes of an individual for different levels of peer exposure, the extent to which an individual is exposed to the treatments, actions, or behaviors of peers. Estimating peer effects requires deciding how to represent peer exposure. Typically, researchers define an exposure mapping function that aggregates peer treatments and outputs peer exposure. Most existing approaches for defining exposure mapping functions assume peer exposure based on the number or fraction of treated peers. Recent studies have investigated more complex functions of peer exposure which capture that different peers can exert different degrees of influence. However, none of these works have explicitly considered the problem of automatically learning the exposure mapping function. In this work, we focus on learning this function for the purpose of estimating heterogeneous peer effects, where heterogeneity refers to the variation in counterfactual outcomes for the same peer exposure but different individual's contexts. We develop EgoNetGNN, a graph neural network (GNN)-based method, to automatically learn the appropriate exposure mapping function allowing for complex peer influence mechanisms that, in addition to peer treatments, can involve the local neighborhood structure and edge attributes. We show that GNN models that use peer exposure based on the number or fraction of treated peers or learn peer exposure naively face difficulty accounting for such influence mechanisms. Our comprehensive evaluation on synthetic and semi-synthetic network data shows that our method is more robust to different unknown underlying influence mechanisms when estimating heterogeneous peer effects when compared to state-of-the-art baselines.         ",
    "url": "https://arxiv.org/abs/2503.01722",
    "authors": [
      "Shishir Adhikari",
      "Sourav Medya",
      "Elena Zheleva"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.01723",
    "title": "How Low Can You Go? Searching for the Intrinsic Dimensionality of Complex Networks using Metric Node Embeddings",
    "abstract": "           Low-dimensional embeddings are essential for machine learning tasks involving graphs, such as node classification, link prediction, community detection, network visualization, and network compression. Although recent studies have identified exact low-dimensional embeddings, the limits of the required embedding dimensions remain unclear. We presently prove that lower dimensional embeddings are possible when using Euclidean metric embeddings as opposed to vector-based Logistic PCA (LPCA) embeddings. In particular, we provide an efficient logarithmic search procedure for identifying the exact embedding dimension and demonstrate how metric embeddings enable inference of the exact embedding dimensions of large-scale networks by exploiting that the metric properties can be used to provide linearithmic scaling. Empirically, we show that our approach extracts substantially lower dimensional representations of networks than previously reported for small-sized networks. For the first time, we demonstrate that even large-scale networks can be effectively embedded in very low-dimensional spaces, and provide examples of scalable, exact reconstruction for graphs with up to a million nodes. Our approach highlights that the intrinsic dimensionality of networks is substantially lower than previously reported and provides a computationally efficient assessment of the exact embedding dimension also of large-scale networks. The surprisingly low dimensional representations achieved demonstrate that networks in general can be losslessly represented using very low dimensional feature spaces, which can be used to guide existing network analysis tasks from community detection and node classification to structure revealing exact network visualizations.         ",
    "url": "https://arxiv.org/abs/2503.01723",
    "authors": [
      "Nikolaos Nakis",
      "Niels Raunkj\u00e6r Holm",
      "Andreas Lyhne Fiehn",
      "Morten M\u00f8rup"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.01724",
    "title": "Syntactic Learnability of Echo State Neural Language Models at Scale",
    "abstract": "           What is a neural model with minimum architectural complexity that exhibits reasonable language learning capability? To explore such a simple but sufficient neural language model, we revisit a basic reservoir computing (RC) model, Echo State Network (ESN), a restricted class of simple Recurrent Neural Networks. Our experiments showed that ESN with a large hidden state is comparable or superior to Transformer in grammaticality judgment tasks when trained with about 100M words, suggesting that architectures as complex as that of Transformer may not always be necessary for syntactic learning.         ",
    "url": "https://arxiv.org/abs/2503.01724",
    "authors": [
      "Ryo Ueda",
      "Tatsuki Kuribayashi",
      "Shunsuke Kando",
      "Kentaro Inui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.01734",
    "title": "Adversarial Agents: Black-Box Evasion Attacks with Reinforcement Learning",
    "abstract": "           Reinforcement learning (RL) offers powerful techniques for solving complex sequential decision-making tasks from experience. In this paper, we demonstrate how RL can be applied to adversarial machine learning (AML) to develop a new class of attacks that learn to generate adversarial examples: inputs designed to fool machine learning models. Unlike traditional AML methods that craft adversarial examples independently, our RL-based approach retains and exploits past attack experience to improve future attacks. We formulate adversarial example generation as a Markov Decision Process and evaluate RL's ability to (a) learn effective and efficient attack strategies and (b) compete with state-of-the-art AML. On CIFAR-10, our agent increases the success rate of adversarial examples by 19.4% and decreases the median number of victim model queries per adversarial example by 53.2% from the start to the end of training. In a head-to-head comparison with a state-of-the-art image attack, SquareAttack, our approach enables an adversary to generate adversarial examples with 13.1% more success after 5000 episodes of training. From a security perspective, this work demonstrates a powerful new attack vector that uses RL to attack ML models efficiently and at scale.         ",
    "url": "https://arxiv.org/abs/2503.01734",
    "authors": [
      "Kyle Domico",
      "Jean-Charles Noirot Ferrand",
      "Ryan Sheatsley",
      "Eric Pauley",
      "Josiah Hanna",
      "Patrick McDaniel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01744",
    "title": "Application of the List Viterbi Algorithm for Satellite-based AIS Detection",
    "abstract": "           Satellites receiving Automatic Identification System (AIS) packets in dense areas are particularly prone to AIS channel overload due to the extensive number of vessels. Thus a failure of detection might be caused by the collisions among AIS messages. To improve the detection capability, we propose to exploit the presence of the cyclic redundancy check (CRC) in AIS frames by using the parallel list Viterbi algorithm (PLVA) instead of the classical Viterbi algorithm (VA) often used for decoding AIS signals. The performance of combining the PLVA with AIS post processing including the CRC is studied with two detectors, one coherent and the other differential, in two channel models: a single-user AWGN channel and a more realistic multiple-access AIS channel. We also show the impact of the PLVA parameters on the success recovery rate. The simulation results show that the resulting procedure can significantly improve the packet error rate (PER) at the cost of a limited increase of the computational complexity. The proposed technique could be applied to improve the performance of interference cancellation receivers by significantly lowering the AIS decoding threshold.         ",
    "url": "https://arxiv.org/abs/2503.01744",
    "authors": [
      "Linda Kanaan",
      "Karine Amis",
      "Fr\u00e9d\u00e9ric Guilloud",
      "R\u00e9mi Chauvat"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.01756",
    "title": "Nanosatellite Constellation and Ground Station Co-design for Low-Latency Critical Event Detection",
    "abstract": "           Advancements in nanosatellite technology lead to more Earth-observation satellites in low-Earth orbit. We explore using nanosatellite constellations to achieve low-latency detection for time-critical events, such as forest fires, oil spills, and floods. The detection latency comprises three parts: capture, compute and transmission. Previous solutions reduce transmission latency, but we find that the bottleneck is capture latency, accounting for more than 90% of end-to-end latency. We present a measurement study on how various satellite and ground station design factors affect latency. We offer design guidance to operators on how to choose satellite orbital configurations and design an algorithm to choose ground station locations. For six use cases, our design guidance reduces end-to-end latency by 5.6 to 8.2 times compared to the existing system.         ",
    "url": "https://arxiv.org/abs/2503.01756",
    "authors": [
      "Zhuo Cheng",
      "Brandon Lucia"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.01758",
    "title": "Zero-Trust Artificial Intelligence Model Security Based on Moving Target Defense and Content Disarm and Reconstruction",
    "abstract": "           This paper examines the challenges in distributing AI models through model zoos and file transfer mechanisms. Despite advancements in security measures, vulnerabilities persist, necessitating a multi-layered approach to mitigate risks effectively. The physical security of model files is critical, requiring stringent access controls and attack prevention solutions. This paper proposes a novel solution architecture composed of two prevention approaches. The first is Content Disarm and Reconstruction (CDR), which focuses on disarming serialization attacks that enable attackers to run malicious code as soon as the model is loaded. The second is protecting the model architecture and weights from attacks by using Moving Target Defense (MTD), alerting the model structure, and providing verification steps to detect such attacks. The paper focuses on the highly exploitable Pickle and PyTorch file formats. It demonstrates a 100% disarm rate while validated against known AI model repositories and actual malware attacks from the HuggingFace model zoo.         ",
    "url": "https://arxiv.org/abs/2503.01758",
    "authors": [
      "Daniel Gilkarov",
      "Ran Dubin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.01770",
    "title": "m4: A Learned Flow-level Network Simulator",
    "abstract": "           Flow-level simulation is widely used to model large-scale data center networks due to its scalability. Unlike packet-level simulators that model individual packets, flow-level simulators abstract traffic as continuous flows with dynamically assigned transmission rates. While this abstraction enables orders-of-magnitude speedup, it is inaccurate by omitting critical packet-level effects such as queuing, congestion control, and retransmissions. We present m4, an accurate and scalable flow-level simulator that uses machine learning to learn the dynamics of the network of interest. At the core of m4 lies a novel ML architecture that decomposes state transition computations into distinct spatial and temporal components, each represented by a suitable neural network. To efficiently learn the underlying flow-level dynamics, m4 adds dense supervision signals by predicting intermediate network metrics such as remaining flow size and queue length during training. m4 achieves a speedup of up to 104$\\times$ over packet-level simulation. Relative to a traditional flow-level simulation, m4 reduces per-flow estimation errors by 45.3% (mean) and 53.0% (p90). For closed-loop applications, m4 accurately predicts network throughput under various congestion control schemes and workloads.         ",
    "url": "https://arxiv.org/abs/2503.01770",
    "authors": [
      "Chenning Li",
      "Anton A. Zabreyko",
      "Arash Nasr-Esfahany",
      "Kevin Zhao",
      "Prateesh Goyal",
      "Mohammad Alizadeh",
      "Thomas Anderson"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.01775",
    "title": "Structure-Preserving Neural Ordinary Differential Equations for Stiff Systems",
    "abstract": "           Neural ordinary differential equations (NODEs) are an effective approach for data-based modeling of dynamical systems arising from simulations and experiments. One of the major shortcomings of NODEs, especially when coupled with explicit integrators, is its long-term stability, which impedes their efficiency and robustness when encountering stiff problems. In this work we present a structure-preserving NODE approach, which integrates with a linear and nonlinear split and an exponential integrator, latter of which is an explicit integrator with stability properties comparable to implicit methods. We demonstrate that our model has advantages in both learning and deployment over standard explicit or even implicit NODE methods. The long-time stability is further enhanced by the Hurwitz matrix decomposition that constrains the spectrum of the linear operator, therefore stabilizing the linearized dynamics. When combined with a Lipschitz-controlled neural network treatment for the nonlinear operator, we show the nonlinear dynamics of the NODE are provably stable in the sense of Lyapunov. For high-dimensional data, we further rely on an autoencoder performing dimension reduction and Higham's algorithm for the matrix-free application of the matrix exponential on a vector. We demonstrate the effectiveness of the proposed NODE approach in various examples, including the Grad-13 moment equations and the Kuramoto-Sivashinky equation.         ",
    "url": "https://arxiv.org/abs/2503.01775",
    "authors": [
      "Allen Alvarez Loya",
      "Daniel A. Serino",
      "Qi Tang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2503.01776",
    "title": "Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation",
    "abstract": "           Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval, search, and generative modeling. Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it requires full model retraining and suffers from noticeable performance degradations at short lengths. In this paper, we show that sparse coding offers a compelling alternative for achieving adaptive representation with minimal overhead and higher fidelity. We propose Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained embeddings into a high-dimensional but selectively activated feature space. By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic quality while allowing flexible, cost-effective inference at different sparsity levels. Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently outperforms MRL in terms of both accuracy and retrieval speed-often by large margins-while also cutting training time to a fraction of that required by MRL. Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world applications where efficiency and fidelity are both paramount. Code is available at this https URL ",
    "url": "https://arxiv.org/abs/2503.01776",
    "authors": [
      "Tiansheng Wen",
      "Yifei Wang",
      "Zequn Zeng",
      "Zhong Peng",
      "Yudi Su",
      "Xinyang Liu",
      "Bo Chen",
      "Hongwei Liu",
      "Stefanie Jegelka",
      "Chenyu You"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.01781",
    "title": "Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning Models",
    "abstract": "           We investigate the robustness of reasoning models trained for step-by-step problem solving by introducing query-agnostic adversarial triggers - short, irrelevant text that, when appended to math problems, systematically mislead models to output incorrect answers without altering the problem's semantics. We propose CatAttack, an automated iterative attack pipeline for generating triggers on a weaker, less expensive proxy model (DeepSeek V3) and successfully transfer them to more advanced reasoning target models like DeepSeek R1 and DeepSeek R1-distilled-Qwen-32B, resulting in greater than 300% increase in the likelihood of the target model generating an incorrect answer. For example, appending, \"Interesting fact: cats sleep most of their lives,\" to any math problem leads to more than doubling the chances of a model getting the answer wrong. Our findings highlight critical vulnerabilities in reasoning models, revealing that even state-of-the-art models remain susceptible to subtle adversarial inputs, raising security and reliability concerns. The CatAttack triggers dataset with model responses is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.01781",
    "authors": [
      "Meghana Rajeev",
      "Rajkumar Ramamurthy",
      "Prapti Trivedi",
      "Vikas Yadav",
      "Oluwanifemi Bamgbose",
      "Sathwik Tejaswi Madhusudan",
      "James Zou",
      "Nazneen Rajani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.01783",
    "title": "vS-Graphs: Integrating Visual SLAM and Situational Graphs through Multi-level Scene Understanding",
    "abstract": "           Current Visual Simultaneous Localization and Mapping (VSLAM) systems often struggle to create maps that are both semantically rich and easily interpretable. While incorporating semantic scene knowledge aids in building richer maps with contextual associations among mapped objects, representing them in structured formats like scene graphs has not been widely addressed, encountering complex map comprehension and limited scalability. This paper introduces visual S-Graphs (vS-Graphs), a novel real-time VSLAM framework that integrates vision-based scene understanding with map reconstruction and comprehensible graph-based representation. The framework infers structural elements (i.e., rooms and corridors) from detected building components (i.e., walls and ground surfaces) and incorporates them into optimizable 3D scene graphs. This solution enhances the reconstructed map's semantic richness, comprehensibility, and localization accuracy. Extensive experiments on standard benchmarks and real-world datasets demonstrate that vS-Graphs outperforms state-of-the-art VSLAM methods, reducing trajectory error by an average of 3.38% and up to 9.58% on real-world data. Furthermore, the proposed framework achieves environment-driven semantic entity detection accuracy comparable to precise LiDAR-based frameworks using only visual features. A web page containing more media and evaluation outcomes is available on this https URL.         ",
    "url": "https://arxiv.org/abs/2503.01783",
    "authors": [
      "Ali Tourani",
      "Saad Ejaz",
      "Hriday Bavle",
      "David Morilla-Cabello",
      "Jose Luis Sanchez-Lopez",
      "Holger Voos"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01794",
    "title": "OFF-CLIP: Improving Normal Detection Confidence in Radiology CLIP with Simple Off-Diagonal Term Auto-Adjustment",
    "abstract": "           Contrastive Language-Image Pre-Training (CLIP) has enabled zero-shot classification in radiology, reducing reliance on manual annotations. However, conventional contrastive learning struggles with normal case detection due to its strict intra-sample alignment, which disrupts normal sample clustering and leads to high false positives (FPs) and false negatives (FNs). To address these issues, we propose OFF-CLIP, a contrastive learning refinement that improves normal detection by introducing an off-diagonal term loss to enhance normal sample clustering and applying sentence-level text filtering to mitigate FNs by removing misaligned normal statements from abnormal reports. OFF-CLIP can be applied to radiology CLIP models without requiring any architectural modifications. Experimental results show that OFF-CLIP significantly improves normal classification, achieving a 0.61 Area under the curve (AUC) increase on VinDr-CXR over CARZero, the state-of-the-art zero-shot classification baseline, while maintaining or improving abnormal classification performance. Additionally, OFF-CLIP enhances zero-shot grounding by improving pointing game accuracy, confirming better anomaly localization. These results demonstrate OFF-CLIP's effectiveness as a robust and efficient enhancement for medical vision-language models.         ",
    "url": "https://arxiv.org/abs/2503.01794",
    "authors": [
      "Junhyun Park",
      "Chanyu Moon",
      "Donghwan Lee",
      "Kyungsu Kim",
      "Minho Hwang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.01799",
    "title": "PhishVQC: Optimizing Phishing URL Detection with Correlation Based Feature Selection and Variational Quantum Classifier",
    "abstract": "           Phishing URL detection is crucial in cybersecurity as malicious websites disguise themselves to steal sensitive infor mation. Traditional machine learning techniques struggle to per form well in complex real-world scenarios due to large datasets and intricate patterns. Motivated by quantum computing, this paper proposes using Variational Quantum Classifiers (VQC) to enhance phishing URL detection. We present PhishVQC, a quantum model that combines quantum feature maps and vari ational ansatzes such as RealAmplitude and EfficientSU2. The model is evaluated across two experimental setups with varying dataset sizes and feature map repetitions. PhishVQC achieves a maximum macro average F1-score of 0.89, showing a 22% improvement over prior studies. This highlights the potential of quantum machine learning to improve phishing detection accuracy. The study also notes computational challenges, with execution wall times increasing as dataset size grows.         ",
    "url": "https://arxiv.org/abs/2503.01799",
    "authors": [
      "Md. Farhan Shahriyar",
      "Gazi Tanbhir",
      "Abdullah Md Raihan Chy",
      "Mohammed Abdul Al Arafat Tanzin",
      "Md. Jisan Mashrafi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01803",
    "title": "Deep Reinforcement Learning-Based User Association in Hybrid LiFi/WiFi Indoor Networks",
    "abstract": "           Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) indoor networks has been envisioned as a promising technology to alleviate radio frequency spectrum crunch to accommodate the ever-increasing data rate demand in indoor scenarios. The hybrid LiFi/WiFi indoor networks can leverage the advantages of fast data transmission from LiFi and wider coverage of WiFi, thus complementing well with each other and further improving the network performance compared with the standalone networks. However, to leverage the co-existence, several challenges should be addressed, including but not limited to user association, mobility support, and efficient resource allocation. Therefore, the objective of the paper is to design a new user-access point association algorithm to maximize the sum throughput of the hybrid networks. We first mathematically formulate the sum data rate maximization problem by determining the AP selection for each user in indoor networks with consideration of user mobility and practical capacity limitations, which is a nonconvex binary integer programming problem. To solve this problem, we then propose a sequential-proximal policy optimization (S-PPO) based deep reinforcement learning method. Extensive simulations are conducted to evaluate the proposed method by comparing it with exhaustive search (ES), signal strength strategy (SSS), and trust region policy optimization (TRPO) methods. Comprehensive simulation results demonstrate that our solution algorithm can outperform SSS by about 32.25% of the sum throughput and 19.09% of the fairness on average, and outperform TRPO by about 10.34% and 10.23%, respectively.         ",
    "url": "https://arxiv.org/abs/2503.01803",
    "authors": [
      "Peijun Hou",
      "Nan Cen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.01805",
    "title": "Depth-Width tradeoffs in Algorithmic Reasoning of Graph Tasks with Transformers",
    "abstract": "           Transformers have revolutionized the field of machine learning. In particular, they can be used to solve complex algorithmic problems, including graph-based tasks. In such algorithmic tasks a key question is what is the minimal size of a transformer that can implement a task. Recent work has begun to explore this problem for graph-based tasks, showing that for sub-linear embedding dimension (i.e., model width) logarithmic depth suffices. However, an open question, which we address here, is what happens if width is allowed to grow linearly. Here we analyze this setting, and provide the surprising result that with linear width, constant depth suffices for solving a host of graph-based problems. This suggests that a moderate increase in width can allow much shallower models, which are advantageous in terms of inference time. For other problems, we show that quadratic width is required. Our results demonstrate the complex and intriguing landscape of transformer implementations of graph-based algorithms. We support our theoretical results with empirical evaluations.         ",
    "url": "https://arxiv.org/abs/2503.01805",
    "authors": [
      "Gilad Yehudai",
      "Clayton Sanford",
      "Maya Bechler-Speicher",
      "Orr Fischer",
      "Ran Gilad-Bachrach",
      "Amir Globerson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.01808",
    "title": "Visualization of Event Graphs for Train Schedules",
    "abstract": "           Software that is used to compute or adjust train schedules is based on so-called event graphs. The vertices of such a graph correspond to events; each event is associated with a point in time, a location, and a train. A train line corresponds to a sequence of events (ordered by time) that are associated with the same train. The event graph has a directed edge from an earlier to a later event if they are consecutive along a train line. Events that occur at the same location do not occur at the same time. In this paper, we present a way to visualize such graphs, namely time-space diagrams. A time-space diagram is a straight-line drawing of the event graph with the additional constraint that all vertices that belong to the same location lie on the same horizontal line and that the x-coordinate of each vertex is given by its point in time. Hence, it remains to determine the y-coordinates of the locations. A good drawing of a time-space diagram supports users (or software developers) when creating (software for computing) train schedules. To enhance readability, we aim to minimize the number of turns in time-space diagrams. To this end, we establish a connection between this problem and Maximum Betweenness. Then we develop exact reduction rules to reduce the instance size. We also propose a parameterized algorithm and devise a heuristic that we evaluate experimentally on a real-world dataset.         ",
    "url": "https://arxiv.org/abs/2503.01808",
    "authors": [
      "Johann Hartleb",
      "Marie Schmidt",
      "Samuel Wolf",
      "Alexander Wolff"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2503.01811",
    "title": "AutoAdvExBench: Benchmarking autonomous exploitation of adversarial example defenses",
    "abstract": "           We introduce AutoAdvExBench, a benchmark to evaluate if large language models (LLMs) can autonomously exploit defenses to adversarial examples. Unlike existing security benchmarks that often serve as proxies for real-world tasks, bench directly measures LLMs' success on tasks regularly performed by machine learning security experts. This approach offers a significant advantage: if a LLM could solve the challenges presented in bench, it would immediately present practical utility for adversarial machine learning researchers. We then design a strong agent that is capable of breaking 75% of CTF-like (\"homework exercise\") adversarial example defenses. However, we show that this agent is only able to succeed on 13% of the real-world defenses in our benchmark, indicating the large gap between difficulty in attacking \"real\" code, and CTF-like code. In contrast, a stronger LLM that can attack 21% of real defenses only succeeds on 54% of CTF-like defenses. We make this benchmark available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.01811",
    "authors": [
      "Nicholas Carlini",
      "Javier Rando",
      "Edoardo Debenedetti",
      "Milad Nasr",
      "Florian Tram\u00e8r"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01824",
    "title": "From superposition to sparse codes: interpretable representations in neural networks",
    "abstract": "           Understanding how information is represented in neural networks is a fundamental challenge in both neuroscience and artificial intelligence. Despite their nonlinear architectures, recent evidence suggests that neural networks encode features in superposition, meaning that input concepts are linearly overlaid within the network's representations. We present a perspective that explains this phenomenon and provides a foundation for extracting interpretable representations from neural activations. Our theoretical framework consists of three steps: (1) Identifiability theory shows that neural networks trained for classification recover latent features up to a linear transformation. (2) Sparse coding methods can extract disentangled features from these representations by leveraging principles from compressed sensing. (3) Quantitative interpretability metrics provide a means to assess the success of these methods, ensuring that extracted features align with human-interpretable concepts. By bridging insights from theoretical neuroscience, representation learning, and interpretability research, we propose an emerging perspective on understanding neural representations in both artificial and biological systems. Our arguments have implications for neural coding theories, AI transparency, and the broader goal of making deep learning models more interpretable.         ",
    "url": "https://arxiv.org/abs/2503.01824",
    "authors": [
      "David Klindt",
      "Charles O'Neill",
      "Patrik Reizinger",
      "Harald Maurer",
      "Nina Miolane"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01830",
    "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network",
    "abstract": "           Large language models (LLMs) exhibit remarkable similarity to neural activity in the human language network. However, the key properties of language shaping brain-like representations, and their evolution during training as a function of different tasks remain unclear. We here benchmark 34 training checkpoints spanning 300B tokens across 8 different model sizes to analyze how brain alignment relates to linguistic competence. Specifically, we find that brain alignment tracks the development of formal linguistic competence -- i.e., knowledge of linguistic rules -- more closely than functional linguistic competence. While functional competence, which involves world knowledge and reasoning, continues to develop throughout training, its relationship with brain alignment is weaker, suggesting that the human language network primarily encodes formal linguistic structure rather than broader cognitive functions. We further show that model size is not a reliable predictor of brain alignment when controlling for feature size and find that the correlation between next-word prediction, behavioral alignment and brain alignment fades once models surpass human language proficiency. Finally, using the largest set of rigorous neural language benchmarks to date, we show that language brain alignment benchmarks remain unsaturated, highlighting opportunities for improving future models. Taken together, our findings suggest that the human language network is best modeled by formal, rather than functional, aspects of language.         ",
    "url": "https://arxiv.org/abs/2503.01830",
    "authors": [
      "Badr AlKhamissi",
      "Greta Tuckute",
      "Yingtian Tang",
      "Taha Binhuraib",
      "Antoine Bosselut",
      "Martin Schrimpf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.01838",
    "title": "GRAIN: Exact Graph Reconstruction from Gradients",
    "abstract": "           Federated learning claims to enable collaborative model training among multiple clients with data privacy by transmitting gradient updates instead of the actual client data. However, recent studies have shown the client privacy is still at risk due to the, so called, gradient inversion attacks which can precisely reconstruct clients' text and image data from the shared gradient updates. While these attacks demonstrate severe privacy risks for certain domains and architectures, the vulnerability of other commonly-used data types, such as graph-structured data, remain under-explored. To bridge this gap, we present GRAIN, the first exact gradient inversion attack on graph data in the honest-but-curious setting that recovers both the structure of the graph and the associated node features. Concretely, we focus on Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) -- two of the most widely used frameworks for learning on graphs. Our method first utilizes the low-rank structure of GNN gradients to efficiently reconstruct and filter the client subgraphs which are then joined to complete the input graph. We evaluate our approach on molecular, citation, and social network datasets using our novel metric. We show that GRAIN reconstructs up to 80% of all graphs exactly, significantly outperforming the baseline, which achieves up to 20% correctly positioned nodes.         ",
    "url": "https://arxiv.org/abs/2503.01838",
    "authors": [
      "Maria Drencheva",
      "Ivo Petrov",
      "Maximilian Baader",
      "Dimitar I. Dimitrov",
      "Martin Vechev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2503.00036",
    "title": "A Novel Spatiotemporal Correlation Anomaly Detection Method Based on Time-Frequency-Domain Feature Fusion and a Dynamic Graph Neural Network in Wireless Sensor Network",
    "abstract": "           Attention-based transformers have played an important role in wireless sensor network (WSN) timing anomaly detection due to their ability to capture long-term dependencies. However, there are several issues that must be addressed, such as the fact that their ability to capture long-term dependencies is not completely reliable, their computational complexity levels are high, and the spatiotemporal features of WSN timing data are not sufficiently extracted for detecting the correlation anomalies of multinode WSN timing data. To address these limitations, this paper proposes a WSN anomaly detection method that integrates frequency-domain features with dynamic graph neural networks (GNN) under a designed self-encoder reconstruction framework. First, the discrete wavelet transform effectively decomposes trend and seasonal components of time series to solve the poor long-term reliability of transformers. Second, a frequency-domain attention mechanism is designed to make full use of the difference between the amplitude distributions of normal data and anomalous data in this domain. Finally, a multimodal fusion-based dynamic graph convolutional network (MFDGCN) is designed by combining an attention mechanism and a graph convolutional network (GCN) to adaptively extract spatial correlation features. A series of experiments conducted on public datasets and their results demonstrate that the anomaly detection method designed in this paper exhibits superior precision and recall than the existing methods do, with an F1 score of 93.5%, representing an improvement of 2.9% over that of the existing models.         ",
    "url": "https://arxiv.org/abs/2503.00036",
    "authors": [
      "Miao Ye",
      "Zhibang Jiang",
      "Xingsi Xue",
      "Xingwang Li",
      "Peng Wen",
      "Yong Wang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00047",
    "title": "PCE-GAN: A Generative Adversarial Network for Point Cloud Attribute Quality Enhancement based on Optimal Transport",
    "abstract": "           Point cloud compression significantly reduces data volume but sacrifices reconstruction quality, highlighting the need for advanced quality enhancement techniques. Most existing approaches focus primarily on point-to-point fidelity, often neglecting the importance of perceptual quality as interpreted by the human visual system. To address this issue, we propose a generative adversarial network for point cloud quality enhancement (PCE-GAN), grounded in optimal transport theory, with the goal of simultaneously optimizing both data fidelity and perceptual quality. The generator consists of a local feature extraction (LFE) unit, a global spatial correlation (GSC) unit and a feature squeeze unit. The LFE unit uses dynamic graph construction and a graph attention mechanism to efficiently extract local features, placing greater emphasis on points with severe distortion. The GSC unit uses the geometry information of neighboring patches to construct an extended local neighborhood and introduces a transformer-style structure to capture long-range global correlations. The discriminator computes the deviation between the probability distributions of the enhanced point cloud and the original point cloud, guiding the generator to achieve high quality reconstruction. Experimental results show that the proposed method achieves state-of-the-art performance. Specifically, when applying PCE-GAN to the latest geometry-based point cloud compression (G-PCC) test model, it achieves an average BD-rate of -19.2% compared with the PredLift coding configuration and -18.3% compared with the RAHT coding configuration. Subjective comparisons show a significant improvement in texture clarity and color transitions, revealing finer details and more natural color gradients.         ",
    "url": "https://arxiv.org/abs/2503.00047",
    "authors": [
      "Tian Guo",
      "Hui Yuan",
      "Qi Liu",
      "Honglei Su",
      "Raouf Hamzaoui",
      "Sam Kwong"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2503.00140",
    "title": "Approaching the Harm of Gradient Attacks While Only Flipping Labels",
    "abstract": "           Availability attacks are one of the strongest forms of training-phase attacks in machine learning, making the model unusable. While prior work in distributed ML has demonstrated such effect via gradient attacks and, more recently, data poisoning, we ask: can similar damage be inflicted solely by flipping training labels, without altering features? In this work, we introduce a novel formalization of label flipping attacks and derive an attacker-optimized loss function that better illustrates label flipping capabilities. To compare the damaging effect of label flipping with that of gradient attacks, we use a setting that allows us to compare their \\emph{writing power} on the ML model. Our contribution is threefold, (1) we provide the first evidence for an availability attack through label flipping alone, (2) we shed light on an interesting interplay between what the attacker gains from more \\emph{write access} versus what they gain from more \\emph{flipping budget} and (3) we compare the power of targeted label flipping attack to that of an untargeted label flipping attack.         ",
    "url": "https://arxiv.org/abs/2503.00140",
    "authors": [
      "Abdessamad El-Kabid",
      "El-Mahdi El-Mhamdi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00143",
    "title": "RecCrysFormer: Refined Protein Structural Prediction from 3D Patterson Maps via Recycling Training Runs",
    "abstract": "           Determining protein structures at an atomic level remains a significant challenge in structural biology. We introduce $\\texttt{RecCrysFormer}$, a hybrid model that exploits the strengths of transformers with the aim of integrating experimental and ML approaches to protein structure determination from crystallographic data. $\\texttt{RecCrysFormer}$ leverages Patterson maps and incorporates known standardized partial structures of amino acid residues to directly predict electron density maps, which are essential for constructing detailed atomic models through crystallographic refinement processes. $\\texttt{RecCrysFormer}$ benefits from a ``recycling'' training regimen that iteratively incorporates results from crystallographic refinements and previous training runs as additional inputs in the form of template maps. Using a preliminary dataset of synthetic peptide fragments based on Protein Data Bank, $\\texttt{RecCrysFormer}$ achieves good accuracy in structural predictions and shows robustness against variations in crystal parameters, such as unit cell dimensions and angles.         ",
    "url": "https://arxiv.org/abs/2503.00143",
    "authors": [
      "Tom Pan",
      "Evan Dramko",
      "Mitchell D. Miller",
      "George N. Phillips Jr.",
      "Anastasios Kyrillidis"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2503.00156",
    "title": "Neural Posterior Estimation for Cataloging Astronomical Images with Spatially Varying Backgrounds and Point Spread Functions",
    "abstract": "           Neural posterior estimation (NPE), a type of amortized variational inference, is a computationally efficient means of constructing probabilistic catalogs of light sources from astronomical images. To date, NPE has not been used to perform inference in models with spatially varying covariates. However, ground-based astronomical images have spatially varying sky backgrounds and point spread functions (PSFs), and accounting for this variation is essential for constructing accurate catalogs of imaged light sources. In this work, we introduce a method of performing NPE with spatially varying backgrounds and PSFs. In this method, we generate synthetic catalogs and semi-synthetic images for these catalogs using randomly sampled PSF and background estimates from existing surveys. Using this data, we train a neural network, which takes an astronomical image and representations of its background and PSF as input, to output a probabilistic catalog. Our experiments with Sloan Digital Sky Survey data demonstrate the effectiveness of NPE in the presence of spatially varying backgrounds and PSFs for light source detection, star/galaxy separation, and flux measurement.         ",
    "url": "https://arxiv.org/abs/2503.00156",
    "authors": [
      "Aakash Patel",
      "Tianqing Zhang",
      "Camille Avestruz",
      "Jeffrey Regier",
      "LSST Dark Energy Science Collaboration"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2503.00267",
    "title": "SegImgNet: Segmentation-Guided Dual-Branch Network for Retinal Disease Diagnoses",
    "abstract": "           Retinal image plays a crucial role in diagnosing various diseases, as retinal structures provide essential diagnostic information. However, effectively capturing structural features while integrating them with contextual information from retinal images remains a challenge. In this work, we propose segmentation-guided dual-branch network for retinal disease diagnosis using retinal images and their segmentation maps, named SegImgNet. SegImgNet incorporates a segmentation module to generate multi-scale retinal structural feature maps from retinal images. The classification module employs two encoders to independently extract features from segmented images and retinal images for disease classification. To further enhance feature extraction, we introduce the Segmentation-Guided Attention (SGA) block, which leverages feature maps from the segmentation module to refine the classification process. We evaluate SegImgNet on the public AIROGS dataset and the private e-ROP dataset. Experimental results demonstrate that SegImgNet consistently outperforms existing methods, underscoring its effectiveness in retinal disease diagnosis. The code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.00267",
    "authors": [
      "Xinwei Luo",
      "Songlin Zhao",
      "Yun Zong",
      "Yong Chen",
      "Gui-shuang Ying",
      "Lifang He"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00274",
    "title": "Particle-based plasma simulation using a graph neural network",
    "abstract": "           A surrogate model for particle-in-cell plasma simulations based on a graph neural network is presented. The graph is constructed in such a way as to enable the representation of electromagnetic fields on a fixed spatial grid. The model is applied to simulate beams of electrons in one dimension over a wide range of temperatures, drift momenta and densities, and is shown to reproduce two-stream instabilities - a common and fundamental plasma instability. Qualitatively, the characteristic phase-space mixing of counterpropagating electron beams is observed. Quantitatively, the model's performance is evaluated in terms of the accuracy of its predictions of number density distributions, the electric field, and their Fourier decompositions, particularly the growth rate of the fastest-growing unstable mode, as well as particle position, momentum distributions, energy conservation and run time. The model achieves high accuracy with a time step longer than conventional simulation by two orders of magnitude. This work demonstrates that complex plasma dynamics can be learned and shows promise for the development of fast differentiable simulators suitable for solving forward and inverse problems in plasma physics.         ",
    "url": "https://arxiv.org/abs/2503.00274",
    "authors": [
      "Marin Mlinarevi\u0107",
      "George K. Holt",
      "Adriano Agnello"
    ],
    "subjectives": [
      "Plasma Physics (physics.plasm-ph)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2503.00292",
    "title": "Generalization Bounds for Equivariant Networks on Markov Data",
    "abstract": "           Equivariant neural networks play a pivotal role in analyzing datasets with symmetry properties, particularly in complex data structures. However, integrating equivariance with Markov properties presents notable challenges due to the inherent dependencies within such data. Previous research has primarily concentrated on establishing generalization bounds under the assumption of independently and identically distributed data, frequently neglecting the influence of Markov dependencies. In this study, we investigate the impact of Markov properties on generalization performance alongside the role of equivariance within this context. We begin by applying a new McDiarmid's inequality to derive a generalization bound for neural networks trained on Markov datasets, using Rademacher complexity as a central measure of model capacity. Subsequently, we utilize group theory to compute the covering number under equivariant constraints, enabling us to obtain an upper bound on the Rademacher complexity based on this covering number. This bound provides practical insights into selecting low-dimensional irreducible representations, enhancing generalization performance for fixed-width equivariant neural networks.         ",
    "url": "https://arxiv.org/abs/2503.00292",
    "authors": [
      "Hui Li",
      "Zhiguo Wang",
      "Bohui Chen",
      "Li Sheng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00349",
    "title": "Convergence of energy-based learning in linear resistive networks",
    "abstract": "           Energy-based learning algorithms are alternatives to backpropagation and are well-suited to distributed implementations in analog electronic devices. However, a rigorous theory of convergence is lacking. We make a first step in this direction by analysing a particular energy-based learning algorithm, Contrastive Learning, applied to a network of linear adjustable resistors. It is shown that, in this setup, Contrastive Learning is equivalent to projected gradient descent on a convex function, for any step size, giving a guarantee of convergence for the algorithm.         ",
    "url": "https://arxiv.org/abs/2503.00349",
    "authors": [
      "Anne-Men Huijzer",
      "Thomas Chaffey",
      "Bart Besselink",
      "Henk J. van Waarde"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2503.00366",
    "title": "AI-Augmented Thyroid Scintigraphy for Robust Classification",
    "abstract": "           Thyroid scintigraphy is a key imaging modality for diagnosing thyroid disorders. Deep learning models for thyroid scintigraphy classification often face challenges due to limited and imbalanced datasets, leading to suboptimal generalization. In this study, we investigate the effectiveness of different data augmentation techniques including Stable Diffusion (SD), Flow Matching (FM), and Conventional Augmentation (CA) to enhance the performance of a ResNet18 classifier for thyroid condition classification. Our results showed that FM-based augmentation consistently outperforms SD-based approaches, particularly when combined with original (O) data and CA (O+FM+CA), achieving both high accuracy and fair classification across Diffuse Goiter (DG), Nodular Goiter (NG), Normal (NL), and Thyroiditis (TI) cases. The Wilcoxon statistical analysis further validated the superiority of O+FM and its variants (O+FM+CA) over SD-based augmentations in most scenarios. These findings highlight the potential of FM-based augmentation as a superior approach for generating high-quality synthetic thyroid scintigraphy images and improving model generalization in medical image classification.         ",
    "url": "https://arxiv.org/abs/2503.00366",
    "authors": [
      "Maziar Sabouri",
      "Ghasem Hajianfar",
      "Alireza Rafiei Sardouei",
      "Milad Yazdani",
      "Azin Asadzadeh",
      "Soroush Bagheri",
      "Mohsen Arabi",
      "Seyed Rasoul Zakavi",
      "Emran Askari",
      "Atena Aghaee",
      "Dena Shahriari",
      "Habib Zaidi",
      "Arman Rahmim"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00725",
    "title": "Causal Inference on Outcomes Learned from Text",
    "abstract": "           We propose a machine-learning tool that yields causal inference on text in randomized trials. Based on a simple econometric framework in which text may capture outcomes of interest, our procedure addresses three questions: First, is the text affected by the treatment? Second, which outcomes is the effect on? And third, how complete is our description of causal effects? To answer all three questions, our approach uses large language models (LLMs) that suggest systematic differences across two groups of text documents and then provides valid inference based on costly validation. Specifically, we highlight the need for sample splitting to allow for statistical validation of LLM outputs, as well as the need for human labeling to validate substantive claims about how documents differ across groups. We illustrate the tool in a proof-of-concept application using abstracts of academic manuscripts.         ",
    "url": "https://arxiv.org/abs/2503.00725",
    "authors": [
      "Iman Modarressi",
      "Jann Spiess",
      "Amar Venugopal"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2503.00733",
    "title": "UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation",
    "abstract": "           Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training.         ",
    "url": "https://arxiv.org/abs/2503.00733",
    "authors": [
      "Alexander H. Liu",
      "Sang-gil Lee",
      "Chao-Han Huck Yang",
      "Yuan Gong",
      "Yu-Chiang Frank Wang",
      "James R. Glass",
      "Rafael Valle",
      "Bryan Catanzaro"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2503.00760",
    "title": "NCF: Neural Correspondence Field for Medical Image Registration",
    "abstract": "           Deformable image registration is a fundamental task in medical image processing. Traditional optimization-based methods often struggle with accuracy in dealing with complex deformation. Recently, learning-based methods have achieved good performance on public datasets, but the scarcity of medical image data makes it challenging to build a generalizable model to handle diverse real-world scenarios. To address this, we propose a training-data-free learning-based method, Neural Correspondence Field (NCF), which can learn from just one data pair. Our approach employs a compact neural network to model the correspondence field and optimize model parameters for each individual image pair. Consequently, each pair has a unique set of network weights. Notably, our model is highly efficient, utilizing only 0.06 million parameters. Evaluation results showed that the proposed method achieved superior performance on a public Lung CT dataset and outperformed a traditional method on a head and neck dataset, demonstrating both its effectiveness and efficiency.         ",
    "url": "https://arxiv.org/abs/2503.00760",
    "authors": [
      "Lei Zhou",
      "Nimu Yuan",
      "Katjana Ehrlich",
      "Jinyi Qi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00798",
    "title": "$K_{2,3}$-induced minor-free graphs admit quasi-isometry with additive distortion to graphs of tree-width at most two",
    "abstract": "           A graph $H$ is an induced minor of a graph $G$ if $H$ can be obtained from $G$ by a sequence of edge contractions and vertex deletions. Otherwise, $G$ is $H$-induced minor-free. In this paper, we prove that $K_{2,3}$-induced minor-free graphs admit a quasi-isometry with additive distortion to graphs with tree-width at most two. Our result implies that a recent conjecture of Nguyen et al. [Coarse tree-width (2025)] holds for $K_{2,3}$-induced minor-free graphs.         ",
    "url": "https://arxiv.org/abs/2503.00798",
    "authors": [
      "Dibyayan Chakraborty"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2503.00856",
    "title": "Asymptotic Analysis of Two-Layer Neural Networks after One Gradient Step under Gaussian Mixtures Data with Structure",
    "abstract": "           In this work, we study the training and generalization performance of two-layer neural networks (NNs) after one gradient descent step under structured data modeled by Gaussian mixtures. While previous research has extensively analyzed this model under isotropic data assumption, such simplifications overlook the complexities inherent in real-world datasets. Our work addresses this limitation by analyzing two-layer NNs under Gaussian mixture data assumption in the asymptotically proportional limit, where the input dimension, number of hidden neurons, and sample size grow with finite ratios. We characterize the training and generalization errors by leveraging recent advancements in Gaussian universality. Specifically, we prove that a high-order polynomial model performs equivalent to the nonlinear neural networks under certain conditions. The degree of the equivalent model is intricately linked to both the \"data spread\" and the learning rate employed during one gradient step. Through extensive simulations, we demonstrate the equivalence between the original model and its polynomial counterpart across various regression and classification tasks. Additionally, we explore how different properties of Gaussian mixtures affect learning outcomes. Finally, we illustrate experimental results on Fashion-MNIST classification, indicating that our findings can translate to realistic data.         ",
    "url": "https://arxiv.org/abs/2503.00856",
    "authors": [
      "Samet Demir",
      "Zafer Dogan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01116",
    "title": "Large AI Model for Delay-Doppler Domain Channel Prediction in 6G OTFS-Based Vehicular Networks",
    "abstract": "           Channel prediction is crucial for high-mobility vehicular networks, as it enables the anticipation of future channel conditions and the proactive adjustment of communication strategies. However, achieving accurate vehicular channel prediction is challenging due to significant Doppler effects and rapid channel variations resulting from high-speed vehicle movement and complex propagation environments. In this paper, we propose a novel delay-Doppler (DD) domain channel prediction framework tailored for high-mobility vehicular networks. By transforming the channel representation into the DD domain, we obtain an intuitive, sparse, and stable depiction that closely aligns with the underlying physical propagation processes, effectively reducing the complex vehicular channel to a set of time-series parameters with enhanced predictability. Furthermore, we leverage the large artificial intelligence (AI) model to predict these DD-domain time-series parameters, capitalizing on their advanced ability to model temporal correlations. The zero-shot capability of the pre-trained large AI model facilitates accurate channel predictions without requiring task-specific training, while subsequent fine-tuning on specific vehicular channel data further improves prediction accuracy. Extensive simulation results demonstrate the effectiveness of our DD-domain channel prediction framework and the superior accuracy of the large AI model in predicting time-series channel parameters, thereby highlighting the potential of our approach for robust vehicular communication systems.         ",
    "url": "https://arxiv.org/abs/2503.01116",
    "authors": [
      "Jianzhe Xue",
      "Dongcheng Yuan",
      "Zhanxi Ma",
      "Tiankai Jiang",
      "Yu Sun",
      "Haibo Zhou",
      "Xuemin Shen"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01227",
    "title": "Pre-training Graph Neural Networks with Structural Fingerprints for Materials Discovery",
    "abstract": "           In recent years, pre-trained graph neural networks (GNNs) have been developed as general models which can be effectively fine-tuned for various potential downstream tasks in materials science, and have shown significant improvements in accuracy and data efficiency. The most widely used pre-training methods currently involve either supervised training to fit a general force field or self-supervised training by denoising atomic structures equilibrium. Both methods require datasets generated from quantum mechanical calculations, which quickly become intractable when scaling to larger datasets. Here we propose a novel pre-training objective which instead uses cheaply-computed structural fingerprints as targets while maintaining comparable performance across a range of different structural descriptors. Our experiments show this approach can act as a general strategy for pre-training GNNs with application towards large scale foundational models for atomistic data.         ",
    "url": "https://arxiv.org/abs/2503.01227",
    "authors": [
      "Shuyi Jia",
      "Shitij Govil",
      "Manav Ramprasad",
      "Victor Fung"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01361",
    "title": "Statistical physics analysis of graph neural networks: Approaching optimality in the contextual stochastic block model",
    "abstract": "           Graph neural networks (GNNs) are designed to process data associated with graphs. They are finding an increasing range of applications; however, as with other modern machine learning techniques, their theoretical understanding is limited. GNNs can encounter difficulties in gathering information from nodes that are far apart by iterated aggregation steps. This situation is partly caused by so-called oversmoothing; and overcoming it is one of the practically motivated challenges. We consider the situation where information is aggregated by multiple steps of convolution, leading to graph convolutional networks (GCNs). We analyze the generalization performance of a basic GCN, trained for node classification on data generated by the contextual stochastic block model. We predict its asymptotic performance by deriving the free energy of the problem, using the replica method, in the high-dimensional limit. Calling depth the number of convolutional steps, we show the importance of going to large depth to approach the Bayes-optimality. We detail how the architecture of the GCN has to scale with the depth to avoid oversmoothing. The resulting large depth limit can be close to the Bayes-optimality and leads to a continuous GCN. Technically, we tackle this continuous limit via an approach that resembles dynamical mean-field theory (DMFT) with constraints at the initial and final times. An expansion around large regularization allows us to solve the corresponding equations for the performance of the deep GCN. This promising tool may contribute to the analysis of further deep neural networks.         ",
    "url": "https://arxiv.org/abs/2503.01361",
    "authors": [
      "O. Duranthon",
      "L. Zdeborov\u00e1"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01462",
    "title": "S-R2D2: a spherical extension of the R2D2 deep neural network series paradigm for wide-field radio-interferometric imaging",
    "abstract": "           Recently, the R2D2 paradigm, standing for ''Residual-to-Residual DNN series for high-Dynamic-range imaging'', was introduced for image formation in Radio Interferometry (RI) as a learned version of the traditional algorithm CLEAN. The first incarnations of R2D2 are limited to planar imaging on small fields of view, failing to meet the spherical-imaging requirement of modern telescopes observing wide fields. To address this limitation, we propose the spherical-imaging extension S-R2D2. Firstly, as R2D2, S-R2D2 encapsulates its minor cycles in existing 2D-Euclidean deep neural network (DNN) architectures, but adapts its iterative scheme to incorporate the wide-field measurement model mapping a spherical image to visibility data. We implemented this model as the composition of an efficient Fourier-based interpolator mapping the spherical image onto the equatorial plane, with the standard RI operator mapping the equatorial-plane image to visibility data. Importantly, the interpolation step must inevitably be performed at a lower-than-optimal resolution on the plane, to meet the high-resolution requirement on the sphere of wide-field imaging while preserving scalability. Therefore, secondly, we design S-R2D2's DNN training loss to jointly learn to correct the interpolation approximations and identify residual image structures on the sphere, ensuring consistency with the spherical ground truth using the adjoint plane-to-sphere interpolator. Finally, we demonstrate through simulations S-R2D2's capability to perform fast and accurate reconstructions of spherical monochromatic intensity images, across high-resolution, high-dynamic-range settings.         ",
    "url": "https://arxiv.org/abs/2503.01462",
    "authors": [
      "A. Tajja",
      "A. Aghabiglou",
      "E. Tolley",
      "J-P. Kneib",
      "J-P. Thiran",
      "Y. Wiaux"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01495",
    "title": "Improving the statistical efficiency of cross-conformal prediction",
    "abstract": "           Vovk (2015) introduced cross-conformal prediction, a modification of split conformal designed to improve the width of prediction sets. The method, when trained with a miscoverage rate equal to $\\alpha$ and $n \\gg K$, ensures a marginal coverage of at least $1 - 2\\alpha - 2(1-\\alpha)(K-1)/(n+K)$, where $n$ is the number of observations and $K$ denotes the number of folds. A simple modification of the method achieves coverage of at least $1-2\\alpha$. In this work, we propose new variants of both methods that yield smaller prediction sets without compromising the latter theoretical guarantee. The proposed methods are based on recent results deriving more statistically efficient combination of p-values that leverage exchangeability and randomization. Simulations confirm the theoretical findings and bring out some important tradeoffs.         ",
    "url": "https://arxiv.org/abs/2503.01495",
    "authors": [
      "Matteo Gasparin",
      "Aaditya Ramdas"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2503.01505",
    "title": "Lossy Neural Compression for Geospatial Analytics: A Review",
    "abstract": "           Over the past decades, there has been an explosion in the amount of available Earth Observation (EO) data. The unprecedented coverage of the Earth's surface and atmosphere by satellite imagery has resulted in large volumes of data that must be transmitted to ground stations, stored in data centers, and distributed to end users. Modern Earth System Models (ESMs) face similar challenges, operating at high spatial and temporal resolutions, producing petabytes of data per simulated day. Data compression has gained relevance over the past decade, with neural compression (NC) emerging from deep learning and information theory, making EO data and ESM outputs ideal candidates due to their abundance of unlabeled data. In this review, we outline recent developments in NC applied to geospatial data. We introduce the fundamental concepts of NC including seminal works in its traditional applications to image and video compression domains with focus on lossy compression. We discuss the unique characteristics of EO and ESM data, contrasting them with \"natural images\", and explain the additional challenges and opportunities they present. Moreover, we review current applications of NC across various EO modalities and explore the limited efforts in ESM compression to date. The advent of self-supervised learning (SSL) and foundation models (FM) has advanced methods to efficiently distill representations from vast unlabeled data. We connect these developments to NC for EO, highlighting the similarities between the two fields and elaborate on the potential of transferring compressed feature representations for machine--to--machine communication. Based on insights drawn from this review, we devise future directions relevant to applications in EO and ESM.         ",
    "url": "https://arxiv.org/abs/2503.01505",
    "authors": [
      "Carlos Gomes",
      "Isabelle Wittmann",
      "Damien Robert",
      "Johannes Jakubik",
      "Tim Reichelt",
      "Michele Martone",
      "Stefano Maurogiovanni",
      "Rikard Vinge",
      "Jonas Hurst",
      "Erik Scheurer",
      "Rocco Sedona",
      "Thomas Brunschwiler",
      "Stefan Kesselheim",
      "Matej Batic",
      "Philip Stier",
      "Jan Dirk Wegner",
      "Gabriele Cavallaro",
      "Edzer Pebesma",
      "Michael Marszalek",
      "Miguel A Belenguer-Plomer",
      "Kennedy Adriko",
      "Paolo Fraccaro",
      "Romeo Kienzler",
      "Rania Briq",
      "Sabrina Benassou",
      "Michele Lazzarini",
      "Conrad M Albrecht"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Geophysics (physics.geo-ph)"
    ]
  },
  {
    "id": "arXiv:2503.01568",
    "title": "A network psychometric analysis of maths anxiety factors in Italian psychology students",
    "abstract": "           Dealing with mathematics can induce significant anxiety, strongly affecting psychology students' academic performance and career prospects. This phenomenon is known as maths anxiety and several scales can measure it. Most scales were created in English and abbreviated versions were translated and validated among Italian populations (e.g. Abbreviated Maths Anxiety Scale). This study translated the 3-factor MAS-UK scale in Italian to produce a new tool, MAS-IT, validated specifically in a sample of Italian undergraduates enrolled in psychology or related BSc programmes. A sample of 324 Italian undergraduates completed the MAS-IT. The data were analysed using confirmatory Factor Analysis (CFA), testing the original MAS-UK 3-factor model. CFA results revealed that the original MAS-UK 3-factor model did not fit the Italian data. A subsequent Exploratory Graph Analysis (EGA) identified 4 distinct components/factors of maths anxiety detected by MAS-IT. The items relative to \"Passive Observation maths anxiety\" factor remained stable across the analyses, whereas \"Evaluation maths anxiety\" and \"Everyday/Social maths anxiety\" items showed a reduced or poor item stability. Quantitative findings indicated potential cultural or contextual differences in the expression of maths anxiety in today's psychology undergraduates, underlining the need for more appropriate tools to be used among psychology students.         ",
    "url": "https://arxiv.org/abs/2503.01568",
    "authors": [
      "Emma Franchino",
      "Luciana Ciringione",
      "Luisa Canal",
      "Ottavia Marina Epifania",
      "Luigi Lombardi",
      "Gianluca Lattanzi",
      "Massimo Stella"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2503.01762",
    "title": "Noisy-enhanced quantum search on complex networks",
    "abstract": "           The task of finding an element in an unstructured database is known as spatial search and can be expressed as a quantum walk evolution on a graph. In this Letter, we modify the usual search problem by adding an extra trapping vertex to the graph, which is only connected to the target element. The walker evolution is a mix between classical and quantum walk search dynamics. The balance between unitary and non-unitary dynamics is tuned with a parameter, and we numerically show that depending on the graph topology and the connectivity of the target element, this hybrid approach can outperform a purely classical or quantum evolution for reaching the trapping site. We show that this behavior is only observed in the presence of an extra trapping site, and that depending on the topology, the increase of non-unitary operations can be compensated by increasing the strength of the quantum walk exploration. This compensation comes at the cost of reducing the searching feature of the evolution induced by the Hamiltonian. We also relate the optimal hybrid regime to the entropy's decay rate. As the introduction of non-unitary operations may be considered as noise, we interpret this phenomena as a noisy-assisted quantum evolution.         ",
    "url": "https://arxiv.org/abs/2503.01762",
    "authors": [
      "Ugo Nzongani",
      "Andrea Simonetto",
      "Giuseppe Di Molfetta"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2010.12785",
    "title": "ShiftAddNet: A Hardware-Inspired Deep Network",
    "abstract": "           Multiplication (e.g., convolution) is arguably a cornerstone of modern deep neural networks (DNNs). However, intensive multiplications cause expensive resource costs that challenge DNNs' deployment on resource-constrained edge devices, driving several attempts for multiplication-less deep networks. This paper presented ShiftAddNet, whose main inspiration is drawn from a common practice in energy-efficient hardware implementation, that is, multiplication can be instead performed with additions and logical bit-shifts. We leverage this idea to explicitly parameterize deep networks in this way, yielding a new type of deep network that involves only bit-shift and additive weight layers. This hardware-inspired ShiftAddNet immediately leads to both energy-efficient inference and training, without compromising the expressive capacity compared to standard DNNs. The two complementary operation types (bit-shift and add) additionally enable finer-grained control of the model's learning capacity, leading to more flexible trade-off between accuracy and (training) efficiency, as well as improved robustness to quantization and pruning. We conduct extensive experiments and ablation studies, all backed up by our FPGA-based ShiftAddNet implementation and energy measurements. Compared to existing DNNs or other multiplication-less models, ShiftAddNet aggressively reduces over 80% hardware-quantified energy cost of DNNs training and inference, while offering comparable or better accuracies. Codes and pre-trained models are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2010.12785",
    "authors": [
      "Haoran You",
      "Xiaohan Chen",
      "Yongan Zhang",
      "Chaojian Li",
      "Sicheng Li",
      "Zihao Liu",
      "Zhangyang Wang",
      "Yingyan Celine Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2101.02338",
    "title": "Max-Affine Spline Insights Into Deep Network Pruning",
    "abstract": "           In this paper, we study the importance of pruning in Deep Networks (DNs) and the yin & yang relationship between (1) pruning highly overparametrized DNs that have been trained from random initialization and (2) training small DNs that have been \"cleverly\" initialized. As in most cases practitioners can only resort to random initialization, there is a strong need to develop a grounded understanding of DN pruning. Current literature remains largely empirical, lacking a theoretical understanding of how pruning affects DNs' decision boundary, how to interpret pruning, and how to design corresponding principled pruning techniques. To tackle those questions, we propose to employ recent advances in the theoretical analysis of Continuous Piecewise Affine (CPA) DNs. From this perspective, we will be able to detect the early-bird (EB) ticket phenomenon, provide interpretability into current pruning techniques, and develop a principled pruning strategy. In each step of our study, we conduct extensive experiments supporting our claims and results; while our main goal is to enhance the current understanding towards DN pruning instead of developing a new pruning method, our spline pruning criteria in terms of layerwise and global pruning is on par with or even outperforms state-of-the-art pruning methods.         ",
    "url": "https://arxiv.org/abs/2101.02338",
    "authors": [
      "Haoran You",
      "Randall Balestriero",
      "Zhihan Lu",
      "Yutong Kou",
      "Huihong Shi",
      "Shunyao Zhang",
      "Shang Wu",
      "Yingyan Celine Lin",
      "Richard Baraniuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2107.05165",
    "title": "Test Script Intention Generation for Mobile Application via GUI Image and Code Understanding",
    "abstract": "           Testing is the most direct and effective technique to ensure software quality. Test scripts always play a more important role in mobile app testing than test cases for source code, due to the GUI-intensive and event-driven characteristics of mobile applications (app). Test scripts focus on user interactions and the corresponding response events, which is significant for testing the target app functionalities. Therefore, it is critical to understand the test scripts for better script maintenance and modification. There exist some mature code understanding (i.e., code comment generation) technologies that can be directly applied to functionality source code with business logic. However, such technologies will have difficulties when being applied to test scripts, because test scripts are loosely linked to apps under test (AUT) by widget selectors, and do not contain business logic themselves. In order to solve the test script understanding gap, this paper presents a novel approach, namely TestIntention, to infer the intention of GUI test scripts. Test intention refers to the user expectations of app behaviors for specific operations. TestIntention formalizes test scripts with an operation sequence model. For each operation within the sequence, TestIntention extracts the target widget selector and links the selector to the GUI layout information or the corresponding response events. For widgets identified by XPath, TestIntention utilizes the image understanding technologies to explore the detailed information of the widget images, the intention of which is understood with a deep learning model. For widgets identified by ID, TestIntention first maps the selectors to the response methods with business logic, and then adopts code understanding technologies to describe code in natural language form. Results of all operations are combined to generate test intention for test scripts.         ",
    "url": "https://arxiv.org/abs/2107.05165",
    "authors": [
      "Shengcheng Yu",
      "Chunrong Fang",
      "Jia Liu",
      "Zhenyu Chen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2205.08119",
    "title": "ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks",
    "abstract": "           Neural networks (NNs) with intensive multiplications (e.g., convolutions and transformers) are capable yet power hungry, impeding their more extensive deployment into resource-constrained devices. As such, multiplication-free networks, which follow a common practice in energy-efficient hardware implementation to parameterize NNs with more efficient operators (e.g., bitwise shifts and additions), have gained growing attention. However, multiplication-free networks usually under-perform their vanilla counterparts in terms of the achieved accuracy. To this end, this work advocates hybrid NNs that consist of both powerful yet costly multiplications and efficient yet less powerful operators for marrying the best of both worlds, and proposes ShiftAddNAS, which can automatically search for more accurate and more efficient NNs. Our ShiftAddNAS highlights two enablers. Specifically, it integrates (1) the first hybrid search space that incorporates both multiplication-based and multiplication-free operators for facilitating the development of both accurate and efficient hybrid NNs; and (2) a novel weight sharing strategy that enables effective weight sharing among different operators that follow heterogeneous distributions (e.g., Gaussian for convolutions vs. Laplacian for add operators) and simultaneously leads to a largely reduced supernet size and much better searched networks. Extensive experiments and ablation studies on various models, datasets, and tasks consistently validate the efficacy of ShiftAddNAS, e.g., achieving up to a +7.7% higher accuracy or a +4.9 better BLEU score compared to state-of-the-art NN, while leading to up to 93% or 69% energy and latency savings, respectively. Codes and pretrained models are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2205.08119",
    "authors": [
      "Haoran You",
      "Baopu Li",
      "Huihong Shi",
      "Yonggan Fu",
      "Yingyan Celine Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2209.04821",
    "title": "Local-Aware Global Attention Network for Person Re-Identification Based on Body and Hand Images",
    "abstract": "           Learning representative, robust and discriminative information from images is essential for effective person re-identification (Re-Id). In this paper, we propose a compound approach for end-to-end discriminative deep feature learning for person Re-Id based on both body and hand images. We carefully design the Local-Aware Global Attention Network (LAGA-Net), a multi-branch deep network architecture consisting of one branch for spatial attention, one branch for channel attention, one branch for global feature representations and another branch for local feature representations. The attention branches focus on the relevant features of the image while suppressing the irrelevant backgrounds. In order to overcome the weakness of the attention mechanisms, equivariant to pixel shuffling, we integrate relative positional encodings into the spatial attention module to capture the spatial positions of pixels. The global branch intends to preserve the global context or structural information. For the the local branch, which intends to capture the fine-grained information, we perform uniform partitioning to generate stripes on the conv-layer horizontally. We retrieve the parts by conducting a soft partition without explicitly partitioning the images or requiring external cues such as pose estimation. A set of ablation study shows that each component contributes to the increased performance of the LAGA-Net. Extensive evaluations on four popular body-based person Re-Id benchmarks and two publicly available hand datasets demonstrate that our proposed method consistently outperforms existing state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2209.04821",
    "authors": [
      "Nathanael L. Baisa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2303.15263",
    "title": "Joint Person Identity, Gender and Age Estimation from Hand Images using Deep Multi-Task Representation Learning",
    "abstract": "           In this paper, we propose a multi-task representation learning framework to jointly estimate the identity, gender and age of individuals from their hand images for the purpose of criminal investigations since the hand images are often the only available information in cases of serious crime such as sexual abuse. We investigate different up-to-date deep learning architectures and compare their performance for joint estimation of identity, gender and age from hand images of perpetrators of serious crime. To simplify the age prediction, we create age groups for the age estimation. We make extensive evaluations and comparisons of both convolution-based and transformer-based deep learning architectures on a publicly available 11k hands dataset. Our experimental analysis shows that it is possible to efficiently estimate not only identity but also other attributes such as gender and age of suspects jointly from hand images for criminal investigations, which is crucial in assisting international police forces in the court to identify and convict abusers.         ",
    "url": "https://arxiv.org/abs/2303.15263",
    "authors": [
      "Nathanael L. Baisa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.02484",
    "title": "A Deterministic Construction of a Large Distance Code from the Wozencraft Ensemble",
    "abstract": "           We present an explicit construction of a sequence of rate $1/2$ Wozencraft ensemble codes (over any fixed finite field $\\mathbb{F}_q$) that achieve minimum distance $\\Omega(\\sqrt{k})$ where $k$ is the message length. The coefficients of the Wozencraft ensemble codes are constructed using Sidon Sets and the cyclic structure of $\\mathbb{F}_{q^{k}}$ where $k+1$ is prime with $q$ a primitive root modulo $k+1$. Assuming Artin's conjecture, there are infinitely many such $k$ for any prime power $q$.         ",
    "url": "https://arxiv.org/abs/2305.02484",
    "authors": [
      "Venkatesan Guruswami",
      "Shilun Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2306.12033",
    "title": "End-to-End Augmentation Hyperparameter Tuning for Self-Supervised Anomaly Detection",
    "abstract": "           Self-supervised learning (SSL) has emerged as a promising paradigm that presents supervisory signals to real-world problems, bypassing the extensive cost of manual labeling. Consequently, self-supervised anomaly detection (SSAD) has seen a recent surge of interest, since SSL is especially attractive for unsupervised tasks. However, recent works have reported that the choice of a data augmentation function has significant impact on the accuracy of SSAD, posing augmentation search as an essential but nontrivial problem with the lack of labeled validation data. In this paper, we introduce ST-SSAD, the first systematic approach for rigorous augmentation tuning on SSAD. To this end, our work presents two key contributions. The first is a new unsupervised validation loss that quantifies the alignment between augmented training data and unlabeled validation data. The second is new differentiable augmentation functions, allowing data augmentation hyperparameter(s) to be tuned in an end-to-end manner. Experiments on two testbeds with semantic class anomalies and subtle industrial defects show that ST-SSAD gives significant performance gains over existing works.         ",
    "url": "https://arxiv.org/abs/2306.12033",
    "authors": [
      "Jaemin Yoo",
      "Lingxiao Zhao",
      "Leman Akoglu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2307.06616",
    "title": "SecureFalcon: Are We There Yet in Automated Software Vulnerability Detection with LLMs?",
    "abstract": "           Software vulnerabilities can cause numerous problems, including crashes, data loss, and security breaches. These issues greatly compromise quality and can negatively impact the market adoption of software applications and systems. Traditional bug-fixing methods, such as static analysis, often produce false positives. While bounded model checking, a form of Formal Verification (FV), can provide more accurate outcomes compared to static analyzers, it demands substantial resources and significantly hinders developer productivity. Can Machine Learning (ML) achieve accuracy comparable to FV methods and be used in popular instant code completion frameworks in near real-time? In this paper, we introduce SecureFalcon, an innovative model architecture with only 121 million parameters derived from the Falcon-40B model and explicitly tailored for classifying software vulnerabilities. To achieve the best performance, we trained our model using two datasets, namely the FormAI dataset and the FalconVulnDB. The FalconVulnDB is a combination of recent public datasets, namely the SySeVR framework, Draper VDISC, Bigvul, Diversevul, SARD Juliet, and ReVeal datasets. These datasets contain the top 25 most dangerous software weaknesses, such as CWE-119, CWE-120, CWE-476, CWE-122, CWE-190, CWE-121, CWE-78, CWE-787, CWE-20, and CWE-762. SecureFalcon achieves 94% accuracy in binary classification and up to 92% in multiclassification, with instant CPU inference times. It outperforms existing models such as BERT, RoBERTa, CodeBERT, and traditional ML algorithms, promising to push the boundaries of software vulnerability detection and instant code completion frameworks.         ",
    "url": "https://arxiv.org/abs/2307.06616",
    "authors": [
      "Mohamed Amine Ferrag",
      "Ammar Battah",
      "Norbert Tihanyi",
      "Ridhi Jain",
      "Diana Maimut",
      "Fatima Alwahedi",
      "Thierry Lestable",
      "Narinderjit Singh Thandi",
      "Abdechakour Mechri",
      "Merouane Debbah",
      "Lucas C. Cordeiro"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.01405",
    "title": "Representation Engineering: A Top-Down Approach to AI Transparency",
    "abstract": "           In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.         ",
    "url": "https://arxiv.org/abs/2310.01405",
    "authors": [
      "Andy Zou",
      "Long Phan",
      "Sarah Chen",
      "James Campbell",
      "Phillip Guo",
      "Richard Ren",
      "Alexander Pan",
      "Xuwang Yin",
      "Mantas Mazeika",
      "Ann-Kathrin Dombrowski",
      "Shashwat Goel",
      "Nathaniel Li",
      "Michael J. Byun",
      "Zifan Wang",
      "Alex Mallen",
      "Steven Basart",
      "Sanmi Koyejo",
      "Dawn Song",
      "Matt Fredrikson",
      "J. Zico Kolter",
      "Dan Hendrycks"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2310.04285",
    "title": "Assessing Robustness via Score-Based Adversarial Image Generation",
    "abstract": "           Most adversarial attacks and defenses focus on perturbations within small $\\ell_p$-norm constraints. However, $\\ell_p$ threat models cannot capture all relevant semantics-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate unrestricted adversarial examples that overcome the limitations of $\\ell_p$-norm constraints. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG improves upon the majority of state-of-the-art attacks and defenses across multiple benchmarks. This work highlights the importance of investigating adversarial examples bounded by semantics rather than $\\ell_p$-norm constraints. ScoreAG represents an important step towards more encompassing robustness assessments.         ",
    "url": "https://arxiv.org/abs/2310.04285",
    "authors": [
      "Marcel Kollovieh",
      "Lukas Gosch",
      "Yan Scholten",
      "Marten Lienen",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2310.08045",
    "title": "Model Predictive Inferential Control of Neural State-Space Models for Autonomous Vehicle Motion Planning",
    "abstract": "           Model predictive control (MPC) has proven useful in enabling safe and optimal motion planning for autonomous vehicles. In this paper, we investigate how to achieve MPC-based motion planning when a neural state-space model represents the vehicle dynamics. As the neural state-space model will lead to highly complex, nonlinear and nonconvex optimization landscapes, mainstream gradient-based MPC methods will be computationally too heavy to be a viable solution. In a departure, we propose the idea of model predictive inferential control (MPIC), which seeks to infer the best control decisions from the control objectives and constraints. Following the idea, we convert the MPC problem for motion planning into a Bayesian state estimation problem. Then, we develop a new particle filtering/smoothing approach to perform the estimation. This approach is implemented as banks of unscented Kalman filters/smoothers and offers high sampling efficiency, fast computation, and estimation accuracy. We evaluate the MPIC approach through a simulation study of autonomous driving in different scenarios, along with an exhaustive comparison with gradient-based MPC. The results show that the MPIC approach has considerable computational efficiency, regardless of complex neural network architectures, and shows the capability to solve large-scale MPC problems for neural state-space models.         ",
    "url": "https://arxiv.org/abs/2310.08045",
    "authors": [
      "Iman Askari",
      "Ali Vaziri",
      "Xumein Tu",
      "Shen Zeng",
      "Huazhen Fang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2310.13104",
    "title": "Within-Dataset Disclosure Risk for Differential Privacy",
    "abstract": "           Differential privacy (DP) enables private data analysis. In a typical DP deployment, controllers manage individuals' sensitive data and are responsible for answering analysts' queries while protecting individuals' privacy. They do so by choosing the privacy parameter $\\epsilon$, which controls the degree of privacy for all individuals in all possible datasets. However, it is challenging for controllers to choose $\\epsilon$ because of the difficulty of interpreting the privacy implications of such a choice on the within-dataset individuals. To address this challenge, we first derive a relative disclosure risk indicator (RDR) that indicates the impact of choosing $\\epsilon$ on the within-dataset individuals' disclosure risk. We then design an algorithm to find $\\epsilon$ based on controllers' privacy preferences expressed as a function of the within-dataset individuals' RDRs, and an alternative algorithm that finds and releases $\\epsilon$ while satisfying DP. Lastly, we propose a solution that bounds the total privacy leakage when using the algorithm to answer multiple queries without requiring controllers to set the total privacy budget. We evaluate our contributions through an IRB-approved user study that shows the RDR is useful for helping controllers choose $\\epsilon$, and experimental evaluations showing our algorithms are efficient and scalable.         ",
    "url": "https://arxiv.org/abs/2310.13104",
    "authors": [
      "Zhiru Zhu",
      "Raul Castro Fernandez"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2310.13167",
    "title": "Visualizing Causality in Mixed Reality for Manual Task Learning: An Exploratory Study",
    "abstract": "           Mixed Reality (MR) is gaining prominence in manual task skill learning due to its in-situ, embodied, and immersive experience. To teach manual tasks, current methodologies break the task into hierarchies (tasks into subtasks) and visualize the current subtask and future in terms of causality. Existing psychology literature also shows that humans learn tasks by breaking them into hierarchies. In order to understand the design space of information visualized to the learner for better task understanding, we conducted a user study with 48 users. The study was conducted using a complex assembly task, which involves learning of both actions and tool usage. We aim to explore the effect of visualization of causality in the hierarchy for manual task learning in MR by four options: no causality, event level causality, interaction level causality, and gesture level causality. The results show that the user understands and performs best when all the level of causality is shown to the user. Based on the results, we further provide design recommendations and in-depth discussions for future manual task learning systems.         ",
    "url": "https://arxiv.org/abs/2310.13167",
    "authors": [
      "Rahul Jain",
      "Jingyu Shi",
      "Andrew Benton",
      "Moiz Rasheed",
      "Hyungjun Doh",
      "Subramanian Chidambaram",
      "Karthik Ramani"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2310.16684",
    "title": "Local Statistics for Generative Image Detection",
    "abstract": "           Diffusion models (DMs) are generative models that learn to synthesize images from Gaussian noise. DMs can be trained to do a variety of tasks such as image generation and image super-resolution. Researchers have made significant improvements in the capability of synthesizing photorealistic images in the past few years. These successes also hasten the need to address the potential misuse of synthesized images. In this paper, we highlighted the effectiveness of Bayer pattern and local statistics in distinguishing digital camera images from DM-generated images. We further hypothesized that local statistics should be used to address the spatial non-stationarity problems in images. We showed that our approach produced promising results for distinguishing real images from synthesized images. This approach is also robust to various perturbations such as image resizing and JPEG compression.         ",
    "url": "https://arxiv.org/abs/2310.16684",
    "authors": [
      "Yung Jer Wong",
      "Teck Khim Ng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.19163",
    "title": "RAIFLE: Reconstruction Attacks on Interaction-based Federated Learning with Adversarial Data Manipulation",
    "abstract": "           Federated learning has emerged as a promising privacy-preserving solution for machine learning domains that rely on user interactions, particularly recommender systems and online learning to rank. While there has been substantial research on the privacy of traditional federated learning, little attention has been paid to the privacy properties of these interaction-based settings. In this work, we show that users face an elevated risk of having their private interactions reconstructed by the central server when the server can control the training features of the items that users interact with. We introduce RAIFLE, a novel optimization-based attack framework where the server actively manipulates the features of the items presented to users to increase the success rate of reconstruction. Our experiments with federated recommendation and online learning-to-rank scenarios demonstrate that RAIFLE is significantly more powerful than existing reconstruction attacks like gradient inversion, achieving high performance consistently in most settings. We discuss the pros and cons of several possible countermeasures to defend against RAIFLE in the context of interaction-based federated learning. Our code is open-sourced at this https URL.         ",
    "url": "https://arxiv.org/abs/2310.19163",
    "authors": [
      "Dzung Pham",
      "Shreyas Kulkarni",
      "Amir Houmansadr"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.14922",
    "title": "GDTS: Goal-Guided Diffusion Model with Tree Sampling for Multi-Modal Pedestrian Trajectory Prediction",
    "abstract": "           Accurate prediction of pedestrian trajectories is crucial for improving the safety of autonomous driving. However, this task is generally nontrivial due to the inherent stochasticity of human motion, which naturally requires the predictor to generate multi-modal prediction. Previous works leverage various generative methods, such as GAN and VAE, for pedestrian trajectory prediction. Nevertheless, these methods may suffer from mode collapse and relatively low-quality results. The denoising diffusion probabilistic model (DDPM) has recently been applied to trajectory prediction due to its simple training process and powerful reconstruction ability. However, current diffusion-based methods do not fully utilize input information and usually require many denoising iterations that lead to a long inference time or an additional network for initialization. To address these challenges and facilitate the use of diffusion models in multi-modal trajectory prediction, we propose GDTS, a novel Goal-Guided Diffusion Model with Tree Sampling for multi-modal trajectory prediction. Considering the \"goal-driven\" characteristics of human motion, GDTS leverages goal estimation to guide the generation of the diffusion network. A two-stage tree sampling algorithm is presented, which leverages common features to reduce the inference time and improve accuracy for multi-modal prediction. Experimental results demonstrate that our proposed framework achieves comparable state-of-the-art performance with real-time inference speed in public datasets.         ",
    "url": "https://arxiv.org/abs/2311.14922",
    "authors": [
      "Ge Sun",
      "Sheng Wang",
      "Lei Zhu",
      "Ming Liu",
      "Jun Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2311.18674",
    "title": "LiteQSign: Lightweight and Scalable Post-Quantum Authentication for Heterogeneous IoT Applications",
    "abstract": "           Traditional digital signatures face significant challenges on low-end IoT devices due to limited computational power, memory, and energy resources. Simultaneously, the rise of quantum computing necessitates post-quantum (PQ) secure alternatives. However, PQ-secure signatures typically incur substantial costs, hindering their adoption in IoT applications like wearable health devices, trackers, and smart sensors, where efficient signature generation is critical for prolonged device lifespan and optimal resource utilization. To address these challenges, we present LightQSign (LightQS), a novel lightweight PQ signature scheme that achieves near-optimal signature generation efficiency with only a small, constant number of hash operations per signing process. The core innovation lies in enabling verifiers to obtain one-time hash-based public keys without interacting with signers or third parties through secure computation. This design eliminates the need for non-colluding verification servers, secure enclaves, or trusted assisting entities, thereby reducing security risks and extending IoT device battery life with minimal cryptographic overhead. We formally prove the security of LiteQS in the random oracle model and conduct a thorough performance analysis, demonstrating that it outperforms NIST PQC standards with significantly lower computational overhead, minimal memory usage, and a compact signature footprint. Experiments on an 8-bit microcontroller show that LiteQS achieves 20x faster signature generation while producing smaller signature and private key sizes compared to state-of-the-art schemes.         ",
    "url": "https://arxiv.org/abs/2311.18674",
    "authors": [
      "Attila A. Yavuz",
      "Saleh Darzi",
      "Saif E. Nouma"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2312.01786",
    "title": "Output-sensitive Complexity of Multi-Objective Integer Network Flow Problems",
    "abstract": "           This paper addresses the output-sensitive complexity for linear multi-objective integer minimum cost flow (MOIMCF) problems and provides insights about the time complexity for enumerating all supported nondominated vectors. The paper shows that there can not exist an output-polynomial time algorithm for the enumeration of all supported nondominated vectors that determine the vectors in an ordered way in the outcome space unless NP = P. Moreover, novel methods for identifying supported nondominated vectors in bi-objective minimum cost flow (BOIMCF) problems are proposed, accompanied by a numerical comparison between decision- and objective-space methods. A novel, equivalent and more compact formulation of the minimum cost flow ILP formulation used in the e-constrained-scalarization approach is introduced, demonstrating enhanced efficiency in the numerical tests         ",
    "url": "https://arxiv.org/abs/2312.01786",
    "authors": [
      "David K\u00f6nen",
      "Michael Stiglmayr"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2312.04465",
    "title": "FitDiff: Robust monocular 3D facial shape and reflectance estimation using Diffusion Models",
    "abstract": "           The remarkable progress in 3D face reconstruction has resulted in high-detail and photorealistic facial representations. Recently, Diffusion Models have revolutionized the capabilities of generative methods by surpassing the performance of GANs. In this work, we present FitDiff, a diffusion-based 3D facial avatar generative model. Leveraging diffusion principles, our model accurately generates relightable facial avatars, utilizing an identity embedding extracted from an \"in-the-wild\" 2D facial image. The introduced multi-modal diffusion model is the first to concurrently output facial reflectance maps (diffuse and specular albedo and normals) and shapes, showcasing great generalization capabilities. It is solely trained on an annotated subset of a public facial dataset, paired with 3D reconstructions. We revisit the typical 3D facial fitting approach by guiding a reverse diffusion process using perceptual and face recognition losses. Being the first 3D LDM conditioned on face recognition embeddings, FitDiff reconstructs relightable human avatars, that can be used as-is in common rendering engines, starting only from an unconstrained facial image, and achieving state-of-the-art performance.         ",
    "url": "https://arxiv.org/abs/2312.04465",
    "authors": [
      "Stathis Galanakis",
      "Alexandros Lattas",
      "Stylianos Moschoglou",
      "Stefanos Zafeiriou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.08671",
    "title": "Permutation-Invariant Graph Partitioning:How Graph Neural Networks Capture Structural Interactions?",
    "abstract": "           Graph Neural Networks (GNNs) have paved the way for being a cornerstone in graph-related learning tasks. Yet, the ability of GNNs to capture structural interactions within graphs remains under-explored. In this work, we address this gap by drawing on the insight that permutation invariant graph partitioning enables a powerful way of exploring structural interactions. We establish theoretical connections between permutation invariant graph partitioning and graph isomorphism, and then propose Graph Partitioning Neural Networks (GPNNs), a novel architecture that efficiently enhances the expressive power of GNNs in learning structural interactions. We analyze how partitioning schemes and structural interactions contribute to GNN expressivity and their trade-offs with complexity. Empirically, we demonstrate that GPNNs outperform existing GNN models in capturing structural interactions across diverse graph benchmark tasks.         ",
    "url": "https://arxiv.org/abs/2312.08671",
    "authors": [
      "Asela Hevapathige",
      "Qing Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.08603",
    "title": "Representation Learning in a Decomposed Encoder Design for Bio-inspired Hebbian Learning",
    "abstract": "           Modern data-driven machine learning system designs exploit inductive biases in architectural structure, invariance and equivariance requirements, task-specific loss functions, and computational optimization tools. Previous works have illustrated that human-specified quasi-invariant filters can serve as a powerful inductive bias in the early layers of the encoder, enhancing robustness and transparency in learned classifiers. This paper explores this further within the context of representation learning with bio-inspired Hebbian learning rules. We propose a modular framework trained with a bio-inspired variant of contrastive predictive coding, comprising parallel encoders that leverage different invariant visual descriptors as inductive biases. We evaluate the representation learning capacity of our system in classification scenarios using diverse image datasets (GTSRB, STL10, CODEBRIM) and video datasets (UCF101). Our findings indicate that this form of inductive bias significantly improves the robustness of learned representations and narrows the performance gap between models using local Hebbian plasticity rules and those using backpropagation, while also achieving superior performance compared to non-decomposed encoders.         ",
    "url": "https://arxiv.org/abs/2401.08603",
    "authors": [
      "Achref Jaziri",
      "Sina Ditzel",
      "Iuliia Pliushch",
      "Visvanathan Ramesh"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.02005",
    "title": "Topology-Informed Graph Transformer",
    "abstract": "           Transformers have revolutionized performance in Natural Language Processing and Vision, paving the way for their integration with Graph Neural Networks (GNNs). One key challenge in enhancing graph transformers is strengthening the discriminative power of distinguishing isomorphisms of graphs, which plays a crucial role in boosting their predictive performances. To address this challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel transformer enhancing both discriminative power in detecting graph isomorphisms and the overall performance of Graph Transformers. TIGT consists of four components: A topological positional embedding layer using non-isomorphic universal covers based on cyclic subgraphs of graphs to ensure unique graph representation: A dual-path message-passing layer to explicitly encode topological characteristics throughout the encoder layers: A global attention mechanism: And a graph information layer to recalibrate channel-wise graph features for better feature representation. TIGT outperforms previous Graph Transformers in classifying synthetic dataset aimed at distinguishing isomorphism classes of graphs. Additionally, mathematical analysis and empirical evaluations highlight our model's competitive edge over state-of-the-art Graph Transformers across various benchmark datasets.         ",
    "url": "https://arxiv.org/abs/2402.02005",
    "authors": [
      "Yun Young Choi",
      "Sun Woo Park",
      "Minho Lee",
      "Youngho Woo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.02112",
    "title": "S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and Generation",
    "abstract": "           Autonomous driving simulation system plays a crucial role in enhancing self-driving data and simulating complex and rare traffic scenarios, ensuring navigation safety. However, traditional simulation systems, which often heavily rely on manual modeling and 2D image editing, struggled with scaling to extensive scenes and generating realistic simulation data. In this study, we present S-NeRF++, an innovative autonomous driving simulation system based on neural reconstruction. Trained on widely-used self-driving datasets such as nuScenes and Waymo, S-NeRF++ can generate a large number of realistic street scenes and foreground objects with high rendering quality as well as offering considerable flexibility in manipulation and simulation. Specifically, S-NeRF++ is an enhanced neural radiance field for synthesizing large-scale scenes and moving vehicles, with improved scene parameterization and camera pose learning. The system effectively utilizes noisy and sparse LiDAR data to refine training and address depth outliers, ensuring high-quality reconstruction and novel-view rendering. It also provides a diverse foreground asset bank by reconstructing and generating different foreground vehicles to support comprehensive scenario this http URL, we have developed an advanced foreground-background fusion pipeline that skillfully integrates illumination and shadow effects, further enhancing the realism of our simulations. With the high-quality simulated data provided by our S-NeRF++, we found the perception methods enjoy performance boosts on several autonomous driving downstream tasks, further demonstrating our proposed simulator's effectiveness.         ",
    "url": "https://arxiv.org/abs/2402.02112",
    "authors": [
      "Yurui Chen",
      "Junge Zhang",
      "Ziyang Xie",
      "Wenye Li",
      "Feihu Zhang",
      "Jiachen Lu",
      "Li Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.09154",
    "title": "Attacking Large Language Models with Projected Gradient Descent",
    "abstract": "           Current LLM alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.         ",
    "url": "https://arxiv.org/abs/2402.09154",
    "authors": [
      "Simon Geisler",
      "Tom Wollschl\u00e4ger",
      "M. H. I. Abdalla",
      "Johannes Gasteiger",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.09911",
    "title": "Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering",
    "abstract": "           Mitigating the hallucinations of Large Language Models is a crucial task. Although some existing methods employ self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Meanwhile, Knowledge Graph (KG) enhancement approaches fail to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, we propose a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification (PG\\&AKV). Enhancement of open-ended question-answering begins with leveraging the Pseudo-Graph Generation to provide the related knowledge framework. Subsequently, Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ended questions. For precise-answered questions, we observe a minimum accuracy improvement of 7.5%. Moreover, PG\\&AKV also exhibits generalizability across different KG sources. Utilizing KG different from the question sources, PG\\&AKV can even achieve at least a 3.5 % performance improvement. In summary, our results pave the way for enhancing LLMs by incorporating Pseudo- and Multisource-KGs, particularly in the filed of open-ended questions.         ",
    "url": "https://arxiv.org/abs/2402.09911",
    "authors": [
      "Jiaxiang Liu",
      "Tong Zhou",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.13496",
    "title": "Heterogeneous Graph Neural Network on Semantic Tree",
    "abstract": "           The recent past has seen an increasing interest in Heterogeneous Graph Neural Networks (HGNNs), since many real-world graphs are heterogeneous in nature, from citation graphs to email graphs. However, existing methods ignore a tree hierarchy among metapaths, naturally constituted by different node types and relation types. In this paper, we present HetTree, a novel HGNN that models both the graph structure and heterogeneous aspects in a scalable and effective manner. Specifically, HetTree builds a semantic tree data structure to capture the hierarchy among metapaths. To effectively encode the semantic tree, HetTree uses a novel subtree attention mechanism to emphasize metapaths that are more helpful in encoding parent-child relationships. Moreover, HetTree proposes carefully matching pre-computed features and labels correspondingly, constituting a complete metapath representation. Our evaluation of HetTree on a variety of real-world datasets demonstrates that it outperforms all existing baselines on open benchmarks and efficiently scales to large real-world graphs with millions of nodes and edges.         ",
    "url": "https://arxiv.org/abs/2402.13496",
    "authors": [
      "Mingyu Guan",
      "Jack W. Stokes",
      "Qinlong Luo",
      "Fuchen Liu",
      "Purvanshi Mehta",
      "Elnaz Nouri",
      "Taesoo Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.18477",
    "title": "Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes",
    "abstract": "           Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via \"which variables enter the differential of which other variables\". In this paper, we develop conditional independence (CI) constraints on coordinate processes over selected intervals that are Markov with respect to the acyclic dependence graph (allowing self-loops) induced by a general SDE model. We then provide a sound and complete causal discovery algorithm, capable of handling both fully and partially observed data, and uniquely recovering the underlying or induced ancestral graph by exploiting time directionality assuming a CI oracle. Finally, to make our algorithm practically usable, we also propose a flexible, consistent signature kernel-based CI test to infer these constraints from data. We extensively benchmark the CI test in isolation and as part of our causal discovery algorithms, outperforming existing approaches in SDE models and beyond.         ",
    "url": "https://arxiv.org/abs/2402.18477",
    "authors": [
      "Georg Manten",
      "Cecilia Casolo",
      "Emilio Ferrucci",
      "S\u00f8ren Wengel Mogensen",
      "Cristopher Salvi",
      "Niki Kilbertus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.18945",
    "title": "SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via Syntactic Transfer",
    "abstract": "           Although pre-training achieves remarkable performance, it suffers from task-agnostic backdoor attacks due to vulnerabilities in data and training mechanisms. These attacks can transfer backdoors to various downstream tasks. In this paper, we introduce $\\mathtt{maxEntropy}$, an entropy-based poisoning filter that mitigates such risks. To overcome the limitations of manual target setting and explicit triggers, we propose $\\mathtt{SynGhost}$, an invisible and universal task-agnostic backdoor attack via syntactic transfer, further exposing vulnerabilities in pre-trained language models (PLMs). Specifically, $\\mathtt{SynGhost}$ injects multiple syntactic backdoors into the pre-training space through corpus poisoning, while preserving the PLM's pre-training capabilities. Second, $\\mathtt{SynGhost}$ adaptively selects optimal targets based on contrastive learning, creating a uniform distribution in the pre-training space. To identify syntactic differences, we also introduce an awareness module to minimize interference between backdoors. Experiments show that $\\mathtt{SynGhost}$ poses significant threats and can transfer to various downstream tasks. Furthermore, $\\mathtt{SynGhost}$ resists defenses based on perplexity, fine-pruning, and $\\mathtt{maxEntropy}$. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2402.18945",
    "authors": [
      "Pengzhou Cheng",
      "Wei Du",
      "Zongru Wu",
      "Fengwei Zhang",
      "Libo Chen",
      "Zhuosheng Zhang",
      "Gongshen Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2403.08743",
    "title": "Prompting Fairness: Integrating Causality to Debias Large Language Models",
    "abstract": "           Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical. In this work, we propose a causality-guided debiasing framework to tackle social biases, aiming to reduce the objectionable dependence between LLMs' decisions and the social information in the input. Our framework introduces a novel perspective to identify how social information can affect an LLM's decision through different causal pathways. Leveraging these causal insights, we outline principled prompting strategies that regulate these pathways through selection mechanisms. This framework not only unifies existing prompting-based debiasing techniques, but also opens up new directions for reducing bias by encouraging the model to prioritize fact-based reasoning over reliance on biased social cues. We validate our framework through extensive experiments on real-world datasets across multiple domains, demonstrating its effectiveness in debiasing LLM decisions, even with only black-box access to the model.         ",
    "url": "https://arxiv.org/abs/2403.08743",
    "authors": [
      "Jingling Li",
      "Zeyu Tang",
      "Xiaoyu Liu",
      "Peter Spirtes",
      "Kun Zhang",
      "Liu Leqi",
      "Yang Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.09752",
    "title": "Explainable Machine Learning-Based Security and Privacy Protection Framework for Internet of Medical Things Systems",
    "abstract": "           The Internet of Medical Things (IoMT) transcends traditional medical boundaries, enabling a transition from reactive treatment to proactive prevention. This innovative method revolutionizes healthcare by facilitating early disease detection and tailored care, particularly in chronic disease management, where IoMT automates treatments based on real-time health data collection. Nonetheless, its benefits are countered by significant security challenges that endanger the lives of its users due to the sensitivity and value of the processed data, thereby attracting malicious interests. Moreover, the utilization of wireless communication for data transmission exposes medical data to interception and tampering by cybercriminals. Additionally, anomalies may arise due to human error, network interference, or hardware malfunctions. In this context, anomaly detection based on Machine Learning (ML) is an interesting solution, but it comes up against obstacles in terms of explicability and privacy protection. To address these challenges, a new framework for Intrusion Detection Systems is introduced, leveraging Artificial Neural Networks for intrusion detection while utilizing Federated Learning (FL) for privacy preservation. Additionally, eXplainable Artificial Intelligence methods are incorporated to enhance model explanation and interpretation. The efficacy of the proposed framework is evaluated and compared with centralized approaches using multiple datasets containing network and medical data, simulating various attack types impacting the confidentiality, integrity, and availability of medical and physiological data. The results obtained offer compelling evidence that the FL method performs comparably to the centralized method, demonstrating high performance. Additionally, it affords the dual advantage of safeguarding privacy and providing model explanation while adhering to ethical principles.         ",
    "url": "https://arxiv.org/abs/2403.09752",
    "authors": [
      "Ayoub Si-ahmed",
      "Mohammed Ali Al-Garadi",
      "Narhimene Boustia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2403.10344",
    "title": "ViiNeuS: Volumetric Initialization for Implicit Neural Surface reconstruction of urban scenes with limited image overlap",
    "abstract": "           Neural implicit surface representation methods have recently shown impressive 3D reconstruction results. However, existing solutions struggle to reconstruct driving scenes due to their large size, highly complex nature and their limited visual observation overlap. Hence, to achieve accurate reconstructions, additional supervision data such as LiDAR, strong geometric priors, and long training times are required. To tackle such limitations, we present ViiNeuS, a new hybrid implicit surface learning method that efficiently initializes the signed distance field to reconstruct large driving scenes from 2D street view images. ViiNeuS's hybrid architecture models two separate implicit fields: one representing the volumetric density of the scene, and another one representing the signed distance to the surface. To accurately reconstruct urban outdoor driving scenarios, we introduce a novel volume-rendering strategy that relies on self-supervised probabilistic density estimation to sample points near the surface and transition progressively from volumetric to surface representation. Our solution permits a proper and fast initialization of the signed distance field without relying on any geometric prior on the scene, compared to concurrent methods. By conducting extensive experiments on four outdoor driving datasets, we show that ViiNeuS can learn an accurate and detailed 3D surface representation of various urban scene while being two times faster to train compared to previous state-of-the-art solutions.         ",
    "url": "https://arxiv.org/abs/2403.10344",
    "authors": [
      "Hala Djeghim",
      "Nathan Piasco",
      "Moussab Bennehar",
      "Luis Rold\u00e3o",
      "Dzmitry Tsishkou",
      "D\u00e9sir\u00e9 Sidib\u00e9"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.16513",
    "title": "Transfer Learning of Real Image Features with Soft Contrastive Loss for Fake Image Detection",
    "abstract": "           In the last few years, the artifact patterns in fake images synthesized by different generative models have been inconsistent, leading to the failure of previous research that relied on spotting subtle differences between real and fake. In our preliminary experiments, we find that the artifacts in fake images always change with the development of the generative model, while natural images exhibit stable statistical properties. In this paper, we employ natural traces shared only by real images as an additional target for a classifier. Specifically, we introduce a self-supervised feature mapping process for natural trace extraction and develop a transfer learning based on soft contrastive loss to bring them closer to real images and further away from fake ones. This motivates the detector to make decisions based on the proximity of images to the natural traces. To conduct a comprehensive experiment, we built a high-quality and diverse dataset that includes generative models comprising GANs and diffusion models, to evaluate the effectiveness in generalizing unknown forgery techniques and robustness in surviving different transformations. Experimental results show that our proposed method gives 96.2% mAP significantly outperforms the baselines. Extensive experiments conducted on popular commercial platforms reveal that our proposed method achieves an accuracy exceeding 78.4%, underscoring its practicality for real-world application deployment.         ",
    "url": "https://arxiv.org/abs/2403.16513",
    "authors": [
      "Ziyou Liang",
      "Weifeng Liu",
      "Run Wang",
      "Mengjie Wu",
      "Boheng Li",
      "Yuyang Zhang",
      "Lina Wang",
      "Xinyi Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2403.17710",
    "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge",
    "abstract": "           LLM-as-a-Judge uses a large language model (LLM) to select the best response from a set of candidates for a given question. LLM-as-a-Judge has many applications such as LLM-powered search, reinforcement learning with AI feedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver, an optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver injects a carefully crafted sequence into an attacker-controlled candidate response such that LLM-as-a-Judge selects the candidate response for an attacker-chosen question no matter what other candidate responses are. Specifically, we formulate finding such sequence as an optimization problem and propose a gradient based method to approximately solve it. Our extensive evaluation shows that JudgeDeceive is highly effective, and is much more effective than existing prompt injection attacks that manually craft the injected sequences and jailbreak attacks when extended to our problem. We also show the effectiveness of JudgeDeceiver in three case studies, i.e., LLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses including known-answer detection, perplexity detection, and perplexity windowed detection. Our results show these defenses are insufficient, highlighting the urgent need for developing new defense strategies. Our implementation is available at this repository: this https URL.         ",
    "url": "https://arxiv.org/abs/2403.17710",
    "authors": [
      "Jiawen Shi",
      "Zenghui Yuan",
      "Yinuo Liu",
      "Yue Huang",
      "Pan Zhou",
      "Lichao Sun",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.13982",
    "title": "Liquid-Graph Time-Constant Network for Multi-Agent Systems Control",
    "abstract": "           In this paper, we propose the Liquid-Graph Time-constant (LGTC) network, a continuous graph neural network(GNN) model for control of multi-agent systems based on therecent Liquid Time Constant (LTC) network. We analyse itsstability leveraging contraction analysis and propose a closed-form model that preserves the model contraction rate and doesnot require solving an ODE at each iteration. Compared todiscrete models like Graph Gated Neural Networks (GGNNs),the higher expressivity of the proposed model guaranteesremarkable performance while reducing the large amountof communicated variables normally required by GNNs. Weevaluate our model on a distributed multi-agent control casestudy (flocking) taking into account variable communicationrange and scalability under non-instantaneous communication         ",
    "url": "https://arxiv.org/abs/2404.13982",
    "authors": [
      "Antonio Marino",
      "Claudio Pacchierotti",
      "Paolo Robuffo Giordano"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2404.19228",
    "title": "Weighted Point Set Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric",
    "abstract": "           In typical multimodal contrastive learning, such as CLIP, encoders produce one point in the latent representation space for each input. However, one-point representation has difficulty in capturing the relationship and the similarity structure of a huge amount of instances in the real world. For richer classes of the similarity, we propose the use of weighted point sets, namely, sets of pairs of weight and vector, as representations of instances. In this work, we theoretically show the benefit of our proposed method through a new understanding of the contrastive loss of CLIP, which we call symmetric InfoNCE. We clarify that the optimal similarity that minimizes symmetric InfoNCE is the pointwise mutual information, and show an upper bound of excess risk on downstream classification tasks of representations that achieve the optimal similarity. In addition, we show that our proposed similarity based on weighted point sets consistently achieves the optimal similarity. To verify the effectiveness of our proposed method, we demonstrate pretraining of text-image representation models and classification tasks on common benchmarks.         ",
    "url": "https://arxiv.org/abs/2404.19228",
    "authors": [
      "Toshimitsu Uesaka",
      "Taiji Suzuki",
      "Yuhta Takida",
      "Chieh-Hsin Lai",
      "Naoki Murata",
      "Yuki Mitsufuji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.19489",
    "title": "EvGNN: An Event-driven Graph Neural Network Accelerator for Edge Vision",
    "abstract": "           Edge vision systems combining sensing and embedded processing promise low-latency, decentralized, and energy-efficient solutions that forgo reliance on the cloud. As opposed to conventional frame-based vision sensors, event-based cameras deliver a microsecond-scale temporal resolution with sparse information encoding, thereby outlining new opportunities for edge vision systems. However, mainstream algorithms for frame-based vision, which mostly rely on convolutional neural networks (CNNs), can hardly exploit the advantages of event-based vision as they are typically optimized for dense matrix-vector multiplications. While event-driven graph neural networks (GNNs) have recently emerged as a promising solution for sparse event-based vision, their irregular structure is a challenge that currently hinders the design of efficient hardware accelerators. In this paper, we propose EvGNN, the first event-driven GNN accelerator for low-footprint, ultra-low-latency, and high-accuracy edge vision with event-based cameras. It relies on three central ideas: (i) directed dynamic graphs exploiting single-hop nodes with edge-free storage, (ii) event queues for the efficient identification of local neighbors within a spatiotemporally decoupled search range, and (iii) a novel layer-parallel processing scheme allowing for a low-latency execution of multi-layer GNNs. We deployed EvGNN on a Xilinx KV260 Ultrascale+ MPSoC platform and benchmarked it on the N-CARS dataset for car recognition, demonstrating a classification accuracy of 87.8% and an average latency per event of 16$\\mu$s, thereby enabling real-time, microsecond-resolution event-based vision at the edge.         ",
    "url": "https://arxiv.org/abs/2404.19489",
    "authors": [
      "Yufeng Yang",
      "Adrian Kneip",
      "Charlotte Frenkel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Hardware Architecture (cs.AR)",
      "Emerging Technologies (cs.ET)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2405.01229",
    "title": "Boosting Jailbreak Attack with Momentum",
    "abstract": "           Large Language Models (LLMs) have achieved remarkable success across diverse tasks, yet they remain vulnerable to adversarial attacks, notably the well-known jailbreak attack. In particular, the Greedy Coordinate Gradient (GCG) attack has demonstrated efficacy in exploiting this vulnerability by optimizing adversarial prompts through a combination of gradient heuristics and greedy search. However, the efficiency of this attack has become a bottleneck in the attacking process. To mitigate this limitation, in this paper we rethink the generation of the adversarial prompts through an optimization lens, aiming to stabilize the optimization process and harness more heuristic insights from previous optimization iterations. Specifically, we propose the \\textbf{M}omentum \\textbf{A}ccelerated G\\textbf{C}G (\\textbf{MAC}) attack, which integrates a momentum term into the gradient heuristic to boost and stabilize the random search for tokens in adversarial prompts. Experimental results showcase the notable enhancement achieved by MAC over baselines in terms of attack success rate and optimization efficiency. Moreover, we demonstrate that MAC can still exhibit superior performance for transfer attacks and models under defense mechanisms. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2405.01229",
    "authors": [
      "Yihao Zhang",
      "Zeming Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2405.01649",
    "title": "Improving Complex Reasoning over Knowledge Graph with Logic-Aware Curriculum Tuning",
    "abstract": "           Answering complex queries over incomplete knowledge graphs (KGs) is a challenging job. Most previous works have focused on learning entity/relation embeddings and simulating first-order logic operators with various neural networks. However, they are bottlenecked by the inability to share world knowledge to improve logical reasoning, thus resulting in suboptimal performance. In this paper, we propose a complex reasoning schema over KG upon large language models (LLMs), containing a curriculum-based logical-aware instruction tuning framework, named LACT. Specifically, we augment the arbitrary first-order logical queries via binary tree decomposition, to stimulate the reasoning capability of LLMs. To address the difficulty gap among different types of complex queries, we design a simple and flexible logic-aware curriculum learning framework. Experiments across widely used datasets demonstrate that LACT has substantial improvements~(brings an average +5.5% MRR score) over advanced methods, achieving the new state-of-the-art.         ",
    "url": "https://arxiv.org/abs/2405.01649",
    "authors": [
      "Tianle Xia",
      "Liang Ding",
      "Guojia Wan",
      "Yibing Zhan",
      "Bo Du",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2405.02318",
    "title": "NL2FOL: Translating Natural Language to First-Order Logic for Logical Fallacy Detection",
    "abstract": "           Translating natural language into formal language such as First-Order Logic (FOL) is a foundational challenge in NLP with wide-ranging applications in automated reasoning, misinformation tracking, and knowledge validation. In this paper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework to autoformalize natural language to FOL step by step using Large Language Models (LLMs). Our approach addresses key challenges in this translation process, including the integration of implicit background knowledge. By leveraging structured representations generated by NL2FOL, we use Satisfiability Modulo Theory (SMT) solvers to reason about the logical validity of natural language statements. We present logical fallacy detection as a case study to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach also provides interpretable insights into the reasoning process and demonstrates robustness without requiring model fine-tuning or labeled training data. Our framework achieves strong performance on multiple datasets. On the LOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing effectively to the LOGICCLIMATE dataset with an F1-score of 80%.         ",
    "url": "https://arxiv.org/abs/2405.02318",
    "authors": [
      "Abhinav Lalwani",
      "Tasha Kim",
      "Lovish Chopra",
      "Christopher Hahn",
      "Zhijing Jin",
      "Mrinmaya Sachan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2405.04286",
    "title": "Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore",
    "abstract": "           The efficacy of detectors for texts generated by large language models (LLMs) substantially depends on the availability of large-scale training data. However, white-box zero-shot detectors, which require no such data, are limited by the accessibility of the source model of the LLM-generated text. In this paper, we propose a simple yet effective black-box zero-shot detection approach based on the observation that, from the perspective of LLMs, human-written texts typically contain more grammatical errors than LLM-generated texts. This approach involves calculating the Grammar Error Correction Score (GECScore) for the given text to differentiate between human-written and LLM-generated text. Experimental results show that our method outperforms current state-of-the-art (SOTA) zero-shot and supervised methods, achieving an average AUROC of 98.62% across XSum and Writing Prompts dataset. Additionally, our approach demonstrates strong reliability in the wild, exhibiting robust generalization and resistance to paraphrasing attacks. Data and code are available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2405.04286",
    "authors": [
      "Junchao Wu",
      "Runzhe Zhan",
      "Derek F. Wong",
      "Shu Yang",
      "Xuebo Liu",
      "Lidia S. Chao",
      "Min Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2405.13967",
    "title": "Model Editing as a Robust and Denoised variant of DPO: A Case Study on Toxicity",
    "abstract": "           Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data. However, these methods are both computationally intensive and lacking in controllability and transparency, inhibiting their widespread use. Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data. In this paper, we introduce a tuning-free alignment alternative, ProFS (Projection Filter for Subspaces), and demonstrate its effectiveness under the use case of toxicity reduction. Grounded on theory from factor analysis, ProFS is a sample-efficient model editing approach that identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The toxic subspace is identified by extracting preference data embeddings from the language model, and removing non-toxic information from these embeddings. We show that ProFS is more sample-efficient than DPO, further showcasing greater robustness to noisy data. Finally, we attempt to connect tuning based alignment with editing, by establishing both theoretical and empirical connections between ProFS and DPO, showing that ProFS can be interpreted as a denoised version of a single DPO step.         ",
    "url": "https://arxiv.org/abs/2405.13967",
    "authors": [
      "Rheeya Uppaal",
      "Apratim Dey",
      "Yiting He",
      "Yiqiao Zhong",
      "Junjie Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2405.15273",
    "title": "Towards a General Time Series Anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders",
    "abstract": "           Time series anomaly detection plays a vital role in a wide range of applications. Existing methods require training one specific model for each dataset, which exhibits limited generalization capability across different target datasets, hindering anomaly detection performance in various scenarios with scarce training data. Aiming at this problem, we propose constructing a general time series anomaly detection model, which is pre-trained on extensive multi-domain datasets and can subsequently apply to a multitude of downstream scenarios. The significant divergence of time series data across different domains presents two primary challenges in building such a general model: (1) meeting the diverse requirements of appropriate information bottlenecks tailored to different datasets in one unified model, and (2) enabling distinguishment between multiple normal and abnormal patterns, both are crucial for effective anomaly detection in various target scenarios. To tackle these two challenges, we propose a General time series anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders (DADA), which enables flexible selection of bottlenecks based on different data and explicitly enhances clear differentiation between normal and abnormal series. We conduct extensive experiments on nine target datasets from different domains. After pre-training on multi-domain data, DADA, serving as a zero-shot anomaly detector for these datasets, still achieves competitive or even superior results compared to those models tailored to each specific dataset. The code is made available at this https URL.         ",
    "url": "https://arxiv.org/abs/2405.15273",
    "authors": [
      "Qichao Shentu",
      "Beibu Li",
      "Kai Zhao",
      "Yang Shu",
      "Zhongwen Rao",
      "Lujia Pan",
      "Bin Yang",
      "Chenjuan Guo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.15598",
    "title": "MCDFN: Supply Chain Demand Forecasting via an Explainable Multi-Channel Data Fusion Network Model",
    "abstract": "           Accurate demand forecasting is crucial for optimizing supply chain management. Traditional methods often fail to capture complex patterns from seasonal variability and special events. Despite advancements in deep learning, interpretable forecasting models remain a challenge. To address this, we introduce the Multi-Channel Data Fusion Network (MCDFN), a hybrid architecture that integrates Convolutional Neural Networks (CNN), Long Short-Term Memory networks (LSTM), and Gated Recurrent Units (GRU) to enhance predictive performance by extracting spatial and temporal features from time series data. Our comparative benchmarking demonstrates that MCDFN outperforms seven other deep-learning models, achieving superior metrics: MSE (23.5738), RMSE (4.8553), MAE (3.9991), and MAPE (20.1575%). Theil's U statistic of 0.1181 (U<1) of MCDFN indicates its superiority over the naive forecasting approach, and a 10-fold cross-validated statistical paired t-test with a p-value of 5% indicated no significant difference between MCDFN's predictions and actual values. We apply explainable AI techniques like ShapTime and Permutation Feature Importance to enhance interpretability. This research advances demand forecasting methodologies and offers practical guidelines for integrating MCDFN into supply chain systems, highlighting future research directions for scalability and user-friendly deployment.         ",
    "url": "https://arxiv.org/abs/2405.15598",
    "authors": [
      "Md Abrar Jahin",
      "Asef Shahriar",
      "Md Al Amin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2405.18448",
    "title": "Multi-objective Representation for Numbers in Clinical Narratives: A CamemBERT-Bio-Based Alternative to Large-Scale LLMs",
    "abstract": "           The processing of numerical values is a rapidly developing area in the field of Language Models (LLMs). Despite numerous advancements achieved by previous research, significant challenges persist, particularly within the healthcare domain. This paper investigates the limitations of Transformer models in understanding numerical values. \\textit{Objective:} this research aims to categorize numerical values extracted from medical documents into eight specific physiological categories using CamemBERT-bio. \\textit{Methods:} In a context where scalable methods and Large Language Models (LLMs) are emphasized, we explore lifting the limitations of transformer-based models. We examine two strategies: fine-tuning CamemBERT-bio on a small medical dataset, integrating Label Embedding for Self-Attention (LESA), and combining LESA with additional enhancement techniques such as Xval. Given that CamemBERT-bio is already pre-trained on a large medical dataset, the first approach aims to update its encoder with the newly added label embeddings technique. In contrast, the second approach seeks to develop multiple representations of numbers (contextual and magnitude-based) to achieve more robust number embeddings. \\textit{Results:} As anticipated, fine-tuning the standard CamemBERT-bio on our small medical dataset did not improve F1 scores. However, significant improvements were observed with CamemBERT-bio + LESA, resulting in an over 13\\% increase. Similar enhancements were noted when combining LESA with Xval, outperforming conventional methods and giving comparable results to GPT-4 \\textit{Conclusions and Novelty:} This study introduces two innovative techniques for handling numerical data, which are also applicable to other modalities. We illustrate how these techniques can improve the performance of Transformer-based models, achieving more reliable classification results even with small datasets.         ",
    "url": "https://arxiv.org/abs/2405.18448",
    "authors": [
      "Boammani Aser Lompo",
      "Thanh-Dung Le"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2405.19316",
    "title": "Robust Preference Optimization through Reward Model Distillation",
    "abstract": "           Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations. Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning. However, the empirical evidence suggests that DPO typically assigns implicit rewards that overfit, and trend towards infinite magnitude. This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero. In this work, we analyze this phenomenon and use distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM such that its induced implicit reward, i.e., the scaled log-likelihood ratio of the model to the reference model, matches an explicit reward model trained on the preference data. Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution. Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO.         ",
    "url": "https://arxiv.org/abs/2405.19316",
    "authors": [
      "Adam Fisch",
      "Jacob Eisenstein",
      "Vicky Zayats",
      "Alekh Agarwal",
      "Ahmad Beirami",
      "Chirag Nagpal",
      "Pete Shaw",
      "Jonathan Berant"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.00987",
    "title": "Enhancing Fairness in Unsupervised Graph Anomaly Detection through Disentanglement",
    "abstract": "           Graph anomaly detection (GAD) is increasingly crucial in various applications, ranging from financial fraud detection to fake news detection. However, current GAD methods largely overlook the fairness problem, which might result in discriminatory decisions skewed toward certain demographic groups defined on sensitive attributes (e.g., gender, religion, ethnicity, etc.). This greatly limits the applicability of these methods in real-world scenarios in light of societal and ethical restrictions. To address this critical gap, we make the first attempt to integrate fairness with utility in GAD decision-making. Specifically, we devise a novel DisEntangle-based FairnEss-aware aNomaly Detection framework on the attributed graph, named DEFEND. DEFEND first introduces disentanglement in GNNs to capture informative yet sensitive-irrelevant node representations, effectively reducing societal bias inherent in graph representation learning. Besides, to alleviate discriminatory bias in evaluating anomalous nodes, DEFEND adopts a reconstruction-based anomaly detection, which concentrates solely on node attributes without incorporating any graph structure. Additionally, given the inherent association between input and sensitive attributes, DEFEND constrains the correlation between the reconstruction error and the predicted sensitive attributes. Our empirical evaluations on real-world datasets reveal that DEFEND performs effectively in GAD and significantly enhances fairness compared to state-of-the-art baselines. To foster reproducibility, our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.00987",
    "authors": [
      "Wenjing Chang",
      "Kay Liu",
      "Philip S. Yu",
      "Jianjun Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2406.07413",
    "title": "Towards Continuous Reuse of Graph Models via Holistic Memory Diversification",
    "abstract": "           This paper addresses the challenge of incremental learning in growing graphs with increasingly complex tasks. The goal is to continuously train a graph model to handle new tasks while retaining proficiency in previous tasks via memory replay. Existing methods usually overlook the importance of memory diversity, limiting in selecting high-quality memory from previous tasks and remembering broad previous knowledge within the scarce memory on graphs. To address that, we introduce a novel holistic Diversified Memory Selection and Generation (DMSG) framework for incremental learning in graphs, which first introduces a buffer selection strategy that considers both intra-class and inter-class diversities, employing an efficient greedy algorithm for sampling representative training nodes from graphs into memory buffers after learning each new task. Then, to adequately rememorize the knowledge preserved in the memory buffer when learning new tasks, a diversified memory generation replay method is introduced. This method utilizes a variational layer to generate the distribution of buffer node embeddings and sample synthesized ones for replaying. Furthermore, an adversarial variational embedding learning method and a reconstruction-based decoder are proposed to maintain the integrity and consolidate the generalization of the synthesized node embeddings, respectively. Extensive experimental results on publicly accessible datasets demonstrate the superiority of \\method{} over state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2406.07413",
    "authors": [
      "Ziyue Qiao",
      "Junren Xiao",
      "Qingqiang Sun",
      "Meng Xiao",
      "Xiao Luo",
      "Hui Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.08305",
    "title": "Large Language Model(LLM) assisted End-to-End Network Health Management based on Multi-Scale Semanticization",
    "abstract": "           Network device and system health management is the foundation of modern network operations and maintenance. Traditional health management methods, relying on expert identification or simple rule-based algorithms, struggle to cope with the dynamic heterogeneous networks (DHNs) environment. Moreover, current state-of-the-art distributed anomaly detection methods, which utilize specific machine learning techniques, lack multi-scale adaptivity for heterogeneous device information, resulting in unsatisfactory diagnostic accuracy for DHNs. In this paper, we develop an LLM-assisted end-to-end intelligent network health management framework. The framework first proposes a Multi-Scale Semanticized Anomaly Detection Model (MSADM), incorporating semantic rule trees with an attention mechanism to address the multi-scale anomaly detection problem in DHNs. Secondly, a chain-of-thought-based large language model is embedded in downstream to adaptively analyze the fault detection results and produce an analysis report with detailed fault information and optimization strategies. Experimental results show that the accuracy of our proposed MSADM for heterogeneous network entity anomaly detection is as high as 91.31\\%.         ",
    "url": "https://arxiv.org/abs/2406.08305",
    "authors": [
      "Fengxiao Tang",
      "Xiaonan Wang",
      "Xun Yuan",
      "Linfeng Luo",
      "Ming Zhao",
      "Tianchi Huang",
      "Nei Kato"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2406.09870",
    "title": "IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph Learning",
    "abstract": "           Deep graph learning has gained grand popularity over the past years due to its versatility and success in representing graph data across a wide range of domains. However, the pervasive issue of imbalanced graph data distributions, where certain parts exhibit disproportionally abundant data while others remain sparse, undermines the efficacy of conventional graph learning algorithms, leading to biased outcomes. To address this challenge, Imbalanced Graph Learning (IGL) has garnered substantial attention, enabling more balanced data distributions and better task performance. Despite the proliferation of IGL algorithms, the absence of consistent experimental protocols and fair performance comparisons pose a significant barrier to comprehending advancements in this field. To bridge this gap, we introduce IGL-Bench, a foundational comprehensive benchmark for imbalanced graph learning, embarking on 16 diverse graph datasets and 24 distinct IGL algorithms with uniform data processing and splitting strategies. Specifically, IGL-Bench systematically investigates state-of-the-art IGL algorithms in terms of effectiveness, robustness, and efficiency on node-level and graph-level tasks, with the scope of class-imbalance and topology-imbalance. Extensive experiments demonstrate the potential benefits of IGL algorithms on various imbalanced conditions, offering insights and opportunities in the IGL field. Further, we have developed an open-sourced and unified package to facilitate reproducible evaluation and inspire further innovative research, which is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.09870",
    "authors": [
      "Jiawen Qin",
      "Haonan Yuan",
      "Qingyun Sun",
      "Lyujin Xu",
      "Jiaqi Yuan",
      "Pengfeng Huang",
      "Zhaonan Wang",
      "Xingcheng Fu",
      "Hao Peng",
      "Jianxin Li",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.10279",
    "title": "We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs",
    "abstract": "           The reliance of popular programming languages such as Python and JavaScript on centralized package repositories and open-source software, combined with the emergence of code-generating Large Language Models (LLMs), has created a new type of threat to the software supply chain: package hallucinations. These hallucinations, which arise from fact-conflicting errors when generating code using LLMs, represent a novel form of package confusion attack that poses a critical threat to the integrity of the software supply chain. This paper conducts a rigorous and comprehensive evaluation of package hallucinations across different programming languages, settings, and parameters, exploring how a diverse set of models and configurations affect the likelihood of generating erroneous package recommendations and identifying the root causes of this phenomenon. Using 16 popular LLMs for code generation and two unique prompt datasets, we generate 576,000 code samples in two programming languages that we analyze for package hallucinations. Our findings reveal that that the average percentage of hallucinated packages is at least 5.2% for commercial models and 21.7% for open-source models, including a staggering 205,474 unique examples of hallucinated package names, further underscoring the severity and pervasiveness of this threat. To overcome this problem, we implement several hallucination mitigation strategies and show that they are able to significantly reduce the number of package hallucinations while maintaining code quality. Our experiments and findings highlight package hallucinations as a persistent and systemic phenomenon while using state-of-the-art LLMs for code generation, and a significant challenge which deserves the research community's urgent attention.         ",
    "url": "https://arxiv.org/abs/2406.10279",
    "authors": [
      "Joseph Spracklen",
      "Raveen Wijewickrama",
      "A H M Nazmus Sakib",
      "Anindya Maiti",
      "Bimal Viswanath",
      "Murtuza Jadliwala"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.10481",
    "title": "DCILP: A Distributed Approach for Large-Scale Causal Structure Learning",
    "abstract": "           Causal learning tackles the computationally demanding task of estimating causal graphs. This paper introduces a new divide-and-conquer approach for causal graph learning, called DCILP. In the divide phase, the Markov blanket MB($X_i$) of each variable $X_i$ is identified, and causal learning subproblems associated with each MB($X_i$) are independently addressed in parallel. This approach benefits from a more favorable ratio between the number of data samples and the number of variables considered. In counterpart, it can be adversely affected by the presence of hidden confounders, as variables external to MB($X_i$) might influence those within it. The reconciliation of the local causal graphs generated during the divide phase is a challenging combinatorial optimization problem, especially in large-scale applications. The main novelty of DCILP is an original formulation of this reconciliation as an integer linear programming (ILP) problem, which can be delegated and efficiently handled by an ILP solver. Through experiments on medium to large scale graphs, and comparisons with state-of-the-art methods, DCILP demonstrates significant improvements in terms of computational complexity, while preserving the learning accuracy on real-world problem and suffering at most a slight loss of accuracy on synthetic problems.         ",
    "url": "https://arxiv.org/abs/2406.10481",
    "authors": [
      "Shuyu Dong",
      "Mich\u00e8le Sebag",
      "Kento Uemura",
      "Akito Fujii",
      "Shuang Chang",
      "Yusuke Koyanagi",
      "Koji Maruhashi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2407.02452",
    "title": "A Hardware-Friendly Shuffling Countermeasure Against Side-Channel Attacks for Kyber",
    "abstract": "           CRYSTALS-Kyber has been standardized as the only key-encapsulation mechanism (KEM) scheme by NIST to withstand attacks by large-scale quantum computers. However, the side-channel attacks (SCAs) on its implementation are still needed to be well considered for the upcoming migration. In this brief, we propose a secure and efficient hardware implementation for Kyber by incorporating a novel compact shuffling architecture. First of all, we modify the Fisher-Yates shuffle to make it more hardware-friendly. We then design an optimized shuffling architecture for the well-known open-source Kyber hardware implementation to enhance the security of all known and potential side-channel leakage points. Finally, we implement the modified Kyber design on FPGA and evaluate its security and performance. The security is verified by conducting correlation power analysis (CPA) and test vector leakage assessment (TVLA) on the hardware. Meanwhile, FPGA place-and-route results show that the proposed design reports only 8.7% degradation on the hardware efficiency compared with the original unprotected version, much better than existing hardware hiding schemes.         ",
    "url": "https://arxiv.org/abs/2407.02452",
    "authors": [
      "Dejun Xu",
      "Kai Wang",
      "Jing Tian"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.04752",
    "title": "SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via Saliency-based Spiking",
    "abstract": "           Recent advancements in large language models (LLMs) with billions of parameters have improved performance in various applications, but their inference processes demand significant energy and computational resources. In contrast, the human brain, with approximately 86 billion neurons, is much more energy-efficient than LLMs with similar parameters. Inspired by this, we redesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking mechanisms, emulating the efficient behavior of the human brain. We propose the first spiking large language model, SpikeLLM. Coupled with the proposed model, two essential approaches are proposed to improve spike training efficiency: Generalized Integrate-and-Fire (GIF) neurons to compress spike length from $T$ to $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to divide outlier channels and allocate different $T$ for GIF neurons, which further compresses spike length to approximate $log_2T$ bits. The necessity of spike-driven LLM is proved by comparison with quantized LLMs with similar operations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2 perplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B W4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear layers, significantly exceeding PB-LLMs.         ",
    "url": "https://arxiv.org/abs/2407.04752",
    "authors": [
      "Xingrun Xing",
      "Boyan Gao",
      "Zheng Zhang",
      "David A. Clifton",
      "Shitao Xiao",
      "Li Du",
      "Guoqi Li",
      "Jiajun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2407.06142",
    "title": "Delay-Aware Robust Edge Network Hardening Under Decision-Dependent Uncertainty",
    "abstract": "           Edge computing promises to offer low-latency and ubiquitous computation to numerous devices at the network edge. For delay-sensitive applications, link delays can have a direct impact on service quality. These delays can fluctuate drastically over time due to various factors such as network congestion, changing traffic conditions, cyberattacks, component failures, and natural disasters. Thus, it is crucial to efficiently harden the edge network to mitigate link delay variation as well as ensure a stable and improved user experience. To this end, we propose a novel robust model for optimal edge network hardening, considering the link delay uncertainty. Departing from the existing literature that treats uncertainties as exogenous, our model incorporates an endogenous uncertainty set to properly capture the impact of hardening and workload allocation decisions on link delays. However, the endogenous set introduces additional complexity to the problem due to the interdependence between decisions and uncertainties. We present two efficient methods to transform the problem into a solvable form. Extensive numerical results are shown to demonstrate the effectiveness of the proposed approach.         ",
    "url": "https://arxiv.org/abs/2407.06142",
    "authors": [
      "Jiaming Cheng",
      "Duong Thuy Anh Nguyen",
      "Ni Trieu",
      "Duong Tung Nguyen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2407.10967",
    "title": "BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning",
    "abstract": "           Offline model-based reinforcement learning (MBRL) enhances data efficiency by utilizing pre-collected datasets to learn models and policies, especially in scenarios where exploration is costly or infeasible. Nevertheless, its performance often suffers from the objective mismatch between model and policy learning, resulting in inferior performance despite accurate model predictions. This paper first identifies the primary source of this mismatch comes from the underlying confounders present in offline data for MBRL. Subsequently, we introduce \\textbf{B}ilin\\textbf{E}ar \\textbf{CAUS}al r\\textbf{E}presentation~(BECAUSE), an algorithm to capture causal representation for both states and actions to reduce the influence of the distribution shift, thus mitigating the objective mismatch problem. Comprehensive evaluations on 18 tasks that vary in data quality and environment context demonstrate the superior performance of BECAUSE over existing offline RL algorithms. We show the generalizability and robustness of BECAUSE under fewer samples or larger numbers of confounders. Additionally, we offer theoretical analysis of BECAUSE to prove its error bound and sample efficiency when integrating causal representation into offline MBRL.         ",
    "url": "https://arxiv.org/abs/2407.10967",
    "authors": [
      "Haohong Lin",
      "Wenhao Ding",
      "Jian Chen",
      "Laixi Shi",
      "Jiacheng Zhu",
      "Bo Li",
      "Ding Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.13929",
    "title": "Unmasking Social Bots: How Confident Are We?",
    "abstract": "           Social bots remain a major vector for spreading disinformation on social media and a menace to the public. Despite the progress made in developing multiple sophisticated social bot detection algorithms and tools, bot detection remains a challenging, unsolved problem that is fraught with uncertainty due to the heterogeneity of bot behaviors, training data, and detection algorithms. Detection models often disagree on whether to label the same account as bot or human-controlled. However, they do not provide any measure of uncertainty to indicate how much we should trust their results. We propose to address both bot detection and the quantification of uncertainty at the account level - a novel feature of this research. This dual focus is crucial as it allows us to leverage additional information related to the quantified uncertainty of each prediction, thereby enhancing decision-making and improving the reliability of bot classifications. Specifically, our approach facilitates targeted interventions for bots when predictions are made with high confidence and suggests caution (e.g., gathering more data) when predictions are uncertain.         ",
    "url": "https://arxiv.org/abs/2407.13929",
    "authors": [
      "James Giroux",
      "Ariyarathne Gangani",
      "Alexander C. Nwala",
      "Cristiano Fanelli"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.15787",
    "title": "Self-supervised Mamba-based Mastoidectomy Shape Prediction for Cochlear Implant Surgery",
    "abstract": "           Cochlear Implant (CI) procedures require the insertion of an electrode array into the cochlea within the inner ear. To achieve this, mastoidectomy, a surgical procedure involving the removal of part of the mastoid region of the temporal bone using a high-speed drill provides safe access to the cochlea through the middle and inner ear. In this paper, we propose a novel Mamba-based method to synthesize the mastoidectomy volume using only preoperative Computed Tomography (CT) scans, where the mastoid remains intact. Our approach introduces a self-supervised learning framework designed to predict the mastoidectomy shape and reconstruct a 3D post-mastoidectomy surface directly from preoperative CT scans. This reconstruction aligns with intraoperative microscope views, enabling various downstream surgical applications. For training, we leverage postoperative CT scans to bypass manual data cleaning and labeling, even when the region removed during mastoidectomy is affected by challenges such as metal artifacts, low signal-to-noise ratio, or electrode wiring. Our method achieves a mean Dice score of 0.70 in estimating mastoidectomy regions, demonstrating its effectiveness for accurate and efficient surgical preoperative planning.         ",
    "url": "https://arxiv.org/abs/2407.15787",
    "authors": [
      "Yike Zhang",
      "Eduardo Davalos",
      "Dingjie Su",
      "Ange Lou",
      "Jack H. Noble"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.17674",
    "title": "Struc2mapGAN: improving synthetic cryo-EM density maps with generative adversarial networks",
    "abstract": "           Generating synthetic cryogenic electron microscopy 3D density maps from molecular structures has potential important applications in structural biology. Yet existing simulation-based methods cannot mimic all the complex features present in experimental maps, such as secondary structure elements. As an alternative, we propose struc2mapGAN, a novel data-driven method that employs a generative adversarial network to produce improved experimental-like density maps from molecular structures. More specifically, struc2mapGAN uses a nested U-Net architecture as the generator, with an additional L1 loss term and further processing of raw training experimental maps to enhance learning efficiency. While struc2mapGAN can promptly generate maps after training, we demonstrate that it outperforms existing simulation-based methods for a wide array of tested maps and across various evaluation metrics.         ",
    "url": "https://arxiv.org/abs/2407.17674",
    "authors": [
      "Chenwei Zhang",
      "Anne Condon",
      "Khanh Dao Duc"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2407.20207",
    "title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval",
    "abstract": "           In dense retrieval, embedding long texts into dense vectors can result in information loss, leading to inaccurate query-text matching. Additionally, low-quality texts with excessive noise or sparse key information are unlikely to align well with relevant queries. Recent studies mainly focus on improving the sentence embedding model or retrieval process. In this work, we introduce a novel text augmentation framework for dense retrieval. This framework transforms raw documents into information-dense text formats, which supplement the original texts to effectively address the aforementioned issues without modifying embedding or retrieval methodologies. Two text representations are generated via large language models (LLMs) zero-shot prompting: question-answer pairs and element-driven events. We term this approach QAEA-DR: unifying question-answer generation and event extraction in a text augmentation framework for dense retrieval. To further enhance the quality of generated texts, a scoring-based evaluation and regeneration mechanism is introduced in LLM prompting. Our QAEA-DR model has a positive impact on dense retrieval, supported by both theoretical analysis and empirical experiments.         ",
    "url": "https://arxiv.org/abs/2407.20207",
    "authors": [
      "Hongming Tan",
      "Shaoxiong Zhan",
      "Hai Lin",
      "Hai-Tao Zheng",
      "Wai Kin Chan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2408.02950",
    "title": "Kolmogorov-Arnold PointNet: Deep learning for prediction of fluid fields on irregular geometries",
    "abstract": "           Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to traditional Multilayer Perceptrons (MLPs) in deep learning. KANs have already been integrated into various architectures, such as convolutional neural networks, graph neural networks, and transformers, and their potential has been assessed for predicting physical quantities. However, the combination of KANs with point-cloud-based neural networks (e.g., PointNet) for computational physics has not yet been explored. To address this, we present Kolmogorov-Arnold PointNet (KA-PointNet) as a novel supervised deep learning framework for the prediction of incompressible steady-state fluid flow fields in irregular domains, where the predicted fields are a function of the geometry of the domains. In KA-PointNet, we implement shared KANs in the segmentation branch of the PointNet architecture. We utilize Jacobi polynomials to construct shared KANs. As a benchmark test case, we consider incompressible laminar steady-state flow over a cylinder, where the geometry of its cross-section varies over the data set. We investigate the performance of Jacobi polynomials with different degrees as well as special cases of Jacobi polynomials such as Legendre polynomials, Chebyshev polynomials of the first and second kinds, and Gegenbauer polynomials, in terms of the computational cost of training and accuracy of prediction of the test set. Additionally, we compare the performance of PointNet with shared KANs (i.e., KA-PointNet) and PointNet with shared MLPs. It is observed that when the number of trainable parameters is approximately equal, PointNet with shared KANs (i.e., KA-PointNet) outperforms PointNet with shared MLPs. Moreover, KA-PointNet predicts the pressure and velocity distributions along the surface of cylinders more accurately, resulting in more precise computations of lift and drag.         ",
    "url": "https://arxiv.org/abs/2408.02950",
    "authors": [
      "Ali Kashefi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2408.02965",
    "title": "Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model and Neural Operator",
    "abstract": "           Closure models are widely used in simulating complex multiscale dynamical systems such as turbulence and the earth system, for which direct numerical simulation that resolves all scales is often too expensive. For those systems without a clear scale separation, deterministic and local closure models often lack enough generalization capability, which limits their performance in many real-world applications. In this work, we propose a data-driven modeling framework for constructing stochastic and non-local closure models via conditional diffusion model and neural operator. Specifically, the Fourier neural operator is incorporated into a score-based diffusion model, which serves as a data-driven stochastic closure model for complex dynamical systems governed by partial differential equations (PDEs). We also demonstrate how accelerated sampling methods can improve the efficiency of the data-driven stochastic closure model. The results show that the proposed methodology provides a systematic approach via generative machine learning techniques to construct data-driven stochastic closure models for multiscale dynamical systems with continuous spatiotemporal fields.         ",
    "url": "https://arxiv.org/abs/2408.02965",
    "authors": [
      "Xinghao Dong",
      "Chuanqi Chen",
      "Jin-Long Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2408.04591",
    "title": "HiLo: A Learning Framework for Generalized Category Discovery Robust to Domain Shifts",
    "abstract": "           Generalized Category Discovery (GCD) is a challenging task in which, given a partially labelled dataset, models must categorize all unlabelled instances, regardless of whether they come from labelled categories or from new ones. In this paper, we challenge a remaining assumption in this task: that all images share the same domain. Specifically, we introduce a new task and method to handle GCD when the unlabelled data also contains images from different domains to the labelled set. Our proposed `HiLo' networks extract High-level semantic and Low-level domain features, before minimizing the mutual information between the representations. Our intuition is that the clusterings based on domain information and semantic information should be independent. We further extend our method with a specialized domain augmentation tailored for the GCD task, as well as a curriculum learning approach. Finally, we construct a benchmark from corrupted fine-grained datasets as well as a large-scale evaluation on DomainNet with real-world domain shifts, reimplementing a number of GCD baselines in this setting. We demonstrate that HiLo outperforms SoTA category discovery models by a large margin on all evaluations.         ",
    "url": "https://arxiv.org/abs/2408.04591",
    "authors": [
      "Hongjun Wang",
      "Sagar Vaze",
      "Kai Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.07517",
    "title": "Advancing Spatio-Temporal Processing in Spiking Neural Networks through Adaptation",
    "abstract": "           Implementations of spiking neural networks on neuromorphic hardware promise orders of magnitude less power consumption than their non-spiking counterparts. The standard neuron model for spike-based computation on such systems has long been the leaky integrate-and-fire (LIF) neuron. A computationally light augmentation of the LIF neuron model with an adaptation mechanism has recently been shown to exhibit superior performance on spatio-temporal processing tasks. The root of the superiority of these so-called adaptive LIF neurons however is not well understood. In this article, we thoroughly analyze the dynamical, computational, and learning properties of adaptive LIF neurons and networks thereof. Our investigation reveals significant challenges related to stability and parameterization when employing the conventional Euler-Forward discretization for this class of models. We report a rigorous theoretical and empirical demonstration that these challenges can be effectively addressed by adopting an alternative discretization approach - the Symplectic Euler method, allowing to improve over state-of-the-art performances on common event-based benchmark datasets. Our further analysis of the computational properties of networks of adaptive LIF neurons shows that they are particularly well suited to exploit the spatio-temporal structure of input sequences without any normalization techniques.         ",
    "url": "https://arxiv.org/abs/2408.07517",
    "authors": [
      "Maximilian Baronig",
      "Romain Ferrand",
      "Silvester Sabathiel",
      "Robert Legenstein"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2408.08291",
    "title": "The ShareLM Collection and Plugin: Contributing Human-Model Chats for the Benefit of the Community",
    "abstract": "           Human-model conversations provide a window into users' real-world scenarios, behavior, and needs, and thus are a valuable resource for model development and research. While for-profit companies collect user data through the APIs of their models, using it internally to improve their own models, the open source and research community lags behind. We introduce the ShareLM collection, a unified set of human conversations with large language models, and its accompanying plugin, a Web extension for voluntarily contributing user-model conversations. Where few platforms share their chats, the ShareLM plugin adds this functionality, thus, allowing users to share conversations from most platforms. The plugin allows the user to rate their conversations, both at the conversation and the response levels, and delete conversations they prefer to keep private before they ever leave the user's local storage. We release the plugin conversations as part of the ShareLM collection, and call for more community effort in the field of open human-model data. The code, plugin, and data are available.         ",
    "url": "https://arxiv.org/abs/2408.08291",
    "authors": [
      "Shachar Don-Yehiya",
      "Leshem Choshen",
      "Omri Abend"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.10557",
    "title": "Speech Representation Learning Revisited: The Necessity of Separate Learnable Parameters and Robust Data Augmentation",
    "abstract": "           Speech modeling methods learn one embedding for a fixed segment of speech, typically in between 10-25 ms. The information present in speech can be divided into two categories: \"what is being said\" (content) and \"how it is expressed\" (other) and these two are orthogonal in nature causing the optimization algorithm to find a sub-optimal solution if forced to optimize together. This leads to sub-optimal performance in one or all downstream tasks as shown by previous studies. Current self-supervised learning (SSL) methods such as HuBERT are very good at modeling the content information present in speech. Data augmentation improves the performance on tasks which require effective modeling of other information but this leads to a divided capacity of the model. In this work, we conduct a preliminary study to understand the importance of modeling other information using separate learnable parameters. We propose a modified version of HuBERT, termed Other HuBERT (O-HuBERT), to test our hypothesis. Our findings are twofold: first, the O-HuBERT method is able to utilize all layers to build complex features to encode other information; second, a robust data augmentation strategy is essential for learning the information required by tasks that depend on other information and to achieve state-of-the-art (SOTA) performance on the SUPERB benchmark with a similarly sized model (100 million parameters) and pre-training data (960 hours).         ",
    "url": "https://arxiv.org/abs/2408.10557",
    "authors": [
      "Hemant Yadav",
      "Sunayana Sitaram",
      "Rajiv Ratn Shah"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.11561",
    "title": "Self-Supervised Iterative Refinement for Anomaly Detection in Industrial Quality Control",
    "abstract": "           This study introduces the Iterative Refinement Process (IRP), a robust anomaly detection methodology designed for high-stakes industrial quality control. The IRP enhances defect detection accuracy through a cyclic data refinement strategy, iteratively removing misleading data points to improve model performance and robustness. We validate the IRP's effectiveness using two benchmark datasets, Kolektor SDD2 (KSDD2) and MVTec AD, covering a wide range of industrial products and defect types. Our experimental results demonstrate that the IRP consistently outperforms traditional anomaly detection models, particularly in environments with high noise levels. This study highlights the IRP's potential to significantly enhance anomaly detection processes in industrial settings, effectively managing the challenges of sparse and noisy data.         ",
    "url": "https://arxiv.org/abs/2408.11561",
    "authors": [
      "Muhammad Aqeel",
      "Shakiba Sharifi",
      "Marco Cristani",
      "Francesco Setti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.04607",
    "title": "Self-Supervised Contrastive Learning for Videos using Differentiable Local Alignment",
    "abstract": "           Robust frame-wise embeddings are essential to perform video analysis and understanding tasks. We present a self-supervised method for representation learning based on aligning temporal video sequences. Our framework uses a transformer-based encoder to extract frame-level features and leverages them to find the optimal alignment path between video sequences. We introduce the novel Local-Alignment Contrastive (LAC) loss, which combines a differentiable local alignment loss to capture local temporal dependencies with a contrastive loss to enhance discriminative learning. Prior works on video alignment have focused on using global temporal ordering across sequence pairs, whereas our loss encourages identifying the best-scoring subsequence alignment. LAC uses the differentiable Smith-Waterman (SW) affine method, which features a flexible parameterization learned through the training phase, enabling the model to adjust the temporal gap penalty length dynamically. Evaluations show that our learned representations outperform existing state-of-the-art approaches on action recognition tasks.         ",
    "url": "https://arxiv.org/abs/2409.04607",
    "authors": [
      "Keyne Oei",
      "Amr Gomaa",
      "Anna Maria Feit",
      "Jo\u00e3o Belo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.06214",
    "title": "Towards Generalizable Scene Change Detection",
    "abstract": "           While current state-of-the-art Scene Change Detection (SCD) approaches achieve impressive results in well-trained research data, they become unreliable under unseen environments and different temporal conditions; in-domain performance drops from 77.6\\% to 8.0\\% in a previously unseen environment and to 4.6\\% under a different temporal condition -- calling for generalizable SCD and benchmark. In this work, we propose the Generalizable Scene Change Detection Framework (GeSCF), which addresses unseen domain performance and temporal consistency -- to meet the growing demand for anything SCD. Our method leverages the pre-trained Segment Anything Model (SAM) in a zero-shot manner. For this, we design Initial Pseudo-mask Generation and Geometric-Semantic Mask Matching -- seamlessly turning user-guided prompt and single-image based segmentation into scene change detection for a pair of inputs without guidance. Furthermore, we define the Generalizable Scene Change Detection (GeSCD) benchmark along with novel metrics and an evaluation protocol to facilitate SCD research in generalizability. In the process, we introduce the ChangeVPR dataset, a collection of challenging image pairs with diverse environmental scenarios -- including urban, suburban, and rural settings. Extensive experiments across various datasets demonstrate that GeSCF achieves an average performance gain of 19.2\\% on existing SCD datasets and 30.0\\% on the ChangeVPR dataset, nearly doubling the prior art performance. We believe our work can lay a solid foundation for robust and generalizable SCD research.         ",
    "url": "https://arxiv.org/abs/2409.06214",
    "authors": [
      "Jaewoo Kim",
      "Uehwan Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.06533",
    "title": "Two-level Restricted Additive Schwarz preconditioner based on Multiscale Spectral Generalized FEM for Heterogeneous Helmholtz Problems",
    "abstract": "           We present and analyze a two-level restricted additive Schwarz (RAS) preconditioner for heterogeneous Helmholtz problems, based on a multiscale spectral generalized finite element method (MS-GFEM) proposed in [C. Ma, C. Alber, and R. Scheichl, SIAM. J. Numer. Anal., 61 (2023), pp. 1546--1584]. The preconditioner uses local solves with impedance boundary conditions, and a global coarse solve based on the MS-GFEM approximation space constructed from local eigenproblems. It is derived by first formulating MS-GFEM as a Richardson iterative method, and without using an oversampling technique, reduces to the preconditioner recently proposed and analyzed in [Q. Hu and this http URL, arXiv 2402.06905]. We prove that both the Richardson iterative method and the preconditioner used within GMRES converge at a rate of $\\Lambda$ under some reasonable conditions, where $\\Lambda$ denotes the error of the underlying MS-GFEM \\rs{approximation}. Notably, the convergence proof of GMRES does not rely on the `Elman theory'. An exponential convergence property of MS-GFEM, resulting from oversampling, ensures that only a few iterations are needed for convergence with a small coarse space. Moreover, the convergence rate $\\Lambda$ is not only independent of the fine-mesh size $h$ and the number of subdomains, but decays with increasing wavenumber $k$. In particular, in the constant-coefficient case, with $h\\sim k^{-1-\\gamma}$ for some $\\gamma\\in (0,1]$, it holds that $\\Lambda \\sim k^{-1+\\frac{\\gamma}{2}}$. We present extensive numerical experiments to illustrate the performance of the preconditioner, including 2D and 3D benchmark geophysics tests, and a high-contrast coefficient example arising in applications.         ",
    "url": "https://arxiv.org/abs/2409.06533",
    "authors": [
      "Chupeng Ma",
      "Christian Alber",
      "Robert Scheichl",
      "Yongwei Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2409.07002",
    "title": "AdvLogo: Adversarial Patch Attack against Object Detectors based on Diffusion Models",
    "abstract": "           With the rapid development of deep learning, object detectors have demonstrated impressive performance; however, vulnerabilities still exist in certain scenarios. Current research exploring the vulnerabilities using adversarial patches often struggles to balance the trade-off between attack effectiveness and visual quality. To address this problem, we propose a novel framework of patch attack from semantic perspective, which we refer to as AdvLogo. Based on the hypothesis that every semantic space contains an adversarial subspace where images can cause detectors to fail in recognizing objects, we leverage the semantic understanding of the diffusion denoising process and drive the process to adversarial subareas by perturbing the latent and unconditional embeddings at the last timestep. To mitigate the distribution shift that exposes a negative impact on image quality, we apply perturbation to the latent in frequency domain with the Fourier Transform. Experimental results demonstrate that AdvLogo achieves strong attack performance while maintaining high visual quality.         ",
    "url": "https://arxiv.org/abs/2409.07002",
    "authors": [
      "Boming Miao",
      "Chunxiao Li",
      "Yao Zhu",
      "Weixiang Sun",
      "Zizhe Wang",
      "Xiaoyi Wang",
      "Chuanlong Xie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.09619",
    "title": "Compositional Audio Representation Learning",
    "abstract": "           Human auditory perception is compositional in nature -- we identify auditory streams from auditory scenes with multiple sound events. However, such auditory scenes are typically represented using clip-level representations that do not disentangle the constituent sound sources. In this work, we learn source-centric audio representations where each sound source is represented using a distinct, disentangled source embedding in the audio representation. We propose two novel approaches to learning source-centric audio representations: a supervised model guided by classification and an unsupervised model guided by feature reconstruction, both of which outperform the baselines. We thoroughly evaluate the design choices of both approaches using an audio classification task. We find that supervision is beneficial to learn source-centric representations, and that reconstructing audio features is more useful than reconstructing spectrograms to learn unsupervised source-centric representations. Leveraging source-centric models can help unlock the potential of greater interpretability and more flexible decoding in machine listening.         ",
    "url": "https://arxiv.org/abs/2409.09619",
    "authors": [
      "Sripathi Sridhar",
      "Mark Cartwright"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2409.10071",
    "title": "Towards Physically Realizable Adversarial Attacks in Embodied Vision Navigation",
    "abstract": "           The significant advancements in embodied vision navigation have raised concerns about its susceptibility to adversarial attacks exploiting deep neural networks. Investigating the adversarial robustness of embodied vision navigation is crucial, especially given the threat of 3D physical attacks that could pose risks to human safety. However, existing attack methods for embodied vision navigation often lack physical feasibility due to challenges in transferring digital perturbations into the physical world. Moreover, current physical attacks for object detection struggle to achieve both multi-view effectiveness and visual naturalness in navigation scenarios. To address this, we propose a practical attack method for embodied navigation by attaching adversarial patches to objects, where both opacity and textures are learnable. Specifically, to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling, which optimizes the patch's texture based on feedback from the vision-based perception model used in navigation. To make the patch inconspicuous to human observers, we introduce a two-stage opacity optimization mechanism, in which opacity is fine-tuned after texture optimization. Experimental results demonstrate that our adversarial patches decrease the navigation success rate by an average of 22.39%, outperforming previous methods in practicality, effectiveness, and naturalness. Code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2409.10071",
    "authors": [
      "Meng Chen",
      "Jiawei Tu",
      "Chao Qi",
      "Yonghao Dang",
      "Feng Zhou",
      "Wei Wei",
      "Jianqin Yin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.10878",
    "title": "P2 Explore: Efficient Exploration in Unknown Cluttered Environment with Floor Plan Prediction",
    "abstract": "           Robot exploration aims at the reconstruction of unknown environments, and it is important to achieve it with shorter paths. Traditional methods focus on optimizing the visiting order of frontiers based on current observations, which may lead to local-minimal results. Recently, by predicting the structure of the unseen environment, the exploration efficiency can be further improved. However, in a cluttered environment, due to the randomness of obstacles, the ability to predict is weak. Moreover, this inaccuracy will lead to limited improvement in exploration. Therefore, we propose FPUNet which can be efficient in predicting the layout of noisy indoor environments. Then, we extract the segmentation of rooms and construct their topological connectivity based on the predicted map. The visiting order of these predicted rooms is optimized which can provide high-level guidance for exploration. The FPUNet is compared with other network architectures which demonstrates it is the SOTA method for this task. Extensive experiments in simulations show that our method can shorten the path length by 2.18% to 34.60% compared to the baselines.         ",
    "url": "https://arxiv.org/abs/2409.10878",
    "authors": [
      "Kun Song",
      "Gaoming Chen",
      "Masayoshi Tomizuka",
      "Wei Zhan",
      "Zhenhua Xiong",
      "Mingyu Ding"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.11113",
    "title": "CageCoOpt: Enhancing Manipulation Robustness through Caging-Guided Morphology and Policy Co-Optimization",
    "abstract": "           Uncertainties in contact dynamics and object geometry remain significant barriers to robust robotic manipulation. Caging mitigates these uncertainties by constraining an object's mobility without requiring precise contact modeling. However, existing caging research has largely treated morphology and policy optimization as separate problems, overlooking their inherent synergy. In this paper, we introduce CageCoOpt, a hierarchical framework that jointly optimizes manipulator morphology and control policy for robust manipulation. The framework employs reinforcement learning for policy optimization at the lower level and multi-task Bayesian optimization for morphology optimization at the upper level. A robustness metric in caging, Minimum Escape Energy, is incorporated into the objectives of both levels to promote caging configurations and enhance manipulation robustness. The evaluation results through four manipulation tasks demonstrate that co-optimizing morphology and policy improves success rates under uncertainties, establishing caging-guided co-optimization as a viable approach for robust manipulation.         ",
    "url": "https://arxiv.org/abs/2409.11113",
    "authors": [
      "Yifei Dong",
      "Shaohang Han",
      "Xianyi Cheng",
      "Werner Friedl",
      "Rafael I. Cabral Muchacho",
      "M\u00e1ximo A. Roa",
      "Jana Tumova",
      "Florian T. Pokorny"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.13195",
    "title": "Guaranteed Reach-Avoid for Black-Box Systems through Narrow Gaps via Neural Network Reachability",
    "abstract": "           In the classical reach-avoid problem, autonomous mobile robots are tasked to reach a goal while avoiding obstacles. However, it is difficult to provide guarantees on the robot's performance when the obstacles form a narrow gap and the robot is a black-box (i.e. the dynamics are not known analytically, but interacting with the system is cheap). To address this challenge, this paper presents NeuralPARC. The method extends the authors' prior Piecewise Affine Reach-avoid Computation (PARC) method to systems modeled by rectified linear unit (ReLU) neural networks, which are trained to represent parameterized trajectory data demonstrated by the robot. NeuralPARC computes the reachable set of the network while accounting for modeling error, and returns a set of states and parameters with which the black-box system is guaranteed to reach the goal and avoid obstacles. NeuralPARC is shown to outperform PARC, generating provably-safe extreme vehicle drift parking maneuvers in simulations and in real life on a model car, as well as enabling safety on an autonomous surface vehicle (ASV) subjected to large disturbances and controlled by a deep reinforcement learning (RL) policy.         ",
    "url": "https://arxiv.org/abs/2409.13195",
    "authors": [
      "Long Kiu Chung",
      "Wonsuhk Jung",
      "Srivatsank Pullabhotla",
      "Parth Shinde",
      "Yadu Sunil",
      "Saihari Kota",
      "Luis Felipe Wolf Batista",
      "C\u00e9dric Pradalier",
      "Shreyas Kousik"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2409.13607",
    "title": "RECON: Reducing Causal Confusion with Human-Placed Markers",
    "abstract": "           Imitation learning enables robots to learn new tasks from human examples. One fundamental limitation while learning from humans is causal confusion. Causal confusion occurs when the robot's observations include both task-relevant and extraneous information: for instance, a robot's camera might see not only the intended goal, but also clutter and changes in lighting within its environment. Because the robot does not know which aspects of its observations are important a priori, it often misinterprets the human's examples and fails to learn the desired task. To address this issue, we highlight that -- while the robot learner may not know what to focus on -- the human teacher does. In this paper we propose that the human proactively marks key parts of their task with small, lightweight beacons. Under our framework (RECON) the human attaches these beacons to task-relevant objects before providing demonstrations: as the human shows examples of the task, beacons track the position of marked objects. We then harness this offline beacon data to train a task-relevant state embedding. Specifically, we embed the robot's observations to a latent state that is correlated with the measured beacon readings: in practice, this causes the robot to autonomously filter out extraneous observations and make decisions based on features learned from the beacon data. Our simulations and a real robot experiment suggest that this framework for human-placed beacons mitigates causal confusion. Indeed, we find that using RECON significantly reduces the number of demonstrations needed to convey the task, lowering the overall time required for human teaching. See videos here: this https URL ",
    "url": "https://arxiv.org/abs/2409.13607",
    "authors": [
      "Robert Ramirez Sanchez",
      "Heramb Nemlekar",
      "Shahabedin Sagheb",
      "Cara M. Nunez",
      "Dylan P. Losey"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.18341",
    "title": "Navigation-Guided Sparse Scene Representation for End-to-End Autonomous Driving",
    "abstract": "           End-to-End Autonomous Driving (E2EAD) methods typically rely on supervised perception tasks to extract explicit scene information (e.g., objects, maps). This reliance necessitates expensive annotations and constrains deployment and data scalability in real-time applications. In this paper, we introduce SSR, a novel framework that utilizes only 16 navigation-guided tokens as Sparse Scene Representation, efficiently extracting crucial scene information for E2EAD. Our method eliminates the need for human-designed supervised sub-tasks, allowing computational resources to concentrate on essential elements directly related to navigation intent. We further introduce a temporal enhancement module, aligning predicted future scenes with actual future scenes through self-supervision. SSR achieves a 27.2\\% relative reduction in L2 error and a 51.6\\% decrease in collision rate to UniAD in nuScenes, with a 10.9$\\times$ faster inference speed and 13$\\times$ faster training time. Moreover, SSR outperforms VAD-Base with a 48.6-point improvement on driving score in CARLA's Town05 Long benchmark. This framework represents a significant leap in real-time autonomous driving systems and paves the way for future scalable deployment. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.18341",
    "authors": [
      "Peidong Li",
      "Dixiao Cui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.19599",
    "title": "DATransNet: Dynamic Attention Transformer Network for Infrared Small Target Detection",
    "abstract": "           Infrared small target detection (ISTD) is widely used in civilian and military applications. However, ISTD encounters several challenges, including the tendency for small and dim targets to be obscured by complex backgrounds. To address this issue, we propose the Dynamic Attention Transformer Network (DATransNet), which aims to extract and preserve detailed information vital for small targets. DATransNet employs the Dynamic Attention Transformer (DATrans), simulating central difference convolutions (CDC) to extract gradient features. Furthermore, we propose a global feature extraction module (GFEM) that offers a comprehensive perspective to prevent the network from focusing solely on details while neglecting the global information. We compare the network with state-of-the-art (SOTA) approaches and demonstrate that our method performs effectively. Our source code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.19599",
    "authors": [
      "Chen Hu",
      "Yian Huang",
      "Kexuan Li",
      "Luping Zhang",
      "Chang Long",
      "Yiming Zhu",
      "Tian Pu",
      "Zhenming Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.19788",
    "title": "Exploring Adversarial Robustness in Classification tasks using DNA Language Models",
    "abstract": "           DNA Language Models, such as GROVER, DNABERT2 and the Nucleotide Transformer, operate on DNA sequences that inherently contain sequencing errors, mutations, and laboratory-induced noise, which may significantly impact model performance. Despite the importance of this issue, the robustness of DNA language models remains largely underexplored. In this paper, we comprehensivly investigate their robustness in DNA classification by applying various adversarial attack strategies: the character (nucleotide substitutions), word (codon modifications), and sentence levels (back-translation-based transformations) to systematically analyze model vulnerabilities. Our results demonstrate that DNA language models are highly susceptible to adversarial attacks, leading to significant performance degradation. Furthermore, we explore adversarial training method as a defense mechanism, which enhances both robustness and classification accuracy. This study highlights the limitations of DNA language models and underscores the necessity of robustness in bioinformatics.         ",
    "url": "https://arxiv.org/abs/2409.19788",
    "authors": [
      "Hyunwoo Yoo",
      "Haebin Shin",
      "Kaidi Xu",
      "Gail Rosen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2409.20164",
    "title": "Erase, then Redraw: A Novel Data Augmentation Approach for Free Space Detection Using Diffusion Model",
    "abstract": "           Data augmentation is one of the most common tools in deep learning, underpinning many recent advances including tasks such as classification, detection, and semantic segmentation. The standard approach to data augmentation involves simple transformations like rotation and flipping to generate new images. However, these new images often lack diversity along the main semantic dimensions within the data. Traditional data augmentation methods cannot alter high-level semantic attributes such as the presence of vehicles, trees, and buildings in a scene to enhance data diversity. In recent years, the rapid development of generative models has injected new vitality into the field of data augmentation. In this paper, we address the lack of diversity in data augmentation for road detection task by using a pre-trained text-to-image diffusion model to parameterize image-to-image transformations. Our method involves editing images using these diffusion models to change their semantics. In essence, we achieve this goal by erasing instances of real objects from the original dataset and generating new instances with similar semantics in the erased regions using the diffusion model, thereby expanding the original dataset. We evaluate our approach on the KITTI road dataset and achieve the best results compared to other data augmentation methods, which demonstrates the effectiveness of our proposed development.         ",
    "url": "https://arxiv.org/abs/2409.20164",
    "authors": [
      "Fulong Ma",
      "Weiqing Qi",
      "Guoyang Zhao",
      "Ming Liu",
      "Jun Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.20171",
    "title": "Annotation-Free Curb Detection Leveraging Altitude Difference Image",
    "abstract": "           Road curbs are considered as one of the crucial and ubiquitous traffic features, which are essential for ensuring the safety of autonomous vehicles. Current methods for detecting curbs primarily rely on camera imagery or LiDAR point clouds. Image-based methods are vulnerable to fluctuations in lighting conditions and exhibit poor robustness, while methods based on point clouds circumvent the issues associated with lighting variations. However, it is the typical case that significant processing delays are encountered due to the voluminous amount of 3D points contained in each frame of the point cloud data. Furthermore, the inherently unstructured characteristics of point clouds poses challenges for integrating the latest deep learning advancements into point cloud data applications. To address these issues, this work proposes an annotation-free curb detection method leveraging Altitude Difference Image (ADI), which effectively mitigates the aforementioned challenges. Given that methods based on deep learning generally demand extensive, manually annotated datasets, which are both expensive and labor-intensive to create, we present an Automatic Curb Annotator (ACA) module. This module utilizes a deterministic curb detection algorithm to automatically generate a vast quantity of training data. Consequently, it facilitates the training of the curb detection model without necessitating any manual annotation of data. Finally, by incorporating a post-processing module, we manage to achieve state-of-the-art results on the KITTI 3D curb dataset with considerably reduced processing delays compared to existing methods, which underscores the effectiveness of our approach in curb detection tasks.         ",
    "url": "https://arxiv.org/abs/2409.20171",
    "authors": [
      "Fulong Ma",
      "Peng Hou",
      "Yuxuan Liu",
      "Yang Liu",
      "Ming Liu",
      "Jun Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.00722",
    "title": "On the Geometry and Optimization of Polynomial Convolutional Networks",
    "abstract": "           We study convolutional neural networks with monomial activation functions. Specifically, we prove that their parameterization map is regular and is an isomorphism almost everywhere, up to rescaling the filters. By leveraging on tools from algebraic geometry, we explore the geometric properties of the image in function space of this map - typically referred to as neuromanifold. In particular, we compute the dimension and the degree of the neuromanifold, which measure the expressivity of the model, and describe its singularities. Moreover, for a generic large dataset, we derive an explicit formula that quantifies the number of critical points arising in the optimization of a regression loss.         ",
    "url": "https://arxiv.org/abs/2410.00722",
    "authors": [
      "Vahid Shahverdi",
      "Giovanni Luca Marchetti",
      "Kathl\u00e9n Kohn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Algebraic Geometry (math.AG)"
    ]
  },
  {
    "id": "arXiv:2410.00812",
    "title": "Generative causal testing to bridge data-driven models and scientific theories in language neuroscience",
    "abstract": "           Representations from large language models are highly effective at predicting BOLD fMRI responses to language stimuli. However, these representations are largely opaque: it is unclear what features of the language stimulus drive the response in each brain area. We present generative causal testing (GCT), a framework for generating concise explanations of language selectivity in the brain from predictive models and then testing those explanations in follow-up experiments using LLM-generated this http URL approach is successful at explaining selectivity both in individual voxels and cortical regions of interest (ROIs), including newly identified microROIs in prefrontal cortex. We show that explanatory accuracy is closely related to the predictive power and stability of the underlying predictive models. Finally, we show that GCT can dissect fine-grained differences between brain areas with similar functional selectivity. These results demonstrate that LLMs can be used to bridge the widening gap between data-driven models and formal scientific theories.         ",
    "url": "https://arxiv.org/abs/2410.00812",
    "authors": [
      "Richard Antonello",
      "Chandan Singh",
      "Shailee Jain",
      "Aliyah Hsu",
      "Sihang Guo",
      "Jianfeng Gao",
      "Bin Yu",
      "Alexander Huth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2410.01337",
    "title": "PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems",
    "abstract": "           Solving partial differential equations (PDEs) serves as a cornerstone for modeling complex dynamical systems. Recent progresses have demonstrated grand benefits of data-driven neural-based models for predicting spatiotemporal dynamics (e.g., tremendous speedup gain compared with classical numerical methods). However, most existing neural models rely on rich training data, have limited extrapolation and generalization abilities, and suffer to produce precise or reliable physical prediction under intricate conditions (e.g., irregular mesh or geometry, complex boundary conditions, diverse PDE parameters, etc.). To this end, we propose a new graph learning approach, namely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model spatiotemporal PDE systems on irregular meshes given small training datasets. Specifically, we incorporate a GNN into a numerical integrator to approximate the temporal marching of spatiotemporal dynamics for a given PDE system. Considering that many physical phenomena are governed by diffusion processes, we further design a learnable Laplace block, which encodes the discrete Laplace-Beltrami operator, to aid and guide the GNN learning in a physically feasible solution space. A boundary condition padding strategy is also designed to improve the model convergence and accuracy. Extensive experiments demonstrate that PhyMPGN is capable of accurately predicting various types of spatiotemporal dynamics on coarse unstructured meshes, consistently achieves the state-of-the-art results, and outperforms other baselines with considerable gains.         ",
    "url": "https://arxiv.org/abs/2410.01337",
    "authors": [
      "Bocheng Zeng",
      "Qi Wang",
      "Mengtao Yan",
      "Yang Liu",
      "Ruizhi Chengze",
      "Yi Zhang",
      "Hongsheng Liu",
      "Zidong Wang",
      "Hao Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2410.02242",
    "title": "Robust Weight Initialization for Tanh Neural Networks with Fixed Point Analysis",
    "abstract": "           As a neural network's depth increases, it can improve generalization performance. However, training deep networks is challenging due to gradient and signal propagation issues. To address these challenges, extensive theoretical research and various methods have been introduced. Despite these advances, effective weight initialization methods for tanh neural networks remain insufficiently investigated. This paper presents a novel weight initialization method for neural networks with tanh activation function. Based on an analysis of the fixed points of the function $\\tanh(ax)$, the proposed method aims to determine values of $a$ that mitigate activation saturation. A series of experiments on various classification datasets and physics-informed neural networks demonstrates that the proposed method outperforms Xavier initialization methods~(with or without normalization) in terms of robustness across different network sizes, data efficiency, and convergence speed. Code is available at this https URL ",
    "url": "https://arxiv.org/abs/2410.02242",
    "authors": [
      "Hyunwoo Lee",
      "Hayoung Choi",
      "Hyunju Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.03524",
    "title": "Steering Large Language Models between Code Execution and Textual Reasoning",
    "abstract": "           While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\\% success through direct coding, which is more scalable and avoids the computational overhead associated with textual iterating and searching. Textual reasoning has inherent limitations in solving tasks with challenges in math, logics, optimization, and searching, which is unlikely to be solved by simply scaling up the model and data size. The recently released OpenAI GPT Code Interpreter and multi-agent frameworks such as AutoGen have demonstrated remarkable proficiency of integrating code generation and execution to solve complex tasks using LLMs. However, based on our experiments on 7 existing popular methods for steering code/text generation in both single- and multi-turn settings with 14 tasks and 6 types of LLMs (including the new O1-preview), currently there is no optimal method to correctly steer LLMs to write code when needed. We discover some interesting patterns on when models use code vs. textual reasoning with the evolution to task complexity and model sizes, which even result in an astonishingly inverse scaling behavior. We also discover that results from LLM written code are not always better than using textual reasoning, even if the task could be solved through code. To mitigate the above issues, we propose three methods to better steer LLM code/text generation and achieve a notable improvement. The costs of token lengths and runtime are thoroughly discussed for all the methods. We believe the problem of steering LLM code/text generation is critical for future research and has much space for further improvement. Project Page, Datasets, and Codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.03524",
    "authors": [
      "Yongchao Chen",
      "Harsh Jhamtani",
      "Srinagesh Sharma",
      "Chuchu Fan",
      "Chi Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2410.04592",
    "title": "CardioAI: A Multimodal AI-based System to Support Symptom Monitoring and Risk Detection of Cancer Treatment-Induced Cardiotoxicity",
    "abstract": "           Despite recent advances in cancer treatments that prolong patients' lives, treatment-induced cardiotoxicity remains one severe side effect. The clinical decision-making of cardiotoxicity is challenging, as non-clinical symptoms can be missed until life-threatening events occur at a later stage, and clinicians already have a high workload centered on the treatment, not the side effects. Our project starts with a participatory design study with 11 clinicians to understand their practices and needs; then we build a multimodal AI system, CardioAI, that integrates wearables and LLM-powered voice assistants to monitor multimodal non-clinical symptoms. Also, the system includes an explainable risk prediction module that can generate cardiotoxicity risk scores and summaries as explanations to support clinicians' decision-making. We conducted a heuristic evaluation with four clinical experts and found that they all believe CardioAI integrates well into their workflow, reduces their information overload, and enables them to make more informed decisions.         ",
    "url": "https://arxiv.org/abs/2410.04592",
    "authors": [
      "Siyi Wu",
      "Weidan Cao",
      "Shihan Fu",
      "Bingsheng Yao",
      "Ziqi Yang",
      "Changchang Yin",
      "Varun Mishra",
      "Daniel Addison",
      "Ping Zhang",
      "Dakuo Wang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2410.04810",
    "title": "FedBiP: Heterogeneous One-Shot Federated Learning with Personalized Latent Diffusion Models",
    "abstract": "           One-Shot Federated Learning (OSFL), a special decentralized machine learning paradigm, has recently gained significant attention. OSFL requires only a single round of client data or model upload, which reduces communication costs and mitigates privacy threats compared to traditional FL. Despite these promising prospects, existing methods face challenges due to client data heterogeneity and limited data quantity when applied to real-world OSFL systems. Recently, Latent Diffusion Models (LDM) have shown remarkable advancements in synthesizing high-quality images through pretraining on large-scale datasets, thereby presenting a potential solution to overcome these issues. However, directly applying pretrained LDM to heterogeneous OSFL results in significant distribution shifts in synthetic data, leading to performance degradation in classification models trained on such data. This issue is particularly pronounced in rare domains, such as medical imaging, which are underrepresented in LDM's pretraining data. To address this challenge, we propose Federated Bi-Level Personalization (FedBiP), which personalizes the pretrained LDM at both instance-level and concept-level. Hereby, FedBiP synthesizes images following the client's local data distribution without compromising the privacy regulations. FedBiP is also the first approach to simultaneously address feature space heterogeneity and client data scarcity in OSFL. Our method is validated through extensive experiments on three OSFL benchmarks with feature space heterogeneity, as well as on challenging medical and satellite image datasets with label heterogeneity. The results demonstrate the effectiveness of FedBiP, which substantially outperforms other OSFL methods.         ",
    "url": "https://arxiv.org/abs/2410.04810",
    "authors": [
      "Haokun Chen",
      "Hang Li",
      "Yao Zhang",
      "Jinhe Bi",
      "Gengyuan Zhang",
      "Yueqi Zhang",
      "Philip Torr",
      "Jindong Gu",
      "Denis Krompass",
      "Volker Tresp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2410.05595",
    "title": "Disruption Risk Evaluation on Large-scale Production Network with Establishments and Products",
    "abstract": "           We constructed an establishment-level production network where each establishment inputs and outputs multiple products, using data that includes the firm-level production network and establishments covering nearly all Japanese entities. The network represents the manufacturing sector with 183,951 establishments across 157,537 firms and 919,982 inter-establishment linkages. A probabilistic model of supply chain disruptions was applied to this network. The key findings are as follows: (1) The establishment-level network exhibits greater shock propagation compared to the firm-level network. (2) Incorporating actual product information leads to a larger impact on propagation compared to using industry-level information. (3) Regional shock simulations reveal that while the firm-level network shows greater shock propagation when the shock originates in Tokyo, no such difference is observed in the establishment-level network.         ",
    "url": "https://arxiv.org/abs/2410.05595",
    "authors": [
      "Hiroyasu Inoue",
      "Yasuyuki Todo"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2410.05643",
    "title": "TRACE: Temporal Grounding Video LLM via Causal Event Modeling",
    "abstract": "           Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs for VTG tasks. However, current video LLM-based methods rely exclusively on natural language generation, lacking the ability to model the clear structure inherent in videos, which restricts their effectiveness in tackling VTG tasks. To address this issue, this paper first formally introduces causal event modeling framework, which represents video LLM outputs as sequences of events, and predict the current event using previous events, video inputs, and textural instructions. Each event consists of three components: timestamps, salient scores, and textual captions. We then propose a novel task-interleaved video LLM called TRACE to effectively implement the causal event modeling framework in practice. The TRACE process visual frames, timestamps, salient scores, and text as distinct tasks, employing various encoders and decoding heads for each. Task tokens are arranged in an interleaved sequence according to the causal event modeling framework's formulation. Extensive experiments on various VTG tasks and datasets demonstrate the superior performance of TRACE compared to state-of-the-art video LLMs. Our model and code are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.05643",
    "authors": [
      "Yongxin Guo",
      "Jingyu Liu",
      "Mingda Li",
      "Qingbin Liu",
      "Xi Chen",
      "Xiaoying Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.06700",
    "title": "Optimizing Integrated Terrestrial and Non-Terrestrial Networks Performance with Traffic-Aware Resource Management",
    "abstract": "           To address an ever-increasing demand for ubiquitous high-speed connectivity, mobile network deployments are becoming increasingly dense. However, this densification has also led to a surge in overall energy consumption, making the process increasingly challenging. In recent years, non-terrestrial networks (NTNs) have been mainly endorsed as a potential solution to enhance coverage by complementing the coverage of the terrestrial network (TN) in areas with limited network deployment. However, their ability to reduce TN energy consumption, though often overlooked, remains a significant advantage. To this end, this paper introduces a novel radio resource management algorithm, BLASTER (Bandwidth SpLit, User ASsociation, and PowEr ContRol), which integrates bandwidth allocation, user equipment (UE) association, power control, and base station activation within an integrated terrestrial and non-terrestrial network (TN-NTN). This algorithm aims to optimize network resource allocation fairness and energy consumption dynamically, demonstrating new opportunities in deploying satellite networks in legacy cellular systems. Our study offers a comprehensive analysis of the integrated network model, emphasizing the effective balance between energy saving and Quality of Service (QoS), and proposing practical solutions to meet the fluctuating traffic demands of cellular networks.         ",
    "url": "https://arxiv.org/abs/2410.06700",
    "authors": [
      "Henri Alam",
      "Antonio de Domenico",
      "David L\u00f3pez-P\u00e9rez",
      "Florian Kaltenberger"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2410.07168",
    "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
    "abstract": "           Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.         ",
    "url": "https://arxiv.org/abs/2410.07168",
    "authors": [
      "Cheol Jun Cho",
      "Nicholas Lee",
      "Akshat Gupta",
      "Dhruv Agarwal",
      "Ethan Chen",
      "Alan W Black",
      "Gopala K. Anumanchipalli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2410.08190",
    "title": "Poison-splat: Computation Cost Attack on 3D Gaussian Splatting",
    "abstract": "           3D Gaussian splatting (3DGS), known for its groundbreaking performance and efficiency, has become a dominant 3D representation and brought progress to many 3D vision tasks. However, in this work, we reveal a significant security vulnerability that has been largely overlooked in 3DGS: the computation cost of training 3DGS could be maliciously tampered by poisoning the input data. By developing an attack named Poison-splat, we reveal a novel attack surface where the adversary can poison the input images to drastically increase the computation memory and time needed for 3DGS training, pushing the algorithm towards its worst computation complexity. In extreme cases, the attack can even consume all allocable memory, leading to a Denial-of-Service (DoS) that disrupts servers, resulting in practical damages to real-world 3DGS service vendors. Such a computation cost attack is achieved by addressing a bi-level optimization problem through three tailored strategies: attack objective approximation, proxy model rendering, and optional constrained optimization. These strategies not only ensure the effectiveness of our attack but also make it difficult to defend with simple defensive measures. We hope the revelation of this novel attack surface can spark attention to this crucial yet overlooked vulnerability of 3DGS systems. Our code is available at this https URL .         ",
    "url": "https://arxiv.org/abs/2410.08190",
    "authors": [
      "Jiahao Lu",
      "Yifan Zhang",
      "Qiuhong Shen",
      "Xinchao Wang",
      "Shuicheng Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.08208",
    "title": "SPA: 3D Spatial-Awareness Enables Effective Embodied Representation",
    "abstract": "           In this paper, we introduce SPA, a novel representation learning framework that emphasizes the importance of 3D spatial awareness in embodied AI. Our approach leverages differentiable neural rendering on multi-view images to endow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding. We present the most comprehensive evaluation of embodied representation learning to date, covering 268 tasks across 8 simulators with diverse policies in both single-task and language-conditioned multi-task scenarios. The results are compelling: SPA consistently outperforms more than 10 state-of-the-art representation methods, including those specifically designed for embodied AI, vision-centric tasks, and multi-modal applications, while using less training data. Furthermore, we conduct a series of real-world experiments to confirm its effectiveness in practical scenarios. These results highlight the critical role of 3D spatial awareness for embodied representation learning. Our strongest model takes more than 6000 GPU hours to train and we are committed to open-sourcing all code and model weights to foster future research in embodied representation learning. Project Page: this https URL.         ",
    "url": "https://arxiv.org/abs/2410.08208",
    "authors": [
      "Haoyi Zhu",
      "Honghui Yang",
      "Yating Wang",
      "Jiange Yang",
      "Limin Wang",
      "Tong He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2410.09878",
    "title": "Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning",
    "abstract": "           Conformal prediction provides model-agnostic and distribution-free uncertainty quantification through prediction sets that are guaranteed to include the ground truth with any user-specified probability. Yet, conformal prediction is not reliable under poisoning attacks where adversaries manipulate both training and calibration data, which can significantly alter prediction sets in practice. As a solution, we propose reliable prediction sets (RPS): the first efficient method for constructing conformal prediction sets with provable reliability guarantees under poisoning. To ensure reliability under training poisoning, we introduce smoothed score functions that reliably aggregate predictions of classifiers trained on distinct partitions of the training data. To ensure reliability under calibration poisoning, we construct multiple prediction sets, each calibrated on distinct subsets of the calibration data. We then aggregate them into a majority prediction set, which includes a class only if it appears in a majority of the individual sets. Both proposed aggregations mitigate the influence of datapoints in the training and calibration data on the final prediction set. We experimentally validate our approach on image classification tasks, achieving strong reliability while maintaining utility and preserving coverage on clean data. Overall, our approach represents an important step towards more trustworthy uncertainty quantification in the presence of data poisoning.         ",
    "url": "https://arxiv.org/abs/2410.09878",
    "authors": [
      "Yan Scholten",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.10322",
    "title": "Feature Averaging: An Implicit Bias of Gradient Descent Leading to Non-Robustness in Neural Networks",
    "abstract": "           In this work, we investigate a particular implicit bias in gradient descent training, which we term \"Feature Averaging,\" and argue that it is one of the principal factors contributing to the non-robustness of deep neural networks. We show that, even when multiple discriminative features are present in the input data, neural networks trained by gradient descent tend to rely on an average (or a certain combination) of these features for classification, rather than distinguishing and leveraging each feature individually. Specifically, we provide a detailed theoretical analysis of the training dynamics of two-layer ReLU networks on a binary classification task, where the data distribution consists of multiple clusters with mutually orthogonal centers. We rigorously prove that gradient descent biases the network towards feature averaging, where the weights of each hidden neuron represent an average of the cluster centers (each corresponding to a distinct feature), thereby making the network vulnerable to input perturbations aligned with the negative direction of the averaged features. On the positive side, we demonstrate that this vulnerability can be mitigated through more granular supervision. In particular, we prove that a two-layer ReLU network can achieve optimal robustness when trained to classify individual features rather than merely the original binary classes. Finally, we validate our theoretical findings with experiments on synthetic datasets, MNIST, and CIFAR-10, and confirm the prevalence of feature averaging and its impact on adversarial robustness. We hope these theoretical and empirical insights deepen the understanding of how gradient descent shapes feature learning and adversarial robustness, and how more detailed supervision can enhance robustness.         ",
    "url": "https://arxiv.org/abs/2410.10322",
    "authors": [
      "Binghui Li",
      "Zhixuan Pan",
      "Kaifeng Lyu",
      "Jian Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2410.11112",
    "title": "Differentiable Weightless Neural Networks",
    "abstract": "           We introduce the Differentiable Weightless Neural Network (DWN), a model based on interconnected lookup tables. Training of DWNs is enabled by a novel Extended Finite Difference technique for approximate differentiation of binary values. We propose Learnable Mapping, Learnable Reduction, and Spectral Regularization to further improve the accuracy and efficiency of these models. We evaluate DWNs in three edge computing contexts: (1) an FPGA-based hardware accelerator, where they demonstrate superior latency, throughput, energy efficiency, and model area compared to state-of-the-art solutions, (2) a low-power microcontroller, where they achieve preferable accuracy to XGBoost while subject to stringent memory constraints, and (3) ultra-low-cost chips, where they consistently outperform small models in both accuracy and projected hardware area. DWNs also compare favorably against leading approaches for tabular datasets, with higher average rank. Overall, our work positions DWNs as a pioneering solution for edge-compatible high-throughput neural networks.         ",
    "url": "https://arxiv.org/abs/2410.11112",
    "authors": [
      "Alan T. L. Bacellar",
      "Zachary Susskind",
      "Mauricio Breternitz Jr.",
      "Eugene John",
      "Lizy K. John",
      "Priscila M. V. Lima",
      "Felipe M. G. Fran\u00e7a"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.11570",
    "title": "A Data-Driven Aggressive Autonomous Racing Framework Utilizing Local Trajectory Planning with Velocity Prediction",
    "abstract": "           The development of autonomous driving has boosted the research on autonomous racing. However, existing local trajectory planning methods have difficulty planning trajectories with optimal velocity profiles at racetracks with sharp corners, thus weakening the performance of autonomous racing. To address this problem, we propose a local trajectory planning method that integrates Velocity Prediction based on Model Predictive Contouring Control (VPMPCC). The optimal parameters of VPMPCC are learned through Bayesian Optimization (BO) based on a proposed novel Objective Function adapted to Racing (OFR). Specifically, VPMPCC achieves velocity prediction by encoding the racetrack as a reference velocity profile and incorporating it into the optimization problem. This method optimizes the velocity profile of local trajectories, especially at corners with significant curvature. The proposed OFR balances racing performance with vehicle safety, ensuring safe and efficient BO training. In the simulation, the number of training iterations for OFR-based BO is reduced by 42.86% compared to the state-of-the-art method. The optimal simulation-trained parameters are then applied to a real-world F1TENTH vehicle without retraining. During prolonged racing on a custom-built racetrack featuring significant sharp corners, the mean projected velocity of VPMPCC reaches 93.18% of the vehicle's handling limits. The released code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.11570",
    "authors": [
      "Zhouheng Li",
      "Bei Zhou",
      "Cheng Hu",
      "Lei Xie",
      "Hongye Su"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2410.12343",
    "title": "Federated Temporal Graph Clustering",
    "abstract": "           Temporal graph clustering is a complex task that involves discovering meaningful structures in dynamic graphs where relationships and entities change over time. Existing methods typically require centralized data collection, which poses significant privacy and communication challenges. In this work, we introduce a novel Federated Temporal Graph Clustering (FTGC) framework that enables decentralized training of graph neural networks (GNNs) across multiple clients, ensuring data privacy throughout the process. Our approach incorporates a temporal aggregation mechanism to effectively capture the evolution of graph structures over time and a federated optimization strategy to collaboratively learn high-quality clustering representations. By preserving data privacy and reducing communication overhead, our framework achieves competitive performance on temporal graph datasets, making it a promising solution for privacy-sensitive, real-world applications involving dynamic data.         ",
    "url": "https://arxiv.org/abs/2410.12343",
    "authors": [
      "Zihao Zhou",
      "Yang Liu",
      "Xianghong Xu",
      "Qian Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2410.14109",
    "title": "Improving Graph Neural Networks by Learning Continuous Edge Directions",
    "abstract": "           Graph Neural Networks (GNNs) traditionally employ a message-passing mechanism that resembles diffusion over undirected graphs, which often leads to homogenization of node features and reduced discriminative power in tasks such as node classification. Our key insight for addressing this limitation is to assign fuzzy edge directions -- that can vary continuously from node $i$ pointing to node $j$ to vice versa -- to the edges of a graph so that features can preferentially flow in one direction between nodes to enable long-range information transmission across the graph. We also introduce a novel complex-valued Laplacian for directed graphs with fuzzy edges where the real and imaginary parts represent information flow in opposite directions. Using this Laplacian, we propose a general framework, called Continuous Edge Direction (CoED) GNN, for learning on graphs with fuzzy edges and prove its expressivity limits using a generalization of the Weisfeiler-Leman (WL) graph isomorphism test for directed graphs with fuzzy edges. Our architecture aggregates neighbor features scaled by the learned edge directions and processes the aggregated messages from in-neighbors and out-neighbors separately alongside the self-features of the nodes. Since continuous edge directions are differentiable, they can be learned jointly with the GNN weights via gradient-based optimization. CoED GNN is particularly well-suited for graph ensemble data where the graph structure remains fixed but multiple realizations of node features are available, such as in gene regulatory networks, web connectivity graphs, and power grids. We demonstrate through extensive experiments on both synthetic and real graph ensemble datasets that learning continuous edge directions significantly improves performance both for undirected and directed graphs compared with existing methods.         ",
    "url": "https://arxiv.org/abs/2410.14109",
    "authors": [
      "Seong Ho Pahng",
      "Sahand Hormoz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.14493",
    "title": "Safeguarding Blockchain Ecosystem: Understanding and Detecting Attack Transactions on Cross-chain Bridges",
    "abstract": "           Cross-chain bridges are essential decentralized applications (DApps) to facilitate interoperability between different blockchain networks. Unlike regular DApps, the functionality of cross-chain bridges relies on the collaboration of information both on and off the chain, which exposes them to a wider risk of attacks. According to our statistics, attacks on cross-chain bridges have resulted in losses of nearly 4.3 billion dollars since 2021. Therefore, it is particularly necessary to understand and detect attacks on cross-chain bridges. In this paper, we collect the largest number of cross-chain bridge attack incidents to date, including 49 attacks that occurred between June 2021 and September 2024. Our analysis reveal that attacks against cross-chain business logic cause significantly more damage than those that do not. These cross-chain attacks exhibit different patterns compared to normal transactions in terms of call structure, which effectively indicates potential attack behaviors. Given the significant losses in these cases and the scarcity of related research, this paper aims to detect attacks against cross-chain business logic, and propose the BridgeGuard tool. Specifically, BridgeGuard models cross-chain transactions from a graph perspective, and employs a two-stage detection framework comprising global and local graph mining to identify attack patterns in cross-chain transactions. We conduct multiple experiments on the datasets with 203 attack transactions and 40,000 normal cross-chain transactions. The results show that BridgeGuard's reported recall score is 36.32\\% higher than that of state-of-the-art tools and can detect unknown attack transactions.         ",
    "url": "https://arxiv.org/abs/2410.14493",
    "authors": [
      "Jiajing Wu",
      "Kaixin Lin",
      "Dan Lin",
      "Bozhao Zhang",
      "Zhiying Wu",
      "Jianzhong Su"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2410.14666",
    "title": "DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph",
    "abstract": "           Summarizing movie screenplays presents a unique set of challenges compared to standard document summarization. Screenplays are not only lengthy, but also feature a complex interplay of characters, dialogues, and scenes, with numerous direct and subtle relationships and contextual nuances that are difficult for machine learning models to accurately capture and comprehend. Recent attempts at screenplay summarization focus on fine-tuning transformer-based pre-trained models, but these models often fall short in capturing long-term dependencies and latent relationships, and frequently encounter the \"lost in the middle\" issue. To address these challenges, we introduce DiscoGraMS, a novel resource that represents movie scripts as a movie character-aware discourse graph (CaD Graph). This approach is well-suited for various downstream tasks, such as summarization, question-answering, and salience detection. The model aims to preserve all salient information, offering a more comprehensive and faithful representation of the screenplay's content. We further explore a baseline method that combines the CaD Graph with the corresponding movie script through a late fusion of graph and text modalities, and we present very initial promising results.         ",
    "url": "https://arxiv.org/abs/2410.14666",
    "authors": [
      "Maitreya Prafulla Chitale",
      "Uday Bindal",
      "Rajakrishnan Rajkumar",
      "Rahul Mishra"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.16699",
    "title": "Graph Transformers Dream of Electric Flow",
    "abstract": "           We show theoretically and empirically that the linear Transformer, when applied to graph data, can implement algorithms that solve canonical problems such as electric flow and eigenvector decomposition. The Transformer has access to information on the input graph only via the graph's incidence matrix. We present explicit weight configurations for implementing each algorithm, and we bound the constructed Transformers' errors by the errors of the underlying algorithms. Our theoretical findings are corroborated by experiments on synthetic data. Additionally, on a real-world molecular regression task, we observe that the linear Transformer is capable of learning a more effective positional encoding than the default one based on Laplacian eigenvectors. Our work is an initial step towards elucidating the inner-workings of the Transformer for graph data. Code is available at this https URL ",
    "url": "https://arxiv.org/abs/2410.16699",
    "authors": [
      "Xiang Cheng",
      "Lawrence Carin",
      "Suvrit Sra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.20026",
    "title": "Towards Robust Algorithms for Surgical Phase Recognition via Digital Twin Representation",
    "abstract": "           Surgical phase recognition (SPR) is an integral component of surgical data science, enabling high-level surgical analysis. End-to-end trained neural networks that predict surgical phase directly from videos have shown excellent performance on benchmarks. However, these models struggle with robustness due to non-causal associations in the training set. Our goal is to improve model robustness to variations in the surgical videos by leveraging the digital twin (DT) paradigm -- an intermediary layer to separate high-level analysis (SPR) from low-level processing. As a proof of concept, we present a DT representation-based framework for SPR from videos. The framework employs vision foundation models with reliable low-level scene understanding to craft DT representation. We embed the DT representation in place of raw video inputs in the state-of-the-art SPR model. The framework is trained on the Cholec80 dataset and evaluated on out-of-distribution (OOD) and corrupted test samples. Contrary to the vulnerability of the baseline model, our framework demonstrates strong robustness on both OOD and corrupted samples, with a video-level accuracy of 80.3 on a highly corrupted Cholec80 test set, 67.9 on the challenging CRCD dataset, and 99.8 on an internal robotic surgery dataset, outperforming the baseline by 3.9, 16.8, and 90.9 respectively. We also find that using DT representation as an augmentation to the raw input can significantly improve model robustness. Our findings lend support to the thesis that DT representations are effective in enhancing model robustness. Future work will seek to improve the feature informativeness and incorporate interpretability for a more comprehensive framework.         ",
    "url": "https://arxiv.org/abs/2410.20026",
    "authors": [
      "Hao Ding",
      "Yuqian Zhang",
      "Wenzheng Cheng",
      "Xinyu Wang",
      "Xu Lian",
      "Chenhao Yu",
      "Hongchao Shu",
      "Ji Woong Kim",
      "Axel Krieger",
      "Mathias Unberath"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.21637",
    "title": "Mitigating Paraphrase Attacks on Machine-Text Detectors via Paraphrase Inversion",
    "abstract": "           High-quality paraphrases are easy to produce using instruction-tuned language models or specialized paraphrasing models. Although this capability has a variety of benign applications, paraphrasing attacks$\\unicode{x2013}$paraphrases applied to machine-generated texts$\\unicode{x2013}$are known to significantly degrade the performance of machine-text detectors. This motivates us to consider the novel problem of paraphrase inversion, where, given paraphrased text, the objective is to recover an approximation of the original text. The closer the approximation is to the original text, the better machine-text detectors will perform. We propose an approach which frames the problem as translation from paraphrased text back to the original text, which requires examples of texts and corresponding paraphrases to train the inversion model. Fortunately, such training data can easily be generated, given a corpus of original texts and one or more paraphrasing models. We find that language models such as GPT-4 and Llama-3 exhibit biases when paraphrasing which an inversion model can learn with a modest amount of data. Perhaps surprisingly, we also find that such models generalize well, including to paraphrase models unseen at training time. Finally, we show that when combined with a paraphrased-text detector, our inversion models provide an effective defense against paraphrasing attacks, and overall our approach yields an average improvement of +22% AUROC across seven machine-text detectors and three different domains.         ",
    "url": "https://arxiv.org/abs/2410.21637",
    "authors": [
      "Rafael Rivera Soto",
      "Barry Chen",
      "Nicholas Andrews"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.22371",
    "title": "Error Bounds for Physics-Informed Neural Networks in Fokker-Planck PDEs",
    "abstract": "           Stochastic differential equations are commonly used to describe the evolution of stochastic processes. The state uncertainty of such processes is best represented by the probability density function (PDF), whose evolution is governed by the Fokker-Planck partial differential equation (FP-PDE). However, it is generally infeasible to solve the FP-PDE in closed form. In this work, we show that physics-informed neural networks (PINNs) can be trained to approximate the solution PDF. Our main contribution is the analysis of PINN approximation error: we develop a theoretical framework to construct tight error bounds using PINNs. In addition, we derive a practical error bound that can be efficiently constructed with standard training methods. We discuss that this error-bound framework generalizes to approximate solutions of other linear PDEs. Empirical results on nonlinear, high-dimensional, and chaotic systems validate the correctness of our error bounds while demonstrating the scalability of PINNs and their significant computational speedup in obtaining accurate PDF solutions compared to the Monte Carlo approach.         ",
    "url": "https://arxiv.org/abs/2410.22371",
    "authors": [
      "Chun-Wei Kong",
      "Luca Laurenti",
      "Jay McMahon",
      "Morteza Lahijanian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2411.02322",
    "title": "LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation",
    "abstract": "           Directed acyclic graphs (DAGs) serve as crucial data representations in domains such as hardware synthesis and compiler/program optimization for computing systems. DAG generative models facilitate the creation of synthetic DAGs, which can be used for benchmarking computing systems while preserving intellectual property. However, generating realistic DAGs is challenging due to their inherent directional and logical dependencies. This paper introduces LayerDAG, an autoregressive diffusion model, to address these challenges. LayerDAG decouples the strong node dependencies into manageable units that can be processed sequentially. By interpreting the partial order of nodes as a sequence of bipartite graphs, LayerDAG leverages autoregressive generation to model directional dependencies and employs diffusion models to capture logical dependencies within each bipartite graph. Comparative analyses demonstrate that LayerDAG outperforms existing DAG generative models in both expressiveness and generalization, particularly for generating large-scale DAGs with up to 400 nodes-a critical scenario for system benchmarking. Extensive experiments on both synthetic and real-world flow graphs from various computing platforms show that LayerDAG generates valid DAGs with superior statistical properties and benchmarking performance. The synthetic DAGs generated by LayerDAG enhance the training of ML-based surrogate models, resulting in improved accuracy in predicting performance metrics of real-world DAGs across diverse computing platforms.         ",
    "url": "https://arxiv.org/abs/2411.02322",
    "authors": [
      "Mufei Li",
      "Viraj Shitole",
      "Eli Chien",
      "Changhai Man",
      "Zhaodong Wang",
      "Srinivas Sridharan",
      "Ying Zhang",
      "Tushar Krishna",
      "Pan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2411.08402",
    "title": "V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with Denoising Diffusion",
    "abstract": "           Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D object detection using LiDAR and camera data. However, these methods suffer from performance degradation in adverse weather conditions. The weatherrobust 4D radar provides Doppler and additional geometric information, raising the possibility of addressing this challenge. To this end, we present V2X-R, the first simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-R contains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point clouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes. Subsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for 3D object detection and implement it with various fusion strategies. To achieve weather-robust detection, we additionally propose a Multi-modal Denoising Diffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D radar feature as a condition to prompt the diffusion model to denoise noisy LiDAR features. Experiments show that our LiDAR-4D radar fusion pipeline demonstrates superior performance in the V2X-R dataset. Over and above this, our MDD module further improved the performance of basic fusion model by up to 5.73%/6.70% in foggy/snowy conditions with barely disrupting normal performance. The dataset and code will be publicly available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2411.08402",
    "authors": [
      "Xun Huang",
      "Jinlong Wang",
      "Qiming Xia",
      "Siheng Chen",
      "Bisheng Yang",
      "Xin Li",
      "Cheng Wang",
      "Chenglu Wen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.08470",
    "title": "HyperFace: Generating Synthetic Face Recognition Datasets by Exploring Face Embedding Hypersphere",
    "abstract": "           Face recognition datasets are often collected by crawling Internet and without individuals' consents, raising ethical and privacy concerns. Generating synthetic datasets for training face recognition models has emerged as a promising alternative. However, the generation of synthetic datasets remains challenging as it entails adequate inter-class and intra-class variations. While advances in generative models have made it easier to increase intra-class variations in face datasets (such as pose, illumination, etc.), generating sufficient inter-class variation is still a difficult task. In this paper, we formulate the dataset generation as a packing problem on the embedding space (represented on a hypersphere) of a face recognition model and propose a new synthetic dataset generation approach, called HyperFace. We formalize our packing problem as an optimization problem and solve it with a gradient descent-based approach. Then, we use a conditional face generator model to synthesize face images from the optimized embeddings. We use our generated datasets to train face recognition models and evaluate the trained models on several benchmarking real datasets. Our experimental results show that models trained with HyperFace achieve state-of-the-art performance in training face recognition using synthetic datasets.         ",
    "url": "https://arxiv.org/abs/2411.08470",
    "authors": [
      "Hatef Otroshi Shahreza",
      "S\u00e9bastien Marcel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.10509",
    "title": "TESGNN: Temporal Equivariant Scene Graph Neural Networks for Efficient and Robust Multi-View 3D Scene Understanding",
    "abstract": "           Scene graphs have proven to be highly effective for various scene understanding tasks due to their compact and explicit representation of relational information. However, current methods often overlook the critical importance of preserving symmetry when generating scene graphs from 3D point clouds, which can lead to reduced accuracy and robustness, particularly when dealing with noisy, multi-view data. Furthermore, a major limitation of prior approaches is the lack of temporal modeling to capture time-dependent relationships among dynamically evolving entities in a scene. To address these challenges, we propose Temporal Equivariant Scene Graph Neural Network (TESGNN), consisting of two key components: (1) an Equivariant Scene Graph Neural Network (ESGNN), which extracts information from 3D point clouds to generate scene graph while preserving crucial symmetry properties, and (2) a Temporal Graph Matching Network, which fuses scene graphs generated by ESGNN across multiple time sequences into a unified global representation using an approximate graph-matching algorithm. Our combined architecture TESGNN outperforms current state-of-the-art methods in scene graph generation, achieving higher accuracy and faster training convergence. Moreover, we show that leveraging the symmetry-preserving property produces a more stable and accurate global scene representation compared to existing approaches. Last but not least, it is computationally efficient and easily implementable using existing frameworks, making it well-suited for real-time applications in robotics and computer vision. This approach paves the way for more robust and scalable solutions to complex multi-view scene understanding challenges. Our source code is publicly available at: this https URL ",
    "url": "https://arxiv.org/abs/2411.10509",
    "authors": [
      "Quang P. M. Pham",
      "Khoi T. N. Nguyen",
      "Lan C. Ngo",
      "Truong Do",
      "Dezhen Song",
      "Truong-Son Hy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.10868",
    "title": "Destabilizing a Social Network Model via Intrinsic Feedback Vulnerabilities",
    "abstract": "           Social influence plays a significant role in shaping individual sentiments and actions, particularly in a world of ubiquitous digital interconnection. The rapid development of generative AI has engendered well-founded concerns regarding the potential scalable implementation of radicalization techniques in social media. Motivated by these developments, we present a case study investigating the effects of small but intentional perturbations on a simple social network. We employ Taylor's classic model of social influence and tools from robust control theory (most notably the Dynamical Structure Function (DSF)), to identify perturbations that qualitatively alter the system's behavior while remaining as unobtrusive as possible. We examine two such scenarios: perturbations to an existing link and perturbations that introduce a new link to the network. In each case, we identify destabilizing perturbations of minimal norm and simulate their effects. Remarkably, we find that small but targeted alterations to network structure may lead to the radicalization of all agents, exhibiting the potential for large-scale shifts in collective behavior to be triggered by comparatively minuscule adjustments in social influence. Given that this method of identifying perturbations that are innocuous yet destabilizing applies to any suitable dynamical system, our findings emphasize a need for similar analyses to be carried out on real systems (e.g., real social networks), to identify the places where such dynamics may already exist.         ",
    "url": "https://arxiv.org/abs/2411.10868",
    "authors": [
      "Lane H. Rogers",
      "Emma J. Reid",
      "Robert A. Bridges"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Optimization and Control (math.OC)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2411.12556",
    "title": "UMGAD: Unsupervised Multiplex Graph Anomaly Detection",
    "abstract": "           Graph anomaly detection (GAD) is a critical task in graph machine learning, with the primary objective of identifying anomalous nodes that deviate significantly from the majority. This task is widely applied in various real-world scenarios, including fraud detection and social network analysis. However, existing GAD methods still face two major challenges: (1) They are often limited to detecting anomalies in single-type interaction graphs and struggle with multiple interaction types in multiplex heterogeneous graphs. (2) In unsupervised scenarios, selecting appropriate anomaly score thresholds remains a significant challenge for accurate anomaly detection. To address the above challenges, we propose a novel Unsupervised Multiplex Graph Anomaly Detection method, named UMGAD. We first learn multi-relational correlations among nodes in multiplex heterogeneous graphs and capture anomaly information during node attribute and structure reconstruction through graph-masked autoencoder (GMAE). Then, to further extract abnormal information, we generate attribute-level and subgraph-level augmented-view graphs respectively, and perform attribute and structure reconstruction through GMAE. Finally, we learn to optimize node attributes and structural features through contrastive learning between original-view and augmented-view graphs to improve the model's ability to capture anomalies. Meanwhile, we also propose a new anomaly score threshold selection strategy, which allows the model to be independent of ground truth information in real unsupervised scenarios. Extensive experiments on four datasets show that our UMGAD significantly outperforms state-of-the-art methods, achieving average improvements of 13.48% in AUC and 11.68% in Macro-F1 across all datasets.         ",
    "url": "https://arxiv.org/abs/2411.12556",
    "authors": [
      "Xiang Li",
      "Jianpeng Qi",
      "Zhongying Zhao",
      "Guanjie Zheng",
      "Lei Cao",
      "Junyu Dong",
      "Yanwei Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.14808",
    "title": "High-Resolution Image Synthesis via Next-Token Prediction",
    "abstract": "           Recently, autoregressive models have demonstrated remarkable performance in class-conditional image generation. However, the application of next-token prediction to high-resolution text-to-image generation remains largely unexplored. In this paper, we introduce \\textbf{D-JEPA$\\cdot$T2I}, an autoregressive model based on continuous tokens that incorporates innovations in both architecture and training strategy to generate high-quality, photorealistic images at arbitrary resolutions, up to 4K. Architecturally, we adopt the denoising joint embedding predictive architecture (D-JEPA) while leveraging a multimodal visual transformer to effectively integrate textual and visual features. Additionally, we introduce flow matching loss alongside the proposed Visual Rotary Positional Embedding (VoPE) to enable continuous resolution learning. In terms of training strategy, we propose a data feedback mechanism that dynamically adjusts the sampling procedure based on statistical analysis and an online learning critic model. This encourages the model to move beyond its comfort zone, reducing redundant training on well-mastered scenarios and compelling it to address more challenging cases with suboptimal generation quality. For the first time, we achieve state-of-the-art high-resolution image synthesis via next-token prediction.         ",
    "url": "https://arxiv.org/abs/2411.14808",
    "authors": [
      "Dengsheng Chen",
      "Jie Hu",
      "Tiezhu Yue",
      "Xiaoming Wei",
      "Enhua Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.19289",
    "title": "ADUGS-VINS: Generalized Visual-Inertial Odometry for Robust Navigation in Highly Dynamic and Complex Environments",
    "abstract": "           Visual-inertial odometry (VIO) is widely used in various fields, such as robots, drones, and autonomous vehicles. However, real-world scenes often feature dynamic objects, compromising the accuracy of VIO. The diversity and partial occlusion of these objects present a tough challenge for existing dynamic VIO methods. To tackle this challenge, we introduce ADUGS-VINS, which integrates an enhanced SORT algorithm along with a promptable foundation model into VIO, thereby improving pose estimation accuracy in environments with diverse dynamic objects and frequent occlusions. We evaluated our proposed method using multiple public datasets representing various scenes, as well as in a real-world scenario involving diverse dynamic objects. The experimental results demonstrate that our proposed method performs impressively in multiple scenarios, outperforming other state-of-the-art methods. This highlights its remarkable generalization and adaptability in diverse dynamic environments, showcasing its potential to handle various dynamic objects in practical applications.         ",
    "url": "https://arxiv.org/abs/2411.19289",
    "authors": [
      "Rui Zhou",
      "Jingbin Liu",
      "Junbin Xie",
      "Jianyu Zhang",
      "Yingze Hu",
      "Jiele Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.00537",
    "title": "Exact Certification of (Graph) Neural Networks Against Label Poisoning",
    "abstract": "           Machine learning models are highly vulnerable to label flipping, i.e., the adversarial modification (poisoning) of training labels to compromise performance. Thus, deriving robustness certificates is important to guarantee that test predictions remain unaffected and to understand worst-case robustness behavior. However, for Graph Neural Networks (GNNs), the problem of certifying label flipping has so far been unsolved. We change this by introducing an exact certification method, deriving both sample-wise and collective certificates. Our method leverages the Neural Tangent Kernel (NTK) to capture the training dynamics of wide networks enabling us to reformulate the bilevel optimization problem representing label flipping into a Mixed-Integer Linear Program (MILP). We apply our method to certify a broad range of GNN architectures in node classification tasks. Thereby, concerning the worst-case robustness to label flipping: $(i)$ we establish hierarchies of GNNs on different benchmark graphs; $(ii)$ quantify the effect of architectural choices such as activations, depth and skip-connections; and surprisingly, $(iii)$ uncover a novel phenomenon of the robustness plateauing for intermediate perturbation budgets across all investigated datasets and architectures. While we focus on GNNs, our certificates are applicable to sufficiently wide NNs in general through their NTK. Thus, our work presents the first exact certificate to a poisoning attack ever derived for neural networks, which could be of independent interest. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.00537",
    "authors": [
      "Mahalakshmi Sabanayagam",
      "Lukas Gosch",
      "Stephan G\u00fcnnemann",
      "Debarghya Ghoshdastidar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2412.02956",
    "title": "Curriculum-style Data Augmentation for LLM-based Metaphor Detection",
    "abstract": "           Recently, utilizing large language models (LLMs) for metaphor detection has achieved promising results. However, these methods heavily rely on the capabilities of closed-source LLMs, which come with relatively high inference costs and latency. To address this, we propose a method for metaphor detection by fine-tuning open-source LLMs, effectively reducing inference costs and latency with a single inference step. Furthermore, metaphor detection suffers from a severe data scarcity problem, which hinders effective fine-tuning of LLMs. To tackle this, we introduce Curriculum-style Data Augmentation (CDA). Specifically, before fine-tuning, we evaluate the training data to identify correctly predicted instances for fine-tuning, while incorrectly predicted instances are used as seed data for data augmentation. This approach enables the model to quickly learn simpler knowledge and progressively acquire more complex knowledge, thereby improving performance incrementally. Experimental results demonstrate that our method achieves state-of-the-art performance across all baselines. Additionally, we provide detailed ablation studies to validate the effectiveness of CDA.         ",
    "url": "https://arxiv.org/abs/2412.02956",
    "authors": [
      "Kaidi Jia",
      "Yanxia Wu",
      "Ming Liu",
      "Rongsheng Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2412.03173",
    "title": "IRisPath: Enhancing Costmap for Off-Road Navigation with Robust IR-RGB Fusion for Improved Day and Night Traversability",
    "abstract": "           Autonomous off-road navigation is required for applications in agriculture, construction, search and rescue and defence. Traditional on-road autonomous methods struggle with dynamic terrains, leading to poor vehicle control in off-road conditions. Recent deep-learning models have used perception sensors along with kinesthetic feedback for navigation on such terrains. However, this approach has out-of-domain uncertainty. Factors like change in time of day and weather impacts the performance of the model. We propose a multi modal fusion network \"IRisPath\" capable of using Thermal and RGB images to provide robustness against dynamic weather and light conditions. To aid further works in this domain, we also open-source a day-night dataset with Thermal and RGB images along with pseudo-labels for traversability. In order to co-register for fusion model we also develop a novel method for targetless extrinsic calibration of Thermal, LiDAR and RGB cameras with translation accuracy of +/-1.7cm and rotation accuracy of +/-0.827degrees.         ",
    "url": "https://arxiv.org/abs/2412.03173",
    "authors": [
      "Saksham Sharma",
      "Akshit Raizada",
      "Suresh Sundaram"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.05707",
    "title": "Segment-Level Road Obstacle Detection Using Visual Foundation Model Priors and Likelihood Ratios",
    "abstract": "           Detecting road obstacles is essential for autonomous vehicles to navigate dynamic and complex traffic environments safely. Current road obstacle detection methods typically assign a score to each pixel and apply a threshold to generate final predictions. However, selecting an appropriate threshold is challenging, and the per-pixel classification approach often leads to fragmented predictions with numerous false positives. In this work, we propose a novel method that leverages segment-level features from visual foundation models and likelihood ratios to predict road obstacles directly. By focusing on segments rather than individual pixels, our approach enhances detection accuracy, reduces false positives, and offers increased robustness to scene variability. We benchmark our approach against existing methods on the RoadObstacle and LostAndFound datasets, achieving state-of-the-art performance without needing a predefined threshold.         ",
    "url": "https://arxiv.org/abs/2412.05707",
    "authors": [
      "Youssef Shoeb",
      "Nazir Nayal",
      "Azarm Nowzad",
      "Fatma G\u00fcney",
      "Hanno Gottschalk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.07298",
    "title": "The Rise and Down of Babel Tower: Investigating the Evolution Process of Multilingual Code Large Language Model",
    "abstract": "           Large language models (LLMs) have shown significant multilingual capabilities. However, the mechanisms underlying the development of these capabilities during pre-training are not well understood. In this paper, we use code LLMs as an experimental platform to explore the evolution of multilingual capabilities in LLMs during the pre-training process. Based on our observations, we propose the Babel Tower Hypothesis, which describes the entire process of LLMs acquiring new language capabilities. During the learning process, multiple languages initially share a single knowledge system dominated by the primary language and gradually develop language-specific knowledge systems. We then validate the above hypothesis by tracking the internal states of the LLMs through identifying working languages and language transferring neurons. Experimental results show that the internal state changes of the LLM are consistent with our Babel Tower Hypothesis. Building on these insights, we propose a novel method to construct an optimized pre-training corpus for multilingual code LLMs, which significantly outperforms LLMs trained on the original corpus. The proposed Babel Tower Hypothesis provides new insights into designing pre-training data distributions to achieve optimal multilingual capabilities in LLMs.         ",
    "url": "https://arxiv.org/abs/2412.07298",
    "authors": [
      "Jiawei Chen",
      "Wentao Chen",
      "Jing Su",
      "Jingjing Xu",
      "Hongyu Lin",
      "Mengjie Ren",
      "Yaojie Lu",
      "Xianpei Han",
      "Le Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2412.07407",
    "title": "Towards Graph Foundation Models: A Study on the Generalization of Positional and Structural Encodings",
    "abstract": "           Recent advances in integrating positional and structural encodings (PSEs) into graph neural networks (GNNs) have significantly enhanced their performance across various graph learning tasks. However, the general applicability of these encodings and their potential to serve as foundational representations for graphs remain uncertain. This paper investigates the fine-tuning efficiency, scalability with sample size, and generalization capability of learnable PSEs across diverse graph datasets. Specifically, we evaluate their potential as universal pre-trained models that can be easily adapted to new tasks with minimal fine-tuning and limited data. Furthermore, we assess the expressivity of the learned representations, particularly, when used to augment downstream GNNs. We demonstrate through extensive benchmarking and empirical analysis that PSEs generally enhance downstream models. However, some datasets may require specific PSE-augmentations to achieve optimal performance. Nevertheless, our findings highlight their significant potential to become integral components of future graph foundation models. We provide new insights into the strengths and limitations of PSEs, contributing to the broader discourse on foundation models in graph learning.         ",
    "url": "https://arxiv.org/abs/2412.07407",
    "authors": [
      "Billy Joe Franks",
      "Moshe Eliasof",
      "Semih Cant\u00fcrk",
      "Guy Wolf",
      "Carola-Bibiane Sch\u00f6nlieb",
      "Sophie Fellenz",
      "Marius Kloft"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.12164",
    "title": "GAMED: Knowledge Adaptive Multi-Experts Decoupling for Multimodal Fake News Detection",
    "abstract": "           Multimodal fake news detection often involves modelling heterogeneous data sources, such as vision and language. Existing detection methods typically rely on fusion effectiveness and cross-modal consistency to model the content, complicating understanding how each modality affects prediction accuracy. Additionally, these methods are primarily based on static feature modelling, making it difficult to adapt to the dynamic changes and relationships between different data modalities. This paper develops a significantly novel approach, GAMED, for multimodal modelling, which focuses on generating distinctive and discriminative features through modal decoupling to enhance cross-modal synergies, thereby optimizing overall performance in the detection process. GAMED leverages multiple parallel expert networks to refine features and pre-embed semantic knowledge to improve the experts' ability in information selection and viewpoint sharing. Subsequently, the feature distribution of each modality is adaptively adjusted based on the respective experts' opinions. GAMED also introduces a novel classification technique to dynamically manage contributions from different modalities, while improving the explainability of decisions. Experimental results on the Fakeddit and Yang datasets demonstrate that GAMED performs better than recently developed state-of-the-art models. The source code can be accessed at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.12164",
    "authors": [
      "Lingzhi Shen",
      "Yunfei Long",
      "Xiaohao Cai",
      "Imran Razzak",
      "Guanming Chen",
      "Kang Liu",
      "Shoaib Jameel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.14663",
    "title": "IOHunter: Graph Foundation Model to Uncover Online Information Operations",
    "abstract": "           Social media platforms have become vital spaces for public discourse, serving as modern agor\u00e0s where a wide range of voices influence societal narratives. However, their open nature also makes them vulnerable to exploitation by malicious actors, including state-sponsored entities, who can conduct information operations (IOs) to manipulate public opinion. The spread of misinformation, false news, and misleading claims threatens democratic processes and societal cohesion, making it crucial to develop methods for the timely detection of inauthentic activity to protect the integrity of online discourse. In this work, we introduce a methodology designed to identify users orchestrating information operations, a.k.a. IO drivers, across various influence campaigns. Our framework, named IOHunter, leverages the combined strengths of Language Models and Graph Neural Networks to improve generalization in supervised, scarcely-supervised, and cross-IO contexts. Our approach achieves state-of-the-art performance across multiple sets of IOs originating from six countries, significantly surpassing existing approaches. This research marks a step toward developing Graph Foundation Models specifically tailored for the task of IO detection on social media platforms.         ",
    "url": "https://arxiv.org/abs/2412.14663",
    "authors": [
      "Marco Minici",
      "Luca Luceri",
      "Francesco Fabbri",
      "Emilio Ferrara"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.15598",
    "title": "Long-Term EEG Partitioning for Seizure Onset Detection",
    "abstract": "           Deep learning models have recently shown great success in classifying epileptic patients using EEG recordings. Unfortunately, classification-based methods lack a sound mechanism to detect the onset of seizure events. In this work, we propose a two-stage framework, SODor, that explicitly models seizure onset through a novel task formulation of subsequence clustering. Given an EEG sequence, the framework first learns a set of second-level embeddings with label supervision. It then employs model-based clustering to explicitly capture long-term temporal dependencies in EEG sequences and identify meaningful subsequences. Epochs within a subsequence share a common cluster assignment (normal or seizure), with cluster or state transitions representing successful onset detections. Extensive experiments on three datasets demonstrate that our method can correct misclassifications, achieving 5\\%-11\\% classification improvements over other baselines and accurately detecting seizure onsets.         ",
    "url": "https://arxiv.org/abs/2412.15598",
    "authors": [
      "Zheng Chen",
      "Yasuko Matsubara",
      "Yasushi Sakurai",
      "Jimeng Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2412.16577",
    "title": "A Meta-Learning Approach to Bayesian Causal Discovery",
    "abstract": "           Discovering a unique causal structure is difficult due to both inherent identifiability issues, and the consequences of finite data. As such, uncertainty over causal structures, such as those obtained from a Bayesian posterior, are often necessary for downstream tasks. Finding an accurate approximation to this posterior is challenging, due to the large number of possible causal graphs, as well as the difficulty in the subproblem of finding posteriors over the functional relationships of the causal edges. Recent works have used meta-learning to view the problem of estimating the maximum a-posteriori causal graph as supervised learning. Yet, these methods are limited when estimating the full posterior as they fail to encode key properties of the posterior, such as correlation between edges and permutation equivariance with respect to nodes. Further, these methods also cannot reliably sample from the posterior over causal structures. To address these limitations, we propose a Bayesian meta learning model that allows for sampling causal structures from the posterior and encodes these key properties. We compare our meta-Bayesian causal discovery against existing Bayesian causal discovery methods, demonstrating the advantages of directly learning a posterior over causal structure.         ",
    "url": "https://arxiv.org/abs/2412.16577",
    "authors": [
      "Anish Dhir",
      "Matthew Ashman",
      "James Requeima",
      "Mark van der Wilk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2412.16667",
    "title": "The Good, the Bad, and the (Un)Usable: A Rapid Literature Review on Privacy as Code",
    "abstract": "           Privacy and security are central to the design of information systems endowed with sound data protection and cyber resilience capabilities. Still, developers often struggle to incorporate these properties into software projects as they either lack proper cybersecurity training or do not consider them a priority. Prior work has tried to support privacy and security engineering activities through threat modeling methods for scrutinizing flaws in system architectures. Moreover, several techniques for the automatic identification of vulnerabilities and the generation of secure code implementations have also been proposed in the current literature. Conversely, such as-code approaches seem under-investigated in the privacy domain, with little work elaborating on (i) the automatic detection of privacy properties in source code or (ii) the generation of privacy-friendly code. In this work, we seek to characterize the current research landscape of Privacy as Code (PaC) methods and tools by conducting a rapid literature review. Our results suggest that PaC research is in its infancy, especially regarding the performance evaluation and usability assessment of the existing approaches. Based on these findings, we outline and discuss prospective research directions concerning empirical studies with software practitioners, the curation of benchmark datasets, and the role of generative AI technologies.         ",
    "url": "https://arxiv.org/abs/2412.16667",
    "authors": [
      "Nicol\u00e1s E. D\u00edaz Ferreyra",
      "Sirine Khelifi",
      "Nalin Arachchilage",
      "Riccardo Scandariato"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2412.16874",
    "title": "A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information",
    "abstract": "           Automatic detection and severity assessment of dysarthria are crucial for delivering targeted therapeutic interventions to patients. While most existing research focuses primarily on speech modality, this study introduces a novel approach that leverages both speech and text modalities. By employing cross-attention mechanism, our method learns the acoustic and linguistic similarities between speech and text representations. This approach assesses specifically the pronunciation deviations across different severity levels, thereby enhancing the accuracy of dysarthric detection and severity assessment. All the experiments have been performed using UA-Speech dysarthric database. Improved accuracies of 99.53% and 93.20% in detection, and 98.12% and 51.97% for severity assessment have been achieved when speaker-dependent and speaker-independent, unseen and seen words settings are used. These findings suggest that by integrating text information, which provides a reference linguistic knowledge, a more robust framework has been developed for dysarthric detection and assessment, thereby potentially leading to more effective diagnoses.         ",
    "url": "https://arxiv.org/abs/2412.16874",
    "authors": [
      "M Anuprabha",
      "Krishna Gurugubelli",
      "V Kesavaraj",
      "Anil Kumar Vuppala"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2501.01564",
    "title": "Semialgebraic Neural Networks: From roots to representations",
    "abstract": "           Many numerical algorithms in scientific computing -- particularly in areas like numerical linear algebra, PDE simulation, and inverse problems -- produce outputs that can be represented by semialgebraic functions; that is, the graph of the computed function can be described by finitely many polynomial equalities and inequalities. In this work, we introduce Semialgebraic Neural Networks (SANNs), a neural network architecture capable of representing any bounded semialgebraic function, and computing such functions up to the accuracy of a numerical ODE solver chosen by the programmer. Conceptually, we encode the graph of the learned function as the kernel of a piecewise polynomial selected from a class of functions whose roots can be evaluated using a particular homotopy continuation method. We show by construction that the SANN architecture is able to execute this continuation method, thus evaluating the learned semialgebraic function. Furthermore, the architecture can exactly represent even discontinuous semialgebraic functions by executing a continuation method on each connected component of the target function. Lastly, we provide example applications of these networks and show they can be trained with traditional deep-learning techniques.         ",
    "url": "https://arxiv.org/abs/2501.01564",
    "authors": [
      "S. David Mis",
      "Matti Lassas",
      "Maarten V. de Hoop"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2501.02020",
    "title": "Enhancing Uncertainty Modeling with Semantic Graph for Hallucination Detection",
    "abstract": "           Large Language Models (LLMs) are prone to hallucination with non-factual or unfaithful statements, which undermines the applications in real-world scenarios. Recent researches focus on uncertainty-based hallucination detection, which utilizes the output probability of LLMs for uncertainty calculation and does not rely on external knowledge or frequent sampling from LLMs. Whereas, most approaches merely consider the uncertainty of each independent token, while the intricate semantic relations among tokens and sentences are not well studied, which limits the detection of hallucination that spans over multiple tokens and sentences in the passage. In this paper, we propose a method to enhance uncertainty modeling with semantic graph for hallucination detection. Specifically, we first construct a semantic graph that well captures the relations among entity tokens and sentences. Then, we incorporate the relations between two entities for uncertainty propagation to enhance sentence-level hallucination detection. Given that hallucination occurs due to the conflict between sentences, we further present a graph-based uncertainty calibration method that integrates the contradiction probability of the sentence with its neighbors in the semantic graph for uncertainty calculation. Extensive experiments on two datasets show the great advantages of our proposed approach. In particular, we obtain substantial improvements with 19.78% in passage-level hallucination detection.         ",
    "url": "https://arxiv.org/abs/2501.02020",
    "authors": [
      "Kedi Chen",
      "Qin Chen",
      "Jie Zhou",
      "Xinqi Tao",
      "Bowen Ding",
      "Jingwen Xie",
      "Mingchen Xie",
      "Peilong Li",
      "Feng Zheng",
      "Liang He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.03775",
    "title": "Strip R-CNN: Large Strip Convolution for Remote Sensing Object Detection",
    "abstract": "           While witnessed with rapid development, remote sensing object detection remains challenging for detecting high aspect ratio objects. This paper shows that large strip convolutions are good feature representation learners for remote sensing object detection and can detect objects of various aspect ratios well. Based on large strip convolutions, we build a new network architecture called Strip R-CNN, which is simple, efficient, and powerful. Unlike recent remote sensing object detectors that leverage large-kernel convolutions with square shapes, our Strip R-CNN takes advantage of sequential orthogonal large strip convolutions in our backbone network StripNet to capture spatial information. In addition, we improve the localization capability of remote-sensing object detectors by decoupling the detection heads and equipping the localization branch with strip convolutions in our strip head. Extensive experiments on several benchmarks, for example DOTA, FAIR1M, HRSC2016, and DIOR, show that our Strip R-CNN can greatly improve previous work. In particular, our 30M model achieves 82.75% mAP on DOTA-v1.0, setting a new state-of-the-art record. Our code will be made publicly this http URL is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2501.03775",
    "authors": [
      "Xinbin Yuan",
      "Zhaohui Zheng",
      "Yuxuan Li",
      "Xialei Liu",
      "Li Liu",
      "Xiang Li",
      "Qibin Hou",
      "Ming-Ming Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.11069",
    "title": "Refinement Module based on Parse Graph of Feature Map for Human Pose Estimation",
    "abstract": "           Parse graphs of the human body can be obtained in the human brain to help humans complete the human Pose Estimation better (HPE). It contains a hierarchical structure, like a tree structure, and context relations among nodes. To equip models with such capabilities, many researchers predefine the parse graph of body structure to design HPE frameworks. However, these frameworks struggle to adapt to instances that deviate from the predefined parse graph and are often parameter-heavy. Unlike them, we view the feature map holistically, much like the human body. It can be optimized using parse graphs, where each node's feature is an implicit expression rather than a fixed one. This allows it to adapt to more instances, unconstrained by rigid structural features. In this paper, we design the Refinement Module based on the Parse Graph of feature map (RMPG), which includes two stages: top-down decomposition and bottom-up combination. In the first stage, the feature map is decomposed into multiple sub-feature maps along the channel. In the second stage, the context relations of sub-feature maps are calculated to obtain their respective context information and the sub-feature maps with context information are concatenated along channels to obtain the refined feature map. Additionally, we design a hierarchical network with fewer parameters using multiple RMPG modules to model the context relations and hierarchies in the parse graph of body structure for HPE, some of which are supervised to obtain context relations among body parts. Our network achieves excellent results on multiple mainstream human pose datasets. More importantly, the effectiveness of RMPG is proven on different methods. The code of RMPG will be open.         ",
    "url": "https://arxiv.org/abs/2501.11069",
    "authors": [
      "Shibang Liu",
      "Xuemei Xie",
      "Guangming Shi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.15394",
    "title": "Doracamom: Joint 3D Detection and Occupancy Prediction with Multi-view 4D Radars and Cameras for Omnidirectional Perception",
    "abstract": "           3D object detection and occupancy prediction are critical tasks in autonomous driving, attracting significant attention. Despite the potential of recent vision-based methods, they encounter challenges under adverse conditions. Thus, integrating cameras with next-generation 4D imaging radar to achieve unified multi-task perception is highly significant, though research in this domain remains limited. In this paper, we propose Doracamom, the first framework that fuses multi-view cameras and 4D radar for joint 3D object detection and semantic occupancy prediction, enabling comprehensive environmental perception. Specifically, we introduce a novel Coarse Voxel Queries Generator that integrates geometric priors from 4D radar with semantic features from images to initialize voxel queries, establishing a robust foundation for subsequent Transformer-based refinement. To leverage temporal information, we design a Dual-Branch Temporal Encoder that processes multi-modal temporal features in parallel across BEV and voxel spaces, enabling comprehensive spatio-temporal representation learning. Furthermore, we propose a Cross-Modal BEV-Voxel Fusion module that adaptively fuses complementary features through attention mechanisms while employing auxiliary tasks to enhance feature quality. Extensive experiments on the OmniHD-Scenes, View-of-Delft (VoD), and TJ4DRadSet datasets demonstrate that Doracamom achieves state-of-the-art performance in both tasks, establishing new benchmarks for multi-modal 3D perception. Code and models will be publicly available.         ",
    "url": "https://arxiv.org/abs/2501.15394",
    "authors": [
      "Lianqing Zheng",
      "Jianan Liu",
      "Runwei Guan",
      "Long Yang",
      "Shouyi Lu",
      "Yuanzhe Li",
      "Xiaokai Bai",
      "Jie Bai",
      "Zhixiong Ma",
      "Hui-Liang Shen",
      "Xichan Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.16981",
    "title": "Modulating CNN Features with Pre-Trained ViT Representations for Open-Vocabulary Object Detection",
    "abstract": "           Owing to large-scale image-text contrastive training, pre-trained vision language model (VLM) like CLIP shows superior open-vocabulary recognition ability. Most existing open-vocabulary object detectors attempt to utilize the pre-trained VLMs to attain generalized representation. F-ViT uses the pre-trained visual encoder as the backbone network and freezes it during training. However, its frozen backbone doesn't benefit from the labeled data to strengthen the representation for detection. Therefore, we propose a novel two-branch backbone network, named as \\textbf{V}iT-Feature-\\textbf{M}odulated Multi-Scale \\textbf{C}onvolutional Network (VMCNet), which consists of a trainable convolutional branch, a frozen pre-trained ViT branch and a VMC module. The trainable CNN branch could be optimized with labeled data while the frozen pre-trained ViT branch could keep the representation ability derived from large-scale pre-training. Then, the proposed VMC module could modulate the multi-scale CNN features with the representations from ViT branch. With this proposed mixed structure, the detector is more likely to discover objects of novel categories. Evaluated on two popular benchmarks, our method boosts the detection performance on novel category and outperforms state-of-the-art methods. On OV-COCO, the proposed method achieves 44.3 AP$_{50}^{\\mathrm{novel}}$ with ViT-B/16 and 48.5 AP$_{50}^{\\mathrm{novel}}$ with ViT-L/14. On OV-LVIS, VMCNet with ViT-B/16 and ViT-L/14 reaches 27.8 and 38.4 mAP$_{r}$.         ",
    "url": "https://arxiv.org/abs/2501.16981",
    "authors": [
      "Xiangyu Gao",
      "Yu Dai",
      "Benliu Qiu",
      "Lanxiao Wang",
      "Heqian Qiu",
      "Hongliang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.19069",
    "title": "Improving vision-language alignment with graph spiking hybrid Networks",
    "abstract": "           To bridge the semantic gap between vision and language (VL), it is necessary to develop a good alignment strategy, which includes handling semantic diversity, abstract representation of visual information, and generalization ability of models. Recent works use detector-based bounding boxes or patches with regular partitions to represent visual semantics. While current paradigms have made strides, they are still insufficient for fully capturing the nuanced contextual relations among various objects. This paper proposes a comprehensive visual semantic representation module, necessitating the utilization of panoptic segmentation to generate coherent fine-grained semantic features. Furthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that integrates the complementary advantages of Spiking Neural Networks (SNNs) and Graph Attention Networks (GATs) to encode visual semantic information. Intriguingly, the model not only encodes the discrete and continuous latent variables of instances but also adeptly captures both local and global contextual features, thereby significantly enhancing the richness and diversity of semantic representations. Leveraging the spatiotemporal properties inherent in SNNs, we employ contrastive learning (CL) to enhance the similarity-based representation of embeddings. This strategy alleviates the computational overhead of the model and enriches meaningful visual representations by constructing positive and negative sample pairs. We design an innovative pre-training method, Spiked Text Learning (STL), which uses text features to improve the encoding ability of discrete semantics. Experiments show that the proposed GSHN exhibits promising results on multiple VL downstream tasks.         ",
    "url": "https://arxiv.org/abs/2501.19069",
    "authors": [
      "Siyu Zhang",
      "Wenzhe Liu",
      "Yeming Chen",
      "Yiming Wu",
      "Heming Zheng",
      "Cheng Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.00156",
    "title": "ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition",
    "abstract": "           Bias in machine learning models can lead to unfair decision making, and while it has been well-studied in the image and text domains, it remains underexplored in action recognition. Action recognition models often suffer from background bias (i.e., inferring actions based on background cues) and foreground bias (i.e., relying on subject appearance), which can be detrimental to real-life applications such as autonomous vehicles or assisted living monitoring. While prior approaches have mainly focused on mitigating background bias using specialized augmentations, we thoroughly study both foreground and background bias. We propose ALBAR, a novel adversarial training method that mitigates foreground and background biases without requiring specialized knowledge of the bias attributes. Our framework applies an adversarial cross-entropy loss to the sampled static clip (where all the frames are the same) and aims to make its class probabilities uniform using a proposed entropy maximization loss. Additionally, we introduce a gradient penalty loss for regularization against the debiasing process. We evaluate our method on established background and foreground bias protocols, setting a new state-of-the-art and strongly improving combined debiasing performance by over 12% absolute on HMDB51. Furthermore, we identify an issue of background leakage in the existing UCF101 protocol for bias evaluation which provides a shortcut to predict actions and does not provide an accurate measure of the debiasing capability of a model. We address this issue by proposing more fine-grained segmentation boundaries for the actor, where our method also outperforms existing approaches. Project Page: this https URL ",
    "url": "https://arxiv.org/abs/2502.00156",
    "authors": [
      "Joseph Fioresi",
      "Ishan Rajendrakumar Dave",
      "Mubarak Shah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.01129",
    "title": "Deep Reinforcement Learning for Dynamic Resource Allocation in Wireless Networks",
    "abstract": "           This report investigates the application of deep reinforcement learning (DRL) algorithms for dynamic resource allocation in wireless communication systems. An environment that includes a base station, multiple antennas, and user equipment is created. Using the RLlib library, various DRL algorithms such as Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) are then applied. These algorithms are compared based on their ability to optimize resource allocation, focusing on the impact of different learning rates and scheduling policies. The findings demonstrate that the choice of algorithm and learning rate significantly influences system performance, with DRL providing more efficient resource allocation compared to traditional methods.         ",
    "url": "https://arxiv.org/abs/2502.01129",
    "authors": [
      "Shubham Malhotra",
      "Fnu Yashu",
      "Muhammad Saqib",
      "Dipkumar Mehta",
      "Jagdish Jangid",
      "Sachin Dixit"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.03391",
    "title": "Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise Sufficient Reasons",
    "abstract": "           *Minimal sufficient reasons* represent a prevalent form of explanation - the smallest subset of input features which, when held constant at their corresponding values, ensure that the prediction remains unchanged. Previous *post-hoc* methods attempt to obtain such explanations but face two main limitations: (1) Obtaining these subsets poses a computational challenge, leading most scalable methods to converge towards suboptimal, less meaningful subsets; (2) These methods heavily rely on sampling out-of-distribution input assignments, potentially resulting in counterintuitive behaviors. To tackle these limitations, we propose in this work a self-supervised training approach, which we term *sufficient subset training* (SST). Using SST, we train models to generate concise sufficient reasons for their predictions as an integral part of their output. Our results indicate that our framework produces succinct and faithful subsets substantially more efficiently than competing post-hoc methods, while maintaining comparable predictive performance.         ",
    "url": "https://arxiv.org/abs/2502.03391",
    "authors": [
      "Shahaf Bassan",
      "Ron Eliav",
      "Shlomit Gur"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2502.07176",
    "title": "MatrixKAN: Parallelized Kolmogorov-Arnold Network",
    "abstract": "           Kolmogorov-Arnold Networks (KAN) are a new class of neural network architecture representing a promising alternative to the Multilayer Perceptron (MLP), demonstrating improved expressiveness and interpretability. However, KANs suffer from slow training and inference speeds relative to MLPs due in part to the recursive nature of the underlying B-spline calculations. This issue is particularly apparent with respect to KANs utilizing high-degree B-splines, as the number of required non-parallelizable recursions is proportional to B-spline degree. We solve this issue by proposing MatrixKAN, a novel optimization that parallelizes B-spline calculations with matrix representation and operations, thus significantly improving effective computation time for models utilizing high-degree B-splines. In this paper, we demonstrate the superior scaling of MatrixKAN's computation time relative to B-spline degree. Further, our experiments demonstrate speedups of approximately 40x relative to KAN, with significant additional speedup potential for larger datasets or higher spline degrees.         ",
    "url": "https://arxiv.org/abs/2502.07176",
    "authors": [
      "Cale Coffman",
      "Lizhong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.08079",
    "title": "MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained Models",
    "abstract": "           Current adversarial attacks for evaluating the robustness of vision-language pre-trained (VLP) models in multi-modal tasks suffer from limited transferability, where attacks crafted for a specific model often struggle to generalize effectively across different models, limiting their utility in assessing robustness more broadly. This is mainly attributed to the over-reliance on model-specific features and regions, particularly in the image modality. In this paper, we propose an elegant yet highly effective method termed Meticulous Adversarial Attack (MAA) to fully exploit model-independent characteristics and vulnerabilities of individual samples, achieving enhanced generalizability and reduced model dependence. MAA emphasizes fine-grained optimization of adversarial images by developing a novel resizing and sliding crop (RScrop) technique, incorporating a multi-granularity similarity disruption (MGSD) strategy. Extensive experiments across diverse VLP models, multiple benchmark datasets, and a variety of downstream tasks demonstrate that MAA significantly enhances the effectiveness and transferability of adversarial attacks. A large cohort of performance studies is conducted to generate insights into the effectiveness of various model configurations, guiding future advancements in this domain.         ",
    "url": "https://arxiv.org/abs/2502.08079",
    "authors": [
      "Peng-Fei Zhang",
      "Guangdong Bai",
      "Zi Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.08332",
    "title": "Modification and Generated-Text Detection: Achieving Dual Detection Capabilities for the Outputs of LLM by Watermark",
    "abstract": "           The development of large language models (LLMs) has raised concerns about potential misuse. One practical solution is to embed a watermark in the text, allowing ownership verification through watermark extraction. Existing methods primarily focus on defending against modification attacks, often neglecting other spoofing attacks. For example, attackers can alter the watermarked text to produce harmful content without compromising the presence of the watermark, which could lead to false attribution of this malicious content to the LLM. This situation poses a serious threat to the LLMs service providers and highlights the significance of achieving modification detection and generated-text detection simultaneously. Therefore, we propose a technique to detect modifications in text for unbiased watermark which is sensitive to modification. We introduce a new metric called ``discarded tokens\", which measures the number of tokens not included in watermark detection. When a modification occurs, this metric changes and can serve as evidence of the modification. Additionally, we improve the watermark detection process and introduce a novel method for unbiased watermark. Our experiments demonstrate that we can achieve effective dual detection capabilities: modification detection and generated-text detection by watermark.         ",
    "url": "https://arxiv.org/abs/2502.08332",
    "authors": [
      "Yuhang Cai",
      "Yaofei Wang",
      "Donghui Hu",
      "Chen Gu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.10122",
    "title": "Modern Hopfield Networks with Continuous-Time Memories",
    "abstract": "           Recent research has established a connection between modern Hopfield networks (HNs) and transformer attention heads, with guarantees of exponential storage capacity. However, these models still face challenges scaling storage efficiently. Inspired by psychological theories of continuous neural resource allocation in working memory, we propose an approach that compresses large discrete Hopfield memories into smaller, continuous-time memories. Leveraging continuous attention, our new energy function modifies the update rule of HNs, replacing the traditional softmax-based probability mass function with a probability density, over the continuous memory. This formulation aligns with modern perspectives on human executive function, offering a principled link between attractor dynamics in working memory and resource-efficient memory allocation. Our framework maintains competitive performance with HNs while leveraging a compressed memory, reducing computational costs across synthetic and video datasets.         ",
    "url": "https://arxiv.org/abs/2502.10122",
    "authors": [
      "Saul Santos",
      "Ant\u00f3nio Farinhas",
      "Daniel C. McNamee",
      "Andr\u00e9 F.T. Martins"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.10603",
    "title": "Adaptive Neural Networks for Intelligent Data-Driven Development",
    "abstract": "           Advances in machine learning methods for computer vision tasks have led to their consideration for safety-critical applications like autonomous driving. However, effectively integrating these methods into the automotive development lifecycle remains challenging. Since the performance of machine learning algorithms relies heavily on the training data provided, the data and model development lifecycle play a key role in successfully integrating these components into the product development lifecycle. Existing models frequently encounter difficulties recognizing or adapting to novel instances not present in the original training dataset. This poses a significant risk for reliable deployment in dynamic environments. To address this challenge, we propose an adaptive neural network architecture and an iterative development framework that enables users to efficiently incorporate previously unknown objects into the current perception system. Our approach builds on continuous learning, emphasizing the necessity of dynamic updates to reflect real-world deployment conditions. Specifically, we introduce a pipeline with three key components: (1) a scalable network extension strategy to integrate new classes while preserving existing performance, (2) a dynamic OoD detection component that requires no additional retraining for newly added classes, and (3) a retrieval-based data augmentation process tailored for safety-critical deployments. The integration of these components establishes a pragmatic and adaptive pipeline for the continuous evolution of perception systems in the context of autonomous driving.         ",
    "url": "https://arxiv.org/abs/2502.10603",
    "authors": [
      "Youssef Shoeb",
      "Azarm Nowzad",
      "Hanno Gottschalk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.11167",
    "title": "SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors",
    "abstract": "           Neural surrogate models have emerged as powerful and efficient tools in data mining. Meanwhile, large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks. We investigate a novel application: using LLMs as surrogate models for code execution prediction. Given LLMs' unique ability to understand and process diverse programs, they present a promising direction for building general-purpose surrogate models. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark with $1160$ problems covering $8$ key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. Through extensive empirical analysis of $21$ open-source and proprietary LLMs, we examine scaling laws, data efficiency, and predictive accuracy. Our findings reveal important insights about the feasibility of LLMs as efficient surrogates for computational processes, with implications for automated software testing, program analysis, and computational resource optimization in data mining applications. Code and dataset are released at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.11167",
    "authors": [
      "Bohan Lyu",
      "Siqiao Huang",
      "Zichen Liang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.11299",
    "title": "Grassroots Platforms with Atomic Transactions: Social Networks, Cryptocurrencies, and Democratic Federations",
    "abstract": "           Grassroots platforms aim to offer an egalitarian alternative to global platforms -- centralized/autocratic (Facebook etc.) and decentralized/plutocratic (Bitcoin etc.) alike. Key grassroots platforms include grassroots social networks, grassroots cryptocurrencies, and grassroots democratic federations. Previously, grassroots platforms were defined formally and proven grassroots using unary distributed transition systems, in which each transition is carried out by a single agent. However, grassroots platforms cater for a more abstract specification using transactions carried out atomically by multiple agents, something that cannot be expressed by unary transition systems. As a result, their original specifications and proofs were unnecessarily cumbersome and opaque. Here, we aim to provide a more suitable formal foundation for grassroots platforms. To do so, we enhance the notion of a distributed transition system to include atomic transactions and revisit the notion of grassroots platforms within this new foundation. We present crisp specifications of key grassroots platforms using atomic transactions: befriending and defriending for grassroots social networks, coin swaps for grassroots cryptocurrencies, and communities forming, joining, and leaving a federation for grassroots democratic federations. We prove a general theorem that a platform specified by atomic transactions that are so-called interactive is grassroots; show that the atomic transactions used to specify all three platforms are interactive; and conclude that the platforms thus specified are indeed grassroots. We thus provide a better mathematical foundation for grassroots platforms and a solid and clear starting point from which their implementation can commence.         ",
    "url": "https://arxiv.org/abs/2502.11299",
    "authors": [
      "Ehud Shapiro"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2502.11858",
    "title": "Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives",
    "abstract": "           While audio-visual learning equips models with a richer understanding of the real world by leveraging multiple sensory modalities, this integration also introduces new vulnerabilities to adversarial attacks. In this paper, we present a comprehensive study of the adversarial robustness of audio-visual models, considering both temporal and modality-specific vulnerabilities. We propose two powerful adversarial attacks: 1) a temporal invariance attack that exploits the inherent temporal redundancy across consecutive time segments and 2) a modality misalignment attack that introduces incongruence between the audio and visual modalities. These attacks are designed to thoroughly assess the robustness of audio-visual models against diverse threats. Furthermore, to defend against such attacks, we introduce a novel audio-visual adversarial training framework. This framework addresses key challenges in vanilla adversarial training by incorporating efficient adversarial perturbation crafting tailored to multi-modal data and an adversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds dataset demonstrate that our proposed temporal and modality-based attacks in degrading model performance can achieve state-of-the-art performance, while our adversarial training defense largely improves the adversarial robustness as well as the adversarial training efficiency.         ",
    "url": "https://arxiv.org/abs/2502.11858",
    "authors": [
      "Zeliang Zhang",
      "Susan Liang",
      "Daiki Shimada",
      "Chenliang Xu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.12361",
    "title": "ConFit v2: Improving Resume-Job Matching using Hypothetical Resume Embedding and Runner-Up Hard-Negative Mining",
    "abstract": "           A reliable resume-job matching system helps a company recommend suitable candidates from a pool of resumes and helps a job seeker find relevant jobs from a list of job posts. However, since job seekers apply only to a few jobs, interaction labels in resume-job datasets are sparse. We introduce ConFit v2, an improvement over ConFit to tackle this sparsity problem. We propose two techniques to enhance the encoder's contrastive training process: augmenting job data with hypothetical reference resume generated by a large language model; and creating high-quality hard negatives from unlabeled resume/job pairs using a novel hard-negative mining strategy. We evaluate ConFit v2 on two real-world datasets and demonstrate that it outperforms ConFit and prior methods (including BM25 and OpenAI text-embedding-003), achieving an average absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking and resume-ranking tasks.         ",
    "url": "https://arxiv.org/abs/2502.12361",
    "authors": [
      "Xiao Yu",
      "Ruize Xu",
      "Chengyuan Xue",
      "Jinzhong Zhang",
      "Xu Ma",
      "Zhou Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.12381",
    "title": "Linear Diffusion Networks",
    "abstract": "           Diffusion kernels capture global dependencies. We present Linear Diffusion Networks (LDNs), a novel architecture that reinterprets sequential data processing as a unified diffusion process. Our model integrates adaptive diffusion modules with localized nonlinear updates and a diffusion-inspired attention mechanism. This design enables efficient global information propagation while preserving fine-grained temporal details. LDN overcomes the limitations of conventional recurrent and transformer models by allowing full parallelization across time steps and supporting robust multi-scale temporal representations. Experiments on benchmark sequence modeling tasks demonstrate that LDN delivers competitive performance across ImageNet and GLUE tasks.         ",
    "url": "https://arxiv.org/abs/2502.12381",
    "authors": [
      "Jacob Fein-Ashley"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.14309",
    "title": "On Theoretical Limits of Learning with Label Differential Privacy",
    "abstract": "           Label differential privacy (DP) is designed for learning problems involving private labels and public features. While various methods have been proposed for learning under label DP, the theoretical limits remain largely unexplored. In this paper, we investigate the fundamental limits of learning with label DP in both local and central models for both classification and regression tasks, characterized by minimax convergence rates. We establish lower bounds by converting each task into a multiple hypothesis testing problem and bounding the test error. Additionally, we develop algorithms that yield matching upper bounds. Our results demonstrate that under label local DP (LDP), the risk has a significantly faster convergence rate than that under full LDP, i.e. protecting both features and labels, indicating the advantages of relaxing the DP definition to focus solely on labels. In contrast, under the label central DP (CDP), the risk is only reduced by a constant factor compared to full DP, indicating that the relaxation of CDP only has limited benefits on the performance.         ",
    "url": "https://arxiv.org/abs/2502.14309",
    "authors": [
      "Puning Zhao",
      "Chuan Ma",
      "Li Shen",
      "Shaowei Wang",
      "Rongfei Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2502.18176",
    "title": "CLIPure: Purification in Latent Space via CLIP for Adversarially Robust Zero-Shot Classification",
    "abstract": "           In this paper, we aim to build an adversarially robust zero-shot image classifier. We ground our work on CLIP, a vision-language pre-trained encoder model that can perform zero-shot classification by matching an image with text prompts ``a photo of a <class-name>.''. Purification is the path we choose since it does not require adversarial training on specific attack types and thus can cope with any foreseen attacks. We then formulate purification risk as the KL divergence between the joint distributions of the purification process of denoising the adversarial samples and the attack process of adding perturbations to benign samples, through bidirectional Stochastic Differential Equations (SDEs). The final derived results inspire us to explore purification in the multi-modal latent space of CLIP. We propose two variants for our CLIPure approach: CLIPure-Diff which models the likelihood of images' latent vectors with the DiffusionPrior module in DaLLE-2 (modeling the generation process of CLIP's latent vectors), and CLIPure-Cos which models the likelihood with the cosine similarity between the embeddings of an image and ``a photo of a.''. As far as we know, CLIPure is the first purification method in multi-modal latent space and CLIPure-Cos is the first purification method that is not based on generative models, which substantially improves defense efficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13 datasets that previous CLIP-based defense methods used for evaluating zero-shot classification robustness. Results show that CLIPure boosts the SOTA robustness by a large margin, e.g., from 71.7% to 91.1% on CIFAR10, from 59.6% to 72.6% on ImageNet, and 108% relative improvements of average robustness on the 13 datasets over previous SOTA. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.18176",
    "authors": [
      "Mingkun Zhang",
      "Keping Bi",
      "Wei Chen",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18696",
    "title": "Interpretable Data-Driven Ship Dynamics Model: Enhancing Physics-Based Motion Prediction with Parameter Optimization",
    "abstract": "           The deployment of autonomous navigation systems on ships necessitates accurate motion prediction models tailored to individual vessels. Traditional physics-based models, while grounded in hydrodynamic principles, often fail to account for ship-specific behaviors under real-world conditions. Conversely, purely data-driven models offer specificity but lack interpretability and robustness in edge cases. This study proposes a data-driven physics-based model that integrates physics-based equations with data-driven parameter optimization, leveraging the strengths of both approaches to ensure interpretability and adaptability. The model incorporates physics-based components such as 3-DoF dynamics, rudder, and propeller forces, while parameters such as resistance curve and rudder coefficients are optimized using synthetic data. By embedding domain knowledge into the parameter optimization process, the fitted model maintains physical consistency. Validation of the approach is realized with two container ships by comparing, both qualitatively and quantitatively, predictions against ground-truth trajectories. The results demonstrate significant improvements, in predictive accuracy and reliability, of the data-driven physics-based models over baseline physics-based models tuned with traditional marine engineering practices. The fitted models capture ship-specific behaviors in diverse conditions with their predictions being, 51.6% (ship A) and 57.8% (ship B) more accurate, 72.36% (ship A) and 89.67% (ship B) more consistent.         ",
    "url": "https://arxiv.org/abs/2502.18696",
    "authors": [
      "Christos Papandreou",
      "Michail Mathioudakis",
      "Theodoros Stouraitis",
      "Petros Iatropoulos",
      "Antonios Nikitakis",
      "Stavros Paschalakis",
      "Konstantinos Kyriakopoulos"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.18874",
    "title": "Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework",
    "abstract": "           Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.         ",
    "url": "https://arxiv.org/abs/2502.18874",
    "authors": [
      "Kaishuai Xu",
      "Tiezheng Yu",
      "Wenjun Hou",
      "Yi Cheng",
      "Liangyou Li",
      "Xin Jiang",
      "Lifeng Shang",
      "Qun Liu",
      "Wenjie Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18960",
    "title": "Nonparametric Heterogeneous Long-term Causal Effect Estimation via Data Combination",
    "abstract": "           Long-term causal inference has drawn increasing attention in many scientific domains. Existing methods mainly focus on estimating average long-term causal effects by combining long-term observational data and short-term experimental data. However, it is still understudied how to robustly and effectively estimate heterogeneous long-term causal effects, significantly limiting practical applications. In this paper, we propose several two-stage style nonparametric estimators for heterogeneous long-term causal effect estimation, including propensity-based, regression-based, and multiple robust estimators. We conduct a comprehensive theoretical analysis of their asymptotic properties under mild assumptions, with the ultimate goal of building a better understanding of the conditions under which some estimators can be expected to perform better. Extensive experiments across several semi-synthetic and real-world datasets validate the theoretical results and demonstrate the effectiveness of the proposed estimators.         ",
    "url": "https://arxiv.org/abs/2502.18960",
    "authors": [
      "Weilin Chen",
      "Ruichu Cai",
      "Junjie Wan",
      "Zeqin Yang",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.19047",
    "title": "A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification in Diffusion Models",
    "abstract": "           Diffusion models have emerged as state-of-the-art generative frameworks, excelling in producing high-quality multi-modal samples. However, recent studies have revealed their vulnerability to backdoor attacks, where backdoored models generate specific, undesirable outputs called backdoor target (e.g., harmful images) when a pre-defined trigger is embedded to their inputs. In this paper, we propose PureDiffusion, a dual-purpose framework that simultaneously serves two contrasting roles: backdoor defense and backdoor attack amplification. For defense, we introduce two novel loss functions to invert backdoor triggers embedded in diffusion models. The first leverages trigger-induced distribution shifts across multiple timesteps of the diffusion process, while the second exploits the denoising consistency effect when a backdoor is activated. Once an accurate trigger inversion is achieved, we develop a backdoor detection method that analyzes both the inverted trigger and the generated backdoor targets to identify backdoor attacks. In terms of attack amplification with the role of an attacker, we describe how our trigger inversion algorithm can be used to reinforce the original trigger embedded in the backdoored diffusion model. This significantly boosts attack performance while reducing the required backdoor training time. Experimental results demonstrate that PureDiffusion achieves near-perfect detection accuracy, outperforming existing defenses by a large margin, particularly against complex trigger patterns. Additionally, in an attack scenario, our attack amplification approach elevates the attack success rate (ASR) of existing backdoor attacks to nearly 100\\% while reducing training time by up to 20x.         ",
    "url": "https://arxiv.org/abs/2502.19047",
    "authors": [
      "Vu Tuan Truong",
      "Long Bao Le"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19082",
    "title": "Trust-Enabled Privacy: Social Media Designs to Support Adolescent User Boundary Regulation",
    "abstract": "           Through a three-part co-design study involving 19 teens aged 13-18, we identify key barriers to effective boundary regulation on social media, including ambiguous audience expectations, social risks associated with oversharing, and the lack of design affordances that facilitate trust-building. Our findings reveal that while adolescents seek casual, frequent sharing to strengthen relationships, existing platform norms and designs often discourage such interactions, leading to withdrawal. To address these challenges, we introduce trust-enabled privacy as a design framework that recognizes trust, whether building or eroding, as central to boundary regulation. When trust is supported, boundary regulation becomes more adaptive and empowering; when it erodes, users default to self-censorship or withdrawal. We propose concrete design affordances, including guided disclosure, contextual audience segmentation, intentional engagement signaling, and trust-centered norms, to help platforms foster a more dynamic and nuanced privacy experience for teen social media users. By reframing privacy as a trust-driven process rather than a rigid control-based trade-off, this work provides empirical insights and actionable guidelines for designing social media environments that empower teens to manage their online presence while fostering meaningful social connections.         ",
    "url": "https://arxiv.org/abs/2502.19082",
    "authors": [
      "JaeWon Kim",
      "Robert Wolfe",
      "Ramya Bhagirathi Subramanian",
      "Mei-Hsuan Lee",
      "Jessica Colnago",
      "Alexis Hiniker"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2502.19412",
    "title": "The Mighty ToRR: A Benchmark for Table Reasoning and Robustness",
    "abstract": "           Despite its real-world significance, model performance on tabular data remains underexplored, leaving uncertainty about which model to rely on and which prompt configuration to adopt. To address this gap, we create ToRR, a benchmark for Table Reasoning and Robustness, measuring model performance and robustness on table-related tasks. The benchmark includes 10 datasets that cover different types of table reasoning capabilities across varied domains. ToRR goes beyond model performance rankings, and is designed to reflect whether models can handle tabular data consistently and robustly, across a variety of common table representation formats. We present a leaderboard as well as comprehensive analyses of the results of leading models over ToRR. Our results reveal a striking pattern of brittle model behavior, where even strong models are unable to perform robustly on tabular data tasks. Although no specific table format leads to consistently better performance, we show that testing over multiple formats is crucial for reliably estimating model capabilities. Moreover, we show that the reliability boost from testing multiple prompts can be equivalent to adding more test examples. Overall, our findings show that table understanding and reasoning tasks remain a significant challenge.         ",
    "url": "https://arxiv.org/abs/2502.19412",
    "authors": [
      "Shir Ashury-Tahan",
      "Yifan Mai",
      "Rajmohan C",
      "Ariel Gera",
      "Yotam Perlitz",
      "Asaf Yehudai",
      "Elron Bandel",
      "Leshem Choshen",
      "Eyal Shnarch",
      "Percy Liang",
      "Michal Shmueli-Scheuer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.19649",
    "title": "Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models",
    "abstract": "           Representation Engineering (RepE) is a novel paradigm for controlling the behavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune the model, RepE directly manipulates the model's internal representations. As a result, it may offer more effective, interpretable, data-efficient, and flexible control over models' behavior. We present the first comprehensive survey of RepE for LLMs, reviewing the rapidly growing literature to address key questions: What RepE methods exist and how do they differ? For what concepts and problems has RepE been applied? What are the strengths and weaknesses of RepE compared to other methods? To answer these, we propose a unified framework describing RepE as a pipeline comprising representation identification, operationalization, and control. We posit that while RepE methods offer significant potential, challenges remain, including managing multiple concepts, ensuring reliability, and preserving models' performance. Towards improving RepE, we identify opportunities for experimental and methodological improvements and construct a guide for best practices.         ",
    "url": "https://arxiv.org/abs/2502.19649",
    "authors": [
      "Jan Wehner",
      "Sahar Abdelnabi",
      "Daniel Tan",
      "David Krueger",
      "Mario Fritz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.19842",
    "title": "CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation",
    "abstract": "           Contrastive Language-Image Pre-training (CLIP) models excel in zero-shot classification, yet face challenges in complex multi-object scenarios. This study offers a comprehensive analysis of CLIP's limitations in these contexts using a specialized dataset, ComCO, designed to evaluate CLIP's encoders in diverse multi-object scenarios. Our findings reveal significant biases: the text encoder prioritizes first-mentioned objects, and the image encoder favors larger objects. Through retrieval and classification tasks, we quantify these biases across multiple CLIP variants and trace their origins to CLIP's training process, supported by analyses of the LAION dataset and training progression. Our image-text matching experiments show substantial performance drops when object size or token order changes, underscoring CLIP's instability with rephrased but semantically similar captions. Extending this to longer captions and text-to-image models like Stable Diffusion, we demonstrate how prompt order influences object prominence in generated images. For more details and access to our dataset and analysis code, visit our project repository: this https URL.         ",
    "url": "https://arxiv.org/abs/2502.19842",
    "authors": [
      "Reza Abbasi",
      "Ali Nazari",
      "Aminreza Sefid",
      "Mohammadali Banayeeanzade",
      "Mohammad Hossein Rohban",
      "Mahdieh Soleymani Baghshah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.20041",
    "title": "3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds",
    "abstract": "           3D Affordance detection is a challenging problem with broad applications on various robotic tasks. Existing methods typically formulate the detection paradigm as a label-based semantic segmentation task. This paradigm relies on predefined labels and lacks the ability to comprehend complex natural language, resulting in limited generalization in open-world scene. To address these limitations, we reformulate the traditional affordance detection paradigm into \\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This task is designed to output a affordance mask region given a query reasoning text, which avoids fixed categories of input labels. We accordingly propose the \\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning affordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces large language models (LLMs) to 3D affordance perception with a custom-designed decoder for generating affordance masks, thus achieving open-world reasoning affordance detection. In addition, given the scarcity of 3D affordance datasets for training large models, we seek to extract knowledge from general segmentation data and transfer it to affordance detection. Thus, we propose a multi-stage training strategy that begins with a novel pre-training task, i.e., \\textit{Referring Object Part Segmentation}~(ROPS). This stage is designed to equip the model with general recognition and segmentation capabilities at the object-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLM obtains the reasoning ability for affordance detection. In summary, 3D-ADLLM leverages the rich world knowledge and human-object interaction reasoning ability of LLMs, achieving approximately an 8\\% improvement in mIoU on open-vocabulary affordance detection tasks.         ",
    "url": "https://arxiv.org/abs/2502.20041",
    "authors": [
      "Hengshuo Chu",
      "Xiang Deng",
      "Qi Lv",
      "Xiaoyang Chen",
      "Yinchuan Li",
      "Jianye Hao",
      "Liqiang Nie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.20212",
    "title": "Learning Hamiltonian Systems with Pseudo-symplectic Neural Network",
    "abstract": "           In this paper, we introduces a Pseudo-Symplectic Neural Network (PSNN) for learning general Hamiltonian systems (both separable and non-separable) from data. To address the limitations of existing structure-preserving methods (e.g., implicit symplectic integrators restricted to separable systems or explicit approximations requiring high computational costs), PSNN integrates an explicit pseudo-symplectic integrator as its dynamical core, achieving nearly exact symplecticity with minimal structural error. Additionally, the authors propose learnable Pad\u00e9-type activation functions based on Pad\u00e9 approximation theory, which empirically outperform classical ReLU, Taylor-based activations, and PAU. By combining these innovations, PSNN demonstrates superior performance in learning and forecasting diverse Hamiltonian systems (e.g., modified pendulum, galactic dynamics), surpassing state-of-the-art models in accuracy, long-term stability, and energy preservation, while requiring shorter training time, fewer samples, and reduced parameters. This framework bridges the gap between computational efficiency and geometric structure preservation in Hamiltonian system modeling.         ",
    "url": "https://arxiv.org/abs/2502.20212",
    "authors": [
      "Xupeng Cheng",
      "Lijin Wang",
      "Yanzhao Cao",
      "Chen Chen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2502.20317",
    "title": "Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases",
    "abstract": "           Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.20317",
    "authors": [
      "Yongjia Lei",
      "Haoyu Han",
      "Ryan A. Rossi",
      "Franck Dernoncourt",
      "Nedim Lipka",
      "Mahantesh M Halappanavar",
      "Jiliang Tang",
      "Yu Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2502.20854",
    "title": "A Pilot Empirical Study on When and How to Use Knowledge Graphs as Retrieval Augmented Generation",
    "abstract": "           The integration of Knowledge Graphs (KGs) into the Retrieval Augmented Generation (RAG) framework has attracted significant interest, with early studies showing promise in mitigating hallucinations and improving model accuracy. However, a systematic understanding and comparative analysis of the rapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the foundation for systematically answering the question of when and how to use KG-RAG by analyzing their performance in various application scenarios associated with different technical configurations. After outlining the mind map using KG-RAG framework and summarizing its popular pipeline, we conduct a pilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG methods across 7 datasets in diverse scenarios, analyzing the impact of 9 KG-RAG configurations in combination with 17 LLMs. Our results underscore the critical role of appropriate application conditions and optimal configurations of KG-RAG components.         ",
    "url": "https://arxiv.org/abs/2502.20854",
    "authors": [
      "Xujie Yuan",
      "Yongxu Liu",
      "Shimin Di",
      "Shiwen Wu",
      "Libin Zheng",
      "Rui Meng",
      "Lei Chen",
      "Xiaofang Zhou",
      "Jian Yin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.21026",
    "title": "Artemis: Toward Accurate Detection of Server-Side Request Forgeries through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis",
    "abstract": "           Server-side request forgery (SSRF) vulnerabilities are inevitable in PHP web applications. Existing static tools in detecting vulnerabilities in PHP web applications neither contain SSRF-related features to enhance detection accuracy nor consider PHP's dynamic type features. In this paper, we present Artemis, a static taint analysis tool for detecting SSRF vulnerabilities in PHP web applications. First, Artemis extracts both PHP built-in and third-party functions as candidate source and sink functions. Second, Artemis constructs both explicit and implicit call graphs to infer functions' relationships. Third, Artemis performs taint analysis based on a set of rules that prevent over-tainting and pauses when SSRF exploitation is impossible. Fourth, Artemis analyzes the compatibility of path conditions to prune false positives. We have implemented a prototype of Artemis and evaluated it on 250 PHP web applications. Artemis reports 207 true vulnerable paths (106 true SSRFs) with 15 false positives. Of the 106 detected SSRFs, 35 are newly found and reported to developers, with 24 confirmed and assigned CVE IDs.         ",
    "url": "https://arxiv.org/abs/2502.21026",
    "authors": [
      "Yuchen Ji",
      "Ting Dai",
      "Zhichao Zhou",
      "Yutian Tang",
      "Jingzhu He"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.21123",
    "title": "Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models",
    "abstract": "           Ensuring trustworthiness in machine learning (ML) systems is crucial as they become increasingly embedded in high-stakes domains. This paper advocates for integrating causal methods into machine learning to navigate the trade-offs among key principles of trustworthy ML, including fairness, privacy, robustness, accuracy, and explainability. While these objectives should ideally be satisfied simultaneously, they are often addressed in isolation, leading to conflicts and suboptimal solutions. Drawing on existing applications of causality in ML that successfully align goals such as fairness and accuracy or privacy and robustness, this paper argues that a causal approach is essential for balancing multiple competing objectives in both trustworthy ML and foundation models. Beyond highlighting these trade-offs, we examine how causality can be practically integrated into ML and foundation models, offering solutions to enhance their reliability and interpretability. Finally, we discuss the challenges, limitations, and opportunities in adopting causal frameworks, paving the way for more accountable and ethically sound AI systems.         ",
    "url": "https://arxiv.org/abs/2502.21123",
    "authors": [
      "Ruta Binkyte",
      "Ivaxi Sheth",
      "Zhijing Jin",
      "Mohammad Havaei",
      "Bernhard Sch\u00f6lkopf",
      "Mario Fritz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02581",
    "title": "Online Estimation and Inference for Robust Policy Evaluation in Reinforcement Learning",
    "abstract": "           Reinforcement learning has emerged as one of the prominent topics attracting attention in modern statistical learning, with policy evaluation being a key component. Unlike the traditional machine learning literature on this topic, our work emphasizes statistical inference for the model parameters and value functions of reinforcement learning algorithms. While most existing analyses assume random rewards to follow standard distributions, we embrace the concept of robust statistics in reinforcement learning by simultaneously addressing issues of outlier contamination and heavy-tailed rewards within a unified framework. In this paper, we develop a fully online robust policy evaluation procedure, and establish the Bahadur-type representation of our estimator. Furthermore, we develop an online procedure to efficiently conduct statistical inference based on the asymptotic distribution. This paper connects robust statistics and statistical inference in reinforcement learning, offering a more versatile and reliable approach to online policy evaluation. Finally, we validate the efficacy of our algorithm through numerical experiments conducted in simulations and real-world reinforcement learning experiments.         ",
    "url": "https://arxiv.org/abs/2310.02581",
    "authors": [
      "Weidong Liu",
      "Jiyuan Tu",
      "Xi Chen",
      "Yichen Zhang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.13810",
    "title": "Bridging Classical and Quantum Machine Learning: Knowledge Transfer From Classical to Quantum Neural Networks Using Knowledge Distillation",
    "abstract": "           Quantum neural networks (QNNs), harnessing superposition and entanglement, have shown potential to surpass classical methods in complex learning tasks but remain limited by hardware constraints and noisy conditions. In this work, we present a novel framework for transferring knowledge from classical convolutional neural networks (CNNs) to QNNs via knowledge distillation, thereby reducing the need for resource intensive quantum training and error mitigation. We conduct extensive experiments using two parameterized quantum circuits (PQCs) with 4 and 8 qubits on MNIST, Fashion MNIST, and CIFAR10 datasets. The approach demonstrates consistent accuracy improvements attributed to distilled knowledge from larger classical networks. Through ablation studies, we systematically compare the effect of state of the art dimensionality reduction techniques fully connected layers, center cropping, principal component analysis, and pooling to compress high-dimensional image data prior to quantum encoding. Our findings reveal that fully connected layers retain the most salient features for QNN inference, thereby surpassing other down sampling approaches. Additionally, we examine state of the art data encoding methods (amplitude, angle, and qubit encoding) and identify amplitude encoding as the optimal strategy, yielding superior accuracy across all tested datasets and qubit configurations. Through computational analyses, we show that our distilled 4-qubit and 8-qubit QNNs achieve competitive performance while utilizing significantly fewer parameters than their classical counterparts. Our results establish a promising paradigm for bridging classical deep learning and emerging quantum computing, paving the way for more powerful, resource conscious models in quantum machine intelligence.         ",
    "url": "https://arxiv.org/abs/2311.13810",
    "authors": [
      "Mohammad Junayed Hasan",
      "M.R.C.Mahdy"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.15109",
    "title": "Robust Stability of Neural Network Control Systems with Interval Matrix Uncertainties",
    "abstract": "           Neural networks have become increasingly popular in controller design due to their versatility and efficiency. However, their integration into feedback systems can pose stability challenges, particularly in the presence of uncertainties. This work addresses the problem of certifying robust stability in neural network control systems with interval matrix uncertainties. Leveraging classical robust stability techniques and the quadratic constraint-based method to characterize the input-output behavior of neural networks, we derive novel robust stability certificates formulated as linear matrix inequalities. To reduce computational complexity, we introduce three relaxed sufficient conditions and establish their equivalence in terms of feasibility. Additionally, we explore their connections to existing robust stability results. The effectiveness of the proposed approach is demonstrated through inverted pendulum and mass-spring-damper examples.         ",
    "url": "https://arxiv.org/abs/2311.15109",
    "authors": [
      "Yuhao Zhang",
      "Xiangru Xu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2401.15262",
    "title": "Asymptotic Behavior of Adversarial Training Estimator under $\\ell_\\infty$-Perturbation",
    "abstract": "           Adversarial training has been proposed to protect machine learning models against adversarial attacks. This paper focuses on adversarial training under $\\ell_\\infty$-perturbation, which has recently attracted much research attention. The asymptotic behavior of the adversarial training estimator is investigated in the generalized linear model. The results imply that the asymptotic distribution of the adversarial training estimator under $\\ell_\\infty$-perturbation could put a positive probability mass at $0$ when the true parameter is $0$, providing a theoretical guarantee of the associated sparsity-recovery ability. Alternatively, a two-step procedure is proposed -- adaptive adversarial training, which could further improve the performance of adversarial training under $\\ell_\\infty$-perturbation. Specifically, the proposed procedure could achieve asymptotic variable-selection consistency and unbiasedness. Numerical experiments are conducted to show the sparsity-recovery ability of adversarial training under $\\ell_\\infty$-perturbation and to compare the empirical performance between classic adversarial training and adaptive adversarial training.         ",
    "url": "https://arxiv.org/abs/2401.15262",
    "authors": [
      "Yiling Xie",
      "Xiaoming Huo"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2401.17116",
    "title": "Quantum time dynamics mediated by the Yang-Baxter equation and artificial neural networks",
    "abstract": "           Quantum computing shows great potential, but errors pose a significant challenge. This study explores new strategies for mitigating quantum errors using artificial neural networks (ANN) and the Yang-Baxter equation (YBE). Unlike traditional error mitigation methods, which are computationally intensive, we investigate artificial error mitigation. We developed a novel method that combines ANN for noise mitigation combined with the YBE to generate noisy data. This approach effectively reduces noise in quantum simulations, enhancing the accuracy of the results. The YBE rigorously preserves quantum correlations and symmetries in spin chain simulations in certain classes of integrable lattice models, enabling effective compression of quantum circuits while retaining linear scalability with the number of qubits. This compression facilitates both full and partial implementations, allowing the generation of noisy quantum data on hardware alongside noiseless simulations using classical platforms. By introducing controlled noise through the YBE, we enhance the dataset for error mitigation. We train an ANN model on partial data from quantum simulations, demonstrating its effectiveness in mitigating errors in time-evolving quantum states, providing a scalable framework to enhance quantum computation fidelity, particularly in noisy intermediate-scale quantum (NISQ) systems. We demonstrate the efficacy of this approach by performing quantum time dynamics simulations using the Heisenberg XY Hamiltonian on real quantum devices.         ",
    "url": "https://arxiv.org/abs/2401.17116",
    "authors": [
      "Sahil Gulania",
      "Yuri Alexeev",
      "Stephen K. Gray",
      "Bo Peng",
      "Niranjan Govind"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2407.07516",
    "title": "HDKD: Hybrid Data-Efficient Knowledge Distillation Network for Medical Image Classification",
    "abstract": "           Vision Transformers (ViTs) have achieved significant advancement in computer vision tasks due to their powerful modeling capacity. However, their performance notably degrades when trained with insufficient data due to lack of inherent inductive biases. Distilling knowledge and inductive biases from a Convolutional Neural Network (CNN) teacher has emerged as an effective strategy for enhancing the generalization of ViTs on limited datasets. Previous approaches to Knowledge Distillation (KD) have pursued two primary paths: some focused solely on distilling the logit distribution from CNN teacher to ViT student, neglecting the rich semantic information present in intermediate features due to the structural differences between them. Others integrated feature distillation along with logit distillation, yet this introduced alignment operations that limits the amount of knowledge transferred due to mismatched architectures and increased the computational overhead. To this end, this paper presents Hybrid Data-efficient Knowledge Distillation (HDKD) paradigm which employs a CNN teacher and a hybrid student. The choice of hybrid student serves two main aspects. First, it leverages the strengths of both convolutions and transformers while sharing the convolutional structure with the teacher model. Second, this shared structure enables the direct application of feature distillation without any information loss or additional computational overhead. Additionally, we propose an efficient light-weight convolutional block named Mobile Channel-Spatial Attention (MBCSA), which serves as the primary convolutional block in both teacher and student models. Extensive experiments on two medical public datasets showcase the superiority of HDKD over other state-of-the-art models and its computational efficiency. Source code at: this https URL ",
    "url": "https://arxiv.org/abs/2407.07516",
    "authors": [
      "Omar S. EL-Assiouti",
      "Ghada Hamed",
      "Dina Khattab",
      "Hala M. Ebied"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.14651",
    "title": "Improving Representation of High-frequency Components for Medical Visual Foundation Models",
    "abstract": "           Foundation models have recently attracted significant attention for their impressive generalizability across diverse downstream tasks. However, these models are demonstrated to exhibit great limitations in representing high-frequency components and fine-grained details. In many medical imaging tasks, the precise representation of such information is crucial due to the inherently intricate anatomical structures, sub-visual features, and complex boundaries involved. Consequently, the limited representation of prevalent foundation models can result in significant performance degradation or even failure in these tasks. To address these challenges, we propose a novel pretraining strategy, named Frequency-advanced Representation Autoencoder (Frepa). Through high-frequency masking and low-frequency perturbation combined with adversarial learning, Frepa encourages the encoder to effectively represent and preserve high-frequency components in the image embeddings. Additionally, we introduce an innovative histogram-equalized image masking strategy, extending the Masked Autoencoder approach beyond ViT to other architectures such as Swin Transformer and convolutional networks. We develop Frepa across nine medical modalities and validate it on 32 downstream tasks for both 2D images and 3D volume data. Without fine-tuning, Frepa can outperform other self-supervised pretraining methods and, in some cases, even surpasses task-specific trained models. This improvement is particularly significant for tasks involving fine-grained details, such as achieving up to a +15% increase in DSC for retina vessel segmentation and a +7% increase in IoU for lung nodule detection. Further experiments quantitatively reveal that Frepa enables superior high-frequency representations and preservation in the embeddings, underscoring its potential for developing more generalized and universal medical image foundation models.         ",
    "url": "https://arxiv.org/abs/2407.14651",
    "authors": [
      "Yuetan Chu",
      "Yilan Zhang",
      "Zhongyi Han",
      "Changchun Yang",
      "Longxi Zhou",
      "Gongning Luo",
      "Chao Huang",
      "Xin Gao"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.00773",
    "title": "Spike Talk: Genesis and Neural Coding Scheme Translations",
    "abstract": "           Although digitalization of future power grids offer several coordination incentives, the reliability and security of information and communication technologies (ICT) hinders its overall performance. In this paper, we introduce a novel architecture Spike Talk via a unified representation of power and information as a means of data normalization using spikes for coordinated control of microgrids. This grid-edge technology allows each distributed energy resource (DER) to execute decentralized secondary control philosophy independently by interacting among each other using power flow along the tie-lines. Inspired from the field of computational neuroscience, Spike Talk basically builds on a fine-grained parallelism on the information transfer theory in our brains, particularly when neurons (modeled as DERs) transmit information (inferred from power streams measurable at each DER) through synapses (modeled as tie-lines). Not only does Spike Talk simplify and address the current bottlenecks of the cyber-physical architectural operation by dismissing the ICT layer, it provides intrinsic operational and cost-effective opportunities in terms of infrastructure development, computations and modeling. Hence, this paper provides a pedagogic illustration of the key concepts and design theories. Since we focus on coordinated control of microgrids in this paper, the signaling accuracy and system performance is studied for several neural coding schemes responsible for converting the real-valued local measurements into spikes.         ",
    "url": "https://arxiv.org/abs/2408.00773",
    "authors": [
      "Subham Sahoo"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2410.03011",
    "title": "Towards Understanding the Universality of Transformers for Next-Token Prediction",
    "abstract": "           Causal Transformers are trained to predict the next token for a given context. While it is widely accepted that self-attention is crucial for encoding the causal structure of sequences, the precise underlying mechanism behind this in-context autoregressive learning ability remains unclear. In this paper, we take a step towards understanding this phenomenon by studying the approximation ability of Transformers for next-token prediction. Specifically, we explore the capacity of causal Transformers to predict the next token $x_{t+1}$ given an autoregressive sequence $(x_1, \\dots, x_t)$ as a prompt, where $ x_{t+1} = f(x_t) $, and $ f $ is a context-dependent function that varies with each sequence. On the theoretical side, we focus on specific instances, namely when $ f $ is linear or when $ (x_t)_{t \\geq 1} $ is periodic. We explicitly construct a Transformer (with linear, exponential, or softmax attention) that learns the mapping $f$ in-context through a causal kernel descent method. The causal kernel descent method we propose provably estimates $x_{t+1} $ based solely on past and current observations $ (x_1, \\dots, x_t) $, with connections to the Kaczmarz algorithm in Hilbert spaces. We present experimental results that validate our theoretical findings and suggest their applicability to more general mappings $f$.         ",
    "url": "https://arxiv.org/abs/2410.03011",
    "authors": [
      "Michael E. Sander",
      "Gabriel Peyr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.07267",
    "title": "Scintillation pulse characterization with spectrum-inspired temporal neural networks: case studies on particle detector signals",
    "abstract": "           Particle detectors based on scintillators are widely used in high-energy physics and astroparticle physics experiments, nuclear medicine imaging, industrial and environmental detection, etc. Precisely extracting scintillation signal characteristics at the event level is important for these applications, not only in respect of understanding the scintillator itself, but also kinds and physical property of incident particles. Recent researches demonstrate data-driven neural networks surpass traditional statistical methods, especially when the analytical form of signals is hard to obtain, or noise is significant. However, most densely connected or convolution-based networks fail to fully exploit the spectral and temporal structure of scintillation signals, leaving large space for performance improvement. In this paper, we propose a network architecture specially tailored for scintillation pulse characterization based on previous works on time series analysis. The core insight is that, by directly applying Fast Fourier Transform on original signals and utilizing different frequency components, the proposed network architecture can serve as a lightweight and enhanced representation learning backbone. We prove our idea in two case studies: (a) simulation data generated with the setting of the LUX dark matter detector, and (b) experimental electrical signals with fast electronics to emulate scintillation variations for the NICA/MPD calorimeter. The proposed model achieves significantly better results than the reference model in literature and densely connected models, and demonstrates higher cost-efficiency than conventional machine learning methods.         ",
    "url": "https://arxiv.org/abs/2410.07267",
    "authors": [
      "Pengcheng Ai",
      "Xiangming Sun",
      "Zhi Deng",
      "Xinchi Ran"
    ],
    "subjectives": [
      "Instrumentation and Detectors (physics.ins-det)",
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2410.22729",
    "title": "Identifying Drift, Diffusion, and Causal Structure from Temporal Snapshots",
    "abstract": "           Stochastic differential equations (SDEs) are a fundamental tool for modelling dynamic processes, including gene regulatory networks (GRNs), contaminant transport, financial markets, and image generation. However, learning the underlying SDE from data is a challenging task, especially if individual trajectories are not observable. Motivated by burgeoning research in single-cell datasets, we present the first comprehensive approach for jointly identifying the drift and diffusion of an SDE from its temporal marginals. Assuming linear drift and additive diffusion, we prove that these parameters are identifiable from marginals if and only if the initial distribution lacks any generalized rotational symmetries. We further prove that the causal graph of any SDE with additive diffusion can be recovered from the SDE parameters. To complement this theory, we adapt entropy-regularized optimal transport to handle anisotropic diffusion, and introduce APPEX (Alternating Projection Parameter Estimation from $X_0$), an iterative algorithm designed to estimate the drift, diffusion, and causal graph of an additive noise SDE, solely from temporal marginals. We show that APPEX iteratively decreases Kullback-Leibler divergence to the true solution, and demonstrate its effectiveness on simulated data from linear additive noise SDEs.         ",
    "url": "https://arxiv.org/abs/2410.22729",
    "authors": [
      "Vincent Guan",
      "Joseph Janssen",
      "Hossein Rahmani",
      "Andrew Warren",
      "Stephen Zhang",
      "Elina Robeva",
      "Geoffrey Schiebinger"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2411.10027",
    "title": "XLSR-Mamba: A Dual-Column Bidirectional State Space Model for Spoofing Attack Detection",
    "abstract": "           Transformers and their variants have achieved great success in speech processing. However, their multi-head self-attention mechanism is computationally expensive. Therefore, one novel selective state space model, Mamba, has been proposed as an alternative. Building on its success in automatic speech recognition, we apply Mamba for spoofing attack detection. Mamba is well-suited for this task as it can capture the artifacts in spoofed speech signals by handling long-length sequences. However, Mamba's performance may suffer when it is trained with limited labeled data. To mitigate this, we propose combining a new structure of Mamba based on a dual-column architecture with self-supervised learning, using the pre-trained wav2vec 2.0 model. The experiments show that our proposed approach achieves competitive results and faster inference on the ASVspoof 2021 LA and DF datasets, and on the more challenging In-the-Wild dataset, it emerges as the strongest candidate for spoofing attack detection. The code has been publicly released in this https URL.         ",
    "url": "https://arxiv.org/abs/2411.10027",
    "authors": [
      "Yang Xiao",
      "Rohan Kumar Das"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2411.18018",
    "title": "Neural Finite-State Machines for Surgical Phase Recognition",
    "abstract": "           Surgical phase recognition (SPR) is crucial for applications in workflow optimization, performance evaluation, and real-time intervention guidance. However, current deep learning models often struggle with fragmented predictions, failing to capture the sequential nature of surgical workflows. We propose the Neural Finite-State Machine (NFSM), a novel approach that enforces temporal coherence by integrating classical state-transition priors with modern neural networks. NFSM leverages learnable global state embeddings as unique phase identifiers and dynamic transition tables to model phase-to-phase progressions. Additionally, a future phase forecasting mechanism employs repeated frame padding to anticipate upcoming transitions. Implemented as a plug-and-play module, NFSM can be integrated into existing SPR pipelines without changing their core architectures. We demonstrate state-of-the-art performance across multiple benchmarks, including a significant improvement on the BernBypass70 dataset - raising video-level accuracy by 0.9 points and phase-level precision, recall, F1-score, and mAP by 3.8, 3.1, 3.3, and 4.1, respectively. Ablation studies confirm each component's effectiveness and the module's adaptability to various architectures. By unifying finite-state principles with deep learning, NFSM offers a robust path toward consistent, long-term surgical video analysis.         ",
    "url": "https://arxiv.org/abs/2411.18018",
    "authors": [
      "Hao Ding",
      "Zhongpai Gao",
      "Benjamin Planche",
      "Tianyu Luan",
      "Abhishek Sharma",
      "Meng Zheng",
      "Ange Lou",
      "Terrence Chen",
      "Mathias Unberath",
      "Ziyan Wu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.01055",
    "title": "On the Surprising Robustness of Sequential Convex Optimization for Contact-Implicit Motion Planning",
    "abstract": "           Contact-implicit motion planning-embedding contact sequencing as implicit complementarity constraints-holds the promise of leveraging continuous optimization to discover new contact patterns online. Nevertheless, the resulting optimization, being an instance of Mathematical Programming with Complementary Constraints, fails the classical constraint qualifications that are crucial for the convergence of popular numerical solvers. We present robust contact-implicit motion planning with sequential convex programming (CRISP), a solver that departs from the usual primal-dual algorithmic framework but instead only focuses on the primal problem. CRISP solves a convex quadratic program with an adaptive trust region radius at each iteration, and its convergence is evaluated by a merit function using weighted penalty. We (i) provide sufficient conditions on CRISP's convergence to first-order stationary points of the merit function; (ii) release a high-performance C++ implementation of CRISP with a generic nonlinear programming interface; and (iii) demonstrate CRISP's surprising robustness in solving contact-implicit planning with naive initialization. In fact, CRISP solves several contact-implicit problems with all-zero initialization.         ",
    "url": "https://arxiv.org/abs/2502.01055",
    "authors": [
      "Yulin Li",
      "Haoyu Han",
      "Shucheng Kang",
      "Jun Ma",
      "Heng Yang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.11203",
    "title": "Multiscale autonomous forecasting of plasma systems' dynamics using neural networks",
    "abstract": "           Plasma systems exhibit complex multiscale dynamics, resolving which poses significant challenges for conventional numerical simulations. Machine learning (ML) offers an alternative by learning data-driven representations of these dynamics. Yet existing ML time-stepping models suffer from error accumulation, instability, and limited long-term forecasting horizons. This paper demonstrates the application of a hierarchical multiscale neural network architecture for autonomous plasma forecasting. The framework integrates multiple neural networks trained across different temporal scales to capture both fine-scale and large-scale behaviors while mitigating compounding error in recursive evaluation. Fine-scale networks accurately resolve fast-evolving features, while coarse-scale networks provide broader temporal context, reducing the frequency of recursive updates and limiting the accumulation of small prediction errors over time. We first evaluate the method using canonical nonlinear dynamical systems and compare its performance against classical single-scale neural networks. The results demonstrate that single-scale neural networks experience rapid divergence due to recursive error accumulation, whereas the multiscale approach improves stability and extends prediction horizons. Next, our ML model is applied to two plasma configurations of high scientific and applied significance, demonstrating its ability to preserve spatial structures and capture multiscale plasma dynamics. By leveraging multiple time-stepping resolutions, the applied framework is shown to outperform conventional single-scale networks for the studied plasma test cases. The results of this work position the hierarchical multiscale neural network as a promising tool for efficient plasma forecasting and digital twin applications.         ",
    "url": "https://arxiv.org/abs/2502.11203",
    "authors": [
      "Farbod Faraji",
      "Maryam Reza"
    ],
    "subjectives": [
      "Plasma Physics (physics.plasm-ph)",
      "Machine Learning (cs.LG)"
    ]
  }
]