[
  {
    "id": "arXiv:2406.06535",
    "title": "Utilizing Graph Generation for Enhanced Domain Adaptive Object Detection",
    "abstract": "           The problem of Domain Adaptive in the field of Object Detection involves the transfer of object detection models from labeled source domains to unannotated target domains. Recent advancements in this field aim to address domain discrepancies by aligning pixel-pairs across domains within a non-Euclidean graphical space, thereby minimizing semantic distribution variance. Despite their remarkable achievements, these methods often use coarse semantic representations to model graphs, mainly due to ignoring non-informative elements and failing to focus on precise semantic alignment. Additionally, the generation of coarse graphs inherently introduces abnormal nodes, posing challenges and potentially biasing domain adaptation outcomes. Consequently, we propose a framework, which utilizes the Graph Generation to enhance the quality of DAOD (\\method{}). Specifically, we introduce a Node Refinement module that utilizes a memory bank to reconstruct noisy sampled nodes while applying contrastive regularization to noisy features. To enhance semantic alignment, we propose separating domain-specific styles from category invariance encoded within graph covariances, which allows us to selectively remove domain-specific styles while preserving category-invariant information, thus facilitating more accurate semantic alignment across different domains. Furthermore, we propose a Graph Optimization adaptor, leveraging variational inference to mitigate the impact of abnormal nodes. Extensive experimentation across three adaptation benchmarks validates that \\method{} achieves state-of-the-art performance in the task of unsupervised domain adaptation.         ",
    "url": "https://arxiv.org/abs/2406.06535",
    "authors": [
      "Mu Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.06553",
    "title": "Ensemble Model With Bert,Roberta and Xlnet For Molecular property prediction",
    "abstract": "           This paper presents a novel approach for predicting molecular properties with high accuracy without the need for extensive pre-training. Employing ensemble learning and supervised fine-tuning of BERT, RoBERTa, and XLNet, our method demonstrates significant effectiveness compared to existing advanced models. Crucially, it addresses the issue of limited computational resources faced by experimental groups, enabling them to accurately predict molecular properties. This innovation provides a cost-effective and resource-efficient solution, potentially advancing further research in the molecular domain.         ",
    "url": "https://arxiv.org/abs/2406.06553",
    "authors": [
      "Junling Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Chemical Physics (physics.chem-ph)"
    ]
  },
  {
    "id": "arXiv:2406.06558",
    "title": "Enhancing Text Authenticity: A Novel Hybrid Approach for AI-Generated Text Detection",
    "abstract": "           The rapid advancement of Large Language Models (LLMs) has ushered in an era where AI-generated text is increasingly indistinguishable from human-generated content. Detecting AI-generated text has become imperative to combat misinformation, ensure content authenticity, and safeguard against malicious uses of AI. In this paper, we propose a novel hybrid approach that combines traditional TF-IDF techniques with advanced machine learning models, including Bayesian classifiers, Stochastic Gradient Descent (SGD), Categorical Gradient Boosting (CatBoost), and 12 instances of Deberta-v3-large models. Our approach aims to address the challenges associated with detecting AI-generated text by leveraging the strengths of both traditional feature extraction methods and state-of-the-art deep learning models. Through extensive experiments on a comprehensive dataset, we demonstrate the effectiveness of our proposed method in accurately distinguishing between human and AI-generated text. Our approach achieves superior performance compared to existing methods. This research contributes to the advancement of AI-generated text detection techniques and lays the foundation for developing robust solutions to mitigate the challenges posed by AI-generated content.         ",
    "url": "https://arxiv.org/abs/2406.06558",
    "authors": [
      "Ye Zhang",
      "Qian Leng",
      "Mengran Zhu",
      "Rui Ding",
      "Yue Wu",
      "Jintong Song",
      "Yulu Gong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.06572",
    "title": "Graph Neural Network Enhanced Retrieval for Question Answering of LLMs",
    "abstract": "           Retrieval augmented generation has revolutionized large language model (LLM) outputs by providing factual supports. Nevertheless, it struggles to capture all the necessary knowledge for complex reasoning questions. Existing retrieval methods typically divide reference documents into passages, treating them in isolation. These passages, however, are often interrelated, such as passages that are contiguous or share the same keywords. Therefore, recognizing the relatedness is crucial for enhancing the retrieval process. In this paper, we propose a novel retrieval method, called GNN-Ret, which leverages graph neural networks (GNNs) to enhance retrieval by considering the relatedness between passages. Specifically, we first construct a graph of passages by connecting passages that are structure-related and keyword-related. A graph neural network (GNN) is then leveraged to exploit the relationships between passages and improve the retrieval of supporting passages. Furthermore, we extend our method to handle multi-hop reasoning questions using a recurrent graph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates the graphs of passages from previous steps, thereby enhancing the retrieval of supporting passages. Extensive experiments on benchmark datasets demonstrate that GNN-Ret achieves higher accuracy for question answering with a single query of LLMs than strong baselines that require multiple queries, and RGNN-Ret further improves accuracy and achieves state-of-the-art performance, with up to 10.4% accuracy improvement on the 2WikiMQA dataset.         ",
    "url": "https://arxiv.org/abs/2406.06572",
    "authors": [
      "Zijian Li",
      "Qingyan Guo",
      "Jiawei Shao",
      "Lei Song",
      "Jiang Bian",
      "Jun Zhang",
      "Rui Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2406.06573",
    "title": "MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering",
    "abstract": "           Large language models (LLM) have achieved impressive performance on medical question-answering benchmarks. However, high benchmark accuracy does not imply that the performance generalizes to real-world clinical settings. Medical question-answering benchmarks rely on assumptions consistent with quantifying LLM performance but that may not hold in the open world of the clinic. Yet LLMs learn broad knowledge that can help the LLM generalize to practical conditions regardless of unrealistic assumptions in celebrated benchmarks. We seek to quantify how well LLM medical question-answering benchmark performance generalizes when benchmark assumptions are violated. Specifically, we present an adversarial method that we call MedFuzz (for medical fuzzing). MedFuzz attempts to modify benchmark questions in ways aimed at confounding the LLM. We demonstrate the approach by targeting strong assumptions about patient characteristics presented in the MedQA benchmark. Successful \"attacks\" modify a benchmark item in ways that would be unlikely to fool a medical expert but nonetheless \"trick\" the LLM into changing from a correct to an incorrect answer. Further, we present a permutation test technique that can ensure a successful attack is statistically significant. We show how to use performance on a \"MedFuzzed\" benchmark, as well as individual successful attacks. The methods show promise at providing insights into the ability of an LLM to operate robustly in more realistic settings.         ",
    "url": "https://arxiv.org/abs/2406.06573",
    "authors": [
      "Robert Osazuwa Ness",
      "Katie Matton",
      "Hayden Helm",
      "Sheng Zhang",
      "Junaid Bajwa",
      "Carey E. Priebe",
      "Eric Horvitz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06578",
    "title": "SMS Spam Detection and Classification to Combat Abuse in Telephone Networks Using Natural Language Processing",
    "abstract": "           In the modern era, mobile phones have become ubiquitous, and Short Message Service (SMS) has grown to become a multi-million-dollar service due to the widespread adoption of mobile devices and the millions of people who use SMS daily. However, SMS spam has also become a pervasive problem that endangers users' privacy and security through phishing and fraud. Despite numerous spam filtering techniques, there is still a need for a more effective solution to address this problem [1]. This research addresses the pervasive issue of SMS spam, which poses threats to users' privacy and security. Despite existing spam filtering techniques, the high false-positive rate persists as a challenge. The study introduces a novel approach utilizing Natural Language Processing (NLP) and machine learning models, particularly BERT (Bidirectional Encoder Representations from Transformers), for SMS spam detection and classification. Data preprocessing techniques, such as stop word removal and tokenization, are applied, along with feature extraction using BERT. Machine learning models, including SVM, Logistic Regression, Naive Bayes, Gradient Boosting, and Random Forest, are integrated with BERT for differentiating spam from ham messages. Evaluation results revealed that the Na\u00efve Bayes classifier + BERT model achieves the highest accuracy at 97.31% with the fastest execution time of 0.3 seconds on the test dataset. This approach demonstrates a notable enhancement in spam detection efficiency and a low false-positive rate. The developed model presents a valuable solution to combat SMS spam, ensuring faster and more accurate detection. This model not only safeguards users' privacy but also assists network providers in effectively identifying and blocking SMS spam messages.         ",
    "url": "https://arxiv.org/abs/2406.06578",
    "authors": [
      "Dare Azeez Oyeyemi",
      "Adebola K. Ojo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06583",
    "title": "Adaptive multiple optimal learning factors for neural network training",
    "abstract": "           This thesis presents a novel approach to neural network training that addresses the challenge of determining the optimal number of learning factors. The proposed Adaptive Multiple Optimal Learning Factors (AMOLF) algorithm dynamically adjusts the number of learning factors based on the error change per multiply, leading to improved training efficiency and accuracy. The thesis also introduces techniques for grouping weights based on the curvature of the objective function and for compressing large Hessian matrices. Experimental results demonstrate the superior performance of AMOLF compared to existing methods like OWO-MOLF and Levenberg-Marquardt.         ",
    "url": "https://arxiv.org/abs/2406.06583",
    "authors": [
      "Jeshwanth Challagundla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.06595",
    "title": "Beyond 5G Network Failure Classification for Network Digital Twin Using Graph Neural Network",
    "abstract": "           Fifth-generation (5G) core networks in network digital twins (NDTs) are complex systems with numerous components, generating considerable data. Analyzing these data can be challenging due to rare failure types, leading to imbalanced classes in multiclass classification. To address this problem, we propose a novel method of integrating a graph Fourier transform (GFT) into a message-passing neural network (MPNN) designed for NDTs. This approach transforms the data into a graph using the GFT to address class imbalance, whereas the MPNN extracts features and models dependencies between network components. This combined approach identifies failure types in real and simulated NDT environments, demonstrating its potential for accurate failure classification in 5G and beyond (B5G) networks. Moreover, the MPNN is adept at learning complex local structures among neighbors in an end-to-end setting. Extensive experiments have demonstrated that the proposed approach can identify failure types in three multiclass domain datasets at multiple failure points in real networks and NDT environments. The results demonstrate that the proposed GFT-MPNN can accurately classify network failures in B5G networks, especially when employed within NDTs to detect failure types.         ",
    "url": "https://arxiv.org/abs/2406.06595",
    "authors": [
      "Abubakar Isah",
      "Ibrahim Aliyu",
      "Jaechan Shim",
      "Hoyong Ryu",
      "Jinsul Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2406.06606",
    "title": "Prototypical Reward Network for Data-Efficient RLHF",
    "abstract": "           The reward model for Reinforcement Learning from Human Feedback (RLHF) has proven effective in fine-tuning Large Language Models (LLMs). Notably, collecting human feedback for RLHF can be resource-intensive and lead to scalability issues for LLMs and complex tasks. Our proposed framework Proto-RM leverages prototypical networks to enhance reward models under limited human feedback. By enabling stable and reliable structural learning from fewer samples, Proto-RM significantly enhances LLMs' adaptability and accuracy in interpreting human preferences. Extensive experiments on various datasets demonstrate that Proto-RM significantly improves the performance of reward models and LLMs in human feedback tasks, achieving comparable and usually better results than traditional methods, while requiring significantly less data. in data-limited scenarios. This research offers a promising direction for enhancing the efficiency of reward models and optimizing the fine-tuning of language models under restricted feedback conditions.         ",
    "url": "https://arxiv.org/abs/2406.06606",
    "authors": [
      "Jinghan Zhang",
      "Xiting Wang",
      "Yiqiao Jin",
      "Changyu Chen",
      "Xinhao Zhang",
      "Kunpeng Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.06607",
    "title": "Continuous Test-time Domain Adaptation for Efficient Fault Detection under Evolving Operating Conditions",
    "abstract": "           Fault detection is crucial in industrial systems to prevent failures and optimize performance by distinguishing abnormal from normal operating conditions. Data-driven methods have been gaining popularity for fault detection tasks as the amount of condition monitoring data from complex industrial systems increases. Despite these advances, early fault detection remains a challenge under real-world scenarios. The high variability of operating conditions and environments makes it difficult to collect comprehensive training datasets that can represent all possible operating conditions, especially in the early stages of system operation. Furthermore, these variations often evolve over time, potentially leading to entirely new data distributions in the future that were previously unseen. These challenges prevent direct knowledge transfer across different units and over time, leading to the distribution gap between training and testing data and inducing performance degradation of those methods in real-world scenarios. To overcome this, our work introduces a novel approach for continuous test-time domain adaptation. This enables early-stage robust anomaly detection by addressing domain shifts and limited data representativeness issues. We propose a Test-time domain Adaptation Anomaly Detection (TAAD) framework that separates input variables into system parameters and measurements, employing two domain adaptation modules to independently adapt to each input category. This method allows for effective adaptation to evolving operating conditions and is particularly beneficial in systems with scarce data. Our approach, tested on a real-world pump monitoring dataset, shows significant improvements over existing domain adaptation methods in fault detection, demonstrating enhanced accuracy and reliability.         ",
    "url": "https://arxiv.org/abs/2406.06607",
    "authors": [
      "Han Sun",
      "Kevin Ammann",
      "Stylianos Giannoulakis",
      "Olga Fink"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.06611",
    "title": "Building Hybrid B-Spline And Neural Network Operators",
    "abstract": "           Control systems are indispensable for ensuring the safety of cyber-physical systems (CPS), spanning various domains such as automobiles, airplanes, and missiles. Safeguarding CPS necessitates runtime methodologies that continuously monitor safety-critical conditions and respond in a verifiably safe manner. A fundamental aspect of many safety approaches involves predicting the future behavior of systems. However, achieving this requires accurate models that can operate in real time. Motivated by DeepONets, we propose a novel strategy that combines the inductive bias of B-splines with data-driven neural networks to facilitate real-time predictions of CPS behavior. We introduce our hybrid B-spline neural operator, establishing its capability as a universal approximator and providing rigorous bounds on the approximation error. These findings are applicable to a broad class of nonlinear autonomous systems and are validated through experimentation on a controlled 6-degree-of-freedom (DOF) quadrotor with a 12 dimensional state space. Furthermore, we conduct a comparative analysis of different network architectures, specifically fully connected networks (FCNN) and recurrent neural networks (RNN), to elucidate the practical utility and trade-offs associated with each architecture in real-world scenarios.         ",
    "url": "https://arxiv.org/abs/2406.06611",
    "authors": [
      "Raffaele Romagnoli",
      "Jasmine Ratchford",
      "Mark H. Klein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2406.06618",
    "title": "PANDORA: Deep graph learning based COVID-19 infection risk level forecasting",
    "abstract": "           COVID-19 as a global pandemic causes a massive disruption to social stability that threatens human life and the economy. Policymakers and all elements of society must deliver measurable actions based on the pandemic's severity to minimize the detrimental impact of COVID-19. A proper forecasting system is arguably important to provide an early signal of the risk of COVID-19 infection so that the authorities are ready to protect the people from the worst. However, making a good forecasting model for infection risks in different cities or regions is not an easy task, because it has a lot of influential factors that are difficult to be identified manually. To address the current limitations, we propose a deep graph learning model, called PANDORA, to predict the infection risks of COVID-19, by considering all essential factors and integrating them into a geographical network. The framework uses geographical position relations and transportation frequency as higher-order structural properties formulated by higher-order network structures (i.e., network motifs). Moreover, four significant node attributes (i.e., multiple features of a particular area, including climate, medical condition, economy, and human mobility) are also considered. We propose three different aggregators to better aggregate node attributes and structural features, namely, Hadamard, Summation, and Connection. Experimental results over real data show that PANDORA outperforms the baseline method with higher accuracy and faster convergence speed, no matter which aggregator is chosen. We believe that PANDORA using deep graph learning provides a promising approach to get superior performance in infection risk level forecasting and help humans battle the COVID-19 crisis.         ",
    "url": "https://arxiv.org/abs/2406.06618",
    "authors": [
      "Shuo Yu",
      "Feng Xia",
      "Yueru Wang",
      "Shihao Li",
      "Falih Febrinanto",
      "Madhu Chetty"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2406.06620",
    "title": "DualTime: A Dual-Adapter Multimodal Language Model for Time Series Representation",
    "abstract": "           The recent rapid development of language models (LMs) has attracted attention in the field of time series, including multimodal time series modeling. However, we note that current time series multimodal methods are biased, often assigning a primary role to one modality while the other assumes a secondary role. They overlook the mutual benefits and complementary of different modalities. For example, in seizure diagnosis, relying solely on textual clinical reports makes it difficult to pinpoint the area and type of the disease, while electroencephalograms (EEGs) alone cannot provide an accurate diagnosis without considering the symptoms. In this study, based on the complementary information mining of time series multimodal data, we propose DualTime, a Dual-adapter multimodal language model for Time series representation implementing temporal-primary and textual-primary modeling simultaneously. By injecting lightweight adaption tokens, the LM pipeline shared by dual adapters encourages embedding alignment and achieves efficient fine-tuning. Empirically, our method outperforms state-of-the-art models in both supervised and unsupervised settings, highlighting the complementary benefits of different modalities. In addition, we conduct few-shot label transfer experiments, which further verifies the transferability and expressiveness of our proposed DualTime.         ",
    "url": "https://arxiv.org/abs/2406.06620",
    "authors": [
      "Weiqi Zhang",
      "Jiexia Ye",
      "Ziyue Li",
      "Jia Li",
      "Fugee Tsung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.06621",
    "title": "LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering",
    "abstract": "           We present LinkQ, a system that leverages a large language model (LLM) to facilitate knowledge graph (KG) query construction through natural language question-answering. Traditional approaches often require detailed knowledge of complex graph querying languages, limiting the ability for users -- even experts -- to acquire valuable insights from KG data. LinkQ simplifies this process by first interpreting a user's question, then converting it into a well-formed KG query. By using the LLM to construct a query instead of directly answering the user's question, LinkQ guards against the LLM hallucinating or generating false, erroneous information. By integrating an LLM into LinkQ, users are able to conduct both exploratory and confirmatory data analysis, with the LLM helping to iteratively refine open-ended questions into precise ones. To demonstrate the efficacy of LinkQ, we conducted a qualitative study with five KG practitioners and distill their feedback. Our results indicate that practitioners find LinkQ effective for KG question-answering, and desire future LLM-assisted systems for the exploratory analysis of graph databases.         ",
    "url": "https://arxiv.org/abs/2406.06621",
    "authors": [
      "Harry Li",
      "Gabriel Appleby",
      "Ashley Suh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06622",
    "title": "Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs",
    "abstract": "           Although safely enhanced Large Language Models (LLMs) have achieved remarkable success in tackling various complex tasks in a zero-shot manner, they remain susceptible to jailbreak attacks, particularly the unknown jailbreak attack. To enhance LLMs' generalized defense capabilities, we propose a two-stage adversarial tuning framework, which generates adversarial prompts to explore worst-case scenarios by optimizing datasets containing pairs of adversarial prompts and their safe responses. In the first stage, we introduce the hierarchical meta-universal adversarial prompt learning to efficiently and effectively generate token-level adversarial prompts. In the second stage, we propose the automatic adversarial prompt learning to iteratively refine semantic-level adversarial prompts, further enhancing LLM's defense capabilities. We conducted comprehensive experiments on three widely used jailbreak datasets, comparing our framework with six defense baselines under five representative attack scenarios. The results underscore the superiority of our proposed methods. Furthermore, our adversarial tuning framework exhibits empirical generalizability across various attack strategies and target LLMs, highlighting its potential as a transferable defense mechanism.         ",
    "url": "https://arxiv.org/abs/2406.06622",
    "authors": [
      "Fan Liu",
      "Zhao Xu",
      "Hao Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2406.06626",
    "title": "Benchmarking Neural Decoding Backbones towards Enhanced On-edge iBCI Applications",
    "abstract": "           Traditional invasive Brain-Computer Interfaces (iBCIs) typically depend on neural decoding processes conducted on workstations within laboratory settings, which prevents their everyday usage. Implementing these decoding processes on edge devices, such as the wearables, introduces considerable challenges related to computational demands, processing speed, and maintaining accuracy. This study seeks to identify an optimal neural decoding backbone that boasts robust performance and swift inference capabilities suitable for edge deployment. We executed a series of neural decoding experiments involving nonhuman primates engaged in random reaching tasks, evaluating four prospective models, Gated Recurrent Unit (GRU), Transformer, Receptance Weighted Key Value (RWKV), and Selective State Space model (Mamba), across several metrics: single-session decoding, multi-session decoding, new session fine-tuning, inference speed, calibration speed, and scalability. The findings indicate that although the GRU model delivers sufficient accuracy, the RWKV and Mamba models are preferable due to their superior inference and calibration speeds. Additionally, RWKV and Mamba comply with the scaling law, demonstrating improved performance with larger data sets and increased model sizes, whereas GRU shows less pronounced scalability, and the Transformer model requires computational resources that scale prohibitively. This paper presents a thorough comparative analysis of the four models in various scenarios. The results are pivotal in pinpointing an optimal backbone that can handle increasing data volumes and is viable for edge implementation. This analysis provides essential insights for ongoing research and practical applications in the field.         ",
    "url": "https://arxiv.org/abs/2406.06626",
    "authors": [
      "Zhou Zhou",
      "Guohang He",
      "Zheng Zhang",
      "Luziwei Leng",
      "Qinghai Guo",
      "Jianxing Liao",
      "Xuan Song",
      "Ran Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2406.06632",
    "title": "Transfer Entropy in Graph Convolutional Neural Networks",
    "abstract": "           Graph Convolutional Networks (GCN) are Graph Neural Networks where the convolutions are applied over a graph. In contrast to Convolutional Neural Networks, GCN's are designed to perform inference on graphs, where the number of nodes can vary, and the nodes are unordered. In this study, we address two important challenges related to GCNs: i) oversmoothing; and ii) the utilization of node relational properties (i.e., heterophily and homophily). Oversmoothing is the degradation of the discriminative capacity of nodes as a result of repeated aggregations. Heterophily is the tendency for nodes of different classes to connect, whereas homophily is the tendency of similar nodes to connect. We propose a new strategy for addressing these challenges in GCNs based on Transfer Entropy (TE), which measures of the amount of directed transfer of information between two time varying nodes. Our findings indicate that using node heterophily and degree information as a node selection mechanism, along with feature-based TE calculations, enhances accuracy across various GCN models. Our model can be easily modified to improve classification accuracy of a GCN model. As a trade off, this performance boost comes with a significant computational overhead when the TE is computed for many graph nodes.         ",
    "url": "https://arxiv.org/abs/2406.06632",
    "authors": [
      "Adrian Moldovan",
      "Angel Ca\u0163aron",
      "R\u0103zvan Andonie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2406.06641",
    "title": "Investigation of the Impact of Economic and Social Factors on Energy Demand through Natural Language Processing",
    "abstract": "           The relationship between energy demand and variables such as economic activity and weather is well established. However, this paper aims to explore the connection between energy demand and other social aspects, which receive little attention. Through the use of natural language processing on a large news corpus, we shed light on this important link. This study was carried out in five regions of the UK and Ireland and considers multiple horizons from 1 to 30 days. It also considers economic variables such as GDP, unemployment and inflation. We found that: 1) News about military conflicts, transportation, the global pandemic, regional economics, and the international energy market are related to electricity demand. 2) Economic indicators are more important in the East Midlands and Northern Ireland, while social indicators are more useful in the West Midlands and the South West of England. 3) The use of these indices improved forecasting performance by up to 9%.         ",
    "url": "https://arxiv.org/abs/2406.06641",
    "authors": [
      "Yun Bai",
      "Simon Camal",
      "Andrea Michiorri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06645",
    "title": "Network-Based Transfer Learning Helps Improve Short-Term Crime Prediction Accuracy",
    "abstract": "           Deep learning architectures enhanced with human mobility data have been shown to improve the accuracy of short-term crime prediction models trained with historical crime data. However, human mobility data may be scarce in some regions, negatively impacting the correct training of these models. To address this issue, we propose a novel transfer learning framework for short-term crime prediction models, whereby weights from the deep learning crime prediction models trained in source regions with plenty of mobility data are transferred to target regions to fine-tune their local crime prediction models and improve crime prediction accuracy. Our results show that the proposed transfer learning framework improves the F1 scores for target cities with mobility data scarcity, especially when the number of months of available mobility data is small. We also show that the F1 score improvements are pervasive across different types of crimes and diverse cities in the US.         ",
    "url": "https://arxiv.org/abs/2406.06645",
    "authors": [
      "Jiahui Wu",
      "Vanessa Frias-Martinez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2406.06652",
    "title": "Improving Generalization of Neural Vehicle Routing Problem Solvers Through the Lens of Model Architecture",
    "abstract": "           Neural models produce promising results when solving Vehicle Routing Problems (VRPs), but often fall short in generalization. Recent attempts to enhance model generalization often incur unnecessarily large training cost or cannot be directly applied to other models solving different VRP variants. To address these issues, we take a novel perspective on model architecture in this study. Specifically, we propose a plug-and-play Entropy-based Scaling Factor (ESF) and a Distribution-Specific (DS) decoder to enhance the size and distribution generalization, respectively. ESF adjusts the attention weight pattern of the model towards familiar ones discovered during training when solving VRPs of varying sizes. The DS decoder explicitly models VRPs of multiple training distribution patterns through multiple auxiliary light decoders, expanding the model representation space to encompass a broader range of distributional scenarios. We conduct extensive experiments on both synthetic and widely recognized real-world benchmarking datasets and compare the performance with seven baseline models. The results demonstrate the effectiveness of using ESF and DS decoder to obtain a more generalizable model and showcase their applicability to solve different VRP variants, i.e., travelling salesman problem and capacitated VRP. Notably, our proposed generic components require minimal computational resources, and can be effortlessly integrated into conventional generalization strategies to further elevate model generalization.         ",
    "url": "https://arxiv.org/abs/2406.06652",
    "authors": [
      "Yubin Xiao",
      "Di Wang",
      "Xuan Wu",
      "Yuesong Wu",
      "Boyang Li",
      "Wei Du",
      "Liupu Wang",
      "You Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.06653",
    "title": "DKDL-Net: A Lightweight Bearing Fault Detection Model via Decoupled Knowledge Distillation and Low-Rank Adaptation Fine-tuning",
    "abstract": "           Rolling bearing fault detection has developed rapidly in the field of fault diagnosis technology, and it occupies a very important position in this field. Deep learning-based bearing fault diagnosis models have achieved significant success. At the same time, with the continuous improvement of new signal processing technologies such as Fourier transform, wavelet transform and empirical mode decomposition, the fault diagnosis technology of rolling bearings has also been greatly developed, and it can be said that it has entered a new research stage. However, most of the existing methods are limited to varying degrees in the industrial field. The main ones are fast feature extraction and computational complexity. The key to this paper is to propose a lightweight bearing fault diagnosis model DKDL-Net to solve these challenges. The model is trained on the CWRU data set by decoupling knowledge distillation and low rank adaptive fine tuning. Specifically, we built and trained a teacher model based on a 6-layer neural network with 69,626 trainable parameters, and on this basis, using decoupling knowledge distillation (DKD) and Low-Rank adaptive (LoRA) fine-tuning, we trained the student sag model DKDL-Net, which has only 6838 parameters. Experiments show that DKDL-Net achieves 99.48\\% accuracy in computational complexity on the test set while maintaining model performance, which is 0.58\\% higher than the state-of-the-art (SOTA) model, and our model has lower parameters. Our code is available at Github link: this https URL.         ",
    "url": "https://arxiv.org/abs/2406.06653",
    "authors": [
      "Ovanes Petrosyan",
      "Li Pengyi",
      "He Yulong",
      "Liu Jiarui",
      "Sun Zhaoruikun",
      "Fu Guofeng",
      "Meng Liping"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06657",
    "title": "Harnessing AI for efficient analysis of complex policy documents: a case study of Executive Order 14110",
    "abstract": "           Policy documents, such as legislation, regulations, and executive orders, are crucial in shaping society. However, their length and complexity make interpretation and application challenging and time-consuming. Artificial intelligence (AI), particularly large language models (LLMs), has the potential to automate the process of analyzing these documents, improving accuracy and efficiency. This study aims to evaluate the potential of AI in streamlining policy analysis and to identify the strengths and limitations of current AI approaches. The research focuses on question answering and tasks involving content extraction from policy documents. A case study was conducted using Executive Order 14110 on \"Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\" as a test case. Four commercial AI systems were used to analyze the document and answer a set of representative policy questions. The performance of the AI systems was compared to manual analysis conducted by human experts. The study found that two AI systems, Gemini 1.5 Pro and Claude 3 Opus, demonstrated significant potential for supporting policy analysis, providing accurate and reliable information extraction from complex documents. They performed comparably to human analysts but with significantly higher efficiency. However, achieving reproducibility remains a challenge, necessitating further research and development.         ",
    "url": "https://arxiv.org/abs/2406.06657",
    "authors": [
      "Mark A. Kramer",
      "Allen Leavens",
      "Alexander Scarlat"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2406.06658",
    "title": "Link Prediction in Bipartite Networks",
    "abstract": "           Bipartite networks serve as highly suitable models to represent systems involving interactions between two distinct types of entities, such as online dating platforms, job search services, or ecommerce websites. These models can be leveraged to tackle a number of tasks, including link prediction among the most useful ones, especially to design recommendation systems. However, if this task has garnered much interest when conducted on unipartite (i.e. standard) networks, it is far from being the case for bipartite ones. In this study, we address this gap by performing an experimental comparison of 19 link prediction methods able to handle bipartite graphs. Some come directly from the literature, and some are adapted by us from techniques originally designed for unipartite networks. We also propose to repurpose recommendation systems based on graph convolutional networks (GCN) as a novel link prediction solution for bipartite networks. To conduct our experiments, we constitute a benchmark of 3 real-world bipartite network datasets with various topologies. Our results indicate that GCN-based personalized recommendation systems, which have received significant attention in recent years, can produce successful results for link prediction in bipartite networks. Furthermore, purely heuristic metrics that do not rely on any learning process, like the Structural Perturbation Method (SPM), can also achieve success.         ",
    "url": "https://arxiv.org/abs/2406.06658",
    "authors": [
      "\u015e\u00fckr\u00fc Demir \u0130nan \u00d6zer",
      "G\u00fcnce Keziban Orman",
      "Vincent Labatut"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2406.06660",
    "title": "Space-Time Continuous PDE Forecasting using Equivariant Neural Fields",
    "abstract": "           Recently, Conditional Neural Fields (NeFs) have emerged as a powerful modelling paradigm for PDEs, by learning solutions as flows in the latent space of the Conditional NeF. Although benefiting from favourable properties of NeFs such as grid-agnosticity and space-time-continuous dynamics modelling, this approach limits the ability to impose known constraints of the PDE on the solutions -- e.g. symmetries or boundary conditions -- in favour of modelling flexibility. Instead, we propose a space-time continuous NeF-based solving framework that - by preserving geometric information in the latent space - respects known symmetries of the PDE. We show that modelling solutions as flows of pointclouds over the group of interest $G$ improves generalization and data-efficiency. We validated that our framework readily generalizes to unseen spatial and temporal locations, as well as geometric transformations of the initial conditions - where other NeF-based PDE forecasting methods fail - and improve over baselines in a number of challenging geometries.         ",
    "url": "https://arxiv.org/abs/2406.06660",
    "authors": [
      "David M. Knigge",
      "David R. Wessels",
      "Riccardo Valperga",
      "Samuele Papa",
      "Jan-Jakob Sonke",
      "Efstratios Gavves",
      "Erik J. Bekkers"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2406.06663",
    "title": "SecureNet: A Comparative Study of DeBERTa and Large Language Models for Phishing Detection",
    "abstract": "           Phishing, whether through email, SMS, or malicious websites, poses a major threat to organizations by using social engineering to trick users into revealing sensitive information. It not only compromises company's data security but also incurs significant financial losses. In this paper, we investigate whether the remarkable performance of Large Language Models (LLMs) can be leveraged for particular task like text classification, particularly detecting malicious content and compare its results with state-of-the-art Deberta V3 (DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing) model. We systematically assess the potential and limitations of both approaches using comprehensive public datasets comprising diverse data sources such as email, HTML, URL, SMS, and synthetic data generation. Additionally, we demonstrate how LLMs can generate convincing phishing emails, making it harder to spot scams and evaluate the performance of both models in this context. Our study delves further into the challenges encountered by DeBERTa V3 during its training phases, fine-tuning methodology and transfer learning processes. Similarly, we examine the challenges associated with LLMs and assess their respective performance. Among our experimental approaches, the transformer-based DeBERTa method emerged as the most effective, achieving a test dataset (HuggingFace phishing dataset) recall (sensitivity) of 95.17% closely followed by GPT-4 providing a recall of 91.04%. We performed additional experiments with other datasets on the trained DeBERTa V3 model and LLMs like GPT 4 and Gemini 1.5. Based on our findings, we provide valuable insights into the effectiveness and robustness of these advanced language models, offering a detailed comparative analysis that can inform future research efforts in strengthening cybersecurity measures for detecting and mitigating phishing threats.         ",
    "url": "https://arxiv.org/abs/2406.06663",
    "authors": [
      "Sakshi Mahendru",
      "Tejul Pandit"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06671",
    "title": "Controlling Counterfactual Harm in Decision Support Systems Based on Prediction Sets",
    "abstract": "           Decision support systems based on prediction sets help humans solve multiclass classification tasks by narrowing down the set of potential label values to a subset of them, namely a prediction set, and asking them to always predict label values from the prediction sets. While this type of systems have been proven to be effective at improving the average accuracy of the predictions made by humans, by restricting human agency, they may cause harm$\\unicode{x2014}$a human who has succeeded at predicting the ground-truth label of an instance on their own may have failed had they used these systems. In this paper, our goal is to control how frequently a decision support system based on prediction sets may cause harm, by design. To this end, we start by characterizing the above notion of harm using the theoretical framework of structural causal models. Then, we show that, under a natural, albeit unverifiable, monotonicity assumption, we can estimate how frequently a system may cause harm using only predictions made by humans on their own. Further, we also show that, under a weaker monotonicity assumption, which can be verified experimentally, we can bound how frequently a system may cause harm again using only predictions made by humans on their own. Building upon these assumptions, we introduce a computational framework to design decision support systems based on prediction sets that are guaranteed to cause harm less frequently than a user-specified value using conformal risk control. We validate our framework using real human predictions from two different human subject studies and show that, in decision support systems based on prediction sets, there is a trade-off between accuracy and counterfactual harm.         ",
    "url": "https://arxiv.org/abs/2406.06671",
    "authors": [
      "Eleni Straitouri",
      "Suhas Thejaswi",
      "Manuel Gomez Rodriguez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2406.06699",
    "title": "In-Context Learning and Fine-Tuning GPT for Argument Mining",
    "abstract": "           Large Language Models (LLMs) have become ubiquitous in NLP and deep learning. In-Context Learning (ICL) has been suggested as a bridging paradigm between the training-free and fine-tuning LLMs settings. In ICL, an LLM is conditioned to solve tasks by means of a few solved demonstration examples included as prompt. Argument Mining (AM) aims to extract the complex argumentative structure of a text, and Argument Type Classification (ATC) is an essential sub-task of AM. We introduce an ICL strategy for ATC combining kNN-based examples selection and majority vote ensembling. In the training-free ICL setting, we show that GPT-4 is able to leverage relevant information from only a few demonstration examples and achieve very competitive classification accuracy on ATC. We further set up a fine-tuning strategy incorporating well-crafted structural features given directly in textual form. In this setting, GPT-3.5 achieves state-of-the-art performance on ATC. Overall, these results emphasize the emergent ability of LLMs to grasp global discursive flow in raw text in both off-the-shelf and fine-tuned setups.         ",
    "url": "https://arxiv.org/abs/2406.06699",
    "authors": [
      "J\u00e9r\u00e9mie Cabessa",
      "Hugo Hernault",
      "Umer Mushtaq"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06703",
    "title": "Video-based Exercise Classification and Activated Muscle Group Prediction with Hybrid X3D-SlowFast Network",
    "abstract": "           This paper introduces a simple yet effective strategy for exercise classification and muscle group activation prediction (MGAP). These tasks have significant implications for personal fitness, facilitating more affordable, accessible, safer, and simpler exercise routines. This is particularly relevant for novices and individuals with disabilities. Previous research in the field is mostly dominated by the reliance on mounted sensors and a limited scope of exercises, reducing practicality for everyday use. Furthermore, existing MGAP methodologies suffer from a similar dependency on sensors and a restricted range of muscle groups, often excluding strength training exercises, which are pivotal for a comprehensive fitness regimen. Addressing these limitations, our research employs a video-based deep learning framework that encompasses a broad spectrum of exercises and muscle groups, including those vital for strength training. Utilizing the \"Workout/Exercises Video\" dataset, our approach integrates the X3D and SlowFast video activity recognition models in an effective way to enhance exercise classification and MGAP performance. Our findings demonstrate that this hybrid method obtained via weighted ensemble outperforms existing baseline models in accuracy. Pretrained models play a crucial role in enhancing overall performance, with optimal channel reduction values for the SlowFast model identified near 10. Through an ablation study that explores fine-tuning, we further elucidate the interrelation between the two tasks. Our composite model, a weighted-average ensemble of X3D and SlowFast, sets a new benchmark in both exercise classification and MGAP across all evaluated categories, offering a robust solution to the limitations of previous approaches.         ",
    "url": "https://arxiv.org/abs/2406.06703",
    "authors": [
      "Manvik Pasula",
      "Pramit Saha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06732",
    "title": "Mining for sustainability in cloud architecture among the discussions of software practitioners: building a dataset",
    "abstract": "           The adoption of cloud computing is steadily increasing in designing and implementing software systems, thus it becomes imperative to consider the sustainability implications of these processes. While there has already been some academic research on this topic, there is a lack of perspective from practitioners. To bridge this gap, we utilize software repository mining techniques to examine 192 discussions among practitioners on the Software Engineering forum of the StackExchange platform, aiming to build an annotated dataset containing cloud architectural discussions and to understand the current discussion on sustainability in cloud architecture. To identify these discussions, we first put together a list of terms indicating sustainability as the topic. Our initial findings indicate practitioners mainly focus on design aspects (analysis, synthesis, and implementation) while avoiding complex activities like evaluation and maintenance. Technical sustainability is emphasized, while the economic dimension has the most discussions exclusively focused on it. This contrasts with previous academic literature, which highlighted environmental sustainability.         ",
    "url": "https://arxiv.org/abs/2406.06732",
    "authors": [
      "Sahar Ahmadisakha",
      "Vasilios Andrikopoulos"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2406.06742",
    "title": "An Elliptic Kernel Unsupervised Autoencoder-Graph Convolutional Network Ensemble Model for Hyperspectral Unmixing",
    "abstract": "           Spectral Unmixing is an important technique in remote sensing used to analyze hyperspectral images to identify endmembers and estimate abundance maps. Over the past few decades, performance of techniques for endmember extraction and fractional abundance map estimation have significantly improved. This article presents an ensemble model workflow called Autoencoder Graph Ensemble Model (AEGEM) designed to extract endmembers and fractional abundance maps. An elliptical kernel is applied to measure spectral distances, generating the adjacency matrix within the elliptical neighborhood. This information is used to construct an elliptical graph, with centroids as senders and remaining pixels within the geometry as receivers. The next step involves stacking abundance maps, senders, and receivers as inputs to a Graph Convolutional Network, which processes this input to refine abundance maps. Finally, an ensemble decision-making process determines the best abundance maps based on root mean square error metric. The proposed AEGEM is assessed with benchmark datasets such as Samson, Jasper, and Urban, outperforming results obtained by baseline algorithms. For the Samson dataset, AEGEM excels in three abundance maps: water, tree and soil yielding values of 0.081, 0.158, and 0.182, respectively. For the Jasper dataset, results are improved for the tree and water endmembers with values of 0.035 and 0.060 in that order, as well as for the mean average of the spectral angle distance metric 0.109. For the Urban dataset, AEGEM outperforms previous results for the abundance maps of roof and asphalt, achieving values of 0.135 and 0.240, respectively. Additionally, for the endmembers of grass and roof, AEGEM achieves values of 0.063 and 0.094.         ",
    "url": "https://arxiv.org/abs/2406.06742",
    "authors": [
      "Estefania Alfaro-Mejia",
      "Carlos J Delgado",
      "Vidya Manian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2406.06744",
    "title": "A Multi-module Robust Method for Transient Stability Assessment against False Label Injection Cyberattacks",
    "abstract": "           The success of deep learning in transient stability assessment (TSA) heavily relies on high-quality training data. However, the label information in TSA datasets is vulnerable to contamination through false label injection (FLI) cyberattacks, resulting in degraded performance of deep TSA models. To address this challenge, a Multi-Module Robust TSA method (MMR) is proposed to rectify the supervised training process misguided by FLI in an unsupervised manner. In MMR, a supervised classification module and an unsupervised clustering module are alternatively trained to improve the clustering friendliness of representation leaning, thereby achieving accurate clustering assignments. Leveraging the clustering assignments, we construct a training label corrector to rectify the injected false labels and progressively enhance robustness and resilience against FLI. However, there is still a gap on accuracy and convergence speed between MMR and FLI-free deep TSA models. To narrow this gap, we further propose a human-in-the-loop training strategy, named MMR-HIL. In MMR-HIL, potential false samples can be detected by modeling the training loss with a Gaussian distribution. From these samples, the most likely false samples and most ambiguous samples are re-labeled by a TSA experts guided bi-directional annotator and then subjected to penalized optimization, aimed at improving accuracy and convergence speed. Extensive experiments indicate that MMR and MMR-HIL both exhibit powerful robustness against FLI in TSA performance. Moreover, the contaminated labels can also be effectively corrected, demonstrating superior resilience of the proposed methods.         ",
    "url": "https://arxiv.org/abs/2406.06744",
    "authors": [
      "Hanxuan Wang",
      "Na Lu",
      "Yinhong Liu",
      "Zhuqing Wang",
      "Zixuan Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2406.06746",
    "title": "Multi-Objective Neural Architecture Search for In-Memory Computing",
    "abstract": "           In this work, we employ neural architecture search (NAS) to enhance the efficiency of deploying diverse machine learning (ML) tasks on in-memory computing (IMC) architectures. Initially, we design three fundamental components inspired by the convolutional layers found in VGG and ResNet models. Subsequently, we utilize Bayesian optimization to construct a convolutional neural network (CNN) model with adaptable depths, employing these components. Through the Bayesian search algorithm, we explore a vast search space comprising over 640 million network configurations to identify the optimal solution, considering various multi-objective cost functions like accuracy/latency and accuracy/energy. Our evaluation of this NAS approach for IMC architecture deployment spans three distinct image classification datasets, demonstrating the effectiveness of our method in achieving a balanced solution characterized by high accuracy and reduced latency and energy consumption.         ",
    "url": "https://arxiv.org/abs/2406.06746",
    "authors": [
      "Md Hasibul Amin",
      "Mohammadreza Mohammadi",
      "Ramtin Zand"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2406.06751",
    "title": "Complexity-Aware Deep Symbolic Regression with Robust Risk-Seeking Policy Gradients",
    "abstract": "           This paper proposes a novel deep symbolic regression approach to enhance the robustness and interpretability of data-driven mathematical expression discovery. Despite the success of the state-of-the-art method, DSR, it is built on recurrent neural networks, purely guided by data fitness, and potentially meet tail barriers, which can zero out the policy gradient and cause inefficient model updates. To overcome these limitations, we use transformers in conjunction with breadth-first-search to improve the learning performance. We use Bayesian information criterion (BIC) as the reward function to explicitly account for the expression complexity and optimize the trade-off between interpretability and data fitness. We propose a modified risk-seeking policy that not only ensures the unbiasness of the gradient, but also removes the tail barriers, thus ensuring effective updates from top performers. Through a series of benchmarks and systematic experiments, we demonstrate the advantages of our approach.         ",
    "url": "https://arxiv.org/abs/2406.06751",
    "authors": [
      "Zachary Bastiani",
      "Robert M. Kirby",
      "Jacob Hochhalter",
      "Shandian Zhe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.06754",
    "title": "Incremental Sliding Window Connectivity over Streaming Graphs",
    "abstract": "           We study index-based processing for connectivity queries within sliding windows on streaming graphs. These queries, which determine whether two vertices belong to the same connected component, are fundamental operations in real-time graph data processing and demand high throughput and low latency. While indexing methods that leverage data structures for fully dynamic connectivity can facilitate efficient query processing, they encounter significant challenges with deleting expired edges from the window during window updates. We introduce a novel indexing approach that eliminates the need for physically performing edge deletions. This is achieved through a unique bidirectional incremental computation framework, referred to as the BIC model. The BIC model implements two distinct incremental computations to compute connected components within the window, operating along and against the timeline, respectively. These computations are then merged to efficiently compute queries in the window. We propose techniques for optimized index storage, incremental index updates, and efficient query processing to improve BIC effectiveness. Empirically, BIC achieves a 14$\\times$ increase in throughput and a reduction in P95 latency by up to 3900$\\times$ when compared to state-of-the-art indexes.         ",
    "url": "https://arxiv.org/abs/2406.06754",
    "authors": [
      "Chao Zhang",
      "Angela Bonifati",
      "M. Tamer \u00d6zsu"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2406.06792",
    "title": "Reinforced Compressive Neural Architecture Search for Versatile Adversarial Robustness",
    "abstract": "           Prior neural architecture search (NAS) for adversarial robustness works have discovered that a lightweight and adversarially robust neural network architecture could exist in a non-robust large teacher network, generally disclosed by heuristic rules through statistical analysis and neural architecture search, generally disclosed by heuristic rules from neural architecture search. However, heuristic methods cannot uniformly handle different adversarial attacks and \"teacher\" network capacity. To solve this challenge, we propose a Reinforced Compressive Neural Architecture Search (RC-NAS) for Versatile Adversarial Robustness. Specifically, we define task settings that compose datasets, adversarial attacks, and teacher network information. Given diverse tasks, we conduct a novel dual-level training paradigm that consists of a meta-training and a fine-tuning phase to effectively expose the RL agent to diverse attack scenarios (in meta-training), and making it adapt quickly to locate a sub-network (in fine-tuning) for any previously unseen scenarios. Experiments show that our framework could achieve adaptive compression towards different initial teacher networks, datasets, and adversarial attacks, resulting in more lightweight and adversarially robust architectures.         ",
    "url": "https://arxiv.org/abs/2406.06792",
    "authors": [
      "Dingrong Wang",
      "Hitesh Sapkota",
      "Zhiqiang Tao",
      "Qi Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.06796",
    "title": "FlexLoc: Conditional Neural Networks for Zero-Shot Sensor Perspective Invariance in Object Localization with Distributed Multimodal Sensors",
    "abstract": "           Localization is a critical technology for various applications ranging from navigation and surveillance to assisted living. Localization systems typically fuse information from sensors viewing the scene from different perspectives to estimate the target location while also employing multiple modalities for enhanced robustness and accuracy. Recently, such systems have employed end-to-end deep neural models trained on large datasets due to their superior performance and ability to handle data from diverse sensor modalities. However, such neural models are often trained on data collected from a particular set of sensor poses (i.e., locations and orientations). During real-world deployments, slight deviations from these sensor poses can result in extreme inaccuracies. To address this challenge, we introduce FlexLoc, which employs conditional neural networks to inject node perspective information to adapt the localization pipeline. Specifically, a small subset of model weights are derived from node poses at run time, enabling accurate generalization to unseen perspectives with minimal additional overhead. Our evaluations on a multimodal, multiview indoor tracking dataset showcase that FlexLoc improves the localization accuracy by almost 50% in the zero-shot case (no calibration data available) compared to the baselines. The source code of FlexLoc is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.06796",
    "authors": [
      "Jason Wu",
      "Ziqi Wang",
      "Xiaomin Ouyang",
      "Ho Lyun Jeong",
      "Colin Samplawski",
      "Lance Kaplan",
      "Benjamin Marlin",
      "Mani Srivastava"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2406.06808",
    "title": "Fast White-Box Adversarial Streaming Without a Random Oracle",
    "abstract": "           Recently, the question of adversarially robust streaming, where the stream is allowed to depend on the randomness of the streaming algorithm, has gained a lot of attention. In this work, we consider a strong white-box adversarial model (Ajtai et al. PODS 2022), in which the adversary has access to all past random coins and the parameters used by the streaming algorithm. We focus on the sparse recovery problem and extend our result to other tasks such as distinct element estimation and low-rank approximation of matrices and tensors. The main drawback of previous work is that it requires a random oracle, which is especially problematic in the streaming model since the amount of randomness is counted in the space complexity of a streaming algorithm. Also, the previous work suffers from large update time. We construct a near-optimal solution for the sparse recovery problem in white-box adversarial streams, based on the subexponentially secure Learning with Errors assumption. Importantly, our solution does not require a random oracle and has a polylogarithmic per item processing time. We also give results in a related white-box adversarially robust distributed model. Our constructions are based on homomorphic encryption schemes satisfying very mild structural properties that are currently satisfied by most known schemes.         ",
    "url": "https://arxiv.org/abs/2406.06808",
    "authors": [
      "Ying Feng",
      "Aayush Jain",
      "David P. Woodruff"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06812",
    "title": "On Learning what to Learn: heterogeneous observations of dynamics and establishing (possibly causal) relations among them",
    "abstract": "           Before we attempt to learn a function between two (sets of) observables of a physical process, we must first decide what the inputs and what the outputs of the desired function are going to be. Here we demonstrate two distinct, data-driven ways of initially deciding ``the right quantities'' to relate through such a function, and then proceed to learn it. This is accomplished by processing multiple simultaneous heterogeneous data streams (ensembles of time series) from observations of a physical system: multiple observation processes of the system. We thus determine (a) what subsets of observables are common between the observation processes (and therefore observable from each other, relatable through a function); and (b) what information is unrelated to these common observables, and therefore particular to each observation process, and not contributing to the desired function. Any data-driven function approximation technique can subsequently be used to learn the input-output relation, from k-nearest neighbors and Geometric Harmonics to Gaussian Processes and Neural Networks. Two particular ``twists'' of the approach are discussed. The first has to do with the identifiability of particular quantities of interest from the measurements. We now construct mappings from a single set of observations of one process to entire level sets of measurements of the process, consistent with this single set. The second attempts to relate our framework to a form of causality: if one of the observation processes measures ``now'', while the second observation process measures ``in the future'', the function to be learned among what is common across observation processes constitutes a dynamical model for the system evolution.         ",
    "url": "https://arxiv.org/abs/2406.06812",
    "authors": [
      "David W. Sroczynski",
      "Felix Dietrich",
      "Eleni D. Koronaki",
      "Ronen Talmon",
      "Ronald R. Coifman",
      "Erik Bollt",
      "Ioannis G. Kevrekidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2406.06814",
    "title": "Temporal Link Prediction in Social Networks Based on Agent Behavior Synchrony and a Cognitive Mechanism",
    "abstract": "           Temporality, a crucial characteristic in the formation of social relationships, was used to quantify the long-term time effects of networks for link prediction models, ignoring the heterogeneity of time effects on different time scales. In this work, we propose a novel approach to link prediction in temporal networks, extending existing methods with a cognitive mechanism that captures the dynamics of the interactions. Our approach computes the weight of the edges and their change over time, similar to memory traces in the human brain, by simulating the process of forgetting and strengthening connections depending on the intensity of interactions. We utilized five ground-truth datasets, which were used to predict social ties, missing events, and potential links. We found: (a) the cognitive mechanism enables more accurate capture of the heterogeneity of the temporal effect, leading to an average precision improvement of 9\\% compared to baselines with competitive AUC. (b) the local structure and synchronous agent behavior contribute differently to different types of datasets. (c) appropriately increasing the time intervals, which may reduce the negative impact from noise when dividing time windows to calculate the behavioral synchrony of agents, is effective for link prediction tasks.         ",
    "url": "https://arxiv.org/abs/2406.06814",
    "authors": [
      "Yueran Duan",
      "Mateusz Nurek",
      "Qing Guan",
      "Rados\u0142aw Michalski",
      "Petter Holme"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2406.06818",
    "title": "Conformal Prediction for Class-wise Coverage via Augmented Label Rank Calibration",
    "abstract": "           Conformal prediction (CP) is an emerging uncertainty quantification framework that allows us to construct a prediction set to cover the true label with a pre-specified marginal or conditional probability. Although the valid coverage guarantee has been extensively studied for classification problems, CP often produces large prediction sets which may not be practically useful. This issue is exacerbated for the setting of class-conditional coverage on imbalanced classification tasks. This paper proposes the Rank Calibrated Class-conditional CP (RC3P) algorithm to reduce the prediction set sizes to achieve class-conditional coverage, where the valid coverage holds for each class. In contrast to the standard class-conditional CP (CCP) method that uniformly thresholds the class-wise conformity score for each class, the augmented label rank calibration step allows RC3P to selectively iterate this class-wise thresholding subroutine only for a subset of classes whose class-wise top-k error is small. We prove that agnostic to the classifier and data distribution, RC3P achieves class-wise coverage. We also show that RC3P reduces the size of prediction sets compared to the CCP method. Comprehensive experiments on multiple real-world datasets demonstrate that RC3P achieves class-wise coverage and 26.25% reduction in prediction set sizes on average.         ",
    "url": "https://arxiv.org/abs/2406.06818",
    "authors": [
      "Yuanjie Shi",
      "Subhankar Ghosh",
      "Taha Belkhouja",
      "Janardhan Rao Doppa",
      "Yan Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06822",
    "title": "An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection",
    "abstract": "           Large Language Models (LLMs) have transformed code completion tasks, providing context-based suggestions to boost developer productivity in software engineering. As users often fine-tune these models for specific applications, poisoning and backdoor attacks can covertly alter the model outputs. To address this critical security challenge, we introduce CodeBreaker, a pioneering LLM-assisted backdoor attack framework on code completion models. Unlike recent attacks that embed malicious payloads in detectable or irrelevant sections of the code (e.g., comments), CodeBreaker leverages LLMs (e.g., GPT-4) for sophisticated payload transformation (without affecting functionalities), ensuring that both the poisoned data for fine-tuning and generated code can evade strong vulnerability detection. CodeBreaker stands out with its comprehensive coverage of vulnerabilities, making it the first to provide such an extensive set for evaluation. Our extensive experimental evaluations and user studies underline the strong attack performance of CodeBreaker across various settings, validating its superiority over existing approaches. By integrating malicious payloads directly into the source code with minimal transformation, CodeBreaker challenges current security measures, underscoring the critical need for more robust defenses for code completion.         ",
    "url": "https://arxiv.org/abs/2406.06822",
    "authors": [
      "Shenao Yan",
      "Shen Wang",
      "Yue Duan",
      "Hanbin Hong",
      "Kiho Lee",
      "Doowon Kim",
      "Yuan Hong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2406.06829",
    "title": "Personalized Binomial DAGs Learning with Network Structured Covariates",
    "abstract": "           The causal dependence in data is often characterized by Directed Acyclic Graphical (DAG) models, widely used in many areas. Causal discovery aims to recover the DAG structure using observational data. This paper focuses on causal discovery with multi-variate count data. We are motivated by real-world web visit data, recording individual user visits to multiple websites. Building a causal diagram can help understand user behavior in transitioning between websites, inspiring operational strategy. A challenge in modeling is user heterogeneity, as users with different backgrounds exhibit varied behaviors. Additionally, social network connections can result in similar behaviors among friends. We introduce personalized Binomial DAG models to address heterogeneity and network dependency between observations, which are common in real-world applications. To learn the proposed DAG model, we develop an algorithm that embeds the network structure into a dimension-reduced covariate, learns each node's neighborhood to reduce the DAG search space, and explores the variance-mean relation to determine the ordering. Simulations show our algorithm outperforms state-of-the-art competitors in heterogeneous data. We demonstrate its practical usefulness on a real-world web visit dataset.         ",
    "url": "https://arxiv.org/abs/2406.06829",
    "authors": [
      "Boxin Zhao",
      "Weishi Wang",
      "Dingyuan Zhu",
      "Ziqi Liu",
      "Dong Wang",
      "Zhiqiang Zhang",
      "Jun Zhou",
      "Mladen Kolar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.06852",
    "title": "A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures",
    "abstract": "           The large language models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings. Despite the demonstrable efficacy of LMMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms. However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and attacks without fine-tuning. Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms.         ",
    "url": "https://arxiv.org/abs/2406.06852",
    "authors": [
      "Shuai Zhao",
      "Meihuizi Jia",
      "Zhongliang Guo",
      "Leilei Gan",
      "Jie Fu",
      "Yichao Feng",
      "Fengjun Pan",
      "Luu Anh Tuan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.06870",
    "title": "What's in an embedding? Would a rose by any embedding smell as sweet?",
    "abstract": "           Large Language Models (LLMs) are often criticized for lacking true \"understanding\" and an ability to \"reason\" with their knowledge, being seen merely as advanced autocomplete systems. We believe that this perspective might be missing an important insight. We suggest that LLMs do develop a kind of empirical \"understanding\" that is \"geometry\"-like, which seems quite sufficient for a range of applications in NLP, computer vision, coding assistance, etc. However, this \"geometric\" understanding, built from incomplete and noisy data, makes them unreliable, difficult to generalize, and lacking in inference capabilities and explanations, similar to the challenges faced by heuristics-based expert systems decades ago. To overcome these limitations, we suggest that LLMs should be integrated with an \"algebraic\" representation of knowledge that includes symbolic AI elements used in expert systems. This integration aims to create large knowledge models (LKMs) that not only possess \"deep\" knowledge grounded in first principles, but also have the ability to reason and explain, mimicking human expert capabilities. To harness the full potential of generative AI safely and effectively, a paradigm shift from LLMs to the more comprehensive LKMs is needed.         ",
    "url": "https://arxiv.org/abs/2406.06870",
    "authors": [
      "Venkat Venkatasubramanian"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.06887",
    "title": "PLUM: Preference Learning Plus Test Cases Yields Better Code Language Models",
    "abstract": "           Instruction-finetuned code language models (LMs) have shown promise in various programming tasks. They are trained, using a language modeling objective, on natural language instructions and gold code snippet pairs. Recent evidence suggests that these models, never exposed to incorrect solutions during training, often struggle to distinguish between correct and incorrect solutions. This observation raises our inquiry: Can preference learning, which trains models to prefer correct solutions over incorrect ones, help push the boundaries of code LMs even further? We propose PLUM, a novel \\textbf{p}reference \\textbf{l}earning framework a\\textbf{u}gmented with test cases tailored for code L\\textbf{M}s.PLUM aims to investigate the key success factors and potential benefits of preference learning in code LMs, which remain elusive despite its success in aligning LMs with human values. PLUM consists of three stages: (1) Generating test cases for natural language instructions, (2) sampling candidate solutions from the policy and evaluating them against the test cases to create a preference dataset, which is then used to (3) train the policy with a preference learning algorithm. Experiments demonstrate that PLUM substantially improves the performance of existing code LMs on established code generation benchmarks such as HumanEval (+) and MBPP (+), even for the state-of-the-art open-source language model CodeQwen-1.5-7B-Chat. PLUM complements the supervised fine-tuning (SFT) stage, demonstrating synergistic effects.         ",
    "url": "https://arxiv.org/abs/2406.06887",
    "authors": [
      "Dylan Zhang",
      "Shizhe Diao",
      "Xueyan Zou",
      "Hao Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2406.06894",
    "title": "Nonlinear time-series embedding by monotone variational inequality",
    "abstract": "           In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a new method to learn low-dimensional representations of nonlinear time series without supervision and can have provable recovery guarantees. The learned representation can be used for downstream machine-learning tasks such as clustering and classification. The method is based on the assumption that the observed sequences arise from a common domain, but each sequence obeys its own autoregressive models that are related to each other through low-rank regularization. We cast the problem as a computationally efficient convex matrix parameter recovery problem using monotone Variational Inequality and encode the common domain assumption via low-rank constraint across the learned representations, which can learn the geometry for the entire domain as well as faithful representations for the dynamics of each individual sequence using the domain information in totality. We show the competitive performance of our method on real-world time-series data with the baselines and demonstrate its effectiveness for symbolic text modeling and RNA sequence clustering.         ",
    "url": "https://arxiv.org/abs/2406.06894",
    "authors": [
      "Jonathan Y. Zhou",
      "Yao Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.06902",
    "title": "CodeScore-R: An Automated Robustness Metric for Assessing the FunctionalCorrectness of Code Synthesis",
    "abstract": "           Evaluation metrics are crucial in the field of code synthesis. Commonly used code evaluation metrics canbe classified into three types: match-based, semantic-based, and execution-based. Among them, the execution-basedPass@k metric accurately assesses the functionality of predicted code by executing test cases. However, calculatingthis metric requires a significant amount of overhead, necessitating the design of an automated evaluation metric thatcan assess the functionality of predicted code without the need for test cases. Additionally, a good evaluation metricshould be robust, that is the metric can maintain its accuracy even when the predicted code undergoes minor this http URL address these challenges, we propose an automated robust metric, called CodeScore-R, based on UniXcoder andcontrastive learning, for evaluating the functionality of code synthesis. CodeScore-R employs techniques such assketch-based processing, syntactic-equivalent transformations, and mutation testing to effectively mitigate theinterference caused by identifiers, syntax structures, and operators on evaluation results. Experimental resultsdemonstrate that in the tasks of code generation and migration in Java and Python, CodeScore-R outperforms otherevaluation metrics and is more closely aligned with the Pass@k metric, while exhibiting stronger robustness.         ",
    "url": "https://arxiv.org/abs/2406.06902",
    "authors": [
      "Guang Yang",
      "Yu Zhou",
      "Xiang Chen",
      "Xiangyu Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2406.06918",
    "title": "Towards more realistic evaluation of LLM-based code generation: an experimental study and beyond",
    "abstract": "           To evaluate the code generation capabilities of Large Language Models (LLMs) in complex real-world software development scenarios, many evaluation approaches have been developed. They typically leverage contextual code from the latest version of a project to facilitate LLMs in accurately generating the desired function. However, such evaluation approaches fail to consider the dynamic evolution of software projects over time, which we refer to as evolving-ignored situation, leading to issues of future context leakage and useful context missing. This in turn results in inaccurate evaluation of LLMs' performance. In this paper, we conduct an empirical study to deeply understand LLMs' code generation performance within settings that reflect the evolving nature of software development. To achieve this, we first construct an evolving-aware repository-level code generation dataset, namely HumanEvo, equipped with an automated execution-based evaluation tool. Second, we manually categorize HumanEvo according to dependency levels to more comprehensively analyze the model's performance in generating functions with different dependency levels. Third, we conduct extensive experiments on HumanEvo with seven representative and diverse LLMs to verify the effectiveness of the proposed benchmark. We obtain many important findings through our experimental study. For example, we find that previous evolving-ignored evaluation approaches lead to inflated performance of the LLMs, ranging from 10.0% to 61.1%. Based on the findings, we give actionable suggestions on more realistic evaluation of LLMs on code generation. We also build a shared evolving-aware code generation toolbox to facilitate future research. Replication package including source code, datasets and appendix is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.06918",
    "authors": [
      "Dewu Zheng",
      "Yanlin Wang",
      "Ensheng Shi",
      "Ruikai Zhang",
      "Yuchi Ma",
      "Hongyu Zhang",
      "Zibin Zheng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2406.06929",
    "title": "Social Learning with Bounded Rationality: Negative Reviews Persist under Newest First",
    "abstract": "           We study a model of social learning from reviews where customers are computationally limited and make purchases based on reading only the first few reviews displayed by the platform. Under this bounded rationality, we establish that the review ordering policy can have a significant impact. In particular, the popular Newest First ordering induces a negative review to persist as the most recent review longer than a positive review. This phenomenon, which we term the Cost of Newest First, can make the long-term revenue unboundedly lower than a counterpart where reviews are exogenously drawn for each customer. We show that the impact of the Cost of Newest First can be mitigated under dynamic pricing, which allows the price to depend on the set of displayed reviews. Under the optimal dynamic pricing policy, the revenue loss is at most a factor of 2. On the way, we identify a structural property for this optimal dynamic pricing: the prices should ensure that the probability of a purchase is always the same, regardless of the state of reviews. We also study an extension of the model where customers put more weight on more recent reviews (and discount older reviews based on their time of posting), and we show that Newest First is still not the optimal ordering policy if customers discount slowly. Lastly, we corroborate our theoretical findings using a real-world review dataset. We find that the average rating of the first page of reviews is statistically significantly smaller than the overall average rating, which is in line with our theoretical results.         ",
    "url": "https://arxiv.org/abs/2406.06929",
    "authors": [
      "Jackie Baek",
      "Atanas Dinev",
      "Thodoris Lykouris"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2406.06930",
    "title": "Explaining Representation Learning with Perceptual Components",
    "abstract": "           Self-supervised models create representation spaces that lack clear semantic meaning. This interpretability problem of representations makes traditional explainability methods ineffective in this context. In this paper, we introduce a novel method to analyze representation spaces using three key perceptual components: color, shape, and texture. We employ selective masking of these components to observe changes in representations, resulting in distinct importance maps for each. In scenarios, where labels are absent, these importance maps provide more intuitive explanations as they are integral to the human visual system. Our approach enhances the interpretability of the representation space, offering explanations that resonate with human visual perception. We analyze how different training objectives create distinct representation spaces using perceptual components. Additionally, we examine the representation of images across diverse image domains, providing insights into the role of these components in different contexts.         ",
    "url": "https://arxiv.org/abs/2406.06930",
    "authors": [
      "Yavuz Yarici",
      "Kiran Kokilepersaud",
      "Mohit Prabhushankar",
      "Ghassan AlRegib"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.06934",
    "title": "Decentralized Social Networks and the Future of Free Speech Online",
    "abstract": "           Decentralized social networks like Mastodon and BlueSky are trending topics that have drawn much attention and discussion in recent years. By devolving powers from the central node to the end users, decentralized social networks aim to cure existing pathologies on the centralized platforms and have been viewed by many as the future of the Internet. This article critically and systematically assesses the decentralization project's prospect for communications online. It uses normative theories of free speech to examine whether and how the decentralization design could facilitate users' freedom of expression online. The analysis shows that both promises and pitfalls exist, highlighting the importance of value-based design in this area. Two most salient issues for the design of the decentralized networks are: how to balance the decentralization ideal with constant needs of centralization on the network, and how to empower users to make them truly capable of exercising their control. The article then uses some design examples, such as the shared blocklist and the opt-in search function, to illustrate the value considerations underlying the design choices. Some tentative proposals for law and policy interventions are offered to better facilitate the design of the new network. Rather than providing clear answers, the article seeks to map the value implications of the design choices, highlight the stakes, and point directions for future research.         ",
    "url": "https://arxiv.org/abs/2406.06934",
    "authors": [
      "Tao Huang"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Emerging Technologies (cs.ET)",
      "Networking and Internet Architecture (cs.NI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2406.06943",
    "title": "FAULT+PROBE: A Generic Rowhammer-based Bit Recovery Attack",
    "abstract": "           Rowhammer is a security vulnerability that allows unauthorized attackers to induce errors within DRAM cells. To prevent fault injections from escalating to successful attacks, a widely accepted mitigation is implementing fault checks on instructions and data. We challenge the validity of this assumption by examining the impact of the fault on the victim's functionality. Specifically, we illustrate that an attacker can construct a profile of the victim's memory based on the directional patterns of bit flips. This profile is then utilized to identify the most susceptible bit locations within DRAM rows. These locations are then subsequently leveraged during an online attack phase with side information observed from the change in the victim's behavior to deduce sensitive bit values. Consequently, the primary objective of this study is to utilize Rowhammer as a probe, shifting the emphasis away from the victim's memory integrity and toward statistical fault analysis (SFA) based on the victim's operational behavior. We show FAULT+PROBE may be used to circumvent the verify-after-sign fault check mechanism, which is designed to prevent the generation of erroneous signatures that leak sensitive information. It does so by injecting directional faults into key positions identified during a memory profiling stage. The attacker observes the signature generation rate and decodes the secret bit value accordingly. This circumvention is enabled by an observable channel in the victim. FAULT+PROBE is not limited to signing victims and can be used to probe secret bits on arbitrary systems where an observable channel is present that leaks the result of the fault injection attempt. To demonstrate the attack, we target the fault-protected ECDSA in wolfSSL's implementation of the TLS 1.3 handshake. We recover 256-bit session keys with an average recovery rate of 22 key bits/hour and a 100% success rate.         ",
    "url": "https://arxiv.org/abs/2406.06943",
    "authors": [
      "Kemal Derya",
      "M. Caner Tol",
      "Berk Sunar"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2406.06948",
    "title": "Neural Visibility Field for Uncertainty-Driven Active Mapping",
    "abstract": "           This paper presents Neural Visibility Field (NVF), a novel uncertainty quantification method for Neural Radiance Fields (NeRF) applied to active mapping. Our key insight is that regions not visible in the training views lead to inherently unreliable color predictions by NeRF at this region, resulting in increased uncertainty in the synthesized views. To address this, we propose to use Bayesian Networks to composite position-based field uncertainty into ray-based uncertainty in camera observations. Consequently, NVF naturally assigns higher uncertainty to unobserved regions, aiding robots to select the most informative next viewpoints. Extensive evaluations show that NVF excels not only in uncertainty quantification but also in scene reconstruction for active mapping, outperforming existing methods.         ",
    "url": "https://arxiv.org/abs/2406.06948",
    "authors": [
      "Shangjie Xue",
      "Jesse Dill",
      "Pranay Mathur",
      "Frank Dellaert",
      "Panagiotis Tsiotra",
      "Danfei Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2406.06949",
    "title": "Triple-domain Feature Learning with Frequency-aware Memory Enhancement for Moving Infrared Small Target Detection",
    "abstract": "           Moving infrared small target detection presents significant challenges due to tiny target sizes and low contrast against backgrounds. Currently-existing methods primarily focus on extracting target features only from the spatial-temporal domain. For further enhancing feature representation, more information domains such as frequency are believed to be potentially valuable. To extend target feature learning, we propose a new Triple-domain Strategy (Tridos) with the frequency-aware memory enhancement on the spatial-temporal domain. In our scheme, it effectively detaches and enhances frequency features by a local-global frequency-aware module with Fourier transform. Inspired by the human visual system, our memory enhancement aims to capture the target spatial relations between video frames. Furthermore, it encodes temporal dynamics motion features via differential learning and residual enhancing. Additionally, we further design a residual compensation unit to reconcile possible cross-domain feature mismatches. To our best knowledge, our Tridos is the first work to explore target feature learning comprehensively in spatial-temporal-frequency domains. The extensive experiments on three datasets (DAUB, ITSDT-15K, and IRDST) validate that our triple-domain learning scheme could be obviously superior to state-of-the-art ones. Source codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.06949",
    "authors": [
      "Weiwei Duan",
      "Luping Ji",
      "Shengjia Chen",
      "Sicheng Zhu",
      "Mao Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.06950",
    "title": "A Probabilistic Framework for LLM Hallucination Detection via Belief Tree Propagation",
    "abstract": "           This paper focuses on the task of hallucination detection, which aims to determine the truthfulness of LLM-generated statements. To address this problem, a popular class of methods utilize the LLM's self-consistencies in its beliefs in a set of logically related augmented statements generated by the LLM, which does not require external knowledge databases and can work with both white-box and black-box LLMs. However, in many existing approaches, the augmented statements tend to be very monotone and unstructured, which makes it difficult to integrate meaningful information from the LLM beliefs in these statements. Also, many methods work with the binarized version of the LLM's belief, instead of the continuous version, which significantly loses information. To overcome these limitations, in this paper, we propose Belief Tree Propagation (BTProp), a probabilistic framework for LLM hallucination detection. BTProp introduces a belief tree of logically related statements by recursively decomposing a parent statement into child statements with three decomposition strategies, and builds a hidden Markov tree model to integrate the LLM's belief scores in these statements in a principled way. Experiment results show that our method improves baselines by 3%-9% (evaluated by AUROC and AUC-PR) on multiple hallucination detection benchmarks. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.06950",
    "authors": [
      "Bairu Hou",
      "Yang Zhang",
      "Jacob Andreas",
      "Shiyu Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.06953",
    "title": "Stepwise Regression and Pre-trained Edge for Robust Stereo Matching",
    "abstract": "           Due to the difficulty in obtaining real samples and ground truth, the generalization performance and the fine-tuned performance are critical for the feasibility of stereo matching methods in real-world applications. However, the presence of substantial disparity distributions and density variations across different datasets presents significant challenges for the generalization and fine-tuning of the model. In this paper, we propose a novel stereo matching method, called SR-Stereo, which mitigates the distributional differences across different datasets by predicting the disparity clips and uses a loss weight related to the regression target scale to improve the accuracy of the disparity clips. Moreover, this stepwise regression architecture can be easily extended to existing iteration-based methods to improve the performance without changing the structure. In addition, to mitigate the edge blurring of the fine-tuned model on sparse ground truth, we propose Domain Adaptation Based on Pre-trained Edges (DAPE). Specifically, we use the predicted disparity and RGB image to estimate the edge map of the target domain image. The edge map is filtered to generate edge map background pseudo-labels, which together with the sparse ground truth disparity on the target domain are used as a supervision to jointly fine-tune the pre-trained stereo matching model. These proposed methods are extensively evaluated on SceneFlow, KITTI, Middbury 2014 and ETH3D. The SR-Stereo achieves competitive disparity estimation performance and state-of-the-art cross-domain generalisation performance. Meanwhile, the proposed DAPE significantly improves the disparity estimation performance of fine-tuned models, especially in the textureless and detail regions.         ",
    "url": "https://arxiv.org/abs/2406.06953",
    "authors": [
      "Weiqing Xiao",
      "Wei Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.06958",
    "title": "Turning the Tide on Dark Pools? Towards Multi-Stakeholder Vulnerability Notifications in the Ad-Tech Supply Chain",
    "abstract": "           Online advertising relies on a complex and opaque supply chain that involves multiple stakeholders, including advertisers, publishers, and ad-networks, each with distinct and sometimes conflicting incentives. Recent research has demonstrated the existence of ad-tech supply chain vulnerabilities such as dark pooling, where low-quality publishers bundle their ad inventory with higher-quality ones to mislead advertisers. We investigate the effectiveness of vulnerability notification campaigns aimed at mitigating dark pooling. Prior research on vulnerability notifications has primarily focused on single-stakeholder scenarios, and it is unclear whether vulnerability notifications can be effective in the multi-stakeholder ad-tech supply chain. We implement an automated vulnerability notification pipeline to systematically evaluate the responsiveness of various stakeholders, including publishers, ad-networks, and advertisers to vulnerability notifications by academics and activists. Our nine-month long multi-stakeholder notification study shows that notifications are an effective method for reducing dark pooling vulnerabilities in the online advertising ecosystem, especially when targeted towards ad-networks. Further, the sender reputation does not impact responses to notifications from activists and academics in a statistically different way. In addition to being the first notification study targeting the online advertising ecosystem, we are also the first to study multi-stakeholder context in vulnerability notifications.         ",
    "url": "https://arxiv.org/abs/2406.06958",
    "authors": [
      "Yash Vekaria",
      "Rishab Nithyanand",
      "Zubair Shafiq"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Multiagent Systems (cs.MA)",
      "Networking and Internet Architecture (cs.NI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2406.06964",
    "title": "Missingness-resilient Video-enhanced Multimodal Disfluency Detection",
    "abstract": "           Most existing speech disfluency detection techniques only rely upon acoustic data. In this work, we present a practical multimodal disfluency detection approach that leverages available video data together with audio. We curate an audiovisual dataset and propose a novel fusion technique with unified weight-sharing modality-agnostic encoders to learn the temporal and semantic context. Our resilient design accommodates real-world scenarios where the video modality may sometimes be missing during inference. We also present alternative fusion strategies when both modalities are assured to be complete. In experiments across five disfluency-detection tasks, our unified multimodal approach significantly outperforms Audio-only unimodal methods, yielding an average absolute improvement of 10% (i.e., 10 percentage point increase) when both video and audio modalities are always available, and 7% even when video modality is missing in half of the samples.         ",
    "url": "https://arxiv.org/abs/2406.06964",
    "authors": [
      "Payal Mohapatra",
      "Shamika Likhite",
      "Subrata Biswas",
      "Bashima Islam",
      "Qi Zhu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2406.06967",
    "title": "Dual Thinking and Perceptual Analysis of Deep Learning Models using Human Adversarial Examples",
    "abstract": "           The dual thinking framework considers fast, intuitive processing and slower, logical processing. The perception of dual thinking in vision requires images where inferences from intuitive and logical processing differ. We introduce an adversarial dataset to provide evidence for the dual thinking framework in human vision, which also aids in studying the qualitative behavior of deep learning models. Our study also addresses a major criticism of using classification models as computational models of human vision by using instance segmentation models that localize objects. The evidence underscores the importance of shape in identifying instances in human vision and shows that deep learning models lack an understanding of sub-structures, as indicated by errors related to the position and number of sub-components. Additionally, the similarity in errors made by models and intuitive human processing indicates that models only address intuitive thinking in human vision.         ",
    "url": "https://arxiv.org/abs/2406.06967",
    "authors": [
      "Kailas Dayanandan",
      "Anand Sinha",
      "Brejesh Lall"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2406.06968",
    "title": "Beyond the Norms: Detecting Prediction Errors in Regression Models",
    "abstract": "           This paper tackles the challenge of detecting unreliable behavior in regression algorithms, which may arise from intrinsic variability (e.g., aleatoric uncertainty) or modeling errors (e.g., model uncertainty). First, we formally introduce the notion of unreliability in regression, i.e., when the output of the regressor exceeds a specified discrepancy (or error). Then, using powerful tools for probabilistic modeling, we estimate the discrepancy density, and we measure its statistical diversity using our proposed metric for statistical dissimilarity. In turn, this allows us to derive a data-driven score that expresses the uncertainty of the regression outcome. We show empirical improvements in error detection for multiple regression tasks, consistently outperforming popular baseline approaches, and contributing to the broader field of uncertainty quantification and safe machine learning systems. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.06968",
    "authors": [
      "Andres Altieri",
      "Marco Romanelli",
      "Georg Pichler",
      "Florence Alberge",
      "Pablo Piantanida"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.06973",
    "title": "RWKV-CLIP: A Robust Vision-Language Representation Learner",
    "abstract": "           Contrastive Language-Image Pre-training (CLIP) has significantly improved performance in various vision-language tasks by expanding the dataset with image-text pairs obtained from websites. This paper further explores CLIP from the perspectives of data and model architecture. To address the prevalence of noisy data and enhance the quality of large-scale image-text data crawled from the internet, we introduce a diverse description generation framework that can leverage Large Language Models (LLMs) to synthesize and refine content from web-based texts, synthetic captions, and detection tags. Furthermore, we propose RWKV-CLIP, the first RWKV-driven vision-language representation learning model that combines the effective parallel training of transformers with the efficient inference of RNNs. Comprehensive experiments across various model scales and pre-training datasets demonstrate that RWKV-CLIP is a robust and efficient vision-language representation learner, it achieves state-of-the-art performance in several downstream tasks, including linear probe, zero-shot classification, and zero-shot image-text retrieval. To facilitate future research, the code and pre-trained models are released at this https URL ",
    "url": "https://arxiv.org/abs/2406.06973",
    "authors": [
      "Tiancheng Gu",
      "Kaicheng Yang",
      "Xiang An",
      "Ziyong Feng",
      "Dongnan Liu",
      "Weidong Cai",
      "Jiankang Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.06976",
    "title": "Discrete Dictionary-based Decomposition Layer for Structured Representation Learning",
    "abstract": "           Neuro-symbolic neural networks have been extensively studied to integrate symbolic operations with neural networks, thereby improving systematic generalization. Specifically, Tensor Product Representation (TPR) framework enables neural networks to perform differentiable symbolic operations by encoding the symbolic structure of data within vector spaces. However, TPR-based neural networks often struggle to decompose unseen data into structured TPR representations, undermining their symbolic operations. To address this decomposition problem, we propose a Discrete Dictionary-based Decomposition (D3) layer designed to enhance the decomposition capabilities of TPR-based models. D3 employs discrete, learnable key-value dictionaries trained to capture symbolic features essential for decomposition operations. It leverages the prior knowledge acquired during training to generate structured TPR representations by mapping input data to pre-learned symbolic features within these dictionaries. D3 is a straightforward drop-in layer that can be seamlessly integrated into any TPR-based model without modifications. Our experimental results demonstrate that D3 significantly improves the systematic generalization of various TPR-based models while requiring fewer additional parameters. Notably, D3 outperforms baseline models on the synthetic task that demands the systematic decomposition of unseen combinatorial data.         ",
    "url": "https://arxiv.org/abs/2406.06976",
    "authors": [
      "Taewon Park",
      "Hyun-Chul Kim",
      "Minho Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.06979",
    "title": "AudioMarkBench: Benchmarking Robustness of Audio Watermarking",
    "abstract": "           The increasing realism of synthetic speech, driven by advancements in text-to-speech models, raises ethical concerns regarding impersonation and disinformation. Audio watermarking offers a promising solution via embedding human-imperceptible watermarks into AI-generated audios. However, the robustness of audio watermarking against common/adversarial perturbations remains understudied. We present AudioMarkBench, the first systematic benchmark for evaluating the robustness of audio watermarking against watermark removal and watermark forgery. AudioMarkBench includes a new dataset created from Common-Voice across languages, biological sexes, and ages, 3 state-of-the-art watermarking methods, and 15 types of perturbations. We benchmark the robustness of these methods against the perturbations in no-box, black-box, and white-box settings. Our findings highlight the vulnerabilities of current watermarking techniques and emphasize the need for more robust and fair audio watermarking solutions. Our dataset and code are publicly available at \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2406.06979",
    "authors": [
      "Hongbin Liu",
      "Moyang Guo",
      "Zhengyuan Jiang",
      "Lun Wang",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2406.06984",
    "title": "On the H\\\"{o}lder Stability of Multiset and Graph Neural Networks",
    "abstract": "           Famously, multiset neural networks based on sum-pooling can separate all distinct multisets, and as a result can be used by message passing neural networks (MPNNs) to separate all pairs of graphs that can be separated by the 1-WL graph isomorphism test. However, the quality of this separation may be very weak, to the extent that the embeddings of \"separable\" multisets and graphs might even be considered identical when using fixed finite precision. In this work, we propose to fully analyze the separation quality of multiset models and MPNNs via a novel adaptation of Lipschitz and H\u00f6lder continuity to parametric functions. We prove that common sum-based models are lower-H\u00f6lder continuous, with a H\u00f6lder exponent that decays rapidly with the network's depth. Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful MPNNs. To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz continuous. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks.         ",
    "url": "https://arxiv.org/abs/2406.06984",
    "authors": [
      "Yair Davidson",
      "Nadav Dym"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06999",
    "title": "Teaching with Uncertainty: Unleashing the Potential of Knowledge Distillation in Object Detection",
    "abstract": "           Knowledge distillation (KD) is a widely adopted and effective method for compressing models in object detection tasks. Particularly, feature-based distillation methods have shown remarkable performance. Existing approaches often ignore the uncertainty in the teacher model's knowledge, which stems from data noise and imperfect training. This limits the student model's ability to learn latent knowledge, as it may overly rely on the teacher's imperfect guidance. In this paper, we propose a novel feature-based distillation paradigm with knowledge uncertainty for object detection, termed \"Uncertainty Estimation-Discriminative Knowledge Extraction-Knowledge Transfer (UET)\", which can seamlessly integrate with existing distillation methods. By leveraging the Monte Carlo dropout technique, we introduce knowledge uncertainty into the training process of the student model, facilitating deeper exploration of latent knowledge. Our method performs effectively during the KD process without requiring intricate structures or extensive computational resources. Extensive experiments validate the effectiveness of our proposed approach across various distillation strategies, detectors, and backbone architectures. Specifically, following our proposed paradigm, the existing FGD method achieves state-of-the-art (SoTA) performance, with ResNet50-based GFL achieving 44.1% mAP on the COCO dataset, surpassing the baselines by 3.9%.         ",
    "url": "https://arxiv.org/abs/2406.06999",
    "authors": [
      "Junfei Yi",
      "Jianxu Mao",
      "Tengfei Liu",
      "Mingjie Li",
      "Hanyu Gu",
      "Hui Zhang",
      "Xiaojun Chang",
      "Yaonan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07003",
    "title": "GraphCoder: Enhancing Repository-Level Code Completion via Code Context Graph-based Retrieval and Language Model",
    "abstract": "           The performance of repository-level code completion depends upon the effective leverage of both general and repository-specific knowledge. Despite the impressive capability of code LLMs in general code completion tasks, they often exhibit less satisfactory performance on repository-level completion due to the lack of repository-specific knowledge in these LLMs. To address this problem, we propose GraphCoder, a retrieval-augmented code completion framework that leverages LLMs' general code knowledge and the repository-specific knowledge via a graph-based retrieval-generation process. In particular, GraphCoder captures the context of completion target more accurately through code context graph (CCG) that consists of control-flow, data- and control-dependence between code statements, a more structured way to capture the completion target context than the sequence-based context used in existing retrieval-augmented approaches; based on CCG, GraphCoder further employs a coarse-to-fine retrieval process to locate context-similar code snippets with the completion target from the current repository. Experimental results demonstrate both the effectiveness and efficiency of GraphCoder: Compared to baseline retrieval-augmented methods, GraphCoder achieves higher exact match (EM) on average, with increases of +6.06 in code match and +6.23 in identifier match, while using less time and space.         ",
    "url": "https://arxiv.org/abs/2406.07003",
    "authors": [
      "Wei Liu",
      "Ailun Yu",
      "Daoguang Zan",
      "Bo Shen",
      "Wei Zhang",
      "Haiyan Zhao",
      "Zhi Jin",
      "Qianxiang Wang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2406.07017",
    "title": "MoreauPruner: Robust Pruning of Large Language Models against Weight Perturbations",
    "abstract": "           Few-shot gradient methods have been extensively utilized in existing model pruning methods, where the model weights are regarded as static values and the effects of potential weight perturbations are not considered. However, the widely used large language models (LLMs) have several billion model parameters, which could increase the fragility of few-shot gradient pruning. In this work, we experimentally show that one-shot gradient pruning algorithms could lead to unstable results under perturbations to model weights. And the minor error of switching between data formats bfloat16 and float16 could result in drastically different outcomes. To address such instabilities, we leverage optimization analysis and propose an LLM structural pruning method, called MoreauPruner, with provable robustness against weight perturbations. In MoreauPruner, the model weight importance is estimated based on the neural network's Moreau envelope, which can be flexibly combined with $\\ell_1$-norm regularization techniques to induce the sparsity required in the pruning task. We extensively evaluate the MoreauPruner algorithm on several well-known LLMs, including LLaMA-7B, LLaMA-13B, LLaMA3-8B, and Vicuna-7B. Our numerical results suggest the robustness of MoreauPruner against weight perturbations, and indicate the MoreauPruner's successful accuracy-based scores in comparison to several existing pruning methods. We have released the code in \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2406.07017",
    "authors": [
      "Zixiao Wang",
      "Jingwei Zhang",
      "Wenqian Zhao",
      "Farzan Farnia",
      "Bei Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.07023",
    "title": "LiSD: An Efficient Multi-Task Learning Framework for LiDAR Segmentation and Detection",
    "abstract": "           With the rapid proliferation of autonomous driving, there has been a heightened focus on the research of lidar-based 3D semantic segmentation and object detection methodologies, aiming to ensure the safety of traffic participants. In recent decades, learning-based approaches have emerged, demonstrating remarkable performance gains in comparison to conventional algorithms. However, the segmentation and detection tasks have traditionally been examined in isolation to achieve the best precision. To this end, we propose an efficient multi-task learning framework named LiSD which can address both segmentation and detection tasks, aiming to optimize the overall performance. Our proposed LiSD is a voxel-based encoder-decoder framework that contains a hierarchical feature collaboration module and a holistic information aggregation module. Different integration methods are adopted to keep sparsity in segmentation while densifying features for query initialization in detection. Besides, cross-task information is utilized in an instance-aware refinement module to obtain more accurate predictions. Experimental results on the nuScenes dataset and Waymo Open Dataset demonstrate the effectiveness of our proposed model. It is worth noting that LiSD achieves the state-of-the-art performance of 83.3% mIoU on the nuScenes segmentation benchmark for lidar-only methods.         ",
    "url": "https://arxiv.org/abs/2406.07023",
    "authors": [
      "Jiahua Xu",
      "Si Zuo",
      "Chenfeng Wei",
      "Wei Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07028",
    "title": "Heterogeneous Learning Rate Scheduling for Neural Architecture Search on Long-Tailed Datasets",
    "abstract": "           In this paper, we attempt to address the challenge of applying Neural Architecture Search (NAS) algorithms, specifically the Differentiable Architecture Search (DARTS), to long-tailed datasets where class distribution is highly imbalanced. We observe that traditional re-sampling and re-weighting techniques, which are effective in standard classification tasks, lead to performance degradation when combined with DARTS. To mitigate this, we propose a novel adaptive learning rate scheduling strategy tailored for the architecture parameters of DARTS when integrated with the Bilateral Branch Network (BBN) for handling imbalanced datasets. Our approach dynamically adjusts the learning rate of the architecture parameters based on the training epoch, preventing the disruption of well-trained representations in the later stages of training. Additionally, we explore the impact of branch mixing factors on the algorithm's performance. Through extensive experiments on the CIFAR-10 dataset with an artificially induced long-tailed distribution, we demonstrate that our method achieves comparable accuracy to using DARTS alone. And the experiment results suggest that re-sampling methods inherently harm the performance of the DARTS algorithm. Our findings highlight the importance of careful data augment when applying DNAS to imbalanced learning scenarios.         ",
    "url": "https://arxiv.org/abs/2406.07028",
    "authors": [
      "Chenxia Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.07034",
    "title": "Improving Multi-hop Logical Reasoning in Knowledge Graphs with Context-Aware Query Representation Learning",
    "abstract": "           Multi-hop logical reasoning on knowledge graphs is a pivotal task in natural language processing, with numerous approaches aiming to answer First-Order Logic (FOL) queries. Recent geometry (e.g., box, cone) and probability (e.g., beta distribution)-based methodologies have effectively addressed complex FOL queries. However, a common challenge across these methods lies in determining accurate geometric bounds or probability parameters for these queries. The challenge arises because existing methods rely on linear sequential operations within their computation graphs, overlooking the logical structure of the query and the relation-induced information that can be gleaned from the relations of the query, which we call the context of the query. To address the problem, we propose a model-agnostic methodology that enhances the effectiveness of existing multi-hop logical reasoning approaches by fully integrating the context of the FOL query graph. Our approach distinctively discerns (1) the structural context inherent to the query structure and (2) the relation-induced context unique to each node in the query graph as delineated in the corresponding knowledge graph. This dual-context paradigm helps nodes within a query graph attain refined internal representations throughout the multi-hop reasoning steps. Through experiments on two datasets, our method consistently enhances the three multi-hop reasoning foundation models, achieving performance improvements of up to 19.5%. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.07034",
    "authors": [
      "Jeonghoon Kim",
      "Heesoo Jung",
      "Hyeju Jang",
      "Hogun Park"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.07042",
    "title": "EFFOcc: A Minimal Baseline for EFficient Fusion-based 3D Occupancy Network",
    "abstract": "           3D occupancy prediction (Occ) is a rapidly rising challenging perception task in the field of autonomous driving which represents the driving scene as uniformly partitioned 3D voxel grids with semantics. Compared to 3D object detection, grid perception has great advantage of better recognizing irregularly shaped, unknown category, or partially occluded general objects. However, existing 3D occupancy networks (occnets) are both computationally heavy and label-hungry. In terms of model complexity, occnets are commonly composed of heavy Conv3D modules or transformers on the voxel level. In terms of label annotations requirements, occnets are supervised with large-scale expensive dense voxel labels. Model and data inefficiency, caused by excessive network parameters and label annotations requirement, severely hinder the onboard deployment of occnets. This paper proposes an efficient 3d occupancy network (EFFOcc), that targets the minimal network complexity and label requirement while achieving state-of-the-art accuracy. EFFOcc only uses simple 2D operators, and improves Occ accuracy to the state-of-the-art on multiple large-scale benchmarks: Occ3D-nuScenes, Occ3D-Waymo, and OpenOccupancy-nuScenes. On Occ3D-nuScenes benchmark, EFFOcc has only 18.4M parameters, and achieves 50.46 in terms of mean IoU (mIoU), to our knowledge, it is the occnet with minimal parameters compared with related occnets. Moreover, we propose a two-stage active learning strategy to reduce the requirements of labelled data. Active EFFOcc trained with 6\\% labelled voxels achieves 47.19 mIoU, which is 95.7% fully supervised performance. The proposed EFFOcc also supports improved vision-only occupancy prediction with the aid of region-decomposed distillation. Code and demo videos will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.07042",
    "authors": [
      "Yining Shi",
      "Kun Jiang",
      "Ke Wang",
      "Kangan Qian",
      "Yunlong Wang",
      "Jiusi Li",
      "Tuopu Wen",
      "Mengmeng Yang",
      "Yiliang Xu",
      "Diange Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07045",
    "title": "Research on High-precision Detection Technology for Un-derground Space Information Data",
    "abstract": "           The quality of underground space information data has become a major problem endangering the safety of underground space. After research and analysis, we found that the current high accuracy information data remote detection methods are limited to the detection of overground spaces objects, and are not applicable to the detection of various information data in underground space. In this paper, we analyze the spectral properties of the fractional-order differential (FDO) operator, and establish mathematical model of remote transmission and high-precision detection of information data, which realizes the functions of high-precision and remote detection of information data. By fusing the information data to detect the mathematical model in a long distance and with high accuracy, A mathematical model has been established to improve the quality of underground spatial information data. Through the application in engineering practice, the effectiveness of this method in underground space information data detection is verified.         ",
    "url": "https://arxiv.org/abs/2406.07045",
    "authors": [
      "Zuo Yanhong",
      "Zhou Chao",
      "Xia Shilong",
      "Yang kun"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2406.07050",
    "title": "DualMamba: A Lightweight Spectral-Spatial Mamba-Convolution Network for Hyperspectral Image Classification",
    "abstract": "           The effectiveness and efficiency of modeling complex spectral-spatial relations are both crucial for Hyperspectral image (HSI) classification. Most existing methods based on CNNs and transformers still suffer from heavy computational burdens and have room for improvement in capturing the global-local spectral-spatial feature representation. To this end, we propose a novel lightweight parallel design called lightweight dual-stream Mamba-convolution network (DualMamba) for HSI classification. Specifically, a parallel lightweight Mamba and CNN block are first developed to extract global and local spectral-spatial features. First, the cross-attention spectral-spatial Mamba module is proposed to leverage the global modeling of Mamba at linear complexity. Within this module, dynamic positional embedding is designed to enhance the spatial location information of visual sequences. The lightweight spectral/spatial Mamba blocks comprise an efficient scanning strategy and a lightweight Mamba design to efficiently extract global spectral-spatial features. And the cross-attention spectral-spatial fusion is designed to learn cross-correlation and fuse spectral-spatial features. Second, the lightweight spectral-spatial residual convolution module is proposed with lightweight spectral and spatial branches to extract local spectral-spatial features through residual learning. Finally, the adaptive global-local fusion is proposed to dynamically combine global Mamba features and local convolution features for a global-local spectral-spatial representation. Compared with state-of-the-art HSI classification methods, experimental results demonstrate that DualMamba achieves significant classification accuracy on three public HSI datasets and a superior reduction in model parameters and floating point operations (FLOPs).         ",
    "url": "https://arxiv.org/abs/2406.07050",
    "authors": [
      "Jiamu Sheng",
      "Jingyi Zhou",
      "Jiong Wang",
      "Peng Ye",
      "Jiayuan Fan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07060",
    "title": "Reading Miscue Detection in Primary School through Automatic Speech Recognition",
    "abstract": "           Automatic reading diagnosis systems can benefit both teachers for more efficient scoring of reading exercises and students for accessing reading exercises with feedback more easily. However, there are limited studies on Automatic Speech Recognition (ASR) for child speech in languages other than English, and limited research on ASR-based reading diagnosis systems. This study investigates how efficiently state-of-the-art (SOTA) pretrained ASR models recognize Dutch native children speech and manage to detect reading miscues. We found that Hubert Large finetuned on Dutch speech achieves SOTA phoneme-level child speech recognition (PER at 23.1\\%), while Whisper (Faster Whisper Large-v2) achieves SOTA word-level performance (WER at 9.8\\%). Our findings suggest that Wav2Vec2 Large and Whisper are the two best ASR models for reading miscue detection. Specifically, Wav2Vec2 Large shows the highest recall at 0.83, whereas Whisper exhibits the highest precision at 0.52 and an F1 score of 0.52.         ",
    "url": "https://arxiv.org/abs/2406.07060",
    "authors": [
      "Lingyun Gao",
      "Cristian Tejedor-Garcia",
      "Helmer Strik",
      "Catia Cucchiarini"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07080",
    "title": "DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs",
    "abstract": "           Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning autonomous language agents in various real-life applications. To improve the neural-symbolic reasoning capabilities of language agents powered by Large Language Models (LLMs) in KGQA, we propose the DecompositionAlignment-Reasoning Agent (DARA) framework. DARA effectively parses questions into formal queries through a dual mechanism: high-level iterative task decomposition and low-level task grounding. Importantly, DARA can be efficiently trained with a small number of high-quality reasoning trajectories. Our experimental results demonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms both in-context learning-based agents with GPT-4 and alternative fine-tuned agents, across different benchmarks in zero-shot evaluation, making such models more accessible for real-life applications. We also show that DARA attains performance comparable to state-of-the-art enumerating-and-ranking-based methods for KGQA.         ",
    "url": "https://arxiv.org/abs/2406.07080",
    "authors": [
      "Haishuo Fang",
      "Xiaodan Zhu",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.07094",
    "title": "Grapevine Disease Prediction Using Climate Variables from Multi-Sensor Remote Sensing Imagery via a Transformer Model",
    "abstract": "           Early detection and management of grapevine diseases are important in pursuing sustainable viticulture. This paper introduces a novel framework leveraging the TabPFN model to forecast blockwise grapevine diseases using climate variables from multi-sensor remote sensing imagery. By integrating advanced machine learning techniques with detailed environmental data, our approach significantly enhances the accuracy and efficiency of disease prediction in vineyards. The TabPFN model's experimental evaluations showcase comparable performance to traditional gradient-boosted decision trees, such as XGBoost, CatBoost, and LightGBM. The model's capability to process complex data and provide per-pixel disease-affecting probabilities enables precise, targeted interventions, contributing to more sustainable disease management practices. Our findings underscore the transformative potential of combining Transformer models with remote sensing data in precision agriculture, offering a scalable solution for improving crop health and productivity while reducing environmental impact.         ",
    "url": "https://arxiv.org/abs/2406.07094",
    "authors": [
      "Weiying Zhao",
      "Natalia Efremova"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2406.07111",
    "title": "NeRSP: Neural 3D Reconstruction for Reflective Objects with Sparse Polarized Images",
    "abstract": "           We present NeRSP, a Neural 3D reconstruction technique for Reflective surfaces with Sparse Polarized images. Reflective surface reconstruction is extremely challenging as specular reflections are view-dependent and thus violate the multiview consistency for multiview stereo. On the other hand, sparse image inputs, as a practical capture setting, commonly cause incomplete or distorted results due to the lack of correspondence matching. This paper jointly handles the challenges from sparse inputs and reflective surfaces by leveraging polarized images. We derive photometric and geometric cues from the polarimetric image formation model and multiview azimuth consistency, which jointly optimize the surface geometry modeled via implicit neural representation. Based on the experiments on our synthetic and real datasets, we achieve the state-of-the-art surface reconstruction results with only 6 views as input.         ",
    "url": "https://arxiv.org/abs/2406.07111",
    "authors": [
      "Yufei Han",
      "Heng Guo",
      "Koki Fukai",
      "Hiroaki Santo",
      "Boxin Shi",
      "Fumio Okura",
      "Zhanyu Ma",
      "Yunpeng Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07113",
    "title": "Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene Graph",
    "abstract": "           Locating objects referred to in natural language poses a significant challenge for autonomous agents. Existing CLIP-based open-vocabulary methods successfully perform 3D object retrieval with simple (bare) queries but cannot cope with ambiguous descriptions that demand an understanding of object relations. To tackle this problem, we propose a modular approach called BBQ (Beyond Bare Queries), which constructs 3D scene spatial graph representation with metric edges and utilizes a large language model as a human-to-agent interface through our deductive scene reasoning algorithm. BBQ employs robust DINO-powered associations to form 3D objects, an advanced raycasting algorithm to project them to 2D, and a vision-language model to describe them as graph nodes. On Replica and ScanNet datasets, we show that the designed method accurately constructs 3D object-centric maps. We have demonstrated that their quality takes a leading place for open-vocabulary 3D semantic segmentation against other zero-shot methods. Also, we show that leveraging spatial relations is especially effective for scenes containing multiple entities of the same semantic class. On Sr3D and Nr3D benchmarks, our deductive approach demonstrates a significant improvement, enabling retrieving objects by complex queries compared to other state-of-the-art methods. Considering our design solutions, we achieved a processing speed approximately x3 times faster than the closest analog. This promising performance enables our approach for usage in applied intelligent robotics projects. We make the code publicly available at this http URL.         ",
    "url": "https://arxiv.org/abs/2406.07113",
    "authors": [
      "Sergey Linok",
      "Tatiana Zemskova",
      "Svetlana Ladanova",
      "Roman Titkov",
      "Dmitry Yudin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.07124",
    "title": "CHARME: A chain-based reinforcement learning approach for the minor embedding problem",
    "abstract": "           Quantum Annealing (QA) holds great potential for solving combinatorial optimization problems efficiently. However, the effectiveness of QA algorithms heavily relies on the embedding of problem instances, represented as logical graphs, into the quantum unit processing (QPU) whose topology is in form of a limited connectivity graph, known as the minor embedding Problem. Existing methods for the minor embedding problem suffer from scalability issues when confronted with larger problem sizes. In this paper, we propose a novel approach utilizing Reinforcement Learning (RL) techniques to address the minor embedding problem, named CHARME. CHARME includes three key components: a Graph Neural Network (GNN) architecture for policy modeling, a state transition algorithm ensuring solution validity, and an order exploration strategy for effective training. Through comprehensive experiments on synthetic and real-world instances, we demonstrate that the efficiency of our proposed order exploration strategy as well as our proposed RL framework, CHARME. In details, CHARME yields superior solutions compared to fast embedding methods such as Minorminer and ATOM. Moreover, our method surpasses the OCT-based approach, known for its slower runtime but high-quality solutions, in several cases. In addition, our proposed exploration enhances the efficiency of the training of the CHARME framework by providing better solutions compared to the greedy strategy.         ",
    "url": "https://arxiv.org/abs/2406.07124",
    "authors": [
      "Hoang M. Ngo",
      "Nguyen H K. Do",
      "Minh N. Vu",
      "Tamer Kahveci",
      "My T. Thai"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07125",
    "title": "CARACAS: vehiCular ArchitectuRe for detAiled Can Attacks Simulation",
    "abstract": "           Modern vehicles are increasingly vulnerable to attacks that exploit network infrastructures, particularly the Controller Area Network (CAN) networks. To effectively counter such threats using contemporary tools like Intrusion Detection Systems (IDSs) based on data analysis and classification, large datasets of CAN messages become imperative. This paper delves into the feasibility of generating synthetic datasets by harnessing the modeling capabilities of simulation frameworks such as Simulink coupled with a robust representation of attack models to present CARACAS, a vehicular model, including component control via CAN messages and attack injection capabilities. CARACAS showcases the efficacy of this methodology, including a Battery Electric Vehicle (BEV) model, and focuses on attacks targeting torque control in two distinct scenarios.         ",
    "url": "https://arxiv.org/abs/2406.07125",
    "authors": [
      "Sadek Misto Kirdi",
      "Nicola Scarano",
      "Franco Oberti",
      "Luca Mannella",
      "Stefano Di Carlo",
      "Alessandro Savino"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07126",
    "title": "Logical Distillation of Graph Neural Networks",
    "abstract": "           We present a logic based interpretable model for learning on graphs and an algorithm to distill this model from a Graph Neural Network (GNN). Recent results have shown connections between the expressivity of GNNs and the two-variable fragment of first-order logic with counting quantifiers (C2). We introduce a decision-tree based model which leverages an extension of C2 to distill interpretable logical classifiers from GNNs. We test our approach on multiple GNN architectures. The distilled models are interpretable, succinct, and attain similar accuracy to the underlying GNN. Furthermore, when the ground truth is expressible in C2, our approach outperforms the GNN.         ",
    "url": "https://arxiv.org/abs/2406.07126",
    "authors": [
      "Alexander Pluska",
      "Pascal Welke",
      "Thomas G\u00e4rtner",
      "Sagar Malhotra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.07129",
    "title": "Mining Frequent Structures in Conceptual Models",
    "abstract": "           The problem of using structured methods to represent knowledge is well-known in conceptual modeling and has been studied for many years. It has been proven that adopting modeling patterns represents an effective structural method. Patterns are, indeed, generalizable recurrent structures that can be exploited as solutions to design problems. They aid in understanding and improving the process of creating models. The undeniable value of using patterns in conceptual modeling was demonstrated in several experimental studies. However, discovering patterns in conceptual models is widely recognized as a highly complex task and a systematic solution to pattern identification is currently lacking. In this paper, we propose a general approach to the problem of discovering frequent structures, as they occur in conceptual modeling languages. As proof of concept for our scientific contribution, we provide an implementation of the approach, by focusing on UML class diagrams, in particular OntoUML models. This implementation comprises an exploratory tool, which, through the combination of a frequent subgraph mining algorithm and graph manipulation techniques, can process multiple conceptual models and discover recurrent structures according to multiple criteria. The primary objective is to offer a support facility for language engineers. This can be employed to leverage both good and bad modeling practices, to evolve and maintain the conceptual modeling language, and to promote the reuse of encoded experience in designing better models with the given language.         ",
    "url": "https://arxiv.org/abs/2406.07129",
    "authors": [
      "Mattia Fumagalli",
      "Tiago Prince Sales",
      "Pedro Paulo F. Barcelos",
      "Giovanni Micale",
      "Vadim Zaytsev",
      "Diego Calvanese",
      "Giancarlo Guizzardi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.07131",
    "title": "ICGAN: An implicit conditioning method for interpretable feature control of neural audio synthesis",
    "abstract": "           Neural audio synthesis methods can achieve high-fidelity and realistic sound generation by utilizing deep generative models. Such models typically rely on external labels which are often discrete as conditioning information to achieve guided sound generation. However, it remains difficult to control the subtle changes in sounds without appropriate and descriptive labels, especially given a limited dataset. This paper proposes an implicit conditioning method for neural audio synthesis using generative adversarial networks that allows for interpretable control of the acoustic features of synthesized sounds. Our technique creates a continuous conditioning space that enables timbre manipulation without relying on explicit labels. We further introduce an evaluation metric to explore controllability and demonstrate that our approach is effective in enabling a degree of controlled variation of different synthesized sound effects for in-domain and cross-domain sounds.         ",
    "url": "https://arxiv.org/abs/2406.07131",
    "authors": [
      "Yunyi Liu",
      "Craig Jin"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2406.07135",
    "title": "Smart Wireless Environment Enhanced Telecommunications: A Network Stabilisation Paradigm for Mobile Operators",
    "abstract": "           Due to the uncontrolled and complex real-life radio propagation environments, Claude Shannon's information theory of communications describes fundamental limits to state-of-the-art 5G radio access network (RAN) capacity, with respect to fixed radio resource usage. Fortunately, recent research has found that a holographic metasurface-based new physical layer architecture may hold the key to overcome these fundamental limits of current mobile networks under a new paradigm, smart wireless environment (SWE), where the long-standing challenge of mobile communications, fading channel hostility, may be solved, leading to a step-change boost in network performance and user experience. Despite recent research activities in SWE, the best way to implement it as a network operator remains an open challenge. In this industrial review, we adopt a novel yet realistic mobile channel stabilisation perspective for network operators to understand this paradigm shift. More specifically, we provide a technical analysis of the synergy between key next-gen mobile network enablers, e.g., holographic metasurface, wireless sensing, and machine intelligence, as well as of how this synergy leads to a robust future RAN architecture. Against the as yet unclear theoretical boundaries and low technology readiness level (TRL) of SWE enhanced telecommunications, we conclude by identifying critical challenges in future commercial deployments.         ",
    "url": "https://arxiv.org/abs/2406.07135",
    "authors": [
      "Yangyishi Zhang",
      "Khethiwe Mhlope",
      "Aaron Walker",
      "Fraser Burton"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2406.07141",
    "title": "Identifiable Object-Centric Representation Learning via Probabilistic Slot Attention",
    "abstract": "           Learning modular object-centric representations is crucial for systematic generalization. Existing methods show promising object-binding capabilities empirically, but theoretical identifiability guarantees remain relatively underdeveloped. Understanding when object-centric representations can theoretically be identified is crucial for scaling slot-based methods to high-dimensional images with correctness guarantees. To that end, we propose a probabilistic slot-attention algorithm that imposes an aggregate mixture prior over object-centric slot representations, thereby providing slot identifiability guarantees without supervision, up to an equivalence relation. We provide empirical verification of our theoretical identifiability result using both simple 2-dimensional data and high-resolution imaging datasets.         ",
    "url": "https://arxiv.org/abs/2406.07141",
    "authors": [
      "Avinash Kori",
      "Francesco Locatello",
      "Ainkaran Santhirasekaram",
      "Francesca Toni",
      "Ben Glocker",
      "Fabio De Sousa Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.07160",
    "title": "Deep Learning-Based Approach for User Activity Detection with Grant-Free Random Access in Cell-Free Massive MIMO",
    "abstract": "           Modern wireless networks must reliably support a wide array of connectivity demands, encompassing various user needs across diverse scenarios. Machine-Type Communication (mMTC) is pivotal in these networks, particularly given the challenges posed by massive connectivity and sporadic device activation patterns. Traditional grant-based random access (GB-RA) protocols face limitations due to constrained orthogonal preamble resources. In response, the adoption of grant-free random access (GF-RA) protocols offers a promising solution. This paper explores the application of supervised machine learning models to tackle activity detection issues in scenarios where non-orthogonal preamble design is considered. We introduce a data-driven algorithm specifically designed for user activity detection in Cell-Free Massive Multiple-Input Multiple-Output (CF-mMIMO) networks operating under GF-RA protocols. Additionally, this study presents a novel clustering strategy that simplifies and enhances activity detection accuracy, assesses the resilience of the algorithm to input perturbations, and investigates the effects of adopting floating-to-fixed-point conversion on algorithm performance. Simulations conducted adhere to 3GPP standards, ensuring accurate channel modeling, and employ a deep learning approach to boost the detection capabilities of mMTC GF-RA devices. The results are compelling: the algorithm achieves an exceptional 99\\% accuracy rate, confirming its efficacy in real-world applications.         ",
    "url": "https://arxiv.org/abs/2406.07160",
    "authors": [
      "Ali Elkeshawy",
      "Ha\u00cffa Far\u00e8s",
      "Amor Nafkha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07163",
    "title": "FaceGPT: Self-supervised Learning to Chat about 3D Human Faces",
    "abstract": "           We introduce FaceGPT, a self-supervised learning framework for Large Vision-Language Models (VLMs) to reason about 3D human faces from images and text. Typical 3D face reconstruction methods are specialized algorithms that lack semantic reasoning capabilities. FaceGPT overcomes this limitation by embedding the parameters of a 3D morphable face model (3DMM) into the token space of a VLM, enabling the generation of 3D faces from both textual and visual inputs. FaceGPT is trained in a self-supervised manner as a model-based autoencoder from in-the-wild images. In particular, the hidden state of LLM is projected into 3DMM parameters and subsequently rendered as 2D face image to guide the self-supervised learning process via image-based reconstruction. Without relying on expensive 3D annotations of human faces, FaceGPT obtains a detailed understanding about 3D human faces, while preserving the capacity to understand general user instructions. Our experiments demonstrate that FaceGPT not only achieves high-quality 3D face reconstructions but also retains the ability for general-purpose visual instruction following. Furthermore, FaceGPT learns fully self-supervised to generate 3D faces based on complex textual inputs, which opens a new direction in human face analysis.         ",
    "url": "https://arxiv.org/abs/2406.07163",
    "authors": [
      "Haoran Wang",
      "Mohit Mendiratta",
      "Christian Theobalt",
      "Adam Kortylewski"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07170",
    "title": "VoxNeuS: Enhancing Voxel-Based Neural Surface Reconstruction via Gradient Interpolation",
    "abstract": "           Neural Surface Reconstruction learns a Signed Distance Field~(SDF) to reconstruct the 3D model from multi-view images. Previous works adopt voxel-based explicit representation to improve efficiency. However, they ignored the gradient instability of interpolation in the voxel grid, leading to degradation on convergence and smoothness. Besides, previous works entangled the optimization of geometry and radiance, which leads to the deformation of geometry to explain radiance, causing artifacts when reconstructing textured planes. In this work, we reveal that the instability of gradient comes from its discontinuity during trilinear interpolation, and propose to use the interpolated gradient instead of the original analytical gradient to eliminate the discontinuity. Based on gradient interpolation, we propose VoxNeuS, a lightweight surface reconstruction method for computational and memory efficient neural surface reconstruction. Thanks to the explicit representation, the gradient of regularization terms, i.e. Eikonal and curvature loss, are directly solved, avoiding computation and memory-access overhead. Further, VoxNeuS adopts a geometry-radiance disentangled architecture to handle the geometry deformation from radiance optimization. The experimental results show that VoxNeuS achieves better reconstruction quality than previous works. The entire training process takes 15 minutes and less than 3 GB of memory on a single 2080ti GPU.         ",
    "url": "https://arxiv.org/abs/2406.07170",
    "authors": [
      "Sidun Liu",
      "Peng Qiao",
      "Zongxin Ye",
      "Wenyu Li",
      "Yong Dou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07176",
    "title": "RAD: A Comprehensive Dataset for Benchmarking the Robustness of Image Anomaly Detection",
    "abstract": "           Robustness against noisy imaging is crucial for practical image anomaly detection systems. This study introduces a Robust Anomaly Detection (RAD) dataset with free views, uneven illuminations, and blurry collections to systematically evaluate the robustness of current anomaly detection methods. Specifically, RAD aims to identify foreign objects on working platforms as anomalies. The collection process incorporates various sources of imaging noise, such as viewpoint changes, uneven illuminations, and blurry collections, to replicate real-world inspection scenarios. Subsequently, we assess and analyze 11 state-of-the-art unsupervised and zero-shot methods on RAD. Our findings indicate that: 1) Variations in viewpoint, illumination, and blurring affect anomaly detection methods to varying degrees; 2) Methods relying on memory banks and assisted by synthetic anomalies demonstrate stronger robustness; 3) Effectively leveraging the general knowledge of foundational models is a promising avenue for enhancing the robustness of anomaly detection methods.         ",
    "url": "https://arxiv.org/abs/2406.07176",
    "authors": [
      "Yuqi Cheng",
      "Yunkang Cao",
      "Rui Chen",
      "Weiming Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07188",
    "title": "Merging Improves Self-Critique Against Jailbreak Attacks",
    "abstract": "           The robustness of large language models (LLMs) against adversarial manipulations, such as jailbreak attacks, remains a significant challenge. In this work, we propose an approach that enhances the self-critique capability of the LLM and further fine-tunes it over sanitized synthetic data. This is done with the addition of an external critic model that can be merged with the original, thus bolstering self-critique capabilities and improving the robustness of the LLMs response to adversarial prompts. Our results demonstrate that the combination of merging and self-critique can reduce the attack success rate of adversaries significantly, thus offering a promising defense mechanism against jailbreak attacks. Code, data and models released at this https URL .         ",
    "url": "https://arxiv.org/abs/2406.07188",
    "authors": [
      "Victor Gallego"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.07221",
    "title": "Open-World Human-Object Interaction Detection via Multi-modal Prompts",
    "abstract": "           In this paper, we develop \\textbf{MP-HOI}, a powerful Multi-modal Prompt-based HOI detector designed to leverage both textual descriptions for open-set generalization and visual exemplars for handling high ambiguity in descriptions, realizing HOI detection in the open world. Specifically, it integrates visual prompts into existing language-guided-only HOI detectors to handle situations where textual descriptions face difficulties in generalization and to address complex scenarios with high interaction ambiguity. To facilitate MP-HOI training, we build a large-scale HOI dataset named Magic-HOI, which gathers six existing datasets into a unified label space, forming over 186K images with 2.4K objects, 1.2K actions, and 20K HOI interactions. Furthermore, to tackle the long-tail issue within the Magic-HOI dataset, we introduce an automated pipeline for generating realistically annotated HOI images and present SynHOI, a high-quality synthetic HOI dataset containing 100K images. Leveraging these two datasets, MP-HOI optimizes the HOI task as a similarity learning process between multi-modal prompts and objects/interactions via a unified contrastive loss, to learn generalizable and transferable objects/interactions representations from large-scale data. MP-HOI could serve as a generalist HOI detector, surpassing the HOI vocabulary of existing expert models by more than 30 times. Concurrently, our results demonstrate that MP-HOI exhibits remarkable zero-shot capability in real-world scenarios and consistently achieves a new state-of-the-art performance across various benchmarks.         ",
    "url": "https://arxiv.org/abs/2406.07221",
    "authors": [
      "Jie Yang",
      "Bingliang Li",
      "Ailing Zeng",
      "Lei Zhang",
      "Ruimao Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07256",
    "title": "AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection",
    "abstract": "           The rapid advancements in speech technologies over the past two decades have led to human-level performance in tasks like automatic speech recognition (ASR) for fluent speech. However, the efficacy of these models diminishes when applied to atypical speech, such as stuttering. This paper introduces AS-70, the first publicly available Mandarin stuttered speech dataset, which stands out as the largest dataset in its category. Encompassing conversational and voice command reading speech, AS-70 includes verbatim manual transcription, rendering it suitable for various speech-related tasks. Furthermore, baseline systems are established, and experimental results are presented for ASR and stuttering event detection (SED) tasks. By incorporating this dataset into the model fine-tuning, significant improvements in the state-of-the-art ASR models, e.g., Whisper and Hubert, are observed, enhancing their inclusivity in addressing stuttered speech.         ",
    "url": "https://arxiv.org/abs/2406.07256",
    "authors": [
      "Rong Gong",
      "Hongfei Xue",
      "Lezhi Wang",
      "Xin Xu",
      "Qisheng Li",
      "Lei Xie",
      "Hui Bu",
      "Shaomei Wu",
      "Jiaming Zhou",
      "Yong Qin",
      "Binbin Zhang",
      "Jun Du",
      "Jia Bin",
      "Ming Li"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2406.07263",
    "title": "Active learning for affinity prediction of antibodies",
    "abstract": "           The primary objective of most lead optimization campaigns is to enhance the binding affinity of ligands. For large molecules such as antibodies, identifying mutations that enhance antibody affinity is particularly challenging due to the combinatorial explosion of potential mutations. When the structure of the antibody-antigen complex is available, relative binding free energy (RBFE) methods can offer valuable insights into how different mutations will impact the potency and selectivity of a drug candidate, thereby reducing the reliance on costly and time-consuming wet-lab experiments. However, accurately simulating the physics of large molecules is computationally intensive. We present an active learning framework that iteratively proposes promising sequences for simulators to evaluate, thereby accelerating the search for improved binders. We explore different modeling approaches to identify the most effective surrogate model for this task, and evaluate our framework both using pre-computed pools of data and in a realistic full-loop setting.         ",
    "url": "https://arxiv.org/abs/2406.07263",
    "authors": [
      "Alexandra Gessner",
      "Sebastian W. Ober",
      "Owen Vickery",
      "Dino Ogli\u0107",
      "Talip U\u00e7ar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.07282",
    "title": "Optimal policy design for decision problems under social influence",
    "abstract": "           This paper focuses on devising strategies for control-oriented decision-making scenarios, in the presence of social and external influences, e.g. within recommending systems in social contexts. More precisely, we extend the classical Friedkin and Johnsen model of opinion dynamics to incorporate random factors, such as variability in individual predisposition, and uncertainty in social acceptance towards a specific action that a recommending system aims to promote. Furthermore, we formulate an optimization-based control problem aimed at fostering the social acceptance of particular actions within the network. Initially conceptualized as an economic cost minimization, in this preliminary work, we simplify our problem by reformulating it into an MPC framework. Through our analysis and numerical simulations, we illustrate the effectiveness of the proposed methodologies.         ",
    "url": "https://arxiv.org/abs/2406.07282",
    "authors": [
      "Valentina Breschi",
      "Chiara Ravazzi",
      "Paolo Frasca",
      "Fabrizio Dabbene",
      "Mara Tanelli"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2406.07284",
    "title": "Unsupervised Object Detection with Theoretical Guarantees",
    "abstract": "           Unsupervised object detection using deep neural networks is typically a difficult problem with few to no guarantees about the learned representation. In this work we present the first unsupervised object detection method that is theoretically guaranteed to recover the true object positions up to quantifiable small shifts. We develop an unsupervised object detection architecture and prove that the learned variables correspond to the true object positions up to small shifts related to the encoder and decoder receptive field sizes, the object sizes, and the widths of the Gaussians used in the rendering process. We perform detailed analysis of how the error depends on each of these variables and perform synthetic experiments validating our theoretical predictions up to a precision of individual pixels. We also perform experiments on CLEVR-based data and show that, unlike current SOTA object detection methods (SAM, CutLER), our method's prediction errors always lie within our theoretical bounds. We hope that this work helps open up an avenue of research into object detection methods with theoretical guarantees.         ",
    "url": "https://arxiv.org/abs/2406.07284",
    "authors": [
      "Marian Longa",
      "Jo\u00e3o F. Henriques"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.07294",
    "title": "OTO Planner: An Efficient Only Travelling Once Exploration Planner for Complex and Unknown Environments",
    "abstract": "           Autonomous exploration in complex and cluttered environments is essential for various applications. However, there are many challenges due to the lack of global heuristic information. Existing exploration methods suffer from the repeated paths and considerable computational resource requirement in large-scale environments. To address the above issues, this letter proposes an efficient exploration planner that reduces repeated paths in complex environments, hence it is called \"Only Travelling Once Planner\". OTO Planner includes fast frontier updating, viewpoint evaluation and viewpoint refinement. A selective frontier updating mechanism is designed, saving a large amount of computational resources. In addition, a novel viewpoint evaluation system is devised to reduce the repeated paths utilizing the enclosed sub-region detection. Besides, a viewpoint refinement approach is raised to concentrate the redundant viewpoints, leading to smoother paths. We conduct extensive simulation and real-world experiments to validate the proposed method. Compared to the state-of-the-art approach, the proposed method reduces the exploration time and movement distance by 10%-20% and improves the speed of frontier detection by 6-9 times.         ",
    "url": "https://arxiv.org/abs/2406.07294",
    "authors": [
      "Bo Zhou",
      "Chuanzhao Lu",
      "Yan Pan",
      "Fu Chen"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07298",
    "title": "Enhanced In-Flight Connectivity for Urban Air Mobility via LEO Satellite Networks",
    "abstract": "           Urban Air Mobility (UAM) is the envisioned future of inter-city aerial transportation. This paper presents a novel, in-flight connectivity link allocation method for UAM, which dynamically switches between terrestrial cellular and Low Earth Orbit (LEO) satellite networks based on real-time conditions. Our approach prefers cellular networks for cost efficiency, switching to LEO satellites under poor cellular conditions to ensure continuous UAM connectivity. By integrating real-time metrics like signal strength, network congestion, and flight trajectory into the selection process, our algorithm effectively balances cost, minimum data rate requirements, and continuity of communication. Numerical results validate minimization of data-loss while ensuring an optimal selection from the set of available above-threshold data rates at every time sample. Furthermore, insights derived from our study emphasize the importance of hybrid connectivity solutions in ensuring seamless, uninterrupted communication for future urban aerial vehicles.         ",
    "url": "https://arxiv.org/abs/2406.07298",
    "authors": [
      "Karnika Biswas",
      "Hakim Ghazzai",
      "Abdullah Khanfor",
      "Lokman Sboui"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2406.07314",
    "title": "Rethinking the impact of noisy labels in graph classification: A utility and privacy perspective",
    "abstract": "           Graph neural networks based on message-passing mechanisms have achieved advanced results in graph classification tasks. However, their generalization performance degrades when noisy labels are present in the training data. Most existing noisy labeling approaches focus on the visual domain or graph node classification tasks and analyze the impact of noisy labels only from a utility perspective. Unlike existing work, in this paper, we measure the effects of noise labels on graph classification from data privacy and model utility perspectives. We find that noise labels degrade the model's generalization performance and enhance the ability of membership inference attacks on graph data privacy. To this end, we propose the robust graph neural network approach with noisy labeled graph classification. Specifically, we first accurately filter the noisy samples by high-confidence samples and the first feature principal component vector of each class. Then, the robust principal component vectors and the model output under data augmentation are utilized to achieve noise label correction guided by dual spatial information. Finally, supervised graph contrastive learning is introduced to enhance the embedding quality of the model and protect the privacy of the training graph data. The utility and privacy of the proposed method are validated by comparing twelve different methods on eight real graph classification datasets. Compared with the state-of-the-art methods, the RGLC method achieves at most and at least 7.8% and 0.8% performance gain at 30% noisy labeling rate, respectively, and reduces the accuracy of privacy attacks to below 60%.         ",
    "url": "https://arxiv.org/abs/2406.07314",
    "authors": [
      "De Li",
      "Xianxian Li",
      "Zeming Gan",
      "Qiyu Li",
      "Bin Qu",
      "Jinyan Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07318",
    "title": "Embedded Graph Convolutional Networks for Real-Time Event Data Processing on SoC FPGAs",
    "abstract": "           The utilisation of event cameras represents an important and swiftly evolving trend aimed at addressing the constraints of traditional video systems. Particularly within the automotive domain, these cameras find significant relevance for their integration into embedded real-time systems due to lower latency and energy consumption. One effective approach to ensure the necessary throughput and latency for event processing systems is through the utilisation of graph convolutional networks (GCNs). In this study, we introduce a series of hardware-aware optimisations tailored for PointNet++, a GCN architecture designed for point cloud processing. The proposed techniques result in more than a 100-fold reduction in model size compared to Asynchronous Event-based GNN (AEGNN), one of the most recent works in the field, with a relatively small decrease in accuracy (2.3% for N-Caltech101 classification, 1.7% for N-Cars classification), thus following the TinyML trend. Based on software research, we designed a custom EFGCN (Event-Based FPGA-accelerated Graph Convolutional Network) and we implemented it on ZCU104 SoC FPGA platform, achieving a throughput of 13.3 million events per second (MEPS) and real-time partially asynchronous processing with a latency of 4.47 ms. We also address the scalability of the proposed hardware model to improve the obtained accuracy score. To the best of our knowledge, this study marks the first endeavour in accelerating PointNet++ networks on SoC FPGAs, as well as the first hardware architecture exploration of graph convolutional networks implementation for real-time continuous event data processing. We publish both software and hardware source code in an open repository: this https URL (will be published upon acceptance).         ",
    "url": "https://arxiv.org/abs/2406.07318",
    "authors": [
      "Kamil Jeziorek",
      "Piotr Wzorek",
      "Krzysztof Blachut",
      "Andrea Pinna",
      "Tomasz Kryjak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Hardware Architecture (cs.AR)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2406.07333",
    "title": "Global-Regularized Neighborhood Regression for Efficient Zero-Shot Texture Anomaly Detection",
    "abstract": "           Texture surface anomaly detection finds widespread applications in industrial settings. However, existing methods often necessitate gathering numerous samples for model training. Moreover, they predominantly operate within a close-set detection framework, limiting their ability to identify anomalies beyond the training dataset. To tackle these challenges, this paper introduces a novel zero-shot texture anomaly detection method named Global-Regularized Neighborhood Regression (GRNR). Unlike conventional approaches, GRNR can detect anomalies on arbitrary textured surfaces without any training data or cost. Drawing from human visual cognition, GRNR derives two intrinsic prior supports directly from the test texture image: local neighborhood priors characterized by coherent similarities and global normality priors featuring typical normal patterns. The fundamental principle of GRNR involves utilizing the two extracted intrinsic support priors for self-reconstructive regression of the query sample. This process employs the transformation facilitated by local neighbor support while being regularized by global normality support, aiming to not only achieve visually consistent reconstruction results but also preserve normality properties. We validate the effectiveness of GRNR across various industrial scenarios using eight benchmark datasets, demonstrating its superior detection performance without the need for training data. Remarkably, our method is applicable for open-set texture defect detection and can even surpass existing vanilla approaches that require extensive training.         ",
    "url": "https://arxiv.org/abs/2406.07333",
    "authors": [
      "Haiming Yao",
      "Wei Luo",
      "Yunkang Cao",
      "Yiheng Zhang",
      "Wenyong Yu",
      "Weiming Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07349",
    "title": "Erasing Radio Frequency Fingerprints via Active Adversarial Perturbation",
    "abstract": "           Radio Frequency (RF) fingerprinting is to identify a wireless device from its uniqueness of the analog circuitry or hardware imperfections. However, unlike the MAC address which can be modified, such hardware feature is inevitable for the signal emitted to air, which can possibly reveal device whereabouts, e.g., a sniffer can use a pre-trained model to identify a nearby device when receiving its signal. Such fingerprint may expose critical private information, e.g., the associated upper-layer applications or the end-user. In this paper, we propose to erase such RF feature for wireless devices, which can prevent fingerprinting by actively perturbation from the signal perspective. Specifically, we consider a common RF fingerprinting scenario, where machine learning models are trained from pilot signal data for identification. A novel adversarial attack solution is designed to generate proper perturbations, whereby the perturbed pilot signal can hide the hardware feature and misclassify the model. We theoretically show that the perturbation would not affect the communication function within a tolerable perturbation threshold. We also implement the pilot signal fingerprinting and the proposed perturbation process in a practical LTE system. Extensive experiment results demonstrate that the RF fingerprints can be effectively erased to protect the user privacy.         ",
    "url": "https://arxiv.org/abs/2406.07349",
    "authors": [
      "Zhaoyi Lu",
      "Wenchao Xu",
      "Ming Tu",
      "Xin Xie",
      "Cunqing Hua",
      "Nan Cheng"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2406.07352",
    "title": "Stochastic Analysis of Homogeneous Wireless Networks Assisted by Intelligent Reflecting Surfaces",
    "abstract": "           In this paper, we study the impact of the existence of multiple IRSs in a homogeneous wireless network, in which all BSs, users (U), and IRSs are spatially distributed by an independent homogeneous PPP, with density $\\lambda_{\\rm BS}\\rm{[BS/m^2]}$, $\\lambda_{\\rm U}\\rm{[U/m^2]}$, and $\\lambda_{\\rm IRS}\\rm{[IRS/m^2]}$, respectively. We utilize a uniformly random serving strategy for BS and IRS to create stochastic symmetry in the network. We analyze the performance of the network and study the effect of the existence of the IRS on the network performance. To this end, for a typical user in the system, we derive analytical upper and lower bounds on the expectation of the power (second statistical moment) of the desired signal and the interference caused by BSs and other users. After that, we obtain analytical upper bounds on the decay of the probability of the power of the desired signal and the interference for the typical user (which results in a lower bound for the cumulative distribution function (CDF)). Moreover, we derive upper bounds on the decay of the probability of the capacity of one typical user, which results in a lower bound for the outage probability. In the numerical results, we observe that the numerical calculation of the power of the desired signal and the interference is near the derived lower bounds and we show that the increment of the parameter ${(\\lambda_{\\rm IRS})}$ causes increment in powers of both the desired and interference signals. We also observe that the increment of the parameter ${\\lambda_{\\rm IRS}}$ causes the decrement of outage probability.         ",
    "url": "https://arxiv.org/abs/2406.07352",
    "authors": [
      "Ali H. Abdollahi Bafghi",
      "Mahtab Mirmohseni",
      "Masoumeh Nasiri-Kenari",
      "Behrouz Maham",
      "Umberto Spagnolini"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2406.07353",
    "title": "Toxic Memes: A Survey of Computational Perspectives on the Detection and Explanation of Meme Toxicities",
    "abstract": "           Internet memes, channels for humor, social commentary, and cultural expression, are increasingly used to spread toxic messages. Studies on the computational analyses of toxic memes have significantly grown over the past five years, and the only three surveys on computational toxic meme analysis cover only work published until 2022, leading to inconsistent terminology and unexplored trends. Our work fills this gap by surveying content-based computational perspectives on toxic memes, and reviewing key developments until early 2024. Employing the PRISMA methodology, we systematically extend the previously considered papers, achieving a threefold result. First, we survey 119 new papers, analyzing 158 computational works focused on content-based toxic meme analysis. We identify over 30 datasets used in toxic meme analysis and examine their labeling systems. Second, after observing the existence of unclear definitions of meme toxicity in computational works, we introduce a new taxonomy for categorizing meme toxicity types. We also note an expansion in computational tasks beyond the simple binary classification of memes as toxic or non-toxic, indicating a shift towards achieving a nuanced comprehension of toxicity. Third, we identify three content-based dimensions of meme toxicity under automatic study: target, intent, and conveyance tactics. We develop a framework illustrating the relationships between these dimensions and meme toxicities. The survey analyzes key challenges and recent trends, such as enhanced cross-modal reasoning, integrating expert and cultural knowledge, the demand for automatic toxicity explanations, and handling meme toxicity in low-resource languages. Also, it notes the rising use of Large Language Models (LLMs) and generative AI for detecting and generating toxic memes. Finally, it proposes pathways for advancing toxic meme detection and interpretation.         ",
    "url": "https://arxiv.org/abs/2406.07353",
    "authors": [
      "Delfina Sol Martinez Pandiani",
      "Erik Tjong Kim Sang",
      "Davide Ceolin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2406.07357",
    "title": "PSMC: Provable and Scalable Algorithms for Motif Conductance Based Graph Clustering",
    "abstract": "           Higher-order graph clustering aims to partition the graph using frequently occurring subgraphs. Motif conductance is one of the most promising higher-order graph clustering models due to its strong interpretability. However, existing motif conductance based graph clustering algorithms are mainly limited by a seminal two-stage reweighting computing framework, needing to enumerate all motif instances to obtain an edge-weighted graph for partitioning. However, such a framework has two-fold vital defects: (1) It can only provide a quadratic bound for the motif with three vertices, and whether there is provable clustering quality for other motifs is still an open question. (2) The enumeration procedure of motif instances incurs prohibitively high costs against large motifs or large dense graphs due to combinatorial explosions. Besides, expensive spectral clustering or local graph diffusion on the edge-weighted graph also makes existing methods unable to handle massive graphs with millions of nodes. To overcome these dilemmas, we propose a Provable and Scalable Motif Conductance algorithm PSMC, which has a fixed and motif-independent approximation ratio for any motif. Specifically, PSMC first defines a new vertex metric Motif Resident based on the given motif, which can be computed locally. Then, it iteratively deletes the vertex with the smallest motif resident value very efficiently using novel dynamic update technologies. Finally, it outputs the locally optimal result during the above iterative process. To further boost efficiency, we propose several effective bounds to estimate the motif resident value of each vertex, which can greatly reduce computational costs. Empirical results show that our proposed algorithms achieve 3.2-32 times speedup and improve the quality by at least 12 times than the baselines.         ",
    "url": "https://arxiv.org/abs/2406.07357",
    "authors": [
      "Longlong Lin",
      "Tao Jia",
      "Zeli Wang",
      "Jin Zhao",
      "Rong-Hua Li"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2406.07361",
    "title": "Deep Implicit Optimization for Robust and Flexible Image Registration",
    "abstract": "           Deep Learning in Image Registration (DLIR) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time. However, DLIR methods forego many of the benefits of classical optimization-based methods. The functional nature of deep networks do not guarantee that the predicted transformation is a local minima of the registration objective, the representation of the transformation (displacement/velocity field/affine) is fixed, and the networks are not robust to domain shift. Our method aims to bridge this gap between classical and learning methods by incorporating optimization as a layer in a deep network. A deep network is trained to predict multi-scale dense feature images that are registered using a black box iterative optimization solver. This optimal warp is then used to minimize image and label alignment errors. By implicitly differentiating end-to-end through an iterative optimization solver, our learned features are registration and label-aware, and the warp functions are guaranteed to be local minima of the registration objective in the feature space. Our framework shows excellent performance on in-domain datasets, and is agnostic to domain shift such as anisotropy and varying intensity profiles. For the first time, our method allows switching between arbitrary transformation representations (free-form to diffeomorphic) at test time with zero retraining. End-to-end feature learning also facilitates interpretability of features, and out-of-the-box promptability using additional label-fidelity terms at inference.         ",
    "url": "https://arxiv.org/abs/2406.07361",
    "authors": [
      "Rohit Jena",
      "Pratik Chaudhari",
      "James C. Gee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2406.07365",
    "title": "BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction",
    "abstract": "           Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including aspect term, opinion term, aspect category, and sentiment polarity. In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model. Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications. Therefore, we first construct a few-shot ASQP dataset (FSQP) that contains richer categories and is more balanced for the few-shot study. Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence. However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates. To tackle this issue, we further propose a Broadview Soft Prompting (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates. Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensen-Shannon divergence. BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates. Then, we aggregate the results of multi-templates by voting mechanism. Empirical results demonstrate that BvSP significantly outperforms the stateof-the-art methods under four few-shot settings and other public datasets. Our code and dataset are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.07365",
    "authors": [
      "Yinhao Bai",
      "Yalan Xie",
      "Xiaoyi Liu",
      "Yuhua Zhao",
      "Zhixin Han",
      "Mengting Hu",
      "Hang Gao",
      "Renhong Cheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.07377",
    "title": "COLoRIS: Localization-agnostic Smart Surfaces Enabling Opportunistic ISAC in 6G Networks",
    "abstract": "           The integration of Smart Surfaces in 6G communication networks, also dubbed as Reconfigurable Intelligent Surfaces (RISs), is a promising paradigm change gaining significant attention given its disruptive features. RISs are a key enabler in the realm of 6G Integrated Sensing and Communication (ISAC) systems where novel services can be offered together with the future mobile networks communication capabilities. This paper addresses the critical challenge of precisely localizing users within a communication network by leveraging the controlled-reflective properties of RIS elements without relying on more power-hungry traditional methods, e.g., GPS, adverting the need of deploying additional infrastructure and even avoiding interfering with communication efforts. Moreover, we go one step beyond: we build COLoRIS, an Opportunistic ISAC approach that leverages localization agnostic RIS configurations to accurately position mobile users via trained learning models. Extensive experimental validation and simulations in large-scale synthetic scenarios show 5% positioning errors (with respect to field size) under different conditions. Further, we show that a low-complexity version running in a limited off-the-shelf (embedded, low-power) system achieves positioning errors in the 11% range at a negligible +2% energy expense with respect to the classical RIS.         ",
    "url": "https://arxiv.org/abs/2406.07377",
    "authors": [
      "Guillermo Encinas-Lago",
      "Francesco Devoti",
      "Marco Rossanese",
      "Vincenzo Sciancalepore",
      "Marco Di Renzo",
      "Xavier Costa-P\u00e9rez"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2406.07378",
    "title": "Large Language Models for Constrained-Based Causal Discovery",
    "abstract": "           Causality is essential for understanding complex systems, such as the economy, the brain, and the climate. Constructing causal graphs often relies on either data-driven or expert-driven approaches, both fraught with challenges. The former methods, like the celebrated PC algorithm, face issues with data requirements and assumptions of causal sufficiency, while the latter demand substantial time and domain knowledge. This work explores the capabilities of Large Language Models (LLMs) as an alternative to domain experts for causal graph generation. We frame conditional independence queries as prompts to LLMs and employ the PC algorithm with the answers. The performance of the LLM-based conditional independence oracle on systems with known causal graphs shows a high degree of variability. We improve the performance through a proposed statistical-inspired voting schema that allows some control over false-positive and false-negative rates. Inspecting the chain-of-thought argumentation, we find causal reasoning to justify its answer to a probabilistic query. We show evidence that knowledge-based CIT could eventually become a complementary tool for data-driven causal discovery.         ",
    "url": "https://arxiv.org/abs/2406.07378",
    "authors": [
      "Kai-Hendrik Cohrs",
      "Gherardo Varando",
      "Emiliano Diaz",
      "Vasileios Sitokonstantinou",
      "Gustau Camps-Valls"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.07387",
    "title": "Machine Learning-Based Channel Prediction for RIS-assisted MIMO Systems With Channel Aging",
    "abstract": "           Reconfigurable intelligent surfaces (RISs) have emerged as a promising technology to enhance the performance of sixth-generation (6G) and beyond communication systems. The passive nature of RISs and their large number of reflecting elements pose challenges to the channel estimation process. The associated complexity further escalates when the channel coefficients are fast-varying as in scenarios with user mobility. In this paper, we propose an extended channel estimation framework for RIS-assisted multiple-input multiple-output (MIMO) systems based on a convolutional neural network (CNN) integrated with an autoregressive (AR) predictor. The implemented framework is designed for identifying the aging pattern and predicting enhanced estimates of the wireless channels in correlated fast-fading environments. Insightful simulation results demonstrate that our proposed CNN-AR approach is robust to channel aging, exhibiting a high-precision estimation accuracy. The results also show that our approach can achieve high spectral efficiency and low pilot overhead compared to traditional methods.         ",
    "url": "https://arxiv.org/abs/2406.07387",
    "authors": [
      "Nipuni Ginige",
      "Arthur Sousa de Sena",
      "Nurul Huda Mahmood",
      "Nandana Rajatheva",
      "Matti Latva-aho"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2406.07389",
    "title": "Robust Image Semantic Coding with Learnable CSI Fusion Masking over MIMO Fading Channels",
    "abstract": "           Though achieving marvelous progress in various scenarios, existing semantic communication frameworks mainly consider single-input single-output Gaussian channels or Rayleigh fading channels, neglecting the widely-used multiple-input multiple-output (MIMO) channels, which hinders the application into practical systems. One common solution to combat MIMO fading is to utilize feedback MIMO channel state information (CSI). In this paper, we incorporate MIMO CSI into system designs from a new perspective and propose the learnable CSI fusion semantic communication (LCFSC) framework, where CSI is treated as side information by the semantic extractor to enhance the semantic coding. To avoid feature fusion due to abrupt combination of CSI with features, we present a non-invasive CSI fusion multi-head attention module inside the Swin Transformer. With the learned attention masking map determined by both source and channel states, more robust attention distribution could be generated. Furthermore, the percentage of mask elements could be flexibly adjusted by the learnable mask ratio, which is produced based on the conditional variational interference in an unsupervised manner. In this way, CSI-aware semantic coding is achieved through learnable CSI fusion masking. Experiment results testify the superiority of LCFSC over traditional schemes and state-of-the-art Swin Transformer-based semantic communication frameworks in MIMO fading channels.         ",
    "url": "https://arxiv.org/abs/2406.07389",
    "authors": [
      "Bingyan Xie",
      "Yongpeng Wu",
      "Yuxuan Shi",
      "Wenjun Zhang",
      "Shuguang Cui",
      "Merouane Debbah"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07398",
    "title": "Visual Representation Learning with Stochastic Frame Prediction",
    "abstract": "           Self-supervised learning of image representations by predicting future frames is a promising direction but still remains a challenge. This is because of the under-determined nature of frame prediction; multiple potential futures can arise from a single current frame. To tackle this challenge, in this paper, we revisit the idea of stochastic video generation that learns to capture uncertainty in frame prediction and explore its effectiveness for representation learning. Specifically, we design a framework that trains a stochastic frame prediction model to learn temporal information between frames. Moreover, to learn dense information within each frame, we introduce an auxiliary masked image modeling objective along with a shared decoder architecture. We find this architecture allows for combining both objectives in a synergistic and compute-efficient manner. We demonstrate the effectiveness of our framework on a variety of tasks from video label propagation and vision-based robot learning domains, such as video segmentation, pose tracking, vision-based robotic locomotion, and manipulation tasks. Code is available on the project webpage: this https URL.         ",
    "url": "https://arxiv.org/abs/2406.07398",
    "authors": [
      "Huiwon Jang",
      "Dongyoung Kim",
      "Junsu Kim",
      "Jinwoo Shin",
      "Pieter Abbeel",
      "Younggyo Seo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2406.07402",
    "title": "A Survey on Recent Random Walk-based Methods for Embedding Knowledge Graphs",
    "abstract": "           Machine learning, deep learning, and NLP methods on knowledge graphs are present in different fields and have important roles in various domains from self-driving cars to friend recommendations on social media platforms. However, to apply these methods to knowledge graphs, the data usually needs to be in an acceptable size and format. In fact, knowledge graphs normally have high dimensions and therefore we need to transform them to a low-dimensional vector space. An embedding is a low-dimensional space into which you can translate high dimensional vectors in a way that intrinsic features of the input data are preserved. In this review, we first explain knowledge graphs and their embedding and then review some of the random walk-based embedding methods that have been developed recently.         ",
    "url": "https://arxiv.org/abs/2406.07402",
    "authors": [
      "Elika Bozorgi",
      "Sakher Khalil Alqaiidi",
      "Afsaneh Shams",
      "Hamid Reza Arabnia",
      "Krzysztof Kochut"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07411",
    "title": "VersiCode: Towards Version-controllable Code Generation",
    "abstract": "           Significant research has focused on improving the performance of large language model on code-related tasks due to their practical importance. Although performance is typically evaluated using public benchmark datasets, the existing datasets do not account for the concept of \\emph{version}, which is crucial in professional software development. In this paper, we introduce VersiCode, the first comprehensive dataset designed to assess the ability of large language models to generate verifiable code for specific library versions. VersiCode encompasses 300 libraries across more than 2,000 versions spanning 9 years. We design two dedicated evaluation tasks: version-specific code completion (VSCC) and version-aware code editing (VACE). Comprehensive experiments are conducted to benchmark the performance of LLMs, revealing the challenging nature of these tasks and VersiCode, that even state-of-the-art LLMs struggle to generate version-correct code. This dataset, together with the proposed tasks, sheds light on LLMs' capabilities and limitations in handling version-specific code generation, and opens up an important new area of research for further investigation. The resources can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.07411",
    "authors": [
      "Tongtong Wu",
      "Weigang Wu",
      "Xingyu Wang",
      "Kang Xu",
      "Suyu Ma",
      "Bo Jiang",
      "Ping Yang",
      "Zhenchang Xing",
      "Yuan-Fang Li",
      "Gholamreza Haffari"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.07413",
    "title": "Holistic Memory Diversification for Incremental Learning in Growing Graphs",
    "abstract": "           This paper addresses the challenge of incremental learning in growing graphs with increasingly complex tasks. The goal is to continually train a graph model to handle new tasks while retaining its inference ability on previous tasks. Existing methods usually neglect the importance of memory diversity, limiting in effectively selecting high-quality memory from previous tasks and remembering broad previous knowledge within the scarce memory on graphs. To address that, we introduce a novel holistic Diversified Memory Selection and Generation (DMSG) framework for incremental learning in graphs, which first introduces a buffer selection strategy that considers both intra-class and inter-class diversities, employing an efficient greedy algorithm for sampling representative training nodes from graphs into memory buffers after learning each new task. Then, to adequately rememorize the knowledge preserved in the memory buffer when learning new tasks, we propose a diversified memory generation replay method. This method first utilizes a variational layer to generate the distribution of buffer node embeddings and sample synthesized ones for replaying. Furthermore, an adversarial variational embedding learning method and a reconstruction-based decoder are proposed to maintain the integrity and consolidate the generalization of the synthesized node embeddings, respectively. Finally, we evaluate our model on node classification tasks involving increasing class numbers. Extensive experimental results on publicly accessible datasets demonstrate the superiority of DMSG over state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2406.07413",
    "authors": [
      "Ziyue Qiao",
      "Junren Xiao",
      "Qingqiang Sun",
      "Meng Xiao",
      "Hui Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07420",
    "title": "Graph Reasoning for Explainable Cold Start Recommendation",
    "abstract": "           The cold start problem, where new users or items have no interaction history, remains a critical challenge in recommender systems (RS). A common solution involves using Knowledge Graphs (KG) to train entity embeddings or Graph Neural Networks (GNNs). Since KGs incorporate auxiliary data and not just user/item interactions, these methods can make relevant recommendations for cold users or items. Graph Reasoning (GR) methods, however, find paths from users to items to recommend using relations in the KG and, in the context of RS, have been used for interpretability. In this study, we propose GRECS: a framework for adapting GR to cold start recommendations. By utilizing explicit paths starting for users rather than relying only on entity embeddings, GRECS can find items corresponding to users' preferences by navigating the graph, even when limited information about users is available. Our experiments show that GRECS mitigates the cold start problem and outperforms competitive baselines across 5 standard datasets while being explainable. This study highlights the potential of GR for developing explainable recommender systems better suited for managing cold users and items.         ",
    "url": "https://arxiv.org/abs/2406.07420",
    "authors": [
      "Jibril Frej",
      "Marta Knezevic",
      "Tanja Kaser"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2406.07421",
    "title": "A Comprehensive Investigation on Speaker Augmentation for Speaker Recognition",
    "abstract": "           Data augmentation (DA) has played a pivotal role in the success of deep speaker recognition. Current DA techniques primarily focus on speaker-preserving augmentation, which does not change the speaker trait of the speech and does not create new speakers. Recent research has shed light on the potential of speaker augmentation, which generates new speakers to enrich the training dataset. In this study, we delve into two speaker augmentation approaches: speed perturbation (SP) and vocal tract length perturbation (VTLP). Despite the empirical utilization of both methods, a comprehensive investigation into their efficacy is lacking. Our study, conducted using two public datasets, VoxCeleb and CN-Celeb, revealed that both SP and VTLP are proficient at generating new speakers, leading to significant performance improvements in speaker recognition. Furthermore, they exhibit distinct properties in sensitivity to perturbation factors and data complexity, hinting at the potential benefits of their fusion. Our research underscores the substantial potential of speaker augmentation, highlighting the importance of in-depth exploration and analysis.         ",
    "url": "https://arxiv.org/abs/2406.07421",
    "authors": [
      "Zhenyu Zhou",
      "Shibiao Xu",
      "Shi Yin",
      "Lantian Li",
      "Dong Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2406.07430",
    "title": "Learning Domain-Invariant Features for Out-of-Context News Detection",
    "abstract": "           Multimodal out-of-context news is a common type of misinformation on online media platforms. This involves posting a caption, alongside an invalid out-of-context news image. Reflecting its importance, researchers have developed models to detect such misinformation. However, a common limitation of these models is that they only consider the scenario where pre-labeled data is available for each domain, failing to address the out-of-context news detection on unlabeled domains (e.g., unverified news on new topics or agencies). In this work, we therefore focus on domain adaptive out-of-context news detection. In order to effectively adapt the detection model to unlabeled news topics or agencies, we propose ConDA-TTA (Contrastive Domain Adaptation with Test-Time Adaptation) which applies contrastive learning and maximum mean discrepancy (MMD) to learn the domain-invariant feature. In addition, it leverages target domain statistics during test-time to further assist domain adaptation. Experimental results show that our approach outperforms baselines in 5 out of 7 domain adaptation settings on two public datasets, by as much as 2.93% in F1 and 2.08% in accuracy.         ",
    "url": "https://arxiv.org/abs/2406.07430",
    "authors": [
      "Yimeng Gu",
      "Mengqi Zhang",
      "Ignacio Castro",
      "Shu Wu",
      "Gareth Tyson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2406.07431",
    "title": "Active Scout: Multi-Target Tracking Using Neural Radiance Fields in Dense Urban Environments",
    "abstract": "           We study pursuit-evasion games in highly occluded urban environments, e.g. tall buildings in a city, where a scout (quadrotor) tracks multiple dynamic targets on the ground. We show that we can build a neural radiance field (NeRF) representation of the city -- online -- using RGB and depth images from different vantage points. This representation is used to calculate the information gain to both explore unknown parts of the city and track the targets -- thereby giving a completely first-principles approach to actively tracking dynamic targets. We demonstrate, using a custom-built simulator using Open Street Maps data of Philadelphia and New York City, that we can explore and locate 20 stationary targets within 300 steps. This is slower than a greedy baseline which which does not use active perception. But for dynamic targets that actively hide behind occlusions, we show that our approach maintains, at worst, a tracking error of 200m; the greedy baseline can have a tracking error as large as 600m. We observe a number of interesting properties in the scout's policies, e.g., it switches its attention to track a different target periodically, as the quality of the NeRF representation improves over time, the scout also becomes better in terms of target tracking.         ",
    "url": "https://arxiv.org/abs/2406.07431",
    "authors": [
      "Christopher D. Hsu",
      "Pratik Chaudhari"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07432",
    "title": "Matryoshka Representation Learning for Recommendation",
    "abstract": "           Representation learning is essential for deep-neural-network-based recommender systems to capture user preferences and item features within fixed-dimensional user and item vectors. Unlike existing representation learning methods that either treat each user preference and item feature uniformly or categorize them into discrete clusters, we argue that in the real world, user preferences and item features are naturally expressed and organized in a hierarchical manner, leading to a new direction for representation learning. In this paper, we introduce a novel matryoshka representation learning method for recommendation (MRL4Rec), by which we restructure user and item vectors into matryoshka representations with incrementally dimensional and overlapping vector spaces to explicitly represent user preferences and item features at different hierarchical levels. We theoretically establish that constructing training triplets specific to each level is pivotal in guaranteeing accurate matryoshka representation learning. Subsequently, we propose the matryoshka negative sampling mechanism to construct training triplets, which further ensures the effectiveness of the matryoshka representation learning in capturing hierarchical user preferences and item features. The experiments demonstrate that MRL4Rec can consistently and substantially outperform a number of state-of-the-art competitors on several real-life datasets. Our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.07432",
    "authors": [
      "Riwei Lai",
      "Li Chen",
      "Weixin Chen",
      "Rui Chen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2406.07435",
    "title": "Beware of Aliases -- Signal Preservation is Crucial for Robust Image Restoration",
    "abstract": "           Image restoration networks are usually comprised of an encoder and a decoder, responsible for aggregating image content from noisy, distorted data and to restore clean, undistorted images, respectively. Data aggregation as well as high-resolution image generation both usually come at the risk of involving aliases, i.e.~standard architectures put their ability to reconstruct the model input in jeopardy to reach high PSNR values on validation data. The price to be paid is low model robustness. In this work, we show that simply providing alias-free paths in state-of-the-art reconstruction transformers supports improved model robustness at low costs on the restoration performance. We do so by proposing BOA-Restormer, a transformer-based image restoration model that executes downsampling and upsampling operations partly in the frequency domain to ensure alias-free paths along the entire model while potentially preserving all relevant high-frequency information.         ",
    "url": "https://arxiv.org/abs/2406.07435",
    "authors": [
      "Shashank Agnihotri",
      "Julia Grabinski",
      "Janis Keuper",
      "Margret Keuper"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2406.07436",
    "title": "McEval: Massively Multilingual Code Evaluation",
    "abstract": "           Code large language models (LLMs) have shown remarkable advances in code understanding, completion, and generation tasks. Programming benchmarks, comprised of a selection of code challenges and corresponding test cases, serve as a standard to evaluate the capability of different LLMs in such tasks. However, most existing benchmarks primarily focus on Python and are still restricted to a limited number of languages, where other languages are translated from the Python samples (e.g. MultiPL-E) degrading the data diversity. To further facilitate the research of code LLMs, we propose a massively multilingual code benchmark covering 40 programming languages (McEval) with 16K test samples, which substantially pushes the limits of code LLMs in multilingual scenarios. The benchmark contains challenging code completion, understanding, and generation evaluation tasks with finely curated massively multilingual instruction corpora McEval-Instruct. In addition, we introduce an effective multilingual coder mCoder trained on McEval-Instruct to support multilingual programming language generation. Extensive experimental results on McEval show that there is still a difficult journey between open-source models and closed-source LLMs (e.g. GPT-series models) in numerous languages. The instruction corpora, evaluation benchmark, and leaderboard are available at \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2406.07436",
    "authors": [
      "Linzheng Chai",
      "Shukai Liu",
      "Jian Yang",
      "Yuwei Yin",
      "Ke Jin",
      "Jiaheng Liu",
      "Tao Sun",
      "Ge Zhang",
      "Changyu Ren",
      "Hongcheng Guo",
      "Zekun Wang",
      "Boyang Wang",
      "Xianjie Wu",
      "Bing Wang",
      "Tongliang Li",
      "Liqun Yang",
      "Sufeng Duan",
      "Zhoujun Li"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2406.07444",
    "title": "On the Robustness of Document-Level Relation Extraction Models to Entity Name Variations",
    "abstract": "           Driven by the demand for cross-sentence and large-scale relation extraction, document-level relation extraction (DocRE) has attracted increasing research interest. Despite the continuous improvement in performance, we find that existing DocRE models which initially perform well may make more mistakes when merely changing the entity names in the document, hindering the generalization to novel entity names. To this end, we systematically investigate the robustness of DocRE models to entity name variations in this work. We first propose a principled pipeline to generate entity-renamed documents by replacing the original entity names with names from Wikidata. By applying the pipeline to DocRED and Re-DocRED datasets, we construct two novel benchmarks named Env-DocRED and Env-Re-DocRED for robustness evaluation. Experimental results show that both three representative DocRE models and two in-context learned large language models consistently lack sufficient robustness to entity name variations, particularly on cross-sentence relation instances and documents with more entities. Finally, we propose an entity variation robust training method which not only improves the robustness of DocRE models but also enhances their understanding and reasoning capabilities. We further verify that the basic idea of this method can be effectively transferred to in-context learning for DocRE as well.         ",
    "url": "https://arxiv.org/abs/2406.07444",
    "authors": [
      "Shiao Meng",
      "Xuming Hu",
      "Aiwei Liu",
      "Fukun Ma",
      "Yawen Yang",
      "Shuang Li",
      "Lijie Wen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.07450",
    "title": "Benchmarking Vision-Language Contrastive Methods for Medical Representation Learning",
    "abstract": "           We perform a comprehensive benchmarking of contrastive frameworks for learning multimodal representations in the medical domain. Through this study, we aim to answer the following research questions: (i) How transferable are general-domain representations to the medical domain? (ii) Is multimodal contrastive training sufficient, or does it benefit from unimodal training as well? (iii) What is the impact of feature granularity on the effectiveness of multimodal medical representation learning? To answer these questions, we investigate eight contrastive learning approaches under identical training setups, and train them on 2.8 million image-text pairs from four datasets, and evaluate them on 25 downstream tasks, including classification (zero-shot and linear probing), image-to-text and text-to-image retrieval, and visual question-answering. Our findings suggest a positive answer to the first question, a negative answer to the second question, and the benefit of learning fine-grained features. Finally, we make our code publicly available.         ",
    "url": "https://arxiv.org/abs/2406.07450",
    "authors": [
      "Shuvendu Roy",
      "Yasaman Parhizkar",
      "Franklin Ogidi",
      "Vahid Reza Khazaie",
      "Michael Colacci",
      "Ali Etemad",
      "Elham Dolatabadi",
      "Arash Afkanpour"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07453",
    "title": "HTVM: Efficient Neural Network Deployment On Heterogeneous TinyML Platforms",
    "abstract": "           Optimal deployment of deep neural networks (DNNs) on state-of-the-art Systems-on-Chips (SoCs) is crucial for tiny machine learning (TinyML) at the edge. The complexity of these SoCs makes deployment non-trivial, as they typically contain multiple heterogeneous compute cores with limited, programmer-managed memory to optimize latency and energy efficiency. We propose HTVM - a compiler that merges TVM with DORY to maximize the utilization of heterogeneous accelerators and minimize data movements. HTVM allows deploying the MLPerf(TM) Tiny suite on DIANA, an SoC with a RISC-V CPU, and digital and analog compute-in-memory AI accelerators, at 120x improved performance over plain TVM deployment.         ",
    "url": "https://arxiv.org/abs/2406.07453",
    "authors": [
      "Josse Van Delm",
      "Maarten Vandersteegen",
      "Alessio Burrello",
      "Giuseppe Maria Sarda",
      "Francesco Conti",
      "Daniele Jahier Pagliari",
      "Luca Benini",
      "Marian Verhelst"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2406.07456",
    "title": "fKAN: Fractional Kolmogorov-Arnold Networks with trainable Jacobi basis functions",
    "abstract": "           Recent advancements in neural network design have given rise to the development of Kolmogorov-Arnold Networks (KANs), which enhance speed, interpretability, and precision. This paper presents the Fractional Kolmogorov-Arnold Network (fKAN), a novel neural network architecture that incorporates the distinctive attributes of KANs with a trainable adaptive fractional-orthogonal Jacobi function as its basis function. By leveraging the unique mathematical properties of fractional Jacobi functions, including simple derivative formulas, non-polynomial behavior, and activity for both positive and negative input values, this approach ensures efficient learning and enhanced accuracy. The proposed architecture is evaluated across a range of tasks in deep learning and physics-informed deep learning. Precision is tested on synthetic regression data, image classification, image denoising, and sentiment analysis. Additionally, the performance is measured on various differential equations, including ordinary, partial, and fractional delay differential equations. The results demonstrate that integrating fractional Jacobi functions into KANs significantly improves training speed and performance across diverse fields and applications.         ",
    "url": "https://arxiv.org/abs/2406.07456",
    "authors": [
      "Alireza Afzal Aghaei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2406.07466",
    "title": "Multimodal Belief Prediction",
    "abstract": "           Recognizing a speaker's level of commitment to a belief is a difficult task; humans do not only interpret the meaning of the words in context, but also understand cues from intonation and other aspects of the audio signal. Many papers and corpora in the NLP community have approached the belief prediction task using text-only approaches. We are the first to frame and present results on the multimodal belief prediction task. We use the CB-Prosody corpus (CBP), containing aligned text and audio with speaker belief annotations. We first report baselines and significant features using acoustic-prosodic features and traditional machine learning methods. We then present text and audio baselines for the CBP corpus fine-tuning on BERT and Whisper respectively. Finally, we present our multimodal architecture which fine-tunes on BERT and Whisper and uses multiple fusion methods, improving on both modalities alone.         ",
    "url": "https://arxiv.org/abs/2406.07466",
    "authors": [
      "John Murzaku",
      "Adil Soubki",
      "Owen Rambow"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2406.07467",
    "title": "Anomaly Detection on Unstable Logs with GPT Models",
    "abstract": "           Log-based anomaly detection has been widely studied in the literature as a way to increase the dependability of software-intensive systems. In reality, logs can be unstable due to changes made to the software during its evolution. This, in turn, degrades the performance of downstream log analysis activities, such as anomaly detection. The critical challenge in detecting anomalies on these unstable logs is the lack of information about the new logs, due to insufficient log data from new software versions. The application of Large Language Models (LLMs) to many software engineering tasks has revolutionized various domains. In this paper, we report on an experimental comparison of a fine-tuned LLM and alternative models for anomaly detection on unstable logs. The main motivation is that the pre-training of LLMs on vast datasets may enable a robust understanding of diverse patterns and contextual information, which can be leveraged to mitigate the data insufficiency issue in the context of software evolution. Our experimental results on the two-version dataset of LOGEVOL-Hadoop show that the fine-tuned LLM (GPT-3) fares slightly better than supervised baselines when evaluated on unstable logs. The difference between GPT-3 and other supervised approaches tends to become more significant as the degree of changes in log sequences increases. However, it is unclear whether the difference is practically significant in all cases. Lastly, our comparison of prompt engineering (with GPT-4) and fine-tuning reveals that the latter provides significantly superior performance on both stable and unstable logs, offering valuable insights into the effective utilization of LLMs in this domain.         ",
    "url": "https://arxiv.org/abs/2406.07467",
    "authors": [
      "Fatemeh Hadadi",
      "Qinghua Xu",
      "Domenico Bianculli",
      "Lionel Briand"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2406.07473",
    "title": "Choreographing the Rhythms of Observation: Dynamics for Ranged Observer Bipartite-Unipartite SpatioTemporal (ROBUST) Networks",
    "abstract": "           Existing network analysis methods struggle to optimize observer placements in dynamic environments with limited visibility. This dissertation introduces the novel ROBUST (Ranged Observer Bipartite-Unipartite SpatioTemporal) framework, offering a significant advancement in modeling, analyzing, and optimizing observer networks within complex spatiotemporal domains. ROBUST leverages a unique bipartite-unipartite approach, distinguishing between observer and observable entities while incorporating spatial constraints and temporal dynamics. This research extends spatiotemporal network theory by introducing novel graph-based measures, including myopic degree, spatial closeness centrality, and edge length proportion. These measures, coupled with advanced clustering techniques like Proximal Recurrence, provide insights into network structure, resilience, and the effectiveness of observer placements. The ROBUST framework demonstrates superior resource allocation and strategic responsiveness compared to conventional models. Case studies in oceanographic monitoring, urban safety networks, and multi-agent path planning showcases its practical applicability and adaptability. Results demonstrate significant improvements in coverage, response times, and overall network efficiency. This work paves the way for future research in incorporating imperfect knowledge, refining temporal pathing methodologies, and expanding the scope of applications. By bridging theoretical advancements with practical solutions, ROBUST stands as a significant contribution to the field, promising to inform and inspire ongoing and future endeavors in network optimization and multi-agent system planning.         ",
    "url": "https://arxiv.org/abs/2406.07473",
    "authors": [
      "Ted Edward Holmberg"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2406.07480",
    "title": "Image Neural Field Diffusion Models",
    "abstract": "           Diffusion models have shown an impressive ability to model complex data distributions, with several key advantages over GANs, such as stable training, better coverage of the training distribution's modes, and the ability to solve inverse problems without extra training. However, most diffusion models learn the distribution of fixed-resolution images. We propose to learn the distribution of continuous images by training diffusion models on image neural fields, which can be rendered at any resolution, and show its advantages over fixed-resolution models. To achieve this, a key challenge is to obtain a latent space that represents photorealistic image neural fields. We propose a simple and effective method, inspired by several recent techniques but with key changes to make the image neural fields photorealistic. Our method can be used to convert existing latent diffusion autoencoders into image neural field autoencoders. We show that image neural field diffusion models can be trained using mixed-resolution image datasets, outperform fixed-resolution diffusion models followed by super-resolution models, and can solve inverse problems with conditions applied at different scales efficiently.         ",
    "url": "https://arxiv.org/abs/2406.07480",
    "authors": [
      "Yinbo Chen",
      "Oliver Wang",
      "Richard Zhang",
      "Eli Shechtman",
      "Xiaolong Wang",
      "Michael Gharbi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07483",
    "title": "Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing",
    "abstract": "           In the rapidly evolving landscape of Natural Language Processing (NLP), the use of Large Language Models (LLMs) for automated text annotation in social media posts has garnered significant interest. Despite the impressive innovations in developing LLMs like ChatGPT, their efficacy, and accuracy as annotation tools are not well understood. In this paper, we analyze the performance of eight open-source and proprietary LLMs for annotating the stance expressed in social media posts, benchmarking their performance against human annotators' (i.e., crowd-sourced) judgments. Additionally, we investigate the conditions under which LLMs are likely to disagree with human judgment. A significant finding of our study is that the explicitness of text expressing a stance plays a critical role in how faithfully LLMs' stance judgments match humans'. We argue that LLMs perform well when human annotators do, and when LLMs fail, it often corresponds to situations in which human annotators struggle to reach an agreement. We conclude with recommendations for a comprehensive approach that combines the precision of human expertise with the scalability of LLM predictions. This study highlights the importance of improving the accuracy and comprehensiveness of automated stance detection, aiming to advance these technologies for more efficient and unbiased analysis of social media.         ",
    "url": "https://arxiv.org/abs/2406.07483",
    "authors": [
      "Mao Li",
      "Frederick Conrad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.07484",
    "title": "Towards Generalized Hydrological Forecasting using Transformer Models for 120-Hour Streamflow Prediction",
    "abstract": "           This study explores the efficacy of a Transformer model for 120-hour streamflow prediction across 125 diverse locations in Iowa, US. Utilizing data from the preceding 72 hours, including precipitation, evapotranspiration, and discharge values, we developed a generalized model to predict future streamflow. Our approach contrasts with traditional methods that typically rely on location-specific models. We benchmarked the Transformer model's performance against three deep learning models (LSTM, GRU, and Seq2Seq) and the Persistence approach, employing Nash-Sutcliffe Efficiency (NSE), Kling-Gupta Efficiency (KGE), Pearson's r, and Normalized Root Mean Square Error (NRMSE) as metrics. The study reveals the Transformer model's superior performance, maintaining higher median NSE and KGE scores and exhibiting the lowest NRMSE values. This indicates its capability to accurately simulate and predict streamflow, adapting effectively to varying hydrological conditions and geographical variances. Our findings underscore the Transformer model's potential as an advanced tool in hydrological modeling, offering significant improvements over traditional and contemporary approaches.         ",
    "url": "https://arxiv.org/abs/2406.07484",
    "authors": [
      "Bekir Z. Demiray",
      "Ibrahim Demir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07487",
    "title": "GLAD: Towards Better Reconstruction with Global and Local Adaptive Diffusion Models for Unsupervised Anomaly Detection",
    "abstract": "           Diffusion models have shown superior performance on unsupervised anomaly detection tasks. Since trained with normal data only, diffusion models tend to reconstruct normal counterparts of test images with certain noises added. However, these methods treat all potential anomalies equally, which may cause two main problems. From the global perspective, the difficulty of reconstructing images with different anomalies is uneven. Therefore, instead of utilizing the same setting for all samples, we propose to predict a particular denoising step for each sample by evaluating the difference between image contents and the priors extracted from diffusion models. From the local perspective, reconstructing abnormal regions differs from normal areas even in the same image. Theoretically, the diffusion model predicts a noise for each step, typically following a standard Gaussian distribution. However, due to the difference between the anomaly and its potential normal counterpart, the predicted noise in abnormal regions will inevitably deviate from the standard Gaussian distribution. To this end, we propose introducing synthetic abnormal samples in training to encourage the diffusion models to break through the limitation of standard Gaussian distribution, and a spatial-adaptive feature fusion scheme is utilized during inference. With the above modifications, we propose a global and local adaptive diffusion model (abbreviated to GLAD) for unsupervised anomaly detection, which introduces appealing flexibility and achieves anomaly-free reconstruction while retaining as much normal information as possible. Extensive experiments are conducted on three commonly used anomaly detection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit board dataset (PCB-Bank) we integrated, showing the effectiveness of the proposed method.         ",
    "url": "https://arxiv.org/abs/2406.07487",
    "authors": [
      "Hang Yao",
      "Ming Liu",
      "Haolin Wang",
      "Zhicun Yin",
      "Zifei Yan",
      "Xiaopeng Hong",
      "Wangmeng Zuo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07498",
    "title": "RaD-Net 2: A causal two-stage repairing and denoising speech enhancement network with knowledge distillation and complex axial self-attention",
    "abstract": "           In real-time speech communication systems, speech signals are often degraded by multiple distortions. Recently, a two-stage Repair-and-Denoising network (RaD-Net) was proposed with superior speech quality improvement in the ICASSP 2024 Speech Signal Improvement (SSI) Challenge. However, failure to use future information and constraint receptive field of convolution layers limit the system's performance. To mitigate these problems, we extend RaD-Net to its upgraded version, RaD-Net 2. Specifically, a causality-based knowledge distillation is introduced in the first stage to use future information in a causal way. We use the non-causal repairing network as the teacher to improve the performance of the causal repairing network. In addition, in the second stage, complex axial self-attention is applied in the denoising network's complex feature encoder/decoder. Experimental results on the ICASSP 2024 SSI Challenge blind test set show that RaD-Net 2 brings 0.10 OVRL DNSMOS improvement compared to RaD-Net.         ",
    "url": "https://arxiv.org/abs/2406.07498",
    "authors": [
      "Mingshuai Liu",
      "Zhuangqi Chen",
      "Xiaopeng Yan",
      "Yuanjun Lv",
      "Xianjun Xia",
      "Chuanzeng Huang",
      "Yijian Xiao",
      "Lei Xie"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2406.07499",
    "title": "Trim 3D Gaussian Splatting for Accurate Geometry Representation",
    "abstract": "           In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to reconstruct accurate 3D geometry from images. Previous arts for geometry reconstruction from 3D Gaussians mainly focus on exploring strong geometry regularization. Instead, from a fresh perspective, we propose to obtain accurate 3D geometry of a scene by Gaussian trimming, which selectively removes the inaccurate geometry while preserving accurate structures. To achieve this, we analyze the contributions of individual 3D Gaussians and propose a contribution-based trimming strategy to remove the redundant or inaccurate Gaussians. Furthermore, our experimental and theoretical analyses reveal that a relatively small Gaussian scale is a non-negligible factor in representing and optimizing the intricate details. Therefore the proposed TrimGS maintains relatively small Gaussian scales. In addition, TrimGS is also compatible with the effective geometry regularization strategies in previous arts. When combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS consistently yields more accurate geometry and higher perceptual quality. Our project page is this https URL ",
    "url": "https://arxiv.org/abs/2406.07499",
    "authors": [
      "Lue Fan",
      "Yuxue Yang",
      "Minxing Li",
      "Hongsheng Li",
      "Zhaoxiang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2406.07520",
    "title": "Neural Gaffer: Relighting Any Object via Diffusion",
    "abstract": "           Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting. Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight. Alternatively, some methods explicitly decompose a scene into intrinsic components, such as normals and BRDFs, which can be inaccurate or under-expressive. In this work, we propose a novel end-to-end 2D relighting diffusion model, called Neural Gaffer, that takes a single image of any object and can synthesize an accurate, high-quality relit image under any novel environmental lighting condition, simply by conditioning an image generator on a target environment map, without an explicit scene decomposition. Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model. We evaluate our model on both synthetic and in-the-wild Internet imagery and demonstrate its advantages in terms of generalization and accuracy. Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion. Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field.         ",
    "url": "https://arxiv.org/abs/2406.07520",
    "authors": [
      "Haian Jin",
      "Yuan Li",
      "Fujun Luan",
      "Yuanbo Xiangli",
      "Sai Bi",
      "Kai Zhang",
      "Zexiang Xu",
      "Jin Sun",
      "Noah Snavely"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2406.06545",
    "title": "Boosting end-to-end entanglement fidelity in quantum repeater networks via hybridized strategies",
    "abstract": "           Quantum networks are expected to enhance distributed quantum computing and quantum communication over long distances while providing security dependent upon physical effects rather than mathematical assumptions. Through simulation, we show that a quantum network utilizing only entanglement purification or only quantum error correction as error management strategies cannot create Bell pairs with fidelity that exceeds the requirement for a secured quantum key distribution protocol for a broad range of hardware parameters. We propose hybrid strategies utilizing quantum error correction on top of purification and show that they can produce Bell pairs of sufficiently high fidelity. We identify the error parameter regime for gate and measurement errors in which these hybrid strategies are applicable.         ",
    "url": "https://arxiv.org/abs/2406.06545",
    "authors": [
      "Poramet Pathumsoot",
      "Theerapat Tansuwannont",
      "Naphan Benchasattabuse",
      "Ryosuke Satoh",
      "Michal Hajdu\u0161ek",
      "Poompong Chaiwongkhot",
      "Sujin Suwanna",
      "Rodney Van Meter"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2406.06594",
    "title": "Stock Movement Prediction with Multimodal Stable Fusion via Gated Cross-Attention Mechanism",
    "abstract": "           The accurate prediction of stock movements is crucial for investment strategies. Stock prices are subject to the influence of various forms of information, including financial indicators, sentiment analysis, news documents, and relational structures. Predominant analytical approaches, however, tend to address only unimodal or bimodal sources, neglecting the complexity of multimodal data. Further complicating the landscape are the issues of data sparsity and semantic conflicts between these modalities, which are frequently overlooked by current models, leading to unstable performance and limiting practical applicability. To address these shortcomings, this study introduces a novel architecture, named Multimodal Stable Fusion with Gated Cross-Attention (MSGCA), designed to robustly integrate multimodal input for stock movement prediction. The MSGCA framework consists of three integral components: (1) a trimodal encoding module, responsible for processing indicator sequences, dynamic documents, and a relational graph, and standardizing their feature representations; (2) a cross-feature fusion module, where primary and consistent features guide the multimodal fusion of the three modalities via a pair of gated cross-attention networks; and (3) a prediction module, which refines the fused features through temporal and dimensional reduction to execute precise movement forecasting. Empirical evaluations demonstrate that the MSGCA framework exceeds current leading methods, achieving performance gains of 8.1%, 6.1%, 21.7% and 31.6% on four multimodal datasets, respectively, attributed to its enhanced multimodal fusion stability.         ",
    "url": "https://arxiv.org/abs/2406.06594",
    "authors": [
      "Chang Zong",
      "Jian Shao",
      "Weiming Lu",
      "Yueting Zhuang"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.06646",
    "title": "Emotion-Aware Speech Self-Supervised Representation Learning with Intensity Knowledge",
    "abstract": "           Speech Self-Supervised Learning (SSL) has demonstrated considerable efficacy in various downstream tasks. Nevertheless, prevailing self-supervised models often overlook the incorporation of emotion-related prior information, thereby neglecting the potential enhancement of emotion task comprehension through emotion prior knowledge in speech. In this paper, we propose an emotion-aware speech representation learning with intensity knowledge. Specifically, we extract frame-level emotion intensities using an established speech-emotion understanding model. Subsequently, we propose a novel emotional masking strategy (EMS) to incorporate emotion intensities into the masking process. We selected two representative models based on Transformer and CNN, namely MockingJay and Non-autoregressive Predictive Coding (NPC), and conducted experiments on IEMOCAP dataset. Experiments have demonstrated that the representations derived from our proposed method outperform the original model in SER task.         ",
    "url": "https://arxiv.org/abs/2406.06646",
    "authors": [
      "Rui Liu",
      "Zening Ma"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2406.06749",
    "title": "Federated Nonparametric Hypothesis Testing with Differential Privacy Constraints: Optimal Rates and Adaptive Tests",
    "abstract": "           Federated learning has attracted significant recent attention due to its applicability across a wide range of settings where data is collected and analyzed across disparate locations. In this paper, we study federated nonparametric goodness-of-fit testing in the white-noise-with-drift model under distributed differential privacy (DP) constraints. We first establish matching lower and upper bounds, up to a logarithmic factor, on the minimax separation rate. This optimal rate serves as a benchmark for the difficulty of the testing problem, factoring in model characteristics such as the number of observations, noise level, and regularity of the signal class, along with the strictness of the $(\\epsilon,\\delta)$-DP requirement. The results demonstrate interesting and novel phase transition phenomena. Furthermore, the results reveal an interesting phenomenon that distributed one-shot protocols with access to shared randomness outperform those without access to shared randomness. We also construct a data-driven testing procedure that possesses the ability to adapt to an unknown regularity parameter over a large collection of function classes with minimal additional cost, all while maintaining adherence to the same set of DP constraints.         ",
    "url": "https://arxiv.org/abs/2406.06749",
    "authors": [
      "T. Tony Cai",
      "Abhinav Chakraborty",
      "Lasse Vuursteen"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.06755",
    "title": "Optimal Federated Learning for Nonparametric Regression with Heterogeneous Distributed Differential Privacy Constraints",
    "abstract": "           This paper studies federated learning for nonparametric regression in the context of distributed samples across different servers, each adhering to distinct differential privacy constraints. The setting we consider is heterogeneous, encompassing both varying sample sizes and differential privacy constraints across servers. Within this framework, both global and pointwise estimation are considered, and optimal rates of convergence over the Besov spaces are established. Distributed privacy-preserving estimators are proposed and their risk properties are investigated. Matching minimax lower bounds, up to a logarithmic factor, are established for both global and pointwise estimation. Together, these findings shed light on the tradeoff between statistical accuracy and privacy preservation. In particular, we characterize the compromise not only in terms of the privacy budget but also concerning the loss incurred by distributing data within the privacy framework as a whole. This insight captures the folklore wisdom that it is easier to retain privacy in larger samples, and explores the differences between pointwise and global estimation under distributed privacy constraints.         ",
    "url": "https://arxiv.org/abs/2406.06755",
    "authors": [
      "T. Tony Cai",
      "Abhinav Chakraborty",
      "Lasse Vuursteen"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.06774",
    "title": "ComFeAT: Combination of Neural and Spectral Features for Improved Depression Detection",
    "abstract": "           In this work, we focus on the detection of depression through speech analysis. Previous research has widely explored features extracted from pre-trained models (PTMs) primarily trained for paralinguistic tasks. Although these features have led to sufficient advances in speech-based depression detection, their performance declines in real-world settings. To address this, in this paper, we introduce ComFeAT, an application that employs a CNN model trained on a combination of features extracted from PTMs, a.k.a. neural features and spectral features to enhance depression detection. Spectral features are robust to domain variations, but, they are not as good as neural features in performance, suprisingly, combining them shows complementary behavior and improves over both neural and spectral features individually. The proposed method also improves over previous state-of-the-art (SOTA) works on E-DAIC benchmark.         ",
    "url": "https://arxiv.org/abs/2406.06774",
    "authors": [
      "Orchid Chetia Phukan",
      "Sarthak Jain",
      "Shubham Singh",
      "Muskaan Singh",
      "Arun Balaji Buduru",
      "Rajesh Sharma"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2406.06798",
    "title": "The Reasonable Effectiveness of Speaker Embeddings for Violence Detection",
    "abstract": "           In this paper, we focus on audio violence detection (AVD). AVD is necessary for several reasons, especially in the context of maintaining safety, preventing harm, and ensuring security in various environments. This calls for accurate AVD systems. Like many related applications in audio processing, the most common approach for improving the performance, would be by leveraging self-supervised (SSL) pre-trained models (PTMs). However, as these SSL models are very large models with million of parameters and this can hinder real-world deployment especially in compute-constraint environment. To resolve this, we propose the usage of speaker recognition models which are much smaller compared to the SSL models. Experimentation with speaker recognition model embeddings with SVM & Random Forest as classifiers, we show that speaker recognition model embeddings perform the best in comparison to state-of-the-art (SOTA) SSL models and achieve SOTA results.         ",
    "url": "https://arxiv.org/abs/2406.06798",
    "authors": [
      "Sarthak Jain",
      "Orchid Chetia Phukan",
      "Arun Balaji Buduru",
      "Rajesh Sharma"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2406.06879",
    "title": "SpikePipe: Accelerated Training of Spiking Neural Networks via Inter-Layer Pipelining and Multiprocessor Scheduling",
    "abstract": "           Spiking Neural Networks (SNNs) have gained popularity due to their high energy efficiency. Prior works have proposed various methods for training SNNs, including backpropagation-based methods. Training SNNs is computationally expensive compared to their conventional counterparts and would benefit from multiprocessor hardware acceleration. This is the first paper to propose inter-layer pipelining to accelerate training in SNNs using systolic array-based processors and multiprocessor scheduling. The impact of training using delayed gradients is observed using three networks training on different datasets, showing no degradation for small networks and < 10% degradation for large networks. The mapping of various training tasks of the SNN onto systolic arrays is formulated, and the proposed scheduling method is evaluated on the three networks. The results are compared against standard pipelining algorithms. The results show that the proposed method achieves an average speedup of 1.6X compared to standard pipelining algorithms, with an upwards of 2X improvement in some cases. The incurred communication overhead due to the proposed method is less than 0.5% of the total required communication of training.         ",
    "url": "https://arxiv.org/abs/2406.06879",
    "authors": [
      "Sai Sanjeet",
      "Bibhu Datta Sahoo",
      "Keshab K. Parhi"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2406.06969",
    "title": "Data mining method of single-cell omics data to evaluate a pure tissue environmental effect on gene expression level",
    "abstract": "           While single-cell RNA-seq enables the investigation of the celltype effect on the transcriptome, the pure tissue environmental effect has not been well investigated. The bias in the combination of tissue and celltype in the body made it difficult to evaluate the effect of pure tissue environment by omics data mining. It is important to prevent statistical confounding among discrete variables such as celltype, tissue, and other categorical variables when evaluating the effects of these variables. We propose a novel method to enumerate suitable analysis units of variables for estimating the effects of tissue environment by extending the maximal biclique enumeration problem for bipartite graphs to $k$-partite hypergraphs. We applied the proposed method to a large mouse single-cell transcriptome dataset of Tabala Muris Senis to evaluate pure tissue environmental effects on gene expression. Data Mining using the proposed method revealed pure tissue environment effects on gene expression and its age-related change among adipose sub-tissues. The method proposed in this study helps evaluations of the effects of discrete variables in exploratory data mining of large-scale genomics datasets.         ",
    "url": "https://arxiv.org/abs/2406.06969",
    "authors": [
      "Daigo Okada",
      "Jianshen Zhu",
      "Kan Shota",
      "Yuuki Nishimura",
      "Kazuya Haraguchi"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2406.07005",
    "title": "DecoR: Deconfounding Time Series with Robust Regression",
    "abstract": "           Causal inference on time series data is a challenging problem, especially in the presence of unobserved confounders. This work focuses on estimating the causal effect between two time series, which are confounded by a third, unobserved time series. Assuming spectral sparsity of the confounder, we show how in the frequency domain this problem can be framed as an adversarial outlier problem. We introduce Deconfounding by Robust regression (DecoR), a novel approach that estimates the causal effect using robust linear regression in the frequency domain. Considering two different robust regression techniques, we first improve existing bounds on the estimation error for such techniques. Crucially, our results do not require distributional assumptions on the covariates. We can therefore use them in time series settings. Applying these results to DecoR, we prove, under suitable assumptions, upper bounds for the estimation error of DecoR that imply consistency. We show DecoR's effectiveness through experiments on synthetic data. Our experiments furthermore suggest that our method is robust with respect to model misspecification.         ",
    "url": "https://arxiv.org/abs/2406.07005",
    "authors": [
      "Felix Schur",
      "Jonas Peters"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07247",
    "title": "Dynamical Mean-Field Theory of Self-Attention Neural Networks",
    "abstract": "           Transformer-based models have demonstrated exceptional performance across diverse domains, becoming the state-of-the-art solution for addressing sequential machine learning problems. Even though we have a general understanding of the fundamental components in the transformer architecture, little is known about how they operate or what are their expected dynamics. Recently, there has been an increasing interest in exploring the relationship between attention mechanisms and Hopfield networks, promising to shed light on the statistical physics of transformer networks. However, to date, the dynamical regimes of transformer-like models have not been studied in depth. In this paper, we address this gap by using methods for the study of asymmetric Hopfield networks in nonequilibrium regimes --namely path integral methods over generating functionals, yielding dynamics governed by concurrent mean-field variables. Assuming 1-bit tokens and weights, we derive analytical approximations for the behavior of large self-attention neural networks coupled to a softmax output, which become exact in the large limit size. Our findings reveal nontrivial dynamical phenomena, including nonequilibrium phase transitions associated with chaotic bifurcations, even for very simple configurations with a few encoded features and a very short context window. Finally, we discuss the potential of our analytic approach to improve our understanding of the inner workings of transformer models, potentially reducing computational training costs and enhancing model interpretability.         ",
    "url": "https://arxiv.org/abs/2406.07247",
    "authors": [
      "\u00c1ngel Poc-L\u00f3pez",
      "Miguel Aguilera"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07248",
    "title": "Infinite-Horizon Distributionally Robust Regret-Optimal Control",
    "abstract": "           We study the infinite-horizon distributionally robust (DR) control of linear systems with quadratic costs, where disturbances have unknown, possibly time-correlated distribution within a Wasserstein-2 ambiguity set. We aim to minimize the worst-case expected regret-the excess cost of a causal policy compared to a non-causal one with access to future disturbance. Though the optimal policy lacks a finite-order state-space realization (i.e., it is non-rational), it can be characterized by a finite-dimensional parameter. Leveraging this, we develop an efficient frequency-domain algorithm to compute this optimal control policy and present a convex optimization method to construct a near-optimal state-space controller that approximates the optimal non-rational controller in the $\\mathit{H}_\\infty$-norm. This approach avoids solving a computationally expensive semi-definite program (SDP) that scales with the time horizon in the finite-horizon setting.         ",
    "url": "https://arxiv.org/abs/2406.07248",
    "authors": [
      "Taylan Kargin",
      "Joudi Hajar",
      "Vikrant Malik",
      "Babak Hassibi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2406.07250",
    "title": "Description and Discussion on DCASE 2024 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring",
    "abstract": "           We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 Challenge Task 2: First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring. Continuing from last year's DCASE 2023 Challenge Task 2, we organize the task as a first-shot problem under domain generalization required settings. The main goal of the first-shot problem is to enable rapid deployment of ASD systems for new kinds of machines without the need for machine-specific hyperparameter tunings. This problem setting was realized by (1) giving only one section for each machine type and (2) having completely different machine types for the development and evaluation datasets. For the DCASE 2024 Challenge Task 2, data of completely new machine types were newly collected and provided as the evaluation dataset. In addition, attribute information such as the machine operation conditions were concealed for several machine types to mimic situations where such information are unavailable. We will add challenge results and analysis of the submissions after the challenge submission deadline.         ",
    "url": "https://arxiv.org/abs/2406.07250",
    "authors": [
      "Tomoya Nishida",
      "Noboru Harada",
      "Daisuke Niizumi",
      "Davide Albertini",
      "Roberto Sannino",
      "Simone Pradolini",
      "Filippo Augusti",
      "Keisuke Imoto",
      "Kota Dohi",
      "Harsh Purohit",
      "Takashi Endo",
      "Yohei Kawaguchi"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2206.05091",
    "title": "Muffliato: Peer-to-Peer Privacy Amplification for Decentralized Optimization and Averaging",
    "abstract": "           Decentralized optimization is increasingly popular in machine learning for its scalability and efficiency. Intuitively, it should also provide better privacy guarantees, as nodes only observe the messages sent by their neighbors in the network graph. But formalizing and quantifying this gain is challenging: existing results are typically limited to Local Differential Privacy (LDP) guarantees that overlook the advantages of decentralization. In this work, we introduce pairwise network differential privacy, a relaxation of LDP that captures the fact that the privacy leakage from a node $u$ to a node $v$ may depend on their relative position in the graph. We then analyze the combination of local noise injection with (simple or randomized) gossip averaging protocols on fixed and random communication graphs. We also derive a differentially private decentralized optimization algorithm that alternates between local gradient descent steps and gossip averaging. Our results show that our algorithms amplify privacy guarantees as a function of the distance between nodes in the graph, matching the privacy-utility trade-off of the trusted curator, up to factors that explicitly depend on the graph topology. Finally, we illustrate our privacy gains with experiments on synthetic and real-world datasets.         ",
    "url": "https://arxiv.org/abs/2206.05091",
    "authors": [
      "Edwige Cyffers",
      "Mathieu Even",
      "Aur\u00e9lien Bellet",
      "Laurent Massouli\u00e9"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2210.12089",
    "title": "A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation, and Research Challenges",
    "abstract": "           Graph Neural Networks (GNNs) perform well in community detection and molecule classification. Counterfactual Explanations (CE) provide counter-examples to overcome the transparency limitations of black-box models. Due to the growing attention in graph learning, we focus on the concepts of CE for GNNs. We analysed the SoA to provide a taxonomy, a uniform notation, and the benchmarking datasets and evaluation metrics. We discuss fourteen methods, their evaluation protocols, twenty-two datasets, and nineteen metrics. We integrated the majority of methods into the GRETEL library to conduct an empirical evaluation to understand their strengths and pitfalls. We highlight open challenges and future work.         ",
    "url": "https://arxiv.org/abs/2210.12089",
    "authors": [
      "Mario Alfonso Prado-Romero",
      "Bardh Prenkaj",
      "Giovanni Stilo",
      "Fosca Giannotti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2302.09585",
    "title": "StreamingFlow: Streaming Occupancy Forecasting with Asynchronous Multi-modal Data Streams via Neural Ordinary Differential Equation",
    "abstract": "           Predicting the future occupancy states of the surrounding environment is a vital task for autonomous driving. However, current best-performing single-modality methods or multi-modality fusion perception methods are only able to predict uniform snapshots of future occupancy states and require strictly synchronized sensory data for sensor fusion. We propose a novel framework, StreamingFlow, to lift these strong limitations. StreamingFlow is a novel BEV occupancy predictor that ingests asynchronous multi-sensor data streams for fusion and performs streaming forecasting of the future occupancy map at any future timestamps. By integrating neural ordinary differential equations (N-ODE) into recurrent neural networks, StreamingFlow learns derivatives of BEV features over temporal horizons, updates the implicit sensor's BEV features as part of the fusion process, and propagates BEV states to the desired future time point. It shows good zero-shot generalization ability of prediction, reflected in the interpolation of the observed prediction time horizon and the reasonable inference of the unseen farther future period. Extensive experiments on two large-scale datasets, nuScenes and Lyft L5, demonstrate that StreamingFlow significantly outperforms previous vision-based, LiDAR-based methods, and shows superior performance compared to state-of-the-art fusion-based methods.         ",
    "url": "https://arxiv.org/abs/2302.09585",
    "authors": [
      "Yining Shi",
      "Kun Jiang",
      "Ke Wang",
      "Jiusi Li",
      "Yunlong Wang",
      "Mengmeng Yang",
      "Diange Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2302.11563",
    "title": "Self-supervised network distillation: an effective approach to exploration in sparse reward environments",
    "abstract": "           Reinforcement learning can solve decision-making problems and train an agent to behave in an environment according to a predesigned reward function. However, such an approach becomes very problematic if the reward is too sparse and so the agent does not come across the reward during the environmental exploration. The solution to such a problem may be to equip the agent with an intrinsic motivation that will provide informed exploration during which the agent is likely to also encounter external reward. Novelty detection is one of the promising branches of intrinsic motivation research. We present Self-supervised Network Distillation (SND), a class of intrinsic motivation algorithms based on the distillation error as a novelty indicator, where the predictor model and the target model are both trained. We adapted three existing self-supervised methods for this purpose and experimentally tested them on a set of ten environments that are considered difficult to explore. The results show that our approach achieves faster growth and higher external reward for the same training time compared to the baseline models, which implies improved exploration in a very sparse reward environment. In addition, the analytical methods we applied provide valuable explanatory insights into our proposed models.         ",
    "url": "https://arxiv.org/abs/2302.11563",
    "authors": [
      "Matej Pech\u00e1\u010d",
      "Michal Chovanec",
      "Igor Farka\u0161"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2303.06344",
    "title": "Graph Contrastive Learning under Heterophily via Graph Filters",
    "abstract": "           Graph contrastive learning (CL) methods learn node representations in a self-supervised manner by maximizing the similarity between the augmented node representations obtained via a GNN-based encoder. However, CL methods perform poorly on graphs with heterophily, where connected nodes tend to belong to different classes. In this work, we address this problem by proposing an effective graph CL method, namely HLCL, for learning graph representations under heterophily. HLCL first identifies a homophilic and a heterophilic subgraph based on the cosine similarity of node features. It then uses a low-pass and a high-pass graph filter to aggregate representations of nodes connected in the homophilic subgraph and differentiate representations of nodes in the heterophilic subgraph. The final node representations are learned by contrasting both the augmented high-pass filtered views and the augmented low-pass filtered node views. Our extensive experiments show that HLCL outperforms state-of-the-art graph CL methods on benchmark datasets with heterophily, as well as large-scale real-world graphs, by up to 7%, and outperforms graph supervised learning methods on datasets with heterophily by up to 10%.         ",
    "url": "https://arxiv.org/abs/2303.06344",
    "authors": [
      "Wenhan Yang",
      "Baharan Mirzasoleiman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.16146",
    "title": "Dias: Dynamic Rewriting of Pandas Code",
    "abstract": "           In recent years, dataframe libraries, such as pandas have exploded in popularity. Due to their flexibility, they are increasingly used in ad-hoc exploratory data analysis (EDA) workloads. These workloads are diverse, including custom functions which can span libraries or be written in pure Python. The majority of systems available to accelerate EDA workloads focus on bulk-parallel workloads, which contain vastly different computational patterns, typically within a single library. As a result, they can introduce excessive overheads for ad-hoc EDA workloads due to their expensive optimization techniques. Instead, we identify program rewriting as a lightweight technique which can offer substantial speedups while also avoiding slowdowns. We implemented our techniques in Dias, which rewrites notebook cells to be more efficient for ad-hoc EDA workloads. We develop techniques for efficient rewrites in Dias, including dynamic checking of preconditions under which rewrites are correct and just-in-time rewrites for notebook environments. We show that Dias can rewrite individual cells to be 57$\\times$ faster compared to pandas and 1909$\\times$ faster compared to optimized systems such as modin. Furthermore, Dias can accelerate whole notebooks by up to 3.6$\\times$ compared to pandas and 26.4$\\times$ compared to modin.         ",
    "url": "https://arxiv.org/abs/2303.16146",
    "authors": [
      "Stefanos Baziotis",
      "Daniel Kang",
      "Charith Mendis"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2303.16772",
    "title": "Maximin Headway Control of Automated Vehicles for System Optimal Dynamic Traffic Assignment in General Networks",
    "abstract": "           This study develops the headway control framework in a fully automated road network, as we believe headway of Automated Vehicles (AVs) is another influencing factor to traffic dynamics in addition to conventional vehicle behaviors (e.g. route and departure time choices). Specifically, we aim to search for the optimal time headway between AVs on each link that achieves the network-wide system optimal dynamic traffic assignment (SO-DTA). To this end, the headway-dependent fundamental diagram (HFD) and headway-dependent double queue model (HDQ) are developed to model the effect of dynamic headway on roads, and a dynamic network model is built. It is rigorously proved that the minimum headway could always achieve SO-DTA, yet the optimal headway is non-unique. Motivated by these two findings, this study defines a novel concept of maximin headway, which is the largest headway that still achieves SO-DTA in the network. Mathematical properties regarding maximin headway are analyzed and an efficient solution algorithm is developed. Numerical experiments on both a small and large network verify the effectiveness of the maximin headway control framework as well as the properties of maximin headway. This study sheds light on deriving the desired solution among the non-unique solutions in SO-DTA and provides implications regarding the safety margin of AVs under SO-DTA.         ",
    "url": "https://arxiv.org/abs/2303.16772",
    "authors": [
      "Jinxiao Du",
      "Wei Ma"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2304.01666",
    "title": "A Survey on Contextualised Semantic Shift Detection",
    "abstract": "           Semantic Shift Detection (SSD) is the task of identifying, interpreting, and assessing the possible change over time in the meanings of a target word. Traditionally, SSD has been addressed by linguists and social scientists through manual and time-consuming activities. In the recent years, computational approaches based on Natural Language Processing and word embeddings gained increasing attention to automate SSD as much as possible. In particular, over the past three years, significant advancements have been made almost exclusively based on word contextualised embedding models, which can handle the multiple usages/meanings of the words and better capture the related semantic shifts. In this paper, we survey the approaches based on contextualised embeddings for SSD (i.e., CSSDetection) and we propose a classification framework characterised by meaning representation, time-awareness, and learning modality dimensions. The framework is exploited i) to review the measures for shift assessment, ii) to compare the approaches on performance, and iii) to discuss the current issues in terms of scalability, interpretability, and robustness. Open challenges and future research directions about CSSDetection are finally outlined.         ",
    "url": "https://arxiv.org/abs/2304.01666",
    "authors": [
      "Stefano Montanelli",
      "Francesco Periti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2304.10553",
    "title": "Sparsity in neural networks can improve their privacy",
    "abstract": "           This article measures how sparsity can make neural networks more robust to membership inference attacks. The obtained empirical results show that sparsity improves the privacy of the network, while preserving comparable performances on the task at hand. This empirical study completes and extends existing literature.         ",
    "url": "https://arxiv.org/abs/2304.10553",
    "authors": [
      "Antoine Gonon",
      "L\u00e9on Zheng",
      "Cl\u00e9ment Lalanne",
      "Quoc-Tung Le",
      "Guillaume Lauga",
      "Can Pouliquen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2305.01713",
    "title": "Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks",
    "abstract": "           Disentangled latent spaces usually have better semantic separability and geometrical properties, which leads to better interpretability and more controllable data generation. While this has been well investigated in Computer Vision, in tasks such as image disentanglement, in the NLP domain sentence disentanglement is still comparatively under-investigated. Most previous work have concentrated on disentangling task-specific generative factors, such as sentiment, within the context of style transfer. In this work, we focus on a more general form of sentence disentanglement, targeting the localised modification and control of more general sentence semantic features. To achieve this, we contribute to a novel notion of sentence semantic disentanglement and introduce a flow-based invertible neural network (INN) mechanism integrated with a transformer-based language Autoencoder (AE) in order to deliver latent spaces with better separability properties. Experimental results demonstrate that the model can conform the distributed latent space into a better semantically disentangled sentence space, leading to improved language interpretability and controlled generation when compared to the recent state-of-the-art language VAE models.         ",
    "url": "https://arxiv.org/abs/2305.01713",
    "authors": [
      "Yingji Zhang",
      "Danilo S. Carvalho",
      "Andr\u00e9 Freitas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.17387",
    "title": "Learning from Integral Losses in Physics Informed Neural Networks",
    "abstract": "           This work proposes a solution for the problem of training physics-informed networks under partial integro-differential equations. These equations require an infinite or a large number of neural evaluations to construct a single residual for training. As a result, accurate evaluation may be impractical, and we show that naive approximations at replacing these integrals with unbiased estimates lead to biased loss functions and solutions. To overcome this bias, we investigate three types of potential solutions: the deterministic sampling approaches, the double-sampling trick, and the delayed target method. We consider three classes of PDEs for benchmarking; one defining Poisson problems with singular charges and weak solutions of up to 10 dimensions, another involving weak solutions on electro-magnetic fields and a Maxwell equation, and a third one defining a Smoluchowski coagulation problem. Our numerical results confirm the existence of the aforementioned bias in practice and also show that our proposed delayed target approach can lead to accurate solutions with comparable quality to ones estimated with a large sample size integral. Our implementation is open-source and available at this https URL.         ",
    "url": "https://arxiv.org/abs/2305.17387",
    "authors": [
      "Ehsan Saleh",
      "Saba Ghaffari",
      "Timothy Bretl",
      "Luke Olson",
      "Matthew West"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2306.04748",
    "title": "Analysis, Identification and Prediction of Parkinson Disease Sub-Types and Progression through Machine Learning",
    "abstract": "           This paper represents a groundbreaking advancement in Parkinson disease (PD) research by employing a novel machine learning framework to categorize PD into distinct subtypes and predict its progression. Utilizing a comprehensive dataset encompassing both clinical and neurological parameters, the research applies advanced supervised and unsupervised learning techniques. This innovative approach enables the identification of subtle, yet critical, patterns in PD manifestation, which traditional methodologies often miss. Significantly, this research offers a path toward personalized treatment strategies, marking a major stride in the precision medicine domain and showcasing the transformative potential of integrating machine learning into medical research.         ",
    "url": "https://arxiv.org/abs/2306.04748",
    "authors": [
      "Ashwin Ram"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.16979",
    "title": "Post-train Black-box Defense via Bayesian Boundary Correction",
    "abstract": "           Classifiers based on deep neural networks are susceptible to adversarial attack, where the widely existing vulnerability has invoked the research in defending them from potential threats. Given a vulnerable classifier, existing defense methods are mostly white-box and often require re-training the victim under modified loss functions/training regimes. While the model/data/training specifics of the victim are usually unavailable to the user, re-training is unappealing, if not impossible for reasons such as limited computational resources. To this end, we propose a new post-train black-box defense framework. It can turn any pre-trained classifier into a resilient one with little knowledge of the model specifics. This is achieved by new joint Bayesian treatments on the clean data, the adversarial examples and the classifier, for maximizing their joint probability. It is further equipped with a new post-train strategy which keeps the victim intact, avoiding re-training. We name our framework Bayesian Boundary Correction (BBC). BBC is a general and flexible framework that can easily adapt to different data types. We instantiate BBC for image classification and skeleton-based human activity recognition, for both static and dynamic data. Exhaustive evaluation shows that BBC has superior robustness and can enhance robustness without severely hurting the clean accuracy, compared with existing defense methods.         ",
    "url": "https://arxiv.org/abs/2306.16979",
    "authors": [
      "He Wang",
      "Yunfeng Diao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2307.06701",
    "title": "S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction",
    "abstract": "           We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evaluations despite a much smaller model size. Finally, we boost S-HR-VQVAE by proposing a novel training method to jointly estimate the HR-VQVAE and ST-PixelCNN parameters.         ",
    "url": "https://arxiv.org/abs/2307.06701",
    "authors": [
      "Mohammad Adiban",
      "Kalin Stefanov",
      "Sabato Marco Siniscalchi",
      "Giampiero Salvi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.07107",
    "title": "Graph Positional and Structural Encoder",
    "abstract": "           Positional and structural encodings (PSE) enable better identifiability of nodes within a graph, rendering them essential tools for empowering modern GNNs, and in particular graph Transformers. However, designing PSEs that work optimally for all graph prediction tasks is a challenging and unsolved problem. Here, we present the Graph Positional and Structural Encoder (GPSE), the first-ever graph encoder designed to capture rich PSE representations for augmenting any GNN. GPSE learns an efficient common latent representation for multiple PSEs, and is highly transferable: The encoder trained on a particular graph dataset can be used effectively on datasets drawn from markedly different distributions and modalities. We show that across a wide range of benchmarks, GPSE-enhanced models can significantly outperform those that employ explicitly computed PSEs, and at least match their performance in others. Our results pave the way for the development of foundational pre-trained graph encoders for extracting positional and structural information, and highlight their potential as a more powerful and efficient alternative to explicitly computed PSEs and existing self-supervised pre-training approaches. Our framework and pre-trained models are publicly available at this https URL. For convenience, GPSE has also been integrated into the PyG library to facilitate downstream applications.         ",
    "url": "https://arxiv.org/abs/2307.07107",
    "authors": [
      "Semih Cant\u00fcrk",
      "Renming Liu",
      "Olivier Lapointe-Gagn\u00e9",
      "Vincent L\u00e9tourneau",
      "Guy Wolf",
      "Dominique Beaini",
      "Ladislav Ramp\u00e1\u0161ek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.07354",
    "title": "PG-Triggers: Triggers for Property Graphs",
    "abstract": "           Graph databases are emerging as the leading data management technology for storing large knowledge graphs; significant efforts are ongoing to produce new standards (such as the Graph Query Language, GQL), as well as enrich them with properties, types, schemas, and keys. In this article, we introduce PG-Triggers, a complete proposal for adding triggers to Property Graphs, along the direction marked by the SQL3 Standard. We define the syntax and semantics of PG-Triggers and then illustrate how they can be implemented on top of Neo4j, one of the most popular graph databases. In particular, we introduce a syntax-directed translation from PG-Triggers into Neo4j, which makes use of the so-called {\\it APOC triggers}; APOC is a community-contributed library for augmenting the Cypher query language supported by Neo4j. We also cover Memgraph, and show that our approach applies to this system in a similar way. We illustrate the use of PG-Triggers through a life science application inspired by the COVID-19 pandemic. The main objective of this article is to introduce an active database standard for graph databases as a first-class citizen at a time when reactive graph management is in its infancy, so as to minimize the conversion efforts towards a full-fledged standard proposal.         ",
    "url": "https://arxiv.org/abs/2307.07354",
    "authors": [
      "Stefano Ceri",
      "Anna Bernasconi",
      "Alessia Gagliardi",
      "Davide Martinenghi",
      "Luigi Bellomarini",
      "Davide Magnanimi"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2307.12900",
    "title": "Automotive Object Detection via Learning Sparse Events by Spiking Neurons",
    "abstract": "           Event-based sensors, distinguished by their high temporal resolution of 1 $\\mathrm{\\mu}\\text{s}$ and a dynamic range of 120 $\\text{dB}$, stand out as ideal tools for deployment in fast-paced settings like vehicles and drones. Traditional object detection techniques that utilize Artificial Neural Networks (ANNs) face challenges due to the sparse and asynchronous nature of the events these sensors capture. In contrast, Spiking Neural Networks (SNNs) offer a promising alternative, providing a temporal representation that is inherently aligned with event-based data. This paper explores the unique membrane potential dynamics of SNNs and their ability to modulate sparse events. We introduce an innovative spike-triggered adaptive threshold mechanism designed for stable training. Building on these insights, we present a specialized spiking feature pyramid network (SpikeFPN) optimized for automotive event-based object detection. Comprehensive evaluations demonstrate that SpikeFPN surpasses both traditional SNNs and advanced ANNs enhanced with attention mechanisms. Evidently, SpikeFPN achieves a mean Average Precision (mAP) of 0.477 on the GEN1 Automotive Detection (GAD) benchmark dataset, marking significant increases over the selected SNN baselines. Moreover, the efficient design of SpikeFPN ensures robust performance while optimizing computational resources, attributed to its innate sparse computation capabilities. Source codes are publicly accessible at this https URL.         ",
    "url": "https://arxiv.org/abs/2307.12900",
    "authors": [
      "Hu Zhang",
      "Yanchen Li",
      "Luziwei Leng",
      "Kaiwei Che",
      "Qian Liu",
      "Qinghai Guo",
      "Jianxing Liao",
      "Ran Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2307.16630",
    "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks",
    "abstract": "           The language models, especially the basic text classification models, have been shown to be susceptible to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving the model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\\ell_0$ perturbations in synonym substitution attacks. Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing. Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.         ",
    "url": "https://arxiv.org/abs/2307.16630",
    "authors": [
      "Xinyu Zhang",
      "Hanbin Hong",
      "Yuan Hong",
      "Peng Huang",
      "Binghui Wang",
      "Zhongjie Ba",
      "Kui Ren"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.03578",
    "title": "TeraHAC: Hierarchical Agglomerative Clustering of Trillion-Edge Graphs",
    "abstract": "           We introduce TeraHAC, a $(1+\\epsilon)$-approximate hierarchical agglomerative clustering (HAC) algorithm which scales to trillion-edge graphs. Our algorithm is based on a new approach to computing $(1+\\epsilon)$-approximate HAC, which is a novel combination of the nearest-neighbor chain algorithm and the notion of $(1+\\epsilon)$-approximate HAC. Our approach allows us to partition the graph among multiple machines and make significant progress in computing the clustering within each partition before any communication with other partitions is needed. We evaluate TeraHAC on a number of real-world and synthetic graphs of up to 8 trillion edges. We show that TeraHAC requires over 100x fewer rounds compared to previously known approaches for computing HAC. It is up to 8.3x faster than SCC, the state-of-the-art distributed algorithm for hierarchical clustering, while achieving 1.16x higher quality. In fact, TeraHAC essentially retains the quality of the celebrated HAC algorithm while significantly improving the running time.         ",
    "url": "https://arxiv.org/abs/2308.03578",
    "authors": [
      "Laxman Dhulipala",
      "Jason Lee",
      "Jakub \u0141\u0105cki",
      "Vahab Mirrokni"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Databases (cs.DB)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2308.09210",
    "title": "Efficient Algorithms for Attributed Graph Alignment with Vanishing Edge Correlation",
    "abstract": "           Graph alignment refers to the task of finding the vertex correspondence between two correlated graphs of $n$ vertices. Extensive study has been done on polynomial-time algorithms for the graph alignment problem under the Erd\u0151s-R\u00e9nyi graph pair model, where the two graphs are Erd\u0151s-R\u00e9nyi graphs with edge probability $q_\\mathrm{u}$, correlated under certain vertex correspondence. To achieve exact recovery of the correspondence, all existing algorithms at least require the edge correlation coefficient $\\rho_\\mathrm{u}$ between the two graphs to be \\emph{non-vanishing} as $n\\rightarrow\\infty$. Moreover, it is conjectured that no polynomial-time algorithm can achieve exact recovery under vanishing edge correlation $\\rho_\\mathrm{u}<1/\\mathrm{polylog}(n)$. In this paper, we show that with a vanishing amount of additional \\emph{attribute information}, exact recovery is polynomial-time feasible under \\emph{vanishing} edge correlation $\\rho_\\mathrm{u} \\ge n^{-\\Theta(1)}$. We identify a \\emph{local} tree structure, which incorporates one layer of user information and one layer of attribute information, and apply the subgraph counting technique to such structures. A polynomial-time algorithm is proposed that recovers the vertex correspondence for most of the vertices, and then refines the output to achieve exact recovery. The consideration of attribute information is motivated by real-world applications like LinkedIn and Twitter, where user attributes like birthplace and education background can aid alignment.         ",
    "url": "https://arxiv.org/abs/2308.09210",
    "authors": [
      "Ziao Wang",
      "Weina Wang",
      "Lele Wang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2308.15323",
    "title": "Occlusion-Aware Deep Convolutional Neural Network via Homogeneous Tanh-transforms for Face Parsing",
    "abstract": "           Face parsing infers a pixel-wise label map for each semantic facial component. Previous methods generally work well for uncovered faces, however, they overlook facial occlusion and ignore some contextual areas outside a single face, especially when facial occlusion has become a common situation during the COVID-19 epidemic. Inspired by the lighting phenomena in everyday life, where illumination from four distinct lamps provides a more uniform distribution than a single central light source, we propose a novel homogeneous tanh-transform for image preprocessing, which is made up of four tanh-transforms. These transforms fuse the central vision and the peripheral vision together. Our proposed method addresses the dilemma of face parsing under occlusion and compresses more information from the surrounding context. Based on homogeneous tanh-transforms, we propose an occlusion-aware convolutional neural network for occluded face parsing. It combines information in both Tanh-polar space and Tanh-Cartesian space, capable of enhancing receptive fields. Furthermore, we introduce an occlusion-aware loss to focus on the boundaries of occluded regions. The network is simple, flexible, and can be trained end-to-end. To facilitate future research of occluded face parsing, we also contribute a new cleaned face parsing dataset. This dataset is manually purified from several academic or industrial datasets, including CelebAMask-HQ, Short-video Face Parsing, and the Helen dataset, and will be made public. Experiments demonstrate that our method surpasses state-of-the-art methods in face parsing under occlusion.         ",
    "url": "https://arxiv.org/abs/2308.15323",
    "authors": [
      "Jianhua Qiua",
      "Weihua Liu",
      "Chaochao Lin",
      "Jiaojiao Li",
      "Haoping Yu",
      "Said Boumaraf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2309.04761",
    "title": "A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining",
    "abstract": "           Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, student behavior detection, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. We then analyze the practical challenges in EDM and propose targeted solutions. Finally, we point out emerging trends and future directions in this research area.         ",
    "url": "https://arxiv.org/abs/2309.04761",
    "authors": [
      "Yuanguo Lin",
      "Hong Chen",
      "Wei Xia",
      "Fan Lin",
      "Zongyue Wang",
      "Yong Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2309.11267",
    "title": "From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring",
    "abstract": "           Monitoring surface cracks in infrastructure is crucial for structural health monitoring. Automatic visual inspection offers an effective solution, especially in hard-to-reach areas. Machine learning approaches have proven their effectiveness but typically require large annotated datasets for supervised training. Once a crack is detected, monitoring its severity often demands precise segmentation of the damage. However, pixel-level annotation of images for segmentation is labor-intensive. To mitigate this cost, one can leverage explainable artificial intelligence (XAI) to derive segmentations from the explanations of a classifier, requiring only weak image-level supervision. This paper proposes applying this methodology to segment and monitor surface cracks. We evaluate the performance of various XAI methods and examine how this approach facilitates severity quantification and growth monitoring. Results reveal that while the resulting segmentation masks may exhibit lower quality than those produced by supervised methods, they remain meaningful and enable severity monitoring, thus reducing substantial labeling costs.         ",
    "url": "https://arxiv.org/abs/2309.11267",
    "authors": [
      "Florent Forest",
      "Hugo Porta",
      "Devis Tuia",
      "Olga Fink"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2310.05517",
    "title": "WeatherGNN: Exploiting Meteo- and Spatial-Dependencies for Local Numerical Weather Prediction Bias-Correction",
    "abstract": "           Due to insufficient local area information, numerical weather prediction (NWP) may yield biases for specific areas. Previous studies correct biases mainly by employing handcrafted features or applying data-driven methods intuitively, overlooking the complicated dependencies between weather factors and between areas. To address this issue, we propose WeatherGNN, a local NWP bias-correction method that utilizes Graph Neural Networks (GNNs) to exploit meteorological dependencies and spatial dependencies under the guidance of domain knowledge. Specifically, we introduce a factor GNN to capture area-specific meteorological dependencies adaptively based on spatial heterogeneity and a fast hierarchical GNN to capture dynamic spatial dependencies efficiently guided by Tobler's first and second laws of geography. Our experimental results on two real-world datasets demonstrate that WeatherGNN achieves the state-of-the-art performance, outperforming the best baseline with an average of 4.75 \\% on RMSE.         ",
    "url": "https://arxiv.org/abs/2310.05517",
    "authors": [
      "Binqing Wu",
      "Weiqi Chen",
      "Wengwei Wang",
      "Bingqing Peng",
      "Liang Sun",
      "Ling Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.05862",
    "title": "Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks",
    "abstract": "           Contrastive Language-Image Pre-training (CLIP) on large image-caption datasets has achieved remarkable success in zero-shot classification and enabled transferability to new domains. However, CLIP is extremely more vulnerable to targeted data poisoning and backdoor attacks, compared to supervised learning. Perhaps surprisingly, poisoning 0.0001% of CLIP pre-training data is enough to make targeted data poisoning attacks successful. This is four orders of magnitude smaller than what is required to poison supervised models. Despite this vulnerability, existing methods are very limited in defending CLIP models during pre-training. In this work, we propose a strong defense, SAFECLIP, to safely pre-train CLIP against targeted data poisoning and backdoor attacks. SAFECLIP warms up the model by applying unimodal contrastive learning (CL) on image and text modalities separately. Then, it divides the data into safe and risky sets, by applying a Gaussian Mixture Model to the cosine similarity of image-caption pair representations. SAFECLIP pre-trains the model by applying the CLIP loss to the safe set and applying unimodal CL to image and text modalities of the risky set separately. By gradually increasing the size of the safe set during pre-training, SAFECLIP effectively breaks targeted data poisoning and backdoor attacks without harming the CLIP performance. Our extensive experiments on CC3M, Visual Genome, and MSCOCO demonstrate that SAFECLIP significantly reduces the success rate of targeted data poisoning attacks from 93.75% to 0% and that of various backdoor attacks from up to 100% to 0%, without harming CLIP's performance.         ",
    "url": "https://arxiv.org/abs/2310.05862",
    "authors": [
      "Wenhan Yang",
      "Jingdong Gao",
      "Baharan Mirzasoleiman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2311.11961",
    "title": "NNG-Mix: Improving Semi-supervised Anomaly Detection with Pseudo-anomaly Generation",
    "abstract": "           Anomaly detection (AD) is essential in identifying rare and often critical events in complex systems, finding applications in fields such as network intrusion detection, financial fraud detection, and fault detection in infrastructure and industrial systems. While AD is typically treated as an unsupervised learning task due to the high cost of label annotation, it is more practical to assume access to a small set of labeled anomaly samples from domain experts, as is the case for semi-supervised anomaly detection. Semi-supervised and supervised approaches can leverage such labeled data, resulting in improved performance. In this paper, rather than proposing a new semi-supervised or supervised approach for AD, we introduce a novel algorithm for generating additional pseudo-anomalies on the basis of the limited labeled anomalies and a large volume of unlabeled data. This serves as an augmentation to facilitate the detection of new anomalies. Our proposed algorithm, named Nearest Neighbor Gaussian Mixup (NNG-Mix), efficiently integrates information from both labeled and unlabeled data to generate pseudo-anomalies. We compare the performance of this novel algorithm with commonly applied augmentation techniques, such as Mixup and Cutout. We evaluate NNG-Mix by training various existing semi-supervised and supervised anomaly detection algorithms on the original training data along with the generated pseudo-anomalies. Through extensive experiments on 57 benchmark datasets in ADBench, reflecting different data types, we demonstrate that NNG-Mix outperforms other data augmentation methods. It yields significant performance improvements compared to the baselines trained exclusively on the original training data. Notably, NNG-Mix yields up to 16.4%, 8.8%, and 8.0% improvements on Classical, CV, and NLP datasets in ADBench. Our source code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2311.11961",
    "authors": [
      "Hao Dong",
      "Ga\u00ebtan Frusque",
      "Yue Zhao",
      "Eleni Chatzi",
      "Olga Fink"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2311.12244",
    "title": "Provable Representation with Efficient Planning for Partial Observable Reinforcement Learning",
    "abstract": "           In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in learning, exploration and planning, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinforcement learning towards more practical applications.         ",
    "url": "https://arxiv.org/abs/2311.12244",
    "authors": [
      "Hongming Zhang",
      "Tongzheng Ren",
      "Chenjun Xiao",
      "Dale Schuurmans",
      "Bo Dai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2311.14772",
    "title": "Trainwreck: A damaging adversarial attack on image classifiers",
    "abstract": "           Adversarial attacks are an important security concern for computer vision (CV). As CV models are becoming increasingly valuable assets in applied practice, disrupting them is emerging as a form of economic sabotage. This paper opens up the exploration of damaging adversarial attacks (DAAs) that seek to damage target CV models. DAAs are formalized by defining the threat model, the cost function DAAs maximize, and setting three requirements for success: potency, stealth, and customizability. As a pioneer DAA, this paper proposes Trainwreck, a train-time attack that conflates the data of similar classes in the training data using stealthy ($\\epsilon \\leq 8/255$) class-pair universal perturbations obtained from a surrogate model. Trainwreck is a black-box, transferable attack: it requires no knowledge of the target architecture, and a single poisoned dataset degrades the performance of any model trained on it. The experimental evaluation on CIFAR-10 and CIFAR-100 and various model architectures (EfficientNetV2, ResNeXt-101, and a finetuned ViT-L-16) demonstrates Trainwreck's efficiency. Trainwreck achieves similar or better potency compared to the data poisoning state of the art and is fully customizable by the poison rate parameter. Finally, data redundancy with hashing is identified as a reliable defense against Trainwreck or similar DAAs. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2311.14772",
    "authors": [
      "Jan Zah\u00e1lka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2312.00093",
    "title": "GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs",
    "abstract": "           As pretrained text-to-image diffusion models become increasingly powerful, recent efforts have been made to distill knowledge from these text-to-image pretrained models for optimizing a text-guided 3D model. Most of the existing methods generate a holistic 3D model from a plain text input. This can be problematic when the text describes a complex scene with multiple objects, because the vectorized text embeddings are inherently unable to capture a complex description with multiple entities and relationships. Holistic 3D modeling of the entire scene further prevents accurate grounding of text entities and concepts. To address this limitation, we propose GraphDreamer, a novel framework to generate compositional 3D scenes from scene graphs, where objects are represented as nodes and their interactions as edges. By exploiting node and edge information in scene graphs, our method makes better use of the pretrained text-to-image diffusion model and is able to fully disentangle different objects without image-level supervision. To facilitate modeling of object-wise relationships, we use signed distance fields as representation and impose a constraint to avoid inter-penetration of objects. To avoid manual scene graph creation, we design a text prompt for ChatGPT to generate scene graphs based on text inputs. We conduct both qualitative and quantitative experiments to validate the effectiveness of GraphDreamer in generating high-fidelity compositional 3D scenes with disentangled object entities.         ",
    "url": "https://arxiv.org/abs/2312.00093",
    "authors": [
      "Gege Gao",
      "Weiyang Liu",
      "Anpei Chen",
      "Andreas Geiger",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2312.00335",
    "title": "Learning Anatomically Consistent Embedding for Chest Radiography",
    "abstract": "           Self-supervised learning (SSL) approaches have recently shown substantial success in learning visual representations from unannotated images. Compared with photographic images, medical images acquired with the same imaging protocol exhibit high consistency in anatomy. To exploit this anatomical consistency, this paper introduces a novel SSL approach, called PEAC (patch embedding of anatomical consistency), for medical image analysis. Specifically, in this paper, we propose to learn global and local consistencies via stable grid-based matching, transfer pre-trained PEAC models to diverse downstream tasks, and extensively demonstrate that (1) PEAC achieves significantly better performance than the existing state-of-the-art fully/self-supervised methods, and (2) PEAC captures the anatomical structure consistency across views of the same patient and across patients of different genders, weights, and healthy statuses, which enhances the interpretability of our method for medical image analysis.         ",
    "url": "https://arxiv.org/abs/2312.00335",
    "authors": [
      "Ziyu Zhou",
      "Haozhe Luo",
      "Jiaxuan Pang",
      "Xiaowei Ding",
      "Michael Gotway",
      "Jianming Liang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.06169",
    "title": "Two-Stage Adaptive Network for Semi-Supervised Cross-Domain Crater Detection under Varying Scenario Distributions",
    "abstract": "           Crater detection can provide valuable information for humans to explore the topography and understand the history of extraterrestrial planets. Due to the significantly varying scenario distributions, existing detection models trained on known labelled crater datasets are hardly effective when applied to new unlabelled planets. To address this issue, we propose a two-stage adaptive network (TAN) for semi-supervised cross-domain crater detection. Our network is built on the YOLOv5 detector, where a series of strategies are employed to enhance its cross-domain generalisation ability. In the first stage, we propose an attention-based scale-adaptive fusion (ASAF) strategy to handle objects with significant scale variances. Furthermore, we propose a smoothing hard example mining (SHEM) loss function to address the issue of overfitting on hard examples. In the second stage, we propose a sort-based pseudo-labelling fine-tuning (SPF) strategy for semi-supervised learning to mitigate the distributional differences between source and target domains. For both stages, we employ weak or strong image augmentation to suit different cross-domain tasks. Experimental results on benchmark datasets demonstrate that the proposed network can enhance domain adaptation ability for crater detection under varying scenario distributions.         ",
    "url": "https://arxiv.org/abs/2312.06169",
    "authors": [
      "Yifan Liu",
      "Tiecheng Song",
      "Chengye Xian",
      "Ruiyuan Chen",
      "Yi Zhao",
      "Rui Li",
      "Tan Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.07540",
    "title": "diff History for Neural Language Agents",
    "abstract": "           Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, results in long and verbose textual prompts. As a result, prior work in LM agents is limited to restricted domains with small observation size as well as minimal needs for interaction history or instruction tuning. In this paper, we introduce diff history, a simple and highly effective solution to these issues. By applying the Unix diff command on consecutive text observations in the interaction histories used to prompt LM policies, we can both abstract away redundant information and focus the content of textual inputs on the salient changes in the environment. On NetHack, an unsolved video game that requires long-horizon reasoning for decision-making, LMs tuned with diff history match state-of-the-art performance for neural agents while needing 1800x fewer training examples compared to prior work. Even on the simpler BabyAI-Text environment with concise text observations, we find that although diff history increases the length of prompts, the representation it provides offers a 25% improvement in the efficiency of low-sample instruction tuning. Further, we show that diff history scales favorably across different tuning dataset sizes. We open-source our code and data to this https URL.         ",
    "url": "https://arxiv.org/abs/2312.07540",
    "authors": [
      "Ulyana Piterbarg",
      "Lerrel Pinto",
      "Rob Fergus"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2401.04071",
    "title": "Fun with Flags: Robust Principal Directions via Flag Manifolds",
    "abstract": "           Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, creating novel robust and dual geodesic PCA variations. The remarkable flexibility offered by the 'flagification' introduced here enables even more algorithmic variants identified by specific flag types. Last but not least, we propose an effective convergent solver for these flag-formulations employing the Stiefel manifold. Our empirical results on both real-world and synthetic scenarios, demonstrate the superiority of our novel algorithms, especially in terms of robustness to outliers on manifolds.         ",
    "url": "https://arxiv.org/abs/2401.04071",
    "authors": [
      "Nathan Mankovich",
      "Gustau Camps-Valls",
      "Tolga Birdal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Differential Geometry (math.DG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2401.05967",
    "title": "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding",
    "abstract": "           The primary aim of Knowledge Graph embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach enhances the generality and flexibility of KGE models. The experimental results indicate that our new KGE model, OrthogonalE, is both general and flexible, significantly outperforming state-of-the-art KGE models while substantially reducing the number of relation parameters.         ",
    "url": "https://arxiv.org/abs/2401.05967",
    "authors": [
      "Yihua Zhu",
      "Hidetoshi Shimodaira"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2401.17342",
    "title": "A Latent Space Metric for Enhancing Prediction Confidence in Earth Observation Data",
    "abstract": "           This study presents a new approach for estimating confidence in machine learning model predictions, specifically in regression tasks utilizing Earth Observation (EO) data, with a particular focus on mosquito abundance (MA) estimation. We take advantage of a Variational AutoEncoder architecture, to derive a confidence metric by the latent space representations of EO datasets. This methodology is pivotal in establishing a correlation between the Euclidean distance in latent representations and the Absolute Error (AE) in individual MA predictions. Our research focuses on EO datasets from the Veneto region in Italy and the Upper Rhine Valley in Germany, targeting areas significantly affected by mosquito populations. A key finding is a notable correlation of 0.46 between the AE of MA predictions and the proposed confidence metric. This correlation signifies a robust, new metric for quantifying the reliability and enhancing the trustworthiness of the AI model's predictions in the context of both EO data analysis and mosquito abundance studies.         ",
    "url": "https://arxiv.org/abs/2401.17342",
    "authors": [
      "Ioannis Pitsiorlas",
      "Argyro Tsantalidou",
      "George Arvanitakis",
      "Marios Kountouris",
      "Charalambos Kontoes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.01915",
    "title": "Robust Inverse Graphics via Probabilistic Inference",
    "abstract": "           How do we infer a 3D scene from a single image in the presence of corruptions like rain, snow or fog? Straightforward domain randomization relies on knowing the family of corruptions ahead of time. Here, we propose a Bayesian approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene prior and an uninformative uniform corruption prior, making it applicable to a wide range of corruptions. Given a single image, RIG performs posterior inference jointly over the scene and the corruption. We demonstrate this idea by training a neural radiance field (NeRF) scene prior and using a secondary NeRF to represent the corruptions over which we place an uninformative prior. RIG, trained only on clean data, outperforms depth estimators and alternative NeRF approaches that perform point estimation instead of full inference. The results hold for a number of scene prior architectures based on normalizing flows and diffusion models. For the latter, we develop reconstruction-guidance with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is applicable in the presence of auxiliary latent variables such as the corruption. RIG demonstrates how scene priors can be used beyond generation tasks.         ",
    "url": "https://arxiv.org/abs/2402.01915",
    "authors": [
      "Tuan Anh Le",
      "Pavel Sountsov",
      "Matthew D. Hoffman",
      "Ben Lee",
      "Brian Patton",
      "Rif A. Saurous"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation (stat.CO)"
    ]
  },
  {
    "id": "arXiv:2402.05894",
    "title": "Large Language Model Meets Graph Neural Network in Knowledge Distillation",
    "abstract": "           In service-oriented architectures, accurately predicting the Quality of Service (QoS) is crucial for maintaining reliability and enhancing user satisfaction. However, significant challenges remain due to existing methods always overlooking high-order latent collaborative relationships between users and services and failing to dynamically adjust feature learning for every specific user-service invocation, which are critical for learning accurate features. Additionally, reliance on RNNs for capturing QoS evolution hampers models' ability to detect long-term trends due to difficulties in managing long-range dependencies. To address these challenges, we propose the \\underline{T}arget-Prompt \\underline{O}nline \\underline{G}raph \\underline{C}ollaborative \\underline{L}earning (TOGCL) framework for temporal-aware QoS prediction. TOGCL leverages a dynamic user-service invocation graph to model historical interactions, providing a comprehensive representation of user-service relationships. Building on this graph, it develops a target-prompt graph attention network to extract online deep latent features of users and services at each time slice, simultaneously considering implicit collaborative relationships between target users/services and their neighbors, as well as relevant historical QoS values. Additionally, a multi-layer Transformer encoder is employed to uncover temporal feature evolution patterns of users and services, leading to temporal-aware QoS prediction. Extensive experiments conducted on the WS-DREAM dataset demonstrate that our proposed TOGCL framework significantly outperforms state-of-the-art methods across multiple metrics, achieving improvements of up to 38.80\\%. These results underscore the effectiveness of the TOGCL framework for precise temporal QoS prediction.         ",
    "url": "https://arxiv.org/abs/2402.05894",
    "authors": [
      "Shengxiang Hu",
      "Guobing Zou",
      "Song Yang",
      "Yanglan Gan",
      "Bofeng Zhang",
      "Yixin Chen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.07207",
    "title": "GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting",
    "abstract": "           We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an instance-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. The source codes and models will be available at this http URL.         ",
    "url": "https://arxiv.org/abs/2402.07207",
    "authors": [
      "Xiaoyu Zhou",
      "Xingjian Ran",
      "Yajiao Xiong",
      "Jinlin He",
      "Zhiwei Lin",
      "Yongtao Wang",
      "Deqing Sun",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.07844",
    "title": "Mercury: A Code Efficiency Benchmark for Code Large Language Models",
    "abstract": "           Amidst the recent strides in evaluating Large Language Models for Code (Code LLMs), existing benchmarks have mainly focused on the functional correctness of generated code, neglecting the importance of their computational efficiency. To fill the gap, we present Mercury, the first code efficiency benchmark for Code LLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutions that serve as real-world efficiency baselines, enabling a comprehensive analysis of the runtime distribution. Based on the distribution, we introduce a new metric Beyond, which computes a runtime-percentile-weighted Pass score to reflect functional correctness and code efficiency simultaneously. On Mercury, leading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Given that an ideal Beyond score would be aligned with the Pass score, it indicates that while Code LLMs exhibit impressive capabilities in generating functionally correct code, there remains a notable gap in their efficiency. Finally, our empirical experiments reveal that Direct Preference Optimization (DPO) serves as a robust baseline for enhancing code efficiency compared with Supervised Fine Tuning (SFT), which paves a promising avenue for future exploration of efficient code generation. Our code and data are available on GitHub: this https URL.         ",
    "url": "https://arxiv.org/abs/2402.07844",
    "authors": [
      "Mingzhe Du",
      "Anh Tuan Luu",
      "Bin Ji",
      "Qian Liu",
      "See-Kiong Ng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.08180",
    "title": "Online Structured Prediction with Fenchel--Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss",
    "abstract": "           This paper studies online structured prediction with full-information feedback. For online multiclass classification, Van der Hoeven (2020) established \\emph{finite} surrogate regret bounds, which are independent of the time horizon, by introducing an elegant \\emph{exploit-the-surrogate-gap} framework. However, this framework has been limited to multiclass classification primarily because it relies on a classification-specific procedure for converting estimated scores to outputs. We extend the exploit-the-surrogate-gap framework to online structured prediction with \\emph{Fenchel--Young losses}, a large family of surrogate losses that includes the logistic loss for multiclass classification as a special case, obtaining finite surrogate regret bounds in various structured prediction problems. To this end, we propose and analyze \\emph{randomized decoding}, which converts estimated scores to general structured outputs. Moreover, by applying our decoding to online multiclass classification with the logistic loss, we obtain a surrogate regret bound of $O(\\| \\mathbf{U} \\|_\\mathrm{F}^2)$, where $\\mathbf{U}$ is the best offline linear estimator and $\\| \\cdot \\|_\\mathrm{F}$ denotes the Frobenius norm. This bound is tight up to logarithmic factors and improves the previous bound of $O(d\\| \\mathbf{U} \\|_\\mathrm{F}^2)$ due to Van der Hoeven (2020) by a factor of $d$, the number of classes.         ",
    "url": "https://arxiv.org/abs/2402.08180",
    "authors": [
      "Shinsaku Sakaue",
      "Han Bao",
      "Taira Tsuchiya",
      "Taihei Oki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.09591",
    "title": "Reconstructing the Geometry of Random Geometric Graphs",
    "abstract": "           Random geometric graphs are random graph models defined on metric spaces. Such a model is defined by first sampling points from a metric space and then connecting each pair of sampled points with probability that depends on their distance, independently among pairs. In this work, we show how to efficiently reconstruct the geometry of the underlying space from the sampled graph under the manifold assumption, i.e., assuming that the underlying space is a low dimensional manifold and that the connection probability is a strictly decreasing function of the Euclidean distance between the points in a given embedding of the manifold in $\\mathbb{R}^N$. Our work complements a large body of work on manifold learning, where the goal is to recover a manifold from sampled points sampled in the manifold along with their (approximate) distances.         ",
    "url": "https://arxiv.org/abs/2402.09591",
    "authors": [
      "Han Huang",
      "Pakawut Jiradilok",
      "Elchanan Mossel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2402.11457",
    "title": "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation",
    "abstract": "           Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls.         ",
    "url": "https://arxiv.org/abs/2402.11457",
    "authors": [
      "Shiyu Ni",
      "Keping Bi",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.13521",
    "title": "Test-Driven Development for Code Generation",
    "abstract": "           Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.         ",
    "url": "https://arxiv.org/abs/2402.13521",
    "authors": [
      "Noble Saji Mathews",
      "Meiyappan Nagappan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.14090",
    "title": "Social Environment Design",
    "abstract": "           Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policy-making. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.         ",
    "url": "https://arxiv.org/abs/2402.14090",
    "authors": [
      "Edwin Zhang",
      "Sadie Zhao",
      "Tonghan Wang",
      "Safwan Hossain",
      "Henry Gasztowtt",
      "Stephan Zheng",
      "David C. Parkes",
      "Milind Tambe",
      "Yiling Chen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "General Economics (econ.GN)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.14730",
    "title": "Clifford-Steerable Convolutional Neural Networks",
    "abstract": "           We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of $\\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces $\\mathbb{R}^{p,q}$. They cover, for instance, $\\mathrm{E}(3)$-equivariance on $\\mathbb{R}^3$ and Poincar\u00e9-equivariance on Minkowski spacetime $\\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.         ",
    "url": "https://arxiv.org/abs/2402.14730",
    "authors": [
      "Maksim Zhdanov",
      "David Ruhe",
      "Maurice Weiler",
      "Ana Lucic",
      "Johannes Brandstetter",
      "Patrick Forr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.17233",
    "title": "Hybrid$^2$ Neural ODE Causal Modeling and an Application to Glycemic Response",
    "abstract": "           Hybrid models composing mechanistic ODE-based dynamics with flexible and expressive neural network components have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as the hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: \\emph{ranking} of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a \\emph{causal loss} that we combine with the standard predictive loss to arrive at a \\emph{hybrid loss} that biases our learning towards causally valid hybrid models. We demonstrate our ability to achieve a win-win, state-of-the-art predictive performance \\emph{and} causal validity, in the challenging task of modeling glucose dynamics post-exercise in individuals with type 1 diabetes.         ",
    "url": "https://arxiv.org/abs/2402.17233",
    "authors": [
      "Bob Junyi Zou",
      "Matthew E. Levine",
      "Dessi P. Zaharieva",
      "Ramesh Johari",
      "Emily B. Fox"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2402.18477",
    "title": "Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes",
    "abstract": "           Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via \"which variables enter the differential of which other variables\". In this paper, we develop a kernel-based test of conditional independence (CI) on \"path-space\" -- e.g., solutions to SDEs, but applicable beyond that -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space and provide theoretical consistency results. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for self-loops) that leverage temporal information to recover the entire directed acyclic graph. Assuming faithfulness and a CI oracle, we show that our algorithms are sound and complete. We empirically verify that our developed CI test in conjunction with the causal discovery algorithms outperform baselines across a range of settings.         ",
    "url": "https://arxiv.org/abs/2402.18477",
    "authors": [
      "Georg Manten",
      "Cecilia Casolo",
      "Emilio Ferrucci",
      "S\u00f8ren Wengel Mogensen",
      "Cristopher Salvi",
      "Niki Kilbertus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.18512",
    "title": "Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference",
    "abstract": "           The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path. Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state. As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data. Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel, effective, and efficient method for training NCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution. Log-NCDEs are shown to outperform NCDEs, NRDEs, the linear recurrent unit, S5, and MAMBA on a range of multivariate time series datasets with up to $50{,}000$ observations.         ",
    "url": "https://arxiv.org/abs/2402.18512",
    "authors": [
      "Benjamin Walker",
      "Andrew D. McLeod",
      "Tiexin Qin",
      "Yichuan Cheng",
      "Haoliang Li",
      "Terry Lyons"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.04867",
    "title": "Unified Mechanism-Specific Amplification by Subsampling and Group Privacy Amplification",
    "abstract": "           Amplification by subsampling is one of the main primitives in machine learning with differential privacy (DP): Training a model on random batches instead of complete datasets results in stronger privacy. This is traditionally formalized via mechanism-agnostic subsampling guarantees that express the privacy parameters of a subsampled mechanism as a function of the original mechanism's privacy parameters. We propose the first general framework for deriving mechanism-specific guarantees, which leverage additional information beyond these parameters to more tightly characterize the subsampled mechanism's privacy. Such guarantees are of particular importance for privacy accounting, i.e., tracking privacy over multiple iterations. Overall, our framework based on conditional optimal transport lets us derive existing and novel guarantees for approximate DP, accounting with R\u00e9nyi DP, and accounting with dominating pairs in a unified, principled manner. As an application, we analyze how subsampling affects the privacy of groups of multiple users. Our tight mechanism-specific bounds outperform tight mechanism-agnostic bounds and classic group privacy results.         ",
    "url": "https://arxiv.org/abs/2403.04867",
    "authors": [
      "Jan Schuchardt",
      "Mihail Stoian",
      "Arthur Kosmala",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2403.07120",
    "title": "Comparing Task Graph Scheduling Algorithms: An Adversarial Approach",
    "abstract": "           Scheduling a task graph representing an application over a heterogeneous network of computers is a fundamental problem in distributed computing. It is known to be not only NP-hard but also not polynomial-time approximable within a constant factor. As a result, many heuristic algorithms have been proposed over the past few decades. Yet it remains largely unclear how these algorithms compare to each other in terms of the quality of schedules they produce. We identify gaps in the traditional benchmarking approach to comparing task scheduling algorithms and propose a simulated annealing-based adversarial analysis approach called PISA to help address them. We also introduce SAGA, a new open-source library for comparing task scheduling algorithms. We use SAGA to benchmark 15 algorithms on 16 datasets and PISA to compare the algorithms in a pairwise manner. Algorithms that appear to perform similarly on benchmarking datasets are shown to perform very differently on adversarially chosen problem instances. Interestingly, the results indicate that this is true even when the adversarial search is constrained to selecting among well-structured, application-specific problem instances. This work represents an important step towards a more general understanding of the performance boundaries between task scheduling algorithms on different families of problem instances.         ",
    "url": "https://arxiv.org/abs/2403.07120",
    "authors": [
      "Jared Coleman",
      "Bhaskar Krishnamachari"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2403.08605",
    "title": "Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation",
    "abstract": "           To fully leverage the capabilities of mobile manipulation robots, it is imperative that they are able to autonomously execute long-horizon tasks in large unexplored environments. While large language models (LLMs) have shown emergent reasoning skills on arbitrary tasks, existing work primarily concentrates on explored environments, typically focusing on either navigation or manipulation tasks in isolation. In this work, we propose MoMa-LLM, a novel approach that grounds language models within structured representations derived from open-vocabulary scene graphs, dynamically updated as the environment is explored. We tightly interleave these representations with an object-centric action space. Given object detections, the resulting approach is zero-shot, open-vocabulary, and readily extendable to a spectrum of mobile manipulation and household robotic tasks. We demonstrate the effectiveness of MoMa-LLM in a novel semantic interactive search task in large realistic indoor environments. In extensive experiments in both simulation and the real world, we show substantially improved search efficiency compared to conventional baselines and state-of-the-art approaches, as well as its applicability to more abstract tasks. We make the code publicly available at this http URL.         ",
    "url": "https://arxiv.org/abs/2403.08605",
    "authors": [
      "Daniel Honerkamp",
      "Martin B\u00fcchner",
      "Fabien Despinoy",
      "Tim Welschehold",
      "Abhinav Valada"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2403.08797",
    "title": "Evolutionary Algorithms Simulating Molecular Evolution: A New Field Proposal",
    "abstract": "           The genetic blueprint for the essential functions of life is encoded in DNA, which is translated into proteins -- the engines driving most of our metabolic processes. Recent advancements in genome sequencing have unveiled a vast diversity of protein families, but compared to the massive search space of all possible amino acid sequences, the set of known functional families is minimal. One could say nature has a limited protein \"vocabulary.\" The major question for computational biologists, therefore, is whether this vocabulary can be expanded to include useful proteins that went extinct long ago, or maybe never evolved in the first place. We outline a computational approach to solving this problem. By merging evolutionary algorithms, machine learning (ML), and bioinformatics, we can facilitate the development of completely novel proteins which have never existed before. We envision this work forming a new sub-field of computational evolution we dub evolutionary algorithms simulating molecular evolution (EASME).         ",
    "url": "https://arxiv.org/abs/2403.08797",
    "authors": [
      "James S. L. Browning Jr.",
      "Daniel R. Tauritz",
      "John Beckmann"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2403.10543",
    "title": "Mitigating Oversmoothing Through Reverse Process of GNNs for Heterophilic Graphs",
    "abstract": "           Graph Neural Network (GNN) resembles the diffusion process, leading to the over-smoothing of learned representations when stacking many layers. Hence, the reverse process of message passing can produce the distinguishable node representations by inverting the forward message propagation. The distinguishable representations can help us to better classify neighboring nodes with different labels, such as in heterophilic graphs. In this work, we apply the design principle of the reverse process to the three variants of the GNNs. Through the experiments on heterophilic graph data, where adjacent nodes need to have different representations for successful classification, we show that the reverse process significantly improves the prediction performance in many cases. Additional analysis reveals that the reverse mechanism can mitigate the over-smoothing over hundreds of layers. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2403.10543",
    "authors": [
      "MoonJeong Park",
      "Jaeseung Heo",
      "Dongwoo Kim"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.11996",
    "title": "Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning",
    "abstract": "           Leveraging generative Artificial Intelligence (AI), we have transformed a dataset comprising 1,000 scientific papers into an ontological knowledge graph. Through an in-depth structural analysis, we have calculated node degrees, identified communities and connectivities, and evaluated clustering coefficients and betweenness centrality of pivotal nodes, uncovering fascinating knowledge architectures. The graph has an inherently scale-free nature, is highly connected, and can be used for graph reasoning by taking advantage of transitive and isomorphic properties that reveal unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, propose never-before-seen material designs, and predict material behaviors. We compute deep node embeddings for combinatorial node similarity ranking for use in a path sampling strategy links dissimilar concepts that have previously not been related. One comparison revealed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. In another example, the algorithm proposed a hierarchical mycelium-based composite based on integrating path sampling with principles extracted from Kandinsky's 'Composition VII' painting. The resulting material integrates an innovative set of concepts that include a balance of chaos/order, adjustable porosity, mechanical strength, and complex patterned chemical functionalization. We uncover other isomorphisms across science, technology and art, revealing a nuanced ontology of immanence that reveal a context-dependent heterarchical interplay of constituents. Graph-based generative AI achieves a far higher degree of novelty, explorative capacity, and technical detail, than conventional approaches and establishes a widely useful framework for innovation by revealing hidden connections.         ",
    "url": "https://arxiv.org/abs/2403.11996",
    "authors": [
      "Markus J. Buehler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Mesoscale and Nanoscale Physics (cond-mat.mes-hall)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2403.16792",
    "title": "Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback",
    "abstract": "           Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.         ",
    "url": "https://arxiv.org/abs/2403.16792",
    "authors": [
      "Zhangqian Bi",
      "Yao Wan",
      "Zheng Wang",
      "Hongyu Zhang",
      "Batu Guan",
      "Fangxin Lu",
      "Zili Zhang",
      "Yulei Sui",
      "Hai Jin",
      "Xuanhua Shi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2403.19433",
    "title": "Puzzle game: Prediction and Classification of Wordle Solution Words",
    "abstract": "           In MCM/ICM 2023, we proposed a new result prediction model for the popular game Wordle launched by The New York Times. We first preprocessed the raw data and then established a prediction model based on ARIMA to predict the number of report results as of March 1, 2023. We selected word usage frequency, word information entropy, and the number of repeated letters contained in the word as the attributes of the word, and conducted a correlation analysis between these three attributes and the percentage of seven attempts. We also established a regression model based on the XGBoost algorithm, predicted the distribution of reported results, and predicted the correlation percentage of \"EERIE\". In addition, we also constructed a word classification model that classified words into \"simple\", \"moderate\", and \"difficult\", and explored the relationship between the three attributes and the classification results. Finally, we calculated the percentage of players in the dataset who needed 3 or more attempts for each word. The appendix provides relevant information and problems to be solved for the mathematical modeling competition.         ",
    "url": "https://arxiv.org/abs/2403.19433",
    "authors": [
      "Haidong Xin",
      "Fang Wu",
      "Zhitong Zhou"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2404.09386",
    "title": "Integrating Marketing Channels into Quantile Transformation and Bayesian Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process Models",
    "abstract": "           This study introduces an innovative Gaussian Process (GP) model utilizing an ensemble kernel that integrates Radial Basis Function (RBF), Rational Quadratic, and Mat\u00e9rn kernels for product sales forecasting. By applying Bayesian optimization, we efficiently find the optimal weights for each kernel, enhancing the model's ability to handle complex sales data patterns. Our approach significantly outperforms traditional GP models, achieving a notable 98\\% accuracy and superior performance across key metrics including Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination ($R^2$). This advancement underscores the effectiveness of ensemble kernels and Bayesian optimization in improving predictive accuracy, offering profound implications for machine learning applications in sales forecasting.         ",
    "url": "https://arxiv.org/abs/2404.09386",
    "authors": [
      "Shahin Mirshekari",
      "Negin Hayeri Motedayen",
      "Mohammad Ensaf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2404.12746",
    "title": "Near-Tight Runtime Guarantees for Many-Objective Evolutionary Algorithms",
    "abstract": "           Despite significant progress in the field of mathematical runtime analysis of multi-objective evolutionary algorithms (MOEAs), the performance of MOEAs on discrete many-objective problems is little understood. In particular, the few existing bounds for the SEMO, global SEMO, and SMS-EMOA algorithms on classic benchmarks are all roughly quadratic in the size of the Pareto front. In this work, we prove near-tight runtime guarantees for these three algorithms on the four most common benchmark problems OneMinMax, CountingOnesCountingZeros, LeadingOnesTrailingZeros, and OneJumpZeroJump, and this for arbitrary numbers of objectives. Our bounds depend only linearly on the Pareto front size, showing that these MOEAs on these benchmarks cope much better with many objectives than what previous works suggested. Our bounds are tight apart from small polynomial factors in the number of objectives and length of bitstrings. This is the first time that such tight bounds are proven for many-objective uses of these MOEAs. While it is known that such results cannot hold for the NSGA-II, we do show that our bounds, via a recent structural result, transfer to the NSGA-III algorithm.         ",
    "url": "https://arxiv.org/abs/2404.12746",
    "authors": [
      "Simon Wietheger",
      "Benjamin Doerr"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2404.19419",
    "title": "Active Dendrites Enable Efficient Continual Learning in Time-To-First-Spike Neural Networks",
    "abstract": "           While the human brain efficiently adapts to new tasks from a continuous stream of information, neural network models struggle to learn from sequential information without catastrophically forgetting previously learned tasks. This limitation presents a significant hurdle in deploying edge devices in real-world scenarios where information is presented in an inherently sequential manner. Active dendrites of pyramidal neurons play an important role in the brain ability to learn new tasks incrementally. By exploiting key properties of time-to-first-spike encoding and leveraging its high sparsity, we present a novel spiking neural network model enhanced with active dendrites. Our model can efficiently mitigate catastrophic forgetting in temporally-encoded SNNs, which we demonstrate with an end-of-training accuracy across tasks of 88.3% on the test set using the Split MNIST dataset. Furthermore, we provide a novel digital hardware architecture that paves the way for real-world deployment in edge devices. Using a Xilinx Zynq-7020 SoC FPGA, we demonstrate a 100-% match with our quantized software model, achieving an average inference time of 37.3 ms and an 80.0% accuracy.         ",
    "url": "https://arxiv.org/abs/2404.19419",
    "authors": [
      "Lorenzo Pes",
      "Rick Luiken",
      "Federico Corradi",
      "Charlotte Frenkel"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2405.00131",
    "title": "Finding Diverse Strings and Longest Common Subsequences in a Graph",
    "abstract": "           In this paper, we study for the first time the Diverse Longest Common Subsequences (LCSs) problem under Hamming distance. Given a set of a constant number of input strings, the problem asks to decide if there exists some subset $\\mathcal X$ of $K$ longest common subsequences whose diversity is no less than a specified threshold $\\Delta$, where we consider two types of diversities of a set $\\mathcal X$ of strings of equal length: the Sum diversity and the Min diversity defined as the sum and the minimum of the pairwise Hamming distance between any two strings in $\\mathcal X$, respectively. We analyze the computational complexity of the respective problems with Sum- and Min-diversity measures, called the Max-Sum and Max-Min Diverse LCSs, respectively, considering both approximation algorithms and parameterized complexity. Our results are summarized as follows. When $K$ is bounded, both problems are polynomial time solvable. In contrast, when $K$ is unbounded, both problems become NP-hard, while Max-Sum Diverse LCSs problem admits a PTAS. Furthermore, we analyze the parameterized complexity of both problems with combinations of parameters $K$ and $r$, where $r$ is the length of the candidate strings to be selected. Importantly, all positive results above are proven in a more general setting, where an input is an edge-labeled directed acyclic graph (DAG) that succinctly represents a set of strings of the same length. Negative results are proven in the setting where an input is explicitly given as a set of strings. The latter results are equipped with an encoding such a set as the longest common subsequences of a specific input string set.         ",
    "url": "https://arxiv.org/abs/2405.00131",
    "authors": [
      "Yuto Shida",
      "Giulia Punzi",
      "Yasuaki Kobayashi",
      "Takeaki Uno",
      "Hiroki Arimura"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Formal Languages and Automata Theory (cs.FL)"
    ]
  },
  {
    "id": "arXiv:2405.01843",
    "title": "Closing the Gap: Achieving Global Convergence (Last Iterate) of Actor-Critic under Markovian Sampling with Neural Network Parametrization",
    "abstract": "           The current state-of-the-art theoretical analysis of Actor-Critic (AC) algorithms significantly lags in addressing the practical aspects of AC implementations. This crucial gap needs bridging to bring the analysis in line with practical implementations of AC. To address this, we advocate for considering the MMCLG criteria: \\textbf{M}ulti-layer neural network parametrization for actor/critic, \\textbf{M}arkovian sampling, \\textbf{C}ontinuous state-action spaces, the performance of the \\textbf{L}ast iterate, and \\textbf{G}lobal optimality. These aspects are practically significant and have been largely overlooked in existing theoretical analyses of AC algorithms. In this work, we address these gaps by providing the first comprehensive theoretical analysis of AC algorithms that encompasses all five crucial practical aspects (covers MMCLG criteria). We establish global convergence sample complexity bounds of $\\tilde{\\mathcal{O}}\\left({\\epsilon^{-3}}\\right)$. We achieve this result through our novel use of the weak gradient domination property of MDP's and our unique analysis of the error in critic estimation.         ",
    "url": "https://arxiv.org/abs/2405.01843",
    "authors": [
      "Mudit Gaur",
      "Amrit Singh Bedi",
      "Di Wang",
      "Vaneet Aggarwal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2405.12728",
    "title": "Leveraging Neural Radiance Fields for Pose Estimation of an Unknown Space Object during Proximity Operations",
    "abstract": "           We address the estimation of the 6D pose of an unknown target spacecraft relative to a monocular camera, a key step towards the autonomous rendezvous and proximity operations required by future Active Debris Removal missions. We present a novel method that enables an \"off-the-shelf\" spacecraft pose estimator, which is supposed to known the target CAD model, to be applied on an unknown target. Our method relies on an in-the wild NeRF, i.e., a Neural Radiance Field that employs learnable appearance embeddings to represent varying illumination conditions found in natural scenes. We train the NeRF model using a sparse collection of images that depict the target, and in turn generate a large dataset that is diverse both in terms of viewpoint and illumination. This dataset is then used to train the pose estimation network. We validate our method on the Hardware-In-the-Loop images of SPEED+ that emulate lighting conditions close to those encountered on orbit. We demonstrate that our method successfully enables the training of an off-the-shelf spacecraft pose estimation network from a sparse set of images. Furthermore, we show that a network trained using our method performs similarly to a model trained on synthetic images generated using the CAD model of the target.         ",
    "url": "https://arxiv.org/abs/2405.12728",
    "authors": [
      "Antoine Legrand",
      "Renaud Detry",
      "Christophe De Vleeschouwer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2405.15729",
    "title": "Optimizing Large Language Models for OpenAPI Code Completion",
    "abstract": "           Recent advancements in Large Language Models (LLMs) and their utilization in code generation tasks have significantly reshaped the field of software development. Despite the remarkable efficacy of code completion solutions in mainstream programming languages, their performance lags when applied to less ubiquitous formats such as OpenAPI definitions. This study evaluates the OpenAPI completion performance of GitHub Copilot, a prevalent commercial code completion tool, and proposes a set of task-specific optimizations leveraging Meta's open-source model Code Llama. A semantics-aware OpenAPI completion benchmark proposed in this research is used to perform a series of experiments through which the impact of various prompt-engineering and fine-tuning techniques on the Code Llama model's performance is analyzed. The fine-tuned Code Llama model reaches a peak correctness improvement of 55.2% over GitHub Copilot despite utilizing 25 times fewer parameters than the commercial solution's underlying Codex model. Additionally, this research proposes an enhancement to a widely used code infilling training technique, addressing the issue of underperformance when the model is prompted with context sizes smaller than those used during training. The dataset, the benchmark, and the model fine-tuning code are made publicly available.         ",
    "url": "https://arxiv.org/abs/2405.15729",
    "authors": [
      "Bohdan Petryshyn",
      "Mantas Luko\u0161evi\u010dius"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.18146",
    "title": "Unified Low-rank Compression Framework for Click-through Rate Prediction",
    "abstract": "           Deep Click-Through Rate (CTR) prediction models play an important role in modern industrial recommendation scenarios. However, high memory overhead and computational costs limit their deployment in resource-constrained environments. Low-rank approximation is an effective method for computer vision and natural language processing models, but its application in compressing CTR prediction models has been less explored. Due to the limited memory and computing resources, compression of CTR prediction models often confronts three fundamental challenges, i.e., (1). How to reduce the model sizes to adapt to edge devices? (2). How to speed up CTR prediction model inference? (3). How to retain the capabilities of original models after compression? Previous low-rank compression research mostly uses tensor decomposition, which can achieve a high parameter compression ratio, but brings in AUC degradation and additional computing overhead. To address these challenges, we propose a unified low-rank decomposition framework for compressing CTR prediction models. We find that even with the most classic matrix decomposition SVD method, our framework can achieve better performance than the original model. To further improve the effectiveness of our framework, we locally compress the output features instead of compressing the model weights. Our unified low-rank compression framework can be applied to embedding tables and MLP layers in various CTR prediction models. Extensive experiments on two academic datasets and one real industrial benchmark demonstrate that, with 3-5x model size reduction, our compressed models can achieve both faster inference and higher AUC than the uncompressed original models. Our code is at this https URL.         ",
    "url": "https://arxiv.org/abs/2405.18146",
    "authors": [
      "Hao Yu",
      "Minghao Fu",
      "Jiandong Ding",
      "Yusheng Zhou",
      "Jianxin Wu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.00046",
    "title": "Hate Speech Detection with Generalizable Target-aware Fairness",
    "abstract": "           To counter the side effect brought by the proliferation of social media platforms, hate speech detection (HSD) plays a vital role in halting the dissemination of toxic online posts at an early stage. However, given the ubiquitous topical communities on social media, a trained HSD classifier easily becomes biased towards specific targeted groups (e.g., female and black people), where a high rate of false positive/negative results can significantly impair public trust in the fairness of content moderation mechanisms, and eventually harm the diversity of online society. Although existing fairness-aware HSD methods can smooth out some discrepancies across targeted groups, they are mostly specific to a narrow selection of targets that are assumed to be known and fixed. This inevitably prevents those methods from generalizing to real-world use cases where new targeted groups constantly emerge over time. To tackle this defect, we propose Generalizable target-aware Fairness (GetFair), a new method for fairly classifying each post that contains diverse and even unseen targets during inference. To remove the HSD classifier's spurious dependence on target-related features, GetFair trains a series of filter functions in an adversarial pipeline, so as to deceive the discriminator that recovers the targeted group from filtered post embeddings. To maintain scalability and generalizability, we innovatively parameterize all filter functions via a hypernetwork that is regularized by the semantic affinity among targets. Taking a target's pretrained word embedding as input, the hypernetwork generates the weights used by each target-specific filter on-the-fly without storing dedicated filter parameters. Finally, comparative experiments on two HSD datasets have shown advantageous performance of GetFair on out-of-sample targets.         ",
    "url": "https://arxiv.org/abs/2406.00046",
    "authors": [
      "Tong Chen",
      "Danny Wang",
      "Xurong Liang",
      "Marten Risius",
      "Gianluca Demartini",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.00934",
    "title": "LanEvil: Benchmarking the Robustness of Lane Detection to Environmental Illusions",
    "abstract": "           Lane detection (LD) is an essential component of autonomous driving systems, providing fundamental functionalities like adaptive cruise control and automated lane centering. Existing LD benchmarks primarily focus on evaluating common cases, neglecting the robustness of LD models against environmental illusions such as shadows and tire marks on the road. This research gap poses significant safety challenges since these illusions exist naturally in real-world traffic situations. For the first time, this paper studies the potential threats caused by these environmental illusions to LD and establishes the first comprehensive benchmark LanEvil for evaluating the robustness of LD against this natural corruption. We systematically design 14 prevalent yet critical types of environmental illusions (e.g., shadow, reflection) that cover a wide spectrum of real-world influencing factors in LD tasks. Based on real-world environments, we create 94 realistic and customizable 3D cases using the widely used CARLA simulator, resulting in a dataset comprising 90,292 sampled images. Through extensive experiments, we benchmark the robustness of popular LD methods using LanEvil, revealing substantial performance degradation (-5.37% Accuracy and -10.70% F1-Score on average), with shadow effects posing the greatest risk (-7.39% Accuracy). Additionally, we assess the performance of commercial auto-driving systems OpenPilot and Apollo through collaborative simulations, demonstrating that proposed environmental illusions can lead to incorrect decisions and potential traffic accidents. To defend against environmental illusions, we propose the Attention Area Mixing (AAM) approach using hard examples, which witness significant robustness improvement (+3.76%) under illumination effects. We hope our paper can contribute to advancing more robust auto-driving systems in the future. Website: this https URL.         ",
    "url": "https://arxiv.org/abs/2406.00934",
    "authors": [
      "Tianyuan Zhang",
      "Lu Wang",
      "Hainan Li",
      "Yisong Xiao",
      "Siyuan Liang",
      "Aishan Liu",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.01304",
    "title": "CodeR: Issue Resolving with Multi-Agent and Task Graphs",
    "abstract": "           GitHub issue resolving recently has attracted significant attention from academia and industry. SWE-bench is proposed to measure the performance in resolving issues. In this paper, we propose CodeR, which adopts a multi-agent framework and pre-defined task graphs to Repair & Resolve reported bugs and add new features within code Repository. On SWE-bench lite, CodeR is able to solve 28.33% of issues, when submitting only once for each issue. We examine the performance impact of each design of CodeR and offer insights to advance this research direction.         ",
    "url": "https://arxiv.org/abs/2406.01304",
    "authors": [
      "Dong Chen",
      "Shaoxin Lin",
      "Muhan Zeng",
      "Daoguang Zan",
      "Jian-Gang Wang",
      "Anton Cheshkov",
      "Jun Sun",
      "Hao Yu",
      "Guoliang Dong",
      "Artem Aliev",
      "Jie Wang",
      "Xiao Cheng",
      "Guangtai Liang",
      "Yuchi Ma",
      "Pan Bian",
      "Tao Xie",
      "Qianxiang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2406.02090",
    "title": "WEIRD ICWSM: How Western, Educated, Industrialized, Rich, and Democratic is Social Computing Research?",
    "abstract": "           Much of the research in social computing analyzes data from social media platforms, which may inherently carry biases. An overlooked source of such bias is the over-representation of WEIRD (Western, Educated, Industrialized, Rich, and Democratic) populations, which might not accurately mirror the global demographic diversity. We evaluated the dependence on WEIRD populations in research presented at the AAAI ICWSM conference; the only venue whose proceedings are fully dedicated to social computing research. We did so by analyzing 494 papers published from 2018 to 2022, which included full research papers, dataset papers and posters. After filtering out papers that analyze synthetic datasets or those lacking clear country of origin, we were left with 420 papers from which 188 participants in a crowdsourcing study with full manual validation extracted data for the WEIRD scores computation. This data was then used to adapt existing WEIRD metrics to be applicable for social media data. We found that 37% of these papers focused solely on data from Western countries. This percentage is significantly less than the percentages observed in research from CHI (76%) and FAccT (84%) conferences, suggesting a greater diversity of dataset origins within ICWSM. However, the studies at ICWSM still predominantly examine populations from countries that are more Educated, Industrialized, and Rich in comparison to those in FAccT, with a special note on the 'Democratic' variable reflecting political freedoms and rights. This points out the utility of social media data in shedding light on findings from countries with restricted political freedoms. Based on these insights, we recommend extensions of current \"paper checklists\" to include considerations about the WEIRD bias and call for the community to broaden research inclusivity by encouraging the use of diverse datasets from underrepresented regions.         ",
    "url": "https://arxiv.org/abs/2406.02090",
    "authors": [
      "Ali Akbar Septiandri",
      "Marios Constantinides",
      "Daniele Quercia"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.02921",
    "title": "Text Injection for Neural Contextual Biasing",
    "abstract": "           Neural contextual biasing effectively improves automatic speech recognition (ASR) for crucial phrases within a speaker's context, particularly those that are infrequent in the training data. This work proposes contextual text injection (CTI) to enhance contextual ASR. CTI leverages not only the paired speech-text data, but also a much larger corpus of unpaired text to optimize the ASR model and its biasing component. Unpaired text is converted into speech-like representations and used to guide the model's attention towards relevant bias phrases. Moreover, we introduce a contextual text-injected (CTI) minimum word error rate (MWER) training, which minimizes the expected WER caused by contextual biasing when unpaired text is injected into the model. Experiments show that CTI with 100 billion text sentences can achieve up to 43.3% relative WER reduction from a strong neural biasing model. CTI-MWER provides a further relative improvement of 23.5%.         ",
    "url": "https://arxiv.org/abs/2406.02921",
    "authors": [
      "Zhong Meng",
      "Zelin Wu",
      "Rohit Prabhavalkar",
      "Cal Peyser",
      "Weiran Wang",
      "Nanxin Chen",
      "Tara N. Sainath",
      "Bhuvana Ramabhadran"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2406.03229",
    "title": "Global Clipper: Enhancing Safety and Reliability of Transformer-based Object Detection Models",
    "abstract": "           As transformer-based object detection models progress, their impact in critical sectors like autonomous vehicles and aviation is expected to grow. Soft errors causing bit flips during inference have significantly impacted DNN performance, altering predictions. Traditional range restriction solutions for CNNs fall short for transformers. This study introduces the Global Clipper and Global Hybrid Clipper, effective mitigation strategies specifically designed for transformer-based models. It significantly enhances their resilience to soft errors and reduces faulty inferences to ~ 0\\%. We also detail extensive testing across over 64 scenarios involving two transformer models (DINO-DETR and Lite-DETR) and two CNN models (YOLOv3 and SSD) using three datasets, totalling approximately 3.3 million inferences, to assess model robustness comprehensively. Moreover, the paper explores unique aspects of attention blocks in transformers and their operational differences from CNNs.         ",
    "url": "https://arxiv.org/abs/2406.03229",
    "authors": [
      "Qutub Syed Sha",
      "Michael Paulitsch",
      "Karthik Pattabiraman",
      "Korbinian Hagn",
      "Fabian Oboril",
      "Cornelius Buerkle",
      "Kay-Ulrich Scholl",
      "Gereon Hinz",
      "Alois Knoll"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.03508",
    "title": "Mutual Information Guided Backdoor Mitigation for Pre-trained Encoders",
    "abstract": "           Self-supervised learning (SSL) is increasingly attractive for pre-training encoders without requiring labeled data. Downstream tasks built on top of those pre-trained encoders can achieve nearly state-of-the-art performance. The pre-trained encoders by SSL, however, are vulnerable to backdoor attacks as demonstrated by existing studies. Numerous backdoor mitigation techniques are designed for downstream task models. However, their effectiveness is impaired and limited when adapted to pre-trained encoders, due to the lack of label information when pre-training. To address backdoor attacks against pre-trained encoders, in this paper, we innovatively propose a mutual information guided backdoor mitigation technique, named MIMIC. MIMIC treats the potentially backdoored encoder as the teacher net and employs knowledge distillation to distill a clean student encoder from the teacher net. Different from existing knowledge distillation approaches, MIMIC initializes the student with random weights, inheriting no backdoors from teacher nets. Then MIMIC leverages mutual information between each layer and extracted features to locate where benign knowledge lies in the teacher net, with which distillation is deployed to clone clean features from teacher to student. We craft the distillation loss with two aspects, including clone loss and attention loss, aiming to mitigate backdoors and maintain encoder performance at the same time. Our evaluation conducted on two backdoor attacks in SSL demonstrates that MIMIC can significantly reduce the attack success rate by only utilizing <5% of clean data, surpassing seven state-of-the-art backdoor mitigation techniques.         ",
    "url": "https://arxiv.org/abs/2406.03508",
    "authors": [
      "Tingxu Han",
      "Weisong Sun",
      "Ziqi Ding",
      "Chunrong Fang",
      "Hanwei Qian",
      "Jiaxun Li",
      "Zhenyu Chen",
      "Xiangyu Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2406.04035",
    "title": "Spatio-temporal Early Prediction based on Multi-objective Reinforcement Learning",
    "abstract": "           Accuracy and timeliness are indeed often conflicting goals in prediction tasks. Premature predictions may yield a higher rate of false alarms, whereas delaying predictions to gather more information can render them too late to be useful. In applications such as wildfires, crimes, and traffic jams, timely predictions are vital for safeguarding human life and property. Consequently, finding a balance between accuracy and timeliness is crucial. In this paper, we propose a spatio-temporal early prediction model based on Multi-Objective reinforcement learning that can either implement an optimal policy given a preference or infer the preference based on a small number of samples. The model addresses two primary challenges: 1) enhancing the accuracy of early predictions and 2) providing the optimal policy for determining the most suitable prediction time for each area. Our method demonstrates superior performance on three large-scale real-world datasets, surpassing existing methods in early spatio-temporal prediction tasks.         ",
    "url": "https://arxiv.org/abs/2406.04035",
    "authors": [
      "Wei Shao",
      "Yufan Kang",
      "Ziyan Peng",
      "Xiao Xiao",
      "Lei Wang",
      "Yuhui Yang",
      "Flora D Salim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.04607",
    "title": "MeGA: Merging Multiple Independently Trained Neural Networks Based on Genetic Algorithm",
    "abstract": "           In this paper, we introduce a novel method for merging the weights of multiple pre-trained neural networks using a genetic algorithm called MeGA. Traditional techniques, such as weight averaging and ensemble methods, often fail to fully harness the capabilities of pre-trained networks. Our approach leverages a genetic algorithm with tournament selection, crossover, and mutation to optimize weight combinations, creating a more effective fusion. This technique allows the merged model to inherit advantageous features from both parent models, resulting in enhanced accuracy and robustness. Through experiments on the CIFAR-10 dataset, we demonstrate that our genetic algorithm-based weight merging method improves test accuracy compared to individual models and conventional methods. This approach provides a scalable solution for integrating multiple pre-trained networks across various deep learning applications. Github is available at: this https URL ",
    "url": "https://arxiv.org/abs/2406.04607",
    "authors": [
      "Daniel Yun"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.04825",
    "title": "Graph Mining under Data scarcity",
    "abstract": "           Multitude of deep learning models have been proposed for node classification in graphs. However, they tend to perform poorly under labeled-data scarcity. Although Few-shot learning for graphs has been introduced to overcome this problem, the existing models are not easily adaptable for generic graph learning frameworks like Graph Neural Networks (GNNs). Our work proposes an Uncertainty Estimator framework that can be applied on top of any generic GNN backbone network (which are typically designed for supervised/semi-supervised node classification) to improve the node classification performance. A neural network is used to model the Uncertainty Estimator as a probability distribution rather than probabilistic discrete scalar values. We train these models under the classic episodic learning paradigm in the $n$-way, $k$-shot fashion, in an end-to-end setting. Our work demonstrates that implementation of the uncertainty estimator on a GNN backbone network improves the classification accuracy under Few-shot setting without any meta-learning specific architecture. We conduct experiments on multiple datasets under different Few-shot settings and different GNN-based backbone networks. Our method outperforms the baselines, which demonstrates the efficacy of the Uncertainty Estimator for Few-shot node classification on graphs with a GNN.         ",
    "url": "https://arxiv.org/abs/2406.04825",
    "authors": [
      "Appan Rakaraddi",
      "Lam Siew-Kei",
      "Mahardhika Pratama",
      "Marcus de Carvalho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.04995",
    "title": "Data2Neo -- A Tool for Complex Neo4j Data Integration",
    "abstract": "           This paper introduces Data2Neo, an open-source Python library for converting relational data into knowledge graphs stored in Neo4j databases. With extensive customization options and support for continuous online data integration from various data sources, Data2Neo is designed to be user-friendly, efficient, and scalable to large datasets. The tool significantly lowers the barrier to entry for creating and using knowledge graphs, making this increasingly popular form of data representation accessible to a wider audience. The code is available at this https URL .         ",
    "url": "https://arxiv.org/abs/2406.04995",
    "authors": [
      "Julian Minder",
      "Laurence Brandenberger",
      "Luis Salamanca",
      "Frank Schweitzer"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2406.05376",
    "title": "Adversarial flows: A gradient flow characterization of adversarial attacks",
    "abstract": "           A popular method to perform adversarial attacks on neuronal networks is the so-called fast gradient sign method and its iterative variant. In this paper, we interpret this method as an explicit Euler discretization of a differential inclusion, where we also show convergence of the discretization to the associated gradient flow. To do so, we consider the concept of p-curves of maximal slope in the case $p=\\infty$. We prove existence of $\\infty$-curves of maximum slope and derive an alternative characterization via differential inclusions. Furthermore, we also consider Wasserstein gradient flows for potential energies, where we show that curves in the Wasserstein space can be characterized by a representing measure on the space of curves in the underlying Banach space, which fulfill the differential inclusion. The application of our theory to the finite-dimensional setting is twofold: On the one hand, we show that a whole class of normalized gradient descent methods (in particular signed gradient descent) converge, up to subsequences, to the flow, when sending the step size to zero. On the other hand, in the distributional setting, we show that the inner optimization task of adversarial training objective can be characterized via $\\infty$-curves of maximum slope on an appropriate optimal transport space.         ",
    "url": "https://arxiv.org/abs/2406.05376",
    "authors": [
      "Lukas Weigand",
      "Tim Roith",
      "Martin Burger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)"
    ]
  },
  {
    "id": "arXiv:2406.05482",
    "title": "Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks",
    "abstract": "           In recent years, graph neural networks (GNNs) have emerged as a potent tool for learning on graph-structured data and won fruitful successes in varied fields. The majority of GNNs follow the message-passing paradigm, where representations of each node are learned by recursively aggregating features of its neighbors. However, this mechanism brings severe over-smoothing and efficiency issues over high-degree graphs (HDGs), wherein most nodes have dozens (or even hundreds) of neighbors, such as social networks, transaction graphs, power grids, etc. Additionally, such graphs usually encompass rich and complex structure semantics, which are hard to capture merely by feature aggregations in GNNs. Motivated by the above limitations, we propose TADA, an efficient and effective front-mounted data augmentation framework for GNNs on HDGs. Under the hood, TADA includes two key modules: (i) feature expansion with structure embeddings, and (ii) topology- and attribute-aware graph sparsification. The former obtains augmented node features and enhanced model capacity by encoding the graph structure into high-quality structure embeddings with our highly-efficient sketching method. Further, by exploiting task-relevant features extracted from graph structures and attributes, the second module enables the accurate identification and reduction of numerous redundant/noisy edges from the input graph, thereby alleviating over-smoothing and facilitating faster feature aggregations over HDGs. Empirically, TADA considerably improves the predictive performance of mainstream GNN models on 8 real homophilic/heterophilic HDGs in terms of node classification, while achieving efficient training and inference processes.         ",
    "url": "https://arxiv.org/abs/2406.05482",
    "authors": [
      "Yurui Lai",
      "Xiaoyang Lin",
      "Renchi Yang",
      "Hongtao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.05504",
    "title": "G-Transformer: Counterfactual Outcome Prediction under Dynamic and Time-varying Treatment Regimes",
    "abstract": "           In the context of medical decision making, counterfactual prediction enables clinicians to predict treatment outcomes of interest under alternative courses of therapeutic actions given observed patient history. Prior machine learning approaches for counterfactual predictions under time-varying treatments focus on static time-varying treatment regimes where treatments do not depend on previous covariate history. In this work, we present G-Transformer, a Transformer-based framework supporting g-computation for counterfactual prediction under dynamic and time-varying treatment strategies. G-Transfomer captures complex, long-range dependencies in time-varying covariates using a Transformer architecture. G-Transformer estimates the conditional distribution of relevant covariates given covariate and treatment history at each time point using an encoder architecture, then produces Monte Carlo estimates of counterfactual outcomes by simulating forward patient trajectories under treatment strategies of interest. We evaluate G-Transformer extensively using two simulated longitudinal datasets from mechanistic models, and a real-world sepsis ICU dataset from MIMIC-IV. G-Transformer outperforms both classical and state-of-the-art counterfactual prediction models in these settings. To the best of our knowledge, this is the first Transformer-based architecture for counterfactual outcome prediction under dynamic and time-varying treatment strategies. Code will be released upon publication of the paper.         ",
    "url": "https://arxiv.org/abs/2406.05504",
    "authors": [
      "Hong Xiong",
      "Feng Wu",
      "Leon Deng",
      "Megan Su",
      "Li-wei H Lehman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.05753",
    "title": "Grounding Continuous Representations in Geometry: Equivariant Neural Fields",
    "abstract": "           Recently, Neural Fields have emerged as a powerful modelling paradigm to represent continuous signals. In a conditional neural field, a field is represented by a latent variable that conditions the NeF, whose parametrisation is otherwise shared over an entire dataset. We propose Equivariant Neural Fields based on cross attention transformers, in which NeFs are conditioned on a geometric conditioning variable, a latent point cloud, that enables an equivariant decoding from latent to field. Our equivariant approach induces a steerability property by which both field and latent are grounded in geometry and amenable to transformation laws if the field transforms, the latent represents transforms accordingly and vice versa. Crucially, the equivariance relation ensures that the latent is capable of (1) representing geometric patterns faitfhully, allowing for geometric reasoning in latent space, (2) weightsharing over spatially similar patterns, allowing for efficient learning of datasets of fields. These main properties are validated using classification experiments and a verification of the capability of fitting entire datasets, in comparison to other non-equivariant NeF approaches. We further validate the potential of ENFs by demonstrate unique local field editing properties.         ",
    "url": "https://arxiv.org/abs/2406.05753",
    "authors": [
      "David R Wessels",
      "David M Knigge",
      "Samuele Papa",
      "Riccardo Valperga",
      "Sharvaree Vadgama",
      "Efstratios Gavves",
      "Erik J Bekkers"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.05755",
    "title": "A DeNoising FPN With Transformer R-CNN for Tiny Object Detection",
    "abstract": "           Despite notable advancements in the field of computer vision, the precise detection of tiny objects continues to pose a significant challenge, largely owing to the minuscule pixel representation allocated to these objects in imagery data. This challenge resonates profoundly in the domain of geoscience and remote sensing, where high-fidelity detection of tiny objects can facilitate a myriad of applications ranging from urban planning to environmental monitoring. In this paper, we propose a new framework, namely, DeNoising FPN with Trans R-CNN (DNTR), to improve the performance of tiny object detection. DNTR consists of an easy plug-in design, DeNoising FPN (DN-FPN), and an effective Transformer-based detector, Trans R-CNN. Specifically, feature fusion in the feature pyramid network is important for detecting multiscale objects. However, noisy features may be produced during the fusion process since there is no regularization between the features of different scales. Therefore, we introduce a DN-FPN module that utilizes contrastive learning to suppress noise in each level's features in the top-down path of FPN. Second, based on the two-stage framework, we replace the obsolete R-CNN detector with a novel Trans R-CNN detector to focus on the representation of tiny objects with self-attention. Experimental results manifest that our DNTR outperforms the baselines by at least 17.4% in terms of APvt on the AI-TOD dataset and 9.6% in terms of AP on the VisDrone dataset, respectively. Our code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.05755",
    "authors": [
      "Hou-I Liu",
      "Yu-Wen Tseng",
      "Kai-Cheng Chang",
      "Pin-Jyun Wang",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.06064",
    "title": "6DMA Enhanced Wireless Network with Flexible Antenna Position and Rotation: Opportunities and Challenges",
    "abstract": "           6DMA (six-dimensional movable antenna) is a new and revolutionizing technology that fully exploits the wireless channel spatial variation at the transmitter/receiver by flexibly adjusting the three-dimensional (3D) positions and 3D rotations of distributed antennas/antenna surfaces (arrays). In this article, we provide an overview of 6DMA for unveiling its great potential in wireless networks, including its motivation and competitive advantages over existing technologies, system/channel modeling, and practical implementation. In particular, we present a variety of 6DMA-enabled performance enhancement in terms of array gain, spatial multiplexing, interference suppression, and geometric gain. Furthermore, we illustrate the main applications of 6DMA in wireless communication and sensing, and elaborate their design challenges as well as promising solutions. Finally, numerical results are provided to demonstrate the significant capacity improvement of 6DMA-aided communication in wireless network.         ",
    "url": "https://arxiv.org/abs/2406.06064",
    "authors": [
      "Xiaodan Shao",
      "Rui Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2406.06211",
    "title": "iMotion-LLM: Motion Prediction Instruction Tuning",
    "abstract": "           We introduce iMotion-LLM: a Multimodal Large Language Models (LLMs) with trajectory prediction, tailored to guide interactive multi-agent scenarios. Different from conventional motion prediction approaches, iMotion-LLM capitalizes on textual instructions as key inputs for generating contextually relevant trajectories. By enriching the real-world driving scenarios in the Waymo Open Dataset with textual motion instructions, we created InstructWaymo. Leveraging this dataset, iMotion-LLM integrates a pretrained LLM, fine-tuned with LoRA, to translate scene features into the LLM input space. iMotion-LLM offers significant advantages over conventional motion prediction models. First, it can generate trajectories that align with the provided instructions if it is a feasible direction. Second, when given an infeasible direction, it can reject the instruction, thereby enhancing safety. These findings act as milestones in empowering autonomous navigation systems to interpret and predict the dynamics of multi-agent environments, laying the groundwork for future advancements in this field.         ",
    "url": "https://arxiv.org/abs/2406.06211",
    "authors": [
      "Abdulwahab Felemban",
      "Eslam Mohamed Bakr",
      "Xiaoqian Shen",
      "Jian Ding",
      "Abduallah Mohamed",
      "Mohamed Elhoseiny"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2109.11990",
    "title": "Optimization-based Causal Estimation from Heterogenous Environments",
    "abstract": "           This paper presents a new optimization approach to causal estimation. Given data that contains covariates and an outcome, which covariates are causes of the outcome, and what is the strength of the causality? In classical machine learning (ML), the goal of optimization is to maximize predictive accuracy. However, some covariates might exhibit a non-causal association with the outcome. Such spurious associations provide predictive power for classical ML, but they prevent us from causally interpreting the result. This paper proposes CoCo, an optimization algorithm that bridges the gap between pure prediction and causal inference. CoCo leverages the recently-proposed idea of environments, datasets of covariates/response where the causal relationships remain invariant but where the distribution of the covariates changes from environment to environment. Given datasets from multiple environments-and ones that exhibit sufficient heterogeneity-CoCo maximizes an objective for which the only solution is the causal solution. We describe the theoretical foundations of this approach and demonstrate its effectiveness on simulated and real datasets. Compared to classical ML and existing methods, CoCo provides more accurate estimates of the causal model and more accurate predictions under interventions.         ",
    "url": "https://arxiv.org/abs/2109.11990",
    "authors": [
      "Mingzhang Yin",
      "Yixin Wang",
      "David M. Blei"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.14448",
    "title": "Robust non-computability of dynamical systems and computability of robust dynamical systems",
    "abstract": "           In this paper, we examine the relationship between the stability of the dynamical system $x^{\\prime}=f(x)$ and the computability of its basins of attraction. We present a computable $C^{\\infty}$ system $x^{\\prime}=f(x)$ that possesses a computable and stable equilibrium point, yet whose basin of attraction is robustly non-computable in a neighborhood of $f$ in the sense that both the equilibrium point and the non-computability of its associated basin of attraction persist when $f$ is slightly perturbed. This indicates that local stability near a stable equilibrium point alone is insufficient to guarantee the computability of its basin of attraction. However, we also demonstrate that the basins of attraction associated with a structurally stable - globally stable (robust) - planar system defined on a compact set are computable. Our findings suggest that the global stability of a system and the compactness of the domain play a pivotal role in determining the computability of its basins of attraction.         ",
    "url": "https://arxiv.org/abs/2305.14448",
    "authors": [
      "Daniel S. Gra\u00e7a",
      "Ning Zhong"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2309.08526",
    "title": "Robust IRS-Element Activation for Energy Efficiency Optimization in IRS-Assisted Communication Systems With Imperfect CSI",
    "abstract": "           In this paper, we study an intelligent reflecting surface (IRS)-aided communication system with single-antenna transmitter and receiver, under imperfect channel state information (CSI). More specifically, we deal with the robust selection of binary (on/off) states of the IRS elements in order to maximize the worst-case energy efficiency (EE), given a bounded CSI uncertainty, while satisfying a minimum signal-to-noise ratio (SNR). In addition, we consider not only continuous but also discrete IRS phase shifts. First, we derive closed-form expressions of the worst-case SNRs, and then formulate the robust (discrete) optimization problems for each case. In the case of continuous phase shifts, we design a dynamic programming (DP) algorithm that is theoretically guaranteed to achieve the global maximum with polynomial complexity $O(L\\,{\\log L})$, where $L$ is the number of IRS elements. In the case of discrete phase shifts, we develop a convex-relaxation-based method (CRBM) to obtain a feasible (sub-optimal) solution in polynomial time $O(L^{3.5})$, with a posteriori performance guarantee. Furthermore, numerical simulations provide useful insights and confirm the theoretical results. In particular, the proposed algorithms are several orders of magnitude faster than the exhaustive search when $L$ is large, thus being highly scalable and suitable for practical applications. Moreover, both algorithms outperform a baseline scheme, namely, the activation of all IRS elements.         ",
    "url": "https://arxiv.org/abs/2309.08526",
    "authors": [
      "Christos N. Efrem",
      "Ioannis Krikidis"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2310.13786",
    "title": "Fundamental Limits of Membership Inference Attacks on Machine Learning Models",
    "abstract": "           Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article provides theoretical guarantees by exploring the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. We then theoretically prove that in a non-linear regression setting with overfitting algorithms, attacks may have a high probability of success. Finally, we investigate several situations for which we provide bounds on this quantity of interest. Interestingly, our findings indicate that discretizing the data might enhance the algorithm's security. Specifically, it is demonstrated to be limited by a constant, which quantifies the diversity of the underlying data distribution. We illustrate those results through two simple simulations.         ",
    "url": "https://arxiv.org/abs/2310.13786",
    "authors": [
      "Eric Aubinais",
      "Elisabeth Gassiat",
      "Pablo Piantanida"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.04866",
    "title": "Room Transfer Function Reconstruction Using Complex-valued Neural Networks and Irregularly Distributed Microphones",
    "abstract": "           Reconstructing the room transfer functions needed to calculate the complex sound field in a room has several important real-world applications. However, an unpractical number of microphones is often required. Recently, in addition to classical signal processing methods, deep learning techniques have been applied to reconstruct the room transfer function starting from a very limited set of measurements at scattered points in the room. In this paper, we employ complex-valued neural networks to estimate room transfer functions in the frequency range of the first room resonances, using a few irregularly distributed microphones. To the best of our knowledge, this is the first time that complex-valued neural networks are used to estimate room transfer functions. To analyze the benefits of applying complex-valued optimization to the considered task, we compare the proposed technique with a state-of-the-art kernel-based signal processing approach for sound field reconstruction, showing that the proposed technique exhibits relevant advantages in terms of phase accuracy and overall quality of the reconstructed sound field. For informative purposes, we also compare the model with a similarly-structured data-driven approach that, however, applies a real-valued neural network to reconstruct only the magnitude of the sound field.         ",
    "url": "https://arxiv.org/abs/2402.04866",
    "authors": [
      "Francesca Ronchini",
      "Luca Comanducci",
      "Mirco Pezzoli",
      "Fabio Antonacci",
      "Augusto Sarti"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2402.06856",
    "title": "Community detection in the hypergraph stochastic block model and reconstruction on hypertrees",
    "abstract": "           We study the weak recovery problem on the $r$-uniform hypergraph stochastic block model ($r$-HSBM) with two balanced communities. In this model, $n$ vertices are randomly divided into two communities, and size-$r$ hyperedges are added randomly depending on whether all vertices in the hyperedge are in the same community. The goal of weak recovery is to recover a non-trivial fraction of the communities given the hypergraph. Pal and Zhu (2021); Stephan and Zhu (2022) established that weak recovery is always possible above a natural threshold called the Kesten-Stigum (KS) threshold. For assortative models (i.e., monochromatic hyperedges are preferred), Gu and Polyanskiy (2023) proved that the KS threshold is tight if $r\\le 4$ or the expected degree $d$ is small. For other cases, the tightness of the KS threshold remained open. In this paper we determine the tightness of the KS threshold for a wide range of parameters. We prove that for $r\\le 6$ and $d$ large enough, the KS threshold is tight. This shows that there is no information-computation gap in this regime and partially confirms a conjecture of Angelini et al. (2015). On the other hand, we show that for $r\\ge 5$, there exist parameters for which the KS threshold is not tight. In particular, for $r\\ge 7$, the KS threshold is not tight if the model is disassortative (i.e., polychromatic hyperedges are preferred) or $d$ is large enough. This provides more evidence supporting the existence of an information-computation gap in these cases. Furthermore, we establish asymptotic bounds on the weak recovery threshold for fixed $r$ and large $d$. We also obtain a number of results regarding the broadcasting on hypertrees (BOHT) model, including the asymptotics of the reconstruction threshold for $r\\ge 7$ and impossibility of robust reconstruction at criticality.         ",
    "url": "https://arxiv.org/abs/2402.06856",
    "authors": [
      "Yuzhou Gu",
      "Aaradhya Pandey"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2405.05007",
    "title": "HC-Mamba: Vision MAMBA with Hybrid Convolutional Techniques for Medical Image Segmentation",
    "abstract": "           Automatic medical image segmentation technology has the potential to expedite pathological diagnoses, thereby enhancing the efficiency of patient care. However, medical images often have complex textures and structures, and the models often face the problem of reduced image resolution and information loss due to downsampling. To address this issue, we propose HC-Mamba, a new medical image segmentation model based on the modern state space model Mamba. Specifically, we introduce the technique of dilated convolution in the HC-Mamba model to capture a more extensive range of contextual information without increasing the computational cost by extending the perceptual field of the convolution kernel. In addition, the HC-Mamba model employs depthwise separable convolutions, significantly reducing the number of parameters and the computational power of the model. By combining dilated convolution and depthwise separable convolutions, HC-Mamba is able to process large-scale medical image data at a much lower computational cost while maintaining a high level of performance. We conduct comprehensive experiments on segmentation tasks including organ segmentation and skin lesion, and conduct extensive experiments on Synapse, ISIC17 and ISIC18 to demonstrate the potential of the HC-Mamba model in medical image segmentation. The experimental results show that HC-Mamba exhibits competitive performance on all these datasets, thereby proving its effectiveness and usefulness in medical image segmentation.         ",
    "url": "https://arxiv.org/abs/2405.05007",
    "authors": [
      "Jiashu Xu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.02191",
    "title": "On the Recoverability of Causal Relations from Temporally Aggregated I.I.D. Data",
    "abstract": "           We consider the effect of temporal aggregation on instantaneous (non-temporal) causal discovery in general setting. This is motivated by the observation that the true causal time lag is often considerably shorter than the observational interval. This discrepancy leads to high aggregation, causing time-delay causality to vanish and instantaneous dependence to manifest. Although we expect such instantaneous dependence has consistency with the true causal relation in certain sense to make the discovery results meaningful, it remains unclear what type of consistency we need and when will such consistency be satisfied. We proposed functional consistency and conditional independence consistency in formal way correspond functional causal model-based methods and conditional independence-based methods respectively and provide the conditions under which these consistencies will hold. We show theoretically and experimentally that causal discovery results may be seriously distorted by aggregation especially in complete nonlinear case and we also find causal relationship still recoverable from aggregated data if we have partial linearity or appropriate prior. Our findings suggest community should take a cautious and meticulous approach when interpreting causal discovery results from such data and show why and when aggregation will distort the performance of causal discovery methods.         ",
    "url": "https://arxiv.org/abs/2406.02191",
    "authors": [
      "Shunxing Fan",
      "Mingming Gong",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.05347",
    "title": "MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training",
    "abstract": "           Multiple Sequence Alignment (MSA) plays a pivotal role in unveiling the evolutionary trajectories of protein families. The accuracy of protein structure predictions is often compromised for protein sequences that lack sufficient homologous information to construct high quality MSA. Although various methods have been proposed to generate virtual MSA under these conditions, they fall short in comprehensively capturing the intricate coevolutionary patterns within MSA or require guidance from external oracle models. Here we introduce MSAGPT, a novel approach to prompt protein structure predictions via MSA generative pretraining in the low MSA regime. MSAGPT employs a simple yet effective 2D evolutionary positional encoding scheme to model complex evolutionary patterns. Endowed by this, its flexible 1D MSA decoding framework facilitates zero or few shot learning. Moreover, we demonstrate that leveraging the feedback from AlphaFold2 can further enhance the model capacity via Rejective Fine tuning (RFT) and Reinforcement Learning from AF2 Feedback (RLAF). Extensive experiments confirm the efficacy of MSAGPT in generating faithful virtual MSA to enhance the structure prediction accuracy. The transfer learning capabilities also highlight its great potential for facilitating other protein tasks.         ",
    "url": "https://arxiv.org/abs/2406.05347",
    "authors": [
      "Bo Chen",
      "Zhilei Bei",
      "Xingyi Cheng",
      "Pan Li",
      "Jie Tang",
      "Le Song"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  }
]