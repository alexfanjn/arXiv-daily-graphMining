[
  {
    "id": "arXiv:2509.14252",
    "title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures",
    "abstract": "           Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: this https URL.         ",
    "url": "https://arxiv.org/abs/2509.14252",
    "authors": [
      "Hai Huang",
      "Yann LeCun",
      "Randall Balestriero"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14254",
    "title": "Hallucination Detection with the Internal Layers of LLMs",
    "abstract": "           Large Language Models (LLMs) have succeeded in a variety of natural language processing tasks [Zha+25]. However, they have notable limitations. LLMs tend to generate hallucinations, a seemingly plausible yet factually unsupported output [Hua+24], which have serious real-world consequences [Kay23; Rum+24]. Recent work has shown that probing-based classifiers that utilize LLMs' internal representations can detect hallucinations [AM23; Bei+24; Bur+24; DYT24; Ji+24; SMZ24; Su+24]. This approach, since it does not involve model training, can enhance reliability without significantly increasing computational costs. Building upon this approach, this thesis proposed novel methods for hallucination detection using LLM internal representations and evaluated them across three benchmarks: TruthfulQA, HaluEval, and ReFact. Specifically, a new architecture that dynamically weights and combines internal LLM layers was developed to improve hallucination detection performance. Throughout extensive experiments, two key findings were obtained: First, the proposed approach was shown to achieve superior performance compared to traditional probing methods, though generalization across benchmarks and LLMs remains challenging. Second, these generalization limitations were demonstrated to be mitigated through cross-benchmark training and parameter freezing. While not consistently improving, both techniques yielded better performance on individual benchmarks and reduced performance degradation when transferred to other benchmarks. These findings open new avenues for improving LLM reliability through internal representation analysis.         ",
    "url": "https://arxiv.org/abs/2509.14254",
    "authors": [
      "Martin Prei\u00df"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14256",
    "title": "JU-NLP at Touch\u00e9: Covert Advertisement in Conversational AI-Generation and Detection Strategies",
    "abstract": "           This paper proposes a comprehensive framework for the generation of covert advertisements within Conversational AI systems, along with robust techniques for their detection. It explores how subtle promotional content can be crafted within AI-generated responses and introduces methods to identify and mitigate such covert advertising strategies. For generation (Sub-Task~1), we propose a novel framework that leverages user context and query intent to produce contextually relevant advertisements. We employ advanced prompting strategies and curate paired training data to fine-tune a large language model (LLM) for enhanced stealthiness. For detection (Sub-Task~2), we explore two effective strategies: a fine-tuned CrossEncoder (\\texttt{all-mpnet-base-v2}) for direct classification, and a prompt-based reformulation using a fine-tuned \\texttt{DeBERTa-v3-base} model. Both approaches rely solely on the response text, ensuring practicality for real-world deployment. Experimental results show high effectiveness in both tasks, achieving a precision of 1.0 and recall of 0.71 for ad generation, and F1-scores ranging from 0.99 to 1.00 for ad detection. These results underscore the potential of our methods to balance persuasive communication with transparency in conversational AI.         ",
    "url": "https://arxiv.org/abs/2509.14256",
    "authors": [
      "Arka Dutta",
      "Agrik Majumdar",
      "Sombrata Biswas",
      "Dipankar Das",
      "Sivaji Bandyopadhyay"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14263",
    "title": "Context-Enhanced Granular Edit Representation for Efficient and Accurate ASR Post-editing",
    "abstract": "           Despite ASR technology being full-scale adopted by industry and for large portions of the population, ASR systems often have errors that require editors to post-edit text quality. While LLMs are powerful post-editing tools, baseline full rewrite models have inference inefficiencies because they often generate the same redundant text over and over again. Compact edit representations have existed but often lack the efficacy and context required for optimal accuracy. This paper introduces CEGER (Context-Enhanced Granular Edit Representation), a compact edit representation that was generated for highly accurate, efficient ASR post-editing. CEGER allows LLMs to generate a sequence of structured, fine-grained, contextually rich commands to modify the original ASR output. A separate expansion module deterministically reconstructs the corrected text based on the commands. Extensive experiments on the LibriSpeech dataset that were conducted, CEGER achieves state-of-the-art accuracy, achieving the lowest word error rate (WER) versus full rewrite and prior compact representations.         ",
    "url": "https://arxiv.org/abs/2509.14263",
    "authors": [
      "Luan Vejsiu",
      "Qianyu Zheng",
      "Haoxuan Chen",
      "Yizhou Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2509.14268",
    "title": "DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models",
    "abstract": "           The rapid advancement of large language models (LLMs) has drawn urgent attention to the task of machine-generated text detection (MGTD). However, existing approaches struggle in complex real-world scenarios: zero-shot detectors rely heavily on scoring model's output distribution while training-based detectors are often constrained by overfitting to the training data, limiting generalization. We found that the performance bottleneck of training-based detectors stems from the misalignment between training objective and task needs. To address this, we propose Direct Discrepancy Learning (DDL), a novel optimization strategy that directly optimizes the detector with task-oriented knowledge. DDL enables the detector to better capture the core semantics of the detection task, thereby enhancing both robustness and generalization. Built upon this, we introduce DetectAnyLLM, a unified detection framework that achieves state-of-the-art MGTD performance across diverse LLMs. To ensure a reliable evaluation, we construct MIRAGE, the most diverse multi-task MGTD benchmark. MIRAGE samples human-written texts from 10 corpora across 5 text-domains, which are then re-generated or revised using 17 cutting-edge LLMs, covering a wide spectrum of proprietary models and textual styles. Extensive experiments on MIRAGE reveal the limitations of existing methods in complex environment. In contrast, DetectAnyLLM consistently outperforms them, achieving over a 70% performance improvement under the same training data and base scoring model, underscoring the effectiveness of our DDL. Project page: {this https URL}.         ",
    "url": "https://arxiv.org/abs/2509.14268",
    "authors": [
      "Jiachen Fu",
      "Chun-Le Guo",
      "Chongyi Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2509.14271",
    "title": "Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models",
    "abstract": "           This paper documents early research conducted in 2022 on defending against prompt injection attacks in large language models, providing historical context for the evolution of this critical security domain. This research focuses on two adversarial attacks against Large Language Models (LLMs): prompt injection and goal hijacking. We examine how to construct these attacks, test them on various LLMs, and compare their effectiveness. We propose and evaluate a novel defense technique called Adversarial Fine-Tuning. Our results show that, without this defense, the attacks succeeded 31\\% of the time on GPT-3 series models. When using our Adversarial Fine-Tuning approach, attack success rates were reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie), though we note that subsequent research has revealed limitations of fine-tuning-based defenses. We also find that more flexible models exhibit greater vulnerability to these attacks. Consequently, large models such as GPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the specific models tested are now superseded, the core methodology and empirical findings contributed to the foundation of modern prompt injection defense research, including instruction hierarchy systems and constitutional AI approaches.         ",
    "url": "https://arxiv.org/abs/2509.14271",
    "authors": [
      "Gustavo Sandoval",
      "Denys Fenchenko",
      "Junyao Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14273",
    "title": "Automated and Context-Aware Code Documentation Leveraging Advanced LLMs",
    "abstract": "           Code documentation is essential to improve software maintainability and comprehension. The tedious nature of manual code documentation has led to much research on automated documentation generation. Existing automated approaches primarily focused on code summarization, leaving a gap in template-based documentation generation (e.g., Javadoc), particularly with publicly available Large Language Models (LLMs). Furthermore, progress in this area has been hindered by the lack of a Javadoc-specific dataset that incorporates modern language features, provides broad framework/library coverage, and includes necessary contextual information. This study aims to address these gaps by developing a tailored dataset and assessing the capabilities of publicly available LLMs for context-aware, template-based Javadoc generation. In this work, we present a novel, context-aware dataset for Javadoc generation that includes critical structural and semantic information from modern Java codebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2, Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and provide a comparative analysis of their performance. Our results demonstrate that LLaMA 3.1 performs consistently well and is a reliable candidate for practical, automated Javadoc generation, offering a viable alternative to proprietary systems.         ",
    "url": "https://arxiv.org/abs/2509.14273",
    "authors": [
      "Swapnil Sharma Sarker",
      "Tanzina Taher Ifty"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.14275",
    "title": "FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health",
    "abstract": "           Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive domains (e.g., mental health) requires balancing strict confidentiality with model utility and safety. We propose FedMentor, a federated fine-tuning framework that integrates Low-Rank Adaptation (LoRA) and domain-aware Differential Privacy (DP) to meet per-domain privacy budgets while maintaining performance. Each client (domain) applies a custom DP noise scale proportional to its data sensitivity, and the server adaptively reduces noise when utility falls below a threshold. In experiments on three mental health datasets, we show that FedMentor improves safety over standard Federated Learning without privacy, raising safe output rates by up to three points and lowering toxicity, while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the non-private baseline and close to the centralized upper bound. The framework scales to backbones with up to 1.7B parameters on single-GPU clients, requiring < 173 MB of communication per round. FedMentor demonstrates a practical approach to privately fine-tune LLMs for safer deployments in healthcare and other sensitive fields.         ",
    "url": "https://arxiv.org/abs/2509.14275",
    "authors": [
      "Nobin Sarwar",
      "Shubhashis Roy Dipta"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14278",
    "title": "Beyond Data Privacy: New Privacy Risks for Large Language Models",
    "abstract": "           Large Language Models (LLMs) have achieved remarkable progress in natural language understanding, reasoning, and autonomous decision-making. However, these advancements have also come with significant privacy concerns. While significant research has focused on mitigating the data privacy risks of LLMs during various stages of model training, less attention has been paid to new threats emerging from their deployment. The integration of LLMs into widely used applications and the weaponization of their autonomous abilities have created new privacy vulnerabilities. These vulnerabilities provide opportunities for both inadvertent data leakage and malicious exfiltration from LLM-powered systems. Additionally, adversaries can exploit these systems to launch sophisticated, large-scale privacy attacks, threatening not only individual privacy but also financial security and societal trust. In this paper, we systematically examine these emerging privacy risks of LLMs. We also discuss potential mitigation strategies and call for the research community to broaden its focus beyond data privacy risks, developing new defenses to address the evolving threats posed by increasingly powerful LLMs and LLM-powered systems.         ",
    "url": "https://arxiv.org/abs/2509.14278",
    "authors": [
      "Yuntao Du",
      "Zitao Li",
      "Ninghui Li",
      "Bolin Ding"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14279",
    "title": "Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization",
    "abstract": "           Recent advances in large language models (LLMs) demonstrate their effectiveness in scaling test-time compute for software engineering tasks. However, these approaches often focus on high-level solutions, with limited attention to optimizing low-level CUDA kernel implementations. Additionally, existing kernel generation benchmarks suffer from exploitable loopholes and insufficient diversity in testing conditions, hindering true generalization assessment. To address these limitations, we introduce robust-kbench, a new benchmark for rigorous evaluation of kernel performance and correctness across varied scenarios. Furthermore, we present a comprehensive agentic framework that automates CUDA kernel discovery, verification, and optimization. This pipeline enables frontier LLMs to translate torch code to CUDA kernels and iteratively improve their runtime within our robust evaluation setting. Our sequential workflow first translates PyTorch code into equivalent CUDA kernels. It then optimizes their runtime using a novel evolutionary meta-generation procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for correctness and efficient filtering. Evaluated on robust-kbench, our approach produces CUDA kernels outperforming torch implementations for practical applications, including forward and backward passes. It can fuse operations and deploy various runtime optimization strategies. The verifier workflow accurately classifies incorrect kernels, enhancing hardware verification efficiency.         ",
    "url": "https://arxiv.org/abs/2509.14279",
    "authors": [
      "Robert Tjarko Lange",
      "Qi Sun",
      "Aaditya Prasad",
      "Maxence Faldor",
      "Yujin Tang",
      "David Ha"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14281",
    "title": "SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems",
    "abstract": "           Significant advancements have been made in the capabilities of code large language models, leading to their rapid adoption and application across a wide range of domains. However, their further advancements are often constrained by the scarcity of real-world coding problems. To bridge this gap, we propose a novel framework for synthesizing code problems that emulate authentic real-world scenarios. This framework systematically integrates domain knowledge, domain skills, and coding skills, all of which are meticulously extracted from real-world programming-related datasets, including Stack Overflow and Kaggle. The extracted elements serve as the foundational building blocks for constructing code problems. To align the generated problems with practical applications, application scenarios are also mined from the aforementioned datasets. These scenarios are then utilized to construct a scenario-centric graph that interconnects domain knowledge, domain skills, and coding skills. Based on this structured representation, a sampling strategy on the graph is designed, which effectively controls the generation of a code problem with complexity and diversity, reflects real-world challenges. Experimental results demonstrate that the proposed method consistently achieves superior performance over state-of-the-art open-source large language models of varying sizes and functionalities, including both coders and general-purpose models, across a diverse set of real-world benchmarks.         ",
    "url": "https://arxiv.org/abs/2509.14281",
    "authors": [
      "Xifeng Yao",
      "Dongyu Lang",
      "Wu Zhang",
      "Xintong Guo",
      "Huarui Xie",
      "Yinhao Ni",
      "Ping Liu",
      "Guang Shen",
      "Yi Bai",
      "Dandan Tu",
      "Changzheng Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14282",
    "title": "Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning",
    "abstract": "           The emergence of quantum computing poses significant risks to the security of modern communication networks as it breaks today's public-key cryptographic algorithms. Quantum Key Distribution (QKD) offers a promising solution by harnessing the principles of quantum mechanics to establish secure keys. However, practical QKD implementations remain vulnerable to hardware imperfections and advanced attacks such as Photon Number Splitting and Trojan-Horse attacks. In this work, we investigate the potential of using quantum machine learning (QML) to detect popular QKD attacks. In particular, we propose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the detection of common QKD attacks. By combining quantum-enhanced learning with classical deep learning, the model captures complex temporal patterns in QKD data, improving detection accuracy. To evaluate the proposed model, we introduce a realistic QKD dataset simulating normal QKD operations along with seven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS), Trojan-Horse attacks Random Number Generator (RNG), Detector Blinding, Wavelength-dependent Trojan Horse, and Combined attacks. The dataset includes quantum security metrics such as Quantum Bit Error Rate (QBER), measurement entropy, signal and decoy loss rates, and time-based metrics, ensuring an accurate representation of real-world conditions. Our results demonstrate promising performance of the quantum machine learning approach compared to traditional classical machine learning models, highlighting the potential of hybrid techniques to enhance the security of future quantum communication networks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\\% after 50 training epochs, outperforming classical deep learning models such as LSTM, and CNN.         ",
    "url": "https://arxiv.org/abs/2509.14282",
    "authors": [
      "Ali Al-kuwari",
      "Noureldin Mohamed",
      "Saif Al-kuwari",
      "Ahmed Farouk",
      "Bikash K. Behera"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2509.14284",
    "title": "The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration",
    "abstract": "           As large language models (LLMs) become integral to multi-agent systems, new privacy risks emerge that extend beyond memorization, direct inference, or single-turn evaluations. In particular, seemingly innocuous responses, when composed across interactions, can cumulatively enable adversaries to recover sensitive information, a phenomenon we term compositional privacy leakage. We present the first systematic study of such compositional privacy leaks and possible mitigation methods in multi-agent LLM systems. First, we develop a framework that models how auxiliary knowledge and agent interactions jointly amplify privacy risks, even when each response is benign in isolation. Next, to mitigate this, we propose and evaluate two defense strategies: (1) Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent by anticipating how their outputs may be exploited by adversaries, and (2) Collaborative Consensus Defense (CoDef), where responder agents collaborate with peers who vote based on a shared aggregated state to restrict sensitive information spread. Crucially, we balance our evaluation across compositions that expose sensitive information and compositions that yield benign inferences. Our experiments quantify how these defense strategies differ in balancing the privacy-utility trade-off. We find that while chain-of-thought alone offers limited protection to leakage (~39% sensitive blocking rate), our ToM defense substantially improves sensitive query blocking (up to 97%) but can reduce benign task success. CoDef achieves the best balance, yielding the highest Balanced Outcome (79.8%), highlighting the benefit of combining explicit reasoning with defender collaboration. Together, our results expose a new class of risks in collaborative LLM deployments and provide actionable insights for designing safeguards against compositional, context-driven privacy leakage.         ",
    "url": "https://arxiv.org/abs/2509.14284",
    "authors": [
      "Vaidehi Patil",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.14285",
    "title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks",
    "abstract": "           Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inputs can override system prompts and induce unintended behaviors. This paper presents a novel multi-agent defense framework that employs specialized LLM agents in coordinated pipelines to detect and neutralize prompt injection attacks in real-time. We evaluate our approach using two distinct architectures: a sequential chain-of-agents pipeline and a hierarchical coordinator-based system. Our comprehensive evaluation on 55 unique prompt injection attacks, grouped into 8 categories and totaling 400 attack instances across two LLM platforms (ChatGLM and Llama2), demonstrates significant security improvements. Without defense mechanisms, baseline Attack Success Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent pipeline achieved 100% mitigation, reducing ASR to 0% across all tested scenarios. The framework demonstrates robustness across multiple attack categories including direct overrides, code execution attempts, data exfiltration, and obfuscation techniques, while maintaining system functionality for legitimate queries.         ",
    "url": "https://arxiv.org/abs/2509.14285",
    "authors": [
      "S M Asif Hossain",
      "Ruksat Khan Shayoni",
      "Mohd Ruhul Ameen",
      "Akif Islam",
      "M. F. Mridha",
      "Jungpil Shin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14373",
    "title": "CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning",
    "abstract": "           Context: Automated code generation using Foundation Models (FMs) offers promising solutions for enhancing software development efficiency. However, challenges remain in ensuring domain specificity, cost-effectiveness, and security - especially when relying on third-party APIs. This paper introduces CodeLSI, a framework that combines low-rank optimization and domain-specific instruction tuning to address these challenges. Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel approach for generating high-quality code tailored to specific domains, using FMs fine-tuned on company infrastructure without dependence on external APIs. Methods: CodeLSI applies low-rank adaptation techniques to reduce the computational cost of model pre-training and fine-tuning. Domain-specific instruction tuning is employed to align code generation with organizational needs. We implemented and tested the framework on real-world JavaScript coding tasks using datasets drawn from internal software projects. Results: Experimental evaluations show that CodeLSI produces high-quality, context aware code. It outperforms baseline models in terms of relevance, accuracy, and domain fit. The use of low-rank optimization significantly reduced resource requirements, enabling scalable training on company-owned infrastructure. Conclusion: CodeLSI demonstrates that combining low-rank optimization with domain specific tuning can enhance the practicality and performance of FMs for automated code generation. This approach provides a secure, cost-efficient alternative to commercial API based solutions and supports faster, more targeted innovation in software development.         ",
    "url": "https://arxiv.org/abs/2509.14373",
    "authors": [
      "Huy Le",
      "Phong Nguyen",
      "Hao Do",
      "Tuan Nguyen",
      "Thien Pham",
      "Anh Nguyen-Duc",
      "Tho Quan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.14383",
    "title": "RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings",
    "abstract": "           Unified multi-modal encoders that bind vision, audio, and other sensors into a shared embedding space are attractive building blocks for robot perception and decision-making. However, on-robot deployment exposes the vision branch to adversarial and natural corruptions, making robustness a prerequisite for safety. Prior defenses typically align clean and adversarial features within CLIP-style encoders and overlook broader cross-modal correspondence, yielding modest gains and often degrading zero-shot transfer. We introduce RLBind, a two-stage adversarial-invariant cross-modal alignment framework for robust unified embeddings. Stage 1 performs unsupervised fine-tuning on clean-adversarial pairs to harden the visual encoder. Stage 2 leverages cross-modal correspondence by minimizing the discrepancy between clean/adversarial features and a text anchor, while enforcing class-wise distributional alignment across modalities. Extensive experiments on Image, Audio, Thermal, and Video data show that RLBind consistently outperforms the LanguageBind backbone and standard fine-tuning baselines in both clean accuracy and norm-bounded adversarial robustness. By improving resilience without sacrificing generalization, RLBind provides a practical path toward safer multi-sensor perception stacks for embodied robots in navigation, manipulation, and other autonomy settings.         ",
    "url": "https://arxiv.org/abs/2509.14383",
    "authors": [
      "Yuhong Lu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.14384",
    "title": "A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation",
    "abstract": "           In this paper, we investigate the efficiency of Deep Neural Networks (DNNs) to approximate the solution of a nonlocal conservation law derived from the identical-oscillator Kuramoto model, focusing on the evaluation of an architectural choice and its impact on solution accuracy based on the energy norm and computation time. Through systematic experimentation, we demonstrate that network configuration parameters-specifically, activation function selection (tanh vs. sin vs. ReLU), network depth (4-8 hidden layers), width (64-256 neurons), and training methodology (collocation points, epoch count)-significantly influence convergence characteristics. We observe that tanh activation yields stable convergence across configurations, whereas sine activation can attain marginally lower errors and training times in isolated cases, but occasionally produce nonphysical artefacts. Our comparative analysis with traditional numerical methods shows that optimally configured DNNs offer competitive accuracy with notably different computational trade-offs. Furthermore, we identify fundamental limitations of standard feed-forward architectures when handling singular or piecewise-constant solutions, providing empirical evidence that such networks inherently oversmooth sharp features due to the natural function space limitations of standard activation functions. This work contributes to the growing body of research on neural network-based scientific computing by providing practitioners with empirical guidelines for DNN implementation while illuminating fundamental theoretical constraints that must be overcome to expand their applicability to more challenging physical systems with discontinuities.         ",
    "url": "https://arxiv.org/abs/2509.14384",
    "authors": [
      "Nishantak Panigrahi",
      "Mayank Patwal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2509.14420",
    "title": "Class-invariant Test-Time Augmentation for Domain Generalization",
    "abstract": "           Deep models often suffer significant performance degradation under distribution shifts. Domain generalization (DG) seeks to mitigate this challenge by enabling models to generalize to unseen domains. Most prior approaches rely on multi-domain training or computationally intensive test-time adaptation. In contrast, we propose a complementary strategy: lightweight test-time augmentation. Specifically, we develop a novel Class-Invariant Test-Time Augmentation (CI-TTA) technique. The idea is to generate multiple variants of each input image through elastic and grid deformations that nevertheless belong to the same class as the original input. Their predictions are aggregated through a confidence-guided filtering scheme that remove unreliable outputs, ensuring the final decision relies on consistent and trustworthy cues. Extensive Experiments on PACS and Office-Home datasets demonstrate consistent gains across different DG algorithms and backbones, highlighting the effectiveness and generality of our approach.         ",
    "url": "https://arxiv.org/abs/2509.14420",
    "authors": [
      "Zhicheng Lin",
      "Xiaolin Wu",
      "Xi Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14431",
    "title": "Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control",
    "abstract": "           Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm for coordinating swarms of agents in complex decision-making, yet major challenges remain. In competitive settings such as pursuer-evader tasks, simultaneous adaptation can destabilize training; non-kinetic countermeasures often fail under adverse conditions; and policies trained in one configuration rarely generalize to environments with a different number of agents. To address these issues, we propose the Local-Canonicalization Equivariant Graph Neural Networks (LEGO) framework, which integrates seamlessly with popular MARL algorithms such as MAPPO. LEGO employs graph neural networks to capture permutation equivariance and generalization to different agent numbers, canonicalization to enforce E(n)-equivariance, and heterogeneous representations to encode role-specific inductive biases. Experiments on cooperative and competitive swarm benchmarks show that LEGO outperforms strong baselines and improves generalization. In real-world experiments, LEGO demonstrates robustness to varying team sizes and agent failure.         ",
    "url": "https://arxiv.org/abs/2509.14431",
    "authors": [
      "Keqin Wang",
      "Tao Zhong",
      "David Chang",
      "Christine Allen-Blanchette"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.14434",
    "title": "Value Alignment of Social Media Ranking Algorithms",
    "abstract": "           While social media feed rankings are primarily driven by engagement signals rather than any explicit value system, the resulting algorithmic feeds are not value-neutral: engagement may prioritize specific individualistic values. This paper presents an approach for social media feed value alignment. We adopt Schwartz's theory of Basic Human Values -- a broad set of human values that articulates complementary and opposing values forming the building blocks of many cultures -- and we implement an algorithmic approach that models and then ranks feeds by expressions of Schwartz's values in social media posts. Our approach enables controls where users can express weights on their desired values, combining these weights and post value expressions into a ranking that respects users' articulated trade-offs. Through controlled experiments (N=141 and N=250), we demonstrate that users can use these controls to architect feeds reflecting their desired values. Across users, value-ranked feeds align with personal values, diverging substantially from existing engagement-driven feeds.         ",
    "url": "https://arxiv.org/abs/2509.14434",
    "authors": [
      "Farnaz Jahanbakhsh",
      "Dora Zhao",
      "Tiziano Piccardi",
      "Zachary Robertson",
      "Ziv Epstein",
      "Sanmi Koyejo",
      "Michael S. Bernstein"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2509.14437",
    "title": "Multi-Objective Loss Balancing in Physics-Informed Neural Networks for Fluid Flow Applications",
    "abstract": "           Physics-Informed Neural Networks (PINNs) have emerged as a promising machine learning approach for solving partial differential equations (PDEs). However, PINNs face significant challenges in balancing multi-objective losses, as multiple competing loss terms such as physics residuals, boundary conditions, and initial conditions must be appropriately weighted. While various loss balancing schemes have been proposed, they have been implemented within neural network architectures with fixed activation functions, and their effectiveness has been assessed using simpler PDEs. We hypothesize that the effectiveness of loss balancing schemes depends not only on the balancing strategy itself, but also on the neural network's inherent function approximation capabilities, which are influenced by the choice of activation function. In this paper, we extend existing solutions by incorporating trainable activation functions within the neural network architecture and evaluate the proposed approach on complex fluid flow applications modeled by the Navier-Stokes equations. Our evaluation across diverse Navier-Stokes problems demonstrates that this proposed solution achieves root mean square error (RMSE) improvements ranging from 7.4\\% to 95.2\\% across different scenarios. These findings underscore the importance of carefully considering the interaction between activation function selection and balancing algorithms when designing loss balancing strategies.         ",
    "url": "https://arxiv.org/abs/2509.14437",
    "authors": [
      "Afrah Farea",
      "Saiful Khan",
      "Mustafa Serdar Celebi"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2509.14460",
    "title": "Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring",
    "abstract": "           Learning abstractions directly from data is a core challenge in robotics. Humans naturally operate at an abstract level, reasoning over high-level subgoals while delegating execution to low-level motor skills -- an ability that enables efficient problem solving in complex environments. In robotics, abstractions and hierarchical reasoning have long been central to planning, yet they are typically hand-engineered, demanding significant human effort and limiting scalability. Automating the discovery of useful abstractions directly from visual data would make planning frameworks more scalable and more applicable to real-world robotic domains. In this work, we focus on rearrangement tasks where the state is represented with raw images, and propose a method to induce discrete, graph-structured abstractions by combining structural constraints with an attention-guided visual distance. Our approach leverages the inherent bipartite structure of rearrangement problems, integrating structural constraints and visual embeddings into a unified framework. This enables the autonomous discovery of abstractions from vision alone, which can subsequently support high-level planning. We evaluate our method on two rearrangement tasks in simulation and show that it consistently identifies meaningful abstractions that facilitate effective planning and outperform existing approaches.         ",
    "url": "https://arxiv.org/abs/2509.14460",
    "authors": [
      "Abhiroop Ajith",
      "Constantinos Chamzas"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.14482",
    "title": "On Optimality and Human Prediction of Event Duration in Real-Time, Real-World Contexts",
    "abstract": "           The focus of the current work concerned the psychological processes that underlie prediction of an events duration. The objective was to push forward existing psychological theory on event duration prediction, something made possible by the unique features of our data context. The provisional findings suggested that the prior, existing theoretical mechanism of event duration prediction is incomplete because: i. it does not support adaptive responses when event duration judgments are dependent, ii. it does not afford the integration of new, on the fly, information. Our findings suggest specific directions for future research.         ",
    "url": "https://arxiv.org/abs/2509.14482",
    "authors": [
      "Mark G Orr"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2509.14508",
    "title": "Modeling User Redemption Behavior in Complex Incentive Digital Environment: An Empirical Study Using Large-Scale Transactional Data",
    "abstract": "           The digital economy implements complex incentive systems to retain users through point redemption. Understanding user behavior in such complex incentive structures presents a fundamental challenge, especially in estimating the value of these digital assets against traditional money. This study tackles this question by analyzing large-scale, real-world transaction data from a popular personal finance application that captures both monetary spending and point-based transactions across Japan's deeply integrated loyalty networks. We find that point usage is not random but is systematically linked to demographics, with older users tending to convert points into financial assets. Furthermore, our analysis using a natural experiment and a causal inference technique reveals that a large point grant stimulated an increase in point spending without affecting cash expenditure. We also find that consumers' shopping styles are associated with their point redemption patterns. This study, conducted within a massive real-world economic ecosystem, examines how consumers navigate multi-currency environments, with direct implications for modeling economic behavior and designing digital platforms.         ",
    "url": "https://arxiv.org/abs/2509.14508",
    "authors": [
      "Akira Matsui",
      "Takashi Teramoto",
      "Eiji Motohashi",
      "Hiroyuki Tsurumi"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2509.14523",
    "title": "A Software-Defined Radio Testbed for Distributed LiDAR Point Cloud Sharing with IEEE 802.11p in V2V Networks",
    "abstract": "           We present a Software Defined Radio (SDR)-based IEEE 802.11p testbed for distributed Vehicle-to-Vehicle (V2V) communication. The platform bridges the gap between network simulation and deployment by providing a modular codebase configured for cost-effective ADALM-Pluto SDRs. Any device capable of running a Docker with ROS, executing Matlab and interface with a Pluto via USB can act as a communication node. To demonstrate collaborative sensing, we share LiDAR point clouds between nodes and fuse them into a collective perception environment. We evaluated a theoretical model for leveraging decentralized storage systems (IPFS and Filecoin), analyzing constraints such as node storage convergence, latency, and scalability. In addition, we provide a channel quality study.         ",
    "url": "https://arxiv.org/abs/2509.14523",
    "authors": [
      "Mario Hernandez",
      "Elijah Bryce",
      "Peter Stubberud",
      "Ebrahim Saberinia",
      "Brendan Morris"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2509.14540",
    "title": "Design-Space Exploration of Distributed Neural Networks in Low-Power Wearable Nodes",
    "abstract": "           Wearable devices are revolutionizing personal technology, but their usability is often hindered by frequent charging due to high power consumption. This paper introduces Distributed Neural Networks (DistNN), a framework that distributes neural network computations between resource-constrained wearable nodes and resource-rich hubs to reduce energy at the node without sacrificing performance. We define a Figure of Merit (FoM) to select the optimal split point that minimizes node-side energy. A custom hardware design using low-precision fixed-point arithmetic achieves ultra-low power while maintaining accuracy. The proposed system is ~1000x more energy efficient than a GPU and averages 11x lower power than recent machine learning (ML) ASICs at 30 fps. Evaluated with CNNs and autoencoders, DistNN attains an SSIM of 0.90 for image reconstruction and 0.89 for denoising, enabling scalable, energy-efficient, real-time wearable applications.         ",
    "url": "https://arxiv.org/abs/2509.14540",
    "authors": [
      "Meghna Roy Chowdhury",
      "Ming-che Li",
      "Archisman Ghosh",
      "Md Faizul Bari",
      "Shreyas Sen"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2509.14558",
    "title": "LLM Jailbreak Detection for (Almost) Free!",
    "abstract": "           Large language models (LLMs) enhance security through alignment when widely used, but remain susceptible to jailbreak attacks capable of producing inappropriate content. Jailbreak detection methods show promise in mitigating jailbreak attacks through the assistance of other models or multiple model inferences. However, existing methods entail significant computational costs. In this paper, we first present a finding that the difference in output distributions between jailbreak and benign prompts can be employed for detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak Detection (FJD) which prepends an affirmative instruction to the input and scales the logits by temperature to further distinguish between jailbreak and benign prompts through the confidence of the first token. Furthermore, we enhance the detection performance of FJD through the integration of virtual instruction learning. Extensive experiments on aligned LLMs show that our FJD can effectively detect jailbreak prompts with almost no additional computational costs during LLM inference.         ",
    "url": "https://arxiv.org/abs/2509.14558",
    "authors": [
      "Guorui Chen",
      "Yifan Xia",
      "Xiaojun Jia",
      "Zhijiang Li",
      "Philip Torr",
      "Jindong Gu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.14563",
    "title": "Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework",
    "abstract": "           The discovery of environmental knowledge depends on labeled task-specific data, but is often constrained by the high cost of data collection. Existing machine learning approaches usually struggle to generalize in data-sparse or atypical conditions. To this end, we propose an Augmentation-Adaptive Self-Supervised Learning (A$^2$SL) framework, which retrieves relevant observational samples to enhance modeling of the target ecosystem. Specifically, we introduce a multi-level pairwise learning loss to train a scenario encoder that captures varying degrees of similarity among scenarios. These learned similarities drive a retrieval mechanism that supplements a target scenario with relevant data from different locations or time periods. Furthermore, to better handle variable scenarios, particularly under atypical or extreme conditions where traditional models struggle, we design an augmentation-adaptive mechanism that selectively enhances these scenarios through targeted data augmentation. Using freshwater ecosystems as a case study, we evaluate A$^2$SL in modeling water temperature and dissolved oxygen dynamics in real-world lakes. Experimental results show that A$^2$SL significantly improves predictive accuracy and enhances robustness in data-scarce and atypical scenarios. Although this study focuses on freshwater ecosystems, the A$^2$SL framework offers a broadly applicable solution in various scientific domains.         ",
    "url": "https://arxiv.org/abs/2509.14563",
    "authors": [
      "Shiyuan Luo",
      "Runlong Yu",
      "Chonghao Qiu",
      "Rahul Ghosh",
      "Robert Ladwig",
      "Paul C. Hanson",
      "Yiqun Xie",
      "Xiaowei Jia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14568",
    "title": "Evidential Physics-Informed Neural Networks for Scientific Discovery",
    "abstract": "           We present the fundamental theory and implementation guidelines underlying Evidential Physics-Informed Neural Network (E-PINN) -- a novel class of uncertainty-aware PINN. It leverages the marginal distribution loss function of evidential deep learning for estimating uncertainty of outputs, and infers unknown parameters of the PDE via a learned posterior distribution. Validating our model on two illustrative case studies -- the 1D Poisson equation with a Gaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated empirical coverage probabilities that were calibrated significantly better than Bayesian PINN and Deep Ensemble methods. To demonstrate real-world applicability, we also present a brief case study on applying E-PINN to analyze clinical glucose-insulin datasets that have featured in medical research on diabetes pathophysiology.         ",
    "url": "https://arxiv.org/abs/2509.14568",
    "authors": [
      "Hai Siong Tan",
      "Kuancheng Wang",
      "Rafe McBeth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2509.14571",
    "title": "VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models",
    "abstract": "           Vision-language (VL) models have shown transformative potential across various critical domains due to their capability to comprehend multi-modal information. However, their performance frequently degrades under distribution shifts, making it crucial to assess and improve robustness against real-world data corruption encountered in practical applications. While advancements in VL benchmark datasets and data augmentation (DA) have contributed to robustness evaluation and improvement, there remain challenges due to a lack of in-depth comprehension of model behavior as well as the need for expertise and iterative efforts to explore data patterns. Given the achievement of visualization in explaining complex models and exploring large-scale data, understanding the impact of various data corruption on VL models aligns naturally with a visual analytics approach. To address these challenges, we introduce VisMoDAl, a visual analytics framework designed to evaluate VL model robustness against various corruption types and identify underperformed samples to guide the development of effective DA strategies. Grounded in the literature review and expert discussions, VisMoDAl supports multi-level analysis, ranging from examining performance under specific corruptions to task-driven inspection of model behavior and corresponding data slice. Unlike conventional works, VisMoDAl enables users to reason about the effects of corruption on VL models, facilitating both model behavior understanding and DA strategy formulation. The utility of our system is demonstrated through case studies and quantitative evaluations focused on corruption robustness in the image captioning task.         ",
    "url": "https://arxiv.org/abs/2509.14571",
    "authors": [
      "Huanchen Wang",
      "Wencheng Zhang",
      "Zhiqiang Wang",
      "Zhicong Lu",
      "Yuxin Ma"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14575",
    "title": "Weak Adversarial Neural Pushforward Mappings for Fokker-Planck Equations",
    "abstract": "           This paper presents a novel method for solving Fokker-Planck equations by learning neural samplers via a weak adversarial framework. We represent the solution distribution through a neural pushforward map, bypassing the limitations of density-based methods. A key innovation is our use of computationally efficient plane-wave test functions, whose derivatives are explicitly computed -- a treatment distinct from prior work. This approach handles distributions without densities and naturally enforces probability conservation.         ",
    "url": "https://arxiv.org/abs/2509.14575",
    "authors": [
      "Andrew Qing He",
      "Wei Cai"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2509.14576",
    "title": "TypedSchematics: A Block-based PCB Design Tool with Real-time Detection of Common Connection Errors",
    "abstract": "           Within PCB design, the reuse of circuit design blocks is a major preventing factor inhibiting beginners from reusing designs made by experts, a common practice in software but non-existent in circuit design at large. Despite efforts to improve reusability (e.g. block-based PCB design) by platforms such as SparkFun ALC and Altium Upverter, they lack merging techniques that safely guide users in connecting different circuit blocks without requiring assistance from third-party engineers. In this paper, we propose TypedSchematics, a block-based standalone PCB design tool that supports beginners create their own PCBs by providing a language syntax for typing circuit blocks with circuit data that addresses multiple challenges, from real-time detection of connection errors to automated composition and user-scalable libraries of circuit blocks. Through a user study, we demonstrate TypedSchematics improvements in design support for merging circuit blocks compared to Fusion 360. Three PCBs designed with TypedSchematics further showcase our tool capabilities, one designed by high school students demonstrates the potential of TypedSchematics to significantly lower the PCB design skill-floor.         ",
    "url": "https://arxiv.org/abs/2509.14576",
    "authors": [
      "Jorge Garza",
      "Steven Swanson"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.14581",
    "title": "Can I Trust This Chatbot? Assessing User Privacy in AI-Healthcare Chatbot Applications",
    "abstract": "           As Conversational Artificial Intelligence (AI) becomes more integrated into everyday life, AI-powered chatbot mobile applications are increasingly adopted across industries, particularly in the healthcare domain. These chatbots offer accessible and 24/7 support, yet their collection and processing of sensitive health data present critical privacy concerns. While prior research has examined chatbot security, privacy issues specific to AI healthcare chatbots have received limited attention. Our study evaluates the privacy practices of 12 widely downloaded AI healthcare chatbot apps available on the App Store and Google Play in the United States. We conducted a three-step assessment analyzing: (1) privacy settings during sign-up, (2) in-app privacy controls, and (3) the content of privacy policies. The analysis identified significant gaps in user data protection. Our findings reveal that half of the examined apps did not present a privacy policy during sign up, and only two provided an option to disable data sharing at that stage. The majority of apps' privacy policies failed to address data protection measures. Moreover, users had minimal control over their personal data. The study provides key insights for information science researchers, developers, and policymakers to improve privacy protections in AI healthcare chatbot apps.         ",
    "url": "https://arxiv.org/abs/2509.14581",
    "authors": [
      "Ramazan Yener",
      "Guan-Hung Chen",
      "Ece Gumusel",
      "Masooda Bashir"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2509.14582",
    "title": "Joint Scheduling and Multiflow Maximization in Wireless Networks",
    "abstract": "           Towards the development of 6G mobile networks, it is promising to integrate a large number of devices from multi-dimensional platforms, and it is crucial to have a solid understanding of the theoretical limits of large-scale networks. We revisit a fundamental problem at the heart of network communication theory: the maximum multiflow (MMF) problem in multi-hop networks, with network coding performed at intermediate nodes. To derive the exact-optimal solution to the MMF problem (as opposed to approximations), conventional methods usually involve two steps: first calculate the scheduling rate region, and then find the maximum multiflow that can be supported by the achievable link rates. However, the NP-hardness of the scheduling part makes solving the MMF problem in large networks computationally prohibitive. In this paper, while still focusing on the exact-optimal solution, we provide efficient algorithms that can jointly calculate the scheduling rate region and solve the MMF problem, thereby outputting optimal values without requiring the entire scheduling rate region. We theoretically prove that our algorithms always output optimal solutions in a finite number of iterations, and we use various simulation results to demonstrate our advantages over conventional approaches. Our framework is applicable to the most general scenario in multi-source multi-sink networks: the multiple multicast problem with network coding. Moreover, by employing a graphical framework, we show that our algorithm can be extended to scenarios where propagation delays are large (e.g., underwater networks), in which recent studies have shown that the scheduling rate region can be significantly improved by utilizing such delays.         ",
    "url": "https://arxiv.org/abs/2509.14582",
    "authors": [
      "Yanxiao Liu",
      "Shenghao Yang",
      "Cheuk Ting Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2509.14583",
    "title": "What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System",
    "abstract": "           The web continues to grow, but dependency-monitoring tools and standards for resource integrity lag behind. Currently, there exists no robust method to verify the integrity of web resources, much less in a generalizable yet performant manner, and supply chains remain one of the most targeted parts of the attack surface of web applications. In this paper, we present the design of LiMS, a transparent system to bootstrap link integrity guarantees in web browsing sessions with minimal overhead. At its core, LiMS uses a set of customizable integrity policies to declare the (un)expected properties of resources, verifies these policies, and enforces them for website visitors. We discuss how basic integrity policies can serve as building blocks for a comprehensive set of integrity policies, while providing guarantees that would be sufficient to defend against recent supply chain attacks detailed by security industry reports. Finally, we evaluate our open-sourced prototype by simulating deployments on a representative sample of 450 domains that are diverse in ranking and category. We find that our proposal offers the ability to bootstrap marked security improvements with an overall overhead of hundreds of milliseconds on initial page loads, and negligible overhead on reloads, regardless of network speeds. In addition, from examining archived data for the sample sites, we find that several of the proposed policy building blocks suit their dependency usage patterns, and would incur minimal administrative overhead.         ",
    "url": "https://arxiv.org/abs/2509.14583",
    "authors": [
      "Johnny So",
      "Michael Ferdman",
      "Nick Nikiforakis"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.14610",
    "title": "Enhancing Feature Fusion of U-like Networks with Dynamic Skip Connections",
    "abstract": "           U-like networks have become fundamental frameworks in medical image segmentation through skip connections that bridge high-level semantics and low-level spatial details. Despite their success, conventional skip connections exhibit two key limitations: inter-feature constraints and intra-feature constraints. The inter-feature constraint refers to the static nature of feature fusion in traditional skip connections, where information is transmitted along fixed pathways regardless of feature content. The intra-feature constraint arises from the insufficient modeling of multi-scale feature interactions, thereby hindering the effective aggregation of global contextual information. To overcome these limitations, we propose a novel Dynamic Skip Connection (DSC) block that fundamentally enhances cross-layer connectivity through adaptive mechanisms. The DSC block integrates two complementary components. (1) Test-Time Training (TTT) module. This module addresses the inter-feature constraint by enabling dynamic adaptation of hidden representations during inference, facilitating content-aware feature refinement. (2) Dynamic Multi-Scale Kernel (DMSK) module. To mitigate the intra-feature constraint, this module adaptively selects kernel sizes based on global contextual cues, enhancing the network capacity for multi-scale feature integration. The DSC block is architecture-agnostic and can be seamlessly incorporated into existing U-like network structures. Extensive experiments demonstrate the plug-and-play effectiveness of the proposed DSC block across CNN-based, Transformer-based, hybrid CNN-Transformer, and Mamba-based U-like networks.         ",
    "url": "https://arxiv.org/abs/2509.14610",
    "authors": [
      "Yue Cao",
      "Quansong He",
      "Kaishen Wang",
      "Jianlong Xiong",
      "Tao He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.14619",
    "title": "LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition",
    "abstract": "           Skeleton-based action recognition faces two longstanding challenges: the scarcity of labeled training samples and difficulty modeling short- and long-range temporal dependencies. To address these issues, we propose a unified framework, LSTC-MDA, which simultaneously improves temporal modeling and data diversity. We introduce a novel Long-Short Term Temporal Convolution (LSTC) module with parallel short- and long-term branches, these two feature branches are then aligned and fused adaptively using learned similarity weights to preserve critical long-range cues lost by conventional stride-2 temporal convolutions. We also extend Joint Mixing Data Augmentation (JMDA) with an Additive Mixup at the input level, diversifying training samples and restricting mixup operations to the same camera view to avoid distribution shifts. Ablation studies confirm each component contributes. LSTC-MDA achieves state-of-the-art results: 94.1% and 97.5% on NTU 60 (X-Sub and X-View), 90.4% and 92.0% on NTU 120 (X-Sub and X-Set),97.2% on NW-UCLA. Code: this https URL.         ",
    "url": "https://arxiv.org/abs/2509.14619",
    "authors": [
      "Feng Ding",
      "Haisheng Fu",
      "Soroush Oraki",
      "Jie Liang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14622",
    "title": "Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection",
    "abstract": "           With the deployment of Large Language Models (LLMs) in interactive applications, online malicious intent detection has become increasingly critical. However, existing approaches fall short of handling diverse and complex user queries in real time. To address these challenges, we introduce ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework for robust and efficient online malicious intent detection. In the training stage, a high-capacity teacher model is trained on adversarially perturbed, retrieval-augmented inputs to learn robust decision boundaries over diverse and complex user queries. In the inference stage, a distillation scheduler transfers the teacher's knowledge into a compact student model, with a continually updated knowledge base collected online. At deployment, the compact student model leverages top-K similar safety exemplars retrieved from the online-updated knowledge base to enable both online and real-time malicious query detection. Evaluations across ten safety benchmarks demonstrate that ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on out-of-distribution detection, while simultaneously delivering up to 5.6x lower latency at 300 queries per second (QPS) in real-time applications.         ",
    "url": "https://arxiv.org/abs/2509.14622",
    "authors": [
      "Yihao Guo",
      "Haocheng Bian",
      "Liutong Zhou",
      "Ze Wang",
      "Zhaoyi Zhang",
      "Francois Kawala",
      "Milan Dean",
      "Ian Fischer",
      "Yuantao Peng",
      "Noyan Tokgozoglu",
      "Ivan Barrientos",
      "Riyaaz Shaik",
      "Rachel Li",
      "Chandru Venkataraman",
      "Reza Shifteh Far",
      "Moses Pawar",
      "Venkat Sundaranatha",
      "Michael Xu",
      "Frank Chu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14628",
    "title": "Chameleon: Integrated Sensing and Communication with Sub-Symbol Beam Switching in mmWave Networks",
    "abstract": "           Next-generation cellular networks are envisioned to integrate sensing capabilities with communication, particularly in the millimeter-wave (mmWave) spectrum, where beamforming using large-scale antenna arrays enables directional signal transmissions for improved spatial multiplexing. In current 5G networks, however, beamforming is typically designed either for communication or sensing (e.g., beam training during link establishment). In this paper, we present Chameleon, a novel framework that augments and rapidly switches beamformers during each demodulation reference signal (DMRS) symbol to achieve integrated sensing and communication (ISAC) in 5G mmWave networks. Each beamformer introduces an additional sensing beam toward target angles while maintaining the communication beams toward multiple users. We implement Chameleon on a 28 GHz software-defined radio testbed supporting over-the-air 5G physical downlink shared channel (PDSCH) transmissions. Extensive experiments in open environments show that Chameleon achieves multi-user communication with a sum data rate of up to 0.80 Gbps across two users. Simultaneously, Chameleon employs a beamformer switching interval of only 0.24 {\\mu}s, therefore producing a 31x31-point 2D imaging within just 0.875 ms. Leveraging machine learning, Chameleon further enables object localization with median errors of 0.14 m (distance) and 0.24\u00b0 (angle), and material classification with 99.0% accuracy.         ",
    "url": "https://arxiv.org/abs/2509.14628",
    "authors": [
      "Zhihui Gao",
      "Zhecun Liu",
      "Tingjun Chen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2509.14635",
    "title": "SWE-QA: Can Language Models Answer Repository-level Code Questions?",
    "abstract": "           Understanding and reasoning about entire software repositories is an essential capability for intelligent software engineering tools. While existing benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly focus on small, self-contained code snippets. These setups fail to capture the complexity of real-world repositories, where effective understanding and reasoning often require navigating multiple files, understanding software architecture, and grounding answers in long-range code dependencies. In this paper, we present SWE-QA, a repository-level code question answering (QA) benchmark designed to facilitate research on automated QA systems in realistic code environments. SWE-QA involves 576 high-quality question-answer pairs spanning diverse categories, including intention understanding, cross-file reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis of naturally occurring developer questions extracted from these issues, we developed a two-level taxonomy of repository-level questions and constructed a set of seed questions for each category. For each category, we manually curated and validated questions and collected their corresponding answers. As a prototype application, we further develop SWE-QA-Agent, an agentic framework in which LLM agents reason and act to find answers automatically. We evaluate six advanced LLMs on SWE-QA under various context augmentation strategies. Experimental results highlight the promise of LLMs, particularly our SWE-QA-Agent framework, in addressing repository-level QA, while also revealing open challenges and pointing to future research directions.         ",
    "url": "https://arxiv.org/abs/2509.14635",
    "authors": [
      "Weihan Peng",
      "Yuling Shi",
      "Yuhang Wang",
      "Xinyun Zhang",
      "Beijun Shen",
      "Xiaodong Gu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.14642",
    "title": "DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training",
    "abstract": "           Modeling dynamic temporal dependencies is a critical challenge in time series pre-training, which evolve due to distribution shifts and multi-scale patterns. This temporal variability severely impairs the generalization of pre-trained models to downstream tasks. Existing frameworks fail to capture the complex interactions of short- and long-term dependencies, making them susceptible to spurious correlations that degrade generalization. To address these limitations, we propose DeCoP, a Dependency Controlled Pre-training framework that explicitly models dynamic, multi-scale dependencies by simulating evolving inter-patch dependencies. At the input level, DeCoP introduces Instance-wise Patch Normalization (IPN) to mitigate distributional shifts while preserving the unique characteristics of each patch, creating a robust foundation for representation learning. At the latent level, a hierarchical Dependency Controlled Learning (DCL) strategy explicitly models inter-patch dependencies across multiple temporal scales, with an Instance-level Contrastive Module (ICM) enhances global generalization by learning instance-discriminative representations from time-invariant positive pairs. DeCoP achieves state-of-the-art results on ten datasets with lower computing resources, improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.         ",
    "url": "https://arxiv.org/abs/2509.14642",
    "authors": [
      "Yuemin Wu",
      "Zhongze Wu",
      "Xiu Su",
      "Feng Yang",
      "Hongyan Xu",
      "Xi Lin",
      "Wenti Huang",
      "Shan You",
      "Chang Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14688",
    "title": "exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation",
    "abstract": "           Tactile-aware robot learning faces critical challenges in data collection and representation due to data scarcity and sparsity, and the absence of force feedback in existing systems. To address these limitations, we introduce a tactile robot learning system with both hardware and algorithm innovations. We present exUMI, an extensible data collection device that enhances the vanilla UMI with robust proprioception (via AR MoCap and rotary encoder), modular visuo-tactile sensing, and automated calibration, achieving 100% data usability. Building on an efficient collection of over 1 M tactile frames, we propose Tactile Prediction Pretraining (TPP), a representation learning framework through action-aware temporal tactile prediction, capturing contact dynamics and mitigating tactile sparsity. Real-world experiments show that TPP outperforms traditional tactile imitation learning. Our work bridges the gap between human tactile intuition and robot learning through co-designed hardware and algorithms, offering open-source resources to advance contact-rich manipulation research. Project page: this https URL.         ",
    "url": "https://arxiv.org/abs/2509.14688",
    "authors": [
      "Yue Xu",
      "Litao Wei",
      "Pengyu An",
      "Qingyu Zhang",
      "Yong-Lu Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.14693",
    "title": "RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning",
    "abstract": "           Logs constitute a form of evidence signaling the operational status of software systems. Automated log anomaly detection is crucial for ensuring the reliability of modern software systems. However, existing approaches face significant limitations: traditional deep learning models lack interpretability and generalization, while methods leveraging Large Language Models are often hindered by unreliability and factual inaccuracies. To address these issues, we propose RationAnomaly, a novel framework that enhances log anomaly detection by synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our approach first instills expert-like reasoning patterns using CoT-guided supervised fine-tuning, grounded in a high-quality dataset corrected through a rigorous expert-driven process. Subsequently, a reinforcement learning phase with a multi-faceted reward function optimizes for accuracy and logical consistency, effectively mitigating hallucinations. Experimentally, RationAnomaly outperforms state-of-the-art baselines, achieving superior F1-scores on key benchmarks while providing transparent, step-by-step analytical outputs. We have released the corresponding resources, including code and datasets.         ",
    "url": "https://arxiv.org/abs/2509.14693",
    "authors": [
      "Song Xu",
      "Yilun Liu",
      "Minggui He",
      "Mingchen Dai",
      "Ziang Chen",
      "Chunguang Zhao",
      "Jingzhou Du",
      "Shimin Tao",
      "Weibin Meng",
      "Shenglin Zhang",
      "Yongqian Sun",
      "Boxing Chen",
      "Daimeng Wei"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14705",
    "title": "Secure Short-Packet Communications for RIS-Assisted AAV Networks",
    "abstract": "           Advancements toward 6G have intensified demands for ultra-reliable low-latency communication, positioning shortpacket communications as a critical technology for autonomous aerial vehicle (AAV) networks. However, the open broadcast nature introduces significant security vulnerabilities. Although physical-layer security offers a low-complexity solution by exploiting wireless channel randomness, the AAV communication performance severely degrades in weak-coverage or non-line-of sight scenarios. To overcome these limitations, this paper proposes a short-packet communications framework for AAV networks that leverages reconfigurable intelligent surfaces (RIS) with the aim of extending coverage and enhancing secrecy capabilities. Analytical frameworks are developed to evaluate the average secrecy throughput (AST) in finite blocklength constraints for both external and internal avesdropping scenarios, which incorporates non-orthogonal multiple access with imperfect successive interference cancellation. Asymptotic approximations of AST are derived as transmit power approaches infinity. Furthermore, we formulate a blocklength optimization problem to maximize the AST, effectively resolving the trade-offs among delay, reliability, and secrecy. Extensive simulations validate the analytical frameworks, which reveal that large-scale RIS deployment significantly boosts AST, and the power allocation coefficient exhibits dual effects in the internal eavesdropping scenario. These observations provide useful insights for designing reliable and secure lowlatency AAV communications systems.         ",
    "url": "https://arxiv.org/abs/2509.14705",
    "authors": [
      "Huiling Liu",
      "Junshan Luo",
      "Shilian Wang",
      "Fanggang Wang",
      "Theodoros A. Tsiftsis",
      "Symeon Chatzinotas"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.14722",
    "title": "Towards Pre-trained Graph Condensation via Optimal Transport",
    "abstract": "           Graph condensation (GC) aims to distill the original graph into a small-scale graph, mitigating redundancy and accelerating GNN training. However, conventional GC approaches heavily rely on rigid GNNs and task-specific supervision. Such a dependency severely restricts their reusability and generalization across various tasks and architectures. In this work, we revisit the goal of ideal GC from the perspective of GNN optimization consistency, and then a generalized GC optimization objective is derived, by which those traditional GC methods can be viewed nicely as special cases of this optimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC) via optimal transport is proposed to transcend the limitations of task- and architecture-dependent GC methods. Specifically, a hybrid-interval graph diffusion augmentation is presented to suppress the weak generalization ability of the condensed graph on particular architectures by enhancing the uncertainty of node states. Meanwhile, the matching between optimal graph transport plan and representation transport plan is tactfully established to maintain semantic consistencies across source graph and condensed graph spaces, thereby freeing graph condensation from task dependencies. To further facilitate the adaptation of condensed graphs to various downstream tasks, a traceable semantic harmonizer from source nodes to condensed nodes is proposed to bridge semantic associations through the optimized representation transport plan in pre-training. Extensive experiments verify the superiority and versatility of PreGC, demonstrating its task-independent nature and seamless compatibility with arbitrary GNNs.         ",
    "url": "https://arxiv.org/abs/2509.14722",
    "authors": [
      "Yeyu Yan",
      "Shuai Zheng",
      "Wenjun Hui",
      "Xiangkai Zhu",
      "Dong Chen",
      "Zhenfeng Zhu",
      "Yao Zhao",
      "Kunlun He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14744",
    "title": "On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code",
    "abstract": "           Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write/execute the actual code with minimal human intervention. Key to this process are agent manifests, configuration files (such as this http URL) that provide agents with essential project context, identity, and operational rules. However, the lack of comprehensive and accessible documentation for creating these manifests presents a significant challenge for developers. We analyzed 253 this http URL files from 242 repositories to identify structural patterns and common content. Our findings show that manifests typically have shallow hierarchies with one main heading and several subsections, with content dominated by operational commands, technical implementation notes, and high-level architecture.         ",
    "url": "https://arxiv.org/abs/2509.14744",
    "authors": [
      "Worawalan Chatlatanagulchai",
      "Kundjanasith Thonglek",
      "Brittany Reid",
      "Yutaro Kashiwa",
      "Pattara Leelaprute",
      "Arnon Rungsawang",
      "Bundit Manaskasemsak",
      "Hajimu Iida"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.14750",
    "title": "Enhancing Retrieval Augmentation via Adversarial Collaboration",
    "abstract": "           Retrieval-augmented Generation (RAG) is a prevalent approach for domain-specific LLMs, yet it is often plagued by \"Retrieval Hallucinations\"--a phenomenon where fine-tuned models fail to recognize and act upon poor-quality retrieved documents, thus undermining performance. To address this, we propose the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two heterogeneous agents: a generalist Detector that identifies knowledge gaps, and a domain-specialized Resolver that provides precise solutions. Guided by a moderator, these agents engage in an adversarial collaboration, where the Detector's persistent questioning challenges the Resolver's expertise. This dynamic process allows for iterative problem dissection and refined knowledge retrieval. Extensive experiments show that AC-RAG significantly improves retrieval accuracy and outperforms state-of-the-art RAG methods across various vertical domains.         ",
    "url": "https://arxiv.org/abs/2509.14750",
    "authors": [
      "Letian Zhang",
      "Guanghao Meng",
      "Xudong Ren",
      "Yiming Wang",
      "Shu-Tao Xia"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14754",
    "title": "Variables Ordering Optimization in Boolean Characteristic Set Method Using Simulated Annealing and Machine Learning-based Time Prediction",
    "abstract": "           Solving systems of Boolean equations is a fundamental task in symbolic computation and algebraic cryptanalysis, with wide-ranging applications in cryptography, coding theory, and formal verification. Among existing approaches, the Boolean Characteristic Set (BCS) method[1] has emerged as one of the most efficient algorithms for tackling such problems. However, its performance is highly sensitive to the ordering of variables, with solving times varying drastically under different orderings for fixed variable counts n and equations size m. To address this challenge, this paper introduces a novel optimization framework that synergistically integrates machine learning (ML)-based time prediction with simulated annealing (SA) to efficiently identify high-performance variables orderings. Weconstruct a dataset comprising variable frequency spectrum X and corresponding BCS solving time t for benchmark systems(e.g., n = m = 28). Utilizing this data, we train an accurate ML predictor ft(X) to estimate solving time for any given variables ordering. For each target system, ft serves as the cost function within an SA algorithm, enabling rapid discovery of low-latency orderings that significantly expedite subsequent BCS execution. Extensive experiments demonstrate that our method substantially outperforms the standard BCS algorithm[1], Gr\u00f6bner basis method [2] and SAT solver[3], particularly for larger-scale systems(e.g., n = 32). Furthermore, we derive probabilistic time complexity bounds for the overall algorithm using stochastic process theory, establishing a quantitative relationship between predictor accuracy and expected solving complexity. This work provides both a practical acceleration tool for algebraic cryptanalysis and a theoretical foundation for ML-enhanced combinatorial optimization in symbolic computation.         ",
    "url": "https://arxiv.org/abs/2509.14754",
    "authors": [
      "Minzhong Luo",
      "Yudong Sun",
      "Yin Long"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.14755",
    "title": "Data Augmentation via Latent Diffusion Models for Detecting Smell-Related Objects in Historical Artworks",
    "abstract": "           Finding smell references in historic artworks is a challenging problem. Beyond artwork-specific challenges such as stylistic variations, their recognition demands exceptionally detailed annotation classes, resulting in annotation sparsity and extreme class imbalance. In this work, we explore the potential of synthetic data generation to alleviate these issues and enable accurate detection of smell-related objects. We evaluate several diffusion-based augmentation strategies and demonstrate that incorporating synthetic data into model training can improve detection performance. Our findings suggest that leveraging the large-scale pretraining of diffusion models offers a promising approach for improving detection accuracy, particularly in niche applications where annotations are scarce and costly to obtain. Furthermore, the proposed approach proves to be effective even with relatively small amounts of data, and scaling it up provides high potential for further enhancements.         ",
    "url": "https://arxiv.org/abs/2509.14755",
    "authors": [
      "Ahmed Sheta",
      "Mathias Zinnen",
      "Aline Sindel",
      "Andreas Maier",
      "Vincent Christlein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.14772",
    "title": "UMind: A Unified Multitask Network for Zero-Shot M/EEG Visual Decoding",
    "abstract": "           Decoding visual information from time-resolved brain recordings, such as EEG and MEG, plays a pivotal role in real-time brain-computer interfaces. However, existing approaches primarily focus on direct brain-image feature alignment and are limited to single-task frameworks or task-specific models. In this paper, we propose a Unified MultItask Network for zero-shot M/EEG visual Decoding (referred to UMind), including visual stimulus retrieval, classification, and reconstruction, where multiple tasks mutually enhance each other. Our method learns robust neural-visual and semantic representations through multimodal alignment with both image and text modalities. The integration of both coarse and fine-grained texts enhances the extraction of these neural representations, enabling more detailed semantic and visual decoding. These representations then serve as dual conditional inputs to a pre-trained diffusion model, guiding visual reconstruction from both visual and semantic perspectives. Extensive evaluations on MEG and EEG datasets demonstrate the effectiveness, robustness, and biological plausibility of our approach in capturing spatiotemporal neural dynamics. Our approach sets a multitask pipeline for brain visual decoding, highlighting the synergy of semantic information in visual feature extraction.         ",
    "url": "https://arxiv.org/abs/2509.14772",
    "authors": [
      "Chengjian Xu",
      "Yonghao Song",
      "Zelin Liao",
      "Haochuan Zhang",
      "Qiong Wang",
      "Qingqing Zheng"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2509.14773",
    "title": "A Real-Time Multi-Model Parametric Representation of Point Clouds",
    "abstract": "           In recent years, parametric representations of point clouds have been widely applied in tasks such as memory-efficient mapping and multi-robot collaboration. Highly adaptive models, like spline surfaces or quadrics, are computationally expensive in detection or fitting. In contrast, real-time methods, such as Gaussian mixture models or planes, have low degrees of freedom, making high accuracy with few primitives difficult. To tackle this problem, a multi-model parametric representation with real-time surface detection and fitting is proposed. Specifically, the Gaussian mixture model is first employed to segment the point cloud into multiple clusters. Then, flat clusters are selected and merged into planes or curved surfaces. Planes can be easily fitted and delimited by a 2D voxel-based boundary description method. Surfaces with curvature are fitted by B-spline surfaces and the same boundary description method is employed. Through evaluations on multiple public datasets, the proposed surface detection exhibits greater robustness than the state-of-the-art approach, with 3.78 times improvement in efficiency. Meanwhile, this representation achieves a 2-fold gain in accuracy over Gaussian mixture models, operating at 36.4 fps on a low-power onboard computer.         ",
    "url": "https://arxiv.org/abs/2509.14773",
    "authors": [
      "Yuan Gao",
      "Wei Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.14797",
    "title": "SINAI at eRisk@CLEF 2023: Approaching Early Detection of Gambling with Natural Language Processing",
    "abstract": "           This paper describes the participation of the SINAI team in the eRisk@CLEF lab. Specifically, one of the proposed tasks has been addressed: Task 2 on the early detection of signs of pathological gambling. The approach presented in Task 2 is based on pre-trained models from Transformers architecture with comprehensive preprocessing data and data balancing techniques. Moreover, we integrate Long-short Term Memory (LSTM) architecture with automodels from Transformers. In this Task, our team has been ranked in seventh position, with an F1 score of 0.126, out of 49 participant submissions and achieves the highest values in recall metrics and metrics related to early detection.         ",
    "url": "https://arxiv.org/abs/2509.14797",
    "authors": [
      "Alba Maria Marmol-Romero",
      "Flor Miriam Plaza-del-Arco",
      "Arturo Montejo-Raez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.14801",
    "title": "STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models",
    "abstract": "           While trajectory prediction plays a critical role in enabling safe and effective path-planning in automated vehicles, standardized practices for evaluating such models remain underdeveloped. Recent efforts have aimed to unify dataset formats and model interfaces for easier comparisons, yet existing frameworks often fall short in supporting heterogeneous traffic scenarios, joint prediction models, or user documentation. In this work, we introduce STEP -- a new benchmarking framework that addresses these limitations by providing a unified interface for multiple datasets, enforcing consistent training and evaluation conditions, and supporting a wide range of prediction models. We demonstrate the capabilities of STEP in a number of experiments which reveal 1) the limitations of widely-used testing procedures, 2) the importance of joint modeling of agents for better predictions of interactions, and 3) the vulnerability of current state-of-the-art models against both distribution shifts and targeted attacks by adversarial agents. With STEP, we aim to shift the focus from the ``leaderboard'' approach to deeper insights about model behavior and generalization in complex multi-agent settings.         ",
    "url": "https://arxiv.org/abs/2509.14801",
    "authors": [
      "Julian F. Schumann",
      "Anna M\u00e9sz\u00e1ros",
      "Jens Kober",
      "Arkady Zgonnikov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14806",
    "title": "SINAI at eRisk@CLEF 2022: Approaching Early Detection of Gambling and Eating Disorders with Natural Language Processing",
    "abstract": "           This paper describes the participation of the SINAI team in the eRisk@CLEF lab. Specifically, two of the proposed tasks have been addressed: i) Task 1 on the early detection of signs of pathological gambling, and ii) Task 3 on measuring the severity of the signs of eating disorders. The approach presented in Task 1 is based on the use of sentence embeddings from Transformers with features related to volumetry, lexical diversity, complexity metrics, and emotion-related scores, while the approach for Task 3 is based on text similarity estimation using contextualized word embeddings from Transformers. In Task 1, our team has been ranked in second position, with an F1 score of 0.808, out of 41 participant submissions. In Task 3, our team also placed second out of a total of 3 participating teams.         ",
    "url": "https://arxiv.org/abs/2509.14806",
    "authors": [
      "Alba Maria Marmol-Romero",
      "Salud Maria Jimenez-Zafra",
      "Flor Miriam Plaza-del-Arco",
      "M. Dolores Molina-Gonzalez",
      "Maria-Teresa Martin-Valdivia",
      "Arturo Montejo-Raez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.14821",
    "title": "Precision Neural Networks: Joint Graph And Relational Learning",
    "abstract": "           CoVariance Neural Networks (VNNs) perform convolutions on the graph determined by the covariance matrix of the data, which enables expressive and stable covariance-based learning. However, covariance matrices are typically dense, fail to encode conditional independence, and are often precomputed in a task-agnostic way, which may hinder performance. To overcome these limitations, we study Precision Neural Networks (PNNs), i.e., VNNs on the precision matrix -- the inverse covariance. The precision matrix naturally encodes statistical independence, often exhibits sparsity, and preserves the covariance spectral structure. To make precision estimation task-aware, we formulate an optimization problem that jointly learns the network parameters and the precision matrix, and solve it via alternating optimization, by sequentially updating the network weights and the precision estimate. We theoretically bound the distance between the estimated and true precision matrices at each iteration, and demonstrate the effectiveness of joint estimation compared to two-step approaches on synthetic and real-world data.         ",
    "url": "https://arxiv.org/abs/2509.14821",
    "authors": [
      "Andrea Cavallo",
      "Samuel Rey",
      "Antonio G. Marques",
      "Elvin Isufi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14829",
    "title": "RulER: Automated Rule-Based Semantic Error Localization and Repair for Code Translation",
    "abstract": "           Automated code translation aims to convert programs between different programming languages while maintaining their functionality. Due to the imperfections of code translation models, the generated translations may contain errors that compromise their reliability. Existing automated debugging methods for code translation rely on code alignments and repair patch templates to locate and fix erroneous translations. However, existing methods lack reliable references to construct code alignments and design repair patch templates, which significantly impacts their localization accuracy and repair effectiveness. To address these limitations, we reintroduce code translation rules and propose a rule-based debugging method for code translation, called RulER. RulER automatically derives code translation rules from correct translations generated by LLMs, enabling the efficient collection of diverse translation rules. In addition, RulER dynamically combines the existing rules on expandable nodes like expressions and tokens to further adaptively align more statements. These rules capture clear and detailed structural correspondences between source and target programming languages. Therefore, they can serve as reliable and reusable references for code alignment and repair template design, enabling RulER to locate and fix translation errors effectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++ translations produced by four code translation models demonstrates that RulER outperforms state-of-the-art methods, BatFix and TransMap. Our experimental results show that RulER outperformed the best baseline by 20% and 272% in terms of error localization rates and repair success rates, respectively. RulER exhibits superior repair performance compared to directly prompting LLMs for patch generation, demonstrating a promising methodology for extracting and leveraging coding knowledge from LLMs.         ",
    "url": "https://arxiv.org/abs/2509.14829",
    "authors": [
      "Shuo Jin",
      "Songqiang Chen",
      "Xiaoyuan Xie",
      "Shing-Chi Cheung"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.14832",
    "title": "Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization",
    "abstract": "           Stochastic forecasting is critical for efficient decision-making in uncertain systems, such as energy markets and finance, where estimating the full distribution of future scenarios is essential. We propose Diffusion Scenario Tree (DST), a general framework for constructing scenario trees for multivariate prediction tasks using diffusion-based probabilistic forecasting models. DST recursively samples future trajectories and organizes them into a tree via clustering, ensuring non-anticipativity (decisions depending only on observed history) at each stage. We evaluate the framework on the optimization task of energy arbitrage in New York State's day-ahead electricity market. Experimental results show that our approach consistently outperforms the same optimization algorithms that use scenario trees from more conventional models and Model-Free Reinforcement Learning baselines. Furthermore, using DST for stochastic optimization yields more efficient decision policies, achieving higher performance by better handling uncertainty than deterministic and stochastic MPC variants using the same diffusion-based forecaster.         ",
    "url": "https://arxiv.org/abs/2509.14832",
    "authors": [
      "Stelios Zarifis",
      "Ioannis Kordonis",
      "Petros Maragos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.14837",
    "title": "V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models",
    "abstract": "           Recent advances in causal interpretability have extended from language models to vision-language models (VLMs), seeking to reveal their internal mechanisms through input interventions. While textual interventions often target semantics, visual interventions typically rely on coarse pixel-level perturbations, limiting semantic insights on multimodal integration. In this study, we introduce V-SEAM, a novel framework that combines Visual Semantic Editing and Attention Modulating for causal interpretation of VLMs. V-SEAM enables concept-level visual manipulations and identifies attention heads with positive or negative contributions to predictions across three semantic levels: objects, attributes, and relationships. We observe that positive heads are often shared within the same semantic level but vary across levels, while negative heads tend to generalize broadly. Finally, we introduce an automatic method to modulate key head embeddings, demonstrating enhanced performance for both LLaVA and InstructBLIP across three diverse VQA benchmarks. Our data and code are released at: this https URL.         ",
    "url": "https://arxiv.org/abs/2509.14837",
    "authors": [
      "Qidong Wang",
      "Junjie Hu",
      "Ming Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.14856",
    "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects",
    "abstract": "           Automated code review (CR) is a key application for Large Language Models (LLMs), but progress is hampered by a \"reality gap\": existing benchmarks evaluate models on isolated sub-tasks using simplified, context-poor data. This fails to reflect the holistic context-rich nature of real-world CR. To bridge this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601 high-quality instances from 70 Python projects covering nine Pull-Request (PR) problem domains, where each instance provides rich, multi-faceted context including the associated issue, PR details, and repository state, enabling end-to-end evaluation. Beyond superficial metrics, we also propose a novel evaluation framework that combines rule-based checks for location and syntax with model-based judgments of review quality. We present the first large-scale assessment of state-of-the-art LLMs on this comprehensive CR task. Our results establish crucial baselines and reveal that (1) no single LLM dominates all aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive performance; and (3) different LLMs exhibit varying robustness to redundant context. These findings highlight the necessity of holistic, multi-dimensional evaluation and provide actionable insights for advancing truly intelligent yet practical CR assistants.         ",
    "url": "https://arxiv.org/abs/2509.14856",
    "authors": [
      "Hanyang Guo",
      "Xunjin Zheng",
      "Zihan Liao",
      "Hang Yu",
      "Peng DI",
      "Ziyin Zhang",
      "Hong-Ning Dai"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.14863",
    "title": "Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study",
    "abstract": "           Graph Transformers (GTs) show considerable potential in graph representation learning. The architecture of GTs typically integrates Graph Neural Networks (GNNs) with global attention mechanisms either in parallel or as a precursor to attention mechanisms, yielding a local-and-global or local-to-global attention scheme. However, as the global attention mechanism primarily captures long-range dependencies between nodes, these integration schemes may suffer from information loss, where the local neighborhood information learned by GNN could be diluted by the attention mechanism. Therefore, we propose G2LFormer, featuring a novel global-to-local attention scheme where the shallow network layers use attention mechanisms to capture global information, while the deeper layers employ GNN modules to learn local structural information, thereby preventing nodes from ignoring their immediate neighbors. An effective cross-layer information fusion strategy is introduced to allow local layers to retain beneficial information from global layers and alleviate information loss, with acceptable trade-offs in scalability. To validate the feasibility of the global-to-local attention scheme, we compare G2LFormer with state-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The results indicate that G2LFormer exhibits excellent performance while keeping linear complexity.         ",
    "url": "https://arxiv.org/abs/2509.14863",
    "authors": [
      "Zhengwei Wang",
      "Gang Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14864",
    "title": "A cell centered Galerkin method for miscible displacement in heterogeneous porous media",
    "abstract": "           In this paper we present a cell centered Galerkin (CCG) method applied to miscible displacement problems in heterogeneous porous media. The CCG approach combines concepts from finite volume and discontinuous Galerkin (DG) methods to arrive at an efficient lowest-order approximation (one unknown per cell). We demonstrate that the CCG method can be defined using classical DG weak formulations, only requires one unknown per cell, and is able to deliver comparable accuracy and improved efficiency over traditional higher-order interior penalty DG methods. In addition, we prove that the CCG method for a model Poisson problem gives rise to a inverse-positive matrix in 1D. A plethora of computational experiments in 2D and 3D showcase the effectiveness of the CCG method for highly heterogeneous flow and transport problems in porous media. Comparisons between CCG and classical DG methods are included.         ",
    "url": "https://arxiv.org/abs/2509.14864",
    "authors": [
      "Maurice S. Fabien"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2509.14868",
    "title": "DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting",
    "abstract": "           We conducted rigorous ablation studies to validate DPANet's key components (Table \\ref{tab:ablation-study}). The full model consistently outperforms all variants. To test our dual-domain hypothesis, we designed two specialized versions: a Temporal-Only model (fusing two identical temporal pyramids) and a Frequency-Only model (fusing two spectral pyramids). Both variants underperformed significantly, confirming that the fusion of heterogeneous temporal and frequency information is critical. Furthermore, replacing the cross-attention mechanism with a simpler method (w/o Cross-Fusion) caused the most severe performance degradation. This result underscores that our interactive fusion block is the most essential component.         ",
    "url": "https://arxiv.org/abs/2509.14868",
    "authors": [
      "Qianyang Li",
      "Xingjun Zhang",
      "Shaoxun Wang",
      "Jia Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14872",
    "title": "Temporal Representation Learning of Phenotype Trajectories for pCR Prediction in Breast Cancer",
    "abstract": "           Effective therapy decisions require models that predict the individual response to treatment. This is challenging since the progression of disease and response to treatment vary substantially across patients. Here, we propose to learn a representation of the early dynamics of treatment response from imaging data to predict pathological complete response (pCR) in breast cancer patients undergoing neoadjuvant chemotherapy (NACT). The longitudinal change in magnetic resonance imaging (MRI) data of the breast forms trajectories in the latent space, serving as basis for prediction of successful response. The multi-task model represents appearance, fosters temporal continuity and accounts for the comparably high heterogeneity in the non-responder this http URL experiments on the publicly available ISPY-2 dataset, a linear classifier in the latent trajectory space achieves a balanced accuracy of 0.761 using only pre-treatment data (T0), 0.811 using early response (T0 + T1), and 0.861 using four imaging time points (T0 -> T3). The code will be made available upon paper acceptance.         ",
    "url": "https://arxiv.org/abs/2509.14872",
    "authors": [
      "Ivana Jan\u00ed\u010dkov\u00e1",
      "Yen Y. Tan",
      "Thomas H. Helbich",
      "Konstantin Miloserdov",
      "Zsuzsanna Bago-Horvath",
      "Ulrike Heber",
      "Georg Langs"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.14883",
    "title": "Robust and Secure Computation Offloading and Trajectory Optimization for Multi-UAV MEC Against Aerial Eavesdropper",
    "abstract": "           The unmanned aerial vehicle (UAV) based multi-access edge computing (MEC) appears as a popular paradigm to reduce task processing latency. However, the secure offloading is an important issue when occurring aerial eavesdropping. Besides, the potential uncertainties in practical applications and flexible trajectory optimizations of UAVs pose formidable challenges for realizing robust offloading. In this paper, we consider the aerial secure MEC network including ground users, service unmanned aerial vehicles (S-UAVs) integrated with edge servers, and malicious UAVs overhearing transmission links. To deal with the task computation complexities, which are characterized as uncertainties, a robust problem is formulated with chance constraints. The energy cost is minimized by optimizing the connections, trajectories of S-UAVs and offloading ratios. Then, the proposed non-linear problem is tackled via the distributionally robust optimization and conditional value-at-risk mechanism, which is further transformed into the second order cone programming forms. Moreover, we decouple the reformulated problem and design the successive convex approximation for S-UAV trajectories. The global algorithm is designed to solve the sub-problems in a block coordinate decent manner. Finally, extensive simulations and numerical analyses are conducted to verify the robustness of the proposed algorithms, with just 2\\% more energy cost compared with the ideal circumstance.         ",
    "url": "https://arxiv.org/abs/2509.14883",
    "authors": [
      "Can Cui",
      "Ziye Jia",
      "Jiahao You",
      "Chao Dong",
      "Qihui Wu",
      "Han Zhu"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2509.14887",
    "title": "Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis",
    "abstract": "           Learning the graph underlying a networked system from nodal signals is crucial to downstream tasks in graph signal processing and machine learning. The presence of hidden nodes whose signals are not observable might corrupt the estimated graph. While existing works proposed various robustifications of vanilla graph learning objectives by explicitly accounting for the presence of these hidden nodes, a robustness analysis of \"naive\", hidden-node agnostic approaches is still underexplored. This work demonstrates that vanilla graph topology learning methods are implicitly robust to partial observations of low-pass filtered graph signals. We achieve this theoretical result through extending the restricted isometry property (RIP) to the Dirichlet energy function used in graph learning objectives. We show that smoothness-based graph learning formulation (e.g., the GL-SigRep method) on partial observations can recover the ground truth graph topology corresponding to the observed nodes. Synthetic and real data experiments corroborate our findings.         ",
    "url": "https://arxiv.org/abs/2509.14887",
    "authors": [
      "Hoang-Son Nguyen",
      "Hoi-To Wai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2509.14893",
    "title": "Temporally Heterogeneous Graph Contrastive Learning for Multimodal Acoustic event Classification",
    "abstract": "           Multimodal acoustic event classification plays a key role in audio-visual systems. Although combining audio and visual signals improves recognition, it is still difficult to align them over time and to reduce the effect of noise across modalities. Existing methods often treat audio and visual streams separately, fusing features later with contrastive or mutual information objectives. Recent advances explore multimodal graph learning, but most fail to distinguish between intra- and inter-modal temporal dependencies. To address this, we propose Temporally Heterogeneous Graph-based Contrastive Learning (THGCL). Our framework constructs a temporal graph for each event, where audio and video segments form nodes and their temporal links form edges. We introduce Gaussian processes for intra-modal smoothness, Hawkes processes for inter-modal decay, and contrastive learning to capture fine-grained relationships. Experiments on AudioSet show that THGCL achieves state-of-the-art performance.         ",
    "url": "https://arxiv.org/abs/2509.14893",
    "authors": [
      "Yuanjian Chen",
      "Yang Xiao",
      "Jinjie Huang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2509.14904",
    "title": "Robust Barycenters of Persistence Diagrams",
    "abstract": "           This short paper presents a general approach for computing robust Wasserstein barycenters of persistence diagrams. The classical method consists in computing assignment arithmetic means after finding the optimal transport plans between the barycenter and the persistence diagrams. However, this procedure only works for the transportation cost related to the $q$-Wasserstein distance $W_q$ when $q=2$. We adapt an alternative fixed-point method to compute a barycenter diagram for generic transportation costs ($q > 1$), in particular those robust to outliers, $q \\in (1,2)$. We show the utility of our work in two applications: \\emph{(i)} the clustering of persistence diagrams on their metric space and \\emph{(ii)} the dictionary encoding of persistence diagrams. In both scenarios, we demonstrate the added robustness to outliers provided by our generalized framework. Our Python implementation is available at this address: this https URL .         ",
    "url": "https://arxiv.org/abs/2509.14904",
    "authors": [
      "Keanu Sisouk",
      "Eloi Tanguy",
      "Julie Delon",
      "Julien Tierny"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2509.14922",
    "title": "A Comparative Evaluation of Large Language Models for Persian Sentiment Analysis and Emotion Detection in Social Media Texts",
    "abstract": "           This study presents a comprehensive comparative evaluation of four state-of-the-art Large Language Models (LLMs)--Claude 3.7 Sonnet, DeepSeek-V3, Gemini 2.0 Flash, and GPT-4o--for sentiment analysis and emotion detection in Persian social media texts. Comparative analysis among LLMs has witnessed a significant rise in recent years, however, most of these analyses have been conducted on English language tasks, creating gaps in understanding cross-linguistic performance patterns. This research addresses these gaps through rigorous experimental design using balanced Persian datasets containing 900 texts for sentiment analysis (positive, negative, neutral) and 1,800 texts for emotion detection (anger, fear, happiness, hate, sadness, surprise). The main focus was to allow for a direct and fair comparison among different models, by using consistent prompts, uniform processing parameters, and by analyzing the performance metrics such as precision, recall, F1-scores, along with misclassification patterns. The results show that all models reach an acceptable level of performance, and a statistical comparison of the best three models indicates no significant differences among them. However, GPT-4o demonstrated a marginally higher raw accuracy value for both tasks, while Gemini 2.0 Flash proved to be the most cost-efficient. The findings indicate that the emotion detection task is more challenging for all models compared to the sentiment analysis task, and the misclassification patterns can represent some challenges in Persian language texts. These findings establish performance benchmarks for Persian NLP applications and offer practical guidance for model selection based on accuracy, efficiency, and cost considerations, while revealing cultural and linguistic challenges that require consideration in multilingual AI system deployment.         ",
    "url": "https://arxiv.org/abs/2509.14922",
    "authors": [
      "Kian Tohidi",
      "Kia Dashtipour",
      "Simone Rebora",
      "Sevda Pourfaramarz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.14925",
    "title": "Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation",
    "abstract": "           Reinforcement Learning (RL) methods that incorporate deep neural networks (DNN), though powerful, often lack transparency. Their black-box characteristic hinders interpretability and reduces trustworthiness, particularly in critical domains. To address this challenge in RL tasks, we propose a solution based on Self-Explaining Neural Networks (SENNs) along with explanation extraction methods to enhance interpretability while maintaining predictive accuracy. Our approach targets low-dimensionality problems to generate robust local and global explanations of the model's behaviour. We evaluate the proposed method on the resource allocation problem in mobile networks, demonstrating that SENNs can constitute interpretable solutions with competitive performance. This work highlights the potential of SENNs to improve transparency and trust in AI-driven decision-making for low-dimensional tasks. Our approach strong performance on par with the existing state-of-the-art methods, while providing robust explanations.         ",
    "url": "https://arxiv.org/abs/2509.14925",
    "authors": [
      "Konrad Nowosadko",
      "Franco Ruggeri",
      "Ahmad Terra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2509.14933",
    "title": "DAG: A Dual Causal Network for Time Series Forecasting with Exogenous Variables",
    "abstract": "           Time series forecasting is crucial in various fields such as economics, traffic, and AIOps. However, in real-world applications, focusing solely on the endogenous variables (i.e., target variables), is often insufficient to ensure accurate predictions. Considering exogenous variables (i.e., covariates) provides additional predictive information, thereby improving forecasting accuracy. However, existing methods for time series forecasting with exogenous variables (TSF-X) have the following shortcomings: 1) they do not leverage future exogenous variables, 2) they fail to account for the causal relationships between endogenous and exogenous variables. As a result, their performance is suboptimal. In this study, to better leverage exogenous variables, especially future exogenous variable, we propose a general framework DAG, which utilizes dual causal network along both the temporal and channel dimensions for time series forecasting with exogenous variables. Specifically, we first introduce the Temporal Causal Module, which includes a causal discovery module to capture how historical exogenous variables affect future exogenous variables. Following this, we construct a causal injection module that incorporates the discovered causal relationships into the process of forecasting future endogenous variables based on historical endogenous variables. Next, we propose the Channel Causal Module, which follows a similar design principle. It features a causal discovery module models how historical exogenous variables influence historical endogenous variables, and a causal injection module incorporates the discovered relationships to enhance the prediction of future endogenous variables based on future exogenous variables.         ",
    "url": "https://arxiv.org/abs/2509.14933",
    "authors": [
      "Xiangfei Qiu",
      "Yuhan Zhu",
      "Zhengyu Li",
      "Hanyin Cheng",
      "Xingjian Wu",
      "Chenjuan Guo",
      "Bin Yang",
      "Jilin Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14936",
    "title": "A Comparative Analysis of Transformer Models in Social Bot Detection",
    "abstract": "           Social media has become a key medium of communication in today's society. This realisation has led to many parties employing artificial users (or bots) to mislead others into believing untruths or acting in a beneficial manner to such parties. Sophisticated text generation tools, such as large language models, have further exacerbated this issue. This paper aims to compare the effectiveness of bot detection models based on encoder and decoder transformers. Pipelines are developed to evaluate the performance of these classifiers, revealing that encoder-based classifiers demonstrate greater accuracy and robustness. However, decoder-based models showed greater adaptability through task-specific alignment, suggesting more potential for generalisation across different use cases in addition to superior observa. These findings contribute to the ongoing effort to prevent digital environments being manipulated while protecting the integrity of online discussion.         ",
    "url": "https://arxiv.org/abs/2509.14936",
    "authors": [
      "Rohan Veit",
      "Michael Lones"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14938",
    "title": "Hierarchical Federated Learning for Social Network with Mobility",
    "abstract": "           Federated Learning (FL) offers a decentralized solution that allows collaborative local model training and global aggregation, thereby protecting data privacy. In conventional FL frameworks, data privacy is typically preserved under the assumption that local data remains absolutely private, whereas the mobility of clients is frequently neglected in explicit modeling. In this paper, we propose a hierarchical federated learning framework based on the social network with mobility namely HFL-SNM that considers both data sharing among clients and their mobility patterns. Under the constraints of limited resources, we formulate a joint optimization problem of resource allocation and client scheduling, which objective is to minimize the energy consumption of clients during the FL process. In social network, we introduce the concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate. We analyze the impact of effective data and redundant data on the model performance through preliminary experiments. We decouple the optimization problem into multiple sub-problems, analyze them based on preliminary experimental results, and propose Dynamic Optimization in Social Network with Mobility (DO-SNM) algorithm. Experimental results demonstrate that our algorithm achieves superior model performance while significantly reducing energy consumption, compared to traditional baseline algorithms.         ",
    "url": "https://arxiv.org/abs/2509.14938",
    "authors": [
      "Zeyu Chen",
      "Wen Chen",
      "Jun Li",
      "Qingqing Wu",
      "Ming Ding",
      "Xuefeng Han",
      "Xiumei Deng",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14945",
    "title": "Data-Driven Prediction of Maternal Nutritional Status in Ethiopia Using Ensemble Machine Learning Models",
    "abstract": "           Malnutrition among pregnant women is a major public health challenge in Ethiopia, increasing the risk of adverse maternal and neonatal outcomes. Traditional statistical approaches often fail to capture the complex and multidimensional determinants of nutritional status. This study develops a predictive model using ensemble machine learning techniques, leveraging data from the Ethiopian Demographic and Health Survey (2005-2020), comprising 18,108 records with 30 socio-demographic and health attributes. Data preprocessing included handling missing values, normalization, and balancing with SMOTE, followed by feature selection to identify key predictors. Several supervised ensemble algorithms including XGBoost, Random Forest, CatBoost, and AdaBoost were applied to classify nutritional status. Among them, the Random Forest model achieved the best performance, classifying women into four categories (normal, moderate malnutrition, severe malnutrition, and overnutrition) with 97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86% ROC AUC. These findings demonstrate the effectiveness of ensemble learning in capturing hidden patterns from complex datasets and provide timely insights for early detection of nutritional risks. The results offer practical implications for healthcare providers, policymakers, and researchers, supporting data-driven strategies to improve maternal nutrition and health outcomes in Ethiopia.         ",
    "url": "https://arxiv.org/abs/2509.14945",
    "authors": [
      "Amsalu Tessema",
      "Tizazu Bayih",
      "Kassahun Azezew",
      "Ayenew Kassie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14957",
    "title": "DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection",
    "abstract": "           With the increasing prevalence of synthetic images, evaluating image authenticity and locating forgeries accurately while maintaining human interpretability remains a challenging task. Existing detection models primarily focus on simple authenticity classification, ultimately providing only a forgery probability or binary judgment, which offers limited explanatory insights into image authenticity. Moreover, while MLLM-based detection methods can provide more interpretable results, they still lag behind expert models in terms of pure authenticity classification accuracy. To address this, we propose DF-LLaVA, a simple yet effective framework that unlocks the intrinsic discrimination potential of MLLMs. Our approach first extracts latent knowledge from MLLMs and then injects it into training via prompts. This framework allows LLaVA to achieve outstanding detection accuracy exceeding expert models while still maintaining the interpretability offered by MLLMs. Extensive experiments confirm the superiority of our DF-LLaVA, achieving both high accuracy and explainability in synthetic image detection. Code is available online at: this https URL.         ",
    "url": "https://arxiv.org/abs/2509.14957",
    "authors": [
      "Zhuokang Shen",
      "Kaisen Zhang",
      "Bohan Jia",
      "Yuan Fang",
      "Zhou Yu",
      "Shaohui Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.14965",
    "title": "Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis",
    "abstract": "           Functional magnetic resonance imaging (fMRI) provides a powerful non-invasive window into the brain's functional organization by generating complex functional networks, typically modeled as graphs. These brain networks exhibit a hierarchical topology that is crucial for cognitive processing. However, due to inherent spatial constraints, standard Euclidean GNNs struggle to represent these hierarchical structures without high distortion, limiting their clinical performance. To address this limitation, we propose Brain-HGCN, a geometric deep learning framework based on hyperbolic geometry, which leverages the intrinsic property of negatively curved space to model the brain's network hierarchy with high fidelity. Grounded in the Lorentz model, our model employs a novel hyperbolic graph attention layer with a signed aggregation mechanism to distinctly process excitatory and inhibitory connections, ultimately learning robust graph-level representations via a geometrically sound Fr\u00e9chet mean for graph readout. Experiments on two large-scale fMRI datasets for psychiatric disorder classification demonstrate that our approach significantly outperforms a wide range of state-of-the-art Euclidean baselines. This work pioneers a new geometric deep learning paradigm for fMRI analysis, highlighting the immense potential of hyperbolic GNNs in the field of computational psychiatry.         ",
    "url": "https://arxiv.org/abs/2509.14965",
    "authors": [
      "Junhao Jia",
      "Yunyou Liu",
      "Cheng Yang",
      "Yifei Sun",
      "Feiwei Qin",
      "Changmiao Wang",
      "Yong Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.14968",
    "title": "FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference",
    "abstract": "           The upcoming generations of wireless technologies promise an era where everything is interconnected and intelligent. As the need for intelligence grows, networks must learn to better understand the physical world. However, deploying dedicated hardware to perceive the environment is not always feasible, mainly due to costs and/or complexity. Integrated Sensing and Communication (ISAC) has made a step forward in addressing this challenge. Within ISAC, passive sensing emerges as a cost-effective solution that reuses wireless communications to sense the environment, without interfering with existing communications. Nevertheless, the majority of current solutions are limited to one technology (mostly Wi-Fi or 5G), constraining the maximum accuracy reachable. As different technologies work with different spectrums, we see a necessity in integrating more than one technology to augment the coverage area. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a MultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference. FAWN is based on the original transformers architecture, to fuse information from Wi-Fi and 5G, making the network capable of understanding the physical world without interfering with the current communication. To test our solution, we have built a prototype and integrated it in a real scenario. Results show errors below 0.6 m around 84% of times.         ",
    "url": "https://arxiv.org/abs/2509.14968",
    "authors": [
      "Carlos Barroso-Fern\u00e1ndez",
      "Alejandro Calvillo-Fernandez",
      "Antonio de la Oliva",
      "Carlos J. Bernardos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2509.14980",
    "title": "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation",
    "abstract": "           Mobile manipulation requires the coordinated control of a mobile base and a robotic arm while simultaneously perceiving both global scene context and fine-grained object details. Existing single-view approaches often fail in unstructured environments due to limited fields of view, exploration, and generalization abilities. Moreover, classical controllers, although stable, struggle with efficiency and manipulability near singularities. To address these challenges, we propose M4Diffuser, a hybrid framework that integrates a Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP (ReM-QP) controller for mobile manipulation. The diffusion policy leverages proprioceptive states and complementary camera perspectives with both close-range object details and global scene context to generate task-relevant end-effector goals in the world frame. These high-level goals are then executed by the ReM-QP controller, which eliminates slack variables for computational efficiency and incorporates manipulability-aware preferences for robustness near singularities. Comprehensive experiments in simulation and real-world environments show that M4Diffuser achieves 7 to 56 percent higher success rates and reduces collisions by 3 to 31 percent over baselines. Our approach demonstrates robust performance for smooth whole-body coordination, and strong generalization to unseen tasks, paving the way for reliable mobile manipulation in unstructured environments. Details of the demo and supplemental material are available on our project website this https URL.         ",
    "url": "https://arxiv.org/abs/2509.14980",
    "authors": [
      "Ju Dong",
      "Lei Zhang",
      "Liding Zhang",
      "Yao Ling",
      "Yu Fu",
      "Kaixin Bai",
      "Zolt\u00e1n-Csaba M\u00e1rton",
      "Zhenshan Bing",
      "Zhaopeng Chen",
      "Alois Christian Knoll",
      "Jianwei Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.14989",
    "title": "UCorr: Wire Detection and Depth Estimation for Autonomous Drones",
    "abstract": "           In the realm of fully autonomous drones, the accurate detection of obstacles is paramount to ensure safe navigation and prevent collisions. Among these challenges, the detection of wires stands out due to their slender profile, which poses a unique and intricate problem. To address this issue, we present an innovative solution in the form of a monocular end-to-end model for wire segmentation and depth estimation. Our approach leverages a temporal correlation layer trained on synthetic data, providing the model with the ability to effectively tackle the complex joint task of wire detection and depth estimation. We demonstrate the superiority of our proposed method over existing competitive approaches in the joint task of wire detection and depth estimation. Our results underscore the potential of our model to enhance the safety and precision of autonomous drones, shedding light on its promising applications in real-world scenarios.         ",
    "url": "https://arxiv.org/abs/2509.14989",
    "authors": [
      "Benedikt Kolbeinsson",
      "Krystian Mikolajczyk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.14999",
    "title": "Semantic-LiDAR-Inertial-Wheel Odometry Fusion for Robust Localization in Large-Scale Dynamic Environments",
    "abstract": "           Reliable, drift-free global localization presents significant challenges yet remains crucial for autonomous navigation in large-scale dynamic environments. In this paper, we introduce a tightly-coupled Semantic-LiDAR-Inertial-Wheel Odometry fusion framework, which is specifically designed to provide high-precision state estimation and robust localization in large-scale dynamic environments. Our framework leverages an efficient semantic-voxel map representation and employs an improved scan matching algorithm, which utilizes global semantic information to significantly reduce long-term trajectory drift. Furthermore, it seamlessly fuses data from LiDAR, IMU, and wheel odometry using a tightly-coupled multi-sensor fusion Iterative Error-State Kalman Filter (iESKF). This ensures reliable localization without experiencing abnormal drift. Moreover, to tackle the challenges posed by terrain variations and dynamic movements, we introduce a 3D adaptive scaling strategy that allows for flexible adjustments to wheel odometry measurement weights, thereby enhancing localization precision. This study presents extensive real-world experiments conducted in a one-million-square-meter automated port, encompassing 3,575 hours of operational data from 35 Intelligent Guided Vehicles (IGVs). The results consistently demonstrate that our system outperforms state-of-the-art LiDAR-based localization methods in large-scale dynamic environments, highlighting the framework's reliability and practical value.         ",
    "url": "https://arxiv.org/abs/2509.14999",
    "authors": [
      "Haoxuan Jiang",
      "Peicong Qian",
      "Yusen Xie",
      "Linwei Zheng",
      "Xiaocong Li",
      "Ming Liu",
      "Jun Ma"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.15024",
    "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering",
    "abstract": "           Attention mechanisms have become a cornerstone in modern neural networks, driving breakthroughs across diverse domains. However, their application to graph structured data, where capturing topological connections is essential, remains underexplored and underperforming compared to Graph Neural Networks (GNNs), particularly in the graph clustering task. GNN tends to overemphasize neighborhood aggregation, leading to a homogenization of node representations. Conversely, Transformer tends to over globalize, highlighting distant nodes at the expense of meaningful local patterns. This dichotomy raises a key question: Is attention inherently redundant for unsupervised graph learning? To address this, we conduct a comprehensive empirical analysis, uncovering the complementary weaknesses of GNN and Transformer in graph clustering. Motivated by these insights, we propose the Attentive Graph Clustering Network (AGCN) a novel architecture that reinterprets the notion that graph is attention. AGCN directly embeds the attention mechanism into the graph structure, enabling effective global information extraction while maintaining sensitivity to local topological cues. Our framework incorporates theoretical analysis to contrast AGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV cache mechanism to improve computational efficiency, and (2) a pairwise margin contrastive loss to boost the discriminative capacity of the attention space. Extensive experimental results demonstrate that AGCN outperforms state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2509.15024",
    "authors": [
      "Xuanting Xie",
      "Bingheng Li",
      "Erlin Pan",
      "Rui Hou",
      "Wenyu Chen",
      "Zhao Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2509.15033",
    "title": "Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection",
    "abstract": "           In this paper, we aim to improve multivariate anomaly detection (AD) by modeling the \\textit{time-varying non-linear spatio-temporal correlations} found in multivariate time series data . In multivariate time series data, an anomaly may be indicated by the simultaneous deviation of interrelated time series from their expected collective behavior, even when no individual time series exhibits a clearly abnormal pattern on its own. In many existing approaches, time series variables are assumed to be (conditionally) independent, which oversimplifies real-world interactions. Our approach addresses this by modeling joint dependencies in the latent space and decoupling the modeling of \\textit{marginal distributions, temporal dynamics, and inter-variable dependencies}. We use a transformer encoder to capture temporal patterns, and to model spatial (inter-variable) dependencies, we fit a multi-variate likelihood and a copula. The temporal and the spatial components are trained jointly in a latent space using a self-supervised contrastive learning objective to learn meaningful feature representations to separate normal and anomaly samples.         ",
    "url": "https://arxiv.org/abs/2509.15033",
    "authors": [
      "Padmaksha Roy",
      "Almuatazbellah Boker",
      "Lamine Mili"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.15044",
    "title": "Credit Card Fraud Detection",
    "abstract": "           Credit card fraud remains a significant challenge due to class imbalance and fraudsters mimicking legitimate behavior. This study evaluates five machine learning models - Logistic Regression, Random Forest, XGBoost, K-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using undersampling, SMOTE, and a hybrid approach. Our models are evaluated on the original imbalanced test set to better reflect real-world performance. Results show that the hybrid method achieves the best balance between recall and precision, especially improving MLP and KNN performance.         ",
    "url": "https://arxiv.org/abs/2509.15044",
    "authors": [
      "Iva Popova",
      "Hamza A. A. Gardi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.15045",
    "title": "Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies",
    "abstract": "           This paper addresses the synthetic-to-real domain gap in object detection, focusing on training a YOLOv11 model to detect a specific object (a soup can) using only synthetic data and domain randomization strategies. The methodology involves extensive experimentation with data augmentation, dataset composition, and model scaling. While synthetic validation metrics were consistently high, they proved to be poor predictors of real-world performance. Consequently, models were also evaluated qualitatively, through visual inspection of predictions, and quantitatively, on a manually labeled real-world test set, to guide development. Final mAP@50 scores were provided by the official Kaggle competition. Key findings indicate that increasing synthetic dataset diversity, specifically by including varied perspectives and complex backgrounds, combined with carefully tuned data augmentation, were crucial in bridging the domain gap. The best performing configuration, a YOLOv11l model trained on an expanded and diverse dataset, achieved a final mAP@50 of 0.910 on the competition's hidden test set. This result demonstrates the potential of a synthetic-only training approach while also highlighting the remaining challenges in fully capturing real-world variability.         ",
    "url": "https://arxiv.org/abs/2509.15045",
    "authors": [
      "Luisa Torquato Ni\u00f1o",
      "Hamza A. A. Gardi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.15047",
    "title": "Distributed Batch Matrix Multiplication: Trade-Offs in Download Rate, Randomness, and Privacy",
    "abstract": "           We study the trade-off between communication rate and privacy for distributed batch matrix multiplication of two independent sequences of matrices $\\bold{A}$ and $\\bold{B}$ with uniformly distributed entries. In our setting, $\\bold{B}$ is publicly accessible by all the servers while $\\bold{A}$ must remain private. A user is interested in evaluating the product $\\bold{AB}$ with the responses from the $k$ fastest servers. For a given parameter $\\alpha \\in [0, 1]$, our privacy constraint must ensure that any set of $\\ell$ colluding servers cannot learn more than a fraction $\\alpha$ of $\\bold{A}$. Additionally, we study the trade-off between the amount of local randomness needed at the encoder and privacy. Finally, we establish the optimal trade-offs when the matrices are square and identify a linear relationship between information leakage and communication rate.         ",
    "url": "https://arxiv.org/abs/2509.15047",
    "authors": [
      "Amirhosein Morteza",
      "Remi A. Chou"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.15072",
    "title": "Improving Internet Traffic Matrix Prediction via Time Series Clustering",
    "abstract": "           We present a novel framework that leverages time series clustering to improve internet traffic matrix (TM) prediction using deep learning (DL) models. Traffic flows within a TM often exhibit diverse temporal behaviors, which can hinder prediction accuracy when training a single model across all flows. To address this, we propose two clustering strategies, source clustering and histogram clustering, that group flows with similar temporal patterns prior to model training. Clustering creates more homogeneous data subsets, enabling models to capture underlying patterns more effectively and generalize better than global prediction approaches that fit a single model to the entire TM. Compared to existing TM prediction methods, our method reduces RMSE by up to 92\\% for Abilene and 75\\% for G\u00c9ANT. In routing scenarios, our clustered predictions also reduce maximum link utilization (MLU) bias by 18\\% and 21\\%, respectively, demonstrating the practical benefits of clustering when TMs are used for network optimization.         ",
    "url": "https://arxiv.org/abs/2509.15072",
    "authors": [
      "Martha Cash",
      "Alexander Wyglinski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.15080",
    "title": "Minimum Sum Coloring with Bundles in Trees and Bipartite Graphs",
    "abstract": "           The minimum sum coloring problem with bundles was introduced by Darbouy and Friggstad (SWAT 2024) as a common generalization of the minimum coloring problem and the minimum sum coloring problem. During their presentation, the following open problem was raised: whether the minimum sum coloring problem with bundles could be solved in polynomial time for trees. We answer their question in the negative by proving that the minimum sum coloring problem with bundles is NP-hard even for paths. We complement this hardness by providing algorithms of the following types. First, we provide a fixed-parameter algorithm for trees when the number of bundles is a parameter; this can be extended to graphs of bounded treewidth. Second, we provide a polynomial-time algorithm for trees when bundles form a partition of the vertex set and the difference between the number of vertices and the number of bundles is constant. Third, we provide a polynomial-time algorithm for trees when bundles form a partition of the vertex set and each bundle induces a connected subgraph. We further show that for bipartite graphs, the problem with weights is NP-hard even when the number of bundles is at least three, but is polynomial-time solvable when the number of bundles is at most two. The threshold shifts to three versus four for the problem without weights.         ",
    "url": "https://arxiv.org/abs/2509.15080",
    "authors": [
      "Takehiro Ito",
      "Naonori Kakimura",
      "Naoyuki Kamiyama",
      "Yusuke Kobayashi",
      "Yoshio Okamoto"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2509.15097",
    "title": "The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning",
    "abstract": "           The rising computational and energy demands of deep learning, particularly in large-scale architectures such as foundation models and large language models (LLMs), pose significant challenges to sustainability. Traditional gradient-based training methods are inefficient, requiring numerous iterative updates and high power consumption. To address these limitations, we propose a hybrid framework that combines hierarchical decomposition with FPGA-based direct equation solving and incremental learning. Our method divides the neural network into two functional tiers: lower layers are optimized via single-step equation solving on FPGAs for efficient and parallelizable feature extraction, while higher layers employ adaptive incremental learning to support continual updates without full retraining. Building upon this foundation, we introduce the Compound LLM framework, which explicitly deploys LLM modules across both hierarchy levels. The lower-level LLM handles reusable representation learning with minimal energy overhead, while the upper-level LLM performs adaptive decision-making through energy-aware updates. This integrated design enhances scalability, reduces redundant computation, and aligns with the principles of sustainable AI. Theoretical analysis and architectural insights demonstrate that our method reduces computational costs significantly while preserving high model performance, making it well-suited for edge deployment and real-time adaptation in energy-constrained environments.         ",
    "url": "https://arxiv.org/abs/2509.15097",
    "authors": [
      "Mohammad Saleh Vahdatpour",
      "Huaiyuan Chu",
      "Yanqing Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.15113",
    "title": "Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers",
    "abstract": "           The growing demand for energy-efficient, high-performance AI systems has led to increased attention on alternative computing platforms (e.g., photonic, neuromorphic) due to their potential to accelerate learning and inference. However, integrating such physical components into deep learning pipelines remains challenging, as physical devices often offer limited expressiveness, and their non-differentiable nature renders on-device backpropagation difficult or infeasible. This motivates the development of hybrid architectures that combine digital neural networks with reconfigurable physical layers, which effectively behave as black boxes. In this work, we present a framework for the end-to-end training of such hybrid networks. This framework integrates stochastic zeroth-order optimization for updating the physical layer's internal parameters with a dynamic low-rank surrogate model that enables gradient propagation through the physical layer. A key component of our approach is the implicit projector-splitting integrator algorithm, which updates the lightweight surrogate model after each forward pass with minimal hardware queries, thereby avoiding costly full matrix reconstruction. We demonstrate our method across diverse deep learning tasks, including: computer vision, audio classification, and language modeling. Notably, across all modalities, the proposed approach achieves near-digital baseline accuracy and consistently enables effective end-to-end training of hybrid models incorporating various non-differentiable physical components (spatial light modulators, microring resonators, and Mach-Zehnder interferometers). This work bridges hardware-aware deep learning and gradient-free optimization, thereby offering a practical pathway for integrating non-differentiable physical components into scalable, end-to-end trainable AI systems.         ",
    "url": "https://arxiv.org/abs/2509.15113",
    "authors": [
      "Andrei Chertkov",
      "Artem Basharin",
      "Mikhail Saygin",
      "Evgeny Frolov",
      "Stanislav Straupe",
      "Ivan Oseledets"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.15120",
    "title": "Efficient Conformal Prediction for Regression Models under Label Noise",
    "abstract": "           In high-stakes scenarios, such as medical imaging applications, it is critical to equip the predictions of a regression model with reliable confidence intervals. Recently, Conformal Prediction (CP) has emerged as a powerful statistical framework that, based on a labeled calibration set, generates intervals that include the true labels with a pre-specified probability. In this paper, we address the problem of applying CP for regression models when the calibration set contains noisy labels. We begin by establishing a mathematically grounded procedure for estimating the noise-free CP threshold. Then, we turn it into a practical algorithm that overcomes the challenges arising from the continuous nature of the regression problem. We evaluate the proposed method on two medical imaging regression datasets with Gaussian label noise. Our method significantly outperforms the existing alternative, achieving performance close to the clean-label setting.         ",
    "url": "https://arxiv.org/abs/2509.15120",
    "authors": [
      "Yahav Cohen",
      "Jacob Goldberger",
      "Tom Tirer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.15148",
    "title": "A1: Asynchronous Test-Time Scaling via Conformal Prediction",
    "abstract": "           Large language models (LLMs) benefit from test-time scaling, but existing methods face significant challenges, including severe synchronization overhead, memory bottlenecks, and latency, especially during speculative decoding with long reasoning chains. We introduce A1 (Asynchronous Test-Time Scaling), a statistically guaranteed adaptive inference framework that addresses these challenges. A1 refines arithmetic intensity to identify synchronization as the dominant bottleneck, proposes an online calibration strategy to enable asynchronous inference, and designs a three-stage rejection sampling pipeline that supports both sequential and parallel scaling. Through experiments on the MATH, AMC23, AIME24, and AIME25 datasets, across various draft-target model families, we demonstrate that A1 achieves a remarkable 56.7x speedup in test-time scaling and a 4.14x improvement in throughput, all while maintaining accurate rejection-rate control, reducing latency and memory overhead, and no accuracy loss compared to using target model scaling alone. These results position A1 as an efficient and principled solution for scalable LLM inference. We have released the code at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.15148",
    "authors": [
      "Jing Xiong",
      "Qiujiang Chen",
      "Fanghua Ye",
      "Zhongwei Wan",
      "Chuanyang Zheng",
      "Chenyang Zhao",
      "Hui Shen",
      "Alexander Hanbo Li",
      "Chaofan Tao",
      "Haochen Tan",
      "Haoli Bai",
      "Lifeng Shang",
      "Lingpeng Kong",
      "Ngai Wong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.15150",
    "title": "Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families",
    "abstract": "           Developing editing support for $L$ languages in $E$ editors is complex and time-consuming. Some languages do not provide dedicated editors, while others offer a single native editor. The $\\textit{language server protocol}$ (LSP) reduces the language-editor combinations $L \\times E$ to $L + E$, where a single language server communicates with editors via LSP plugins. However, overlapping implementations of linguistic components remain an issue. Existing language workbenches struggle with modularity, reusability, and leveraging type systems for language server generation. In this work, we propose: (i) Typelang, a family of domain-specific languages for modular, composable, and reusable type system implementation, (ii) a modular language server generation process, producing servers for languages built in a modular workbench, (iii) the variant-oriented programming paradigm and a cross-artifact coordination layer to manage interdependent software variants, and (iv) an LSP plugin generator, reducing $E$ to $1$ by automating plugin creation for multiple editors. To simplify editing support for language families, each language artifact integrates its own Typelang variant, used to generate language servers. This reduces combinations to $T \\times 1$, where $T = L$ represents the number of type systems. Further reuse of language artifacts across languages lowers this to $N \\times 1$, where $N << T$, representing unique type systems. We implement Typelang in Neverlang, generating language servers for each artifact and LSP plugins for three editors. Empirical evaluation shows a 93.48% reduction in characters needed for type system implementation and 100% automation of LSP plugin generation, significantly lowering effort for editing support in language families, especially when artifacts are reused.         ",
    "url": "https://arxiv.org/abs/2509.15150",
    "authors": [
      "Federico Bruzzone",
      "Walter Cazzola",
      "Luca Favalli"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2509.15153",
    "title": "AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use",
    "abstract": "           Multivariate time-series anomaly detection, which is critical for identifying unexpected events, has been explored in the field of machine learning for several decades. However, directly applying these methods to data from forceful tool use tasks is challenging because streaming sensor data in the real world tends to be inherently noisy, exhibits non-stationary behavior, and varies across different tasks and tools. To address these challenges, we propose a method, AnoF-Diff, based on the diffusion model to extract force-torque features from time-series data and use force-torque features to detect anomalies. We compare our method with other state-of-the-art methods in terms of F1-score and Area Under the Receiver Operating Characteristic curve (AUROC) on four forceful tool-use tasks, demonstrating that our method has better performance and is more robust to a noisy dataset. We also propose the method of parallel anomaly score evaluation based on one-step diffusion and demonstrate how our method can be used for online anomaly detection in several forceful tool use experiments.         ",
    "url": "https://arxiv.org/abs/2509.15153",
    "authors": [
      "Yating Lin",
      "Zixuan Huang",
      "Fan Yang",
      "Dmitry Berenson"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.15154",
    "title": "MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation",
    "abstract": "           Ensuring factual consistency and reliable reasoning remains a critical challenge for medical vision-language models. We introduce MEDFACT-R1, a two-stage framework that integrates external knowledge grounding with reinforcement learning to improve the factual medical reasoning. The first stage uses pseudo-label supervised fine-tuning (SFT) to incorporate external factual expertise; while the second stage applies Group Relative Policy Optimization (GRPO) with four tailored factual reward signals to encourage self-consistent reasoning. Across three public medical QA benchmarks, MEDFACT-R1 delivers up to 22.5% absolute improvement in factual accuracy over previous state-of-the-art methods. Ablation studies highlight the necessity of pseudo-label SFT cold start and validate the contribution of each GRPO reward, underscoring the synergy between knowledge grounding and RL-driven reasoning for trustworthy medical AI. Codes are released at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.15154",
    "authors": [
      "Gengliang Li",
      "Rongyu Chen",
      "Bin Li",
      "Linlin Yang",
      "Guodong Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.15159",
    "title": "AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt",
    "abstract": "           Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources to improve factual accuracy and verifiability. However, this reliance introduces new attack surfaces within the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have exposed such vulnerabilities, they largely rely on manipulating user queries, which is often infeasible in practice due to fixed or protected user inputs. This narrow focus overlooks a more realistic and stealthy vector: instructional prompts, which are widely reused, publicly shared, and rarely audited. Their implicit trust makes them a compelling target for adversaries to manipulate RAG behavior covertly. We introduce a novel attack for Adversarial Instructional Prompt (AIP) that exploits adversarial instructional prompts to manipulate RAG outputs by subtly altering retrieval behavior. By shifting the attack surface to the instructional prompts, AIP reveals how trusted yet seemingly benign interface components can be weaponized to degrade system integrity. The attack is crafted to achieve three goals: (1) naturalness, to evade user detection; (2) utility, to encourage use of prompts; and (3) robustness, to remain effective across diverse query variations. We propose a diverse query generation strategy that simulates realistic linguistic variation in user queries, enabling the discovery of prompts that generalize across paraphrases and rephrasings. Building on this, a genetic algorithm-based joint optimization is developed to evolve adversarial prompts by balancing attack success, clean-task utility, and stealthiness. Experimental results show that AIP achieves up to 95.23% ASR while preserving benign functionality. These findings uncover a critical and previously overlooked vulnerability in RAG systems, emphasizing the need to reassess the shared instructional prompts.         ",
    "url": "https://arxiv.org/abs/2509.15159",
    "authors": [
      "Saket S. Chaturvedi",
      "Gaurav Bagwe",
      "Lan Zhang",
      "Xiaoyong Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.15170",
    "title": "Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting",
    "abstract": "           Radio frequency fingerprint identification (RFFI) distinguishes wireless devices by the small variations in their analog circuits, avoiding heavy cryptographic authentication. While deep learning on spectrograms improves accuracy, models remain vulnerable to copying, tampering, and evasion. We present a stronger RFFI system combining watermarking for ownership proof and anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel spectrograms, we embed three watermarks: a simple trigger, an adversarially trained trigger robust to noise and filtering, and a hidden gradient/weight signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler (KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset, our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC, offering verifiable, tamper-resistant authentication.         ",
    "url": "https://arxiv.org/abs/2509.15170",
    "authors": [
      "Aarushi Mahajan",
      "Wayne Burleson"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2509.15174",
    "title": "SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models",
    "abstract": "           WARNING: This paper contains examples of offensive materials. Toxic content has become pervasive on social media platforms. We introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate -- demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs' self-improving capabilities for both classification and explanation.         ",
    "url": "https://arxiv.org/abs/2509.15174",
    "authors": [
      "Huy Nghiem",
      "Advik Sachdeva",
      "Hal Daum\u00e9 III"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.15181",
    "title": "Maize Seedling Detection Dataset (MSDD): A Curated High-Resolution RGB Dataset for Seedling Maize Detection and Benchmarking with YOLOv9, YOLO11, YOLOv12 and Faster-RCNN",
    "abstract": "           Accurate maize seedling detection is crucial for precision agriculture, yet curated datasets remain scarce. We introduce MSDD, a high-quality aerial image dataset for maize seedling stand counting, with applications in early-season crop monitoring, yield prediction, and in-field management. Stand counting determines how many plants germinated, guiding timely decisions such as replanting or adjusting inputs. Traditional methods are labor-intensive and error-prone, while computer vision enables efficient, accurate detection. MSDD contains three classes-single, double, and triple plants-capturing diverse growth stages, planting setups, soil types, lighting conditions, camera angles, and densities, ensuring robustness for real-world use. Benchmarking shows detection is most reliable during V4-V6 stages and under nadir views. Among tested models, YOLO11 is fastest, while YOLOv9 yields the highest accuracy for single plants. Single plant detection achieves precision up to 0.984 and recall up to 0.873, but detecting doubles and triples remains difficult due to rarity and irregular appearance, often from planting errors. Class imbalance further reduces accuracy in multi-plant detection. Despite these challenges, YOLO11 maintains efficient inference at 35 ms per image, with an additional 120 ms for saving outputs. MSDD establishes a strong foundation for developing models that enhance stand counting, optimize resource allocation, and support real-time decision-making. This dataset marks a step toward automating agricultural monitoring and advancing precision agriculture.         ",
    "url": "https://arxiv.org/abs/2509.15181",
    "authors": [
      "Dewi Endah Kharismawati",
      "Toni Kazic"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.15184",
    "title": "Version Age of Information with Contact Mobility in Gossip Networks",
    "abstract": "           A gossip network is considered in which a source node updates its status while other nodes in the network aim at keeping track of it as it varies over time. Information gets disseminated by the source sending status updates to the nodes, and the nodes gossiping with each other. In addition, the nodes in the network are mobile, and can move to other nodes to get information, which we term contact mobility. The goal for the nodes is to remain as fresh as possible, i.e., to have the same information as the source's. To evaluate the freshness of information, we use the Version Age-of-Information (VAoI) metric, defined as the difference between the version of information available at a given node and that at the source. We analyze the effect of contact mobility on information dissemination in the gossip network using a Stochastic Hybrid System (SHS) framework for different topologies and mobility scalings with increasing number of nodes. It is shown that with the presence of contact mobility the freshness of the network improves in both ends of the network connectivity spectrum: disconnected and fully connected gossip networks. We mathematically analyze the average version age scalings and validate our theoretical results via simulations. Finally, we incorporate the cost of mobility for the network by formulating and solving an optimization problem that minimizes a weighted sum of version age and mobility cost. Our results show that contact mobility, with optimized mobility cost, improves the average version age in the network.         ",
    "url": "https://arxiv.org/abs/2509.15184",
    "authors": [
      "Irtiza Hasan",
      "Ahmed Arafa"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2509.15188",
    "title": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning",
    "abstract": "           Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the long decoding-window problem, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks, but this sacrifices speed and bidirectionality, eliminating the main advantage of diffusion models. To overcome this, we propose Convolutional decoding (Conv), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements.         ",
    "url": "https://arxiv.org/abs/2509.15188",
    "authors": [
      "Yeongbin Seo",
      "Dongha Lee",
      "Jaehyung Kim",
      "Jinyoung Yeo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.15192",
    "title": "Channel Prediction under Network Distribution Shift Using Continual Learning-based Loss Regularization",
    "abstract": "           Modern wireless networks face critical challenges when mobile users traverse heterogeneous network configurations with varying antenna layouts, carrier frequencies, and scattering statistics. Traditional predictors degrade under distribution shift, with NMSE rising by 37.5\\% during cross-configuration handovers. This work addresses catastrophic forgetting in channel prediction by proposing a continual learning framework based on loss regularization. The approach augments standard training objectives with penalty terms that selectively preserve network parameters essential for previous configurations while enabling adaptation to new environments. Two prominent regularization strategies are investigated: Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI). Across 3GPP scenarios and multiple architectures, SI lowers the high-SNR NMSE floor by up to 1.8 dB ($\\approx$32--34\\%), while EWC achieves up to 1.4 dB ($\\approx$17--28\\%). Notably, standard EWC incurs $\\mathcal{O}(MK)$ complexity (storing $M$ Fisher diagonal entries and corresponding parameter snapshots across $K$ tasks) unless consolidated, whereas SI maintains $\\mathcal{O}(M)$ memory complexity (storing $M$ model parameters), independent of task sequence length, making it suitable for resource-constrained wireless infrastructure         ",
    "url": "https://arxiv.org/abs/2509.15192",
    "authors": [
      "Muhammad Ahmed Mohsin",
      "Muhammad Umer",
      "Ahsan Bilal",
      "Muhammad Ibtsaam Qadir",
      "Muhammad Ali Jamshed",
      "Dean F. Hougen",
      "John M. Cioffi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2509.15199",
    "title": "CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness",
    "abstract": "           Causal fairness in databases is crucial to preventing biased and inaccurate outcomes in downstream tasks. While most prior work assumes a known causal model, recent efforts relax this assumption by enforcing additional constraints. However, these approaches often fail to capture broader attribute relationships that are critical to maintaining utility. This raises a fundamental question: Can we harness the benefits of causal reasoning to design efficient and effective fairness solutions without relying on strong assumptions about the underlying causal model? In this paper, we seek to answer this question by introducing CausalPre, a scalable and effective causality-guided data pre-processing framework that guarantees justifiable fairness, a strong causal notion of fairness. CausalPre extracts causally fair relationships by reformulating the originally complex and computationally infeasible extraction task into a tailored distribution estimation problem. To ensure scalability, CausalPre adopts a carefully crafted variant of low-dimensional marginal factorization to approximate the joint distribution, complemented by a heuristic algorithm that efficiently tackles the associated computational challenge. Extensive experiments on benchmark datasets demonstrate that CausalPre is both effective and scalable, challenging the conventional belief that achieving causal fairness requires trading off relationship coverage for relaxed model assumptions.         ",
    "url": "https://arxiv.org/abs/2509.15199",
    "authors": [
      "Ying Zheng",
      "Yangfan Jiang",
      "Kian-Lee Tan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2509.15210",
    "title": "Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation",
    "abstract": "           Realistic sound simulation plays a critical role in many applications. A key element in sound simulation is the room impulse response (RIR), which characterizes how sound propagates from a source to a listener within a given space. Recent studies have applied neural implicit methods to learn RIR using context information collected from the environment, such as scene images. However, these approaches do not effectively leverage explicit geometric information from the environment. To further exploit the potential of neural implicit models with direct geometric features, we present Mesh-infused Neural Acoustic Field (MiNAF), which queries a rough room mesh at given locations and extracts distance distributions as an explicit representation of local context. Our approach demonstrates that incorporating explicit local geometric features can better guide the neural network in generating more accurate RIR predictions. Through comparisons with conventional and state-of-the-art baseline methods, we show that MiNAF performs competitively across various evaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets with limited training samples, demonstrating an advance in high-fidelity sound simulation.         ",
    "url": "https://arxiv.org/abs/2509.15210",
    "authors": [
      "Chen Si",
      "Qianyi Wu",
      "Chaitanya Amballa",
      "Romit Roy Choudhury"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.15219",
    "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction",
    "abstract": "           Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at this http URL ",
    "url": "https://arxiv.org/abs/2509.15219",
    "authors": [
      "Haichao Zhang",
      "Yi Xu",
      "Yun Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Multimedia (cs.MM)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.14290",
    "title": "QLook:Quantum-Driven Viewport Prediction for Virtual Reality",
    "abstract": "           We propose QLook, a quantum-driven predictive framework to improve viewport prediction accuracy in immersive virtual reality (VR) environments. The framework utilizes quantum neural networks (QNNs) to model the user movement data, which has multiple interdependent dimensions and is collected in six-degree-of-freedom (6DoF) VR settings. QNN leverages superposition and entanglement to encode and process complex correlations among high-dimensional user positional data. The proposed solution features a cascaded hybrid architecture that integrates classical neural networks with variational quantum circuits (VQCs)-enhanced quantum long short-term memory (QLSTM) networks. We utilize identity block initialization to mitigate training challenges commonly associated with VQCs, particularly those encountered as barren plateaus. Empirical evaluation of QLook demonstrates a 37.4% reduction in mean squared error (MSE) compared to state-of-the-art (SoTA), showcasing superior viewport prediction.         ",
    "url": "https://arxiv.org/abs/2509.14290",
    "authors": [
      "Niusha Sabri Kadijani",
      "Yoga Suhas Kuruba Manjunath",
      "Xiaodan Bi",
      "Lian Zhao"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.14360",
    "title": "Embodied sensorimotor control: computational modeling of the neural control of movement",
    "abstract": "           We review how sensorimotor control is dictated by interacting neural populations, optimal feedback mechanisms, and the biomechanics of bodies. First, we outline the distributed anatomical loops that shuttle sensorimotor signals between cortex, subcortical regions, and spinal cord. We then summarize evidence that neural population activity occupies low-dimensional, dynamically evolving manifolds during planning and execution of movements. Next, we summarize literature explaining motor behavior through the lens of optimal control theory, which clarifies the role of internal models and feedback during motor control. Finally, recent studies on embodied sensorimotor control address gaps within each framework by aiming to elucidate neural population activity through the explicit control of musculoskeletal dynamics. We close by discussing open problems and opportunities: multi-tasking and cognitively rich behavior, multi-regional circuit models, and the level of anatomical detail needed in body and network models. Together, this review and recent advances point towards reaching an integrative account of the neural control of movement.         ",
    "url": "https://arxiv.org/abs/2509.14360",
    "authors": [
      "Muhammad Noman Almani",
      "John Lazzari",
      "Jeff Walker",
      "Shreya Saxena"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14430",
    "title": "Multi-Channel Differential ASR for Robust Wearer Speech Recognition on Smart Glasses",
    "abstract": "           With the growing adoption of wearable devices such as smart glasses for AI assistants, wearer speech recognition (WSR) is becoming increasingly critical to next-generation human-computer interfaces. However, in real environments, interference from side-talk speech remains a significant challenge to WSR and may cause accumulated errors for downstream tasks such as natural language processing. In this work, we introduce a novel multi-channel differential automatic speech recognition (ASR) method for robust WSR on smart glasses. The proposed system takes differential inputs from different frontends that complement each other to improve the robustness of WSR, including a beamformer, microphone selection, and a lightweight side-talk detection model. Evaluations on both simulated and real datasets demonstrate that the proposed system outperforms the traditional approach, achieving up to an 18.0% relative reduction in word error rate.         ",
    "url": "https://arxiv.org/abs/2509.14430",
    "authors": [
      "Yufeng Yang",
      "Yiteng Huang",
      "Yong Xu",
      "Li Wan",
      "Suwon Shon",
      "Yang Liu",
      "Yifeng Fan",
      "Zhaojun Yang",
      "Olivier Siohan",
      "Yue Liu",
      "Ming Sun",
      "Florian Metze"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2509.14559",
    "title": "Radiolunadiff: Estimation of wireless network signal strength in lunar terrain",
    "abstract": "           In this paper, we propose a novel physics-informed deep learning architecture for predicting radio maps over lunar terrain. Our approach integrates a physics-based lunar terrain generator, which produces realistic topography informed by publicly available NASA data, with a ray-tracing engine to create a high-fidelity dataset of radio propagation scenarios. Building on this dataset, we introduce a triplet-UNet architecture, consisting of two standard UNets and a diffusion network, to model complex propagation effects. Experimental results demonstrate that our method outperforms existing deep learning approaches on our terrain dataset across various metrics.         ",
    "url": "https://arxiv.org/abs/2509.14559",
    "authors": [
      "Paolo Torrado",
      "Anders Pearson",
      "Jason Klein",
      "Alexander Moscibroda",
      "Joshua Smith"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14632",
    "title": "Mitigating Intra-Speaker Variability in Diarization with Style-Controllable Speech Augmentation",
    "abstract": "           Speaker diarization systems often struggle with high intrinsic intra-speaker variability, such as shifts in emotion, health, or content. This can cause segments from the same speaker to be misclassified as different individuals, for example, when one raises their voice or speaks faster during conversation. To address this, we propose a style-controllable speech generation model that augments speech across diverse styles while preserving the target speaker's identity. The proposed system starts with diarized segments from a conventional diarizer. For each diarized segment, it generates augmented speech samples enriched with phonetic and stylistic diversity. And then, speaker embeddings from both the original and generated audio are blended to enhance the system's robustness in grouping segments with high intrinsic intra-speaker variability. We validate our approach on a simulated emotional speech dataset and the truncated AMI dataset, demonstrating significant improvements, with error rate reductions of 49% and 35% on each dataset, respectively.         ",
    "url": "https://arxiv.org/abs/2509.14632",
    "authors": [
      "Miseul Kim",
      "Soo Jin Park",
      "Kyungguen Byun",
      "Hyeon-Kyeong Shin",
      "Sunkuk Moon",
      "Shuhua Zhang",
      "Erik Visser"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2509.14789",
    "title": "Acoustic Simulation Framework for Multi-channel Replay Speech Detection",
    "abstract": "           Replay speech attacks pose a significant threat to voice-controlled systems, especially in smart environments where voice assistants are widely deployed. While multi-channel audio offers spatial cues that can enhance replay detection robustness, existing datasets and methods predominantly rely on single-channel recordings. In this work, we introduce an acoustic simulation framework designed to simulate multi-channel replay speech configurations using publicly available resources. Our setup models both genuine and spoofed speech across varied environments, including realistic microphone and loudspeaker impulse responses, room acoustics, and noise conditions. The framework employs measured loudspeaker directionalities during the replay attack to improve the realism of the simulation. We define two spoofing settings, which simulate whether a reverberant or an anechoic speech is used in the replay scenario, and evaluate the impact of omnidirectional and diffuse noise on detection performance. Using the state-of-the-art M-ALRAD model for replay speech detection, we demonstrate that synthetic data can support the generalization capabilities of the detector across unseen enclosures.         ",
    "url": "https://arxiv.org/abs/2509.14789",
    "authors": [
      "Michael Neri",
      "Tuomas Virtanen"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Cryptography and Security (cs.CR)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2509.14836",
    "title": "Sampling Method for Generalized Graph Signals with Pre-selected Vertices via DC Optimization",
    "abstract": "           This paper proposes a method for vertex-wise flexible sampling of a broad class of graph signals, designed to attain the best possible recovery based on the generalized sampling theory. This is achieved by designing a sampling operator by an optimization problem, which is inherently non-convex, as the best possible recovery imposes a rank constraint. An existing method for vertex-wise flexible sampling is able to control the number of active vertices but cannot incorporate prior knowledge of mandatory or forbidden vertices. To address these challenges, we formulate the operator design as a problem that handles a constraint of the number of active vertices and prior knowledge on specific vertices for sampling, mandatory inclusion or exclusion. We transformed this constrained problem into a difference-of-convex (DC) optimization problem by using the nuclear norm and a DC penalty for vertex selection. To solve this, we develop a convergent solver based on the general double-proximal gradient DC algorithm. The effectiveness of our method is demonstrated through experiments on various graph signal models, including real-world data, showing superior performance in the recovery accuracy by comparing to existing methods.         ",
    "url": "https://arxiv.org/abs/2509.14836",
    "authors": [
      "Keitaro Yamashita",
      "Kazuki Naganuma",
      "Shunsuke Ono"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14875",
    "title": "Beyond Spherical geometry: Unraveling complex features of objects orbiting around stars from its transit light curve using deep learning",
    "abstract": "           Characterizing the geometry of an object orbiting around a star from its transit light curve is a powerful tool to uncover various complex phenomena. This problem is inherently ill-posed, since similar or identical light curves can be produced by multiple different shapes. In this study, we investigate the extent to which the features of a shape can be embedded in a transit light curve. We generate a library of two-dimensional random shapes and simulate their transit light curves with light curve simulator, Yuti. Each shape is decomposed into a series of elliptical components expressed in the form of Fourier coefficients that adds increasingly diminishing perturbations to an ideal ellipse. We train deep neural networks to predict these Fourier coefficients directly from simulated light curves. Our results demonstrate that the neural network can successfully reconstruct the low-order ellipses, which describe overall shape, orientation and large-scale perturbations. For higher order ellipses the scale is successfully determined but the inference of eccentricity and orientation is limited, demonstrating the extent of shape information in the light curve. We explore the impact of non-convex shape features in reconstruction, and show its dependence on shape orientation. The level of reconstruction achieved by the neural network underscores the utility of using light curves as a means to extract geometric information from transiting systems.         ",
    "url": "https://arxiv.org/abs/2509.14875",
    "authors": [
      "Ushasi Bhowmick",
      "Shivam Kumaran"
    ],
    "subjectives": [
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.14959",
    "title": "Discrete optimal transport is a strong audio adversarial attack",
    "abstract": "           In this paper, we show that discrete optimal transport (DOT) is an effective black-box adversarial attack against modern audio anti-spoofing countermeasures (CMs). Our attack operates as a post-processing, distribution-alignment step: frame-level WavLM embeddings of generated speech are aligned to an unpaired bona fide pool via entropic OT and a top-$k$ barycentric projection, then decoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with AASIST baselines, DOT yields consistently high equal error rate (EER) across datasets and remains competitive after CM fine-tuning, outperforming several conventional attacks in cross-dataset transfer. Ablation analysis highlights the practical impact of vocoder overlap. Results indicate that distribution-level alignment is a powerful and stable attack surface for deployed CMs.         ",
    "url": "https://arxiv.org/abs/2509.14959",
    "authors": [
      "Anton Selitskiy",
      "Akib Shahriyar",
      "Jishnuraj Prakasan"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.14961",
    "title": "Towards universal property prediction in Cartesian space: TACE is all you need",
    "abstract": "           Machine learning has revolutionized atomistic simulations and materials science, yet current approaches often depend on spherical-harmonic representations. Here we introduce the Tensor Atomic Cluster Expansion and Tensor Moment Potential, the first unified framework formulated entirely in Cartesian space for the systematic prediction of arbitrary structure-determined tensorial properties. TACE achieves this by decomposing atomic environments into a complete hierarchy of (irreducible) Cartesian tensors, ensuring symmetry-consistent representations that naturally encode invariance and equivariance constraints. Beyond geometry, TACE incorporates universal embeddings that flexibly integrate diverse attributes including basis sets, charges, magnetic moments and field perturbations. This allows explicit control over external invariants and equivariants in the prediction process. Long-range interactions are also accurately described through the Latent Ewald Summation module within the short-range approximation, providing a rigorous yet computationally efficient treatment of electrostatic interactions. We demonstrate that TACE attains accuracy, stability, and efficiency on par with or surpassing leading equivariant frameworks across finite molecules and extended materials, including in-domain and out-of-domain benchmarks, spectra, hessians, external-field response, charged systems, magnetic systems, multi-fidelity training, and heterogeneous catalytic systems. Crucially, TACE bridges scalar and tensorial modeling and establishes a Cartesian-space paradigm that unifies and extends beyond the design space of spherical-harmonic-based methods. This work lays the foundation for a new generation of universal atomistic machine learning models capable of systematically capturing the rich interplay of geometry, fields and material properties within a single coherent framework.         ",
    "url": "https://arxiv.org/abs/2509.14961",
    "authors": [
      "Zemin Xu",
      "Wenbo Xie",
      "Daiqian Xie",
      "P. Hu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ]
  },
  {
    "id": "arXiv:2509.14964",
    "title": "On 3-Connected Cubic Planar Graphs and their Strong Embeddings on Orientable Surfaces",
    "abstract": "           Although the strong embedding of a 3-connected planar graph $G$ on the sphere is unique, $G$ can have different inequivalent strong embeddings on a surface of positive genus. If $G$ is cubic, then the strong embeddings of $G$ on the projective plane, the torus and the Klein bottle each are in one-to-one correspondence with certain subgraphs of the dual graph $G^\\ast$. Here, we exploit this characterisation and show that two strong embeddings of $G$ on the projective plane, the torus or the Klein bottle are isomorphic if and only if the corresponding subgraphs of $G^{\\ast}$ are contained in the same orbit under $\\mathrm{Aut}(G^{\\ast})$. This allows us to construct a data base containing all isomorphism classes of strong embeddings on the projective plane, the torus and the Klein bottle of all 3-connected cubic planar graphs with up to 22 vertices. Moreover, we establish that cyclically 4-edge connected cubic planar graphs can be strongly embedded on orientable surfaces of positive genera. We use this to show that a 3-connected cubic planar graph has no strong embedding on orientable surfaces of positive genera if and only if it is the dual of an Apollonian network.         ",
    "url": "https://arxiv.org/abs/2509.14964",
    "authors": [
      "Meike Wei\u00df",
      "Reymond Akpanya",
      "Alice C. Niemeyer"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2509.15001",
    "title": "BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings",
    "abstract": "           Child-centered long-form recordings are essential for studying early language development, but existing speech models trained on clean adult data perform poorly due to acoustic and linguistic differences. We introduce BabyHuBERT, the first self-supervised speech representation model trained on 13,000 hours of multilingual child-centered long-form recordings spanning over 40 languages. We evaluate BabyHuBERT on speaker segmentation, identifying when target children speak versus female adults, male adults, or other children -- a fundamental preprocessing step for analyzing naturalistic language experiences. BabyHuBERT achieves F1-scores from 52.1% to 74.4% across six diverse datasets, consistently outperforming W2V2-LL4300 (trained on English long-forms) and standard HuBERT (trained on clean adult speech). Notable improvements include 13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on Solomon Islands corpora, demonstrating effectiveness on underrepresented languages. By sharing code and models, BabyHuBERT serves as a foundation model for child speech research, enabling fine-tuning on diverse downstream tasks.         ",
    "url": "https://arxiv.org/abs/2509.15001",
    "authors": [
      "Th\u00e9o Charlot",
      "Tarek Kunze",
      "Maxime Poli",
      "Alejandrina Cristia",
      "Emmanuel Dupoux",
      "Marvin Lavechin"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2509.15141",
    "title": "Benefits of Online Tilted Empirical Risk Minimization: A Case Study of Outlier Detection and Robust Regression",
    "abstract": "           Empirical Risk Minimization (ERM) is a foundational framework for supervised learning but primarily optimizes average-case performance, often neglecting fairness and robustness considerations. Tilted Empirical Risk Minimization (TERM) extends ERM by introducing an exponential tilt hyperparameter $t$ to balance average-case accuracy with worst-case fairness and robustness. However, in online or streaming settings where data arrive one sample at a time, the classical TERM objective degenerates to standard ERM, losing tilt sensitivity. We address this limitation by proposing an online TERM formulation that removes the logarithm from the classical objective, preserving tilt effects without additional computational or memory overhead. This formulation enables a continuous trade-off controlled by $t$, smoothly interpolating between ERM ($t \\to 0$), fairness emphasis ($t > 0$), and robustness to outliers ($t < 0$). We empirically validate online TERM on two representative streaming tasks: robust linear regression with adversarial outliers and minority-class detection in binary classification. Our results demonstrate that negative tilting effectively suppresses outlier influence, while positive tilting improves recall with minimal impact on precision, all at per-sample computational cost equivalent to ERM. Online TERM thus recovers the full robustness-fairness spectrum of classical TERM in an efficient single-sample learning regime.         ",
    "url": "https://arxiv.org/abs/2509.15141",
    "authors": [
      "Yigit E. Yildirim",
      "Samet Demir",
      "Zafer Dogan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:1903.07146",
    "title": "Robust Shape Regularity Criteria for Superpixel Evaluation",
    "abstract": "           Regular decompositions are necessary for most superpixel-based object recognition or tracking applications. So far in the literature, the regularity or compactness of a superpixel shape is mainly measured by its circularity. In this work, we first demonstrate that such measure is not adapted for superpixel evaluation, since it does not directly express regularity but circular appearance. Then, we propose a new metric that considers several shape regularity aspects: convexity, balanced repartition, and contour smoothness. Finally, we demonstrate that our measure is robust to scale and noise and enables to more relevantly compare superpixel methods.         ",
    "url": "https://arxiv.org/abs/1903.07146",
    "authors": [
      "R\u00e9mi Giraud",
      "Vinh-Thong Ta",
      "Nicolas Papadakis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2204.09942",
    "title": "Cloud-Edge Collaborative Data Anomaly Detection in Industrial Sensor Networks",
    "abstract": "           Existing research on sensor data anomaly detection for industrial sensor networks still has several inherent limitations. First, most detection models usually consider centralized detection. Thus, all sensor data have to be uploaded to the control center for analysis, leading to a heavy traffic load. However, industrial sensor networks have high requirements for reliable and real-time communication. The heavy traffic load may cause communication delays or packets lost by corruption. Second, there are complex spatial and temporal features in industrial sensor data. The full extraction of such features plays a key role in improving detection this http URL solve the limitations above, this paper develops a cloud-edge collaborative data anomaly detection approach for industrial sensor networks that mainly consists of a sensor data detection model deployed at individual edges and a sensor data analysis model deployed in the cloud. The former is implemented using Gaussian and Bayesian algorithms, which effectively filter the substantial volume of sensor data generated during the normal operation of the industrial sensor network, thereby reducing traffic load. It only uploads all the sensor data to the sensor data analysis model for further analysis when the network is in an anomalous state. The latter based on GCRL is developed by inserting Long Short-Term Memory network (LSTM) into Graph Convolutional Network (GCN), which can effectively extract the spatial and temporal features of the sensor data for anomaly detection.         ",
    "url": "https://arxiv.org/abs/2204.09942",
    "authors": [
      "Tao Yang",
      "Xuefeng Jiang",
      "Wei Li",
      "Peiyu Liu",
      "Jinming Wang",
      "Weijie Hao",
      "Qiang Yang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.04228",
    "title": "Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification",
    "abstract": "           Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural networks (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order data correlations that already exist between nodes of the same field or called attribute in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose a heterogeneous directed hypergraph (HDHG) to represent AST and a heterogeneous directed hypergraph neural network (HDHGN) to process the graph for code classification. Our method improves code understanding and can represent high-order data correlations beyond paired interactions. We assess our heterogeneous directed hypergraph neural network (HDHGN) on public datasets of Python and Java programs. Our method outperforms previous AST-based and GNN-based methods, which demonstrates the capability of our model.         ",
    "url": "https://arxiv.org/abs/2305.04228",
    "authors": [
      "Guang Yang",
      "Tiancheng Jin",
      "Liang Dou"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.05283",
    "title": "On the Identity and Group Problems for Complex Heisenberg Matrices",
    "abstract": "           We study the Identity Problem, the problem of determining if a finitely generated semigroup of matrices contains the identity matrix; see Problem 3 (Chapter 10.3) in ``Unsolved Problems in Mathematical Systems and Control Theory'' by Blondel and Megretski (2004). This fundamental problem is known to be undecidable for $\\mathbb{Z}^{4 \\times 4}$ and decidable for $\\mathbb{Z}^{2 \\times 2}$. The Identity Problem has been recently shown to be in polynomial time by Dong for the Heisenberg group over complex numbers in any fixed dimension with the use of Lie algebra and the Baker-Campbell-Hausdorff formula. We develop alternative proof techniques for the problem making a step forward towards more general problems such as the Membership Problem. Using our techniques we also show that the problem of determining if a given set of Heisenberg matrices generates a group can be decided in polynomial time.         ",
    "url": "https://arxiv.org/abs/2307.05283",
    "authors": [
      "Paul C. Bell",
      "Reino Niskanen",
      "Igor Potapov",
      "Pavel Semukhin"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2308.14250",
    "title": "Rule-Based Error Detection and Correction to Operationalize Movement Trajectory Classification",
    "abstract": "           Classification of movement trajectories has many applications in transportation and is a key component for large-scale movement trajectory generation and anomaly detection which has key safety applications in the aftermath of a disaster or other external shock. However, the current state-of-the-art (SOTA) are based on supervised deep learning - which leads to challenges when the distribution of trajectories changes due to such a shock. We provide a neuro-symbolic rule-based framework to conduct error correction and detection of these models to integrate into our movement trajectory platform. We provide a suite of experiments on several recent SOTA models where we show highly accurate error detection, the ability to improve accuracy with a changing test distribution, and accuracy improvement for the base use case in addition to a suite of theoretical properties that informed algorithm development. Specifically, we show an F1 scores for predicting errors of up to 0.984, significant performance increase for out-of distribution accuracy (8.51% improvement over SOTA for zero-shot accuracy), and accuracy improvement over the SOTA model.         ",
    "url": "https://arxiv.org/abs/2308.14250",
    "authors": [
      "Bowen Xi",
      "Kevin Scaria",
      "Divyagna Bavikadi",
      "Paulo Shakarian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2311.04190",
    "title": "Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality Monitoring of the Hadron Calorimeter",
    "abstract": "           The Compact Muon Solenoid (CMS) experiment is a general-purpose detector for high-energy collision at the Large Hadron Collider (LHC) at CERN. It employs an online data quality monitoring (DQM) system to promptly spot and diagnose particle data acquisition problems to avoid data quality loss. In this study, we present a semi-supervised spatio-temporal anomaly detection (AD) monitoring system for the physics particle reading channels of the Hadron Calorimeter (HCAL) of the CMS using three-dimensional digi-occupancy map data of the DQM. We propose the GraphSTAD system, which employs convolutional and graph neural networks to learn local spatial characteristics induced by particles traversing the detector and the global behavior owing to shared backend circuit connections and housing boxes of the channels, respectively. Recurrent neural networks capture the temporal evolution of the extracted spatial features. We validate the accuracy of the proposed AD system in capturing diverse channel fault types using the LHC collision data sets. The GraphSTAD system achieves production-level accuracy and is being integrated into the CMS core production system for real-time monitoring of the HCAL. We provide a quantitative performance comparison with alternative benchmark models to demonstrate the promising leverage of the presented system. Code: \\href{this https URL}{this https URL\\_HCAL\\_ML\\_OnlineDQM}         ",
    "url": "https://arxiv.org/abs/2311.04190",
    "authors": [
      "Mulugeta Weldezgina Asres",
      "Christian Walter Omlin",
      "Long Wang",
      "David Yu",
      "Pavel Parygin",
      "Jay Dittmann",
      "Georgia Karapostoli",
      "Markus Seidel",
      "Rosamaria Venditti",
      "Luka Lambrecht",
      "Emanuele Usai",
      "Muhammad Ahmad",
      "Javier Fernandez Menendez",
      "Kaori Maeshima",
      "CMS-HCAL Collaboration"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.00241",
    "title": "Image Super-Resolution Reconstruction Network based on Enhanced Swin Transformer via Alternating Aggregation of Local-Global Features",
    "abstract": "           The Swin Transformer image super-resolution (SR) reconstruction network primarily depends on the long-range relationship of the window and shifted window attention to explore features. However, this approach focuses only on global features, ignoring local ones, and considers only spatial interactions, disregarding channel and spatial-channel feature interactions, limiting its nonlinear mapping capability. Therefore, this study proposes an enhanced Swin Transformer network (ESTN) that alternately aggregates local and global features. During local feature aggregation, shift convolution facilitates the interaction between local spatial and channel information. During global feature aggregation, a block sparse global perception module is introduced, wherein spatial information is reorganized and the recombined features are then processed by a dense layer to achieve global perception. Additionally, multiscale self-attention and low-parameter residual channel attention modules are introduced to aggregate information across different scales. Finally, the effectiveness of ESTN on five public datasets and a local attribution map (LAM) are analyzed. Experimental results demonstrate that the proposed ESTN achieves higher average PSNR, surpassing SRCNN, ELAN-light, SwinIR-light, and SMFANER+ models by 2.17dB, 0.13dB, 0.12dB, and 0.1dB, respectively, with LAM further confirming its larger receptive field. ESTN delivers improved quality of SR images. The source code can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2401.00241",
    "authors": [
      "Yuming Huang",
      "Yingpin Chen",
      "Changhui Wu",
      "Binhui Song",
      "Hui Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.01964",
    "title": "Top K Enhanced Reinforcement Learning Attacks on Heterogeneous Graph Node Classification",
    "abstract": "           Graph Neural Networks (GNNs) have attracted substantial interest due to their exceptional performance on graph-based data. However, their robustness, especially on heterogeneous graphs, remains underexplored, particularly against adversarial attacks. This paper proposes HeteroKRLAttack, a targeted evasion black-box attack method for heterogeneous graphs. By integrating reinforcement learning with a Top-K algorithm to reduce the action space, our method efficiently identifies effective attack strategies to disrupt node classification tasks. We validate the effectiveness of HeteroKRLAttack through experiments on multiple heterogeneous graph datasets, showing significant reductions in classification accuracy compared to baseline methods. An ablation study underscores the critical role of the Top-K algorithm in enhancing attack performance. Our findings highlight potential vulnerabilities in current models and provide guidance for future defense strategies against adversarial attacks on heterogeneous graphs.         ",
    "url": "https://arxiv.org/abs/2408.01964",
    "authors": [
      "Honglin Gao",
      "Xiang Li",
      "Yajuan Sun",
      "Gaoxi Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.16612",
    "title": "Data Quality Monitoring for the Hadron Calorimeters Using Transfer Learning for Anomaly Detection",
    "abstract": "           The proliferation of sensors brings an immense volume of spatio-temporal (ST) data in many domains, including monitoring, diagnostics, and prognostics applications. Data curation is a time-consuming process for a large volume of data, making it challenging and expensive to deploy data analytics platforms in new environments. Transfer learning (TL) mechanisms promise to mitigate data sparsity and model complexity by utilizing pre-trained models for a new task. Despite the triumph of TL in fields like computer vision and natural language processing, efforts on complex ST models for anomaly detection (AD) applications are limited. In this study, we present the potential of TL within the context of high-dimensional ST AD with a hybrid autoencoder architecture, incorporating convolutional, graph, and recurrent neural networks. Motivated by the need for improved model accuracy and robustness, particularly in scenarios with limited training data on systems with thousands of sensors, this research investigates the transferability of models trained on different sections of the Hadron Calorimeter of the Compact Muon Solenoid experiment at CERN. The key contributions of the study include exploring TL's potential and limitations within the context of encoder and decoder networks, revealing insights into model initialization and training configurations that enhance performance while substantially reducing trainable parameters and mitigating data contamination effects. Code: \\href{this https URL}{this https URL\\_HCAL\\_ML\\_OnlineDQM}         ",
    "url": "https://arxiv.org/abs/2408.16612",
    "authors": [
      "Mulugeta Weldezgina Asres",
      "Christian Walter Omlin",
      "Long Wang",
      "Pavel Parygin",
      "David Yu",
      "Jay Dittmann",
      "CMS-HCAL Collaboration"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.04103",
    "title": "The Role of Graph Topology in the Performance of Biomedical Knowledge Graph Completion Models",
    "abstract": "           Knowledge Graph Completion has been increasingly adopted as a useful method for helping address several tasks in biomedical research, such as drug repurposing or drug-target identification. To that end, a variety of datasets and Knowledge Graph Embedding models have been proposed over the years. However, little is known about the properties that render a dataset, and associated modelling choices, useful for a given task. Moreover, even though theoretical properties of Knowledge Graph Embedding models are well understood, their practical utility in this field remains controversial. In this work, we conduct a comprehensive investigation into the topological properties of publicly available biomedical Knowledge Graphs and establish links to the accuracy observed in real-world tasks. By releasing all model predictions and a new suite of analysis tools we invite the community to build upon our work and continue improving the understanding of these crucial applications.         ",
    "url": "https://arxiv.org/abs/2409.04103",
    "authors": [
      "Alberto Cattaneo",
      "Stephen Bonner",
      "Thomas Martynec",
      "Edward Morrissey",
      "Carlo Luschi",
      "Ian P Barrett",
      "Daniel Justus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2410.06304",
    "title": "FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning",
    "abstract": "           Hallucinations in large language models (LLMs) pose significant challenges in tasks requiring complex multi-step reasoning, such as mathematical problem-solving. Existing approaches primarily detect the presence of hallucinations but lack a nuanced understanding of their types and manifestations. In this paper, we first introduce a comprehensive taxonomy that categorizes the common hallucinations in mathematical reasoning tasks into six types. We then propose FG-PRM (Fine-Grained Process Reward Model), an augmented model designed to detect and mitigate hallucinations in a fine-grained, step-level manner. To address the limitations of manually labeling training data, we propose an automated method for generating fine-grained hallucination data using LLMs. Our FG-PRM demonstrates superior performance across two key tasks: 1) Fine-grained hallucination detection: classifying hallucination types for each reasoning step; and 2) Verification: ranking multiple LLM-generated outputs to select the most accurate solution. Our experiments show that FG-PRM excels in fine-grained hallucination detection and substantially boosts the performance of LLMs on GSM8K and MATH benchmarks. These results highlight the benefits of fine-grained supervision in enhancing the reliability and interpretability of LLM reasoning processes.         ",
    "url": "https://arxiv.org/abs/2410.06304",
    "authors": [
      "Ruosen Li",
      "Ziming Luo",
      "Xinya Du"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2411.11697",
    "title": "Robust Reinforcement Learning under Diffusion Models for Data with Jumps",
    "abstract": "           Reinforcement Learning (RL) has proven effective in solving complex decision-making tasks across various domains, but challenges remain in continuous-time settings, particularly when state dynamics are governed by stochastic differential equations (SDEs) with jump components. In this paper, we address this challenge by introducing the Mean-Square Bipower Variation Error (MSBVE) algorithm, which enhances robustness and convergence in scenarios involving significant stochastic noise and jumps. We first revisit the Mean-Square TD Error (MSTDE) algorithm, commonly used in continuous-time RL, and highlight its limitations in handling jumps in state dynamics. The proposed MSBVE algorithm minimizes the mean-square quadratic variation error, offering improved performance over MSTDE in environments characterized by SDEs with jumps. Simulations and formal proofs demonstrate that the MSBVE algorithm reliably estimates the value function in complex settings, surpassing MSTDE's performance when faced with jump processes. These findings underscore the importance of alternative error metrics to improve the resilience and effectiveness of RL algorithms in continuous-time frameworks.         ",
    "url": "https://arxiv.org/abs/2411.11697",
    "authors": [
      "Chenyang Jiang",
      "Donggyu Kim",
      "Alejandra Quintos",
      "Yazhen Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2501.16370",
    "title": "Advanced Physics-Informed Neural Network with Residuals for Solving Complex Integral Equations",
    "abstract": "           In this paper, we present the Residual Integral Solver Network (RISN), a novel neural network architecture designed to solve a wide range of integral and integro-differential equations, including one-dimensional, multi-dimensional, ordinary and partial integro-differential, systems, fractional types, and Helmholtz-type integral equations involving oscillatory kernels. RISN integrates residual connections with high-accuracy numerical methods such as Gaussian quadrature and fractional derivative operational matrices, enabling it to achieve higher accuracy and stability than traditional Physics-Informed Neural Networks (PINN). The residual connections help mitigate vanishing gradient issues, allowing RISN to handle deeper networks and more complex kernels, particularly in multi-dimensional problems. Through extensive experiments, we demonstrate that RISN consistently outperforms not only classical PINNs but also advanced variants such as Auxiliary PINN (A-PINN) and Self-Adaptive PINN (SA-PINN), achieving significantly lower Mean Absolute Errors (MAE) across various types of equations. These results highlight RISN's robustness and efficiency in solving challenging integral and integro-differential problems, making it a valuable tool for real-world applications where traditional methods often struggle.         ",
    "url": "https://arxiv.org/abs/2501.16370",
    "authors": [
      "Mahdi Movahedian Moghaddam",
      "Kourosh Parand",
      "Saeed Reza Kheradpisheh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2501.19155",
    "title": "SWAT: Sliding Window Adversarial Training for Gradual Domain Adaptation",
    "abstract": "           Domain shifts are critical issues that harm the performance of machine learning. Unsupervised Domain Adaptation (UDA) mitigates this issue but suffers when the domain shifts are steep and drastic. Gradual Domain Adaptation (GDA) alleviates this problem in a mild way by gradually adapting from the source to the target domain using multiple intermediate domains. In this paper, we propose Sliding Window Adversarial Training (SWAT) for GDA. SWAT first formulates adversarial streams to connect the feature spaces of the source and target domains. Then, a sliding window paradigm is designed that moves along the adversarial stream to gradually narrow the small gap between adjacent intermediate domains. When the window moves to the end of the stream, i.e., the target domain, the domain shift is explicitly reduced. Extensive experiments on six GDA benchmarks demonstrate the significant effectiveness of SWAT, especially 6.1% improvement on Rotated MNIST and 4.1% advantage on CIFAR-100C over the previous methods.         ",
    "url": "https://arxiv.org/abs/2501.19155",
    "authors": [
      "Zixi Wang",
      "Xiangxu Zhao",
      "Tonglan Xie",
      "Mengmeng Jing",
      "Lin Zuo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.19160",
    "title": "Physics-Informed Representation Alignment for Sparse Radio-Map Reconstruction",
    "abstract": "           Radio map reconstruction is essential for enabling advanced applications, yet challenges such as complex signal propagation and sparse observational data hinder accurate reconstruction in practical scenarios. Existing methods often fail to align physical constraints with data-driven features, particularly under sparse measurement conditions. To address these issues, we propose **Phy**sics-Aligned **R**adio **M**ap **D**iffusion **M**odel (**PhyRMDM**), a novel framework that establishes cross-domain representation alignment between physical principles and neural network features through dual learning pathways. The proposed model integrates **Physics-Informed Neural Networks (PINNs)** with a **representation alignment mechanism** that explicitly enforces consistency between Helmholtz equation constraints and environmental propagation patterns. Experimental results demonstrate significant improvements over state-of-the-art methods, achieving **NMSE of 0.0031** under *Static Radio Map (SRM)* conditions, and **NMSE of 0.0047** with **Dynamic Radio Map (DRM)** scenarios. The proposed representation alignment paradigm provides **37.2%** accuracy enhancement in ultra-sparse cases (**1%** sampling rate), confirming its effectiveness in bridging physics-based modeling and deep learning for radio map reconstruction.         ",
    "url": "https://arxiv.org/abs/2501.19160",
    "authors": [
      "Haozhe Jia",
      "Wenshuo Chen",
      "Zhihui Huang",
      "Lei Wang",
      "Hongru Xiao",
      "Nanqian Jia",
      "Keming Wu",
      "Songning Lai",
      "Bowen Tian",
      "Yutao Yue"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.17394",
    "title": "SNaRe: Domain-aware Data Generation for Low-Resource Event Detection",
    "abstract": "           Event Detection (ED) -- the task of identifying event mentions from natural language text -- is critical for enabling reasoning in highly specialized domains such as biomedicine, law, and epidemiology. Data generation has proven to be effective in broadening its utility to wider applications without requiring expensive expert annotations. However, when existing generation approaches are applied to specialized domains, they struggle with label noise, where annotations are incorrect, and domain drift, characterized by a distributional mismatch between generated sentences and the target domain. To address these issues, we introduce SNaRe, a domain-aware synthetic data generation framework composed of three components: Scout, Narrator, and Refiner. Scout extracts triggers from unlabeled target domain data and curates a high-quality domain-specific trigger list using corpus-level statistics to mitigate domain drift. Narrator, conditioned on these triggers, generates high-quality domain-aligned sentences, and Refiner identifies additional event mentions, ensuring high annotation quality. Experimentation on three diverse domain ED datasets reveals how SNaRe outperforms the best baseline, achieving average F1 gains of 3-7% in the zero-shot/few-shot settings and 4-20% F1 improvement for multilingual generation. Analyzing the generated trigger hit rate and human evaluation substantiates SNaRe's stronger annotation quality and reduced domain drift.         ",
    "url": "https://arxiv.org/abs/2502.17394",
    "authors": [
      "Tanmay Parekh",
      "Yuxuan Dong",
      "Lucas Bandarkar",
      "Artin Kim",
      "I-Hung Hsu",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08636",
    "title": "Birds look like cars: Adversarial analysis of intrinsically interpretable deep learning",
    "abstract": "           A common belief is that intrinsically interpretable deep learning models ensure a correct, intuitive understanding of their behavior and offer greater robustness against accidental errors or intentional manipulation. However, these beliefs have not been comprehensively verified, and growing evidence casts doubt on them. In this paper, we highlight the risks related to overreliance and susceptibility to adversarial manipulation of these so-called \"intrinsically (aka inherently) interpretable\" models by design. We introduce two strategies for adversarial analysis with prototype manipulation and backdoor attacks against prototype-based networks, and discuss how concept bottleneck models defend against these attacks. Fooling the model's reasoning by exploiting its use of latent prototypes manifests the inherent uninterpretability of deep neural networks, leading to a false sense of security reinforced by a visual confirmation bias. The reported limitations of part-prototype networks put their trustworthiness and applicability into question, motivating further work on the robustness and alignment of (deep) interpretable models.         ",
    "url": "https://arxiv.org/abs/2503.08636",
    "authors": [
      "Hubert Baniecki",
      "Przemyslaw Biecek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.08578",
    "title": "Multimodal Knowledge Distillation for Egocentric Action Recognition Robust to Missing Modalities",
    "abstract": "           Existing methods for egocentric action recognition often rely solely on RGB videos, while additional modalities, e.g., audio, can improve accuracy in challenging scenarios. However, most prior multimodal approaches assume all modalities are available at inference, leading to significant accuracy drops, or even failure, when inputs are missing. To address this, we introduce KARMMA, a multimodal Knowledge distillation approach for egocentric Action Recognition robust to Missing ModAlities that requires no modality alignment across all samples during training or inference. KARMMA distills knowledge from a multimodal teacher into a multimodal student that benefits from all available modalities while remaining robust to missing ones, making it suitable for diverse multimodal scenarios without retraining. Our student uses approximately 50% fewer computational resources than our teacher, resulting in a lightweight and fast model. Experiments on Epic-Kitchens and Something-Something show that our student achieves competitive accuracy while significantly reducing accuracy drops under missing modality conditions.         ",
    "url": "https://arxiv.org/abs/2504.08578",
    "authors": [
      "Maria Santos-Villafranca",
      "Dustin Carri\u00f3n-Ojeda",
      "Alejandro Perez-Yus",
      "Jesus Bermudez-Cameo",
      "Jose J. Guerrero",
      "Simone Schaub-Meyer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.13379",
    "title": "Radial Basis Function Techniques for Neural Field Models on Surfaces",
    "abstract": "           We present a numerical framework for solving neural field equations on surfaces using Radial Basis Function (RBF) interpolation and quadrature. Neural field models describe the evolution of macroscopic brain activity, but modeling studies often overlook the complex geometry of curved cortical domains. Traditional numerical methods, such as finite element or spectral methods, can be computationally expensive and challenging to implement on irregular domains. In contrast, RBF-based methods provide a flexible alternative by offering interpolation and quadrature schemes that efficiently handle arbitrary geometries with high-order accuracy. We first develop an RBF-based interpolatory projection framework for neural field models on general surfaces. Quadrature for both flat and curved domains are derived in detail, ensuring high-order accuracy and stability as they depend on RBF hyperparameters (basis functions, augmenting polynomials, and stencil size). Through numerical experiments, we demonstrate the convergence of our method, highlighting its advantages over traditional approaches in terms of flexibility and accuracy. We conclude with an exposition of numerical simulations of spatiotemporal activity on complex surfaces, illustrating the method's ability to capture complex wave propagation patterns.         ",
    "url": "https://arxiv.org/abs/2504.13379",
    "authors": [
      "Sage B Shaw",
      "Zachary P Kilpatrick",
      "Daniele Avitabile"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Pattern Formation and Solitons (nlin.PS)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2504.16404",
    "title": "Direct Video-Based Spatiotemporal Deep Learning for Cattle Lameness Detection",
    "abstract": "           Cattle lameness is a prevalent health problem in livestock farming, often resulting from hoof injuries or infections, and severely impacts animal welfare and productivity. Early and accurate detection is critical for minimizing economic losses and ensuring proper treatment. This study proposes a spatiotemporal deep learning framework for automated cattle lameness detection using publicly available video data. We curate and publicly release a balanced set of 50 online video clips featuring 42 individual cattle, recorded from multiple viewpoints in both indoor and outdoor environments. The videos were categorized into lame and non-lame classes based on visual gait characteristics and metadata descriptions. After applying data augmentation techniques to enhance generalization, two deep learning architectures were trained and evaluated: 3D Convolutional Neural Networks (3D CNN) and Convolutional Long-Short-Term Memory (ConvLSTM2D). The 3D CNN achieved a video-level classification accuracy of 90%, with a precision, recall, and F1 score of 90.9% each, outperforming the ConvLSTM2D model, which achieved 85% accuracy. Unlike conventional approaches that rely on multistage pipelines involving object detection and pose estimation, this study demonstrates the effectiveness of a direct end-to-end video classification approach. Compared with the best end-to-end prior method (C3D-ConvLSTM, 90.3%), our model achieves comparable accuracy while eliminating pose estimation this http URL results indicate that deep learning models can successfully extract and learn spatio-temporal features from various video sources, enabling scalable and efficient cattle lameness detection in real-world farm settings.         ",
    "url": "https://arxiv.org/abs/2504.16404",
    "authors": [
      "Md Fahimuzzman Sohan",
      "Raid Alzubi",
      "Hadeel Alzoubi",
      "Eid Albalawi",
      "A. H. Abdul Hafez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2504.21694",
    "title": "Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation",
    "abstract": "           AutomationML has seen widespread adoption as an open data exchange format in the automation domain. It is an open and vendor neutral standard based on the extensible markup language XML. However, AutomationML extends XML with additional semantics that limit the applicability of common XML-tools for applications like querying or data validation. This article demonstrates how the transformation of AutomationML into OWL enables new use cases in querying with SPARQL and validation with SHACL. To support this, it provides practitioners with (1) an up-to-date ontology of the concepts defined in the AutomationML standard and (2) a declarative mapping to automatically transform any AutomationML model into RDF triples. A study on examples from the automation domain concludes that transforming AutomationML to OWL opens up new powerful ways for querying and validation that would have been impossible without this transformation.         ",
    "url": "https://arxiv.org/abs/2504.21694",
    "authors": [
      "Tom Westermann",
      "Malte Ramonat",
      "Johannes Hujer",
      "Felix Gehlhoff",
      "Alexander Fay"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.00908",
    "title": "Learning Conservative Neural Control Barrier Functions from Offline Data",
    "abstract": "           Safety filters, particularly those based on control barrier functions, have gained increased interest as effective tools for safe control of dynamical systems. Existing correct-by-construction synthesis algorithms for such filters, however, suffer from the curse-of-dimensionality. Deep learning approaches have been proposed in recent years to address this challenge. In this paper, we add to this set of approaches an algorithm for training neural control barrier functions from offline datasets. Such functions can be used to design constraints for quadratic programs that are then used as safety filters. Our algorithm trains these functions so that the system is not only prevented from reaching unsafe states but is also disincentivized from reaching out-of-distribution ones, at which they would be less reliable. It is inspired by Conservative Q-learning, an offline reinforcement learning algorithm. We call its outputs Conservative Control Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs outperform existing methods in maintaining safety while minimally affecting task performance. Source code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.00908",
    "authors": [
      "Ihab Tabbara",
      "Hussein Sibai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2505.04368",
    "title": "Pipelining Split Learning in Multi-hop Edge Networks",
    "abstract": "           To support large-scale model training, split learning (SL) enables multiple edge devices/servers to share the intensive training workload. However, most existing works on SL focus solely on two-tier model splitting. Moreover, while some recent works have investigated the model splitting and placement problems for multi-hop SL, these solutions fail to overcome the resource idleness issue, resulting in significant network idle time. In this work, we propose a pipelined SL scheme by addressing the joint optimization problem of model splitting and placement (MSP) in multi-hop edge networks. By applying pipeline parallelism to SL, we identify that the MSP problem can be mapped to a problem of minimizing the weighted sum of a bottleneck cost function (min-max) and a linear cost function (min-sum). Based on graph theory, we devise a bottleneck-aware shortest-path algorithm to obtain the optimal solution. Besides, given the MSP outcomes, we also derive the closed-form solution to the micro-batch size in the pipeline. Finally, we develop an alternating optimization algorithm of MSP and micro-batch size to solve the joint optimization problem to minimize the end-to-end training latency. Extensive simulations have demonstrated the significant advantages of our algorithm compared to existing benchmarks without pipeline parallelism.         ",
    "url": "https://arxiv.org/abs/2505.04368",
    "authors": [
      "Wei Wei",
      "Zheng Lin",
      "Tao Li",
      "Xuanheng Li",
      "Xianhao Chen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2505.07546",
    "title": "GRADA: Graph-based Reranking against Adversarial Documents Attack",
    "abstract": "           Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large language models (LLMs) by integrating external knowledge from retrieved documents, thereby overcoming the limitations of models' static intrinsic knowledge. However, these systems are susceptible to adversarial attacks that manipulate the retrieval process by introducing documents that are adversarial yet semantically similar to the query. Notably, while these adversarial documents resemble the query, they exhibit weak similarity to benign documents in the retrieval set. Thus, we propose a simple yet effective Graph-based Reranking against Adversarial Document Attacks (GRADA) framework aiming at preserving retrieval quality while significantly reducing the success of adversaries. Our study evaluates the effectiveness of our approach through experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b, Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with results from the Natural Questions dataset demonstrating up to an 80% reduction in attack success rates while maintaining minimal loss in accuracy.         ",
    "url": "https://arxiv.org/abs/2505.07546",
    "authors": [
      "Jingjie Zheng",
      "Aryo Pradipta Gema",
      "Giwon Hong",
      "Xuanli He",
      "Pasquale Minervini",
      "Youcheng Sun",
      "Qiongkai Xu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.09929",
    "title": "Security and Privacy Measurement on Chinese Consumer IoT Traffic based on Device Lifecycle",
    "abstract": "           In recent years, consumer Internet of Things (IoT) devices have become widely used in daily life. With the popularity of devices, related security and privacy risks arise at the same time as they collect user-related data and transmit it to various service providers. Although China accounts for a larger share of the consumer IoT industry, current analyses on consumer IoT device traffic primarily focus on regions such as Europe, the United States, and Australia. Research on China, however, is currently relatively rare. This study constructs the first large-scale dataset about consumer IoT device traffic in China. Specifically, we propose a fine-grained traffic collection guidance covering the entire lifecycle of consumer IoT devices, gathering traffic from 77 devices spanning 38 brands and 12 device categories. Based on this dataset, we analyze traffic destinations and encryption practices across different device types during the entire lifecycle and compare the findings with the results of other regions. Compared to other regions, our results show that consumer IoT devices in China rely more on domestic services and overall perform better in terms of encryption practices. However, there are still 23/40 devices improperly conducting certificate validation, and 2/70 devices use insecure encryption protocols. To facilitate future research, we open-source our traffic collection guidance and make our dataset publicly available.         ",
    "url": "https://arxiv.org/abs/2505.09929",
    "authors": [
      "Chenghua Jin",
      "Yuxin Song",
      "Yan Jia",
      "Qingyin Tan",
      "Rui Yang",
      "Zheli Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.10876",
    "title": "Preference Isolation Forest for Structure-based Anomaly Detection",
    "abstract": "           We address the problem of detecting anomalies as samples that do not conform to structured patterns represented by low-dimensional manifolds. To this end, we conceive a general anomaly detection framework called Preference Isolation Forest (PIF), that combines the benefits of adaptive isolation-based methods with the flexibility of preference embedding. The key intuition is to embed the data into a high-dimensional preference space by fitting low-dimensional manifolds, and to identify anomalies as isolated points. We propose three isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most general solution, $ii$) RuzHash-iForest, that avoids explicit computation of distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a locality prior to improve efficiency and effectiveness.         ",
    "url": "https://arxiv.org/abs/2505.10876",
    "authors": [
      "Filippo Leveni",
      "Luca Magri",
      "Cesare Alippi",
      "Giacomo Boracchi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.14982",
    "title": "Generating Sustainability-Targeting Attacks For Cyber-Physical Systems",
    "abstract": "           Sustainability-targeting attacks (STA) are a growing threat to cyber-physical system (CPS)-based infrastructure, as sustainability goals become an integral part of CPS objectives. STA can be especially disruptive if it impacts the long-term sustainability cost of CPS, while its performance goals remain within acceptable parameters. Thus, in this work, we propose a general mathematical framework for modeling such stealthy STA and derive the feasibility conditions for generating a minimum-effort maximum-impact STA on a linear CPS using a max-min formulation. A gradient ascent descent algorithm is used to construct this attack policy with an added constraint on stealthiness. An illustrative example has been simulated to demonstrate the impact of the generated attack on the sustainability cost of the CPS.         ",
    "url": "https://arxiv.org/abs/2505.14982",
    "authors": [
      "Faysal Ahamed",
      "Tanushree Roy"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2505.20646",
    "title": "Binarized Neural Networks Converge Toward Algorithmic Simplicity: Empirical Support for the Learning-as-Compression Hypothesis",
    "abstract": "           Understanding and controlling the informational complexity of neural networks is a central challenge in machine learning, with implications for generalization, optimization, and model capacity. While most approaches rely on entropy-based loss functions and statistical metrics, these measures often fail to capture deeper, causally relevant algorithmic regularities embedded in network structure. We propose a shift toward algorithmic information theory, using Binarized Neural Networks (BNNs) as a first proxy. Grounded in algorithmic probability (AP) and the universal distribution it defines, our approach characterizes learning dynamics through a formal, causally grounded lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation of algorithmic complexity based on AP -- and demonstrate that it more closely tracks structural changes during training than entropy, consistently exhibiting stronger correlations with training loss across varying model sizes and randomized training runs. These results support the view of training as a process of algorithmic compression, where learning corresponds to the progressive internalization of structured regularities. In doing so, our work offers a principled estimate of learning progression and suggests a framework for complexity-aware learning and regularization, grounded in first principles from information theory, complexity, and computability.         ",
    "url": "https://arxiv.org/abs/2505.20646",
    "authors": [
      "Eduardo Y. Sakabe",
      "Felipe S. Abrah\u00e3o",
      "Alexandre Sim\u00f5es",
      "Esther Colombini",
      "Paula Costa",
      "Ricardo Gudwin",
      "Hector Zenil"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2505.22880",
    "title": "Semantic Exploration and Dense Mapping of Complex Environments using Ground Robot with Panoramic LiDAR-Camera Fusion",
    "abstract": "           This paper presents a system for autonomous semantic exploration and dense semantic target mapping of a complex unknown environment using a ground robot equipped with a LiDAR-panoramic camera suite. Existing approaches often struggle to balance collecting high-quality observations from multiple view angles and avoiding unnecessary repetitive traversal. To fill this gap, we propose a complete system combining mapping and planning. We first redefine the task as completing both geometric coverage and semantic viewpoint observation. We then manage semantic and geometric viewpoints separately and propose a novel Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This enables explicit multi-view semantic inspection and voxel coverage without unnecessary repetition. Building on this, we develop a hierarchical planner to ensure efficient global coverage. In addition, we propose a Safe Aggressive Exploration State Machine, which allows aggressive exploration behavior while ensuring the robot's safety. Our system includes a plug-and-play semantic target mapping module that integrates seamlessly with state-of-the-art SLAM algorithms for pointcloud-level dense semantic target mapping. We validate our approach through extensive experiments in both realistic simulations and complex real-world environments. Simulation results show that our planner achieves faster exploration and shorter travel distances while guaranteeing a specified number of multi-view inspections. Real-world experiments further confirm the system's effectiveness in achieving accurate dense semantic object mapping of unstructured environments.         ",
    "url": "https://arxiv.org/abs/2505.22880",
    "authors": [
      "Xiaoyang Zhan",
      "Shixin Zhou",
      "Qianqian Yang",
      "Yixuan Zhao",
      "Hao Liu",
      "Srinivas Chowdary Ramineni",
      "Kenji Shimada"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.01702",
    "title": "mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection",
    "abstract": "           The large language models (LLMs) are able to generate high-quality texts in multiple languages. Such texts are often not recognizable by humans as generated, and therefore present a potential of LLMs for misuse (e.g., plagiarism, spams, disinformation spreading). An automated detection is able to assist humans to indicate the machine-generated texts; however, its robustness to out-of-distribution data is still challenging. This notebook describes our mdok approach in robust detection, based on fine-tuning smaller LLMs for text classification. It is applied to both subtasks of Voight-Kampff Generative AI Detection 2025, providing remarkable performance (1st rank) in both, the binary detection as well as the multiclass classification of various cases of human-AI collaboration.         ",
    "url": "https://arxiv.org/abs/2506.01702",
    "authors": [
      "Dominik Macko"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.05128",
    "title": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning",
    "abstract": "           Zero-shot Event Detection (ED), the task of identifying event mentions in natural language text without any training data, is critical for document understanding in specialized domains. Understanding the complex event ontology, extracting domain-specific triggers from the passage, and structuring them appropriately overloads and limits the utility of Large Language Models (LLMs) for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent reasoning framework that decouples the task of ED using Dreamer and Grounder. Dreamer encourages divergent reasoning through open-ended event discovery, which helps to boost event coverage. Conversely, Grounder introduces convergent reasoning to align the free-form predictions with the task-specific instructions using finite-state machine guided constrained decoding. Additionally, an LLM-Judge verifies the final outputs to ensure high precision. Through extensive experiments on six datasets across five domains and nine LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot, transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains over the best baseline -- establishing DiCoRe as a strong zero-shot ED framework.         ",
    "url": "https://arxiv.org/abs/2506.05128",
    "authors": [
      "Tanmay Parekh",
      "Kartik Mehta",
      "Ninareh Mehrabi",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.09920",
    "title": "Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering",
    "abstract": "           Hyperspectral image (HSI) clustering assigns similar pixels to the same class without any annotations, which is an important yet challenging task. For large-scale HSIs, most methods rely on superpixel segmentation and perform superpixel-level clustering based on graph neural networks (GNNs). However, existing GNNs cannot fully exploit the spectral information of the input HSI, and the inaccurate superpixel topological graph may lead to the confusion of different class semantics during information aggregation. To address these challenges, we first propose a structural-spectral graph convolutional operator (SSGCO) tailored for graph-structured HSI superpixels to improve their representation quality through the co-extraction of spatial and spectral features. Second, we propose an evidence-guided adaptive edge learning (EGAEL) module that adaptively predicts and refines edge weights in the superpixel topological graph. We integrate the proposed method into a contrastive learning framework to achieve clustering, where representation learning and clustering are simultaneously conducted. Experiments demonstrate that the proposed method improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on four HSI datasets. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.09920",
    "authors": [
      "Jianhan Qi",
      "Yuheng Jia",
      "Hui Liu",
      "Junhui Hou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.11882",
    "title": "An Explainable AI Framework for Dynamic Resource Management in Vehicular Network Slicing",
    "abstract": "           Effective resource management and network slicing are essential to meet the diverse service demands of vehicular networks, including Enhanced Mobile Broadband (eMBB) and Ultra-Reliable and Low-Latency Communications (URLLC). This paper introduces an Explainable Deep Reinforcement Learning (XRL) framework for dynamic network slicing and resource allocation in vehicular networks, built upon a near-real-time RAN intelligent controller. By integrating a feature-based approach that leverages Shapley values and an attention mechanism, we interpret and refine the decisions of our reinforcementlearning agents, addressing key reliability challenges in vehicular communication systems. Simulation results demonstrate that our approach provides clear, real-time insights into the resource allocation process and achieves higher interpretability precision than a pure attention mechanism. Furthermore, the Quality of Service (QoS) satisfaction for URLLC services increased from 78.0% to 80.13%, while that for eMBB services improved from 71.44% to 73.21%.         ",
    "url": "https://arxiv.org/abs/2506.11882",
    "authors": [
      "Haochen Sun",
      "Yifan Liu",
      "Ahmed Al-Tahmeesschi",
      "Swarna Chetty",
      "Syed Ali Raza Zaidi",
      "Avishek Nag",
      "Hamed Ahmadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.01607",
    "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems",
    "abstract": "           The widespread deployment of Deep Learning-based Face Recognition Systems raises multiple security concerns. While prior research has identified backdoor vulnerabilities on isolated components, Backdoor Attacks on real-world, unconstrained pipelines remain underexplored. This paper presents the first comprehensive system-level analysis of Backdoor Attacks targeting Face Recognition Systems and provides three contributions. We first show that face feature extractors trained with large margin metric learning losses are susceptible to Backdoor Attacks. By analyzing 20 pipeline configurations and 15 attack scenarios, we then reveal that a single backdoor can compromise an entire Face Recognition System. Finally, we propose effective best practices and countermeasures for stakeholders.         ",
    "url": "https://arxiv.org/abs/2507.01607",
    "authors": [
      "Quentin Le Roux",
      "Yannick Teglia",
      "Teddy Furon",
      "Philippe Loubet-Moundi",
      "Eric Bourbao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.02585",
    "title": "Scalable Interconnect Learning in Boolean Networks",
    "abstract": "           Learned Differentiable Boolean Logic Networks (DBNs) already deliver efficient inference on resource-constrained hardware. We extend them with a trainable, differentiable interconnect whose parameter count remains constant as input width grows, allowing DBNs to scale to far wider layers than earlier learnable-interconnect designs while preserving their advantageous accuracy. To further reduce model size, we propose two complementary pruning stages: an SAT-based logic equivalence pass that removes redundant gates without affecting performance, and a similarity-based, data-driven pass that outperforms a magnitude-style greedy baseline and offers a superior compression-accuracy trade-off.         ",
    "url": "https://arxiv.org/abs/2507.02585",
    "authors": [
      "Fabian Kresse",
      "Emily Yu",
      "Christoph H. Lampert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2507.05129",
    "title": "SMART: Simulated Students Aligned with Item Response Theory for Question Difficulty Prediction",
    "abstract": "           Item (question) difficulties play a crucial role in educational assessments, enabling accurate and efficient assessment of student abilities and personalization to maximize learning outcomes. Traditionally, estimating item difficulties can be costly, requiring real students to respond to items, followed by fitting an item response theory (IRT) model to get difficulty estimates. This approach cannot be applied to the cold-start setting for previously unseen items either. In this work, we present SMART (Simulated Students Aligned with IRT), a novel method for aligning simulated students with instructed ability, which can then be used in simulations to predict the difficulty of open-ended items. We achieve this alignment using direct preference optimization (DPO), where we form preference pairs based on how likely responses are under a ground-truth IRT model. We perform a simulation by generating thousands of responses, evaluating them with a large language model (LLM)-based scoring model, and fit the resulting data to an IRT model to obtain item difficulty estimates. Through extensive experiments on two real-world student response datasets, we show that SMART outperforms other item difficulty prediction methods by leveraging its improved ability alignment.         ",
    "url": "https://arxiv.org/abs/2507.05129",
    "authors": [
      "Alexander Scarlatos",
      "Nigel Fernandez",
      "Christopher Ormerod",
      "Susan Lottridge",
      "Andrew Lan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.08172",
    "title": "Neural Logic Networks for Interpretable Classification",
    "abstract": "           Traditional neural networks have an impressive classification performance, but what they learn cannot be inspected, verified or extracted. Neural Logic Networks on the other hand have an interpretable structure that enables them to learn a logical mechanism relating the inputs and outputs with AND and OR operations. We generalize these networks with NOT operations and biases that take into account unobserved data and develop a rigorous logical and probabilistic modeling in terms of concept combinations to motivate their use. We also propose a novel factorized IF-THEN rule structure for the model as well as a modified learning algorithm. Our method improves the state-of-the-art in Boolean networks discovery and is able to learn relevant, interpretable rules in tabular classification, notably on examples from the medical and industrial fields where interpretability has tangible value.         ",
    "url": "https://arxiv.org/abs/2508.08172",
    "authors": [
      "Vincent Perreault",
      "Katsumi Inoue",
      "Richard Labib",
      "Alain Hertz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2508.08552",
    "title": "Resource-Aware Aggregation and Sparsification in Heterogeneous Ensemble Federated Learning",
    "abstract": "           Federated learning (FL) enables distributed training with private client data, but its convergence is hindered by system heterogeneity under realistic communication scenarios. Most FL schemes addressing system heterogeneity utilize global pruning or ensemble distillation, yet often overlook typical constraints required for communication efficiency. Meanwhile, deep ensembles can aggregate predictions from individually trained models to improve performance, but current ensemble-based FL methods fall short in fully capturing diversity of model predictions. In this work, we propose \\textbf{SHEFL}, a global ensemble-based FL framework suited for clients with diverse computational capacities. We allocate different numbers of global models to clients based on their available resources. We introduce a novel aggregation scheme that mitigates the training bias between clients and dynamically adjusts the sparsification ratio across clients to reduce the computational burden of training deep ensembles. Extensive experiments demonstrate that our method effectively addresses computational heterogeneity, significantly improving accuracy and stability compared to existing approaches.         ",
    "url": "https://arxiv.org/abs/2508.08552",
    "authors": [
      "Keumseo Ryum",
      "Jinu Gong",
      "Joonhyuk Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.15454",
    "title": "Mini-Batch Robustness Verification of Deep Neural Networks",
    "abstract": "           Neural network image classifiers are ubiquitous in many safety-critical applications. However, they are susceptible to adversarial attacks. To understand their robustness to attacks, many local robustness verifiers have been proposed to analyze $\\epsilon$-balls of inputs. Yet, existing verifiers introduce a long analysis time or lose too much precision, making them less effective for a large set of inputs. In this work, we propose a new approach to local robustness: group local robustness verification. The key idea is to leverage the similarity of the network computations of certain $\\epsilon$-balls to reduce the overall analysis time. We propose BaVerLy, a sound and complete verifier that boosts the local robustness verification of a set of $\\epsilon$-balls by dynamically constructing and verifying mini-batches. BaVerLy adaptively identifies successful mini-batch sizes, accordingly constructs mini-batches of $\\epsilon$-balls that have similar network computations, and verifies them jointly. If a mini-batch is verified, all its $\\epsilon$-balls are proven robust. Otherwise, one $\\epsilon$-ball is suspected as not being robust, guiding the refinement. BaVerLy leverages the analysis results to expedite the analysis of that $\\epsilon$-ball as well as the analysis of the mini-batch with the other $\\epsilon$-balls. We evaluate BaVerLy on fully connected and convolutional networks for MNIST and CIFAR-10. Results show that BaVerLy scales the common one by one verification by 2.3x on average and up to 4.1x, in which case it reduces the total analysis time from 24 hours to 6 hours.         ",
    "url": "https://arxiv.org/abs/2508.15454",
    "authors": [
      "Saar Tzour-Shaday",
      "Dana Drachsler-Cohen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2508.15499",
    "title": "Let's Grow an Unbiased Community: Guiding the Fairness of Graphs via New Links",
    "abstract": "           Graph Neural Networks (GNNs) have achieved remarkable success across diverse applications. However, due to the biases in the graph structures, graph neural networks face significant challenges in fairness. Although the original user graph structure is generally biased, it is promising to guide these existing structures toward unbiased ones by introducing new links. The fairness guidance via new links could foster unbiased communities, thereby enhancing fairness in downstream applications. To address this issue, we propose a novel framework named FairGuide. Specifically, to ensure fairness in downstream tasks trained on fairness-guided graphs, we introduce a differentiable community detection task as a pseudo downstream task. Our theoretical analysis further demonstrates that optimizing fairness within this pseudo task effectively enhances structural fairness, promoting fairness generalization across diverse downstream applications. Moreover, FairGuide employs an effective strategy which leverages meta-gradients derived from the fairness-guidance objective to identify new links that significantly enhance structural fairness. Extensive experimental results demonstrate the effectiveness and generalizability of our proposed method across a variety of graph-based fairness tasks.         ",
    "url": "https://arxiv.org/abs/2508.15499",
    "authors": [
      "Jiahua Lu",
      "Huaxiao Liu",
      "Shuotong Bai",
      "Junjie Xu",
      "Renqiang Luo",
      "Enyan Dai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.18106",
    "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code",
    "abstract": "           The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks often lack relevance to real-world AI-assisted programming scenarios, making them inadequate for assessing the practical security risks associated with AI-generated code in production environments. To address this gap, we introduce A.S.E (AI Code Generation Security Evaluation), a repository-level evaluation benchmark designed to closely mirror real-world AI programming tasks, offering a comprehensive and reliable framework for assessing the security of AI-generated code. Our evaluation of leading LLMs on A.S.E reveals several key findings. In particular, current LLMs still struggle with secure coding. The complexity in repository-level scenarios presents challenges for LLMs that typically perform well on snippet-level tasks. Moreover, a larger reasoning budget does not necessarily lead to better code generation. These observations offer valuable insights into the current state of AI code generation and help developers identify the most suitable models for practical tasks. They also lay the groundwork for refining LLMs to generate secure and efficient code in real-world applications.         ",
    "url": "https://arxiv.org/abs/2508.18106",
    "authors": [
      "Keke Lian",
      "Bin Wang",
      "Lei Zhang",
      "Libo Chen",
      "Junjie Wang",
      "Ziming Zhao",
      "Yujiu Yang",
      "Miaoqian Lin",
      "Haotong Duan",
      "Haoran Zhao",
      "Shuang Liao",
      "Mingda Guo",
      "Jiazheng Quan",
      "Yilu Zhong",
      "Chenhao He",
      "Zichuan Chen",
      "Jie Wu",
      "Haoling Li",
      "Zhaoxuan Li",
      "Jiongchi Yu",
      "Hui Li",
      "Dong Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.00033",
    "title": "Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary",
    "abstract": "           This is a research exploring existing models and fine tuning them to combine a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to predict the recipe and generate text creating a step by step guide for the cooking procedure. All the data were gathered by the author for a robust task specific system to perform best in complex and challenging environments proving the extension and endless application of computer vision in daily activities such as kitchen work. This work extends the field for many more crucial task of our day to day life.         ",
    "url": "https://arxiv.org/abs/2509.00033",
    "authors": [
      "Tahoshin Alam Ishat",
      "Mohammad Abdul Qayum"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.02967",
    "title": "AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for Time Series Forecasting",
    "abstract": "           Traditional neural networks struggle to capture the spectral structure of complex signals. Fourier neural networks (FNNs) attempt to address this by embedding Fourier series components, yet many real-world signals are almost-periodic with non-commensurate frequencies, posing additional challenges. Building on prior work showing that ARIMA outperforms large language models (LLMs) for forecasting, we extend the comparison to neural predictors and find ARIMA still superior. We therefore propose the Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network (AR-KAN), which integrates a pre-trained AR module for temporal memory with a KAN for nonlinear representation. The AR module preserves essential temporal features while reducing redundancy. Experiments demonstrate that AR-KAN matches ARIMA on almost-periodic functions and achieves the best results on $72\\%$ of Rdatasets series, with a clear advantage on data with periodic structure. These results highlight AR-KAN as a robust and effective framework for time series forecasting.         ",
    "url": "https://arxiv.org/abs/2509.02967",
    "authors": [
      "Chen Zeng",
      "Tiehang Xu",
      "Qiao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2509.05796",
    "title": "Dual-Mode Deep Anomaly Detection for Medical Manufacturing: Structural Similarity and Feature Distance",
    "abstract": "           Automated visual inspection in medical device manufacturing faces unique challenges, including small and imbalanced datasets, high-resolution imagery, and strict regulatory requirements. To address these, we propose two attention-guided autoencoder architectures for deep anomaly detection. The first employs a structural similarity-based scoring approach that enables lightweight, real-time defect detection with unsupervised thresholding and can be further enhanced through limited supervised tuning. The second applies a feature distance-based strategy using Mahalanobis scoring on reduced latent features, designed to monitor distributional shifts and support supervisory oversight. Evaluations on a representative sterile packaging dataset confirm that both approaches outperform baselines under hardware-constrained, regulated conditions. Cross-domain testing on the MVTec-Zipper benchmark further demonstrates that the structural similarity-based method generalises effectively and achieves performance comparable to state-of-the-art methods, while the feature distance-based method is less transferable but provides complementary monitoring capabilities. These results highlight a dual-pathway inspection strategy: structural similarity for robust inline detection and feature distance for supervisory monitoring. By combining operational performance with interpretability and lifecycle monitoring, the proposed methods also align with emerging regulatory expectations for high-risk AI systems.         ",
    "url": "https://arxiv.org/abs/2509.05796",
    "authors": [
      "Julio Zanon Diaz",
      "Georgios Siogkas",
      "Peter Corcoran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.08661",
    "title": "Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network",
    "abstract": "           Isolated Sign Language Recognition (ISLR) is challenged by gestures that are morphologically similar yet semantically distinct, a problem rooted in the complex interplay between hand shape and motion trajectory. Existing methods, often relying on a single reference frame, struggle to resolve this geometric ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a dual-reference, dual-stream architecture that decouples and models gesture morphology and trajectory in separate, complementary coordinate systems. The architecture processes these streams through specialized networks: a topology-aware graph convolution models the view-invariant shape from a wrist-centric frame, while a Finsler geometry-based encoder captures the context-aware trajectory from a facial-centric frame. These features are then integrated via a geometry-driven optimal transport fusion mechanism. DSLNet sets a new state-of-the-art, achieving 93.70%, 89.97%, and 99.79% accuracy on the challenging WLASL-100, WLASL-300, and LSA64 datasets, respectively, with significantly fewer parameters than competing models.         ",
    "url": "https://arxiv.org/abs/2509.08661",
    "authors": [
      "Liangjin Liu",
      "Haoyang Zheng",
      "Zhengzhong Zhu",
      "Pei Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.09723",
    "title": "ALIGNS: Unlocking nomological networks in psychological measurement through a large language model",
    "abstract": "           Psychological measurement is critical to many disciplines. Despite advances in measurement, building nomological networks, theoretical maps of how concepts and measures relate to establish validity, remains a challenge 70 years after Cronbach and Meehl proposed them as fundamental to validation. This limitation has practical consequences: clinical trials may fail to detect treatment effects, and public policy may target the wrong outcomes. We introduce Analysis of Latent Indicators to Generate Nomological Structures (ALIGNS), a large language model-based system trained with validated questionnaire measures. ALIGNS provides three comprehensive nomological networks containing over 550,000 indicators across psychology, medicine, social policy, and other fields. This represents the first application of large language models to solve a foundational problem in measurement validation. We report classification accuracy tests used to develop the model, as well as three evaluations. In the first evaluation, the widely used NIH PROMIS anxiety and depression instruments are shown to converge into a single dimension of emotional distress. The second evaluation examines child temperament measures and identifies four potential dimensions not captured by current frameworks, and questions one existing dimension. The third evaluation, an applicability check, engages expert psychometricians who assess the system's importance, accessibility, and suitability. ALIGNS is freely available at this http URL, complementing traditional validation methods with large-scale nomological analysis.         ",
    "url": "https://arxiv.org/abs/2509.09723",
    "authors": [
      "Kai R. Larsen",
      "Sen Yan",
      "Roland M. Mueller",
      "Lan Sang",
      "Mikko R\u00f6nkk\u00f6",
      "Ravi Starzl",
      "Donald Edmondson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2509.11686",
    "title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models",
    "abstract": "           Code Large Language Models (Code LLMs) have opened a new era in programming with their impressive capabilities. However, recent research has revealed critical limitations in their ability to reason about runtime behavior and understand the actual functionality of programs, which poses significant challenges for their post-training and practical deployment. Specifically, Code LLMs encounter two principal issues: (1) a lack of proficiency in reasoning about program execution behavior, as they struggle to interpret what programs actually do during runtime, and (2) the inconsistent and fragmented representation of semantic information, such as execution traces, across existing methods, which hinders their ability to generalize and reason effectively. These challenges underscore the necessity for more systematic approaches to enhance the reasoning capabilities of Code LLMs. To address these issues, we introduce a generic framework to support integrating semantic information~(e.g., execution trace) to code task-relevant prompts, and conduct a comprehensive study to explore the role of semantic information in enhancing the reasoning ability of Code LLMs accordingly. Specifically, we focus on investigating the usefulness of trace-based semantic information in boosting supervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The experimental results surprisingly disagree with previous works and demonstrate that semantic information has limited usefulness for SFT and test time scaling of Code LLM.         ",
    "url": "https://arxiv.org/abs/2509.11686",
    "authors": [
      "Jian Wang",
      "Xiaofei Xie",
      "Qiang Hu",
      "Shangqing Liu",
      "Yi Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.12629",
    "title": "Ensembling Large Language Models for Code Vulnerability Detection: An Empirical Evaluation",
    "abstract": "           Code vulnerability detection is crucial for ensuring the security and reliability of modern software systems. Recently, Large Language Models (LLMs) have shown promising capabilities in this domain. However, notable discrepancies in detection results often arise when analyzing identical code segments across different training stages of the same model or among architecturally distinct LLMs. While such inconsistencies may compromise detection stability, they also highlight a key opportunity: the latent complementarity among models can be harnessed through ensemble learning to create more robust vulnerability detection systems. In this study, we explore the potential of ensemble learning to enhance the performance of LLMs in source code vulnerability detection. We conduct comprehensive experiments involving five LLMs (i.e., DeepSeek-Coder-6.7B, CodeLlama-7B, CodeLlama-13B, CodeQwen1.5-7B, and StarCoder2-15B), using three ensemble strategies (i.e., Bagging, Boosting, and Stacking). These experiments are carried out across three widely adopted datasets (i.e., Devign, ReVeal, and BigVul). Inspired by Mixture of Experts (MoE) techniques, we further propose Dynamic Gated Stacking (DGS), a Stacking variant tailored for vulnerability detection. Our results demonstrate that ensemble approaches can significantly improve detection performance, with Boosting excelling in scenarios involving imbalanced datasets. Moreover, DGS consistently outperforms traditional Stacking, particularly in handling class imbalance and multi-class classification tasks. These findings offer valuable insights into building more reliable and effective LLM-based vulnerability detection systems through ensemble learning.         ",
    "url": "https://arxiv.org/abs/2509.12629",
    "authors": [
      "Zhihong Sun",
      "Jia Li",
      "Yao Wan",
      "Chuanyi Li",
      "Hongyu Zhang",
      "Zhi jin",
      "Ge Li",
      "Hong Liu",
      "Chen Lyu",
      "Songlin Hu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.12995",
    "title": "Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection",
    "abstract": "           While specialized detectors for AI-generated images excel on curated benchmarks, they fail catastrophically in real-world scenarios, as evidenced by their critically high false-negative rates on `in-the-wild' benchmarks. Instead of crafting another specialized `knife' for this problem, we bring a `gun' to the fight: a simple linear classifier on a modern Vision Foundation Model (VFM). Trained on identical data, this baseline decisively `outguns' bespoke detectors, boosting in-the-wild accuracy by a striking margin of over 20\\%. Our analysis pinpoints the source of the VFM's `firepower': First, by probing text-image similarities, we find that recent VLMs (e.g., Perception Encoder, Meta CLIP2) have learned to align synthetic images with forgery-related concepts (e.g., `AI-generated'), unlike previous versions. Second, we speculate that this is due to data exposure, as both this alignment and overall accuracy plummet on a novel dataset scraped after the VFM's pre-training cut-off date, ensuring it was unseen during pre-training. Our findings yield two critical conclusions: 1) For the real-world `gunfight' of AI-generated image detection, the raw `firepower' of an updated VFM is far more effective than the `craftsmanship' of a static detector. 2) True generalization evaluation requires test data to be independent of the model's entire training history, including pre-training.         ",
    "url": "https://arxiv.org/abs/2509.12995",
    "authors": [
      "Yue Zhou",
      "Xinan He",
      "Kaiqing Lin",
      "Bing Fan",
      "Feng Ding",
      "Jinhua Zeng",
      "Bin Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.13214",
    "title": "End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection",
    "abstract": "           The powerful generative capabilities of diffusion models have significantly advanced the field of image synthesis, enhancing both full image generation and inpainting-based image editing. Despite their remarkable advancements, diffusion models also raise concerns about potential misuse for malicious purposes. However, existing approaches struggle to identify images generated by diffusion-based inpainting models, even when similar inpainted images are included in their training data. To address this challenge, we propose a novel detection method based on End-to-end denoising diffusion (End4). Specifically, End4 designs a denoising reconstruction model to improve the alignment degree between the latent spaces of the reconstruction and detection processes, thus reconstructing features that are more conducive to detection. Meanwhile, it leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local image features under the guidance of attention pyramid layers at different scales, enhancing feature discriminability. Additionally, to evaluate detection performance on inpainted images, we establish a comprehensive benchmark comprising images generated from five distinct masked regions. Extensive experiments demonstrate that our End4 effectively generalizes to unseen masking patterns and remains robust under various perturbations. Our code and dataset will be released soon.         ",
    "url": "https://arxiv.org/abs/2509.13214",
    "authors": [
      "Fei Wang",
      "Xuecheng Wu",
      "Zheng Zhang",
      "Danlei Huang",
      "Yuheng Huang",
      "Bo Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.13230",
    "title": "Fast Unbiased Sampling of Networks with Given Expected Degrees and Strengths",
    "abstract": "           The configuration model is a cornerstone of statistical assessment of network structure. While the Chung-Lu model is among the most widely used configuration models, it systematically oversamples edges between large-degree nodes, leading to inaccurate statistical conclusions. Although the maximum entropy principle offers unbiased configuration models, its high computational cost has hindered widespread adoption, making the Chung-Lu model an inaccurate yet persistently practical choice. Here, we propose fast and efficient sampling algorithms for the max-entropy-based models by adapting the Miller-Hagberg algorithm. Evaluation on 103 empirical networks demonstrates 10-1000 times speedup, making theoretically rigorous configuration models practical and contributing to a more accurate understanding of network structure.         ",
    "url": "https://arxiv.org/abs/2509.13230",
    "authors": [
      "Xuanchi Li",
      "Xin Wang",
      "Sadamori Kojaku"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2509.13425",
    "title": "Unified Spatiotemporal Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics",
    "abstract": "           Ecological systems exhibit complex multi-scale dynamics that challenge traditional modeling. New methods must capture temporal oscillations and emergent spatiotemporal patterns while adhering to conservation principles. We present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework, a deep learning architecture integrating physics-informed neural networks (PINNs) and conservation laws to model predator-prey dynamics across dimensional scales. The framework provides a unified solution for both ordinary (ODE) and partial (PDE) differential equation systems, describing temporal cycles and reaction-diffusion patterns within a single neural network architecture. Our methodology uses automatic differentiation to enforce physics constraints and adaptive loss weighting to balance data fidelity with physical consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9% correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94). Validation confirms conservation law adherence within 0.5% and shows a 10-50x computational speedup for inference compared to numerical solvers. USPIL also enables mechanistic understanding through interpretable physics constraints, facilitating parameter discovery and sensitivity analysis not possible with purely data-driven methods. Its ability to transition between dimensional formulations opens new avenues for multi-scale ecological modeling. These capabilities make USPIL a transformative tool for ecological forecasting, conservation planning, and understanding ecosystem resilience, establishing physics-informed deep learning as a powerful and scientifically rigorous paradigm.         ",
    "url": "https://arxiv.org/abs/2509.13425",
    "authors": [
      "Julian Evan Chrisnanto",
      "Yulison Herry Chrisnanto",
      "Ferry Faizal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applied Physics (physics.app-ph)"
    ]
  },
  {
    "id": "arXiv:2509.13484",
    "title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes",
    "abstract": "           Understanding group-level social interactions in public spaces is crucial for urban planning, informing the design of socially vibrant and inclusive environments. Detecting such interactions from images involves interpreting subtle visual cues such as relations, proximity, and co-movement - semantically complex signals that go beyond traditional object detection. To address this challenge, we introduce a social group region detection task, which requires inferring and spatially grounding visual regions defined by abstract interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf human detection and depth estimation, (2) VLM-based reasoning to classify pairwise social affiliation, and (3) a lightweight spatial aggregation algorithm to localize socially connected groups. To support this task and encourage future research, we present a new dataset of 100K urban street-view images annotated with bounding boxes and labels for both individuals and socially interacting groups. The annotations combine human-created labels and outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage of real-world scenarios.         ",
    "url": "https://arxiv.org/abs/2509.13484",
    "authors": [
      "Liu Liu",
      "Alexandra Kudaeva",
      "Marco Cipriano",
      "Fatimeh Al Ghannam",
      "Freya Tan",
      "Gerard de Melo",
      "Andres Sevtsuk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2509.13853",
    "title": "Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection",
    "abstract": "           Unsupervised anomalous sound detection aims to detect unknown anomalous sounds by training a model using only normal audio data. Despite advancements in self-supervised methods, the issue of frequent false alarms when handling samples of the same type from different machines remains unresolved. This paper introduces a novel training technique called one-stage supervised contrastive learning (OS-SCL), which significantly addresses this problem by perturbing features in the embedding space and employing a one-stage noisy supervised contrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved 94.64\\% AUC, 88.42\\% pAUC, and 89.24\\% mAUC using only Log-Mel features. Additionally, a time-frequency feature named TFgram is proposed, which is extracted from raw audio. This feature effectively captures critical information for anomalous sound detection, ultimately achieving 95.71\\% AUC, 90.23\\% pAUC, and 91.23\\% mAUC. The source code is available at: \\underline{this http URL}.         ",
    "url": "https://arxiv.org/abs/2509.13853",
    "authors": [
      "Shun Huang",
      "Zhihua Fang",
      "Liang He"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.14084",
    "title": "AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration",
    "abstract": "           Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary novel categories, offering a scalable and annotation-efficient solution. Traditionally, most ZSAD works have been based on the CLIP model, which performs anomaly detection by calculating the similarity between visual and text embeddings. Recently, vision foundation models such as DINOv3 have demonstrated strong transferable representation capabilities. In this work, we are the first to adapt DINOv3 for ZSAD. However, this adaptation presents two key challenges: (i) the domain bias between large-scale pretraining data and anomaly detection tasks leads to feature misalignment; and (ii) the inherent bias toward global semantics in pretrained representations often leads to subtle anomalies being misinterpreted as part of the normal foreground objects, rather than being distinguished as abnormal regions. To overcome these challenges, we introduce AD-DINOv3, a novel vision-language multimodal framework designed for ZSAD. Specifically, we formulate anomaly detection as a multimodal contrastive learning problem, where DINOv3 is employed as the visual backbone to extract patch tokens and a CLS token, and the CLIP text encoder provides embeddings for both normal and abnormal prompts. To bridge the domain gap, lightweight adapters are introduced in both modalities, enabling their representations to be recalibrated for the anomaly detection task. Beyond this baseline alignment, we further design an Anomaly-Aware Calibration Module (AACM), which explicitly guides the CLS token to attend to anomalous regions rather than generic foreground semantics, thereby enhancing discriminability. Extensive experiments on eight industrial and medical benchmarks demonstrate that AD-DINOv3 consistently matches or surpasses state-of-the-art this http URL code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.14084",
    "authors": [
      "Jingyi Yuan",
      "Jianxiong Ye",
      "Wenkang Chen",
      "Chenqiang Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.07939",
    "title": "Media Bias and Polarization through the Lens of a Markov Switching Latent Space Network Model",
    "abstract": "           News outlets are now more than ever incentivized to provide their audience with slanted news, while the intrinsic homophilic nature of online social media may exacerbate polarized opinions. Here, we propose a new dynamic latent space model for time-varying online audience-duplication networks, which exploits social media content to conduct inference on media bias and polarization of news outlets. We contribute to the literature in several directions: 1) Our model provides a novel measure of media bias that combines information from both network data and text-based indicators; 2) we endow our model with Markov-Switching dynamics to capture polarization regimes while maintaining a parsimonious specification; 3) we contribute to the literature on the statistical properties of latent space network models. The proposed model is applied to a set of data on the online activity of national and local news outlets from four European countries in the years 2015 and 2016. We find evidence of a strong positive correlation between our media slant measure and a well-grounded external source of media bias. In addition, we provide insight into the polarization regimes across the four countries considered.         ",
    "url": "https://arxiv.org/abs/2306.07939",
    "authors": [
      "Roberto Casarin",
      "Antonio Peruzzi",
      "Mark F.J. Steel"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2403.15243",
    "title": "Robust Utility Optimization via a GAN Approach",
    "abstract": "           Robust utility optimization enables an investor to deal with market uncertainty in a structured way, with the goal of maximizing the worst-case outcome. In this work, we propose a generative adversarial network (GAN) approach to (approximately) solve robust utility optimization problems in general and realistic settings. In particular, we model both the investor and the market by neural networks (NN) and train them in a mini-max zero-sum game. This approach is applicable for any continuous utility function and in realistic market settings with trading costs, where only observable information of the market can be used. A large empirical study shows the versatile usability of our method. Whenever an optimal reference strategy is available, our method performs on par with it and in the (many) settings without known optimal strategy, our method outperforms all other reference strategies. Moreover, we can conclude from our study that the trained path-dependent strategies do not outperform Markovian ones. Lastly, we uncover that our generative approach for learning optimal, (non-) robust investments under trading costs generates universally applicable alternatives to well known asymptotic strategies of idealized settings.         ",
    "url": "https://arxiv.org/abs/2403.15243",
    "authors": [
      "Florian Krach",
      "Josef Teichmann",
      "Hanna Wutte"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Machine Learning (cs.LG)",
      "Mathematical Finance (q-fin.MF)",
      "Portfolio Management (q-fin.PM)"
    ]
  },
  {
    "id": "arXiv:2409.16311",
    "title": "New Insights into Global Warming: End-to-End Visual Analysis and Prediction of Temperature Variations",
    "abstract": "           Global warming presents an unprecedented challenge to our planet however comprehensive understanding remains hindered by geographical biases temporal limitations and lack of standardization in existing research. An end to end visual analysis of global warming using three distinct temperature datasets is presented. A baseline adjusted from the Paris Agreements one point five degrees Celsius benchmark based on data analysis is employed. A closed loop design from visualization to prediction and clustering is created using classic models tailored to the characteristics of the data. This approach reduces complexity and eliminates the need for advanced feature engineering. A lightweight convolutional neural network and long short term memory model specifically designed for global temperature change is proposed achieving exceptional accuracy in long term forecasting with a mean squared error of three times ten to the power of negative six and an R squared value of zero point nine nine nine nine. Dynamic time warping and KMeans clustering elucidate national level temperature anomalies and carbon emission patterns. This comprehensive method reveals intricate spatiotemporal characteristics of global temperature variations and provides warming trend attribution. The findings offer new insights into climate change dynamics demonstrating that simplicity and precision can coexist in environmental analysis.         ",
    "url": "https://arxiv.org/abs/2409.16311",
    "authors": [
      "Meihua Zhou",
      "Nan Wan",
      "Tianlong Zheng",
      "Hanwen Xu",
      "Li Yang",
      "Tingting Wang"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Human-Computer Interaction (cs.HC)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2501.05310",
    "title": "A Large-Scale Probing Analysis of Speaker-Specific Attributes in Self-Supervised Speech Representations",
    "abstract": "           Speech self-supervised learning (SSL) models are known to learn hierarchical representations, yet how they encode different speaker-specific attributes remains under-explored. This study investigates the layer-wise disentanglement of speaker information across multiple speech SSL model families and their variants. Drawing from phonetic frameworks, we conduct a large-scale probing analysis of attributes categorised into functional groups: Acoustic (Gender), Prosodic (Pitch, Tempo, Energy), and Paralinguistic (Emotion), which we use to deconstruct the model's representation of Speaker Identity. Our findings validate a consistent three-stage hierarchy: initial layers encode fundamental timbre and prosody; middle layers synthesise abstract traits; and final layers suppress speaker identity to abstract linguistic content. An ablation study shows that while specialised speaker embeddings excel at identifying speaker identity, the intermediate layers of speech SSL models better represent dynamic prosody. This work is the first large-scale study covering a wide range of speech SSL model families and variants with fine-grained speaker-specific attributes on how they hierarchically separate the dynamic style of speech from its intrinsic characteristics, offering practical implications for downstream tasks.         ",
    "url": "https://arxiv.org/abs/2501.05310",
    "authors": [
      "Aemon Yat Fei Chiu",
      "Kei Ching Fung",
      "Roger Tsz Yeung Li",
      "Jingyu Li",
      "Tan Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2502.14401",
    "title": "MedFuncta: A Unified Framework for Learning Efficient Medical Neural Fields",
    "abstract": "           Research in medical imaging primarily focuses on discrete data representations that poorly scale with grid resolution and fail to capture the often continuous nature of the underlying signal. Neural Fields (NFs) offer a powerful alternative by modeling data as continuous functions. While single-instance NFs have successfully been applied in medical contexts, extending them to large-scale medical datasets remains an open challenge. We therefore introduce MedFuncta, a unified framework for large-scale NF training on diverse medical signals. Building on Functa, our approach encodes data into a unified representation, namely a 1D latent vector, that modulates a shared, meta-learned NF, enabling generalization across a dataset. We revisit common design choices, introducing a non-constant frequency parameter $\\omega$ in widely used SIREN activations, and establish a connection between this $\\omega$-schedule and layer-wise learning rates, relating our findings to recent work in theoretical learning dynamics. We additionally introduce a scalable meta-learning strategy for shared network learning that employs sparse supervision during training, thereby reducing memory consumption and computational overhead while maintaining competitive performance. Finally, we evaluate MedFuncta across a diverse range of medical datasets and show how to solve relevant downstream tasks on our neural data representation. To promote further research in this direction, we release our code, model weights and the first large-scale dataset - MedNF - containing > 500 k latent vectors for multi-instance medical NFs.         ",
    "url": "https://arxiv.org/abs/2502.14401",
    "authors": [
      "Paul Friedrich",
      "Florentin Bieder",
      "Julian McGinnis",
      "Julia Wolleb",
      "Daniel Rueckert",
      "Philippe C. Cattin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12044",
    "title": "A Simple and Explainable Model for Park-and-Ride Car Park Occupancy Prediction",
    "abstract": "           In a scenario of growing usage of park-and-ride facilities, understanding and predicting car park occupancy is becoming increasingly important. This study presents a model that effectively captures the occupancy patterns of park-and-ride car parks for commuters using truncated normal distributions for vehicle arrival and departure times. The objective is to develop a predictive model with minimal parameters corresponding to commuter behaviour, enabling the estimation of parking saturation and unfulfilled demand. The proposed model successfully identifies the regular, periodic nature of commuter parking behaviour, where vehicles arrive in the morning and depart in the afternoon. It operates using aggregate data, eliminating the need for individual tracking of arrivals and departures. The model's predictive and now-casting capabilities are demonstrated through real-world data from car parks in the Barcelona Metropolitan Area. A simple model extension furthermore enables the prediction of when a car park will reach its occupancy limit and estimates the additional spaces required to accommodate such excess demand. Thus, beyond forecasting, the model serves as a valuable tool for evaluating interventions, such as expanding parking capacity, to optimize park-and-ride facilities.         ",
    "url": "https://arxiv.org/abs/2503.12044",
    "authors": [
      "Andreas Kaltenbrunner",
      "Josep Ferrer",
      "David Moreno",
      "Vicen\u00e7 G\u00f3mez"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Emerging Technologies (cs.ET)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2504.02373",
    "title": "HPGN: Hybrid Priors-Guided Network for Compressed Low-Light Image Enhancement",
    "abstract": "           In practical applications, low-light images are often compressed for efficient storage and transmission. Most existing methods disregard compression artifacts removal or hardly establish a unified framework for joint task enhancement of low-light images with varying compression qualities. To address this problem, we propose a hybrid priors-guided network (HPGN) that enhances compressed low-light images by integrating both compression and illumination priors. Our approach fully utilizes the JPEG quality factor (QF) and DCT quantization matrix to guide the design of efficient plug-and-play modules for joint tasks. Additionally, we employ a random QF generation strategy to guide model training, enabling a single model to enhance low-light images with different compression levels. Experimental results demonstrate the superiority of our proposed method..         ",
    "url": "https://arxiv.org/abs/2504.02373",
    "authors": [
      "Hantang Li",
      "Qiang Zhu",
      "Xiandong Meng",
      "Lei Xiong",
      "Shuyuan Zhu",
      "Xiaopeng Fan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.13253",
    "title": "Life Finds A Way: Emergence of Cooperative Structures in Adaptive Threshold Networks",
    "abstract": "           There has been a long debate on how new levels of organization have evolved. It might seem unlikely, as cooperation must prevail over competition. One well-studied example is the emergence of autocatalytic sets, which seem to be a prerequisite for the evolution of life. Using a simple model, we investigate how varying bias toward cooperation versus antagonism shapes network dynamics, revealing that higher-order organization emerges even amid pervasive antagonistic interactions. In general, we observe that a quantitative increase in the number of elements in a system leads to a qualitative transition. We present a random threshold-directed network model that integrates node-specific traits with dynamic edge formation and node removal, simulating arbitrary levels of cooperation and competition. In our framework, intrinsic node values determine directed links through various threshold rules. Our model generates a multi-digraph with signed edges (reflecting support/antagonism, labeled ``help''/``harm''), which ultimately yields two parallel yet interdependent threshold graphs. Incorporating temporal growth and node turnover in our approach allows exploration of the evolution, adaptation, and potential collapse of communities and reveals phase transitions in both connectivity and resilience. Our findings extend classical random threshold and Erd\u0151s-R\u00e9nyi models, offering new insights into adaptive systems in biological and economic contexts, with emphasis on the application to Collective Affordance Sets. This framework should also be useful for making predictions that will be tested by ongoing experiments of microbial communities in soil.         ",
    "url": "https://arxiv.org/abs/2507.13253",
    "authors": [
      "Sean P. Maley",
      "Carlos Gershenson",
      "Stuart A. Kauffman"
    ],
    "subjectives": [
      "Populations and Evolution (q-bio.PE)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.00091",
    "title": "Provable Non-Convex Euclidean Distance Matrix Completion: Geometry, Reconstruction, and Robustness",
    "abstract": "           The problem of recovering the configuration of points from their partial pairwise distances, referred to as the Euclidean Distance Matrix Completion (EDMC) problem, arises in a broad range of applications, including sensor network localization, molecular conformation, and manifold learning. In this paper, we propose a Riemannian optimization framework for solving the EDMC problem by formulating it as a low-rank matrix completion task over the space of positive semi-definite Gram matrices. The available distance measurements are encoded as expansion coefficients in a non-orthogonal basis, and optimization over the Gram matrix implicitly enforces geometric consistency through nonnegativity and the triangle inequality, a structure inherited from classical multidimensional scaling. Under a Bernoulli sampling model for observed distances, we prove that Riemannian gradient descent on the manifold of rank-$r$ matrices locally converges linearly with high probability when the sampling probability satisfies $p\\geq O(\\nu^2 r^2\\log(n)/n)$, where $\\nu$ is an EDMC-specific incoherence parameter. Furthermore, we provide an initialization candidate using a one-step hard thresholding procedure that yields convergence, provided the sampling probability satisfies $p \\geq O(\\nu r^{3/2}\\log^{3/4}(n)/n^{1/4})$. A key technical contribution of this work is the analysis of a symmetric linear operator arising from a dual basis expansion in the non-orthogonal basis, which requires a novel application of the Hanson-Wright inequality to establish an optimal restricted isometry property in the presence of coupled terms. Empirical evaluations on synthetic data demonstrate that our algorithm achieves competitive performance relative to state-of-the-art methods. Moreover, we provide a geometric interpretation of matrix incoherence tailored to the EDMC setting and provide robustness guarantees for our method.         ",
    "url": "https://arxiv.org/abs/2508.00091",
    "authors": [
      "Chandler Smith",
      "HanQin Cai",
      "Abiy Tasissa"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Geometry (cs.CG)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.12063",
    "title": "Generalized invariants meet constitutive neural networks: A novel framework for hyperelastic materials",
    "abstract": "           The major challenge in determining a hyperelastic model for a given material is the choice of invariants and the selection how the strain energy function depends functionally on these invariants. Here we introduce a new data-driven framework that simultaneously discovers appropriate invariants and constitutive models for isotropic incompressible hyperelastic materials. Our approach identifies both the most suitable invariants in a class of generalized invariants and the corresponding strain energy function directly from experimental observations. Unlike previous methods that rely on fixed invariant choices or sequential fitting procedures, our method integrates the discovery process into a single neural network architecture. By looking at a continuous family of possible invariants, the model can flexibly adapt to different material behaviors. We demonstrate the effectiveness of this approach using popular benchmark datasets for rubber and brain tissue. For rubber, the method recovers a stretch-dominated formulation consistent with classical models. For brain tissue, it identifies a formulation sensitive to small stretches, capturing the nonlinear shear response characteristic of soft biological matter. Compared to traditional and neural-network-based models, our framework provides improved predictive accuracy and interpretability across a wide range of deformation states. This unified strategy offers a robust tool for automated and physically meaningful model discovery in hyperelasticity.         ",
    "url": "https://arxiv.org/abs/2508.12063",
    "authors": [
      "Denisa Martonov\u00e1",
      "Alain Goriely",
      "Ellen Kuhl"
    ],
    "subjectives": [
      "Soft Condensed Matter (cond-mat.soft)",
      "Artificial Intelligence (cs.AI)"
    ]
  }
]