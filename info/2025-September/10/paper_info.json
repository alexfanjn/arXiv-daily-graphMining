[
  {
    "id": "arXiv:2509.06975",
    "title": "GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning",
    "abstract": "           Self-supervised learning (SSL) has shown great promise in graph representation learning. However, most existing graph SSL methods are developed and evaluated under a single-dataset setting, leaving their cross-dataset transferability largely unexplored and limiting their ability to leverage knowledge transfer and large-scale pretraining, factors that are critical for developing generalized intelligence beyond fitting training data. To address this gap and advance foundation model research for graphs, we present GSTBench, the first systematic benchmark for evaluating the transferability of graph SSL methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate five representative SSL methods across a diverse set of target graphs. Our standardized experimental setup decouples confounding factors such as model architecture, dataset characteristics, and adaptation protocols, enabling rigorous comparisons focused solely on pretraining objectives. Surprisingly, we observe that most graph SSL methods struggle to generalize, with some performing worse than random initialization. In contrast, GraphMAE, a masked autoencoder approach, consistently improves transfer performance. We analyze the underlying factors that drive these differences and offer insights to guide future research on transferable graph SSL, laying a solid foundation for the \"pretrain-then-transfer\" paradigm in graph learning. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.06975",
    "authors": [
      "Yu Song",
      "Zhigang Hua",
      "Yan Xie",
      "Jingzhe Liu",
      "Bo Long",
      "Hui Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.06976",
    "title": "A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction",
    "abstract": "           Traffic demand prediction plays a critical role in intelligent transportation systems. Existing traffic prediction models primarily rely on temporal traffic data, with limited efforts incorporating human knowledge and experience for urban traffic demand forecasting. However, in real-world scenarios, traffic knowledge and experience derived from human daily life significantly influence precise traffic prediction. Such knowledge and experiences can guide the model in uncovering latent patterns within traffic data, thereby enhancing the accuracy and robustness of predictions. To this end, this paper proposes integrating structured temporal traffic data with textual data representing human knowledge and experience, resulting in a novel knowledge-guided cross-modal feature representation learning (KGCM) model for traffic demand prediction. Based on regional transportation characteristics, we construct a prior knowledge dataset using a large language model combined with manual authoring and revision, covering both regional and global knowledge and experiences. The KGCM model then learns multimodal data features through designed local and global adaptive graph networks, as well as a cross-modal feature fusion mechanism. A proposed reasoning-based dynamic update strategy enables dynamic optimization of the graph model's parameters, achieving optimal performance. Experiments on multiple traffic datasets demonstrate that our model accurately predicts future traffic demand and outperforms existing state-of-the-art (SOTA) models.         ",
    "url": "https://arxiv.org/abs/2509.06976",
    "authors": [
      "Lingyu Zhang",
      "Pengfei Xu",
      "Guobin Wu",
      "Jian Liang",
      "Ruiyang Dong",
      "Yunhai Wang",
      "Xuan Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.06984",
    "title": "FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities",
    "abstract": "           Foundation models have demonstrated remarkable performance across a wide range of tasks, yet their large parameter sizes pose challenges for practical deployment, especially in decentralized environments. Parameter-efficient fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing and memory overhead, making it attractive for federated learning. However, existing federated LoRA methods typically assume uniform rank configurations and unimodal inputs, overlooking two key real-world challenges: (1) heterogeneous client resources have different LoRA ranks, and (2) multimodal data settings with potentially missing modalities. In this work, we propose FediLoRA, a simple yet effective framework for federated multimodal fine-tuning under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a dimension-wise aggregation strategy that reweights LoRA updates without information dilution during aggregation. It also includes a lightweight layer-wise model editing method that selectively incorporates global parameters to repair local components which improves both client and global model performances. Experimental results on three multimodal benchmark datasets demonstrate that FediLoRA achieves superior performance over competitive baselines in both global and personalized settings, particularly in the presence of modality incompleteness.         ",
    "url": "https://arxiv.org/abs/2509.06984",
    "authors": [
      "Lishan Yang",
      "Nam Kha Nguygen",
      "Po Hu",
      "Wei Emma Zhang",
      "Yanjun Shu",
      "Mong Yuan Sim",
      "Weitong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.06985",
    "title": "Using Variable Interaction Graphs to Improve Particle Swarm Optimization",
    "abstract": "           This paper presents Variable Interaction Graph Particle Swarm Optimization (VIGPSO), an adaptation to Particle Swarm Optimization (PSO) that dynamically learns and exploits variable interactions during the optimization process. PSO is widely used for real-valued optimization problems but faces challenges in high-dimensional search spaces. While Variable Interaction Graphs (VIGs) have proven effective for optimization algorithms operating with known problem structure, their application to black-box optimization remains limited. VIGPSO learns how variables influence each other by analyzing how particles move through the search space, and uses these learned relationships to guide future particle movements. VIGPSO was evaluated against standard PSO on eight benchmark functions (three separable, two partially separable, and three non-separable) across 10, 30, 50 and 1000 dimensions. VIGPSO achieved statistically significant improvements ($p<0.05$) over the standard PSO algorithm in 28 out of 32 test configurations, with particularly strong performance extending to the 1000-dimensional case. The algorithm showed increasing effectiveness with dimensionality, though at the cost of higher variance in some test cases. These results suggest that dynamic VIG learning can bridge the gap between black-box and gray-box optimization effectively in PSO, particularly for high-dimensional problems.         ",
    "url": "https://arxiv.org/abs/2509.06985",
    "authors": [
      "Caz L. Czworkowski",
      "John W. Sheppard"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2509.06986",
    "title": "CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis",
    "abstract": "           Large-scale biological discovery requires integrating massive, heterogeneous datasets like those from the JUMP Cell Painting consortium, but technical batch effects and a lack of generalizable models remain critical roadblocks. To address this, we introduce CellPainTR, a Transformer-based architecture designed to learn foundational representations of cellular morphology that are robust to batch effects. Unlike traditional methods that require retraining on new data, CellPainTR's design, featuring source-specific context tokens, allows for effective out-of-distribution (OOD) generalization to entirely unseen datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP dataset, where it outperforms established methods like ComBat and Harmony in both batch integration and biological signal preservation. Critically, we demonstrate its robustness through a challenging OOD task on the unseen Bray et al. dataset, where it maintains high performance despite significant domain and feature shifts. Our work represents a significant step towards creating truly foundational models for image-based profiling, enabling more reliable and scalable cross-study biological analysis.         ",
    "url": "https://arxiv.org/abs/2509.06986",
    "authors": [
      "Cedric Caruzzo",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.06987",
    "title": "FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection",
    "abstract": "           Multimodal fusion is a multimedia technique that has become popular in the wide range of tasks where image information is accompanied by a signal/audio. The latter may not convey highly semantic information, such as speech or music, but some measures such as audio signal recorded by mics in the goal to detect rail structure elements or defects. While classical detection approaches such as You Only Look Once (YOLO) family detectors can be efficiently deployed for defect detection on the image modality, the single modality approaches remain limited. They yield an overdetection in case of the appearance similar to normal structural elements. The paper proposes a new multimodal fusion architecture built on the basis of domain rules with YOLO and Vision transformer backbones. It integrates YOLOv8n for rapid object detection with a Vision Transformer (ViT) to combine feature maps extracted from multiple layers (7, 16, and 19) and synthesised audio representations for two defect classes: rail Rupture and Surface defect. Fusion is performed between audio and image. Experimental evaluation on a real-world railway dataset demonstrates that our multimodal fusion improves precision and overall accuracy by 0.2 points compared to the vision-only approach. Student's unpaired t-test also confirms statistical significance of differences in the mean accuracy.         ",
    "url": "https://arxiv.org/abs/2509.06987",
    "authors": [
      "Alexey Zhukov",
      "Jenny Benois-Pineau",
      "Amira Youssef",
      "Akka Zemmari",
      "Mohamed Mosbah",
      "Virginie Taillandier"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.06988",
    "title": "Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection",
    "abstract": "           Out-of-distribution (OOD) detection helps models identify data outside the training categories, crucial for security applications. While feature-based post-hoc methods address this by evaluating data differences in the feature space without changing network parameters, they often require access to training data, which may not be suitable for some data privacy scenarios. This may not be suitable in scenarios where data privacy protection is a concern. In this paper, we propose a simple yet effective post-hoc method, termed Classifier-based Feature Reconstruction (ClaFR), from the perspective of subspace projection. It first performs an orthogonal decomposition of the classifier's weights to extract the class-known subspace, then maps the original data features into this subspace to obtain new data representations. Subsequently, the OOD score is determined by calculating the feature reconstruction error of the data within the subspace. Compared to existing OOD detection algorithms, our method does not require access to training data while achieving leading performance on multiple OOD benchmarks. Our code is released at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.06988",
    "authors": [
      "Yingsheng Wang",
      "Shuo Lu",
      "Jian Liang",
      "Aihua Zheng",
      "Ran He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.06992",
    "title": "FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models",
    "abstract": "           Federated Prompt Tuning (FPT) is an efficient method for cross-client collaborative fine-tuning of large Vision-Language Models (VLMs). However, models tuned using FPT are vulnerable to adversarial attacks, leading to misclassification in downstream tasks. In this work, we introduce Federated Adversarial Prompt Tuning (\\textbf{FedAPT}), a novel method designed to enhance the adversarial robustness of FPT. We identify a key issue in FedAPT under non-independent and identically distributed (non-IID) settings: a \\textit{class information gap} between clients and the global model. Clients rely solely on limited local label information to generate adversarial samples for training, while the global model must defend against adversarial attacks from global labels. To address this issue, we propose a \\textbf{class-aware prompt generator} that generates visual prompts from text prompts. This generator is guided by a \\emph{Global Label Embedding} (serving as a ``beacon\") which encodes cross-client label information to create more globally-aligned visual prompts. Additionally, we propose a \\textbf{cross-layer generator sharing} strategy to enhance prompt coupling across different layers of the model, further boosting adversarial robustness. Extensive experiments on multiple image classification datasets demonstrate the superiority of FedAPT in improving adversarial robustness, outperforming existing methods by a large margin. FedAPT also exhibits exceptional generalization in cross-domain and cross-dataset scenarios, indicating its effectiveness in real-world applications.         ",
    "url": "https://arxiv.org/abs/2509.06992",
    "authors": [
      "Kun Zhai",
      "Siheng Chen",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07016",
    "title": "Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV",
    "abstract": "           In response to the prevalent concern of TCP SYN flood attacks within the context of Software-Defined Internet of Vehicles (SD-IoV), this study addresses the significant challenge of network security in rapidly evolving vehicular communication systems. This research focuses on optimizing a Random Forest Classifier model to achieve maximum accuracy and minimal detection time, thereby enhancing vehicular network security. The methodology involves preprocessing a dataset containing SYN attack instances, employing feature scaling and label encoding techniques, and applying Stratified K-Fold cross-validation to target key metrics such as accuracy, precision, recall, and F1-score. This research achieved an average value of 0.999998 for all metrics with a SYN DoS attack detection time of 0.24 seconds. Results show that the fine-tuned Random Forest model, configured with 20 estimators and a depth of 10, effectively differentiates between normal and malicious traffic with high accuracy and minimal detection time, which is crucial for SD-IoV networks. This approach marks a significant advancement and introduces a state-of-the-art algorithm in detecting SYN flood attacks, combining high accuracy with minimal detection time. It contributes to vehicular network security by providing a robust solution against TCP SYN flood attacks while maintaining network efficiency and reliability.         ",
    "url": "https://arxiv.org/abs/2509.07016",
    "authors": [
      "Muhammad Arif Hakimi Zamrai",
      "Kamaludin Mohd Yusof"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07017",
    "title": "From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning",
    "abstract": "           We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning framework that embeds logical rules as spectral templates and performs inference directly in the graph spectral domain. By leveraging graph signal processing (GSP) and frequency-selective filters grounded in the Laplacian eigenstructure of knowledge graphs, the architecture unifies the interpretability of symbolic reasoning with the scalability and adaptability of spectral learning. Beyond the core formulation, we incorporate a comprehensive set of extensions, including dynamic graph and basis learning, rational and diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts for modular specialization, proof-guided training with spectral curricula, and uncertainty quantification for calibrated confidence. Additional enhancements such as large language model coupling, co-spectral transfer alignment, adversarial robustness, efficient GPU kernels, generalized Laplacians, and causal interventions further expand the versatility of the framework. Empirical evaluation on state-of-the-art reasoning benchmarks such as ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior accuracy, faster inference, improved robustness to adversarial perturbations, and higher interpretability compared to leading baselines including transformers, message-passing neural networks, and neuro-symbolic logic programming systems. Spectral attribution and proof-band agreement analyses confirm that model decisions align closely with symbolic proof structures, while transfer experiments validate effective domain adaptation through co-spectral alignment. These results establish Spectral NSR as a scalable and principled foundation for the next generation of reasoning systems, offering transparency, robustness, and generalization beyond conventional approaches.         ",
    "url": "https://arxiv.org/abs/2509.07017",
    "authors": [
      "Andrew Kiruluta",
      "Priscilla Burity"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07025",
    "title": "1 bit is all we need: binary normalized neural networks",
    "abstract": "           The increasing size of large neural network models, specifically language models and foundational image models, poses deployment challenges, prompting efforts to reduce memory requirements and enhance computational efficiency. These efforts are critical to ensure practical deployment and effective utilization of these models across various applications. In this work, a novel type of neural network layers and models is developed that uses only single-bit parameters. In this novel type of models all parameters of all layers, including kernel weights and biases, only have values equal to zero or one. This novel type of models uses layers named as binary normalized layer. These binary normalized layers can be of any type, such as fully connected, convolutional, attention, etc., and they consist of slight variations of the corresponding conventional layers. To show the effectiveness of the binary normalized layers, two different models are configured to solve a multiclass image classification problem and a language decoder to predict the next token of a sequence. The model to solve the image classification has convolutional and fully connected layers, and the language model is composed of transformer blocks with multi-head attention. The results show that models with binary normalized layers present almost the same results obtained by equivalent models with real 32-bit parameters. The binary normalized layers allow to develop models that use 32 times less memory than current models and have equivalent performance. Besides, the binary normalized layers can be easily implemented on current computers using 1-bit arrays, and do not require the development of dedicated electronic hardware. This novel type of layers opens a new era for large neural network models with reduced memory requirements that can be deployed using simple and cheap hardware, such as mobile devices or only cpus.         ",
    "url": "https://arxiv.org/abs/2509.07025",
    "authors": [
      "Eduardo Lobo Lustoda Cabral",
      "Paulo Pirozelli",
      "Larissa Driemeier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07036",
    "title": "Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators",
    "abstract": "           This paper presents a methodological approach to financial time series analysis by combining causal discovery and uncertainty-aware forecasting. As a case study, we focus on four key U.S. macroeconomic indicators -- GDP, economic growth, inflation, and unemployment -- and we apply the LPCMCI framework with Gaussian Process Distance Correlation (GPDC) to uncover dynamic causal relationships in quarterly data from 1970 to 2021. Our results reveal a robust unidirectional causal link from economic growth to GDP and highlight the limited connectivity of inflation, suggesting the influence of latent factors. Unemployment exhibits strong autoregressive dependence, motivating its use as a case study for probabilistic forecasting. Leveraging the Chronos framework, a large language model trained for time series, we perform zero-shot predictions on unemployment. This approach delivers accurate forecasts one and two quarters ahead, without requiring task-specific training. Crucially, the model's uncertainty-aware predictions yield 90\\% confidence intervals, enabling effective anomaly detection through statistically principled deviation analysis. This study demonstrates the value of combining causal structure learning with probabilistic language models to inform economic policy and enhance forecasting robustness.         ",
    "url": "https://arxiv.org/abs/2509.07036",
    "authors": [
      "Federico Cerutti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07039",
    "title": "Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation",
    "abstract": "           Artificial intelligence deployment for automated photovoltaic (PV) monitoring faces interpretability barriers that limit adoption in energy infrastructure applications. While deep learning achieves high accuracy in thermal fault detection, validation that model decisions align with thermal physics principles remains lacking, creating deployment hesitancy where understanding model reasoning is critical. This study provides a systematic comparison of convolutional neural networks (ResNet-18, EfficientNet-B0) and vision transformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI saliency analysis to assess alignment with thermal physics principles. This represents the first systematic comparison of CNNs and vision transformers for thermal PV fault detection with physics-validated interpretability. Evaluation on 20,000 infrared images spanning normal operation and 11 fault categories shows that Swin Transformer achieves the highest performance (94% binary accuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis reveals that models learn physically meaningful features, such as localized hotspots for cell defects, linear thermal paths for diode failures, and thermal boundaries for vegetation shading, consistent with expected thermal signatures. However, performance varies significantly across fault types: electrical faults achieve strong detection (F1-scores >0.90) while environmental factors like soiling remain challenging (F1-scores 0.20-0.33), indicating limitations imposed by thermal imaging resolution. The thermal physics-guided interpretability approach provides methodology for validating AI decision-making in energy monitoring applications, addressing deployment barriers in renewable energy infrastructure.         ",
    "url": "https://arxiv.org/abs/2509.07039",
    "authors": [
      "Serra Aksoy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07055",
    "title": "Sequentially Auditing Differential Privacy",
    "abstract": "           We propose a practical sequential test for auditing differential privacy guarantees of black-box mechanisms. The test processes streams of mechanisms' outputs providing anytime-valid inference while controlling Type I error, overcoming the fixed sample size limitation of previous batch auditing methods. Experiments show this test detects violations with sample sizes that are orders of magnitude smaller than existing methods, reducing this number from 50K to a few hundred examples, across diverse realistic mechanisms. Notably, it identifies DP-SGD privacy violations in \\textit{under} one training run, unlike prior methods needing full model training.         ",
    "url": "https://arxiv.org/abs/2509.07055",
    "authors": [
      "Tom\u00e1s Gonz\u00e1lez",
      "Mateo Dulce-Rubio",
      "Aaditya Ramdas",
      "M\u00f3nica Ribero"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2509.07103",
    "title": "Lookup multivariate Kolmogorov-Arnold Networks",
    "abstract": "           High-dimensional linear mappings, or linear layers, dominate both the parameter count and the computational cost of most modern deep-learning models. We introduce a general drop-in replacement, lookup multivariate Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better trade-off between capacity and inference cost. Our construction expresses a general high-dimensional mapping through trainable low-dimensional multivariate functions. These functions can carry dozens or hundreds of trainable parameters each, and yet it takes only a few multiplications to compute them because they are implemented as spline lookup tables. Empirically, lmKANs reduce inference FLOPs by up to 6.0x while matching the flexibility of MLPs in general high-dimensional function approximation. In another feedforward fully connected benchmark, on the tabular-like dataset of randomly displaced methane configurations, lmKANs enable more than 10x higher H100 throughput at equal accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10 and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA kernels, is available online at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.07103",
    "authors": [
      "Sergey Pozdnyakov",
      "Philippe Schwaller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Performance (cs.PF)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.07130",
    "title": "Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry",
    "abstract": "           Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by fusing camera and Inertial Measurement Unit (IMU) data for real-time pose. However, current trend of offloading VIO to edge servers can lead server-side threat surface where subtle pose spoofing can accumulate into substantial drift, while evading heuristic checks. In this paper, we study this threat and present an unsupervised, label-free detection and recovery mechanism. The proposed model is trained on attack-free sessions to learn temporal regularities of motion to detect runtime deviations and initiate recovery to restore pose consistency. We evaluate the approach in a realistic offloaded-VIO environment using ILLIXR testbed across multiple spoofing intensities. Experimental results in terms of well-known performance metrics show substantial reductions in trajectory and pose error compared to a no-defense baseline.         ",
    "url": "https://arxiv.org/abs/2509.07130",
    "authors": [
      "Soruya Saha",
      "Md Nurul Absur",
      "Saptarshi Debroy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2509.07131",
    "title": "SoK: Security and Privacy of AI Agents for Blockchain",
    "abstract": "           Blockchain and smart contracts have garnered significant interest in recent years as the foundation of a decentralized, trustless digital ecosystem, thereby eliminating the need for traditional centralized authorities. Despite their central role in powering Web3, their complexity still presents significant barriers for non-expert users. To bridge this gap, Artificial Intelligence (AI)-based agents have emerged as valuable tools for interacting with blockchain environments, supporting a range of tasks, from analyzing on-chain data and optimizing transaction strategies to detecting vulnerabilities within smart contracts. While interest in applying AI to blockchain is growing, the literature still lacks a comprehensive survey that focuses specifically on the intersection with AI agents. Most of the related work only provides general considerations, without focusing on any specific domain. This paper addresses this gap by presenting the first Systematization of Knowledge dedicated to AI-driven systems for blockchain, with a special focus on their security and privacy dimensions, shedding light on their applications, limitations, and future research directions.         ",
    "url": "https://arxiv.org/abs/2509.07131",
    "authors": [
      "Nicol\u00f2 Romandini",
      "Carlo Mazzocca",
      "Kai Otsuki",
      "Rebecca Montanari"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07132",
    "title": "Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative Study",
    "abstract": "           The widespread use of generative AI has shown remarkable success in producing highly realistic deepfakes, posing a serious threat to various voice biometric applications, including speaker verification, voice biometrics, audio conferencing, and criminal investigations. To counteract this, several state-of-the-art (SoTA) audio deepfake detection (ADD) methods have been proposed to identify generative AI signatures to distinguish between real and deepfake audio. However, the effectiveness of these methods is severely undermined by anti-forensic (AF) attacks that conceal generative signatures. These AF attacks span a wide range of techniques, including statistical modifications (e.g., pitch shifting, filtering, noise addition, and quantization) and optimization-based attacks (e.g., FGSM, PGD, C \\& W, and DeepFool). In this paper, we investigate the SoTA ADD methods and provide a comparative analysis to highlight their effectiveness in exposing deepfake signatures, as well as their vulnerabilities under adversarial conditions. We conducted an extensive evaluation of ADD methods on five deepfake benchmark datasets using two categories: raw and spectrogram-based approaches. This comparative analysis enables a deeper understanding of the strengths and limitations of SoTA ADD methods against diverse AF attacks. It does not only highlight vulnerabilities of ADD methods, but also informs the design of more robust and generalized detectors for real-world voice biometrics. It will further guide future research in developing adaptive defense strategies that can effectively counter evolving AF techniques.         ",
    "url": "https://arxiv.org/abs/2509.07132",
    "authors": [
      "Kutub Uddin",
      "Muhammad Umar Farooq",
      "Awais Khan",
      "Khalid Mahmood Malik"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07133",
    "title": "Avoiding Over-Personalization with Rule-Guided Knowledge Graph Adaptation for LLM Recommendations",
    "abstract": "           We present a lightweight neuro-symbolic framework to mitigate over-personalization in LLM-based recommender systems by adapting user-side Knowledge Graphs (KGs) at inference time. Instead of retraining models or relying on opaque heuristics, our method restructures a user's Personalized Knowledge Graph (PKG) to suppress feature co-occurrence patterns that reinforce Personalized Information Environments (PIEs), i.e., algorithmically induced filter bubbles that constrain content diversity. These adapted PKGs are used to construct structured prompts that steer the language model toward more diverse, Out-PIE recommendations while preserving topical relevance. We introduce a family of symbolic adaptation strategies, including soft reweighting, hard inversion, and targeted removal of biased triples, and a client-side learning algorithm that optimizes their application per user. Experiments on a recipe recommendation benchmark show that personalized PKG adaptations significantly increase content novelty while maintaining recommendation quality, outperforming global adaptation and naive prompt-based methods.         ",
    "url": "https://arxiv.org/abs/2509.07133",
    "authors": [
      "Fernando Spadea",
      "Oshani Seneviratne"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07143",
    "title": "Of Graphs and Tables: Zero-Shot Node Classification with Tabular Foundation Models",
    "abstract": "           Graph foundation models (GFMs) have recently emerged as a promising paradigm for achieving broad generalization across various graph data. However, existing GFMs are often trained on datasets that were shown to poorly represent real-world graphs, limiting their generalization performance. In contrast, tabular foundation models (TFMs) not only excel at classical tabular prediction tasks but have also shown strong applicability in other domains such as time series forecasting, natural language processing, and computer vision. Motivated by this, we take an alternative view to the standard perspective of GFMs and reformulate node classification as a tabular problem. Each node can be represented as a row with feature, structure, and label information as columns, enabling TFMs to directly perform zero-shot node classification via in-context learning. In this work, we introduce TabGFM, a graph foundation model framework that first converts a graph into a table via feature and structural encoders, applies multiple TFMs to diversely subsampled tables, and then aggregates their outputs through ensemble selection. Through experiments on 28 real-world datasets, TabGFM achieves consistent improvements over task-specific GNNs and state-of-the-art GFMs, highlighting the potential of tabular reformulation for scalable and generalizable graph learning.         ",
    "url": "https://arxiv.org/abs/2509.07143",
    "authors": [
      "Adrian Hayler",
      "Xingyue Huang",
      "\u0130smail \u0130lkan Ceylan",
      "Michael Bronstein",
      "Ben Finkelshtein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07146",
    "title": "Autoencoder-Based Denoising of Muscle Artifacts in ECG to Preserve Skin Nerve Activity (SKNA) for Cognitive Stress Detection",
    "abstract": "           The sympathetic nervous system (SNS) plays a central role in regulating the body's responses to stress and maintaining physiological stability. Its dysregulation is associated with a wide range of conditions, from cardiovascular disease to anxiety disorders. Skin nerve activity (SKNA) extracted from high-frequency electrocardiogram (ECG) recordings provides a noninvasive window into SNS dynamics, but its measurement is highly susceptible to electromyographic (EMG) contamination. Traditional preprocessing based on bandpass filtering within a fixed range (e.g., 500--1000 Hz) is susceptible to overlapping EMG and SKNA spectral components, especially during sustained muscle activity. We present a denoising approach using a lightweight one-dimensional convolutional autoencoder with a long short-term memory (LSTM) bottleneck to reconstruct clean SKNA from EMG-contaminated recordings. Using clean ECG-derived SKNA data from cognitive stress experiments and EMG noise from chaotic muscle stimulation recordings, we simulated contamination at realistic noise levels (--4 dB, --8 dB signal-to-noise ratio) and trained the model in the leave-one-subject-out cross-validation framework. The method improved signal-to-noise ratio by up to 9.65 dB, increased cross correlation with clean SKNA from 0.40 to 0.72, and restored burst-based SKNA features to near-clean discriminability (AUROC $\\geq$ 0.96). Classification of baseline versus sympathetic stimulation (cognitive stress) conditions reached accuracies of 91--98\\% across severe noise levels, comparable to clean data. These results demonstrate that deep learning--based reconstruction can preserve physiologically relevant sympathetic bursts during substantial EMG interference, enabling more robust SKNA monitoring in naturalistic, movement-rich environments.         ",
    "url": "https://arxiv.org/abs/2509.07146",
    "authors": [
      "Farnoush Baghestani",
      "Jihye Moon",
      "Youngsun Kong",
      "Ki Chon"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07198",
    "title": "Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data",
    "abstract": "           Motivated by the high resource costs and privacy concerns associated with centralized machine learning, federated learning (FL) has emerged as an efficient alternative that enables clients to collaboratively train a global model while keeping their data local. However, in real-world deployments, client data distributions often evolve over time and differ significantly across clients, introducing heterogeneity that degrades the performance of standard FL algorithms. In this work, we introduce Fed-REACT, a federated learning framework designed for heterogeneous and evolving client data. Fed-REACT combines representation learning with evolutionary clustering in a two-stage process: (1) in the first stage, each client learns a local model to extracts feature representations from its data; (2) in the second stage, the server dynamically groups clients into clusters based on these representations and coordinates cluster-wise training of task-specific models for downstream objectives such as classification or regression. We provide a theoretical analysis of the representation learning stage, and empirically demonstrate that Fed-REACT achieves superior accuracy and robustness on real-world datasets.         ",
    "url": "https://arxiv.org/abs/2509.07198",
    "authors": [
      "Yiyue Chen",
      "Usman Akram",
      "Chianing Wang",
      "Haris Vikalo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2509.07208",
    "title": "A Hybrid CNN-LSTM Deep Learning Model for Intrusion Detection in Smart Grid",
    "abstract": "           The evolution of the traditional power grid into the \"smart grid\" has resulted in a fundamental shift in energy management, which allows the integration of renewable energy sources with modern communication technology. However, this interconnection has increased smart grids' vulnerability to attackers, which might result in privacy breaches, operational interruptions, and massive outages. The SCADA-based smart grid protocols are critical for real-time data collection and control, but they are vulnerable to attacks like unauthorized access and denial of service (DoS). This research proposes a hybrid deep learning-based Intrusion Detection System (IDS) intended to improve the cybersecurity of smart grids. The suggested model takes advantage of Convolutional Neural Networks' (CNN) feature extraction capabilities as well as Long Short-Term Memory (LSTM) networks' temporal pattern recognition skills. DNP3 and IEC104 intrusion detection datasets are employed to train and test our CNN-LSTM model to recognize and classify the potential cyber threats. Compared to other deep learning approaches, the results demonstrate considerable improvements in accuracy, precision, recall, and F1-score, with a detection accuracy of 99.70%.         ",
    "url": "https://arxiv.org/abs/2509.07208",
    "authors": [
      "Abdulhakim Alsaiari",
      "Mohammad Ilyas"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07225",
    "title": "All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching",
    "abstract": "           Our team, All You Need Is A Fuzzing Brain, was one of seven finalists in DARPA's Artificial Intelligence Cyber Challenge (AIxCC), placing fourth in the final round. During the competition, we developed a Cyber Reasoning System (CRS) that autonomously discovered 28 security vulnerabilities - including six previously unknown zero-days - in real-world open-source C and Java projects, and successfully patched 14 of them. The complete CRS is open source at this https URL. This paper provides a detailed technical description of our CRS, with an emphasis on its LLM-powered components and strategies. Building on AIxCC, we further introduce a public leaderboard for benchmarking state-of-the-art LLMs on vulnerability detection and patching tasks, derived from the AIxCC dataset. The leaderboard is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.07225",
    "authors": [
      "Ze Sheng",
      "Qingxiao Xu",
      "Jianwei Huang",
      "Matthew Woodcock",
      "Heqing Huang",
      "Alastair F. Donaldson",
      "Guofei Gu",
      "Jeff Huang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.07230",
    "title": "JOINT: Join Optimization and Inference via Network Traversal",
    "abstract": "           Traditional relational databases require users to manually specify join keys and assume exact matches between column names and values. In practice, this limits joinability across fragmented or inconsistently named tables. We propose a fuzzy join framework that automatically identifies joinable column pairs and traverses indirect (multi-hop) join paths across multiple databases. Our method combines column name similarity with row-level fuzzy value overlap, computes edge weights using negative log-transformed Jaccard scores, and performs join path discovery via graph traversal. Experiments on synthetic healthcare-style databases demonstrate the system's ability to recover valid joins despite fuzzified column names and partial value mismatches. This research has direct applications in data integration.         ",
    "url": "https://arxiv.org/abs/2509.07230",
    "authors": [
      "Szu-Yun Ko",
      "Ethan Chen",
      "Bo-Cian Chang",
      "Alan Shu-Luen Chang"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2509.07234",
    "title": "Efficient Multi-Agent Coordination via Dynamic Joint-State Graph Construction",
    "abstract": "           Multi-agent pathfinding (MAPF) traditionally focuses on collision avoidance, but many real-world applications require active coordination between agents to improve team performance. This paper introduces Team Coordination on Graphs with Risky Edges (TCGRE), where agents collaborate to reduce traversal costs on high-risk edges via support from teammates. We reformulate TCGRE as a 3D matching problem-mapping robot pairs, support pairs, and time steps-and rigorously prove its NP-hardness via reduction from Minimum 3D Matching. To address this complexity, (in the conference version) we proposed efficient decomposition methods, reducing the problem to tractable subproblems: Joint-State Graph (JSG): Encodes coordination as a single-agent shortest-path problem. Coordination-Exhaustive Search (CES): Optimizes support assignments via exhaustive pairing. Receding-Horizon Optimistic Cooperative A* (RHOCA*): Balances optimality and scalability via horizon-limited planning. Further in this extension, we introduce a dynamic graph construction method (Dynamic-HJSG), leveraging agent homogeneity to prune redundant states and reduce computational overhead by constructing the joint-state graph dynamically. Theoretical analysis shows Dynamic-HJSG preserves optimality while lowering complexity from exponential to polynomial in key cases. Empirical results validate scalability for large teams and graphs, with HJSG outperforming baselines greatly in runtime in different sizes and types of graphs. This work bridges combinatorial optimization and multi-agent planning, offering a principled framework for collaborative pathfinding with provable guarantees, and the key idea of the solution can be widely extended to many other collaborative optimization problems, such as MAPF.         ",
    "url": "https://arxiv.org/abs/2509.07234",
    "authors": [
      "Yanlin Zhou",
      "Manshi Limbu",
      "Xuesu Xiao"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.07236",
    "title": "Breaking the Conventional Forward-Backward Tie in Neural Networks: Activation Functions",
    "abstract": "           Gradient-based neural network training traditionally enforces symmetry between forward and backward propagation, requiring activation functions to be differentiable (or sub-differentiable) and strictly monotonic in certain regions to prevent flat gradient areas. This symmetry, linking forward activations closely to backward gradients, significantly restricts the selection of activation functions, particularly excluding those with substantial flat or non-differentiable regions. In this paper, we challenge this assumption through mathematical analysis, demonstrating that precise gradient magnitudes derived from activation functions are largely redundant, provided the gradient direction is preserved. Empirical experiments conducted on foundational architectures - such as Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Binary Neural Networks (BNNs) - confirm that relaxing forward-backward symmetry and substituting traditional gradients with simpler or stochastic alternatives does not impair learning and may even enhance training stability and efficiency. We explicitly demonstrate that neural networks with flat or non-differentiable activation functions, such as the Heaviside step function, can be effectively trained, thereby expanding design flexibility and computational efficiency. Further empirical validation with more complex architectures remains a valuable direction for future research.         ",
    "url": "https://arxiv.org/abs/2509.07236",
    "authors": [
      "Luigi Troiano",
      "Francesco Gissi",
      "Vincenzo Benedetto",
      "Genny Tortora"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07242",
    "title": "DORA: Dynamic O-RAN Resource Allocation for Multi-Slice 5G Networks",
    "abstract": "           The fifth generation (5G) of wireless networks must simultaneously support heterogeneous service categories, including Ultra-Reliable Low-Latency Communications (URLLC), enhanced Mobile Broadband (eMBB), and massive Machine-Type Communications (mMTC), each with distinct Quality of Service (QoS) requirements. Meeting these demands under limited spectrum resources requires adaptive and standards-compliant radio resource management. We present DORA (Dynamic O-RAN Resource Allocation), a deep reinforcement learning (DRL) framework for dynamic slice-level Physical Resource Block (PRB) allocation in Open RAN. DORA employs a PPO-based RL agent to allocate PRBs across URLLC, eMBB, and mMTC slices based on observed traffic demands and channel conditions. Intra-slice PRB scheduling is handled deterministically via round-robin among active UEs, simplifying control complexity and improving training stability. Unlike prior work, DORA supports online training and adapts continuously to evolving traffic patterns and cross-slice contention. Implemented in the standards-compliant OpenAirInterface (OAI) RAN stack and designed for deployment as an O-RAN xApp, DORA integrates seamlessly with RAN Intelligent Controllers (RICs). Extensive evaluation under congested regimes shows that DORA outperforms three non-learning baselines and a \\texttt{DQN} agent, achieving lower URLLC latency, higher eMBB throughput with fewer SLA violations, and broader mMTC coverage without starving high-priority slices. To our knowledge, this is the first fully online DRL framework for adaptive, slice-aware PRB allocation in O-RAN.         ",
    "url": "https://arxiv.org/abs/2509.07242",
    "authors": [
      "Alireza Ebrahimi Dorcheh",
      "Tolunay Seyfi",
      "Fatemeh Afghah"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2509.07253",
    "title": "Benchmarking Information Retrieval Models on Complex Retrieval Tasks",
    "abstract": "           Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques.         ",
    "url": "https://arxiv.org/abs/2509.07253",
    "authors": [
      "Julian Killingback",
      "Hamed Zamani"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.07277",
    "title": "Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion",
    "abstract": "           Data scarcity hinders deep learning for medical imaging. We propose a framework for breast cancer classification in thermograms that addresses this using a Diffusion Probabilistic Model (DPM) for data augmentation. Our DPM-based augmentation is shown to be superior to both traditional methods and a ProGAN baseline. The framework fuses deep features from a pre-trained ResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived from U-Net segmented tumors. An XGBoost classifier trained on these fused features achieves 98.0\\% accuracy and 98.1\\% sensitivity. Ablation studies and statistical tests confirm that both the DPM augmentation and the nonlinear feature fusion are critical, statistically significant components of this success. This work validates the synergy between advanced generative models and interpretable features for creating highly accurate medical diagnostic tools.         ",
    "url": "https://arxiv.org/abs/2509.07277",
    "authors": [
      "Sepehr Salem",
      "M. Moein Esfahani",
      "Jingyu Liu",
      "Vince Calhoun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07282",
    "title": "ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers",
    "abstract": "           We present cryptogram solving as an ideal testbed for studying neural network generalization in combinatorially complex domains. In this task, models must decrypt text encoded with substitution ciphers, choosing from 26! possible mappings without explicit access to the cipher. We develop ALICE (an Architecture for Learning Interpretable Cryptogram dEcipherment): a simple encoder-only Transformer that sets a new state-of-the-art for both accuracy and speed on this decryption problem. Surprisingly, ALICE generalizes to unseen ciphers after training on only ${\\sim}1500$ unique ciphers, a minute fraction ($3.7 \\times 10^{-24}$) of the possible cipher space. To enhance interpretability, we introduce a novel bijective decoding head that explicitly models permutations via the Gumbel-Sinkhorn method, enabling direct extraction of learned cipher mappings. Through early exit analysis, we reveal how ALICE progressively refines its predictions in a way that appears to mirror common human strategies for this task: early layers employ frequency-based heuristics, middle layers form word structures, and final layers correct individual characters. Our architectural innovations and analysis methods extend beyond cryptograms to any domain with bijective mappings and combinatorial structure, offering new insights into neural network generalization and interpretability.         ",
    "url": "https://arxiv.org/abs/2509.07282",
    "authors": [
      "Jeff Shen",
      "Lindsay Smith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.07283",
    "title": "An Agentic AI Workflow to Simplify Parameter Estimation of Complex Differential Equation Systems",
    "abstract": "           Parameter identification for mechanistic Ordinary Differential Equation (ODE) models underpins prediction and control in several applications, yet remains a labor-intensive and brittle process: datasets are noisy and partial, models can be stiff or misspecified, and differentiable implementations demand framework expertise. An agentic AI workflow is presented that converts a lightweight, human-readable specification into a compiled, parallel, and differentiable calibration pipeline. Users supply an XML description of the problem and fill in a Python code skeleton; the agent automatically validates consistency between spec and code, and auto-remediates common pathologies. It transforms Python callables into pure JAX functions for efficient just-in-time compilation and parallelization. The system then orchestrates a two-stage search comprising global exploration of the parameter space followed by gradient-based refinement. The result is an AD-native, reproducible workflow that lowers the barrier to advanced calibration while preserving expert control. An open-source implementation with a documented API and examples is released, enabling rapid movement from problem statement to fitted, auditable models with minimal boilerplate.         ",
    "url": "https://arxiv.org/abs/2509.07283",
    "authors": [
      "Saakaar Bhatnagar"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2509.07294",
    "title": "Learning Neural Koopman Operators with Dissipativity Guarantees",
    "abstract": "           We address the problem of learning a neural Koopman operator model that provides dissipativity guarantees for an unknown nonlinear dynamical system that is known to be dissipative. We propose a two-stage approach. First, we learn an unconstrained neural Koopman model that closely approximates the system dynamics. Then, we minimally perturb the parameters to enforce strict dissipativity. Crucially, we establish theoretical guarantees that extend the dissipativity properties of the learned model back to the original nonlinear system. We realize this by deriving an exact relationship between the dissipativity of the learned model and the true system through careful characterization of the identification errors from the noisy data, Koopman operator truncation, and generalization to unseen data. We demonstrate our approach through simulation on a Duffing oscillator model.         ",
    "url": "https://arxiv.org/abs/2509.07294",
    "authors": [
      "Yuezhu Xu",
      "S. Sivaranjani",
      "Vijay Gupta"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.07301",
    "title": "Causal Attention with Lookahead Keys",
    "abstract": "           In standard causal attention, each token's query, key, and value (QKV) are static and encode only preceding context. We introduce CAuSal aTtention with Lookahead kEys (CASTLE), an attention mechanism that continually updates each token's keys as the context unfolds. We term these updated keys lookahead keys because they belong to earlier positions yet integrate information from tokens that appear later relative to those positions, while strictly preserving the autoregressive property. Although the mechanism appears sequential, we derive a mathematical equivalence that avoids explicitly materializing lookahead keys at each position and enables efficient parallel training. On language modeling benchmarks, CASTLE consistently outperforms standard causal attention across model scales, reducing validation perplexity and improving performance on a range of downstream tasks.         ",
    "url": "https://arxiv.org/abs/2509.07301",
    "authors": [
      "Zhuoqing Song",
      "Peng Sun",
      "Huizhuo Yuan",
      "Quanquan Gu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07304",
    "title": "Distributed Leader-Follower Consensus for Uncertain Multiagent Systems with Time-Triggered Switching of the Communication Network",
    "abstract": "           A distributed adaptive control strategy is developed for heterogeneous multiagent systems in nonlinear Brunovsky form with \\({\\pd}\\)-dimensional $n^{\\text{th}}$-order dynamics, operating under time-triggered switching communication topologies. The approach uses repulsive potential functions to ensure agent-agent and obstacle safety, while neural network estimators compensate for system uncertainties and disturbances. A high-order control barrier function framework is then employed to certify the positive invariance of the safe sets and the boundedness of the proposed control inputs. The resulting distributed control and adaptive laws, together with dwell-time requirements for topology transitions, achieve leader-following consensus. This integrated design provides synchronized formation and robust disturbance rejection in evolving network configurations, and its effectiveness is demonstrated through numerical simulations.         ",
    "url": "https://arxiv.org/abs/2509.07304",
    "authors": [
      "Armel Koulong",
      "Ali Pakniyat"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.07308",
    "title": "Basis Vector Metric: A Method for Robust Open-Ended State Change Detection",
    "abstract": "           We test a new method, which we will abbreviate using the acronym BVM (Basis Vectors Method), in its ability to judge the state changes in images through using language embeddings. We used the MIT-States dataset, containing about 53,000 images, to gather all of our data, which has 225 nouns and 115 adjectives, with each noun having about 9 different adjectives, forming approximately 1000 noun-adjective pairs. For our first experiment, we test our method's ability to determine the state of each noun class separately against other metrics for comparison. These metrics are cosine similarity, dot product, product quantization, binary index, Naive Bayes, and a custom neural network. Among these metrics, we found that our proposed BVM performs the best in classifying the states for each noun. We then perform a second experiment where we try using BVM to determine if it can differentiate adjectives from one another for each adjective separately. We compared the abilities of BVM to differentiate adjectives against the proposed method the MIT-States paper suggests: using a logistic regression model. In the end, we did not find conclusive evidence that our BVM metric could perform better than the logistic regression model at discerning adjectives. Yet, we were able to find evidence for possible improvements to our method; this leads to the chance of increasing our method's accuracy through certain changes in our methodologies.         ",
    "url": "https://arxiv.org/abs/2509.07308",
    "authors": [
      "David Oprea",
      "Sam Powers"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07309",
    "title": "Instance-level Performance Prediction for Long-form Generation Tasks",
    "abstract": "           We motivate and share a new benchmark for instance-level performance prediction of long-form generation tasks having multi-faceted, fine-grained quality metrics. Our task-, model- and metric-agnostic formulation predicts continuous evaluation metric scores given only black-box model inputs and outputs. Beyond predicting point estimates of metric scores, the benchmark also requires inferring prediction intervals to quantify uncertainty around point estimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs, baselines, and metrics per task. We show that scores can be effectively predicted across long-form generation tasks using as few as 16 training examples. Overall, we introduce a novel and useful task, a valuable benchmark to drive progress, and baselines ready for practical adoption today.         ",
    "url": "https://arxiv.org/abs/2509.07309",
    "authors": [
      "Chi-Yang Hsu",
      "Alexander Braylan",
      "Yiheng Su",
      "Omar Alonso",
      "Matthew Lease"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07319",
    "title": "MEGG: Replay via Maximally Extreme GGscore in Incremental Learning for Neural Recommendation Models",
    "abstract": "           Neural Collaborative Filtering models are widely used in recommender systems but are typically trained under static settings, assuming fixed data distributions. This limits their applicability in dynamic environments where user preferences evolve. Incremental learning offers a promising solution, yet conventional methods from computer vision or NLP face challenges in recommendation tasks due to data sparsity and distinct task paradigms. Existing approaches for neural recommenders remain limited and often lack generalizability. To address this, we propose MEGG, Replay Samples with Maximally Extreme GGscore, an experience replay based incremental learning framework. MEGG introduces GGscore, a novel metric that quantifies sample influence, enabling the selective replay of highly influential samples to mitigate catastrophic forgetting. Being model-agnostic, MEGG integrates seamlessly across architectures and frameworks. Experiments on three neural models and four benchmark datasets show superior performance over state-of-the-art baselines, with strong scalability, efficiency, and robustness. Implementation will be released publicly upon acceptance.         ",
    "url": "https://arxiv.org/abs/2509.07319",
    "authors": [
      "Yunxiao Shi",
      "Shuo Yang",
      "Haimin Zhang",
      "Li Wang",
      "Yongze Wang",
      "Qiang Wu",
      "Min Xu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07320",
    "title": "Data-knowledge fusion driven frequency security assessment: A robust framework for renewable-dominated power grids",
    "abstract": "           Frequency security is critical for power grids, as deviations can trigger widespread outages and result in substantial economic losses. However, modern renewable-dominated power grids face an increased risk of insecurity due to low inertia and nonlinear frequency responses. To mitigate these risks, robust pre-fault frequency security assessment (FSA) is critical, which enables grid operators to implement preventive control strategies. We propose a data-knowledge fusion framework to achieve intelligent FSA in actual power grids. First, we classify FSA domain knowledge into two distinct categories: (1) physics-guided knowledge directs the neural network pre-training process, ensuring that the fusion model's predictions consistent with frequency response mechanisms, and (2) physics-constrained knowledge establishes quantitative relationship on predictions, which forces them within theoretical ranges defined by domain knowledge. Furthermore, we develop a dual-channel neural network architecture to simultaneously capture both local and global characteristics of the power system. Finally, we introduce a data-knowledge fusion training algorithm that integrates guided learning with constrained network architecture to enhance model reliability and generalization. Case studies on China's Yunnan Provincial Power Grid validate the superior performance of our framework: it reduces average prediction error to 1.26% (a 49.2% reduction over data-driven methods), and maintains 97.60% accuracy in untrained scenarios (3.85% higher than data-driven methods), therefore satisfies the accuracy, reliability, and generalization requirements for actual power grids. The proposed methodology establishes a new paradigm for enhancing robustness of FSA in power grids, with potential application to cross-domain security assessment.         ",
    "url": "https://arxiv.org/abs/2509.07320",
    "authors": [
      "Yurun Zhang",
      "Wei Yao",
      "Yutian Lan",
      "Hang Shuai",
      "Shanyang Wei",
      "Wei Gan",
      "Chao Duan",
      "Jinyu Wen",
      "Shijie Cheng"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.07323",
    "title": "When Fine-Tuning is Not Enough: Lessons from HSAD on Hybrid and Adversarial Audio Spoof Detection",
    "abstract": "           The rapid advancement of AI has enabled highly realistic speech synthesis and voice cloning, posing serious risks to voice authentication, smart assistants, and telecom security. While most prior work frames spoof detection as a binary task, real-world attacks often involve hybrid utterances that mix genuine and synthetic speech, making detection substantially more challenging. To address this gap, we introduce the Hybrid Spoofed Audio Dataset (HSAD), a benchmark containing 1,248 clean and 41,044 degraded utterances across four classes: human, cloned, zero-shot AI-generated, and hybrid audio. Each sample is annotated with spoofing method, speaker identity, and degradation metadata to enable fine-grained analysis. We evaluate six transformer-based models, including spectrogram encoders (MIT-AST, MattyB95-AST) and self-supervised waveform models (Wav2Vec2, HuBERT). Results reveal critical lessons: pretrained models overgeneralize and collapse under hybrid conditions; spoof-specific fine-tuning improves separability but struggles with unseen compositions; and dataset-specific adaptation on HSAD yields large performance gains (AST greater than 97 percent and F1 score is approximately 99 percent), though residual errors persist for complex hybrids. These findings demonstrate that fine-tuning alone is not sufficient-robust hybrid-aware benchmarks like HSAD are essential to expose calibration failures, model biases, and factors affecting spoof detection in adversarial environments. HSAD thus provides both a dataset and an analytic framework for building resilient and trustworthy voice authentication systems.         ",
    "url": "https://arxiv.org/abs/2509.07323",
    "authors": [
      "Bin Hu",
      "Kunyang Huang",
      "Daehan Kwak",
      "Meng Xu",
      "Kuan Huang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.07335",
    "title": "G3CN: Gaussian Topology Refinement Gated Graph Convolutional Network for Skeleton-Based Action Recognition",
    "abstract": "           Graph Convolutional Networks (GCNs) have proven to be highly effective for skeleton-based action recognition, primarily due to their ability to leverage graph topology for feature aggregation, a key factor in extracting meaningful representations. However, despite their success, GCNs often struggle to effectively distinguish between ambiguous actions, revealing limitations in the representation of learned topological and spatial features. To address this challenge, we propose a novel approach, Gaussian Topology Refinement Gated Graph Convolution (G$^{3}$CN), to address the challenge of distinguishing ambiguous actions in skeleton-based action recognition. G$^{3}$CN incorporates a Gaussian filter to refine the skeleton topology graph, improving the representation of ambiguous actions. Additionally, Gated Recurrent Units (GRUs) are integrated into the GCN framework to enhance information propagation between skeleton points. Our method shows strong generalization across various GCN backbones. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA benchmarks demonstrate that G$^{3}$CN effectively improves action recognition, particularly for ambiguous samples.         ",
    "url": "https://arxiv.org/abs/2509.07335",
    "authors": [
      "Haiqing Ren",
      "Zhongkai Luo",
      "Heng Fan",
      "Xiaohui Yuan",
      "Guanchen Wang",
      "Libo Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07356",
    "title": "Anti-Disturbance Hierarchical Sliding Mode Controller for Deep-Sea Cranes with Adaptive Control and Neural Network Compensation",
    "abstract": "           To address non-linear disturbances and uncertainties in complex marine environments, this paper proposes a disturbance-resistant controller for deep-sea cranes. The controller integrates hierarchical sliding mode control, adaptive control, and neural network compensation techniques. By designing a global sliding mode surface, the dynamic coordination between the driving and non-driving subsystems is achieved, ensuring overall system stability. The subsystem surfaces reduce oscillations and enhance tracking accuracy. Adaptive control dynamically adjusts system parameters, enhancing robustness against external uncertainties, while the neural network compensates for time-varying disturbances through real-time learning. The stability of the control scheme is verified on the basis of Lyapunov theory. The simulation results demonstrate that, compared to traditional PID control, the proposed controller exhibits significant advantages in trajectory tracking accuracy, response speed, and disturbance rejection.         ",
    "url": "https://arxiv.org/abs/2509.07356",
    "authors": [
      "Qian Zuo",
      "Shujie Wu",
      "Yuzhe Qian"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.07367",
    "title": "Autonomous Code Evolution Meets NP-Completeness",
    "abstract": "           Large language models (LLMs) have recently shown strong coding abilities, enabling not only static code generation but also iterative code self-evolving through agentic frameworks. Recently, AlphaEvolve \\cite{novikov2025alphaevolve} demonstrated that LLM-based coding agents can autonomously improve algorithms and surpass human experts, with scopes limited to isolated kernels spanning hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the first framework to extend LLM-based code evolution to the full repository scale, encompassing hundreds of files and tens of thousands of lines of C/C++ code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem and a cornerstone of both theory and applications. SATLUTION orchestrates LLM agents to directly evolve solver repositories under strict correctness guarantees and distributed runtime feedback, while simultaneously self-evolving its own evolution policies and rules. Starting from SAT Competition 2024 codebases and benchmark, SATLUTION evolved solvers that decisively outperformed the human-designed winners of the SAT Competition 2025, and also surpassed both 2024 and 2025 champions on the 2024 benchmarks.         ",
    "url": "https://arxiv.org/abs/2509.07367",
    "authors": [
      "Cunxi Yu",
      "Rongjian Liang",
      "Chia-Tung Ho",
      "Haoxing Ren"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2509.07373",
    "title": "SBS: Enhancing Parameter-Efficiency of Neural Representations for Neural Networks via Spectral Bias Suppression",
    "abstract": "           Implicit neural representations have recently been extended to represent convolutional neural network weights via neural representation for neural networks, offering promising parameter compression benefits. However, standard multi-layer perceptrons used in neural representation for neural networks exhibit a pronounced spectral bias, hampering their ability to reconstruct high-frequency details effectively. In this paper, we propose SBS, a parameter-efficient enhancement to neural representation for neural networks that suppresses spectral bias using two techniques: (1) a unidirectional ordering-based smoothing that improves kernel smoothness in the output space, and (2) unidirectional ordering-based smoothing aware random fourier features that adaptively modulate the frequency bandwidth of input encodings based on layer-wise parameter count. Extensive evaluations on various ResNet models with datasets CIFAR-10, CIFAR-100, and ImageNet, demonstrate that SBS achieves significantly better reconstruction accuracy with less parameters compared to SOTA.         ",
    "url": "https://arxiv.org/abs/2509.07373",
    "authors": [
      "Qihu Xie",
      "Yuan Li",
      "Yi Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07381",
    "title": "TransMPC: Transformer-based Explicit MPC with Variable Prediction Horizon",
    "abstract": "           Traditional online Model Predictive Control (MPC) methods often suffer from excessive computational complexity, limiting their practical deployment. Explicit MPC mitigates online computational load by pre-computing control policies offline; however, existing explicit MPC methods typically rely on simplified system dynamics and cost functions, restricting their accuracy for complex systems. This paper proposes TransMPC, a novel Transformer-based explicit MPC algorithm capable of generating highly accurate control sequences in real-time for complex dynamic systems. Specifically, we formulate the MPC policy as an encoder-only Transformer leveraging bidirectional self-attention, enabling simultaneous inference of entire control sequences in a single forward pass. This design inherently accommodates variable prediction horizons while ensuring low inference latency. Furthermore, we introduce a direct policy optimization framework that alternates between sampling and learning phases. Unlike imitation-based approaches dependent on precomputed optimal trajectories, TransMPC directly optimizes the true finite-horizon cost via automatic differentiation. Random horizon sampling combined with a replay buffer provides independent and identically distributed (i.i.d.) training samples, ensuring robust generalization across varying states and horizon lengths. Extensive simulations and real-world vehicle control experiments validate the effectiveness of TransMPC in terms of solution accuracy, adaptability to varying horizons, and computational efficiency.         ",
    "url": "https://arxiv.org/abs/2509.07381",
    "authors": [
      "Sichao Wu",
      "Jiang Wu",
      "Xingyu Cao",
      "Fawang Zhang",
      "Guangyuan Yu",
      "Junjie Zhao",
      "Yue Qu",
      "Fei Ma",
      "Jingliang Duan"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.07388",
    "title": "EfficientNet in Digital Twin-based Cardiac Arrest Prediction and Analysis",
    "abstract": "           Cardiac arrest is one of the biggest global health problems, and early identification and management are key to enhancing the patient's prognosis. In this paper, we propose a novel framework that combines an EfficientNet-based deep learning model with a digital twin system to improve the early detection and analysis of cardiac arrest. We use compound scaling and EfficientNet to learn the features of cardiovascular images. In parallel, the digital twin creates a realistic and individualized cardiovascular system model of the patient based on data received from the Internet of Things (IoT) devices attached to the patient, which can help in the constant assessment of the patient and the impact of possible treatment plans. As shown by our experiments, the proposed system is highly accurate in its prediction abilities and, at the same time, efficient. Combining highly advanced techniques such as deep learning and digital twin (DT) technology presents the possibility of using an active and individual approach to predicting cardiac disease.         ",
    "url": "https://arxiv.org/abs/2509.07388",
    "authors": [
      "Qasim Zia",
      "Avais Jan",
      "Zafar Iqbal",
      "Muhammad Mumtaz Ali",
      "Mukarram Ali",
      "Murray Patterson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07392",
    "title": "Hybrid GCN-GRU Model for Anomaly Detection in Cryptocurrency Transactions",
    "abstract": "           Blockchain transaction networks are complex, with evolving temporal patterns and inter-node relationships. To detect illicit activities, we propose a hybrid GCN-GRU model that captures both structural and sequential features. Using real Bitcoin transaction data (2020-2024), our model achieved 0.9470 Accuracy and 0.9807 AUC-ROC, outperforming all baselines.         ",
    "url": "https://arxiv.org/abs/2509.07392",
    "authors": [
      "Gyuyeon Na",
      "Minjung Park",
      "Hyeonjeong Cha",
      "Soyoun Kim",
      "Sunyoung Moon",
      "Sua Lee",
      "Jaeyoung Choi",
      "Hyemin Lee",
      "Sangmi Chai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07399",
    "title": "The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering",
    "abstract": "           Integrating knowledge graphs (KGs) into the reasoning processes of large language models (LLMs) has emerged as a promising approach to mitigate hallucination. However, existing work in this area often relies on proprietary or extremely large models, limiting accessibility and scalability. In this study, we investigate the capabilities of existing integration methods for small language models (SLMs) in KG-based question answering and observe that their performance is often constrained by their limited ability to traverse and reason over knowledge graphs. To address this limitation, we propose leveraging simple and efficient exploration modules to handle knowledge graph traversal in place of the language model itself. Experiment results demonstrate that these lightweight modules effectively improve the performance of small language models on knowledge graph question answering tasks. Source code: this https URL.         ",
    "url": "https://arxiv.org/abs/2509.07399",
    "authors": [
      "Yi-Jie Cheng",
      "Oscar Chew",
      "Yun-Nung Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.07411",
    "title": "Adaptive Evolutionary Framework for Safe, Efficient, and Cooperative Autonomous Vehicle Interactions",
    "abstract": "           Modern transportation systems face significant challenges in ensuring road safety, given serious injuries caused by road accidents. The rapid growth of autonomous vehicles (AVs) has prompted new traffic designs that aim to optimize interactions among AVs. However, effective interactions between AVs remains challenging due to the absence of centralized control. Besides, there is a need for balancing multiple factors, including passenger demands and overall traffic efficiency. Traditional rule-based, optimization-based, and game-theoretic approaches each have limitations in addressing these challenges. Rule-based methods struggle with adaptability and generalization in complex scenarios, while optimization-based methods often require high computational resources. Game-theoretic approaches, such as Stackelberg and Nash games, suffer from limited adaptability and potential inefficiencies in cooperative settings. This paper proposes an Evolutionary Game Theory (EGT)-based framework for AV interactions that overcomes these limitations by utilizing a decentralized and adaptive strategy evolution mechanism. A causal evaluation module (CEGT) is introduced to optimize the evolutionary rate, balancing mutation and evolution by learning from historical interactions. Simulation results demonstrate the proposed CEGT outperforms EGT and popular benchmark games in terms of lower collision rates, improved safety distances, higher speeds, and overall better performance compared to Nash and Stackelberg games across diverse scenarios and parameter settings.         ",
    "url": "https://arxiv.org/abs/2509.07411",
    "authors": [
      "Zhen Tian",
      "Zhihao Lin"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.07413",
    "title": "Robust Docking Maneuvers for Autonomous Trolley Collection: An Optimization-Based Visual Servoing Scheme",
    "abstract": "           Service robots have demonstrated significant potential for autonomous trolley collection and redistribution in public spaces like airports or warehouses to improve efficiency and reduce cost. Usually, a fully autonomous system for the collection and transportation of multiple trolleys is based on a Leader-Follower formation of mobile manipulators, where reliable docking maneuvers of the mobile base are essential to align trolleys into organized queues. However, developing a vision-based robotic docking system faces significant challenges: high precision requirements, environmental disturbances, and inherent robot constraints. To address these challenges, we propose an optimization-based Visual Servoing scheme that incorporates active infrared markers for robust feature extraction across diverse lighting conditions. This framework explicitly models nonholonomic kinematics and visibility constraints within the Hybrid Visual Servoing problem, augmented with an observer for disturbance rejection to ensure precise and stable docking. Experimental results across diverse environments demonstrate the robustness of this system, with quantitative evaluations confirming high docking accuracy.         ",
    "url": "https://arxiv.org/abs/2509.07413",
    "authors": [
      "Yuhan Pang",
      "Bingyi Xia",
      "Zhe Zhang",
      "Zhirui Sun",
      "Peijia Xie",
      "Bike Zhu",
      "Wenjun Xu",
      "Jiankun Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.07463",
    "title": "DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis",
    "abstract": "           Ensuring reliable robot operation when visual input is degraded or insufficient remains a central challenge in robotics. This letter introduces DepthVision, a framework for multimodal scene understanding designed to address this problem. Unlike existing Vision-Language Models (VLMs), which use only camera-based visual input alongside language, DepthVision synthesizes RGB images from sparse LiDAR point clouds using a conditional generative adversarial network (GAN) with an integrated refiner network. These synthetic views are then combined with real RGB data using a Luminance-Aware Modality Adaptation (LAMA), which blends the two types of data dynamically based on ambient lighting conditions. This approach compensates for sensor degradation, such as darkness or motion blur, without requiring any fine-tuning of downstream vision-language models. We evaluate DepthVision on real and simulated datasets across various models and tasks, with particular attention to safety-critical tasks. The results demonstrate that our approach improves performance in low-light conditions, achieving substantial gains over RGB-only baselines while preserving compatibility with frozen VLMs. This work highlights the potential of LiDAR-guided RGB synthesis for achieving robust robot operation in real-world environments.         ",
    "url": "https://arxiv.org/abs/2509.07463",
    "authors": [
      "Sven Kirchner",
      "Nils Purschke",
      "Ross Greer",
      "Alois C. Knoll"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07471",
    "title": "From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation",
    "abstract": "           The linguistic diversity across the African continent presents different challenges and opportunities for machine translation. This study explores the effects of data augmentation techniques in improving translation systems in low-resource African languages. We focus on two data augmentation techniques: sentence concatenation with back translation and switch-out, applying them across six African languages. Our experiments show significant improvements in machine translation performance, with a minimum increase of 25\\% in BLEU score across all six this http URL provide a comprehensive analysis and highlight the potential of these techniques to improve machine translation systems for low-resource languages, contributing to the development of more robust translation systems for under-resourced languages.         ",
    "url": "https://arxiv.org/abs/2509.07471",
    "authors": [
      "Mardiyyah Oduwole",
      "Oluwatosin Olajide",
      "Jamiu Suleiman",
      "Faith Hunja",
      "Busayo Awobade",
      "Fatimo Adebanjo",
      "Comfort Akanni",
      "Chinonyelum Igwe",
      "Peace Ododo",
      "Promise Omoigui",
      "Steven Kolawole",
      "Abraham Owodunni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.07475",
    "title": "HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention",
    "abstract": "           Detecting content that contradicts or is unsupported by a given source text is a critical challenge for the safe deployment of generative language models. We introduce HALT-RAG, a post-hoc verification system designed to identify hallucinations in the outputs of Retrieval-Augmented Generation (RAG) pipelines. Our flexible and task-adaptable framework uses a universal feature set derived from an ensemble of two frozen, off-the-shelf Natural Language Inference (NLI) models and lightweight lexical signals. These features are used to train a simple, calibrated, and task-adapted meta-classifier. Using a rigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and produce unbiased estimates, we evaluate our system on the HaluEval benchmark. By pairing our universal feature set with a lightweight, task-adapted classifier and a precision-constrained decision policy, HALT-RAG achieves strong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA, and dialogue tasks, respectively. The system's well-calibrated probabilities enable a practical abstention mechanism, providing a reliable tool for balancing model performance with safety requirements.         ",
    "url": "https://arxiv.org/abs/2509.07475",
    "authors": [
      "Saumya Goswami",
      "Siddharth Kurra"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07484",
    "title": "LINR Bridge: Vector Graphic Animation via Neural Implicits and Video Diffusion Priors",
    "abstract": "           Vector graphics, known for their scalability and user-friendliness, provide a unique approach to visual content compared to traditional pixel-based images. Animation of these graphics, driven by the motion of their elements, offers enhanced comprehensibility and controllability but often requires substantial manual effort. To automate this process, we propose a novel method that integrates implicit neural representations with text-to-video diffusion models for vector graphic animation. Our approach employs layered implicit neural representations to reconstruct vector graphics, preserving their inherent properties such as infinite resolution and precise color and shape constraints, which effectively bridges the large domain gap between vector graphics and diffusion models. The neural representations are then optimized using video score distillation sampling, which leverages motion priors from pretrained text-to-video diffusion models. Finally, the vector graphics are warped to match the representations resulting in smooth animation. Experimental results validate the effectiveness of our method in generating vivid and natural vector graphic animations, demonstrating significant improvement over existing techniques that suffer from limitations in flexibility and animation quality.         ",
    "url": "https://arxiv.org/abs/2509.07484",
    "authors": [
      "Wenshuo Gao",
      "Xicheng Lan",
      "Luyao Zhang",
      "Shuai Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07492",
    "title": "Constraint-Compliant Network Optimization through Large Language Models",
    "abstract": "           This work develops an LLM-based optimization framework ensuring strict constraint satisfaction in network optimization. While LLMs possess contextual reasoning capabilities, existing approaches often fail to enforce constraints, causing infeasible solutions. Unlike conventional methods that address average constraints, the proposed framework integrates a natural language-based input encoding strategy to restrict the solution space and guarantee feasibility. For multi-access edge computing networks, task allocation is optimized while minimizing worst-case latency. Numerical evaluations demonstrate LLMs as a promising tool for constraint-aware network optimization, offering insights into their inference capabilities.         ",
    "url": "https://arxiv.org/abs/2509.07492",
    "authors": [
      "Youngjin Song",
      "Wookjin Lee",
      "Hong Ki Kim",
      "Sang Hyun Lee"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2509.07495",
    "title": "Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition",
    "abstract": "           Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing significant security threats to their deployment in remote sensing applications. Research on adversarial attacks not only reveals model vulnerabilities but also provides critical insights for enhancing robustness. Although current mixing-based strategies have been proposed to increase the transferability of adversarial examples, they either perform global blending or directly exchange a region in the images, which may destroy global semantic features and mislead the optimization of adversarial examples. Furthermore, their reliance on cross-entropy loss for perturbation optimization leads to gradient diminishing during iterative updates, compromising adversarial example quality. To address these limitations, we focus on non-targeted attacks and propose a novel framework via local mixing and logits optimization. First, we present a local mixing strategy to generate diverse yet semantically consistent inputs. Different from MixUp, which globally blends two images, and MixCut, which stitches images together, our method merely blends local regions to preserve global semantic information. Second, we adapt the logit loss from targeted attacks to non-targeted scenarios, mitigating the gradient vanishing problem of cross-entropy loss. Third, a perturbation smoothing loss is applied to suppress high-frequency noise and enhance transferability. Extensive experiments on FGSCR-42 and MTARSI datasets demonstrate superior performance over 12 state-of-the-art methods across 6 surrogate models. Notably, with ResNet as the surrogate on MTARSI, our method achieves a 17.28% average improvement in black-box attack success rate.         ",
    "url": "https://arxiv.org/abs/2509.07495",
    "authors": [
      "Chun Liu",
      "Hailong Wang",
      "Bingqian Zhu",
      "Panpan Ding",
      "Zheng Zheng",
      "Tao Xu",
      "Zhigang Han",
      "Jiayao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07499",
    "title": "Conv4Rec: A 1-by-1 Convolutional AutoEncoder for User Profiling through Joint Analysis of Implicit and Explicit Feedbacks",
    "abstract": "           We introduce a new convolutional AutoEncoder architecture for user modelling and recommendation tasks with several improvements over the state of the art. Firstly, our model has the flexibility to learn a set of associations and combinations between different interaction types in a way that carries over to each user and item. Secondly, our model is able to learn jointly from both the explicit ratings and the implicit information in the sampling pattern (which we refer to as `implicit feedback'). It can also make separate predictions for the probability of consuming content and the likelihood of granting it a high rating if observed. This not only allows the model to make predictions for both the implicit and explicit feedback, but also increases the informativeness of the predictions: in particular, our model can identify items which users would not have been likely to consume naturally, but would be likely to enjoy if exposed to them. Finally, we provide several generalization bounds for our model, which to the best of our knowledge, are among the first generalization bounds for auto-encoders in a Recommender Systems setting; we also show that optimizing our loss function guarantees the recovery of the exact sampling distribution over interactions up to a small error in total variation. In experiments on several real-life datasets, we achieve state-of-the-art performance on both the implicit and explicit feedback prediction tasks despite relying on a single model for both, and benefiting from additional interpretability in the form of individual predictions for the probabilities of each possible rating.         ",
    "url": "https://arxiv.org/abs/2509.07499",
    "authors": [
      "Antoine Ledent",
      "Petr Kasalick\u00fd",
      "Rodrigo Alves",
      "Hady W. Lauw"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07502",
    "title": "Social Media Clones: Exploring the Impact of Social Delegation with AI Clones through a Design Workbook Study",
    "abstract": "           Social media clones are AI-powered social delegates of ourselves created using our personal data. As our identities and online personas intertwine, these technologies have the potential to greatly enhance our social media experience. If mismanaged, however, these clones may also pose new risks to our social reputation and online relationships. To set the foundation for a productive and responsible integration, we set out to understand how social media clones will impact our online behavior and interactions. We conducted a series of semi-structured interviews introducing eight speculative clone concepts to 32 social media users through a design workbook. Applying existing work in AI-mediated communication in the context of social media, we found that although clones can offer convenience and comfort, they can also threaten the user's authenticity and increase skepticism within the online community. As a result, users tend to behave more like their clones to mitigate discrepancies and interaction breakdowns. These findings are discussed through the lens of past literature in identity and impression management to highlight challenges in the adoption of social media clones by the general public, and propose design considerations for their successful integration into social media platforms.         ",
    "url": "https://arxiv.org/abs/2509.07502",
    "authors": [
      "Jackie Liu",
      "Mehrnoosh Sadat Shirvani",
      "Hwajung Hong",
      "Ig-Jae Kim",
      "Dongwook Yoon"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2509.07504",
    "title": "Backdoor Attacks and Defenses in Computer Vision Domain: A Survey",
    "abstract": "           Backdoor (trojan) attacks embed hidden, controllable behaviors into machine-learning models so that models behave normally on benign inputs but produce attacker-chosen outputs when a trigger is present. This survey reviews the rapidly growing literature on backdoor attacks and defenses in the computer-vision domain. We introduce a multi-dimensional taxonomy that organizes attacks and defenses by injection stage (dataset poisoning, model/parameter modification, inference-time injection), trigger type (patch, blended/frequency, semantic, transformation), labeling strategy (dirty-label vs. clean-label / feature-collision), representation stage (instance-specific, manifold/class-level, neuron/parameter hijacking, distributed encodings), and target task (classification, detection, segmentation, video, multimodal). For each axis we summarize representative methods, highlight evaluation practices, and discuss where defenses succeed or fail. For example, many classical sanitization and reverse-engineering tools are effective against reusable patch attacks but struggle with input-aware, sample-specific, or parameter-space backdoors and with transfer via compromised pre-trained encoders or hardware bit-flips. We synthesize trends, identify persistent gaps (supply-chain and hardware threats, certifiable defenses, cross-task benchmarks), and propose practical guidelines for threat-aware evaluation and layered defenses. This survey aims to orient researchers and practitioners to the current threat landscape and pressing research directions in secure computer vision.         ",
    "url": "https://arxiv.org/abs/2509.07504",
    "authors": [
      "Bilal Hussain Abbasi",
      "Yanjun Zhang",
      "Leo Zhang",
      "Shang Gao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.07505",
    "title": "Extension of Spatial k-Anonymity: New Metrics for Assessing the Anonymity of Geomasked Data Considering Realistic Attack Scenarios",
    "abstract": "           Spatial data are gaining increasing importance in many areas of research. Particularly spatial health data are becoming increasingly important for medical research, for example, to better understand relationships between environmental factors and disease patterns. However, their use is often restricted by legal data protection regulations, since georeferenced personal information carries a high risk of re-identification of individuals. To address this issue, what are called geomasking methods are applied to guarantee data protection through targeted displacement of individual data points, while simultaneously maintaining analytical validity within a tolerable range. In the current literature the degree of anonymity of such anonymized georeferenced datasets is often measured by the so-called metric of spatial k-anonymity. However, this metric has considerable shortcomings, particularly regarding its resilience against realistic data attack scenarios. This article classifies the potential data attack scenarios in the context of anonymized georeferenced microdata and introduces appropriate metrics that enable a comprehensive assessment of anonymity adapted to potential data attack scenarios.         ",
    "url": "https://arxiv.org/abs/2509.07505",
    "authors": [
      "Simon Cremer",
      "Lydia Jehmlich",
      "Rainer Lenz"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.07507",
    "title": "MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection",
    "abstract": "           Annotating 3D data remains a costly bottleneck for 3D object detection, motivating the development of weakly supervised annotation methods that rely on more accessible 2D box annotations. However, relying solely on 2D boxes introduces projection ambiguities since a single 2D box can correspond to multiple valid 3D poses. Furthermore, partial object visibility under a single viewpoint setting makes accurate 3D box estimation difficult. We propose MVAT, a novel framework that leverages temporal multi-view present in sequential data to address these challenges. Our approach aggregates object-centric point clouds across time to build 3D object representations as dense and complete as possible. A Teacher-Student distillation paradigm is employed: The Teacher network learns from single viewpoints but targets are derived from temporally aggregated static objects. Then the Teacher generates high quality pseudo-labels that the Student learns to predict from a single viewpoint for both static and moving objects. The whole framework incorporates a multi-view 2D projection loss to enforce consistency between predicted 3D boxes and all available 2D annotations. Experiments on the nuScenes and Waymo Open datasets demonstrate that MVAT achieves state-of-the-art performance for weakly supervised 3D object detection, significantly narrowing the gap with fully supervised methods without requiring any 3D box annotations. % \\footnote{Code available upon acceptance} Our code is available in our public repository (\\href{this https URL}{code}).         ",
    "url": "https://arxiv.org/abs/2509.07507",
    "authors": [
      "Saad Lahlali",
      "Alexandre Fournier Montgieux",
      "Nicolas Granger",
      "Herv\u00e9 Le Borgne",
      "Quoc Cuong Pham"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07522",
    "title": "Neural Cone Radiosity for Interactive Global Illumination with Glossy Materials",
    "abstract": "           Modeling of high-frequency outgoing radiance distributions has long been a key challenge in rendering, particularly for glossy material. Such distributions concentrate radiative energy within a narrow lobe and are highly sensitive to changes in view direction. However, existing neural radiosity methods, which primarily rely on positional feature encoding, exhibit notable limitations in capturing these high-frequency, strongly view-dependent radiance distributions. To address this, we propose a highly-efficient approach by reflectance-aware ray cone encoding based on the neural radiosity framework, named neural cone radiosity. The core idea is to employ a pre-filtered multi-resolution hash grid to accurately approximate the glossy BSDF lobe, embedding view-dependent reflectance characteristics directly into the encoding process through continuous spatial aggregation. Our design not only significantly improves the network's ability to model high-frequency reflection distributions but also effectively handles surfaces with a wide range of glossiness levels, from highly glossy to low-gloss finishes. Meanwhile, our method reduces the network's burden in fitting complex radiance distributions, allowing the overall architecture to remain compact and efficient. Comprehensive experimental results demonstrate that our method consistently produces high-quality, noise-free renderings in real time under various glossiness conditions, and delivers superior fidelity and realism compared to baseline approaches.         ",
    "url": "https://arxiv.org/abs/2509.07522",
    "authors": [
      "Jierui Ren",
      "Haojie Jin",
      "Bo Pang",
      "Yisong Chen",
      "Guoping Wang",
      "Sheng Li"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07523",
    "title": "RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection",
    "abstract": "           Identifying recurring patterns and rare events in large-scale signals is a fundamental challenge in fields such as astronomy, physical simulations, and biomedical science. Convolutional Dictionary Learning (CDL) offers a powerful framework for modeling local structures in signals, but its use for detecting rare or anomalous events remains largely unexplored. In particular, CDL faces two key challenges in this setting: high computational cost and sensitivity to artifacts and outliers. In this paper, we introduce RoseCDL, a scalable and robust CDL algorithm designed for unsupervised rare event detection in long signals. RoseCDL combines stochastic windowing for efficient training on large datasets with inline outlier detection to enhance robustness and isolate anomalous patterns. This reframes CDL as a practical tool for event discovery and characterization in real-world signals, extending its role beyond traditional tasks like compression or denoising.         ",
    "url": "https://arxiv.org/abs/2509.07523",
    "authors": [
      "Jad Yehya",
      "Mansour Benbakoura",
      "C\u00e9dric Allain",
      "Beno\u00eet Malezieux",
      "Matthieu Kowalski",
      "Thomas Moreau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07531",
    "title": "FLeW: Facet-Level and Adaptive Weighted Representation Learning of Scientific Documents",
    "abstract": "           Scientific document representation learning provides powerful embeddings for various tasks, while current methods face challenges across three approaches. 1) Contrastive training with citation-structural signals underutilizes citation information and still generates single-vector representations. 2) Fine-grained representation learning, which generates multiple vectors at the sentence or aspect level, requires costly integration and lacks domain generalization. 3) Task-aware learning depends on manually predefined task categorization, overlooking nuanced task distinctions and requiring extra training data for task-specific modules. To address these problems, we propose a new method that unifies the three approaches for better representations, namely FLeW. Specifically, we introduce a novel triplet sampling method that leverages citation intent and frequency to enhance citation-structural signals for training. Citation intents (background, method, result), aligned with the general structure of scientific writing, facilitate a domain-generalized facet partition for fine-grained representation learning. Then, we adopt a simple weight search to adaptively integrate three facet-level embeddings into a task-specific document embedding without task-aware fine-tuning. Experiments show the applicability and robustness of FLeW across multiple scientific tasks and fields, compared to prior models.         ",
    "url": "https://arxiv.org/abs/2509.07531",
    "authors": [
      "Zheng Dou",
      "Deqing Wang",
      "Fuzhen Zhuang",
      "Jian Ren",
      "Yanlin Hu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07532",
    "title": "Uncertainty-Driven Hierarchical Sampling for Unbalanced Continual Malware Detection with Time-Series Update-Based Retrieval",
    "abstract": "           Android malware detection continues to face persistent challenges stemming from long-term concept drift and class imbalance, as evolving malicious behaviors and shifting usage patterns dynamically reshape feature distributions. Although continual learning (CL) mitigates drift, existing replay-based methods suffer from inherent bias. Specifically, their reliance on classifier uncertainty for sample selection disproportionately prioritizes the dominant benign class, causing overfitting and reduced generalization to evolving malware. To address these limitations, we propose a novel uncertainty-guided CL framework. First, we introduce a hierarchical balanced sampler that employs a dual-phase uncertainty strategy to dynamically balance benign and malicious samples while simultaneously selecting high-information, high-uncertainty instances within each class. This mechanism ensures class equilibrium across both replay and incremental data, thereby enhancing adaptability to emerging threats. Second, we augment the framework with a vector retrieval mechanism that exploits historical malware embeddings to identify evolved variants via similarity-based retrieval, thereby complementing classifier updates. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods under strict low-label conditions (50 labels per phase). It achieves a true positive rate (TPR) of 92.95\\% and a mean accuracy (mACC) of 94.26\\%, which validates its efficacy for sustainable Android malware detection.         ",
    "url": "https://arxiv.org/abs/2509.07532",
    "authors": [
      "Yi Xie",
      "Ziyuan Yang",
      "Yongqiang Huang",
      "Yinyu Chen",
      "Lei Zhang",
      "Liang Liu",
      "Yi Zhang"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2509.07548",
    "title": "FlexSAN: A Flexible Regenerative Satellite Access Network Architecture",
    "abstract": "           The regenerative satellite access network (SAN) architecture deploys next-generation NodeB (gNBs) on satellites to enable enhanced network management capabilities. It supports two types of regenerative payload, on-board gNB and on-board gNB-Distributed Unit (gNB-DU). Measurement results based on our prototype implementation show that the on-board gNB offers lower latency, while the on-board gNB-DU is more cost-effective, and there is often a trade-off between Quality-of-Service (QoS) and operational expenditure (OPEX) when choosing between the two payload types. However, current SAN configurations are static and inflexible -- either deploying the full on-board gNB or only the on-board gNB-DU. This rigidity can lead to resource waste or poor user experiences. In this paper, we propose Flexible SAN (FlexSAN), an adaptive satellite access network architecture that dynamically configures the optimal regenerative payload based on real-time user demands. FlexSAN selects the lowest OPEX payload configuration when all user demands are satisfied, and otherwise maximizes the number of admitted users while ensuring QoS for connected users. To address the computational complexity of dynamic payload selection, we design an adaptive greedy heuristic algorithm. Extensive experiments validate FlexSAN's effectiveness, showing a 36.1% average improvement in user admission rates and a 15% OPEX reduction over static SANs.         ",
    "url": "https://arxiv.org/abs/2509.07548",
    "authors": [
      "Weize Kong",
      "Chaoqun You",
      "Xuming Pei",
      "YueGao"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2509.07569",
    "title": "uGMM-NN: Univariate Gaussian Mixture Model Neural Network",
    "abstract": "           This paper introduces the Univariate Gaussian Mixture Model Neural Network (uGMM-NN), a novel neural architecture that embeds probabilistic reasoning directly into the computational units of deep networks. Unlike traditional neurons, which apply weighted sums followed by fixed nonlinearities, each uGMM-NN node parameterizes its activations as a univariate Gaussian mixture, with learnable means, variances, and mixing coefficients. This design enables richer representations by capturing multimodality and uncertainty at the level of individual neurons, while retaining the scalability of standard feedforward networks. We demonstrate that uGMM-NN can achieve competitive discriminative performance compared to conventional multilayer perceptrons, while additionally offering a probabilistic interpretation of activations. The proposed framework provides a foundation for integrating uncertainty-aware components into modern neural architectures, opening new directions for both discriminative and generative modeling.         ",
    "url": "https://arxiv.org/abs/2509.07569",
    "authors": [
      "Zakeria Sharif Ali"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2509.07577",
    "title": "Towards explainable decision support using hybrid neural models for logistic terminal automation",
    "abstract": "           The integration of Deep Learning (DL) in System Dynamics (SD) modeling for transportation logistics offers significant advantages in scalability and predictive accuracy. However, these gains are often offset by the loss of explainability and causal reliability $-$ key requirements in critical decision-making systems. This paper presents a novel framework for interpretable-by-design neural system dynamics modeling that synergizes DL with techniques from Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning. The proposed hybrid approach enables the construction of neural network models that operate on semantically meaningful and actionable variables, while retaining the causal grounding and transparency typical of traditional SD models. The framework is conceived to be applied to real-world case-studies from the EU-funded project AutoMoTIF, focusing on data-driven decision support, automation, and optimization of multimodal logistic terminals. We aim at showing how neuro-symbolic methods can bridge the gap between black-box predictive models and the need for critical decision support in complex dynamical environments within cyber-physical systems enabled by the industrial Internet-of-Things.         ",
    "url": "https://arxiv.org/abs/2509.07577",
    "authors": [
      "Riccardo DElia",
      "Alberto Termine",
      "Francesco Flammini"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07579",
    "title": "Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks",
    "abstract": "           Physics-informed neural networks (PINNs) have shown promise in solving partial differential equations (PDEs) relevant to multiscale modeling, but they often fail when applied to materials with discontinuous coefficients, such as media with piecewise constant properties. This paper introduces a dual formulation for the PINN framework to improve the reliability of the homogenization of periodic thermo-conductive composites, for both strong and variational (weak) formulations. The dual approach facilitates the derivation of guaranteed upper and lower error bounds, enabling more robust detection of PINN failure. We compare standard PINNs applied to smoothed material approximations with variational PINNs (VPINNs) using both spectral and neural network-based test functions. Our results indicate that while strong-form PINNs may outperform VPINNs in controlled settings, they are sensitive to material discontinuities and may fail without clear diagnostics. In contrast, VPINNs accommodate piecewise constant material parameters directly but require careful selection of test functions to avoid instability. Dual formulation serves as a reliable indicator of convergence quality, and its integration into PINN frameworks enhances their applicability to homogenization problems in micromechanics.         ",
    "url": "https://arxiv.org/abs/2509.07579",
    "authors": [
      "Liya Gaynutdinova",
      "Martin Do\u0161k\u00e1\u0159",
      "Ond\u0159ej Roko\u0161",
      "Ivana Pultarov\u00e1"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2509.07581",
    "title": "Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks",
    "abstract": "           Deep learning offers a promising avenue for automating many recognition tasks in fields such as medicine and forensics. However, the black-box nature of these models hinders their adoption in high-stakes applications where trust and accountability are required. For 3D shape recognition tasks in particular, this paper introduces the Class Node Graph Attention Network (CGAT) architecture to address this need. Applied to 3D meshes of third molars derived from CBCT images, for Demirjian stage allocation, CGAT utilizes graph attention convolutions and an inherent attention mechanism, visualized via attention rollout, to explain its decision-making process. We evaluated the local mean curvature and distance to centroid node features, both individually and in combination, as well as model depth, finding that models incorporating directed edges to a global CLS node produced more intuitive attention maps, while also yielding desirable classification performance. We analyzed the attention-based explanations of the models, and their predictive performances to propose optimal settings for the CGAT. The combination of local mean curvature and distance to centroid as node features yielded a slight performance increase with 0.76 weighted F1 score, and more comprehensive attention visualizations. The CGAT architecture's ability to generate human-understandable attention maps can enhance trust and facilitate expert validation of model decisions. While demonstrated on dental data, CGAT is broadly applicable to graph-based classification and regression tasks, promoting wider adoption of transparent and competitive deep learning models in high-stakes environments.         ",
    "url": "https://arxiv.org/abs/2509.07581",
    "authors": [
      "Barkin Buyukcakir",
      "Rocharles Cavalcante Fontenele",
      "Reinhilde Jacobs",
      "Jannick De Tobel",
      "Patrick Thevissen",
      "Dirk Vandermeulen",
      "Peter Claes"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07588",
    "title": "BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment",
    "abstract": "           In recent years, there has been substantial progress in using pretrained Language Models (LMs) on a range of tasks aimed at improving the understanding of biomedical texts. Nonetheless, existing biomedical LLMs show limited comprehension of complex, domain-specific concept structures and the factual information encoded in biomedical Knowledge Graphs (KGs). In this work, we propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph. For a given textual sequence, we link biomedical concept mentions to the Unified Medical Language System (UMLS) KG and utilize local KG subgraphs as cross-modal positive samples for these mentions. Our empirical findings indicate that implementing our method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations, even with minimal pre-training on a small alignment dataset sourced from PubMed scientific abstracts.         ",
    "url": "https://arxiv.org/abs/2509.07588",
    "authors": [
      "Andrey Sakhovskiy",
      "Elena Tutubalina"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07594",
    "title": "ELEC: Efficient Large Language Model-Empowered Click-Through Rate Prediction",
    "abstract": "           Click-through rate (CTR) prediction plays an important role in online advertising systems. On the one hand, traditional CTR prediction models capture the collaborative signals in tabular data via feature interaction modeling, but they lose semantics in text. On the other hand, Large Language Models (LLMs) excel in understanding the context and meaning behind text, but they face challenges in capturing collaborative signals and they have long inference latency. In this paper, we aim to leverage the benefits of both types of models and pursue collaboration, semantics and efficiency. We present ELEC, which is an Efficient LLM-Empowered CTR prediction framework. We first adapt an LLM for the CTR prediction task. In order to leverage the ability of the LLM but simultaneously keep efficiency, we utilize the pseudo-siamese network which contains a gain network and a vanilla network. We inject the high-level representation vector generated by the LLM into a collaborative CTR model to form the gain network such that it can take advantage of both tabular modeling and textual modeling. However, its reliance on the LLM limits its efficiency. We then distill the knowledge from the gain network to the vanilla network on both the score level and the representation level, such that the vanilla network takes only tabular data as input, but can still generate comparable performance as the gain network. Our approach is model-agnostic. It allows for the integration with various existing LLMs and collaborative CTR models. Experiments on real-world datasets demonstrate the effectiveness and efficiency of ELEC for CTR prediction.         ",
    "url": "https://arxiv.org/abs/2509.07594",
    "authors": [
      "Rui Dong",
      "Wentao Ouyang",
      "Xiangzheng Liu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2509.07595",
    "title": "AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with FaaS-hosted MCP Services",
    "abstract": "           Generative Artificial Intelligence (GenAI) has rapidly transformed various fields including code generation, text summarization, image generation and so on. Agentic AI is a recent evolution that further advances this by coupling the decision making and generative capabilities of LLMs with actions that can be performed using tools. While seemingly powerful, Agentic systems often struggle when faced with numerous tools, complex multi-step tasks,and long-context management to track history and avoid hallucinations. Workflow patterns such as Chain-of-Thought (CoT) and ReAct help address this. Here, we define a novel agentic workflow pattern, AgentX, composed of stage designer, planner, and executor agents that is competitive or better than the state-of-the-art agentic patterns. We also leverage Model Context Protocol (MCP) tools, and propose two alternative approaches for deploying MCP servers as cloud Functions as a Service (FaaS). We empirically evaluate the success rate, latency and cost for AgentX and two contemporary agentic patterns, ReAct and Magentic One, using these the FaaS and local MCP server alternatives for three practical applications. This highlights the opportunities and challenges of designing and deploying agentic workflows.         ",
    "url": "https://arxiv.org/abs/2509.07595",
    "authors": [
      "Shiva Sai Krishna Anand Tokal",
      "Vaibhav Jha",
      "Anand Eswaran",
      "Praveen Jayachandran",
      "Yogesh Simmhan"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2509.07606",
    "title": "Enhanced cast-128 with adaptive s-box optimization via neural networks for image protection",
    "abstract": "           An improved CAST-128 encryption algorithm, which is done by implementing chaos-based adaptive S-box generation using Logistic sine Map (LSM), has been provided in this paper because of the increasing requirements of efficient and smart image encryption mechanisms. The study aims to address the drawbacks of static S-box models commonly used in traditional cryptographic systems, which are susceptible to linear and differential attacks. In the proposed scheme, the dynamic, non-linear, invertible, and highly cryptographic strength S-boxes are generated through a hybrid chaotic system that may have high non-linearity, strong and rigorous avalanche characteristics, and low differential uniformity. The process here is that the LSM is used to produce S-boxes having key-dependent parameters that are stuffed into the CAST-128 structure to encrypt the image in a block-wise manner. The performance of the encryption is assessed utilizing a set of standard grayscale images. The metrics that are used to evaluate the security are entropy, NPCR, UACI, PSNR, and histogram analysis. Outcomes indicate that randomness, resistance to statistical attacks, and country of encryption are significantly improved compared to the original CAST-128. The study is theoretically and practically relevant since it presents a lightweight S-box generation approach driven by chaos, which can increase the level of robustness of the image encryptions without enlisting machine learning. The system may be applied to secure communications, surveillance systems, and medical image protection on a real-time basis.         ",
    "url": "https://arxiv.org/abs/2509.07606",
    "authors": [
      "Fadhil Abbas Fadhil",
      "Maryam Mahdi Alhusseini",
      "Mohammad-Reza Feizi-Derakhshi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.07623",
    "title": "Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis",
    "abstract": "           Deep learning has shown significant potential in diagnosing neurodegenerative diseases from MRI data. However, most existing methods rely heavily on large volumes of labeled data and often yield representations that lack interpretability. To address both challenges, we propose a novel self-supervised cross-encoder framework that leverages the temporal continuity in longitudinal MRI scans for supervision. This framework disentangles learned representations into two components: a static representation, constrained by contrastive learning, which captures stable anatomical features; and a dynamic representation, guided by input-gradient regularization, which reflects temporal changes and can be effectively fine-tuned for downstream classification tasks. Experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our method achieves superior classification accuracy and improved interpretability. Furthermore, the learned representations exhibit strong zero-shot generalization on the Open Access Series of Imaging Studies (OASIS) dataset and cross-task generalization on the Parkinson Progression Marker Initiative (PPMI) dataset. The code for the proposed method will be made publicly available.         ",
    "url": "https://arxiv.org/abs/2509.07623",
    "authors": [
      "Fangqi Cheng",
      "Yingying Zhao",
      "Xiaochen Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07635",
    "title": "Neural Proxies for Sound Synthesizers: Learning Perceptually Informed Preset Representations",
    "abstract": "           Deep learning appears as an appealing solution for Automatic Synthesizer Programming (ASP), which aims to assist musicians and sound designers in programming sound synthesizers. However, integrating software synthesizers into training pipelines is challenging due to their potential non-differentiability. This work tackles this challenge by introducing a method to approximate arbitrary synthesizers. Specifically, we train a neural network to map synthesizer presets onto an audio embedding space derived from a pretrained model. This facilitates the definition of a neural proxy that produces compact yet effective representations, thereby enabling the integration of audio embedding loss into neural-based ASP systems for black-box synthesizers. We evaluate the representations derived by various pretrained audio models in the context of neural-based nASP and assess the effectiveness of several neural network architectures, including feedforward, recurrent, and transformer-based models, in defining neural proxies. We evaluate the proposed method using both synthetic and hand-crafted presets from three popular software synthesizers and assess its performance in a synthesizer sound matching downstream task. While the benefits of the learned representation are nuanced by resource requirements, encouraging results were obtained for all synthesizers, paving the way for future research into the application of synthesizer proxies for neural-based ASP systems.         ",
    "url": "https://arxiv.org/abs/2509.07635",
    "authors": [
      "Paolo Combes",
      "Stefan Weinzierl",
      "Klaus Obermayer"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2509.07639",
    "title": "Linear time encodable binary code achieving GV bound with linear time encodable dual achieving GV bound",
    "abstract": "           We initiate the study of what we term ``fast good codes'' with ``fast good duals.'' Specifically, we consider the task of constructing a binary linear code $C \\leq \\mathcal{F}_2^n$ such that both it and its dual $C^\\perp :=\\{x \\in \\mathcal{F}_2^n:\\forall c \\in C, \\langle x,c\\rangle=0\\}$ are asymptotically good (in fact, have rate-distance tradeoff approaching the GV bound), and are encodable in $O(n)$ time. While we believe such codes should find applications more broadly, as motivation we describe how such codes can be used the secure computation task of encrypted matrix-vector product, as studied by Behhamouda et al (CCS 2025, to appear). Our main contribution is a construction of such a fast good code with fast good dual. Our construction is inspired by the repeat multiple accumulate (RMA) codes of Divsalar, Jin and McEliece (Allerton, 1998). To create the rate 1/2 code, after repeating each message coordinate, we perform accumulation steps -- where first a uniform coordinate permutation is applied, and afterwards the prefix-sum mod 2 is applied -- which are alternated with discrete derivative steps -- where again a uniform coordinate permutation is applied, and afterwards the previous two coordinates are summed mod 2. Importantly, these two operations are inverse of each other. In particular, the dual of the code is very similar, with the accumulation and discrete derivative steps reversed. Our analysis is inspired by a prior analysis of RMA codes due to Ravazzi and Fagnani (IEEE Trans. Info. Theory, 2009). The main idea is to bound the input-output weight-enumerator function: the expected number of messages of a given weight that are encoded into a codeword of a given weight. We face new challenges in controlling the behaviour of the discrete derivative matrix (which can significantly drop the weight of a vector), which we overcome by careful case analysis.         ",
    "url": "https://arxiv.org/abs/2509.07639",
    "authors": [
      "Martijn Brehm",
      "Nicolas Resch"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2509.07646",
    "title": "Decoding RobKiNet: Insights into Efficient Training of Robotic Kinematics Informed Neural Network",
    "abstract": "           In robots task and motion planning (TAMP), it is crucial to sample within the robot's configuration space to meet task-level global constraints and enhance the efficiency of subsequent motion planning. Due to the complexity of joint configuration sampling under multi-level constraints, traditional methods often lack efficiency. This paper introduces the principle of RobKiNet, a kinematics-informed neural network, for end-to-end sampling within the Continuous Feasible Set (CFS) under multiple constraints in configuration space, establishing its Optimization Expectation Model. Comparisons with traditional sampling and learning-based approaches reveal that RobKiNet's kinematic knowledge infusion enhances training efficiency by ensuring stable and accurate gradient this http URL and quantitative analyses in a 2-DOF space validate its theoretical efficiency, while its application on a 9-DOF autonomous mobile manipulator robot(AMMR) demonstrates superior whole-body and decoupled control, excelling in battery disassembly tasks. RobKiNet outperforms deep reinforcement learning with a training speed 74.29 times faster and a sampling accuracy of up to 99.25%, achieving a 97.33% task completion rate in real-world scenarios.         ",
    "url": "https://arxiv.org/abs/2509.07646",
    "authors": [
      "Yanlong Peng",
      "Zhigang Wang",
      "Ziwen He",
      "Pengxu Chang",
      "Chuangchuang Zhou",
      "Yu Yan",
      "Ming Chen"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.07647",
    "title": "Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity",
    "abstract": "           Semantic watermarking techniques for latent diffusion models (LDMs) are robust against regeneration attacks, but often suffer from detection performance degradation due to the loss of frequency integrity. To tackle this problem, we propose a novel embedding method called Hermitian Symmetric Fourier Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian symmetry. Additionally, we introduce a center-aware embedding strategy that reduces the vulnerability of semantic watermarking due to cropping attacks by ensuring robust information retention. To validate our approach, we apply these techniques to existing semantic watermarking schemes, enhancing their frequency-domain structures for better robustness and retrieval accuracy. Extensive experiments demonstrate that our methods achieve state-of-the-art verification and identification performance, surpassing previous approaches across various attack scenarios. Ablation studies confirm the impact of SFW on detection capabilities, the effectiveness of the center-aware embedding against cropping, and how message capacity influences identification accuracy. Notably, our method achieves the highest detection accuracy while maintaining superior image fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed SFW is shown to be an effective framework for balancing robustness and image fidelity, addressing the inherent trade-offs in semantic watermarking. Code available at this https URL ",
    "url": "https://arxiv.org/abs/2509.07647",
    "authors": [
      "Sung Ju Lee",
      "Nam Ik Cho"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07648",
    "title": "Graph-based Integrated Gradients for Explaining Graph Neural Networks",
    "abstract": "           Integrated Gradients (IG) is a common explainability technique to address the black-box problem of neural networks. Integrated gradients assumes continuous data. Graphs are discrete structures making IG ill-suited to graphs. In this work, we introduce graph-based integrated gradients (GB-IG); an extension of IG to graphs. We demonstrate on four synthetic datasets that GB-IG accurately identifies crucial structural components of the graph used in classification tasks. We further demonstrate on three prevalent real-world graph datasets that GB-IG outperforms IG in highlighting important features for node classification tasks.         ",
    "url": "https://arxiv.org/abs/2509.07648",
    "authors": [
      "Lachlan Simpson",
      "Kyle Millar",
      "Adriel Cheng",
      "Cheng-Chew Lim",
      "Hong Gunn Chew"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07654",
    "title": "Beyond Motion Cues and Structural Sparsity: Revisiting Small Moving Target Detection",
    "abstract": "           Small moving target detection is crucial for many defense applications but remains highly challenging due to low signal-to-noise ratios, ambiguous visual cues, and cluttered backgrounds. In this work, we propose a novel deep learning framework that differs fundamentally from existing approaches, which often rely on target-specific features or motion cues and tend to lack robustness in complex environments. Our key insight is that small target detection and background discrimination are inherently coupled, even cluttered video backgrounds often exhibit strong low-rank structures that can serve as stable priors for detection. We reformulate the task as a tensor-based low-rank and sparse decomposition problem and conduct a theoretical analysis of the background, target, and noise components to guide model design. Building on these insights, we introduce TenRPCANet, a deep neural network that requires minimal assumptions about target characteristics. Specifically, we propose a tokenization strategy that implicitly enforces multi-order tensor low-rank priors through a self-attention mechanism. This mechanism captures both local and non-local self-similarity to model the low-rank background without relying on explicit iterative optimization. In addition, inspired by the sparse component update in tensor RPCA, we design a feature refinement module to enhance target saliency. The proposed method achieves state-of-the-art performance on two highly distinct and challenging tasks: multi-frame infrared small target detection and space object detection. These results demonstrate both the effectiveness and the generalizability of our approach.         ",
    "url": "https://arxiv.org/abs/2509.07654",
    "authors": [
      "Guoyi Zhang",
      "Siyang Chen",
      "Guangsheng Xu",
      "Zhihua Shen",
      "Han Wang",
      "Xiaohu Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07673",
    "title": "Nearest Neighbor Projection Removal Adversarial Training",
    "abstract": "           Deep neural networks have exhibited impressive performance in image classification tasks but remain vulnerable to adversarial examples. Standard adversarial training enhances robustness but typically fails to explicitly address inter-class feature overlap, a significant contributor to adversarial susceptibility. In this work, we introduce a novel adversarial training framework that actively mitigates inter-class proximity by projecting out inter-class dependencies from adversarial and clean samples in the feature space. Specifically, our approach first identifies the nearest inter-class neighbors for each adversarial sample and subsequently removes projections onto these neighbors to enforce stronger feature separability. Theoretically, we demonstrate that our proposed logits correction reduces the Lipschitz constant of neural networks, thereby lowering the Rademacher complexity, which directly contributes to improved generalization and robustness. Extensive experiments across standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that our method demonstrates strong performance that is competitive with leading adversarial training techniques, highlighting significant achievements in both robust and clean accuracy. Our findings reveal the importance of addressing inter-class feature proximity explicitly to bolster adversarial robustness in DNNs.         ",
    "url": "https://arxiv.org/abs/2509.07673",
    "authors": [
      "Himanshu Singh",
      "A. V. Subramanyam",
      "Shivank Rajput",
      "Mohan Kankanhalli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07677",
    "title": "Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems",
    "abstract": "           Voice Authentication Systems (VAS) use unique vocal characteristics for verification. They are increasingly integrated into high-security sectors such as banking and healthcare. Despite their improvements using deep learning, they face severe vulnerabilities from sophisticated threats like deepfakes and adversarial attacks. The emergence of realistic voice cloning complicates detection, as systems struggle to distinguish authentic from synthetic audio. While anti-spoofing countermeasures (CMs) exist to mitigate these risks, many rely on static detection models that can be bypassed by novel adversarial methods, leaving a critical security gap. To demonstrate this vulnerability, we propose the Spectral Masking and Interpolation Attack (SMIA), a novel method that strategically manipulates inaudible frequency regions of AI-generated audio. By altering the voice in imperceptible zones to the human ear, SMIA creates adversarial samples that sound authentic while deceiving CMs. We conducted a comprehensive evaluation of our attack against state-of-the-art (SOTA) models across multiple tasks, under simulated real-world conditions. SMIA achieved a strong attack success rate (ASR) of at least 82% against combined VAS/CM systems, at least 97.5% against standalone speaker verification systems, and 100% against countermeasures. These findings conclusively demonstrate that current security postures are insufficient against adaptive adversarial attacks. This work highlights the urgent need for a paradigm shift toward next-generation defenses that employ dynamic, context-aware frameworks capable of evolving with the threat landscape.         ",
    "url": "https://arxiv.org/abs/2509.07677",
    "authors": [
      "Kamel Kamel",
      "Hridoy Sankar Dutta",
      "Keshav Sood",
      "Sunil Aryal"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07683",
    "title": "Robust Radar SLAM for Vehicle Parking Applications",
    "abstract": "           We address ego-motion estimation for automated parking, where centimeter-level accuracy is crucial due to tight spaces and nearby obstacles. Traditional methods using inertial-measurement units and wheel encoders require calibration, making them costly and time-consuming. To overcome this, we propose a radar-based simultaneous localization and mapping (SLAM) approach that leverages the robustness of radar to adverse weather and support for online calibration. Our robocentric formulation fuses feature positions and Doppler velocities for robust data association and filter convergence. Key contributions include a Doppler-augmented radar SLAM method, multi-radar support and an information-based feature-pruning strategy. Experiments demonstrate high-accuracy localization and improved robustness over state-of-the-art methods, meeting the demands of automated parking.         ",
    "url": "https://arxiv.org/abs/2509.07683",
    "authors": [
      "Luis Diener",
      "Jens Kalkkuhl",
      "Markus Enzweiler"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.07687",
    "title": "Physics-informed low-rank neural operators with application to parametric elliptic PDEs",
    "abstract": "           We present the Physics-Informed Low-Rank Neural Operator (PILNO), a neural operator framework for efficiently approximating solution operators of partial differential equations (PDEs) on point cloud data. PILNO combines low-rank kernel approximations with an encoder--decoder architecture, enabling fast, continuous one-shot predictions while remaining independent of specific discretizations. The model is trained using a physics-informed penalty framework, ensuring that PDE constraints and boundary conditions are satisfied in both supervised and unsupervised settings. We demonstrate its effectiveness on diverse problems, including function fitting, the Poisson equation, the screened Poisson equation with variable coefficients, and parameterized Darcy flow. The low-rank structure provides computational efficiency in high-dimensional parameter spaces, establishing PILNO as a scalable and flexible surrogate modeling tool for PDEs.         ",
    "url": "https://arxiv.org/abs/2509.07687",
    "authors": [
      "Sebastian Schaffer",
      "Lukas Exl"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2509.07703",
    "title": "Prescribed-Time Event-Triggered Control for Matrix-Scaled Networks",
    "abstract": "           This article proposes a distributed control method for matrix-scaled multi-agent networks aimed at achieving convergence within a user-defined time frame. The control law of each individual agent relies only on information from neighboring agents and is updated at discrete intervals determined by state-dependent triggering functions, reducing the frequency of agent interactions. To this end, first, the controller is augmented with a time-varying gain. Then, the dynamics of the closed-loop system over the finite-time interval is transformed into an infinite-time frame using time scaling. Lyapunov-based analysis is employed to derive suitable triggering conditions that guarantee the asymptotic convergence of the time-transformed system, thereby ensuring the prescribed-time convergence of the original system.         ",
    "url": "https://arxiv.org/abs/2509.07703",
    "authors": [
      "Sunny K P",
      "Rakesh R Warier"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.07725",
    "title": "IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing",
    "abstract": "           Multivariate time series forecasting (MTSF) often faces challenges from missing variables, which hinder conventional spatial-temporal graph neural networks in modeling inter-variable correlations. While GinAR addresses variable missing using attention-based imputation and adaptive graph learning for the first time, it lacks interpretability and fails to capture more latent temporal patterns due to its simple recursive units (RUs). To overcome these limitations, we propose the Interpretable Bidirectional-modeling Network (IBN), integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-based Graph Convolution (GGCN). IBN estimates the uncertainty of reconstructed values using MC Dropout and applies an uncertainty-weighted strategy to mitigate high-risk reconstructions. GGCN explicitly models spatial correlations among variables, while a bidirectional RU enhances temporal dependency modeling. Extensive experiments show that IBN achieves state-of-the-art forecasting performance under various missing-rate scenarios, providing a more reliable and interpretable framework for MTSF with missing variables. Code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2509.07725",
    "authors": [
      "Shusen Ma",
      "Tianhao Zhang",
      "Qijiu Xia",
      "Yun-Bo Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07732",
    "title": "Proximity Graphs for Similarity Search: Fast Construction, Lower Bounds, and Euclidean Separation",
    "abstract": "           Proximity graph-based methods have emerged as a leading paradigm for approximate nearest neighbor (ANN) search in the system community. This paper presents fresh insights into the theoretical foundation of these methods. We describe an algorithm to build a proximity graph for $(1+\\epsilon)$-ANN search that has $O((1/\\epsilon)^\\lambda \\cdot n \\log \\Delta)$ edges and guarantees $(1/\\epsilon)^\\lambda \\cdot \\text{polylog }\\Delta$ query time. Here, $n$ and $\\Delta$ are the size and aspect ratio of the data input, respectively, and $\\lambda = O(1)$ is the doubling dimension of the underlying metric space. Our construction time is near-linear to $n$, improving the $\\Omega(n^2)$ bounds of all previous constructions. We complement our algorithm with lower bounds revealing an inherent limitation of proximity graphs: the number of edges needs to be at least $\\Omega((1/\\epsilon)^\\lambda \\cdot n + n \\log \\Delta)$ in the worst case, up to a subpolynomial factor. The hard inputs used in our lower-bound arguments are non-geometric, thus prompting the question of whether improvement is possible in the Euclidean space (a key subclass of metric spaces). We provide an affirmative answer by using geometry to reduce the graph size to $O((1/\\epsilon)^\\lambda \\cdot n)$ while preserving nearly the same query and construction time.         ",
    "url": "https://arxiv.org/abs/2509.07732",
    "authors": [
      "Shangqi Lu",
      "Yufei Tao"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2509.07735",
    "title": "Embedding structures in continua: linear models and finite element discretizations",
    "abstract": "           This work describes models and numerical approximations that describe the mechanical behavior of deformable continua with embedded structural members, such as rigid bodies, beams, shells, etc. The continuum formulation extends an idea first presented in the context of the Arlequin method and constrains the kinematics of the two types of bodies to be compatible in the energy sense. In the article, we exploit the shared similarities of all structural theories to introduce a general framework for energetically coupling the latter with continua. In addition, we show that the problems, as well as their finite element approximations, are well-posed. Numerical examples of bodies with inclusions, fibers, and embedded surfaces are provided to illustrate the generality and robustness of the approach.         ",
    "url": "https://arxiv.org/abs/2509.07735",
    "authors": [
      "David Portillo",
      "Ignacio Romero"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Mathematical Physics (math-ph)"
    ]
  },
  {
    "id": "arXiv:2509.07748",
    "title": "Swarm-optimized Adaptive Augmentation of Missile Autopilot",
    "abstract": "           This paper considers the problem of optimizing a missile autopilot. In particular, the paper investigates the application of an online learning technique to learn and optimize the gains of a three-loop topology autopilot for a planar missile modeled with nonlinear dynamics and nonlinear aerodynamics forces and moments. The classical autopilot for a missile is based on a three-loop topology, where each loop consists of tunable proportional gains. An adaptive three-loop autopilot is constructed by augmenting the classical autopilot's fixed-gain controllers with a learning-based controller, which is recursively optimized using retrospective cost optimization. Numerical simulations show that online learning improves the tracking performance of the classical autopilot in both nominal and off-nominal interception scenarios.         ",
    "url": "https://arxiv.org/abs/2509.07748",
    "authors": [
      "Alexander Dorsey",
      "Parham Oveissi",
      "Jeffrey D. Barton",
      "Ankit Goel"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.07756",
    "title": "Spectral and Rhythm Feature Performance Evaluation for Category and Class Level Audio Classification with Deep Convolutional Neural Networks",
    "abstract": "           Next to decision tree and k-nearest neighbours algorithms deep convolutional neural networks (CNNs) are widely used to classify audio data in many domains like music, speech or environmental sounds. To train a specific CNN various spectral and rhythm features like mel-scaled spectrograms, mel-frequency cepstral coefficients (MFCC), cyclic tempograms, short-time Fourier transform (STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy normalized statistics (CENS) chromagrams can be used as digital image input data for the neural network. The performance of these spectral and rhythm features for audio category level as well as audio class level classification is investigated in detail with a deep CNN and the ESC-50 dataset with 2,000 labeled environmental audio recordings using an end-to-end deep learning pipeline. The evaluated metrics accuracy, precision, recall and F1 score for multiclass classification clearly show that the mel-scaled spectrograms and the mel-frequency cepstral coefficients (MFCC) perform significantly better then the other spectral and rhythm features investigated in this research for audio classification tasks using deep CNNs.         ",
    "url": "https://arxiv.org/abs/2509.07756",
    "authors": [
      "Friedrich Wolf-Monheim"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2509.07764",
    "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents",
    "abstract": "           Large Language Models (LLMs) have been increasingly integrated into computer-use agents, which can autonomously operate tools on a user's computer to accomplish complex tasks. However, due to the inherently unstable and unpredictable nature of LLM outputs, they may issue unintended tool commands or incorrect inputs, leading to potentially harmful operations. Unlike traditional security risks stemming from insecure user prompts, tool execution results from LLM-driven decisions introduce new and unique security challenges. These vulnerabilities span across all components of a computer-use agent. To mitigate these risks, we propose AgentSentinel, an end-to-end, real-time defense framework designed to mitigate potential security threats on a user's computer. AgentSentinel intercepts all sensitive operations within agent-related services and halts execution until a comprehensive security audit is completed. Our security auditing mechanism introduces a novel inspection process that correlates the current task context with system traces generated during task execution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a benchmark consisting of 60 diverse attack scenarios across six attack categories. The benchmark demonstrates a 87% average attack success rate on four state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an average defense success rate of 79.6%, significantly outperforming all baseline defenses.         ",
    "url": "https://arxiv.org/abs/2509.07764",
    "authors": [
      "Haitao Hu",
      "Peng Chen",
      "Yanpeng Zhao",
      "Yuqi Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.07772",
    "title": "XSRD-Net: EXplainable Stroke Relapse Detection",
    "abstract": "           Stroke is the second most frequent cause of death world wide with an annual mortality of around 5.5 million. Recurrence rates of stroke are between 5 and 25% in the first year. As mortality rates for relapses are extraordinarily high (40%) it is of utmost importance to reduce the recurrence rates. We address this issue by detecting patients at risk of stroke recurrence at an early stage in order to enable appropriate therapy planning. To this end we collected 3D intracranial CTA image data and recorded concomitant heart diseases, the age and the gender of stroke patients between 2010 and 2024. We trained single- and multimodal deep learning based neural networks for binary relapse detection (Task 1) and for relapse free survival (RFS) time prediction together with a subsequent classification (Task 2). The separation of relapse from non-relapse patients (Task 1) could be solved with tabular data (AUC on test dataset: 0.84). However, for the main task, the regression (Task 2), our multimodal XSRD-net processed the modalities vision:tabular with 0.68:0.32 according to modality contribution measures. The c-index with respect to relapses for the multimodal model reached 0.68, and the AUC is 0.71 for the test dataset. Final, deeper interpretability analysis results could highlight a link between both heart diseases (tabular) and carotid arteries (vision) for the detection of relapses and the prediction of the RFS time. This is a central outcome that we strive to strengthen with ongoing data collection and model retraining.         ",
    "url": "https://arxiv.org/abs/2509.07772",
    "authors": [
      "Christian Gapp",
      "Elias Tappeiner",
      "Martin Welk",
      "Karl Fritscher",
      "Stephanie Mangesius",
      "Constantin Eisenschink",
      "Philipp Deisl",
      "Michael Knoflach",
      "Astrid E. Grams",
      "Elke R. Gizewski",
      "Rainer Schubert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07773",
    "title": "Quantum Computing for Large-scale Network Optimization: Opportunities and Challenges",
    "abstract": "           The complexity of large-scale 6G-and-beyond networks demands innovative approaches for multi-objective optimization over vast search spaces, a task often intractable. Quantum computing (QC) emerges as a promising technology for efficient large-scale optimization. We present our vision of leveraging QC to tackle key classes of problems in future mobile networks. By analyzing and identifying common features, particularly their graph-centric representation, we propose a unified strategy involving QC algorithms. Specifically, we outline a methodology for optimization using quantum annealing as well as quantum reinforcement learning. Additionally, we discuss the main challenges that QC algorithms and hardware must overcome to effectively optimize future networks.         ",
    "url": "https://arxiv.org/abs/2509.07773",
    "authors": [
      "Sebastian Macaluso",
      "Giovanni Geraci",
      "El\u00edas F. Combarro",
      "Sergi Abadal",
      "Ioannis Arapakis",
      "Sofia Vallecorsa",
      "Eduard Alarc\u00f3n"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2509.07798",
    "title": "Faster, Self-Supervised Super-Resolution for Anisotropic Multi-View MRI Using a Sparse Coordinate Loss",
    "abstract": "           Acquiring images in high resolution is often a challenging task. Especially in the medical sector, image quality has to be balanced with acquisition time and patient comfort. To strike a compromise between scan time and quality for Magnetic Resonance (MR) imaging, two anisotropic scans with different low-resolution (LR) orientations can be acquired. Typically, LR scans are analyzed individually by radiologists, which is time consuming and can lead to inaccurate interpretation. To tackle this, we propose a novel approach for fusing two orthogonal anisotropic LR MR images to reconstruct anatomical details in a unified representation. Our multi-view neural network is trained in a self-supervised manner, without requiring corresponding high-resolution (HR) data. To optimize the model, we introduce a sparse coordinate-based loss, enabling the integration of LR images with arbitrary scaling. We evaluate our method on MR images from two independent cohorts. Our results demonstrate comparable or even improved super-resolution (SR) performance compared to state-of-the-art (SOTA) self-supervised SR methods for different upsampling scales. By combining a patient-agnostic offline and a patient-specific online phase, we achieve a substantial speed-up of up to ten times for patient-specific reconstruction while achieving similar or better SR quality. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.07798",
    "authors": [
      "Maja Schlereth",
      "Moritz Schillinger",
      "Katharina Breininger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07852",
    "title": "Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets",
    "abstract": "           Accurate and timely mapping of burned areas is crucial for environmental monitoring, disaster management, and assessment of climate change. This study presents a novel approach to automated burned area mapping using the AlphaEArth dataset combined with the Siamese U-Net deep learning architecture. The AlphaEArth Dataset, comprising high-resolution optical and thermal infrared imagery with comprehensive ground-truth annotations, provides an unprecedented resource for training robust burned area detection models. We trained our model with the Monitoring Trends in Burn Severity (MTBS) dataset in the contiguous US and evaluated it with 17 regions cross in Europe. Our experimental results demonstrate that the proposed ensemble approach achieves superior performance with an overall accuracy of 95%, IoU of 0.6, and F1-score of 74% on the test dataset. The model successfully identifies burned areas across diverse ecosystems with complex background, showing particular strength in detecting partially burned vegetation and fire boundaries and its transferability and high generalization in burned area mapping. This research contributes to the advancement of automated fire damage assessment and provides a scalable solution for global burn area monitoring using the AlphaEarth dataset.         ",
    "url": "https://arxiv.org/abs/2509.07852",
    "authors": [
      "Seyd Teymoor Seydi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07858",
    "title": "SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs",
    "abstract": "           Existing code large language models (LLMs) often rely on large-scale instruction data distilled from proprietary LLMs for fine-tuning, which typically incurs high costs. In this paper, we explore the potential of small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code instruction data construction. We first observe that the data synthesis capability of small-scale LLMs can be enhanced by training on a few superior data synthesis samples from proprietary LLMs. Building on this, we propose a novel iterative self-distillation approach to bootstrap small-scale LLMs, transforming them into powerful synthesizers that reduce reliance on proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain diverse and high-quality self-distilled data, we design multi-checkpoint sampling and multi-aspect scoring strategies for initial data selection. Furthermore, to identify the most influential samples, we introduce a gradient-based influence estimation method for final data filtering. Based on the code instruction datasets from the small-scale synthesizers, we develop SCoder, a family of code generation models fine-tuned from DeepSeek-Coder. SCoder models achieve state-of-the-art code generation capabilities, demonstrating the effectiveness of our method.         ",
    "url": "https://arxiv.org/abs/2509.07858",
    "authors": [
      "Xinyu Zhang",
      "Changzhi Zhou",
      "Linmei Hu",
      "Luhao Zhang",
      "Xiancai Chen",
      "Haomin Fu",
      "Yang Yang",
      "Mengdi Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07860",
    "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis",
    "abstract": "           Effectively managing intellectual property is a significant challenge. Traditional methods for patent analysis depend on labor-intensive manual searches and rigid keyword matching. These approaches are often inefficient and struggle to reveal the complex relationships hidden within large patent datasets, hindering strategic decision-making. To overcome these limitations, we introduce KLIPA, a novel framework that leverages a knowledge graph and a large language model (LLM) to significantly advance patent analysis. Our approach integrates three key components: a structured knowledge graph to map explicit relationships between patents, a retrieval-augmented generation(RAG) system to uncover contextual connections, and an intelligent agent that dynamically determines the optimal strategy for resolving user queries. We validated KLIPA on a comprehensive, real-world patent database, where it demonstrated substantial improvements in knowledge extraction, discovery of novel connections, and overall operational efficiency. This combination of technologies enhances retrieval accuracy, reduces reliance on domain experts, and provides a scalable, automated solution for any organization managing intellectual property, including technology corporations and legal firms, allowing them to better navigate the complexities of strategic innovation and competitive intelligence.         ",
    "url": "https://arxiv.org/abs/2509.07860",
    "authors": [
      "Guanzhi Deng",
      "Yi Xie",
      "Yu-Keung Ng",
      "Mingyang Liu",
      "Peijun Zheng",
      "Jie Liu",
      "Dapeng Wu",
      "Yinqiao Li",
      "Linqi Song"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2509.07872",
    "title": "Leveraging Support Vector Regression for Outcome Prediction in Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy",
    "abstract": "           Personalized ultra-fractionated stereotactic adaptive radiotherapy (PULSAR) is a novel treatment that delivers radiation in pulses of protracted intervals. Accurate prediction of gross tumor volume (GTV) changes through regression models has substantial prognostic value. This study aims to develop a multi-omics based support vector regression (SVR) model for predicting GTV change. A retrospective cohort of 39 patients with 69 brain metastases was analyzed, based on radiomics (MRI images) and dosiomics (dose maps) features. Delta features were computed to capture relative changes between two time points. A feature selection pipeline using least absolute shrinkage and selection operator (Lasso) algorithm with weight- or frequency-based ranking criterion was implemented. SVR models with various kernels were evaluated using the coefficient of determination (R2) and relative root mean square error (RRMSE). Five-fold cross-validation with 10 repeats was employed to mitigate the limitation of small data size. Multi-omics models that integrate radiomics, dosiomics, and their delta counterparts outperform individual-omics models. Delta-radiomic features play a critical role in enhancing prediction accuracy relative to features at single time points. The top-performing model achieves an R2 of 0.743 and an RRMSE of 0.022. The proposed multi-omics SVR model shows promising performance in predicting continuous change of GTV. It provides a more quantitative and personalized approach to assist patient selection and treatment adjustment in PULSAR.         ",
    "url": "https://arxiv.org/abs/2509.07872",
    "authors": [
      "Yajun Yu",
      "Steve Jiang",
      "Robert Timmerman",
      "Hao Peng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07887",
    "title": "A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges",
    "abstract": "           Graph Neural Networks (GNNs) have gained traction in the complex domain of drug discovery because of their ability to process graph-structured data such as drug molecule models. This approach has resulted in a myriad of methods and models in published literature across several categories of drug discovery research. This paper covers the research categories comprehensively with recent papers, namely molecular property prediction, including drug-target binding affinity prediction, drug-drug interaction study, microbiome interaction prediction, drug repositioning, retrosynthesis, and new drug design, and provides guidance for future work on GNNs for drug discovery.         ",
    "url": "https://arxiv.org/abs/2509.07887",
    "authors": [
      "Katherine Berry",
      "Liang Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07889",
    "title": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing",
    "abstract": "           This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which focuses on sentence-level gender bias detection and mitigation in Chinese. The task aims to promote fairness and controllability in natural language generation by automatically detecting, classifying, and mitigating gender bias. To address this challenge, we adopt a fine-tuning approach based on large language models (LLMs), efficiently adapt to the bias detection task via Low-Rank Adaptation (LoRA). In terms of data processing, we construct a more balanced training set to alleviate class imbalance and introduce heterogeneous samples from multiple sources to enhance model generalization. For the detection and classification sub-tasks, we employ a majority voting strategy that integrates outputs from multiple expert models to boost performance. Additionally, to improve bias generation detection and mitigation, we design a multi-temperature sampling mechanism to capture potential variations in bias expression styles. Experimental results demonstrate the effectiveness of our approach in bias detection, classification, and mitigation. Our method ultimately achieves an average score of 47.90%, ranking fourth in the shared task.         ",
    "url": "https://arxiv.org/abs/2509.07889",
    "authors": [
      "Chengyan Wu",
      "Yiqiang Cai",
      "Yufei Cheng",
      "Yun Xue"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.07905",
    "title": "Bio-KGvec2go: Serving up-to-date Dynamic Biomedical Knowledge Graph Embeddings",
    "abstract": "           Knowledge graphs and ontologies represent entities and their relationships in a structured way, having gained significance in the development of modern AI applications. Integrating these semantic resources with machine learning models often relies on knowledge graph embedding models to transform graph data into numerical representations. Therefore, pre-trained models for popular knowledge graphs and ontologies are increasingly valuable, as they spare the need to retrain models for different tasks using the same data, thereby helping to democratize AI development and enabling sustainable computing. In this paper, we present Bio-KGvec2go, an extension of the KGvec2go Web API, designed to generate and serve knowledge graph embeddings for widely used biomedical ontologies. Given the dynamic nature of these ontologies, Bio-KGvec2go also supports regular updates aligned with ontology version releases. By offering up-to-date embeddings with minimal computational effort required from users, Bio-KGvec2go facilitates efficient and timely biomedical research.         ",
    "url": "https://arxiv.org/abs/2509.07905",
    "authors": [
      "Hamid Ahmad",
      "Heiko Paulheim",
      "Rita T. Sousa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07907",
    "title": "Making congestion control robust to per-packet load balancing in datacenters",
    "abstract": "           Per-packet load-balancing approaches are increasingly deployed in datacenter networks. However, their combination with existing congestion control algorithms (CCAs) may lead to poor performance, and even state-of-the-art CCAs can collapse due to duplicate ACKs. A typical approach to handle this collapse is to make CCAs resilient to duplicate ACKs. In this paper, we first model the throughput collapse of a wide array of CCAs when some of the paths are congested. We show that addressing duplicate ACKs is insufficient. Instead, we explain that since CCAs are typically designed for single-path routing, their estimation function focuses on the latest feedback and mishandles feedback that reflects multiple paths. We propose to use a median feedback that is more robust to the varying signals that come with multiple paths. We introduce MSwift, which applies this principle to make Google's Swift robust to multi-path routing while keeping its incast tolerance and single-path performance. Finally, we demonstrate that MSwift improves the 99th-percentile FCT by up to 25\\%, both with random packet spraying and adaptive routing.         ",
    "url": "https://arxiv.org/abs/2509.07907",
    "authors": [
      "Barak Gerstein",
      "Mark Silberstein",
      "Isaac Keslassy"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2509.07911",
    "title": "Gut-Brain Axis as a Closed-Loop Molecular Communication Network",
    "abstract": "           Molecular communication (MC) provides a quantitative framework for analyzing information transfer within biological systems. This paper introduces a novel and comprehensive MC framework for the gut-brain axis (GBA) as a system of six coupled, nonlinear delay differential equations (DDEs). The proposed model defines a bidirectional feedback loop with a gut-to-brain inflammatory channel and a brain-to-gut neuroendocrine channel. Under prolonged stress, this feedback loop becomes self-perpetuating and drives the system into a pathological state. We evaluate the end-to-end channel across varying conditions using time-domain simulations, small-signal frequency-domain characterization, and an information-theoretic capacity analysis. At homeostasis, the system maintains stable circadian dynamics with higher information throughput, whereas sustained stress drives a shift to dysregulated hypercortisolism. In this pathological state, spectral efficiency decreases due to a narrowed effective bandwidth and a lower passband gain driven by neuroendocrine delays and saturating cytokine-hormone kinetics. These results quantify the impact of these signaling mechanisms on stability and information processing, elucidating the transition from healthy circadian rhythms to a persistent pathological state of hypercortisolism.         ",
    "url": "https://arxiv.org/abs/2509.07911",
    "authors": [
      "Beyza E. Ortlek",
      "Ozgur B. Akan"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2509.07918",
    "title": "Partitioning and Self-organization of Distributed Generation in Large Distribution Networks",
    "abstract": "           Distribution networks will experience more installations of distributed generation (DG) that is unpredictable and stochastic in nature. Greater distributed control and intelligence will allow challenges such as voltage control to be handled effectively. The partitioning of power networks into smaller clusters provides a method to split the control problem into manageable sub-problems. This paper presents a community detection-based partitioning technique for distribution networks considering local DGs, allowing them to be grouped and controlled in a distributed manner by using local signals and measurements. This method also allows each community to control the voltage using only neighboring DGs, and for each community to self-organize to reflect varying DG conditions and to maintain stable control. Simulations demonstrate that the partitioning of the large distribution network is effective, and each community is able to self-organize and to regulate the voltage independently using only its local DGs.         ",
    "url": "https://arxiv.org/abs/2509.07918",
    "authors": [
      "Badr Al Faiya",
      "Stephen McArthur",
      "Ivana Kockar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.07925",
    "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models",
    "abstract": "           Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.07925",
    "authors": [
      "Tuo Wang",
      "Adithya Kulkarni",
      "Tyler Cody",
      "Peter A. Beling",
      "Yujun Yan",
      "Dawei Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07939",
    "title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees",
    "abstract": "           Recent advances in Large Language Models (LLMs) have driven interest in automating cybersecurity penetration testing workflows, offering the promise of faster and more consistent vulnerability assessment for enterprise systems. Existing LLM agents for penetration testing primarily rely on self-guided reasoning, which can produce inaccurate or hallucinated procedural steps. As a result, the LLM agent may undertake unproductive actions, such as exploiting unused software libraries or generating cyclical responses that repeat prior tactics. In this work, we propose a guided reasoning pipeline for penetration testing LLM agents that incorporates a deterministic task tree built from the MITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the LLM's reaoning process to explicitly defined tactics, techniques, and procedures. This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures. To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Our proposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and 78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively. Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that incorporating a deterministic task tree into LLM reasoning pipelines can enhance the accuracy and efficiency of automated cybersecurity assessments         ",
    "url": "https://arxiv.org/abs/2509.07939",
    "authors": [
      "Katsuaki Nakano",
      "Reza Feyyazi",
      "Shanchieh Jay Yang",
      "Michael Zuzak"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07941",
    "title": "ImportSnare: Directed \"Code Manual\" Hijacking in Retrieval-Augmented Code Generation",
    "abstract": "           Code generation has emerged as a pivotal capability of Large Language Models(LLMs), revolutionizing development efficiency for programmers of all skill levels. However, the complexity of data structures and algorithmic logic often results in functional deficiencies and security vulnerabilities in generated code, reducing it to a prototype requiring extensive manual debugging. While Retrieval-Augmented Generation (RAG) can enhance correctness and security by leveraging external code manuals, it simultaneously introduces new attack surfaces. In this paper, we pioneer the exploration of attack surfaces in Retrieval-Augmented Code Generation (RACG), focusing on malicious dependency hijacking. We demonstrate how poisoned documentation containing hidden malicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting dual trust chains: LLM reliance on RAG and developers' blind trust in LLM suggestions. To construct poisoned documents, we propose ImportSnare, a novel attack framework employing two synergistic strategies: 1)Position-aware beam search optimizes hidden ranking sequences to elevate poisoned documents in retrieval results, and 2)Multilingual inductive suggestions generate jailbreaking sequences to manipulate LLMs into recommending malicious dependencies. Through extensive experiments across Python, Rust, and JavaScript, ImportSnare achieves significant attack success rates (over 50% for popular libraries such as matplotlib and seaborn) in general, and is also able to succeed even when the poisoning ratio is as low as 0.01%, targeting both custom and real-world malicious packages. Our findings reveal critical supply chain risks in LLM-powered development, highlighting inadequate security alignment for code generation tasks. To support future research, we will release the multilingual benchmark suite and datasets. The project homepage is this https URL.         ",
    "url": "https://arxiv.org/abs/2509.07941",
    "authors": [
      "Kai Ye",
      "Liangcai Su",
      "Chenxiong Qian"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.07942",
    "title": "Knowledge Isn't Power: The Ethics of Social Robots and the Difficulty of Informed Consent",
    "abstract": "           Contemporary robots are increasingly mimicking human social behaviours to facilitate interaction, such as smiling to signal approachability, or hesitating before taking an action to allow people time to react. Such techniques can activate a person's entrenched social instincts, triggering emotional responses as though they are interacting with a fellow human, and can prompt them to treat a robot as if it truly possesses the underlying life-like processes it outwardly presents, raising significant ethical questions. We engage these issues through the lens of informed consent: drawing upon prevailing legal principles and ethics, we examine how social robots can influence user behaviour in novel ways, and whether under those circumstances users can be appropriately informed to consent to these heightened interactions. We explore the complex circumstances of human-robot interaction and highlight how it differs from more familiar interaction contexts, and we apply legal principles relating to informed consent to social robots in order to reconceptualize the current ethical debates surrounding the field. From this investigation, we synthesize design goals for robot developers to achieve more ethical and informed human-robot interaction.         ",
    "url": "https://arxiv.org/abs/2509.07942",
    "authors": [
      "James M. Berzuk",
      "Lauren Corcoran",
      "Brannen McKenzie-Lefurgey",
      "Katie Szilagyi",
      "James E. Young"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.07979",
    "title": "Visual Representation Alignment for Multimodal Large Language Models",
    "abstract": "           Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.         ",
    "url": "https://arxiv.org/abs/2509.07979",
    "authors": [
      "Heeji Yoon",
      "Jaewoo Jung",
      "Junwan Kim",
      "Hyungyu Choi",
      "Heeseong Shin",
      "Sangbeom Lim",
      "Honggyu An",
      "Chaehyun Kim",
      "Jisang Han",
      "Donghyun Kim",
      "Chanho Eom",
      "Sunghwan Hong",
      "Seungryong Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.05894",
    "title": "Toric geometry of ReLU neural networks",
    "abstract": "           Given a continuous finitely piecewise linear function $f:\\mathbb{R}^{n_0} \\to \\mathbb{R}$ and a fixed architecture $(n_0,\\ldots,n_k;1)$ of feedforward ReLU neural networks, the exact function realization problem is to determine when some network with the given architecture realizes $f$. To develop a systematic way to answer these questions, we establish a connection between toric geometry and ReLU neural networks. This approach enables us to utilize numerous structures and tools from algebraic geometry to study ReLU neural networks. Starting with an unbiased ReLU neural network with rational weights, we define the ReLU fan, the ReLU toric variety, and the ReLU Cartier divisor associated with the network. This work also reveals the connection between the tropical geometry and the toric geometry of ReLU neural networks. As an application of the toric geometry framework, we prove a necessary and sufficient criterion of functions realizable by unbiased shallow ReLU neural networks by computing intersection numbers of the ReLU Cartier divisor and torus-invariant curves.         ",
    "url": "https://arxiv.org/abs/2509.05894",
    "authors": [
      "Yaoying Fu"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2509.06970",
    "title": "Impact of Neuron Models on Spiking Neural Networks performance. A Complexity Based Classification Approach",
    "abstract": "           This study explores how the selection of neuron models and learning rules impacts the classification performance of Spiking Neural Networks (SNNs), with a focus on applications in bio-signal processing. We compare biologically inspired neuron models, including Leaky Integrate-and-Fire (LIF), metaneurons, and probabilistic Levy-Baxter (LB) neurons, across multiple learning rules, including spike-timing-dependent plasticity (STDP), tempotron, and reward-modulated updates. A novel element of this work is the integration of a complexity-based decision mechanism into the evaluation pipeline. Using Lempel-Ziv Complexity (LZC), a measure related to entropy rate, we quantify the structural regularity of spike trains and assess classification outcomes in a consistent and interpretable manner across different SNN configurations. To investigate neural dynamics and assess algorithm performance, we employed synthetic datasets with varying temporal dependencies and stochasticity levels. These included Markov and Poisson processes, well-established models to simulate neuronal spike trains and capture the stochastic firing behavior of biological this http URL of synthetic Poisson and Markov-modeled data reveals clear performance trends: classification accuracy depends on the interaction between neuron model, network size, and learning rule, with the LZC-based evaluation highlighting configurations that remain robust to weak or noisy signals. This work delivers a systematic analysis of how neuron model selection interacts with network parameters and learning strategies, supported by a novel complexity-based evaluation approach that offers a consistent benchmark for SNN performance.         ",
    "url": "https://arxiv.org/abs/2509.06970",
    "authors": [
      "Zofia Rudnicka",
      "Janusz Szczepanski",
      "Agnieszka Pregowska"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.06991",
    "title": "Towards automatizing detection and quantification of intestinal metaplasia: a multi-expert comparative study",
    "abstract": "           Current gastric cancer risk systems are prone to errors since they evaluate a visual estimation of intestinal metaplasia percentages to assign a risk. This study presents an automated method to detect and quantify intestinal metaplasia using deep learning models as well as a comparative analysis with visual estimations of three experienced pathologists. Gastric samples were collected from two different cohorts: 149 asymptomatic volunteers from a region with a high prevalence of GCa in Colombia and 56 patients from a third-level hospital. Deep learning models were selected and trained to classify intestinal metaplasia, and predictions were used to estimate the percentage of intestinal metaplasia and assign the risk score. Results were compared with independent blinded assessments performed by three experienced pathologists. The best-performing deep learning architecture classified intestinal metaplasia with F1-Score of 0.80 +- 0.01 and AUC of 0.91 +- 0.01. Among pathologists, inter-observer agreement by a Fleiss's Kappa score ranged from 0.61 to 0.75. In comparison, agreement between the pathologists and the best-performing model ranged from 0.37 to 0.54. Deep learning models show potential to detect and quantify the percentage of intestinal metaplasia with greater precision and reproducibility than experienced pathologists. Likewise, estimated risk shows high inter-observer variability when visually assigning the intestinal metaplasia percentage.         ",
    "url": "https://arxiv.org/abs/2509.06991",
    "authors": [
      "Fabian Cano",
      "Mauricio Caviedes",
      "Andres Siabatto",
      "Jesus Villarreal",
      "Jose Quijano",
      "Alvaro Bedoya",
      "Satish E. Viswanath",
      "Angel Cruz-Roa",
      "Fabio Gonzalez",
      "Eduardo Romero"
    ],
    "subjectives": [
      "Tissues and Organs (q-bio.TO)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2509.07042",
    "title": "PUUMA (Placental patch and whole-Uterus dual-branch U-Mamba-based Architecture): Functional MRI Prediction of Gestational Age at Birth and Preterm Risk",
    "abstract": "           Preterm birth is a major cause of mortality and lifelong morbidity in childhood. Its complex and multifactorial origins limit the effectiveness of current clinical predictors and impede optimal care. In this study, a dual-branch deep learning architecture (PUUMA) was developed to predict gestational age (GA) at birth using T2* fetal MRI data from 295 pregnancies, encompassing a heterogeneous and imbalanced population. The model integrates both global whole-uterus and local placental features. Its performance was benchmarked against linear regression using cervical length measurements obtained by experienced clinicians from anatomical MRI and other Deep Learning architectures. The GA at birth predictions were assessed using mean absolute error. Accuracy, sensitivity, and specificity were used to assess preterm classification. Both the fully automated MRI-based pipeline and the cervical length regression achieved comparable mean absolute errors (3 weeks) and good sensitivity (0.67) for detecting preterm birth, despite pronounced class imbalance in the dataset. These results provide a proof of concept for automated prediction of GA at birth from functional MRI, and underscore the value of whole-uterus functional imaging in identifying at-risk pregnancies. Additionally, we demonstrate that manual, high-definition cervical length measurements derived from MRI, not currently routine in clinical practice, offer valuable predictive information. Future work will focus on expanding the cohort size and incorporating additional organ-specific imaging to improve generalisability and predictive performance.         ",
    "url": "https://arxiv.org/abs/2509.07042",
    "authors": [
      "Diego Fajardo-Rojas",
      "Levente Baljer",
      "Jordina Aviles Verdera",
      "Megan Hall",
      "Daniel Cromb",
      "Mary A. Rutherford",
      "Lisa Story",
      "Emma C. Robinson",
      "Jana Hutter"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07123",
    "title": "NestGNN: A Graph Neural Network Framework Generalizing the Nested Logit Model for Travel Mode Choice",
    "abstract": "           Nested logit (NL) has been commonly used for discrete choice analysis, including a wide range of applications such as travel mode choice, automobile ownership, or location decisions. However, the classical NL models are restricted by their limited representation capability and handcrafted utility specification. While researchers introduced deep neural networks (DNNs) to tackle such challenges, the existing DNNs cannot explicitly capture inter-alternative correlations in the discrete choice context. To address the challenges, this study proposes a novel concept - alternative graph - to represent the relationships among travel mode alternatives. Using a nested alternative graph, this study further designs a nested-utility graph neural network (NestGNN) as a generalization of the classical NL model in the neural network family. Theoretically, NestGNNs generalize the classical NL models and existing DNNs in terms of model representation, while retaining the crucial two-layer substitution patterns of the NL models: proportional substitution within a nest but non-proportional substitution beyond a nest. Empirically, we find that the NestGNNs significantly outperform the benchmark models, particularly the corresponding NL models by 9.2\\%. As shown by elasticity tables and substitution visualization, NestGNNs retain the two-layer substitution patterns as the NL model, and yet presents more flexibility in its model design space. Overall, our study demonstrates the power of NestGNN in prediction, interpretation, and its flexibility of generalizing the classical NL model for analyzing travel mode choice.         ",
    "url": "https://arxiv.org/abs/2509.07123",
    "authors": [
      "Yuqi Zhou",
      "Zhanhong Cheng",
      "Lingqian Hu",
      "Yuheng Bu",
      "Shenhao Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07155",
    "title": "Quantum algorithms for general nonlinear dynamics based on the Carleman embedding",
    "abstract": "           Important nonlinear dynamics, such as those found in plasma and fluid systems, are typically hard to simulate on classical computers. Thus, if fault-tolerant quantum computers could efficiently solve such nonlinear problems, it would be a transformative change for many industries. In a recent breakthrough [Liu et al., PNAS 2021], the first efficient quantum algorithm for solving nonlinear differential equations was constructed, based on a single condition $R<1$, where $R$ characterizes the ratio of nonlinearity to dissipation. This result, however, is limited to the class of purely dissipative systems with negative log-norm, which excludes application to many important problems. In this work, we correct technical issues with this and other prior analysis, and substantially extend the scope of nonlinear dynamical systems that can be efficiently simulated on a quantum computer in a number of ways. Firstly, we extend the existing results from purely dissipative systems to a much broader class of stable systems, and show that every quadratic Lyapunov function for the linearized system corresponds to an independent $R$-number criterion for the convergence of the Carlemen scheme. Secondly, we extend our stable system results to physically relevant settings where conserved polynomial quantities exist. Finally, we provide extensive results for the class of non-resonant systems. With this, we are able to show that efficient quantum algorithms exist for a much wider class of nonlinear systems than previously known, and prove the BQP-completeness of nonlinear oscillator problems of exponential size. In our analysis, we also obtain several results related to the Poincar\u00e9-Dulac theorem and diagonalization of the Carleman matrix, which could be of independent interest.         ",
    "url": "https://arxiv.org/abs/2509.07155",
    "authors": [
      "David Jennings",
      "Kamil Korzekwa",
      "Matteo Lostaglio",
      "Andrew T Sornborger",
      "Yigit Subasi",
      "Guoming Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2509.07289",
    "title": "Kernel VICReg for Self-Supervised Learning in Reproducing Kernel Hilbert Space",
    "abstract": "           Self-supervised learning (SSL) has emerged as a powerful paradigm for representation learning by optimizing geometric objectives--such as invariance to augmentations, variance preservation, and feature decorrelation--without requiring labels. However, most existing methods operate in Euclidean space, limiting their ability to capture nonlinear dependencies and geometric structures. In this work, we propose Kernel VICReg, a novel self-supervised learning framework that lifts the VICReg objective into a Reproducing Kernel Hilbert Space (RKHS). By kernelizing each term of the loss-variance, invariance, and covariance--we obtain a general formulation that operates on double-centered kernel matrices and Hilbert-Schmidt norms, enabling nonlinear feature learning without explicit mappings. We demonstrate that Kernel VICReg not only avoids representational collapse but also improves performance on tasks with complex or small-scale data. Empirical evaluations across MNIST, CIFAR-10, STL-10, TinyImageNet, and ImageNet100 show consistent gains over Euclidean VICReg, with particularly strong improvements on datasets where nonlinear structures are prominent. UMAP visualizations further confirm that kernel-based embeddings exhibit better isometry and class separation. Our results suggest that kernelizing SSL objectives is a promising direction for bridging classical kernel methods with modern representation learning.         ",
    "url": "https://arxiv.org/abs/2509.07289",
    "authors": [
      "M.Hadi Sepanj",
      "Benyamin Ghojogh",
      "Paul Fieguth"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07300",
    "title": "Identifying Neural Signatures from fMRI using Hybrid Principal Components Regression",
    "abstract": "           Recent advances in neuroimaging analysis have enabled accurate decoding of mental state from brain activation patterns during functional magnetic resonance imaging scans. A commonly applied tool for this purpose is principal components regression regularized with the least absolute shrinkage and selection operator (LASSO PCR), a type of multi-voxel pattern analysis (MVPA). This model presumes that all components are equally likely to harbor relevant information, when in fact the task-related signal may be concentrated in specific components. In such cases, the model will fail to select the optimal set of principal components that maximizes the total signal relevant to the cognitive process under study. Here, we present modifications to LASSO PCR that allow for a regularization penalty tied directly to the index of the principal component, reflecting a prior belief that task-relevant signal is more likely to be concentrated in components explaining greater variance. Additionally, we propose a novel hybrid method, Joint Sparsity-Ranked LASSO (JSRL), which integrates component-level and voxel-level activity under an information parity framework and imposes ranked sparsity to guide component selection. We apply the models to brain activation during risk taking, monetary incentive, and emotion regulation tasks. Results demonstrate that incorporating sparsity ranking into LASSO PCR produces models with enhanced classification performance, with JSRL achieving up to 51.7\\% improvement in cross-validated deviance $R^2$ and 7.3\\% improvement in cross-validated AUC. Furthermore, sparsity-ranked models perform as well as or better than standard LASSO PCR approaches across all classification tasks and allocate predictive weight to brain regions consistent with their established functional roles, offering a robust alternative for MVPA.         ",
    "url": "https://arxiv.org/abs/2509.07300",
    "authors": [
      "Jared Rieck",
      "Julia Wrobel",
      "Joshua L. Gowin",
      "Yue Wang",
      "Martin Paulus",
      "Ryan Peterson"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07396",
    "title": "Toward Lifelong-Sustainable Electronic-Photonic AI Systems via Extreme Efficiency, Reconfigurability, and Robustness",
    "abstract": "           The relentless growth of large-scale artificial intelligence (AI) has created unprecedented demand for computational power, straining the energy, bandwidth, and scaling limits of conventional electronic platforms. Electronic-photonic integrated circuits (EPICs) have emerged as a compelling platform for next-generation AI systems, offering inherent advantages in ultra-high bandwidth, low latency, and energy efficiency for computing and interconnection. Beyond performance, EPICs also hold unique promises for sustainability. Fabricated in relaxed process nodes with fewer metal layers and lower defect densities, photonic devices naturally reduce embodied carbon footprint (CFP) compared to advanced digital electronic integrated circuits, while delivering orders-of-magnitude higher computing performance and interconnect bandwidth. To further advance the sustainability of photonic AI systems, we explore how electronic-photonic design automation (EPDA) and cross-layer co-design methodologies can amplify these inherent benefits. We present how advanced EPDA tools enable more compact layout generation, reducing both chip area and metal layer usage. We will also demonstrate how cross-layer device-circuit-architecture co-design unlocks new sustainability gains for photonic hardware: ultra-compact photonic circuit designs that minimize chip area cost, reconfigurable hardware topology that adapts to evolving AI workloads, and intelligent resilience mechanisms that prolong lifetime by tolerating variations and faults. By uniting intrinsic photonic efficiency with EPDA- and co-design-driven gains in area efficiency, reconfigurability, and robustness, we outline a vision for lifelong-sustainable electronic-photonic AI systems. This perspective highlights how EPIC AI systems can simultaneously meet the performance demands of modern AI and the urgent imperative for sustainable computing.         ",
    "url": "https://arxiv.org/abs/2509.07396",
    "authors": [
      "Ziang Yin",
      "Hongjian Zhou",
      "Chetan Choppali Sudarshan",
      "Vidya Chhabria",
      "Jiaqi Gu"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2509.07451",
    "title": "Synthetic Data Generation with Lorenzetti for Time Series Anomaly Detection in High-Energy Physics Calorimeters",
    "abstract": "           Anomaly detection in multivariate time series is crucial to ensure the quality of data coming from a physics experiment. Accurately identifying the moments when unexpected errors or defects occur is essential, yet challenging due to scarce labels, unknown anomaly types, and complex correlations across dimensions. To address the scarcity and unreliability of labelled data, we use the Lorenzetti Simulator to generate synthetic events with injected calorimeter anomalies. We then assess the sensitivity of several time series anomaly detection methods, including transformer-based and other deep learning models. The approach employed here is generic and applicable to different detector designs and defects.         ",
    "url": "https://arxiv.org/abs/2509.07451",
    "authors": [
      "Laura Boggia",
      "Bogdan Malaescu"
    ],
    "subjectives": [
      "High Energy Physics - Experiment (hep-ex)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07688",
    "title": "Understanding Ice Crystal Habit Diversity with Self-Supervised Learning",
    "abstract": "           Ice-containing clouds strongly impact climate, but they are hard to model due to ice crystal habit (i.e., shape) diversity. We use self-supervised learning (SSL) to learn latent representations of crystals from ice crystal imagery. By pre-training a vision transformer with many cloud particle images, we learn robust representations of crystal morphology, which can be used for various science-driven tasks. Our key contributions include (1) validating that our SSL approach can be used to learn meaningful representations, and (2) presenting a relevant application where we quantify ice crystal diversity with these latent representations. Our results demonstrate the power of SSL-driven representations to improve the characterization of ice crystals and subsequently constrain their role in Earth's climate system.         ",
    "url": "https://arxiv.org/abs/2509.07688",
    "authors": [
      "Joseph Ko",
      "Hariprasath Govindarajan",
      "Fredrik Lindsten",
      "Vanessa Przybylo",
      "Kara Sulia",
      "Marcus van Lier-Walqui",
      "Kara Lamb"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.07766",
    "title": "Toward Quantum Utility in Finance: A Robust Data-Driven Algorithm for Asset Clustering",
    "abstract": "           Clustering financial assets based on return correlations is a fundamental task in portfolio optimization and statistical arbitrage. However, classical clustering methods often fall short when dealing with signed correlation structures, typically requiring lossy transformations and heuristic assumptions such as a fixed number of clusters. In this work, we apply the Graph-based Coalition Structure Generation algorithm (GCS-Q) to directly cluster signed, weighted graphs without relying on such transformations. GCS-Q formulates each partitioning step as a QUBO problem, enabling it to leverage quantum annealing for efficient exploration of exponentially large solution spaces. We validate our approach on both synthetic and real-world financial data, benchmarking against state-of-the-art classical algorithms such as SPONGE and k-Medoids. Our experiments demonstrate that GCS-Q consistently achieves higher clustering quality, as measured by Adjusted Rand Index and structural balance penalties, while dynamically determining the number of clusters. These results highlight the practical utility of near-term quantum computing for graph-based unsupervised learning in financial applications.         ",
    "url": "https://arxiv.org/abs/2509.07766",
    "authors": [
      "Shivam Sharma",
      "Supreeth Mysore Venkatesh",
      "Pushkin Kachroo"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.07924",
    "title": "A Non-Monotonic Relationship: An Empirical Analysis of Hybrid Quantum Classifiers for Unseen Ransomware Detection",
    "abstract": "           Detecting unseen ransomware is a critical cybersecurity challenge where classical machine learning often fails. While Quantum Machine Learning (QML) presents a potential alternative, its application is hindered by the dimensionality gap between classical data and quantum hardware. This paper empirically investigates a hybrid framework using a Variational Quantum Classifier (VQC) interfaced with a high-dimensional dataset via Principal Component Analysis (PCA). Our analysis reveals a dual challenge for practical QML. A significant information bottleneck was evident, as even the best performing 12-qubit VQC fell short of the classical baselines 97.7\\% recall. Furthermore, a non-monotonic performance trend, where performance degraded when scaling from 4 to 8 qubits before improving at 12 qubits suggests a severe trainability issue. These findings highlight that unlocking QMLs potential requires co-developing more efficient data compression techniques and robust quantum optimization strategies.         ",
    "url": "https://arxiv.org/abs/2509.07924",
    "authors": [
      "Huu Phu Le",
      "Phuc Hao Do",
      "Vo Hoang Long Nguyen",
      "Nang Hung Van Nguyen"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2308.11333",
    "title": "FilterFL: Knowledge Filtering-based Data-Free Backdoor Defense for Federated Learning",
    "abstract": "           As a distributed machine learning paradigm, Federated Learning (FL) enables large-scale clients to collaboratively train a model without sharing their raw data. However, due to the lack of data auditing for untrusted clients, FL is vulnerable to poisoning attacks, especially backdoor attacks. By using poisoned data for local training or directly changing the model parameters, attackers can easily inject backdoors into the model, which can trigger the model to make misclassification of targeted patterns in images. To address these issues, we propose a novel data-free trigger-generation-based defense approach based on the two characteristics of backdoor attacks: i) triggers are learned faster than normal knowledge, and ii) trigger patterns have a greater effect on image classification than normal class patterns. Our approach generates the images with newly learned knowledge by identifying the differences between the old and new global models, and filters trigger images by evaluating the effect of these generated images. By using these trigger images, our approach eliminates poisoned models to ensure the updated global model is benign. Comprehensive experiments demonstrate that our approach can defend against almost all the existing types of backdoor attacks and outperform all the seven state-of-the-art defense methods with both IID and non-IID scenarios. Especially, our approach can successfully defend against the backdoor attack even when 80\\% of the clients are malicious.         ",
    "url": "https://arxiv.org/abs/2308.11333",
    "authors": [
      "Yanxin Yang",
      "Ming Hu",
      "Xiaofei Xie",
      "Yue Cao",
      "Pengyu Zhang",
      "Yihao Huang",
      "Mingsong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2309.13305",
    "title": "Multilevel User Credibility Assessment in Social Networks",
    "abstract": "           Online social networks serve as major platforms for disseminating both real and fake news. Many users--intentionally or unintentionally--spread harmful content, misinformation, and rumors in domains such as politics and business. Consequently, user credibility assessment has become a prominent area of research in recent years. Most existing methods suffer from two key limitations. First, they treat credibility as a binary task, labeling users as either genuine or fake, whereas real-world applications often demand a more nuanced, multilevel evaluation. Second, they rely on only a subset of relevant features, which constrains their predictive performance. In this paper, we address the lack of a dataset suitable for multilevel credibility assessment by first devising a collection method tailored to this task. We then propose the \\textit{MultiCred} model, which assigns users to one of several credibility tiers based on a rich and diverse set of features extracted from their profiles, tweets, and comments. MultiCred leverages deep language models for textual analysis and deep neural networks for non-textual data processing. Our extensive experiments demonstrate that MultiCred significantly outperforms existing approaches across multiple accuracy metrics. Our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2309.13305",
    "authors": [
      "Mohammad Moradi",
      "Mostafa Haghir Chehreghani"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2310.01968",
    "title": "PyHexTop: a compact Python code for topology optimization using hexagonal elements",
    "abstract": "           Python serves as an open-source and cost-effective alternative to the MATLAB programming language. This paper introduces a concise topology optimization Python code, named ``\\texttt{PyHexTop},\" primarily intended for educational purposes. Code employs hexagonal elements to parameterize design domains as such elements provide checkerboard-free optimized design naturally. \\texttt{PyHexTop} is developed based on the ``\\texttt{HoneyTop90}\" MATLAB code~\\cite{kumar2023honeytop90} and uses the \\texttt{NumPy} and \\texttt{SciPy} libraries. Code is straightforward and easily comprehensible, proving a helpful tool that can help people new in the topology optimization field to learn and explore. \\texttt{PyHexTop} is specifically tailored to address compliance minimization with specified volume constraints. The paper provides a detailed explanation of the code for solving the Messerschmitt-Bolkow-Blohm beam and extensions to solve problems different problems. The code is publicly shared at: this https URL ",
    "url": "https://arxiv.org/abs/2310.01968",
    "authors": [
      "Aditi Agarwal",
      "Anupam Saxena",
      "Prabhat Kumar"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2310.02069",
    "title": "TOaCNN: Adaptive Convolutional Neural Network for Multidisciplinary Topology Optimization",
    "abstract": "           This paper presents an adaptive convolutional neural network (CNN) architecture that can automate diverse topology optimization (TO) problems having different underlying physics. The architecture uses the encoder-decoder networks with dense layers in the middle which includes an additional adaptive layer to capture complex geometrical features. The network is trained using the dataset obtained from the three open-source TO codes involving different physics. The robustness and success of the presented adaptive CNN are demonstrated on compliance minimization problems with constant and design-dependent loads and material bulk modulus optimization. The architecture takes the user's input of the volume fraction. It instantly generates optimized designs resembling their counterparts obtained via open-source TO codes with negligible performance and volume fraction error.         ",
    "url": "https://arxiv.org/abs/2310.02069",
    "authors": [
      "Khaish Singh Chadha",
      "Prabhat Kumar"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2310.02407",
    "title": "Challenging Bug Prediction and Repair Models with Synthetic Bugs",
    "abstract": "           Bugs are essential in software engineering; many research studies in the past decades have been proposed to detect, localize, and repair bugs in software systems. Effectiveness evaluation of such techniques requires complex bugs, i.e., those that are hard to detect through testing and hard to repair through debugging. From the classic software engineering point of view, a hard-to-repair bug differs from the correct code in multiple locations, making it hard to localize and repair. Hard-to-detect bugs, on the other hand, manifest themselves under specific test inputs and reachability conditions. These two objectives, i.e., generating hard-to-detect and hard-to-repair bugs, are mostly aligned; a bug generation technique can change multiple statements to be covered only under a specific set of inputs. However, these two objectives are conflicting for learning-based techniques: A bug should have a similar code representation to the correct code in the training data to challenge a bug prediction model to distinguish them. The hard-to-repair bug definition remains the same but with a caveat: the more a bug differs from the original code, the more distant their representations are and easier to be detected. We propose BugFarm, to transform arbitrary code into multiple complex bugs. BugFarm leverages LLMs to mutate code in multiple locations (hard-to-repair). To ensure that multiple modifications do not notably change the code representation, BugFarm analyzes the attention of the underlying model and instructs LLMs to only change the least attended locations (hard-to-detect). Our comprehensive evaluation of 435k+ bugs from over 1.9M mutants generated by BUGFARM and two alternative approaches demonstrates our superiority in generating bugs that are hard to detect by learning-based bug prediction approaches and hard-to-repair by state-of-the-art learning-based program repair technique.         ",
    "url": "https://arxiv.org/abs/2310.02407",
    "authors": [
      "Ali Reza Ibrahimzada",
      "Yang Chen",
      "Ryan Rong",
      "Reyhaneh Jabbarvand"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.02346",
    "title": "Closed-Loop Unsupervised Representation Disentanglement with $\u03b2$-VAE Distillation and Diffusion Probabilistic Feedback",
    "abstract": "           Representation disentanglement may help AI fundamentally understand the real world and thus benefit both discrimination and generation tasks. It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data -- causing poor generalization on natural scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evaluation metric, especially for the real label-free data. To address these challenges, we propose a \\textbf{C}losed-\\textbf{L}oop unsupervised representation \\textbf{Dis}entanglement approach dubbed \\textbf{CL-Dis}. Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while resorting to $\\beta$-VAE as a co-pilot to extract semantically disentangled representations. The strong generation ability of diffusion model and the good disentanglement ability of VAE model are complementary. To strengthen disentangling, VAE-latent distillation and diffusion-wise feedback are interconnected in a closed-loop system for a further mutual promotion. Then, a self-supervised \\textbf{Navigation} strategy is introduced to identify interpretable semantic directions in the disentangled latent space. Finally, a new metric based on content tracking is designed to evaluate the disentanglement effect. Experiments demonstrate the superiority of CL-Dis on applications like real image manipulation and visual analysis.         ",
    "url": "https://arxiv.org/abs/2402.02346",
    "authors": [
      "Xin Jin",
      "Bohan Li",
      "BAAO Xie",
      "Wenyao Zhang",
      "Jinming Liu",
      "Ziqiang Li",
      "Tao Yang",
      "Wenjun Zeng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.05666",
    "title": "Prepared for the Worst: A Learning-Based Adversarial Attack for Resilience Analysis of the ICP Algorithm",
    "abstract": "           This paper presents a novel method for assessing the resilience of the ICP algorithm via learning-based, worst-case attacks on lidar point clouds. For safety-critical applications such as autonomous navigation, ensuring the resilience of algorithms before deployments is crucial. The ICP algorithm is the standard for lidar-based localization, but its accuracy can be greatly affected by corrupted measurements from various sources, including occlusions, adverse weather, or mechanical sensor issues. Unfortunately, the complex and iterative nature of ICP makes assessing its resilience to corruption challenging. While there have been efforts to create challenging datasets and develop simulations to evaluate the resilience of ICP, our method focuses on finding the maximum possible ICP error that can arise from corrupted measurements at a location. We demonstrate that our perturbation-based adversarial attacks can be used pre-deployment to identify locations on a map where ICP is particularly vulnerable to corruptions in the measurements. With such information, autonomous robots can take safer paths when deployed, to mitigate against their measurements being corrupted. The proposed attack outperforms baselines more than 88% of the time across a wide range of scenarios.         ",
    "url": "https://arxiv.org/abs/2403.05666",
    "authors": [
      "Ziyu Zhang",
      "Johann Laconte",
      "Daniil Lisus",
      "Timothy D. Barfoot"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.00992",
    "title": "SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency Guidance",
    "abstract": "           Neural Radiance Field (NeRF) technology has made significant strides in creating novel viewpoints. However, its effectiveness is hampered when working with sparsely available views, often leading to performance dips due to overfitting. FreeNeRF attempts to overcome this limitation by integrating implicit geometry regularization, which incrementally improves both geometry and textures. Nonetheless, an initial low positional encoding bandwidth results in the exclusion of high-frequency elements. The quest for a holistic approach that simultaneously addresses overfitting and the preservation of high-frequency details remains ongoing. This study presents a novel feature-matching-based sparse geometry regularization module, enhanced by a spatially consistent geometry filtering mechanism and a frequency-guided geometric regularization strategy. This module excels at accurately identifying high-frequency keypoints, effectively preserving fine structural details. Through progressive refinement of geometry and textures across NeRF iterations, we unveil an effective few-shot neural rendering architecture, designated as SGCNeRF, for enhanced novel view synthesis. Our experiments demonstrate that SGCNeRF not only achieves superior geometry-consistent outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB in PSNR on LLFF and DTU.         ",
    "url": "https://arxiv.org/abs/2404.00992",
    "authors": [
      "Yuru Xiao",
      "Xianming Liu",
      "Deming Zhai",
      "Kui Jiang",
      "Junjun Jiang",
      "Xiangyang Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2405.05831",
    "title": "Common information in well-mixing graphs and applications to information-theoretic cryptography",
    "abstract": "           We study the connection between mixing properties for bipartite graphs and materialization of the mutual information in one-shot settings. We show that mixing properties of a graph imply impossibility to extract the mutual information shared by the ends of an edge randomly sampled in the graph. We apply these impossibility results to some questions motivated by information-theoretic cryptography. In particular, we show that communication complexity of a secret key agreement in one-shot setting is inherently uneven: for some inputs, almost all communication complexity inevitably falls on only one party.         ",
    "url": "https://arxiv.org/abs/2405.05831",
    "authors": [
      "Geoffroy Caillat-Grenier",
      "Andrei Romashchenko",
      "Rustam Zyavgarov"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2406.00595",
    "title": "Model-Based Calculation Method of Mining Fairness in Blockchain",
    "abstract": "           Mining fairness in blockchain refers to equality between the computational resources invested in mining and the block rewards received. There exists a dilemma wherein increasing the transaction processing capacity of a blockchain compromises mining fairness, thereby undermining its decentralization. This dilemma remains unresolved despite methods such as the greedy heaviest observed subtree (GHOST) protocol, indicating that mining fairness is an inherent bottleneck in the transaction processing capacity of the blockchain system. However, despite its significance, existing analyses neglect the impact of blockchain forks, resulting in imprecise evaluations and limited insights. To address this issue, we propose a method for calculating mining fairness that explicitly captures the influence of forks. First, we approximate a complex blockchain network using a simple mathematical model, assuming that no more than two blocks are generated per round. Within this model, we quantitatively determine local mining fairness and derive several measures of global mining fairness based on local mining fairness. Subsequently, we validated by blockchain network simulations that our calculation method computes mining fairness in networks much more accurately than existing methods. The proposed method facilitates a rigorous evaluation of trade-offs between scalability and decentralization by offering a clear, quantitative framework for measuring and comparing reward distribution among miners. Consequently, it is expected to provide valuable insights for future mining fairness research and the design of next-generation blockchain systems.         ",
    "url": "https://arxiv.org/abs/2406.00595",
    "authors": [
      "Akira Sakurai",
      "Kazuyuki Shudo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2406.02463",
    "title": "Click Without Compromise: Online Advertising Measurement via Per User Differential Privacy",
    "abstract": "           Online advertising is a cornerstone of the Internet ecosystem, with advertising measurement playing a crucial role in optimizing efficiency. Ad measurement entails attributing desired behaviors, such as purchases, to ad exposures across various platforms, necessitating the collection of user activities across these platforms. As this practice faces increasing restrictions due to rising privacy concerns, safeguarding user privacy in this context is imperative. Our work is the first to formulate the real-world challenge of advertising measurement systems with real-time reporting of streaming data in advertising campaigns. We introduce AdsBPC, a novel user-level differential privacy protection scheme for online advertising measurement results. This approach optimizes global noise power and results in a non-identically distributed noise distribution that preserves differential privacy while enhancing measurement accuracy. Through experiments on both real-world advertising campaigns and synthetic datasets, AdsBPC achieves a 33% to 95% increase in accuracy over existing streaming DP mechanisms applied to advertising measurement. This highlights our method's effectiveness in achieving superior accuracy alongside a formal privacy guarantee, thereby advancing the state-of-the-art in privacy-preserving advertising measurement.         ",
    "url": "https://arxiv.org/abs/2406.02463",
    "authors": [
      "Yingtai Xiao",
      "Jian Du",
      "Shikun Zhang",
      "Wanrong Zhang",
      "Qiang Yan",
      "Danfeng Zhang",
      "Daniel Kifer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2407.08883",
    "title": "TractGraphFormer: Anatomically Informed Hybrid Graph CNN-Transformer Network for Interpretable Sex and Age Prediction from Diffusion MRI Tractography",
    "abstract": "           The relationship between brain connections and non-imaging phenotypes is increasingly studied using deep neural networks. However, the local and global properties of brain white matter networks are often overlooked in convolutional network design. We introduce TractGraphFormer, a hybrid Graph CNN-Transformer deep learning framework tailored for diffusion MRI tractography. This model leverages local anatomical characteristics and global feature dependencies of white matter structures. The Graph CNN module captures white matter geometry and grey matter connectivity to aggregate local features from anatomically similar white matter connections, while the Transformer module uses self-attention to enhance global information learning. Additionally, TractGraphFormer includes an attention module for interpreting predictive white matter connections. We apply TractGraphFormer to tasks of sex and age prediction. TractGraphFormer shows strong performance in large datasets of children (n=9345) and young adults (n=1065). Overall, our approach suggests that widespread connections in the WM are predictive of the sex and age of an individual. For each prediction task, consistent predictive anatomical tracts are identified across the two datasets. The proposed approach highlights the potential of integrating local anatomical information and global feature dependencies to improve prediction performance in machine learning with diffusion MRI tractography.         ",
    "url": "https://arxiv.org/abs/2407.08883",
    "authors": [
      "Yuqian Chen",
      "Fan Zhang",
      "Meng Wang",
      "Leo R. Zekelman",
      "Suheyla Cetin-Karayumak",
      "Tengfei Xue",
      "Chaoyi Zhang",
      "Yang Song",
      "Jarrett Rushmore",
      "Nikos Makris",
      "Yogesh Rathi",
      "Weidong Cai",
      "Lauren J. O'Donnell"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.03475",
    "title": "Self Supervised Networks for Learning Latent Space Representations of Human Body Scans and Motions",
    "abstract": "           This paper introduces self-supervised neural network models to tackle several fundamental problems in the field of 3D human body analysis and processing. First, we propose VariShaPE (Varifold Shape Parameter Estimator), a novel architecture for the retrieval of latent space representations of body shapes and poses. This network offers a fast and robust method to estimate the embedding of arbitrary unregistered meshes into the latent space. Second, we complement the estimation of latent codes with MoGeN (Motion Geometry Network) a framework that learns the geometry on the latent space itself. This is achieved by lifting the body pose parameter space into a higher dimensional Euclidean space in which body motion mini-sequences from a training set of 4D data can be approximated by simple linear interpolation. Using the SMPL latent space representation we illustrate how the combination of these network models, once trained, can be used to perform a variety of tasks with very limited computational cost. This includes operations such as motion interpolation, extrapolation and transfer as well as random shape and pose generation.         ",
    "url": "https://arxiv.org/abs/2411.03475",
    "authors": [
      "Emmanuel Hartman",
      "Nicolas Charon",
      "Martin Bauer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.09553",
    "title": "OOD-SEG: Exploiting out-of-distribution detection techniques for learning image segmentation from sparse multi-class positive-only annotations",
    "abstract": "           Despite significant advancements, segmentation based on deep neural networks in medical and surgical imaging faces several challenges, two of which we aim to address in this work. First, acquiring complete pixel-level segmentation labels for medical images is time-consuming and requires domain expertise. Second, typical segmentation pipelines cannot detect out-of-distribution (OOD) pixels, leaving them prone to spurious outputs during deployment. In this work, we propose a novel segmentation approach which broadly falls within the positive-unlabelled (PU) learning paradigm and exploits tools from OOD detection techniques. Our framework learns only from sparsely annotated pixels from multiple positive-only classes and does not use any annotation for the background class. These multi-class positive annotations naturally fall within the in-distribution (ID) set. Unlabelled pixels may contain positive classes but also negative ones, including what is typically referred to as \\emph{background} in standard segmentation formulations. Here, we forgo the need for background annotation and consider these together with any other unseen classes as part of the OOD set. Our framework can integrate, at a pixel-level, any OOD detection approaches designed for classification tasks. To address the lack of existing OOD datasets and established evaluation metric for medical image segmentation, we propose a cross-validation strategy that treats held-out labelled classes as OOD. Extensive experiments on both multi-class hyperspectral and RGB surgical imaging datasets demonstrate the robustness and generalisation capability of our proposed framework.         ",
    "url": "https://arxiv.org/abs/2411.09553",
    "authors": [
      "Junwen Wang",
      "Zhonghao Wang",
      "Oscar MacCormac",
      "Jonathan Shapey",
      "Tom Vercauteren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.11683",
    "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation",
    "abstract": "           Robotic manipulation in the physical world is increasingly empowered by \\textit{large language models} (LLMs) and \\textit{vision-language models} (VLMs), leveraging their understanding and perception capabilities. Recently, various attacks against such robotic policies have been proposed, with backdoor attacks drawing considerable attention for their high stealth and strong persistence capabilities. However, existing backdoor efforts are limited to simulators and suffer from physical-world realization. To address this, we propose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic backdoor attack in the physical world. Specifically, we introduce a module-poisoning approach by embedding a backdoor module into the modular robotic policy, enabling backdoor control over the policy's visual perception module thereby backdooring the entire robotic policy. Our vanilla implementation leverages a backdoor-finetuned VLM to serve as the backdoor module. To enhance its generalization in physical environments, we propose a prime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing three types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation}, and \\textit{intentional} attacks, thus achieving finer-grained backdoors. Extensive experiments on the UR3e manipulator with 18 task instructions using robotic policies based on four VLMs demonstrate the broad effectiveness and physical-world stealth of TrojanRobot. Our attack's video demonstrations are available via a github link this https URL.         ",
    "url": "https://arxiv.org/abs/2411.11683",
    "authors": [
      "Xianlong Wang",
      "Hewen Pan",
      "Hangtao Zhang",
      "Minghui Li",
      "Shengshan Hu",
      "Ziqi Zhou",
      "Lulu Xue",
      "Peijin Guo",
      "Aishan Liu",
      "Leo Yu Zhang",
      "Xiaohua Jia"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.12520",
    "title": "VMGNet: A Low Computational Complexity Robotic Grasping Network Based on VMamba with Multi-Scale Feature Fusion",
    "abstract": "           While deep learning-based robotic grasping technology has demonstrated strong adaptability, its computational complexity has also significantly increased, making it unsuitable for scenarios with high real-time requirements. Therefore, we propose a low computational complexity and high accuracy model named VMGNet for robotic grasping. For the first time, we introduce the Visual State Space into the robotic grasping field to achieve linear computational complexity, thereby greatly reducing the model's computational cost. Meanwhile, to improve the accuracy of the model, we propose an efficient and lightweight multi-scale feature fusion module, named Fusion Bridge Module, to extract and fuse information at different scales. We also present a new loss function calculation method to enhance the importance differences between subtasks, improving the model's fitting ability. Experiments show that VMGNet has only 8.7G Floating Point Operations and an inference time of 8.1 ms on our devices. VMGNet also achieved state-of-the-art performance on the Cornell and Jacquard public datasets. To validate VMGNet's effectiveness in practical applications, we conducted real grasping experiments in multi-object scenarios, and VMGNet achieved an excellent performance with a 94.4% success rate in real-world grasping tasks. The video for the real-world robotic grasping experiments is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.12520",
    "authors": [
      "Yuhao Jin",
      "Qizhong Gao",
      "Xiaohui Zhu",
      "Yong Yue",
      "Eng Gee Lim",
      "Yuqing Chen",
      "Prudence Wong",
      "Yijie Chu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.15010",
    "title": "Hybrid-Regularized Magnitude Pruning for Robust Federated Learning under Covariate Shift",
    "abstract": "           Federated Learning offers a solution for decentralised model training, addressing the difficulties associated with distributed data and privacy in machine learning. However, the fact of data heterogeneity in federated learning frequently hinders the global model's generalisation, leading to low performance and adaptability to unseen data. This problem is particularly critical for specialised applications such as medical imaging, where both the data and the number of clients are limited. In this paper, we empirically demonstrate that inconsistencies in client-side training distributions substantially degrade the performance of federated learning models across multiple benchmark datasets. We propose a novel FL framework using a combination of pruning and regularisation of clients' training to improve the sparsity, redundancy, and robustness of neural connections, and thereby the resilience to model aggregation. To address a relatively unexplored dimension of data heterogeneity, we further introduce a novel benchmark dataset, CelebA-Gender, specifically designed to control for within-class distributional shifts across clients based on attribute variations, thereby complementing the predominant focus on inter-class imbalance in prior federated learning research. Comprehensive experiments on many datasets like CIFAR-10, MNIST, and the newly introduced CelebA-Gender dataset demonstrate that our method consistently outperforms standard FL baselines, yielding more robust and generalizable models in heterogeneous settings.         ",
    "url": "https://arxiv.org/abs/2412.15010",
    "authors": [
      "Ozgu Goksu",
      "Nicolas Pugeault"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.16358",
    "title": "Texture- and Shape-based Adversarial Attacks for Overhead Image Vehicle Detection",
    "abstract": "           Detecting vehicles in aerial images is difficult due to complex backgrounds, small object sizes, shadows, and occlusions. Although recent deep learning advancements have improved object detection, these models remain susceptible to adversarial attacks (AAs), challenging their reliability. Traditional AA strategies often ignore practical implementation constraints. Our work proposes realistic and practical constraints on texture (lowering resolution, limiting modified areas, and color ranges) and analyzes the impact of shape modifications on attack performance. We conducted extensive experiments with three object detector architectures, demonstrating the performance-practicality trade-off: more practical modifications tend to be less effective, and vice versa. We release both code and data to support reproducibility at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.16358",
    "authors": [
      "Mikael Yeghiazaryan",
      "Sai Abhishek Siddhartha Namburu",
      "Emily Kim",
      "Stanislav Panev",
      "Celso de Melo",
      "Fernando De la Torre",
      "Jessica K. Hodgins"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.16918",
    "title": "Detect Changes like Humans: Incorporating Semantic Priors for Improved Change Detection",
    "abstract": "           When given two similar images, humans identify their differences by comparing the appearance (e.g., color, texture) with the help of semantics (e.g., objects, relations). However, mainstream binary change detection models adopt a supervised training paradigm, where the annotated binary change map is the main constraint. Thus, such methods primarily emphasize difference-aware features between bi-temporal images, and the semantic understanding of changed landscapes is undermined, resulting in limited accuracy in the face of noise and illumination variations. To this end, this paper explores incorporating semantic priors from visual foundation models to improve the ability to detect changes. Firstly, we propose a Semantic-Aware Change Detection network (SA-CDNet), which transfers the knowledge of visual foundation models (i.e., FastSAM) to change detection. Inspired by the human visual paradigm, a novel dual-stream feature decoder is derived to distinguish changes by combining semantic-aware features and difference-aware features. Secondly, we explore a single-temporal pre-training strategy for better adaptation of visual foundation models. With pseudo-change data constructed from single-temporal segmentation datasets, we employ an extra branch of proxy semantic segmentation task for pre-training. We explore various settings like dataset combinations and landscape types, thus providing valuable insights. Experimental results on five challenging benchmarks demonstrate the superiority of our method over the existing state-of-the-art methods. The code is available at $\\href{this https URL}{github}$.         ",
    "url": "https://arxiv.org/abs/2412.16918",
    "authors": [
      "Yuhang Gan",
      "Wenjie Xuan",
      "Zhiming Luo",
      "Lei Fang",
      "Zengmao Wang",
      "Juhua Liu",
      "Bo Du"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.18934",
    "title": "Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference",
    "abstract": "           With the continuous advancement in the performance of large language models (LLMs), their demand for computational resources and memory has significantly increased, which poses major challenges for efficient inference on consumer-grade devices and legacy servers. These devices typically feature relatively weaker GPUs and stronger CPUs. Although techniques such as parameter offloading and partial offloading can alleviate GPU memory pressure to some extent, their effectiveness is limited due to communication latency and suboptimal hardware resource utilization. To address this issue, we propose Dovetail, a lossless inference acceleration method that leverages the complementary characteristics of heterogeneous devices and the advantages of speculative decoding. Dovetail deploys a draft model on the GPU to perform preliminary predictions, while a target model running on the CPU validates these outputs. By reducing the granularity of data transfer, Dovetail significantly minimizes communication overhead. To further improve efficiency, we optimize the draft model specifically for heterogeneous hardware environments by reducing the number of draft tokens to lower parallel verification latency, increasing model depth to enhance predictive capabilities, and introducing a Dynamic Gating Fusion (DGF) mechanism to improve the integration of feature and embedding information. We conduct comprehensive evaluations of Dovetail across various consumer-grade GPUs, covering multiple tasks and mainstream models. Experimental results on 13B models demonstrate that Dovetail achieves inference speedups ranging from 1.79x to 10.1x across different devices, while maintaining consistency and stability in the distribution of generated texts.         ",
    "url": "https://arxiv.org/abs/2412.18934",
    "authors": [
      "Libo Zhang",
      "Zhaoning Zhang",
      "Baizhou Xu",
      "Rui Li",
      "Zhiliang Tian",
      "Songzhu Mei",
      "Dongsheng Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.05752",
    "title": "PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based Implicit Neural Map",
    "abstract": "           Robots benefit from high-fidelity reconstructions of their environment, which should be geometrically accurate and photorealistic to support downstream tasks. While this can be achieved by building distance fields from range sensors and radiance fields from cameras, realising scalable incremental mapping of both fields consistently and at the same time with high quality is challenging. In this paper, we propose a novel map representation that unifies a continuous signed distance field and a Gaussian splatting radiance field within an elastic and compact point-based implicit neural map. By enforcing geometric consistency between these fields, we achieve mutual improvements by exploiting both modalities. We present a novel LiDAR-visual SLAM system called PINGS using the proposed map representation and evaluate it on several challenging large-scale datasets. Experimental results demonstrate that PINGS can incrementally build globally consistent distance and radiance fields encoded with a compact set of neural points. Compared to state-of-the-art methods, PINGS achieves superior photometric and geometric rendering at novel views by constraining the radiance field with the distance field. Furthermore, by utilizing dense photometric cues and multi-view consistency from the radiance field, PINGS produces more accurate distance fields, leading to improved odometry estimation and mesh reconstruction. We also provide an open-source implementation of PING at: this https URL.         ",
    "url": "https://arxiv.org/abs/2502.05752",
    "authors": [
      "Yue Pan",
      "Xingguang Zhong",
      "Liren Jin",
      "Louis Wiesmann",
      "Marija Popovi\u0107",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2502.09297",
    "title": "When Do Neural Networks Learn World Models?",
    "abstract": "           Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we present the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions--even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.         ",
    "url": "https://arxiv.org/abs/2502.09297",
    "authors": [
      "Tianren Zhang",
      "Guanyu Chen",
      "Feng Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.13061",
    "title": "Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection",
    "abstract": "           Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While Large Multimodal Models (LMMs) have shown promise in hateful meme detection, they face notable challenges like sub-optimal performance and limited out-of-domain generalization capabilities. Recent studies further reveal the limitations of both supervised fine-tuning (SFT) and in-context learning when applied to LMMs in this setting. To address these issues, we propose a robust adaptation framework for hateful meme detection that enhances in-domain accuracy and cross-domain generalization while preserving the general vision-language capabilities of LMMs. Analysis reveals that our approach achieves improved robustness under adversarial attacks compared to SFT models. Experiments on six meme classification datasets show that our approach achieves state-of-the-art performance, outperforming larger agentic systems. Moreover, our method generates higher-quality rationales for explaining hateful content compared to standard SFT, enhancing model interpretability. Code available at this https URL ",
    "url": "https://arxiv.org/abs/2502.13061",
    "authors": [
      "Jingbiao Mei",
      "Jinghong Chen",
      "Guangyu Yang",
      "Weizhe Lin",
      "Bill Byrne"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.19642",
    "title": "Contrastive MIM: A Contrastive Mutual Information Framework for Unified Generative and Discriminative Representation Learning",
    "abstract": "           Learning representations that generalize well to unknown downstream tasks is a central challenge in representation learning. Existing approaches such as contrastive learning, self-supervised masking, and denoising auto-encoders address this challenge with varying trade-offs. In this paper, we introduce the {contrastive Mutual Information Machine} (cMIM), a probabilistic framework that augments the Mutual Information Machine (MIM) with a novel contrastive objective. While MIM maximizes mutual information between inputs and latent variables and encourages clustering of latent codes, its representations underperform on discriminative tasks compared to state-of-the-art alternatives. cMIM addresses this limitation by enforcing global discriminative structure while retaining MIM's generative strengths. We present two main contributions: (1) we propose cMIM, a contrastive extension of MIM that eliminates the need for positive data augmentation and is robust to batch size, unlike InfoNCE-based methods; (2) we introduce {informative embeddings}, a general technique for extracting enriched representations from encoder--decoder models that substantially improve discriminative performance without additional training, and which apply broadly beyond MIM. Empirical results demonstrate that cMIM consistently outperforms MIM and InfoNCE in classification and regression tasks, while preserving comparable reconstruction quality. These findings suggest that cMIM provides a unified framework for learning representations that are simultaneously effective for discriminative and generative applications.         ",
    "url": "https://arxiv.org/abs/2502.19642",
    "authors": [
      "Micha Livne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.00374",
    "title": "MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning via Modality Alignment and Retention",
    "abstract": "           Histopathology and transcriptomics are fundamental modalities in oncology, encapsulating the morphological and molecular aspects of the disease. Multi-modal self-supervised learning has demonstrated remarkable potential in learning pathological representations by integrating diverse data sources. Conventional multi-modal integration methods primarily emphasize modality alignment, while paying insufficient attention to retaining the modality-specific structures. However, unlike conventional scenarios where multi-modal inputs share highly overlapping features, histopathology and transcriptomics exhibit pronounced heterogeneity, offering orthogonal yet complementary insights. Histopathology provides morphological and spatial context, elucidating tissue architecture and cellular topology, whereas transcriptomics delineates molecular signatures through gene expression patterns. This inherent disparity introduces a major challenge in aligning them while maintaining modality-specific fidelity. To address these challenges, we present MIRROR, a novel multi-modal representation learning method designed to foster both modality alignment and retention. MIRROR employs dedicated encoders to extract comprehensive features for each modality, which is further complemented by a modality alignment module to achieve seamless integration between phenotype patterns and molecular profiles. Furthermore, a modality retention module safeguards unique attributes from each modality, while a style clustering module mitigates redundancy and enhances disease-relevant information by modeling and aligning consistent pathological signatures within a clustering space. Extensive evaluations on TCGA cohorts for cancer subtyping and survival analysis highlight MIRROR's superior performance, demonstrating its effectiveness in constructing comprehensive oncological feature representations and benefiting the cancer diagnosis.         ",
    "url": "https://arxiv.org/abs/2503.00374",
    "authors": [
      "Tianyi Wang",
      "Jianan Fan",
      "Dingxin Zhang",
      "Dongnan Liu",
      "Yong Xia",
      "Heng Huang",
      "Weidong Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2503.08994",
    "title": "DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive Neural Predicate Modulation",
    "abstract": "           Research on learned cardinality estimation has made significant progress in recent years. However, existing methods still face distinct challenges that hinder their practical deployment in production environments. We define these challenges as the ``Trilemma of Cardinality Estimation'', where learned cardinality estimation methods struggle to balance generality, accuracy, and updatability. To address these challenges, we introduce DistJoin, a join cardinality estimator based on efficient distribution prediction using multi-autoregressive models. Our contributions are threefold: (1) We propose a method to estimate join cardinality by leveraging the probability distributions of individual tables in a decoupled manner. (2) To meet the requirements of efficiency for DistJoin, we develop Adaptive Neural Predicate Modulation (ANPM), a high-throughput distribution estimation model. (3) We demonstrate that an existing similar approach suffers from variance accumulation issues by formal variance analysis. To mitigate this problem, DistJoin employs a selectivity-based approach to infer join cardinality, effectively reducing variance. In summary, DistJoin not only represents the first data-driven method to support both equi and non-equi joins simultaneously but also demonstrates superior accuracy while enabling fast and flexible updates. The experimental results demonstrate that DistJoin achieves the highest accuracy, robustness to data updates, generality, and comparable update and inference speed relative to existing methods.         ",
    "url": "https://arxiv.org/abs/2503.08994",
    "authors": [
      "Kaixin Zhang",
      "Hongzhi Wang",
      "Ziqi Li",
      "Yabin Lu",
      "Yingze Li",
      "Yu Yan",
      "Yiming Guan"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.11790",
    "title": "Visualizing Thought: Conceptual Diagrams Enable Robust Combinatorial Planning in LMMs",
    "abstract": "           Human reasoning relies on constructing and manipulating mental models -- simplified internal representations of situations that we use to understand and solve problems. Conceptual diagrams (e.g., a sketch drawn by a human to aid reasoning) externalize these mental models, abstracting irrelevant details to efficiently capture how entities interact with each other. In contrast, Large Language Models (LLMs) and Large MultiModal Models (LMMs) predominantly reason through text, limiting their effectiveness in complex multi-step tasks. In this paper, we propose Visual Thinking, a zero-shot framework that enables LMMs to reason through multiple chains of (self-generated) conceptual diagrams, significantly enhancing their combinatorial planning capabilities. Our approach does not require any human initialization beyond the natural language description of the task. It integrates both textual and diagrammatic reasoning within an optimized Graph-of-Thought inference framework, enhanced by beam search and depth-wise backtracking. Evaluated on multiple challenging PDDL planning domains, our method substantially improves LMMs' performance (e.g., GPT-4o: 35.5% -> 90.2% in Blocksworld) and consistently outperforms other text-only search-based inference methods. On more difficult planning domains with solution depths up to 40, our approach outperforms even the o1-preview reasoning model (e.g., 16 percentage points improvement in Floor Tiles). These results highlight the value of conceptual diagrams as a reasoning medium in LMMs.         ",
    "url": "https://arxiv.org/abs/2503.11790",
    "authors": [
      "Nasim Borazjanizadeh",
      "Roei Herzig",
      "Eduard Oks",
      "Trevor Darrell",
      "Rogerio Feris",
      "Leonid Karlinsky"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.15552",
    "title": "Personalized Attacks of Social Engineering in Multi-turn Conversations: LLM Agents for Simulation and Detection",
    "abstract": "           The rapid advancement of conversational agents, particularly chatbots powered by Large Language Models (LLMs), poses a significant risk of social engineering (SE) attacks on social media platforms. SE detection in multi-turn, chat-based interactions is considerably more complex than single-instance detection due to the dynamic nature of these conversations. A critical factor in mitigating this threat is understanding the SE attack mechanisms through which SE attacks operate, specifically how attackers exploit vulnerabilities and how victims' personality traits contribute to their susceptibility. In this work, we propose an LLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating multi-turn conversations. We model victim agents with varying personality traits to assess how psychological profiles influence susceptibility to manipulation. Using a dataset of over 1000 simulated conversations, we examine attack scenarios in which adversaries, posing as recruiters, funding agencies, and journalists, attempt to extract sensitive information. Based on this analysis, we present a proof of concept, SE-OmniGuard, to offer personalized protection to users by leveraging prior knowledge of the victims personality, evaluating attack strategies, and monitoring information exchanges in conversations to identify potential SE attempts.         ",
    "url": "https://arxiv.org/abs/2503.15552",
    "authors": [
      "Tharindu Kumarage",
      "Cameron Johnson",
      "Jadie Adams",
      "Lin Ai",
      "Matthias Kirchner",
      "Anthony Hoogs",
      "Joshua Garland",
      "Julia Hirschberg",
      "Arslan Basharat",
      "Huan Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.01585",
    "title": "Nonlinear Bandwidth and Bode Diagrams based on Scaled Relative Graphs",
    "abstract": "           Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain method for the analysis of Nonlinear (NL) systems. In this paper, we restrict the SRG to particular input spaces to compute frequency-dependent incremental gain bounds for nonlinear systems. This leads to a NL generalization of the Bode diagram, where the sinusoidal, harmonic, and subharmonic inputs are considered separately. When applied to the analysis of the NL loop transfer and sensitivity, we define a notion of bandwidth for both the open-loop and closed-loop, compatible with the Linear Time-Invariant (LTI) definitions. We illustrate the power of our method on the analysis of a DC motor with a parasitic nonlinearity and verify our results in simulations.         ",
    "url": "https://arxiv.org/abs/2504.01585",
    "authors": [
      "Julius P. J. Krebbekx",
      "Roland T\u00f3th",
      "Amritam Das"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2504.18046",
    "title": "DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification",
    "abstract": "           Ophthalmic diseases pose a significant global health burden. However, traditional diagnostic methods and existing monocular image-based deep learning approaches often overlook the pathological correlations between the two eyes. In practical medical robotic diagnostic scenarios, paired retinal images (binocular fundus images) are frequently required as diagnostic evidence. To address this, we propose DMS-Net-a dual-modal multi-scale siamese network for binocular retinal image classification. The framework employs a weight-sharing siamese ResNet-152 architecture to concurrently extract deep semantic features from bilateral fundus images. To tackle challenges like indistinct lesion boundaries and diffuse pathological distributions, we introduce the OmniPool Spatial Integrator Module (OSIM), which achieves multi-resolution feature aggregation through multi-scale adaptive pooling and spatial attention mechanisms. Furthermore, the Calibrated Analogous Semantic Fusion Module (CASFM) leverages spatial-semantic recalibration and bidirectional attention mechanisms to enhance cross-modal interaction, aggregating modality-agnostic representations of fundus structures. To fully exploit the differential semantic information of lesions present in bilateral fundus features, we introduce the Cross-Modal Contrastive Alignment Module (CCAM). Additionally, to enhance the aggregation of lesion-correlated semantic information, we introduce the Cross-Modal Integrative Alignment Module (CIAM). Evaluation on the ODIR-5K dataset demonstrates that DMS-Net achieves state-of-the-art performance with an accuracy of 82.9%, recall of 84.5%, and a Cohen's kappa coefficient of 83.2%, showcasing robust capacity in detecting symmetrical pathologies and improving clinical decision-making for ocular diseases. Code and the processed dataset will be released subsequently.         ",
    "url": "https://arxiv.org/abs/2504.18046",
    "authors": [
      "Guohao Huo",
      "Zibo Lin",
      "Zitong Wang",
      "Ruiting Dai",
      "Hao Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.04416",
    "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models",
    "abstract": "           Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose \\textbf{OBLIVIATE}, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA) ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: \\emph{forget quality} (via a new document-level memorization score), \\emph{model utility}, and \\emph{fluency}. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.         ",
    "url": "https://arxiv.org/abs/2505.04416",
    "authors": [
      "Xiaoyu Xu",
      "Minxin Du",
      "Qingqing Ye",
      "Haibo Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.07322",
    "title": "RealRep: Generalized SDR-to-HDR Conversion via Attribute-Disentangled Representation Learning",
    "abstract": "           High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becoming increasingly widespread, driving a growing need for converting Standard Dynamic Range (SDR) content to HDR. Existing methods primarily rely on fixed tone mapping operators, which struggle to handle the diverse appearances and degradations commonly present in real-world SDR content. To address this limitation, we propose a generalized SDR-to-HDR framework that enhances robustness by learning attribute-disentangled representations. Central to our approach is Realistic Attribute-Disentangled Representation Learning (RealRep), which explicitly disentangles luminance and chrominance components to capture intrinsic content variations across different SDR distributions. Furthermore, we design a Luma-/Chroma-aware negative exemplar generation strategy that constructs degradation-sensitive contrastive pairs, effectively modeling tone discrepancies across SDR styles. Building on these attribute-level priors, we introduce the Degradation-Domain Aware Controlled Mapping Network (DDACMNet), a lightweight, two-stage framework that performs adaptive hierarchical mapping guided by a control-aware normalization mechanism. DDACMNet dynamically modulates the mapping process via degradation-conditioned features, enabling robust adaptation across diverse degradation domains. Extensive experiments demonstrate that RealRep consistently outperforms state-of-the-art methods in both generalization and perceptually faithful HDR color gamut reconstruction.         ",
    "url": "https://arxiv.org/abs/2505.07322",
    "authors": [
      "Gang He",
      "Siqi Wang",
      "Kepeng Xu",
      "Lin Zhang",
      "Li Xu",
      "Weiran Wang",
      "Yu-Wing Tai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.07546",
    "title": "GRADA: Graph-based Reranking against Adversarial Documents Attack",
    "abstract": "           Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large language models (LLMs) by integrating external knowledge from retrieved documents, thereby overcoming the limitations of models' static intrinsic knowledge. However, these systems are susceptible to adversarial attacks that manipulate the retrieval process by introducing documents that are adversarial yet semantically similar to the query. Notably, while these adversarial documents resemble the query, they exhibit weak similarity to benign documents in the retrieval set. Thus, we propose a simple yet effective Graph-based Reranking against Adversarial Document Attacks (GRADA) framework aiming at preserving retrieval quality while significantly reducing the success of adversaries. Our study evaluates the effectiveness of our approach through experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b, Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with results from the Natural Questions dataset demonstrating up to an 80% reduction in attack success rates while maintaining minimal loss in accuracy.         ",
    "url": "https://arxiv.org/abs/2505.07546",
    "authors": [
      "Jingjie Zheng",
      "Aryo Pradipta Gema",
      "Giwon Hong",
      "Xuanli He",
      "Pasquale Minervini",
      "Youcheng Sun",
      "Qiongkai Xu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.18744",
    "title": "LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Complex Reasoning",
    "abstract": "           Text-to-SQL is a critical task in natural language processing that aims to transform natural language questions into accurate and executable SQL queries. In real-world scenarios, these reasoning tasks are often accompanied by complex mathematical computations, domain knowledge, and hypothetical reasoning scenarios. However, existing large-scale Text-to-SQL datasets typically focus on business logic and task logic, neglecting critical factors such as vertical domain knowledge, complex mathematical reasoning, and hypothetical reasoning, which are essential for realistically reflecting the reasoning demands in practical applications and completing data querying and analysis. To bridge this gap, we introduce LogicCat, the first Text-to-SQL benchmark dataset specifically designed for complex reasoning and chain-of-thought parsing, encompassing physics, arithmetic, commonsense, and hypothetical reasoning scenarios. LogicCat comprises 4,038 English questions paired 12,114 detailed chain-of-thought reasoning steps, spanning 45 databases across diverse domains, significantly surpassing existing datasets in complexity. Experimental results demonstrate that LogicCat substantially increases the task difficulty for current state-of-the-art models to at most 33.20% execution accuracy, indicating that this task remains exceptionally challenging. The advancement of LogicCat represents a crucial step toward developing systems suitable for real-world enterprise data analysis and autonomous query generation. We have released our dataset code at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.18744",
    "authors": [
      "Tao Liu",
      "Xutao Mao",
      "Hongying Zan",
      "Dixuan Zhang",
      "Yifan Li",
      "Haixin Liu",
      "Lulu Kong",
      "Jiaming Hou",
      "Rui Li",
      "YunLong Li",
      "aoze zheng",
      "Zhiqiang Zhang",
      "Luo Zhewei",
      "Kunli Zhang",
      "Min Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.22626",
    "title": "SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning",
    "abstract": "           Imitation learning advances robot capabilities by enabling the acquisition of diverse behaviors from human demonstrations. However, large-scale datasets used for policy training often introduce substantial variability in quality, which can negatively impact performance. As a result, automatically curating datasets by filtering low-quality samples to improve quality becomes essential. Existing robotic curation approaches rely on costly manual annotations and perform curation at a coarse granularity, such as the dataset or trajectory level, failing to account for the quality of individual state-action pairs. To address this, we introduce SCIZOR, a self-supervised data curation framework that filters out low-quality state-action pairs to improve the performance of imitation learning policies. SCIZOR targets two complementary sources of low-quality data: suboptimal data, which hinders learning with undesirable actions, and redundant data, which dilutes training with repetitive patterns. SCIZOR leverages a self-supervised task progress predictor for suboptimal data to remove samples lacking task progression, and a deduplication module operating on joint state-action representation for samples with redundant patterns. Empirically, we show that SCIZOR enables imitation learning policies to achieve higher performance with less data, yielding an average improvement of 15.4% across multiple benchmarks. More information is available at: this https URL ",
    "url": "https://arxiv.org/abs/2505.22626",
    "authors": [
      "Yu Zhang",
      "Yuqi Xie",
      "Huihan Liu",
      "Rutav Shah",
      "Michael Wan",
      "Linxi Fan",
      "Yuke Zhu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.07853",
    "title": "Modeling the Diachronic Evolution of Legal Norms: An LRMoo-Based, Component-Level, Event-Centric Approach to Legal Knowledge Graphs",
    "abstract": "           Effectively representing legal norms for automated processing is a critical challenge, particularly in tracking the temporal evolution of their hierarchical components. While foundational conceptual frameworks like IFLA LRMoo provide a generic toolkit for bibliographic data, and encoding standards like Akoma Ntoso offer a robust syntax for legal documents, a dedicated, formal modeling pattern for granular, component-level versioning is still required. This limitation hinders the deterministic point-intime reconstruction of legal texts, a fundamental capability for reliable Legal Tech and AI applications. This paper proposes a structured, temporal modeling pattern grounded in the LRMoo ontology to address this need. Our approach models the evolution of a legal norm as a diachronic chain of F2 Expressions. We introduce a key distinction between a language-agnostic Temporal Version (TV)-a semantic snapshot of the norm's structure-and its concrete monolingual realizations, the Language Versions (LV). Both are modeled as F2 Expressions linked by the canonical R76 is derivative of property. This paradigm is applied recursively to the legal text's internal structure, representing it as a parallel hierarchy of abstract Component Works (F1) and their versioned Component Expressions (F2). Furthermore, we formalize the legislative amendment process using the F28 Expression Creation event, allowing changes to be traced from an amending act to its precise effect on the amended norm. Using the Brazilian Federal Constitution as a case study, we demonstrate how this event-centric architecture enables the precise, deterministic retrieval and reconstruction of any part of a legal text as it existed on a specific date. The model provides a robust foundation for building verifiable knowledge graphs and advanced AI tools, overcoming the limitations of current generative models.         ",
    "url": "https://arxiv.org/abs/2506.07853",
    "authors": [
      "Hudson de Martim"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2506.10582",
    "title": "SPACE-iT: Spatial-Aware Curriculum Exploration and Feedback-Driven Adaptive Augmentation for Vision Transformer Distillation",
    "abstract": "           Knowledge distillation (KD) has proven to be a powerful technique for improving the performance of Vision Transformers (ViTs). However, traditional KD methods often treat all image patches uniformly, overlooking spatial variations in learning difficulty. To address this limitation, we propose SPACE-iT, a novel framework for Spatial-Aware Curriculum Exploration via Feedback-Driven Adaptive Augmentation. At its core, SPACE-iT computes spatial confidence scores at the attention, patch, and logit levels. This confidence map supports a two-fold strategy: (1) dynamically modulating the distillation loss, and (2) guiding an adaptive augmentation module that intensifies reverse curriculum learning. By establishing a feedback-driven reverse curriculum that initially exposes students to challenging regions-progressing from hard to easy-SPACE-iT enables more effective learning of complex spatial patterns and achieves superior performance over vanilla distillation, without introducing additional memory overhead.         ",
    "url": "https://arxiv.org/abs/2506.10582",
    "authors": [
      "Jihyeon Seong",
      "Hyunkyung Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.13905",
    "title": "Spec2RTL-Agent: Automated Hardware Code Generation from Complex Specifications Using LLM Agent Systems",
    "abstract": "           Despite recent progress in generating hardware RTL code with LLMs, existing solutions still suffer from a substantial gap between practical application scenarios and the requirements of real-world RTL code development. Prior approaches either focus on overly simplified hardware descriptions or depend on extensive human guidance to process complex specifications, limiting their scalability and automation potential. In this paper, we address this gap by proposing an LLM agent system, termed Spec2RTL-Agent, designed to directly process complex specification documentation and generate corresponding RTL code implementations, advancing LLM-based RTL code generation toward more realistic application settings. To achieve this goal, Spec2RTL-Agent introduces a novel multi-agent collaboration framework that integrates three key enablers: (1) a reasoning and understanding module that translates specifications into structured, step-by-step implementation plans; (2) a progressive coding and prompt optimization module that iteratively refines the code across multiple representations to enhance correctness and synthesisability for RTL conversion; and (3) an adaptive reflection module that identifies and traces the source of errors during generation, ensuring a more robust code generation flow. Instead of directly generating RTL from natural language, our system strategically generates synthesizable C++ code, which is then optimized for HLS. This agent-driven refinement ensures greater correctness and compatibility compared to naive direct RTL generation approaches. We evaluate Spec2RTL-Agent on three specification documents, showing it generates accurate RTL code with up to 75% fewer human interventions than existing methods. This highlights its role as the first fully automated multi-agent system for RTL generation from unstructured specs, reducing reliance on human effort in hardware design.         ",
    "url": "https://arxiv.org/abs/2506.13905",
    "authors": [
      "Zhongzhi Yu",
      "Mingjie Liu",
      "Michael Zimmer",
      "Yingyan Celine Lin",
      "Yong Liu",
      "Haoxing Ren"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2506.19487",
    "title": "TRMAC: A Time-Reversal-based MAC Protocol for Wireless Networks within Computing Packages",
    "abstract": "           As chiplet-based integration and many-core architectures become the norm in high-performance computing, on-chip wireless communication has emerged as a compelling alternative to traditional interconnects. However, scalable Medium Access Control (MAC) remains a fundamental challenge, particularly under dense traffic and limited spectral resources. This paper presents TRMAC, a novel cross-layer MAC protocol that exploits the spatial focusing capability of Time Reversal (TR) to enable multiple parallel transmissions over a shared frequency channel. By leveraging the quasi-deterministic nature of on-chip wireless channels, TRMAC pre-characterizes channel impulse responses to coordinate access using energy-based thresholds, eliminating the need for orthogonal resource allocation or centralized arbitration. Through detailed physical-layer simulation and system-level evaluation on diverse traffic, TRMAC demonstrates comparable or superior performance to existing multi-channel MAC protocols, achieving low latency, high throughput, and strong scalability across hundreds of cores. TRMAC provides a low-complexity, high-efficiency solution for future Wireless Networks-on-Chip (WNoCs), particularly in chiplet-based systems where spatial reuse and modularity are critical. With simulations we prove that TRMAC can be utilized for parallel transmissions with a single frequency channel with a similar throughput and latency as in using multiple frequency bands omitting the need for complex transceivers. This work establishes a new design direction for MAC protocols that are tightly integrated with the underlying channel physics to meet the demands of next-generation computing platforms.         ",
    "url": "https://arxiv.org/abs/2506.19487",
    "authors": [
      "Ama Bandara",
      "Abhijit Das",
      "Fatima Rodriguez-Galan",
      "Eduard Alarcon",
      "Sergi Abadal"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2507.02074",
    "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges",
    "abstract": "           Crash detection from video feeds is a critical problem in intelligent transportation systems. Recent developments in large language models (LLMs) and vision-language models (VLMs) have transformed how we process, reason about, and summarize multimodal information. This paper surveys recent methods leveraging LLMs for crash detection from video data. We present a structured taxonomy of fusion strategies, summarize key datasets, analyze model architectures, compare performance benchmarks, and discuss ongoing challenges and opportunities. Our review provides a foundation for future research in this fast-growing intersection of video understanding and foundation models.         ",
    "url": "https://arxiv.org/abs/2507.02074",
    "authors": [
      "Sanjeda Akter",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.07867",
    "title": "Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders",
    "abstract": "           Neural audio codecs and autoencoders have emerged as versatile models for audio compression, transmission, feature-extraction, and latent-space generation. However, a key limitation is that most are trained to maximize reconstruction fidelity, often neglecting the specific latent structure necessary for optimal performance in diverse downstream applications. We propose a simple, post-hoc framework to address this by modifying the bottleneck of a pre-trained autoencoder. Our method introduces a \"Re-Bottleneck\", an inner bottleneck trained exclusively through latent space losses to instill user-defined structure. We demonstrate the framework's effectiveness in three experiments. First, we enforce an ordering on latent channels without sacrificing reconstruction quality. Second, we align latents with semantic embeddings, analyzing the impact on downstream diffusion modeling. Third, we introduce equivariance, ensuring that a filtering operation on the input waveform directly corresponds to a specific transformation in the latent space. Ultimately, our Re-Bottleneck framework offers a flexible and efficient way to tailor representations of neural audio models, enabling them to seamlessly meet the varied demands of different applications with minimal additional training.         ",
    "url": "https://arxiv.org/abs/2507.07867",
    "authors": [
      "Dimitrios Bralios",
      "Jonah Casebeer",
      "Paris Smaragdis"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2507.22291",
    "title": "AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data",
    "abstract": "           Unprecedented volumes of Earth observation data are continually collected around the world, but high-quality labels remain scarce given the effort required to make physical measurements and observations. This has led to considerable investment in bespoke modeling efforts translating sparse labels into maps. Here we introduce AlphaEarth Foundations, an embedding field model yielding a highly general, geospatial representation that assimilates spatial, temporal, and measurement contexts across multiple sources, enabling accurate and efficient production of maps and monitoring systems from local to global scales. The embeddings generated by AlphaEarth Foundations are the only to consistently outperform a suite of other well-known/widely accepted featurization approaches tested on a diverse set of mapping evaluations without re-training. We have released a dataset of global, annual, analysis-ready embedding field layers from 2017 through 2024.         ",
    "url": "https://arxiv.org/abs/2507.22291",
    "authors": [
      "Christopher F. Brown",
      "Michal R. Kazmierski",
      "Valerie J. Pasquarella",
      "William J. Rucklidge",
      "Masha Samsikova",
      "Chenhui Zhang",
      "Evan Shelhamer",
      "Estefania Lahera",
      "Olivia Wiles",
      "Simon Ilyushchenko",
      "Noel Gorelick",
      "Lihui Lydia Zhang",
      "Sophia Alj",
      "Emily Schechter",
      "Sean Askay",
      "Oliver Guinan",
      "Rebecca Moore",
      "Alexis Boukouvalas",
      "Pushmeet Kohli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.10542",
    "title": "GCRPNet: Graph-Enhanced Contextual and Regional Perception Network for Salient Object Detection in Optical Remote Sensing Images",
    "abstract": "           Salient object detection (SOD) in optical remote sensing images (ORSIs) faces numerous challenges, including significant variations in target scales and low contrast between targets and the background. Existing methods based on vision transformers (ViTs) and convolutional neural networks (CNNs) architectures aim to leverage both global and local features, but the difficulty in effectively integrating these heterogeneous features limits their overall performance. To overcome these limitations, we propose a graph-enhanced contextual and regional perception network (GCRPNet), which builds upon the Mamba architecture to simultaneously capture long-range dependencies and enhance regional feature representation. Specifically, we employ the visual state space (VSS) encoder to extract multi-scale features. To further achieve deep guidance and enhancement of these features, we first design a difference-similarity guided hierarchical graph attention module (DS-HGAM). This module strengthens cross-layer interaction capabilities between features of different scales while enhancing the model's structural perception,allowing it to distinguish between foreground and background more effectively. Then, we design the LEVSS block as the decoder of GCRPNet. This module integrates our proposed adaptive scanning strategy and multi-granularity collaborative attention enhancement module (MCAEM). It performs adaptive patch scanning on feature maps processed via multi-scale convolutions, thereby capturing rich local region information and enhancing Mamba's local modeling capability. Extensive experimental results demonstrate that the proposed model achieves state-of-the-art performance, validating its effectiveness and superiority.         ",
    "url": "https://arxiv.org/abs/2508.10542",
    "authors": [
      "Mengyu Ren",
      "Yutong Li",
      "Hua Li",
      "Runmin Cong",
      "Sam Kwong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.10587",
    "title": "Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer",
    "abstract": "           To bridge the temporal granularity gap in energy network design and operation based on Energy System Models, resampling of time series is required. While conventional upsampling methods are computationally efficient, they often result in significant information loss or increased noise. Advanced models such as time series generation models, Super-Resolution models and imputation models show potential, but also face fundamental challenges. The goal of time series generative models is to learn the distribution of the original data to generate high-resolution series with similar statistical characteristics. This is not entirely consistent with the definition of upsampling. Time series Super-Resolution models or imputation models can degrade the accuracy of upsampling because the input low-resolution time series are sparse and may have insufficient context. Moreover, such models usually rely on supervised learning paradigms. This presents a fundamental application paradox: their training requires the high-resolution time series that is intrinsically absent in upsampling application scenarios. To address the mentioned upsampling issue, this paper introduces a new method utilizing Generative Adversarial Transformers (GATs), which can be trained without access to any ground-truth high-resolution data. Compared with conventional interpolation methods, the introduced method can reduce the root mean square error (RMSE) of upsampling tasks by 9%, and the accuracy of a model predictive control (MPC) application scenario is improved by 13%.         ",
    "url": "https://arxiv.org/abs/2508.10587",
    "authors": [
      "Xuanhao Mu",
      "G\u00f6khan Demirel",
      "Yuzhe Zhang",
      "Jianlei Liu",
      "Thorsten Schlachter",
      "Veit Hagenmeyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2508.15792",
    "title": "Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym Distinction via Dual-Space Graph Transformers",
    "abstract": "           Antonym vs synonym distinction across multiple languages presents unique computational challenges due to the paradoxical nature of antonymous relationships words that share semantic domains while expressing opposite meanings. This work introduces Bhav-Net, a novel dual-space architecture that enables effective knowledge transfer from complex multilingual models to simpler, language-specific architectures while maintaining robust cross-lingual antonym--synonym distinction capabilities. Our approach combines language-specific BERT encoders with graph transformer networks, creating distinct semantic projections where synonymous pairs cluster in one space while antonymous pairs exhibit high similarity in a complementary space. Through comprehensive evaluation across eight languages (English, German, French, Spanish, Italian, Portuguese, Dutch, and Russian), we demonstrate that semantic relationship modeling transfers effectively across languages. The dual-encoder design achieves competitive performance against state-of-the-art baselines while providing interpretable semantic representations and effective cross-lingual generalization.         ",
    "url": "https://arxiv.org/abs/2508.15792",
    "authors": [
      "Samyak S. Sanghvi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.17111",
    "title": "Personalized Pricing Through Strategic User Profiling in Social Networks",
    "abstract": "           Traditional user profiling techniques rely on browsing history or purchase records to identify users' willingness to pay. This enables sellers to offer personalized prices to profiled users while charging only a uniform price to non-profiled users. However, the emergence of privacy-enhancing technologies has caused users to actively avoid on-site data tracking. Today, major online sellers have turned to public platforms such as online social networks to better track users' profiles from their product-related discussions. This paper presents the first analytical study on how users should best manage their social activities against potential personalized pricing, and how a seller should strategically adjust her pricing scheme to facilitate user profiling in social networks. We formulate a dynamic Bayesian game played between the seller and users under asymmetric information. The key challenge of analyzing this game comes from the double couplings between the seller and the users as well as among the users. Furthermore, the equilibrium analysis needs to ensure consistency between users' revealed information and the seller's belief under random user profiling. We address these challenges by alternately applying backward and forward induction, and successfully characterize the unique perfect Bayesian equilibrium (PBE) in closed form. Our analysis reveals that as the accuracy of profiling technology improves, the seller tends to raise the equilibrium uniform price to motivate users' increased social activities and facilitate user profiling. However, this results in most users being worse off after the informed consent policy is imposed to ensure users' awareness of data access and profiling practices by potential sellers. This finding suggests that recent regulatory evolution towards enhancing users' privacy awareness may have unintended consequences of reducing users' payoffs.         ",
    "url": "https://arxiv.org/abs/2508.17111",
    "authors": [
      "Qinqi Lin",
      "Lingjie Duan",
      "Jianwei Huang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.17450",
    "title": "Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD",
    "abstract": "           Large Language Models (LLMs) can struggle to balance gullibility to misinformation and resistance to valid corrections in persuasive dialogues, a critical challenge for reliable deployment. We introduce DuET-PD (Dual Evaluation for Trust in Persuasive Dialogues), a framework evaluating multi-turn stance-change dynamics across dual dimensions: persuasion type (corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via SALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves only 27.32% accuracy in MMLU-Pro under sustained misleading persuasions. Moreover, results reveal a concerning trend of increasing sycophancy in newer open-source models. To address this, we introduce Holistic DPO, a training approach balancing positive and negative persuasion examples. Unlike prompting or resist-only training, Holistic DPO enhances both robustness to misinformation and receptiveness to corrections, improving Llama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts from 4.21% to 76.54%. These contributions offer a pathway to developing more reliable and adaptable LLMs for multi-turn dialogue. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.17450",
    "authors": [
      "Bryan Chen Zhengyu Tan",
      "Daniel Wai Kit Chin",
      "Zhengyuan Liu",
      "Nancy F. Chen",
      "Roy Ka-Wei Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.17674",
    "title": "Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models",
    "abstract": "           We introduce Advertisement Embedding Attacks (AEA), a new class of LLM security threats that stealthily inject promotional or malicious content into model outputs and AI agents. AEA operate through two low-cost vectors: (1) hijacking third-party service-distribution platforms to prepend adversarial prompts, and (2) publishing back-doored open-source checkpoints fine-tuned with attacker data. Unlike conventional attacks that degrade accuracy, AEA subvert information integrity, causing models to return covert ads, propaganda, or hate speech while appearing normal. We detail the attack pipeline, map five stakeholder victim groups, and present an initial prompt-based self-inspection defense that mitigates these injections without additional model retraining. Our findings reveal an urgent, under-addressed gap in LLM security and call for coordinated detection, auditing, and policy responses from the AI-safety community.         ",
    "url": "https://arxiv.org/abs/2508.17674",
    "authors": [
      "Qiming Guo",
      "Jinwen Tang",
      "Xingran Huang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19742",
    "title": "POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection",
    "abstract": "           Line segment detection in images has been studied for several decades. Existing line segment detectors can be roughly divided into two categories: generic line segment detectors and wireframe line segment detectors. Generic line segment detectors aim to detect all meaningful line segments in images and traditional approaches usually fall into this category. Recent deep learning based approaches are mostly wireframe line segment detectors. They detect only line segments that are geometrically meaningful and have large spatial support. Due to the difference in the aim of design, the performance of generic line segment detectors for the task of wireframe line segment detection won't be satisfactory, and vice versa. In this work, we propose a robust framework that can be used for both generic line segment detection and wireframe line segment detection. The proposed method is an improved version of the Pixel Orientation Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments from edge strength maps, and can be combined with any edge detector. We show in our experiments that by combining the proposed POEv2 with an efficient edge detector, it achieves state-of-the-art performance on three publicly available datasets.         ",
    "url": "https://arxiv.org/abs/2508.19742",
    "authors": [
      "Chenguang Liu",
      "Chisheng Wang",
      "Yuhua Cai",
      "Chuanhua Zhu",
      "Qingquan Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19762",
    "title": "BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions",
    "abstract": "           Pollinator insects such as honeybees and bumblebees are vital to global food production and ecosystem stability, yet their populations are declining due to anthropogenic and environmental stressors. Scalable, automated monitoring in agricultural environments remains an open challenge due to the difficulty of detecting small, fast-moving, and often camouflaged insects. To address this, we present BuzzSet v1.0, a large-scale dataset of high-resolution pollinator images collected under real field conditions. BuzzSet contains 7,856 manually verified images with more than 8,000 annotated instances across three classes: honeybees, bumblebees, and unidentified insects. Initial annotations were produced using a YOLOv12 model trained on external data and refined through human verification with open-source tools. All images were preprocessed into 256 x 256 tiles to improve the detection of small insects. We provide baselines using the RF-DETR transformer-based object detector. The model achieves strong classification accuracy with F1 scores of 0.94 and 0.92 for honeybees and bumblebees, with minimal confusion between these categories. The unidentified class remains more difficult due to label ambiguity and fewer samples, yet still contributes insights for robustness evaluation. Overall detection performance (mAP at 0.50 of 0.559) illustrates the challenging nature of the dataset and its potential to drive advances in small object detection under realistic ecological conditions. Future work focuses on expanding the dataset to version 2.0 with additional annotations and evaluating further detection strategies. BuzzSet establishes a benchmark for ecological computer vision, with the primary challenge being reliable detection of insects frequently camouflaged within natural vegetation, highlighting an open problem for future research.         ",
    "url": "https://arxiv.org/abs/2508.19762",
    "authors": [
      "Ahmed Emam",
      "Mohamed Elbassiouny",
      "Julius Miller",
      "Patrick Donworth",
      "Sabine Seidel",
      "Ribana Roscher"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19990",
    "title": "Heterogeneous Self-Supervised Acoustic Pre-Training with Local Constraints",
    "abstract": "           Self-supervised pre-training using unlabeled data is widely used in automatic speech recognition. In this paper, we propose a new self-supervised pre-training approach to dealing with heterogeneous data. Instead of mixing all the data and minimizing the averaged global loss in the conventional way, we impose additional local constraints to ensure that the model optimizes each source of heterogeneous data to its local optimum after $K$-step gradient descent initialized from the model. We formulate this as a bilevel optimization problem, and use the first-order approximation method to solve the problem. We discuss its connection to model-agnostic meta learning. Experiments are carried out on self-supervised pre-training using multi-domain and multilingual datasets, demonstrating that the proposed approach can significantly improve the adaptivity of the self-supervised pre-trained model for the downstream supervised fine-tuning tasks.         ",
    "url": "https://arxiv.org/abs/2508.19990",
    "authors": [
      "Xiaodong Cui",
      "A F M Saif",
      "Brian Kingsbury",
      "Tianyi Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20392",
    "title": "Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection",
    "abstract": "           Spiking Neural Networks (SNNs), inspired by the brain, are characterized by minimal power consumption and swift inference capabilities on neuromorphic hardware, and have been widely applied to various visual perception tasks. Current ANN-SNN conversion methods have achieved excellent results in classification tasks with ultra-low time-steps, but their performance in visual detection tasks remains suboptimal. In this paper, we propose a delay-spike approach to mitigate the issue of residual membrane potential caused by heterogeneous spiking patterns. Furthermore, we propose a novel temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This enables Integrate-and-fire (IF) neurons to dynamically adjust their accumulation and firing behaviors based on the temporal order of time-steps. Our method enables spikes to exhibit distinct temporal properties, rather than relying solely on frequency-based representations. Moreover, the tdIF neuron maintains energy consumption on par with traditional IF neuron. We demonstrate that our method achieves more precise feature representation with lower time-steps, enabling high performance and ultra-low latency in visual detection tasks. In this study, we conduct extensive evaluation of the tdIF method across two critical vision tasks: object detection and lane line detection. The results demonstrate that the proposed method surpasses current ANN-SNN conversion approaches, achieving state-of-the-art performance with ultra-low latency (within 5 time-steps).         ",
    "url": "https://arxiv.org/abs/2508.20392",
    "authors": [
      "Chengjun Zhang",
      "Yuhao Zhang",
      "Jie Yang",
      "Mohamad Sawan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20670",
    "title": "\"Humor, Art, or Misinformation?\": A Multimodal Dataset for Intent-Aware Synthetic Image Detection",
    "abstract": "           Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 \"in the wild\" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to \"in the wild\" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures.         ",
    "url": "https://arxiv.org/abs/2508.20670",
    "authors": [
      "Anastasios Skoularikis",
      "Stefanos-Iordanis Papadopoulos",
      "Symeon Papadopoulos",
      "Panagiotis C. Petrantonakis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2509.00578",
    "title": "C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection",
    "abstract": "           Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents a formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using a separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains         ",
    "url": "https://arxiv.org/abs/2509.00578",
    "authors": [
      "Abdellah Zakaria Sellam",
      "Ilyes Benaissa",
      "Salah Eddine Bekhouche",
      "Abdenour Hadid",
      "Vito Ren\u00f3",
      "Cosimo Distante"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.00896",
    "title": "AI-Enhanced Intelligent NIDS Framework: Leveraging Metaheuristic Optimization for Robust Attack Detection and Prevention",
    "abstract": "           In todays rapidly evolving digital landscape, safeguarding network infrastructures against cyberattacks has become a critical priority. This research presents an innovative AI-driven real-time intrusion detection framework designed to enhance network security, particularly in Wireless Sensor Networks (WSNs), Cloud Computing (CC), and Internet of Things (IoT) environments. The system employs classical machine learning models, Logistic Regression, decision trees, and K-Nearest Neighbors, optimized through the novel Energy Valley Optimization (EVO) method using the NSL-KDD dataset. Feature selection significantly reduced the number of input features from 42 to 18, while maintaining strong detection capabilities. The proposed system achieved 98.95 percent. accuracy with Decision Tree, 98.47 percent with K-Nearest Neighbors, and 88.84 percent with Logistic Regression. Moreover, high precision, recall, and F1-scores were attained across all classifiers while substantially reducing training and testing times, making the framework highly suitable for real-time applications. To ensure fair detection across diverse attack types, dataset balancing via Downsampling was applied to address class imbalance challenges. This investigation focuses on the significance of advancing IDSs. in cloud computing and WSNs. Overall, this work advances secure communications by delivering a scalable, low-latency, and high-accuracy intrusion detection solution aligned with the latest trends in artificial intelligence, cybersecurity, and real-time digital networks.         ",
    "url": "https://arxiv.org/abs/2509.00896",
    "authors": [
      "Maryam Mahdi Alhusseini",
      "Mohammad Reza Feizi Derakhshi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.01293",
    "title": "Equivariant U-Shaped Neural Operators for the Cahn-Hilliard Phase-Field Model",
    "abstract": "           Phase separation in binary mixtures, governed by the Cahn-Hilliard equation, plays a central role in interfacial dynamics across materials science and soft matter. While numerical solvers are accurate, they are often computationally expensive and lack flexibility across varying initial conditions and geometries. Neural operators provide a data-driven alternative by learning solution operators between function spaces, but current architectures often fail to capture multiscale behavior and neglect underlying physical symmetries. Here we show that an equivariant U-shaped neural operator (E-UNO) can learn the evolution of the phase-field variable from short histories of past dynamics, achieving accurate predictions across space and time. The model combines global spectral convolution with a multi-resolution U-shaped architecture and regulates translation equivariance to align with the underlying physics. E-UNO outperforms standard Fourier neural operator and U-shaped neural operator baselines, particularly on fine-scale and high-frequency structures. By encoding symmetry and scale hierarchy, the model generalizes better, requires less training data, and yields physically consistent dynamics. This establishes E-UNO as an efficient surrogate for complex phase-field systems.         ",
    "url": "https://arxiv.org/abs/2509.01293",
    "authors": [
      "Xiao Xue",
      "Marco F.P. ten Eikelder",
      "Tianyue Yang",
      "Yiqing Li",
      "Kan He",
      "Shuo Wang",
      "Peter V. Coveney"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2509.01535",
    "title": "CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models",
    "abstract": "           Large Language Models (LLMs) have achieved remarkable success across various domains. However, a fundamental question remains: Can LLMs effectively utilize causal knowledge for prediction and generation? Through empirical studies, we find that LLMs trained directly on large-scale data often capture spurious correlations rather than true causal relationships, leading to suboptimal performance, especially in out-of-distribution (OOD) scenarios. To address this challenge, we propose Causal Attention Tuning (CAT), a novel approach that injects fine-grained causal knowledge into the attention mechanism. We propose an automated pipeline that leverages human priors to automatically generate token-level causal signals and introduce the Re-Attention mechanism to guide training, helping the model focus on causal structures while mitigating noise and biases in attention scores. Experimental results on our proposed Spurious Token Game (STG) benchmark and multiple downstream tasks demonstrate that our approach effectively leverages causal knowledge for prediction and remains robust in OOD scenarios. The CAT achieves an average improvement of 5.76% on the STG dataset and 1.56% on downstream tasks. Notably, the OOD performance of the Llama-3.1-8B model on STG_M increased from 64.5% to 90.5%, and Qwen's OOD performance on the STG_H dataset improved from 25.4% to 55.9%. Implementation details can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.01535",
    "authors": [
      "Kairong Han",
      "Wenshuo Zhao",
      "Ziyu Zhao",
      "JunJian Ye",
      "Lujia Pan",
      "Kun Kuang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.02091",
    "title": "CLINN: Conservation Law Informed Neural Network for Approximating Discontinuous Solutions",
    "abstract": "           Physics-informed Neural Network (PINN) faces significant challenges when approximating solutions to conservation laws, particularly in ensuring conservation and accurately resolving discontinuities. To address these limitations, we propose Conservation Law-informed Neural Network (CLINN), a novel framework that incorporates the boundedness constraint, implicit solution form, and Rankine-Hugoniot condition of scalar conservation laws into the loss function, thereby enforcing exact conservation properties. Furthermore, we integrate a residual-based adaptive refinement (RAR) strategy to dynamically prioritize training near discontinuities, substantially improving the network's ability to capture sharp gradients. Numerical experiments are conducted on benchmark problems, including the inviscid Burgers equation, the Lighthill-Whitham-Richards (LWR) traffic flow model, and the Buckley-Leverett problem. Results demonstrate that CLINN achieves superior accuracy in resolving solution profiles and discontinuity locations while reducing numeral oscillations. Compared to conventional PINN, CLINN yields a maximum reduction of 99.2% in mean squared error (MSE).         ",
    "url": "https://arxiv.org/abs/2509.02091",
    "authors": [
      "Weiheng Zeng",
      "Kun Wang",
      "Ruoxi Lu",
      "Tiegang Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2509.05429",
    "title": "Safeguarding Graph Neural Networks against Topology Inference Attacks",
    "abstract": "           Graph Neural Networks (GNNs) have emerged as powerful models for learning from graph-structured data. However, their widespread adoption has raised serious privacy concerns. While prior research has primarily focused on edge-level privacy, a critical yet underexplored threat lies in topology privacy - the confidentiality of the graph's overall structure. In this work, we present a comprehensive study on topology privacy risks in GNNs, revealing their vulnerability to graph-level inference attacks. To this end, we propose a suite of Topology Inference Attacks (TIAs) that can reconstruct the structure of a target training graph using only black-box access to a GNN model. Our findings show that GNNs are highly susceptible to these attacks, and that existing edge-level differential privacy mechanisms are insufficient as they either fail to mitigate the risk or severely compromise model accuracy. To address this challenge, we introduce Private Graph Reconstruction (PGR), a novel defense framework designed to protect topology privacy while maintaining model accuracy. PGR is formulated as a bi-level optimization problem, where a synthetic training graph is iteratively generated using meta-gradients, and the GNN model is concurrently updated based on the evolving graph. Extensive experiments demonstrate that PGR significantly reduces topology leakage with minimal impact on model accuracy. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.05429",
    "authors": [
      "Jie Fu",
      "Hong Yuan",
      "Zhili Chen",
      "Wendy Hui Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.05657",
    "title": "LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding",
    "abstract": "           Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.05657",
    "authors": [
      "Yuxuan Hu",
      "Jihao Liu",
      "Ke Wang",
      "Jinliang Zhen",
      "Weikang Shi",
      "Manyuan Zhang",
      "Qi Dou",
      "Rui Liu",
      "Aojun Zhou",
      "Hongsheng Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.05685",
    "title": "MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions",
    "abstract": "           Transforming road network data into vector representations using deep learning has proven effective for road network analysis. However, urban road networks' heterogeneous and hierarchical nature poses challenges for accurate representation learning. Graph neural networks, which aggregate features from neighboring nodes, often struggle due to their homogeneity assumption and focus on a single structural scale. To address these issues, this paper presents MSRFormer, a novel road network representation learning framework that integrates multi-scale spatial interactions by addressing their flow heterogeneity and long-distance dependencies. It uses spatial flow convolution to extract small-scale features from large trajectory datasets, and identifies scale-dependent spatial interaction regions to capture the spatial structure of road networks and flow heterogeneity. By employing a graph transformer, MSRFormer effectively captures complex spatial dependencies across multiple scales. The spatial interaction features are fused using residual connections, which are fed to a contrastive learning algorithm to derive the final road network representation. Validation on two real-world datasets demonstrates that MSRFormer outperforms baseline methods in two road network analysis tasks. The performance gains of MSRFormer suggest the traffic-related task benefits more from incorporating trajectory data, also resulting in greater improvements in complex road network structures with up to 16% improvements compared to the most competitive baseline method. This research provides a practical framework for developing task-agnostic road network representation models and highlights distinct association patterns of the interplay between scale effects and flow heterogeneity of spatial interactions.         ",
    "url": "https://arxiv.org/abs/2509.05685",
    "authors": [
      "Jian Yang",
      "Jiahui Wu",
      "Li Fang",
      "Hongchao Fan",
      "Bianying Zhang",
      "Huijie Zhao",
      "Guangyi Yang",
      "Rui Xin",
      "Xiong You"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.06011",
    "title": "Light-Weight Cross-Modal Enhancement Method with Benchmark Construction for UAV-based Open-Vocabulary Object Detection",
    "abstract": "           Open-Vocabulary Object Detection (OVD) faces severe performance degradation when applied to UAV imagery due to the domain gap from ground-level datasets. To address this challenge, we propose a complete UAV-oriented solution that combines both dataset construction and model innovation. First, we design a refined UAV-Label Engine, which efficiently resolves annotation redundancy, inconsistency, and ambiguity, enabling the generation of largescale UAV datasets. Based on this engine, we construct two new benchmarks: UAVDE-2M, with over 2.4M instances across 1,800+ categories, and UAVCAP-15K, providing rich image-text pairs for vision-language pretraining. Second, we introduce the Cross-Attention Gated Enhancement (CAGE) module, a lightweight dual-path fusion design that integrates cross-attention, adaptive gating, and global FiLM modulation for robust textvision alignment. By embedding CAGE into the YOLO-World-v2 framework, our method achieves significant gains in both accuracy and efficiency, notably improving zero-shot detection on VisDrone by +5.3 mAP while reducing parameters and GFLOPs, and demonstrating strong cross-domain generalization on SIMD. Extensive experiments and real-world UAV deployment confirm the effectiveness and practicality of our proposed solution for UAV-based OVD         ",
    "url": "https://arxiv.org/abs/2509.06011",
    "authors": [
      "Zhenhai Weng",
      "Xinjie Li",
      "Can Wu",
      "Weijie He",
      "Jianfeng Lv",
      "Dong Zhou",
      "Zhongliang Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.06465",
    "title": "CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction",
    "abstract": "           Antibody binding site prediction plays a pivotal role in computational immunology and therapeutic antibody design. Existing sequence or structure methods rely on single-view features and fail to identify antibody-specific binding sites on the antigens. In this paper, we propose \\textbf{CAME-AB}, a novel Cross-modality Attention framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding site prediction. CAME-AB integrates five biologically grounded modalities, including raw amino acid encodings, BLOSUM substitution profiles, pretrained language model embeddings, structure-aware features, and GCN-refined biochemical graphs, into a unified multimodal representation. To enhance adaptive cross-modal reasoning, we propose an \\emph{adaptive modality fusion} module that learns to dynamically weight each modality based on its global relevance and input-specific contribution. A Transformer encoder combined with an MoE module further promotes feature specialization and capacity expansion. We additionally incorporate a supervised contrastive learning objective to explicitly shape the latent space geometry, encouraging intra-class compactness and inter-class separability. To improve optimization stability and generalization, we apply stochastic weight averaging during training. Extensive experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB consistently outperforms strong baselines on multiple metrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further validate the effectiveness of each architectural component and the benefit of multimodal feature integration. The model implementation details and the codes are available on this https URL ",
    "url": "https://arxiv.org/abs/2509.06465",
    "authors": [
      "Hongzong Li",
      "Jiahao Ma",
      "Zhanpeng Shi",
      "Rui Xiao",
      "Fanming Jin",
      "Ye-Fan Hu",
      "Jian-Dong Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2509.06570",
    "title": "Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition",
    "abstract": "           Existing open set recognition (OSR) methods are typically designed for static scenarios, where models aim to classify known classes and identify unknown ones within fixed scopes. This deviates from the expectation that the model should incrementally identify newly emerging unknown classes from continuous data streams and acquire corresponding knowledge. In such evolving scenarios, the discriminability of OSR decision boundaries is hard to maintain due to restricted access to former training data, causing severe inter-class confusion. To solve this problem, we propose retentive angular representation learning (RARL) for incremental open set recognition (IOSR). In RARL, unknown representations are encouraged to align around inactive prototypes within an angular space constructed under the equiangular tight frame, thereby mitigating excessive representation drift during knowledge updates. Specifically, we adopt a virtual-intrinsic interactive (VII) training strategy, which compacts known representations by enforcing clear inter-class margins through boundary-proximal virtual classes. Furthermore, a stratified rectification strategy is designed to refine decision boundaries, mitigating representation bias and feature space distortion caused by imbalances between old/new and positive/negative class samples. We conduct thorough evaluations on CIFAR100 and TinyImageNet datasets and establish a new benchmark for IOSR. Experimental results across various task setups demonstrate that the proposed method achieves state-of-the-art performance.         ",
    "url": "https://arxiv.org/abs/2509.06570",
    "authors": [
      "Runqing Yang",
      "Yimin Fu",
      "Changyuan Wu",
      "Zhunga Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.06591",
    "title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising",
    "abstract": "           Low-dose computed tomography (LDCT) and positron emission tomography (PET) have emerged as safer alternatives to conventional imaging modalities by significantly reducing radiation exposure. However, this reduction often results in increased noise and artifacts, which can compromise diagnostic accuracy. Consequently, denoising for LDCT/PET has become a vital area of research aimed at enhancing image quality while maintaining radiation safety. In this study, we introduce a novel Hybrid Swin Attention Network (HSANet), which incorporates Efficient Global Attention (EGA) modules and a hybrid upsampling module. The EGA modules enhance both spatial and channel-wise interaction, improving the network's capacity to capture relevant features, while the hybrid upsampling module mitigates the risk of overfitting to noise. We validate the proposed approach using a publicly available LDCT/PET dataset. Experimental results demonstrate that HSANet achieves superior denoising performance compared to existing methods, while maintaining a lightweight model size suitable for deployment on GPUs with standard memory configurations. This makes our approach highly practical for real-world clinical applications.         ",
    "url": "https://arxiv.org/abs/2509.06591",
    "authors": [
      "Yichao Liu",
      "Hengzhi Xue",
      "YueYang Teng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.06694",
    "title": "Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation",
    "abstract": "           While it is well-established that artificial neural networks are universal approximators for continuous functions on compact domains, many modern approaches rely on deep or overparameterized architectures that incur high computational costs. In this paper, a new type of small shallow neural network, called the Barycentric Neural Network (BNN), is proposed, which leverages a fixed set of base points and their barycentric coordinates to define both its structure and its parameters. We demonstrate that our BNN enables the exact representation of continuous piecewise linear functions (CPLFs), ensuring strict continuity across segments. Since any continuous function over a compact domain can be approximated arbitrarily well by CPLFs, the BNN naturally emerges as a flexible and interpretable tool for function approximation. Beyond the use of this representation, the main contribution of the paper is the introduction of a new variant of persistent entropy, a topological feature that is stable and scale invariant, called the length-weighted persistent entropy (LWPE), which is weighted by the lifetime of topological features. Our framework, which combines the BNN with a loss function based on our LWPE, aims to provide flexible and geometrically interpretable approximations of nonlinear continuous functions in resource-constrained settings, such as those with limited base points for BNN design and few training epochs. Instead of optimizing internal weights, our approach directly optimizes the base points that define the BNN. Experimental results show that our approach achieves superior and faster approximation performance compared to classical loss functions such as MSE, RMSE, MAE, and log-cosh.         ",
    "url": "https://arxiv.org/abs/2509.06694",
    "authors": [
      "Victor Toscano-Duran",
      "Rocio Gonzalez-Diaz",
      "Miguel A. Guti\u00e9rrez-Naranjo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2001.03346",
    "title": "Time-Varying Graph Learning with Constraints on Graph Temporal Variation",
    "abstract": "           We propose a novel framework for learning time-varying graphs from spatiotemporal measurements. Given an appropriate prior on the temporal behavior of signals, our proposed method can estimate time-varying graphs from a small number of available measurements. To achieve this, we introduce two regularization terms in convex optimization problems that constrain sparseness of temporal variations of the time-varying networks. Moreover, a computationally-scalable algorithm is introduced to efficiently solve the optimization problem. The experimental results with synthetic and real datasets (point cloud and temperature data) demonstrate our proposed method outperforms the existing state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2001.03346",
    "authors": [
      "Haruki Yokota",
      "Koki Yamada",
      "Yuichi Tanaka",
      "Antonio Ortega"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2401.08550",
    "title": "Expanding Hardware-Efficiently Manipulable Hilbert Space via Hamiltonian Embedding",
    "abstract": "           Many promising quantum applications depend on the efficient quantum simulation of an exponentially large sparse Hamiltonian, a task known as sparse Hamiltonian simulation, which is fundamentally important in quantum computation. Although several theoretically appealing quantum algorithms have been proposed for this task, they typically require a black-box query model of the sparse Hamiltonian, rendering them impractical for near-term implementation on quantum devices. In this paper, we propose a technique named Hamiltonian embedding. This technique simulates a desired sparse Hamiltonian by embedding it into the evolution of a larger and more structured quantum system, allowing for more efficient simulation through hardware-efficient operations. We conduct a systematic study of this new technique and demonstrate significant savings in computational resources for implementing prominent quantum applications. As a result, we can now experimentally realize quantum walks on complicated graphs (e.g., binary trees, glued-tree graphs), quantum spatial search, and the simulation of real-space Schr\u00f6dinger equations on current trapped-ion and neutral-atom platforms. Given the fundamental role of Hamiltonian evolution in the design of quantum algorithms, our technique markedly expands the horizon of implementable quantum advantages in the NISQ era.         ",
    "url": "https://arxiv.org/abs/2401.08550",
    "authors": [
      "Jiaqi Leng",
      "Joseph Li",
      "Yuxiang Peng",
      "Xiaodi Wu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2412.13993",
    "title": "Improved Physics-informed neural networks loss function regularization with a variance-based term",
    "abstract": "           In machine learning and statistical modeling, the mean square or absolute error is commonly used as an error metric, also called a \"loss function.\" While effective in reducing the average error, this approach may fail to address localized outliers, leading to significant inaccuracies in regions with sharp gradients or discontinuities. This issue is particularly evident in physics-informed neural networks (PINNs), where such localized errors are expected and affect the overall solution. To overcome this limitation, we propose a novel loss function that combines the mean and the standard deviation of the chosen error metric. By minimizing this combined loss function, the method ensures a more uniform error distribution and reduces the impact of localized high-error regions. The proposed loss function is easy to implement and tested on problems of varying complexity: the 1D Poisson equation, the unsteady Burgers' equation, 2D linear elastic solid mechanics, and 2D steady Navier-Stokes equations. Results demonstrate improved solution quality and lower maximum error compared to the standard mean-based loss, with minimal impact on computational time.         ",
    "url": "https://arxiv.org/abs/2412.13993",
    "authors": [
      "John M. Hanna",
      "Hugues Talbot",
      "Irene E. Vignon-Clementel"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.07205",
    "title": "VINP: Variational Bayesian Inference with Neural Speech Prior for Joint ASR-Effective Speech Dereverberation and Blind RIR Identification",
    "abstract": "           Reverberant speech, denoting the speech signal degraded by reverberation, contains crucial knowledge of both anechoic source speech and room impulse response (RIR). This work proposes a variational Bayesian inference (VBI) framework with neural speech prior (VINP) for joint speech dereverberation and blind RIR identification. In VINP, a probabilistic signal model is constructed in the time-frequency (T-F) domain based on convolution transfer function (CTF) approximation. For the first time, we propose using an arbitrary discriminative dereverberation deep neural network (DNN) to estimate the prior distribution of anechoic speech within a probabilistic model. By integrating both reverberant speech and the anechoic speech prior, VINP yields the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the anechoic speech spectrum and CTF filter, respectively. After simple transformations, the waveforms of anechoic speech and RIR are estimated. VINP is effective for automatic speech recognition (ASR) systems, which sets it apart from most deep learning (DL)-based single-channel dereverberation approaches. Experiments on single-channel speech dereverberation demonstrate that VINP attains state-of-the-art (SOTA) performance in mean opinion score (MOS) and word error rate (WER). For blind RIR identification, experiments demonstrate that VINP achieves SOTA performance in estimating reverberation time at 60 dB (RT60) and advanced performance in direct-to-reverberation ratio (DRR) estimation. Codes and audio samples are available online.         ",
    "url": "https://arxiv.org/abs/2502.07205",
    "authors": [
      "Pengyu Wang",
      "Ying Fang",
      "Xiaofei Li"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.14779",
    "title": "Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution",
    "abstract": "           Single-image super-resolution (SISR) is a fundamental problem in computer vision that aims to reconstruct high-resolution (HR) images from low-resolution (LR) inputs. Although convolutional neural networks (CNNs) have achieved substantial advancements, deeper architectures often introduce excessive parameters, higher memory usage, and computational cost, limiting their applicability on resource-constrained devices. Recent research has thus focused on lightweight architectures that preserve accuracy while reducing complexity. This paper presents the Involution and BSConv Multi-Depth Distillation Network (IBMDN), a lightweight and effective architecture for SISR. The proposed IBMDN comprises Involution and BSConv Multi-Depth Distillation Blocks (IBMDB) and a Contrast and High-Frequency Attention Block (CHFAB). IBMDB employs varying combinations of Involution and BSConv at multiple depths to perform efficient feature extraction while minimizing computational complexity. CHFAB, a lightweight self-attention mechanism, focuses on extracting high-frequency and contrast information to enhance perceptual quality in the reconstructed images. The flexible design of IBMDB enables it to be seamlessly integrated into diverse SISR frameworks, including information distillation, transformer-based, and GAN-based models. Extensive experiments demonstrate that incorporating IBMDB significantly reduces memory usage, parameters, and floating-point operations (FLOPs), while achieving improvements in both pixel-wise accuracy and visual quality. The source code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2503.14779",
    "authors": [
      "Akram Khatami-Rizi",
      "Ahmad Mahmoudi-Aznaveh"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.11089",
    "title": "Inexact Column Generation for Bayesian Network Structure Learning via Difference-of-Submodular Optimization",
    "abstract": "           In this paper, we consider a score-based Integer Programming (IP) approach for solving the Bayesian Network Structure Learning (BNSL) problem. State-of-the-art BNSL IP formulations suffer from the exponentially large number of variables and constraints. A standard approach in IP to address such challenges is to employ row and column generation techniques, which dynamically generate rows and columns, while the complex pricing problem remains a computational bottleneck for BNSL. For the general class of $\\ell_0$-penalized likelihood scores, we show how the pricing problem can be reformulated as a difference of submodular optimization problem, and how the Difference of Convex Algorithm (DCA) can be applied as an inexact method to efficiently solve the pricing problems. Empirically, we show that, for continuous Gaussian data, our row and column generation approach yields solutions with higher quality than state-of-the-art score-based approaches, especially when the graph density increases, and achieves comparable performance against benchmark constraint-based and hybrid approaches, even when the graph size increases.         ",
    "url": "https://arxiv.org/abs/2505.11089",
    "authors": [
      "Yiran Yang",
      "Rui Chen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2508.21035",
    "title": "A multi-task neural network for atypical mitosis recognition under domain shift",
    "abstract": "           Recognizing atypical mitotic figures in histopathology images allows physicians to correctly assess tumor aggressiveness. Although machine learning models could be exploited for automatically performing such a task, under domain shift these models suffer from significative performance drops. In this work, an approach based on multi-task learning is proposed for addressing this problem. By exploiting auxiliary tasks, correlated to the main classification task, the proposed approach, submitted to the track 2 of the MItosis DOmain Generalization (MIDOG) challenge, aims to aid the model to focus only on the object to classify, ignoring the domain varying background of the image. The proposed approach shows promising performance in a preliminary evaluation conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25 challenge.         ",
    "url": "https://arxiv.org/abs/2508.21035",
    "authors": [
      "Gennaro Percannella",
      "Mattia Sarno",
      "Francesco Tortorella",
      "Mario Vento"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.05793",
    "title": "Spectral Methods in Complex Systems",
    "abstract": "           These notes offer a unified introduction to spectral methods for the study of complex systems. They are intended as an operative manual rather than a theorem-proof textbook: the emphasis is on tools, identities, and perspectives that can be readily applied across disciplines. Beginning with a compendium of matrix identities and inversion techniques, the text develops the connections between spectra, dynamics, and structure in finite-dimensional systems. Applications range from dynamical stability and random walks on networks to input-output economics, PageRank, epidemic spreading, memristive circuits, synchronization phenomena, and financial stability. Throughout, the guiding principle is that eigenvalues, eigenvectors, and resolvent operators provide a common language linking problems in physics, mathematics, computer science, and beyond. The presentation is informal, accessible to advanced undergraduates, yet broad enough to serve as a reference for researchers interested in spectral approaches to complex systems.         ",
    "url": "https://arxiv.org/abs/2509.05793",
    "authors": [
      "Francesco Caravelli"
    ],
    "subjectives": [
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "Mathematical Physics (math-ph)"
    ]
  },
  {
    "id": "arXiv:2509.06756",
    "title": "Enhancing Fault-Tolerant Surface Code Decoding with Iterative Lattice Reweighting",
    "abstract": "           Efficient and realistic error decoding is crucial for fault-tolerant quantum computation (FTQC) on near-term devices. While decoding is a classical post-processing task, its effectiveness depends on accurately modeling quantum noise, which is hardware-dependent. In particular, correlated bit-flip ($X$) and phase-flip ($Z$) errors often arise under circuit-level noise. We introduce the Iterative Reweighting Minimum-Weight Perfect Matching (IRMWPM) decoder, which systematically incorporates such correlations to enhance quantum error correction. Our method leverages fault-detection patterns to guide reweighting: correlated $X$ and $Z$ detection events are identified, and their conditional probabilities update weights on the primal and dual lattices. This iterative procedure improves handling of realistic error propagation in a hardware-agnostic yet noise-aware manner. We prove that IRMWPM converges in finite time while preserving the distance guarantee of MWPM. Numerical results under circuit-level noise show substantial improvements. For distances $\\geq 17$ and physical error rates $\\leq 0.001$, IRMWPM reduces logical error rates by over 20x with only a few iterations. It also raises the accuracy threshold from 1% to 1.16%, making it practical for near-term real-time decoding. Extrapolated estimates suggest that to reach logical error rate $10^{-16}$, IRMWPM requires distance $d=31$, while standard MWPM needs $d=50$, implying a major reduction in qubit overhead.         ",
    "url": "https://arxiv.org/abs/2509.06756",
    "authors": [
      "Yi Tian",
      "Y. Zheng",
      "Xiaoting Wang",
      "Ching-Yi Lai"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ]
  }
]