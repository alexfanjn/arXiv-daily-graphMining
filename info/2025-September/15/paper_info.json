[
  {
    "id": "arXiv:2509.09684",
    "title": "Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation",
    "abstract": "           This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English) benchmark dataset designed for the text-to-SQL task in the process mining domain. Text-to-SQL conversion facilitates natural language querying of databases, increasing accessibility for users without SQL expertise and productivity for those that are experts. The text-2-SQL-4-PM dataset is customized to address the unique challenges of process mining, including specialized vocabularies and single-table relational structures derived from event logs. The dataset comprises 1,655 natural language utterances, including human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods include manual curation by experts, professional translations, and a detailed annotation process to enable nuanced analyses of task complexity. Additionally, a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility of the dataset for text-to-SQL applications. The results show that text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering broader applicability for semantic parsing and other natural language processing tasks.         ",
    "url": "https://arxiv.org/abs/2509.09684",
    "authors": [
      "Bruno Yui Yamate",
      "Thais Rodrigues Neubauer",
      "Marcelo Fantinato",
      "Sarajane Marques Peres"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2509.09691",
    "title": "Wave-Based Semantic Memory with Resonance-Based Retrieval: A Phase-Aware Alternative to Vector Embedding Stores",
    "abstract": "           Conventional vector-based memory systems rely on cosine or inner product similarity within real-valued embedding spaces. While computationally efficient, such approaches are inherently phase-insensitive and limited in their ability to capture resonance phenomena crucial for meaning representation. We propose Wave-Based Semantic Memory, a novel framework that models knowledge as wave patterns $\\psi(x) = A(x) e^{i\\phi(x)}$ and retrieves it through resonance-based interference. This approach preserves both amplitude and phase information, enabling more expressive and robust semantic similarity. We demonstrate that resonance-based retrieval achieves higher discriminative power in cases where vector methods fail, including phase shifts, negations, and compositional queries. Our implementation, ResonanceDB, shows scalability to millions of patterns with millisecond latency, positioning wave-based memory as a viable alternative to vector stores for AGI-oriented reasoning and knowledge representation.         ",
    "url": "https://arxiv.org/abs/2509.09691",
    "authors": [
      "Aleksandr Listopad"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2509.09699",
    "title": "Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs",
    "abstract": "           Mapping clinical documents to standardised clinical vocabularies is an important task, as it provides structured data for information retrieval and analysis, which is essential to clinical research, hospital administration and improving patient care. However, manual coding is both difficult and time-consuming, making it impractical at scale. Automated coding can potentially alleviate this burden, improving the availability and accuracy of structured clinical data. The task is difficult to automate, as it requires mapping to high-dimensional and long-tailed target spaces, such as the International Classification of Diseases (ICD). While external knowledge sources have been readily utilised to enhance output code representation, the use of external resources for representing the input documents has been underexplored. In this work, we compute a structured representation of the input documents, making use of document-level knowledge graphs (KGs) that provide a comprehensive structured view of a patient's condition. The resulting knowledge graph efficiently represents the patient-centred input documents with 23\\% of the original text while retaining 90\\% of the information. We assess the effectiveness of this graph for automated ICD-9 coding by integrating it into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments yield improved Macro-F1 scores by up to 3.20\\% on popular benchmarks, while improving training efficiency. We attribute this improvement to different types of entities and relationships in the KG, and demonstrate the improved explainability potential of the approach over the text-only baseline.         ",
    "url": "https://arxiv.org/abs/2509.09699",
    "authors": [
      "Mingyang Li",
      "Viktor Schlegel",
      "Tingting Mu",
      "Warren Del-Pinto",
      "Goran Nenadic"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.09700",
    "title": "Cross-Layer Attention Probing for Fine-Grained Hallucination Detection",
    "abstract": "           With the large-scale adoption of Large Language Models (LLMs) in various applications, there is a growing reliability concern due to their tendency to generate inaccurate text, i.e. hallucinations. In this work, we propose Cross-Layer Attention Probing (CLAP), a novel activation probing technique for hallucination detection, which processes the LLM activations across the entire residual stream as a joint sequence. Our empirical evaluations using five LLMs and three tasks show that CLAP improves hallucination detection compared to baselines on both greedy decoded responses as well as responses sampled at higher temperatures, thus enabling fine-grained detection, i.e. the ability to disambiguate hallucinations and non-hallucinations among different sampled responses to a given prompt. This allows us to propose a detect-then-mitigate strategy using CLAP to reduce hallucinations and improve LLM reliability compared to direct mitigation approaches. Finally, we show that CLAP maintains high reliability even when applied out-of-distribution.         ",
    "url": "https://arxiv.org/abs/2509.09700",
    "authors": [
      "Malavika Suresh",
      "Rahaf Aljundi",
      "Ikechukwu Nkisi-Orji",
      "Nirmalie Wiratunga"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.09703",
    "title": "CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor",
    "abstract": "           The widespread deployment of large language models (LLMs) has intensified concerns around intellectual property (IP) protection, as model theft and unauthorized redistribution become increasingly feasible. To address this, model fingerprinting aims to embed verifiable ownership traces into LLMs. However, existing methods face inherent trade-offs between stealthness, robustness, and generalizability, being either detectable via distributional shifts, vulnerable to adversarial modifications, or easily invalidated once the fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven fingerprinting framework that encodes contextual correlations across multiple dialogue turns, such as counterfactual, rather than relying on token-level or single-turn triggers. CTCC enables fingerprint verification under black-box access while mitigating false positives and fingerprint leakage, supporting continuous construction under a shared semantic rule even if partial triggers are exposed. Extensive experiments across multiple LLM architectures demonstrate that CTCC consistently achieves stronger stealth and robustness than prior work. Our findings position CTCC as a reliable and practical solution for ownership verification in real-world LLM deployment scenarios. Our code and data are publicly available at <this https URL.         ",
    "url": "https://arxiv.org/abs/2509.09703",
    "authors": [
      "Zhenhua Xu",
      "Xixiang Zhao",
      "Xubin Yue",
      "Shengwei Tian",
      "Changting Lin",
      "Meng Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.09706",
    "title": "Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks",
    "abstract": "           This study evaluates the resilience of large language models (LLMs) against adversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base. Using systematically designed adversarial tests through TextFooler and BERTAttack, we found significant variations in model robustness. RoBERTa-Base and FlanT5 demonstrated remarkable resilience, maintaining accuracy even when subjected to sophisticated attacks, with attack success rates of 0%. In contrast. BERT-Base showed considerable vulnerability, with TextFooler achieving a 93.75% success rate in reducing model accuracy from 48% to just 3%. Our research reveals that while certain LLMs have developed effective defensive mechanisms, these safeguards often require substantial computational resources. This study contributes to the understanding of LLM security by identifying existing strengths and weaknesses in current safeguarding approaches and proposes practical recommendations for developing more efficient and effective defensive strategies.         ",
    "url": "https://arxiv.org/abs/2509.09706",
    "authors": [
      "Taniya Gidatkar",
      "Oluwaseun Ajao",
      "Matthew Shardlow"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.09723",
    "title": "ALIGNS: Unlocking nomological networks in psychological measurement through a large language model",
    "abstract": "           Psychological measurement is critical to many disciplines. Despite advances in measurement, building nomological networks, theoretical maps of how concepts and measures relate to establish validity, remains a challenge 70 years after Cronbach and Meehl proposed them as fundamental to validation. This limitation has practical consequences: clinical trials may fail to detect treatment effects, and public policy may target the wrong outcomes. We introduce Analysis of Latent Indicators to Generate Nomological Structures (ALIGNS), a large language model-based system trained with validated questionnaire measures. ALIGNS provides three comprehensive nomological networks containing over 550,000 indicators across psychology, medicine, social policy, and other fields. This represents the first application of large language models to solve a foundational problem in measurement validation. We report classification accuracy tests used to develop the model, as well as three evaluations. In the first evaluation, the widely used NIH PROMIS anxiety and depression instruments are shown to converge into a single dimension of emotional distress. The second evaluation examines child temperament measures and identifies four potential dimensions not captured by current frameworks, and questions one existing dimension. The third evaluation, an applicability check, engages expert psychometricians who assess the system's importance, accessibility, and suitability. ALIGNS is freely available at this http URL, complementing traditional validation methods with large-scale nomological analysis.         ",
    "url": "https://arxiv.org/abs/2509.09723",
    "authors": [
      "Kai R. Larsen",
      "Sen Yan",
      "Roland M\u00fcller",
      "Lan Sang",
      "Mikko R\u00f6nkk\u00f6",
      "Ravi Starzl",
      "Donald Edmondson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2509.09744",
    "title": "Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis",
    "abstract": "           The limited availability of labeled brain network data makes it challenging to achieve accurate and interpretable psychiatric diagnoses. While self-supervised learning (SSL) offers a promising solution, existing methods often rely on augmentation strategies that can disrupt crucial structural semantics in brain graphs. To address this, we propose SAM-BG, a two-stage framework for learning brain graph representations with structural semantic preservation. In the pre-training stage, an edge masker is trained on a small labeled subset to capture key structural semantics. In the SSL stage, the extracted structural priors guide a structure-aware augmentation process, enabling the model to learn more semantically meaningful and robust representations. Experiments on two real-world psychiatric datasets demonstrate that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled data settings, and uncovers clinically relevant connectivity patterns that enhance interpretability. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.09744",
    "authors": [
      "Mujie Liu",
      "Chenze Wang",
      "Liping Chen",
      "Nguyen Linh Dan Le",
      "Niharika Tewari",
      "Ting Dang",
      "Jiangang Ma",
      "Feng Xia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.09750",
    "title": "A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images",
    "abstract": "           This study proposes a semi-supervised co-training framework for object detection in densely packed retail environments, where limited labeled data and complex conditions pose major challenges. The framework combines Faster R-CNN (utilizing a ResNet backbone) for precise localization with YOLO (employing a Darknet backbone) for global context, enabling mutual pseudo-label exchange that improves accuracy in scenes with occlusion and overlapping objects. To strengthen classification, it employs an ensemble of XGBoost, Random Forest, and SVM, utilizing diverse feature representations for higher robustness. Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing precision and efficiency across models. By minimizing reliance on manual labeling, the approach reduces annotation costs and adapts effectively to frequent product and layout changes common in retail. Experiments on the SKU-110k dataset demonstrate strong performance, highlighting the scalability and practicality of the proposed framework for real-world retail applications such as automated inventory tracking, product monitoring, and checkout systems.         ",
    "url": "https://arxiv.org/abs/2509.09750",
    "authors": [
      "Hossein Yazdanjouei",
      "Arash Mansouri",
      "Mohammad Shokouhifar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.09751",
    "title": "Meta-Learning Reinforcement Learning for Crypto-Return Prediction",
    "abstract": "           Predicting cryptocurrency returns is notoriously difficult: price movements are driven by a fast-shifting blend of on-chain activity, news flow, and social sentiment, while labeled training data are scarce and expensive. In this paper, we present Meta-RL-Crypto, a unified transformer-based architecture that unifies meta-learning and reinforcement learning (RL) to create a fully self-improving trading agent. Starting from a vanilla instruction-tuned LLM, the agent iteratively alternates between three roles-actor, judge, and meta-judge-in a closed-loop architecture. This learning process requires no additional human supervision. It can leverage multimodal market inputs and internal preference feedback. The agent in the system continuously refines both the trading policy and evaluation criteria. Experiments across diverse market regimes demonstrate that Meta-RL-Crypto shows good performance on the technical indicators of the real market and outperforming other LLM-based baselines.         ",
    "url": "https://arxiv.org/abs/2509.09751",
    "authors": [
      "Junqiao Wang",
      "Zhaoyang Guan",
      "Guanyu Liu",
      "Tianze Xia",
      "Xianzhi Li",
      "Shuo Yin",
      "Xinyuan Song",
      "Chuhan Cheng",
      "Tianyu Shi",
      "Alex Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.09752",
    "title": "Combining Textual and Spectral Features for Robust Classification of Pilot Communications",
    "abstract": "           Accurate estimation of aircraft operations, such as takeoffs and landings, is critical for effective airport management, yet remains challenging, especially at non-towered facilities lacking dedicated surveillance infrastructure. This paper presents a novel dual pipeline machine learning framework that classifies pilot radio communications using both textual and spectral features. Audio data collected from a non-towered U.S. airport was annotated by certified pilots with operational intent labels and preprocessed through automatic speech recognition and Mel-spectrogram extraction. We evaluate a wide range of traditional classifiers and deep learning models, including ensemble methods, LSTM, and CNN across both pipelines. To our knowledge, this is the first system to classify operational aircraft intent using a dual-pipeline ML framework on real-world air traffic audio. Our results demonstrate that spectral features combined with deep architectures consistently yield superior classification performance, with F1-scores exceeding 91%. Data augmentation further improves robustness to real-world audio variability. The proposed approach is scalable, cost-effective, and deployable without additional infrastructure, offering a practical solution for air traffic monitoring at general aviation airports.         ",
    "url": "https://arxiv.org/abs/2509.09752",
    "authors": [
      "Abdullah All Tanvir",
      "Chenyu Huang",
      "Moe Alahmad",
      "Chuyang Yang",
      "Xin Zhong"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computers and Society (cs.CY)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2509.09777",
    "title": "Target Defense Using a Turret and Mobile Defender Team",
    "abstract": "           A scenario is considered wherein a stationary, turn constrained agent (Turret) and a mobile agent (Defender) cooperate to protect the former from an adversarial mobile agent (Attacker). The Attacker wishes to reach the Turret prior to getting captured by either the Defender or Turret, if possible. Meanwhile, the Defender and Turret seek to capture the Attacker as far from the Turret as possible. This scenario is formulated as a differential game and solved using a geometric approach. Necessary and sufficient conditions for the Turret-Defender team winning and the Attacker winning are given. In the case of the Turret-Defender team winning equilibrium strategies for the min max terminal distance of the Attacker to the Turret are given. Three cases arise corresponding to solo capture by the Defender, solo capture by the Turret, and capture simultaneously by both Turret and Defender.         ",
    "url": "https://arxiv.org/abs/2509.09777",
    "authors": [
      "Alexander Von Moll",
      "Dipankar Maity",
      "Meir Pachter",
      "Daigo Shishika",
      "Michael Dorothy"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2509.09787",
    "title": "ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full Version)",
    "abstract": "           Split Learning (SL) is a distributed learning approach that enables resource-constrained clients to collaboratively train deep neural networks (DNNs) by offloading most layers to a central server while keeping in- and output layers on the client-side. This setup enables SL to leverage server computation capacities without sharing data, making it highly effective in resource-constrained environments dealing with sensitive data. However, the distributed nature enables malicious clients to manipulate the training process. By sending poisoned intermediate gradients, they can inject backdoors into the shared DNN. Existing defenses are limited by often focusing on server-side protection and introducing additional overhead for the server. A significant challenge for client-side defenses is enforcing malicious clients to correctly execute the defense algorithm. We present ZORRO, a private, verifiable, and robust SL defense scheme. Through our novel design and application of interactive zero-knowledge proofs (ZKPs), clients prove their correct execution of a client-located defense algorithm, resulting in proofs of computational integrity attesting to the benign nature of locally trained DNN portions. Leveraging the frequency representation of model partitions enables ZORRO to conduct an in-depth inspection of the locally trained models in an untrusted environment, ensuring that each client forwards a benign checkpoint to its succeeding client. In our extensive evaluation, covering different model architectures as well as various attack strategies and data scenarios, we show ZORRO's effectiveness, as it reduces the attack success rate to less than 6\\% while causing even for models storing \\numprint{1000000} parameters on the client-side an overhead of less than 10 seconds.         ",
    "url": "https://arxiv.org/abs/2509.09787",
    "authors": [
      "Nojan Sheybani",
      "Alessandro Pegoraro",
      "Jonathan Knauer",
      "Phillip Rieger",
      "Elissa Mollakuqe",
      "Farinaz Koushanfar",
      "Ahmad-Reza Sadeghi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.09796",
    "title": "Superstructure Optimization with Embedded Neural Networks for Sustainable Aviation Fuel Production",
    "abstract": "           This study presents a multi-objective optimization framework for sustainable aviation fuel (SAF) production, integrating artificial neural networks (ANNs) within a mixed-integer quadratically constrained programming (MIQCP) formulation. By embedding data-driven surrogate models into the mathematical optimization structure, the proposed methodology addresses key limitations of conventional superstructure-based approaches, enabling simultaneous optimization of discrete process choices and continuous operating parameters. The framework captures variable input and output stream compositions, facilitating the joint optimization of target product composition and system design. Application to Fischer-Tropsch (FT) kerosene production demonstrates that cost-minimizing configurations under unconstrained CO2 emissions are dominated by the fossil-based autothermal reforming (ATR) route. Imposing carbon emission constraints necessitates the integration of biomass gasification and direct air capture coupled with carbon sequestration (DAC-CS), resulting in substantially reduced net emissions but higher production costs. At the zero-emission limit, hybrid configurations combining ATR and biomass gasification achieve the lowest costs (~2.38 \\$/kg-kerosene), followed closely by biomass gasification-only (~2.43 \\$/kg), both of which outperform the ATR-only pathway with DAC-CS (~2.65 \\$/kg). In contrast, DAC-only systems relying exclusively on atmospheric CO2 and water electrolysis are prohibitively expensive (~10.8 \\$/kg). The results highlight the critical role of process adaptability: configurations exploiting flexible process parameters, facilitated by embedded ANNs, consistently outperform fixed setups, achieving up to 20% cost savings. Sensitivity analyses elucidate the influence of process conditions, such as FT reactor pressure and gasification temperature, on economic and environmental performance.         ",
    "url": "https://arxiv.org/abs/2509.09796",
    "authors": [
      "Alexander Klimek",
      "Christoph Plate",
      "Sebastian Sager",
      "Kai Sundmacher",
      "Caroline Ganzer"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2509.09808",
    "title": "Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test",
    "abstract": "           Numerous visual impairments can be detected in red-eye reflex images from young children. The so-called Bruckner test is traditionally performed by ophthalmologists in clinical settings. Thanks to the recent technological advances in smartphones and artificial intelligence, it is now possible to recreate the Bruckner test using a mobile device. In this paper, we present a first study conducted during the development of KidsVisionCheck, a free application that can perform vision screening with a mobile device using red-eye reflex images. The underlying model relies on deep neural networks trained on children's pupil images collected and labeled by an ophthalmologist. With an accuracy of 90% on unseen test data, our model provides highly reliable performance without the necessity of specialist equipment. Furthermore, we can identify the optimal conditions for data collection, which can in turn be used to provide immediate feedback to the users. In summary, this work marks a first step toward accessible pediatric vision screenings and early intervention for vision abnormalities worldwide.         ",
    "url": "https://arxiv.org/abs/2509.09808",
    "authors": [
      "Judith Massmann",
      "Alexander Lichtenstein",
      "Francisco M. L\u00f3pez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.09812",
    "title": "EDMD-Based Robust Observer Synthesis for Nonlinear Systems",
    "abstract": "           This paper presents a data driven Koopman operator based framework for designing robust state observers for nonlinear systems. Based on a finite dimensional surrogate of the Koopman generator, identified via an extended dynamic mode decomposition procedure, a tractable formulation of the observer design is enabled on the data driven model with conic uncertainties. The resulting problem is cast as a semidefinite program with linear matrix inequalities, guaranteeing exponential convergence of the observer with a predetermined rate in a probabilistic sense. The approach bridges the gap between statistical error tolerance and observer convergence certification, and enables an explicit use of linear systems theory for state observation via a data driven linear surrogate model. Numerical studies demonstrate the effectiveness and flexibility of the proposed method.         ",
    "url": "https://arxiv.org/abs/2509.09812",
    "authors": [
      "Xiuzhen Ye",
      "Wentao Tang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.09826",
    "title": "The Role of Follow Networks and Twitter's Content Recommender on Partisan Skew and Rumor Exposure during the 2022 U.S. Midterm Election",
    "abstract": "           Social media platforms shape users' experiences through the algorithmic systems they deploy. In this study, we examine to what extent Twitter's content recommender, in conjunction with a user's social network, impacts the topic, political skew, and reliability of information served on the platform during a high-stakes election. We utilize automated accounts to document Twitter's algorithmically curated and reverse chronological timelines throughout the U.S. 2022 midterm election. We find that the algorithmic timeline measurably influences exposure to election content, partisan skew, and the prevalence of low-quality information and election rumors. Critically, these impacts are mediated by the partisan makeup of one's personal social network, which often exerts greater influence than the algorithm alone. We find that the algorithmic feed decreases the proportion of election content shown to left-leaning accounts, and that it skews content toward right-leaning sources when compared to the reverse chronological feed. We additionally find evidence that the algorithmic system increases the prevalence of election-related rumors for right-leaning accounts, and has mixed effects on the prevalence of low-quality information sources. Our work provides insight into the outcomes of Twitter's complex recommender system at a crucial time period before controversial changes to the platform and in the midst of nationwide elections and highlights the need for ongoing study of algorithmic systems and their role in democratic processes.         ",
    "url": "https://arxiv.org/abs/2509.09826",
    "authors": [
      "Kayla Duskin",
      "Joseph S. Schafer",
      "Alexandros Efstratiou",
      "Jevin D. West",
      "Emma S. Spiro"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2509.09828",
    "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception",
    "abstract": "           Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model's inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DELIVER datasets. Code and models will be available at this https URL ",
    "url": "https://arxiv.org/abs/2509.09828",
    "authors": [
      "Tim Broedermannn",
      "Christos Sakaridis",
      "Luigi Piccinelli",
      "Wim Abbeloos",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.09841",
    "title": "Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework",
    "abstract": "           Rosacea, which is a chronic inflammatory skin condition that manifests with facial redness, papules, and visible blood vessels, often requirs precise and early detection for significantly improving treatment effectiveness. This paper presents new patch-based automatic rosacea detection strategies using the ResNet-18 deep learning framework. The contributions of the proposed strategies come from the following aspects. First, various image pateches are extracted from the facial images of people in different sizes, shapes, and locations. Second, a number of investigation studies are carried out to evaluate how the localized visual information influences the deep learing model performance. Third, thorough experiments are implemented to reveal that several patch-based automatic rosacea detection strategies achieve competitive or superior accuracy and sensitivity than the full-image based methods. And finally, the proposed patch-based strategies, which use only localized patches, inherently preserve patient privacy by excluding any identifiable facial features from the data. The experimental results indicate that the proposed patch-based strategies guide the deep learning model to focus on clinically relevant regions, enhance robustness and interpretability, and protect patient privacy. As a result, the proposed strategies offer practical insights for improving automated dermatological diagnostics.         ",
    "url": "https://arxiv.org/abs/2509.09841",
    "authors": [
      "Chengyu Yang",
      "Rishik Reddy Yesgari",
      "Chengjun Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.09843",
    "title": "HGEN: Heterogeneous Graph Ensemble Networks",
    "abstract": "           This paper presents HGEN that pioneers ensemble learning for heterogeneous graphs. We argue that the heterogeneity in node types, nodal features, and local neighborhood topology poses significant challenges for ensemble learning, particularly in accommodating diverse graph learners. Our HGEN framework ensembles multiple learners through a meta-path and transformation-based optimization pipeline to uplift classification accuracy. Specifically, HGEN uses meta-path combined with random dropping to create Allele Graph Neural Networks (GNNs), whereby the base graph learners are trained and aligned for later ensembling. To ensure effective ensemble learning, HGEN presents two key components: 1) a residual-attention mechanism to calibrate allele GNNs of different meta-paths, thereby enforcing node embeddings to focus on more informative graphs to improve base learner accuracy, and 2) a correlation-regularization term to enlarge the disparity among embedding matrices generated from different meta-paths, thereby enriching base learner diversity. We analyze the convergence of HGEN and attest its higher regularization magnitude over simple voting. Experiments on five heterogeneous networks validate that HGEN consistently outperforms its state-of-the-art competitors by substantial margin.         ",
    "url": "https://arxiv.org/abs/2509.09843",
    "authors": [
      "Jiajun Shen",
      "Yufei Jin",
      "Yi He",
      "Xingquan Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.09844",
    "title": "Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection",
    "abstract": "           Rosacea is a common but underdiagnosed inflammatory skin condition that primarily affects the central face and presents with subtle redness, pustules, and visible blood vessels. Automated detection remains challenging due to the diffuse nature of symptoms, the scarcity of labeled datasets, and privacy concerns associated with using identifiable facial images. A novel privacy-preserving automated rosacea detection method inspired by clinical priors and trained entirely on synthetic data is presented in this paper. Specifically, the proposed method, which leverages the observation that rosacea manifests predominantly through central facial erythema, first constructs a fixed redness-informed mask by selecting regions with consistently high red channel intensity across facial images. The mask thus is able to focus on diagnostically relevant areas such as the cheeks, nose, and forehead and exclude identity-revealing features. Second, the ResNet-18 deep learning method, which is trained on the masked synthetic images, achieves superior performance over the full-face baselines with notable gains in terms of accuracy, recall and F1 score when evaluated using the real-world test data. The experimental results demonstrate that the synthetic data and clinical priors can jointly enable accurate and ethical dermatological AI systems, especially for privacy sensitive applications in telemedicine and large-scale screening.         ",
    "url": "https://arxiv.org/abs/2509.09844",
    "authors": [
      "Chengyu Yang",
      "Rishik Reddy Yesgari",
      "Chengjun Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.09869",
    "title": "Surrogate Supervision for Robust and Generalizable Deformable Image Registration",
    "abstract": "           Objective: Deep learning-based deformable image registration has achieved strong accuracy, but remains sensitive to variations in input image characteristics such as artifacts, field-of-view mismatch, or modality difference. We aim to develop a general training paradigm that improves the robustness and generalizability of registration networks. Methods: We introduce surrogate supervision, which decouples the input domain from the supervision domain by applying estimated spatial transformations to surrogate images. This allows training on heterogeneous inputs while ensuring supervision is computed in domains where similarity is well defined. We evaluate the framework through three representative applications: artifact-robust brain MR registration, mask-agnostic lung CT registration, and multi-modal MR registration. Results: Across tasks, surrogate supervision demonstrated strong resilience to input variations including inhomogeneity field, inconsistent field-of-view, and modality differences, while maintaining high performance on well-curated data. Conclusions: Surrogate supervision provides a principled framework for training robust and generalizable deep learning-based registration models without increasing complexity. Significance: Surrogate supervision offers a practical pathway to more robust and generalizable medical image registration, enabling broader applicability in diverse biomedical imaging scenarios.         ",
    "url": "https://arxiv.org/abs/2509.09869",
    "authors": [
      "Yihao Liu",
      "Junyu Chen",
      "Lianrui Zuo",
      "Shuwen Wei",
      "Brian D. Boyd",
      "Carmen Andreescu",
      "Olusola Ajilore",
      "Warren D. Taylor",
      "Aaron Carass",
      "Bennett A. Landman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.09898",
    "title": "DBOS Network Sensing: A Web Services Approach to Collaborative Awareness",
    "abstract": "           DBOS (DataBase Operating System) is a novel capability that integrates web services, operating system functions, and database features to significantly reduce web-deployment effort while increasing resilience. Integration of high performance network sensing enables DBOS web services to collaboratively create a shared awareness of their network environments to enhance their collective resilience and security. Network sensing is added to DBOS using GraphBLAS hypersparse traffic matrices via two approaches: (1) Python-GraphBLAS and (2) OneSparse PostgreSQL. These capabilities are demonstrated using the workflow and analytics from the IEEE/MIT/Amazon Anonymized Network Sensing Graph Challenge. The system was parallelized using pPython and benchmarked using 64 compute nodes on the MIT SuperCloud. The web request rate sustained by a single DBOS instance was ${>}10^5$, well above the required maximum, indicating that network sensing can be added to DBOS with negligible overhead. For collaborative awareness, many DBOS instances were connected to a single DBOS aggregator. The Python-GraphBLAS and OneSparse PostgreSQL implementations scaled linearly up to 64 and 32 nodes respectively. These results suggest that DBOS collaborative network awareness can be achieved with a negligible increase in computing resources.         ",
    "url": "https://arxiv.org/abs/2509.09898",
    "authors": [
      "Sophia Lockton",
      "Jeremy Kepner",
      "Michael Stonebraker",
      "Hayden Jananthan",
      "LaToya Anderson",
      "William Arcand",
      "David Bestor",
      "William Bergeron",
      "Alex Bonn",
      "Daniel Burrill",
      "Chansup Byun",
      "Timothy Davis",
      "Vijay Gadepally",
      "Michael Houle",
      "Matthew Hubbell",
      "Michael Jones",
      "Piotr Luszczek",
      "Peter Michaleas",
      "Lauren Milechin",
      "Chasen Milner",
      "Guillermo Morales",
      "Julie Mullen",
      "Michel Pelletier",
      "Alex Poliakov",
      "Andrew Prout",
      "Albert Reuther",
      "Antonio Rosa",
      "Charles Yee",
      "Alex Pentland"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Cryptography and Security (cs.CR)",
      "Databases (cs.DB)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Operating Systems (cs.OS)"
    ]
  },
  {
    "id": "arXiv:2509.09899",
    "title": "Variational Neural Networks for Observable Thermodynamics (V-NOTS)",
    "abstract": "           Much attention has recently been devoted to data-based computing of evolution of physical systems. In such approaches, information about data points from past trajectories in phase space is used to reconstruct the equations of motion and to predict future solutions that have not been observed before. However, in many cases, the available data does not correspond to the variables that define the system's phase space. We focus our attention on the important example of dissipative dynamical systems. In that case, the phase space consists of coordinates, momenta and entropies; however, the momenta and entropies cannot, in general, be observed directly. To address this difficulty, we develop an efficient data-based computing framework based exclusively on observable variables, by constructing a novel approach based on the \\emph{thermodynamic Lagrangian}, and constructing neural networks that respect the thermodynamics and guarantees the non-decreasing entropy evolution. We show that our network can provide an efficient description of phase space evolution based on a limited number of data points and a relatively small number of parameters in the system.         ",
    "url": "https://arxiv.org/abs/2509.09899",
    "authors": [
      "Christopher Eldred",
      "Fran\u00e7ois Gay-Balmaz",
      "Vakhtang Putkaradze"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.09910",
    "title": "Seeing Identity in Data: Can Anthropographics Uncover Racial Homophily in Emotional Responses?",
    "abstract": "           Racial homophily refers to the tendency of individuals to associate with others of the same racial or ethnic background. A recent study found no evidence of racial homophily in responses to mass shooting data visualizations. To increase the likelihood of detecting an effect, we redesigned the experiment by replacing bar charts with anthropographics and expanding the sample size. In a crowdsourced study (N=720), we showed participants a pictograph of mass shooting victims in the United States, with victims from one of three racial groups (Hispanic, Black, or White) highlighted. Each participant was assigned a visualization highlighting either their own racial group or a different racial group, allowing us to assess the influence of racial concordance on changes in affect (emotion). We found that, across all conditions, racial concordance had a modest but significant effect on changes in affect, with participants experiencing greater negative affect change when viewing visualizations highlighting their own race. This study provides initial evidence that racial homophily can emerge in responses to data visualizations, particularly when using anthropographics.         ",
    "url": "https://arxiv.org/abs/2509.09910",
    "authors": [
      "Poorna Talkad Sukumar",
      "Maurizio Porfiri",
      "Oded Nov"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2509.09916",
    "title": "Immersive Invaders: Privacy Threats from Deceptive Design in Virtual Reality Games and Applications",
    "abstract": "           Virtual Reality (VR) technologies offer immersive experiences but collect substantial user data. While deceptive design is well-studied in 2D platforms, little is known about its manifestation in VR environments and its impact on user privacy. This research investigates deceptive designs in privacy communication and interaction mechanisms of 12 top-rated VR games and applications through autoethnographic evaluation of the applications and thematic analysis of privacy policies. We found that while many deceptive designs rely on 2D interfaces, some VR-unique features, while not directly enabling deception, amplified data disclosure behaviors, and obscured actual data practices. Convoluted privacy policies and manipulative consent practices further hinder comprehension and increase privacy risks. We also observed privacy-preserving design strategies and protective considerations in VR privacy policies. We offer recommendations for ethical VR design that balance immersive experiences with strong privacy protections, guiding researchers, designers, and policymakers to improve privacy in VR environments.         ",
    "url": "https://arxiv.org/abs/2509.09916",
    "authors": [
      "Hilda Hadan",
      "Michaela Valiquette",
      "Lennart E. Nacke",
      "Leah Zhang-Kennedy"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2509.09917",
    "title": "SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion",
    "abstract": "           Automatically generating formal specifications from program code can greatly enhance the efficiency of program verification and enable end-to-end automation from requirements to reliable software. However, existing LLM-based approaches often struggle with programs that include complex loop structures, leading to irrelevant specifications. Moreover, the rigorous proof obligations and design constraints imposed by verification tools can further result in incomplete and ambiguous specifications. To address these challenges, we propose SLD-Spec, an LLM-assisted specification generation method tailored for programs with complex loop constructs. SLD-Spec introduces two novel phases into the traditional specification generation framework: (1) A slicing phase, which decomposes each function into code fragments containing independent loop structures, thereby reducing the complexity of specification generation; and (2) A logical deletion phase, which applies LLM-based reasoning to filter out incorrect candidate specifications--especially those not easily identified by verification tool--while retaining valid ones. Experimental results show that on the simple dataset, SLD-Spec successfully verifies five more programs than the state-of-the-art AutoSpec and reduces runtime by 23.73%. To address the limitations of existing research, we manually construct a dataset comprising four categories of complex loop programs. On this dataset, SLD-Spec significantly improves the correctness, relevance, and completeness of generated specifications compared to baseline methods, enabling 95.1% of assertions and 90.91% of programs to pass verification. Ablation studies further reveal that logical deletion is critical for enhancing specification correctness and relevance, while program slicing contributes significantly to specification completeness. Our code and data are publicly available.         ",
    "url": "https://arxiv.org/abs/2509.09917",
    "authors": [
      "Zehan Chen",
      "Long Zhang",
      "Zhiwei Zhang",
      "JingJing Zhang",
      "Ruoyu Zhou",
      "Yulong Shen",
      "JianFeng Ma",
      "Lin Yang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.09919",
    "title": "A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments",
    "abstract": "           Procedural content generation often requires satisfying both designer-specified objectives and adjacency constraints implicitly imposed by the underlying tile set. To address the challenges of jointly optimizing both constraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a Markov Decision Process (MDP), enabling external optimization algorithms to focus exclusively on objective maximization while leveraging WFC's propagation mechanism to enforce constraint satisfaction. We empirically compare optimizing this MDP to traditional evolutionary approaches that jointly optimize global metrics and local tile placement. Across multiple domains with various difficulties, we find that joint optimization not only struggles as task complexity increases, but consistently underperforms relative to optimization over the WFC-MDP, underscoring the advantages of decoupling local constraint satisfaction from global objective optimization.         ",
    "url": "https://arxiv.org/abs/2509.09919",
    "authors": [
      "Franklin Yiu",
      "Mohan Lu",
      "Nina Li",
      "Kevin Joseph",
      "Tianxu Zhang",
      "Julian Togelius",
      "Timothy Merino",
      "Sam Earle"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.09928",
    "title": "Fraud detection and risk assessment of online payment transactions on e-commerce platforms based on LLM and GCN frameworks",
    "abstract": "           With the rapid growth of e-commerce, online payment fraud has become increasingly complex, posing serious threats to financial security and consumer trust. Traditional detection methods often struggle to capture the intricate relational structures inherent in transactional data. This study presents a novel fraud detection framework that combines Large Language Models (LLM) with Graph Convolutional Networks (GCN) to effectively identify fraudulent activities in e-commerce online payment transactions. A dataset of 2,840,000 transactions was collected over 14 days from major platforms such as Amazon, involving approximately 2,000 U.S.-based consumers and 30 merchants. With fewer than 6000 fraudulent instances, the dataset represents a highly imbalanced scenario. Consumers and merchants were modeled as nodes and transactions as edges to form a heterogeneous graph, upon which a GCN was applied to learn complex behavioral patterns. Semantic features extracted via GPT-4o and Tabformer were integrated with structural features to enhance detection performance. Experimental results demonstrate that the proposed model achieves an accuracy of 0.98, effectively balancing precision and sensitivity in fraud detection. This framework offers a scalable and real-time solution for securing online payment environments and provides a promising direction for applying graph-based deep learning in financial fraud prevention.         ",
    "url": "https://arxiv.org/abs/2509.09928",
    "authors": [
      "RuiHan Luo",
      "Nanxi Wang",
      "Xiaotong Zhu"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2509.09935",
    "title": "SCoDA: Self-supervised Continual Domain Adaptation",
    "abstract": "           Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a model to a target domain without access to the data of the source domain. Prevailing methods typically start with a source model pre-trained with full supervision and distill the knowledge by aligning instance-level features. However, these approaches, relying on cosine similarity over L2-normalized feature vectors, inadvertently discard crucial geometric information about the latent manifold of the source model. We introduce Self-supervised Continual Domain Adaptation (SCoDA) to address these limitations. We make two key departures from standard practice: first, we avoid the reliance on supervised pre-training by initializing the proposed framework with a teacher model pre-trained entirely via self-supervision (SSL). Second, we adapt the principle of geometric manifold alignment to the SFDA setting. The student is trained with a composite objective combining instance-level feature matching with a Space Similarity Loss. To combat catastrophic forgetting, the teacher's parameters are updated via an Exponential Moving Average (EMA) of the student's parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA significantly outperforms state-of-the-art SFDA methods.         ",
    "url": "https://arxiv.org/abs/2509.09935",
    "authors": [
      "Chirayu Agrawal",
      "Snehasis Mukherjee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.09946",
    "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation",
    "abstract": "           Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision task for automating large-scale surveillance. With camera calibration and depth information, the targets in the scene can be projected into 3D space, offering unparalleled levels of automatic perception of a 3D environment. However, tracking in the 3D space requires replacing all 2D tracking components from the ground up, which may be infeasible for existing MTMC systems. In this paper, we present an approach for extending any online 2D multi-camera tracking system into 3D space by utilizing depth information to reconstruct a target in point-cloud space, and recovering its 3D box through clustering and yaw refinement following tracking. We also introduced an enhanced online data association mechanism that leverages the target's local ID consistency to assign global IDs across frames. The proposed framework is evaluated on the 2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the leaderboard.         ",
    "url": "https://arxiv.org/abs/2509.09946",
    "authors": [
      "Vu-Minh Le",
      "Thao-Anh Tran",
      "Duc Huy Do",
      "Xuan Canh Do",
      "Huong Ninh",
      "Hai Tran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.09947",
    "title": "Toward Green Code: Prompting Small Language Models for Energy-Efficient Code Generation",
    "abstract": "           There is a growing concern about the environmental impact of large language models (LLMs) in software development, particularly due to their high energy use and carbon footprint. Small Language Models (SLMs) offer a more sustainable alternative, requiring fewer computational resources while remaining effective for fundamental programming tasks. In this study, we investigate whether prompt engineering can improve the energy efficiency of SLMs in code generation. We evaluate four open-source SLMs, StableCode-Instruct-3B, Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct, across 150 Python problems from LeetCode, evenly distributed into easy, medium, and hard categories. Each model is tested under four prompting strategies: role prompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated solution, we measure runtime, memory usage, and energy consumption, comparing the results with a human-written baseline. Our findings show that CoT prompting provides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while CodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any prompting strategy. These results highlight that the benefits of prompting are model-dependent and that carefully designed prompts can guide SLMs toward greener software development.         ",
    "url": "https://arxiv.org/abs/2509.09947",
    "authors": [
      "Humza Ashraf",
      "Syed Muhammad Danish",
      "Zeeshan Sattar"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.09953",
    "title": "Detection of Anomalous Behavior in Robot Systems Based on Machine Learning",
    "abstract": "           Ensuring the safe and reliable operation of robotic systems is paramount to prevent potential disasters and safeguard human well-being. Despite rigorous design and engineering practices, these systems can still experience malfunctions, leading to safety risks. In this study, we present a machine learning-based approach for detecting anomalies in system logs to enhance the safety and reliability of robotic systems. We collected logs from two distinct scenarios using CoppeliaSim and comparatively evaluated several machine learning models, including Logistic Regression (LR), Support Vector Machine (SVM), and an Autoencoder. Our system was evaluated in a quadcopter context (Context 1) and a Pioneer robot context (Context 2). Results showed that while LR demonstrated superior performance in Context 1, the Autoencoder model proved to be the most effective in Context 2. This highlights that the optimal model choice is context-dependent, likely due to the varying complexity of anomalies across different robotic platforms. This research underscores the value of a comparative approach and demonstrates the particular strengths of autoencoders for detecting complex anomalies in robotic systems.         ",
    "url": "https://arxiv.org/abs/2509.09953",
    "authors": [
      "Mahfuzul I. Nissan",
      "Sharmin Aktar"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.09956",
    "title": "Request a Note: How the Request Function Shapes X's Community Notes System",
    "abstract": "           X's Community Notes is a crowdsourced fact-checking system. To improve its scalability, X recently introduced \"Request Community Note\" feature, enabling users to solicit fact-checks from contributors on specific posts. Yet, its implications for the system -- what gets checked, by whom, and with what quality -- remain unclear. Using 98,685 requested posts and their associated notes, we evaluate how requests shape the Community Notes system. We find that contributors prioritize posts with higher misleadingness and from authors with greater misinformation exposure, but neglect political content emphasized by requestors. Selection also diverges along partisan lines: contributors more often annotate posts from Republicans, while requestors surface more from Democrats. Although only 12% of posts receive request-fostered notes from top contributors, these notes are rated as more helpful and less polarized than others, partly reflecting top contributors' selective fact-checking of misleading posts. Our findings highlight both the limitations and promise of requests for scaling high-quality community-based fact-checking.         ",
    "url": "https://arxiv.org/abs/2509.09956",
    "authors": [
      "Yuwei Chuai",
      "Shuning Zhang",
      "Ziming Wang",
      "Xin Yi",
      "Mohsen Mosleh",
      "Gabriele Lenzini"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2509.09988",
    "title": "FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction",
    "abstract": "           Accurate and reliable solar flare predictions are essential to mitigate potential impacts on critical infrastructure. However, the current performance of solar flare forecasting is insufficient. In this study, we address the task of predicting the class of the largest solar flare expected to occur within the next 72 hours. Existing methods often fail to adequately address the severe class imbalance across flare classes. To address this issue, we propose a solar flare prediction model based on multiple deep state space models. In addition, we introduce the frequency & local-boundary-aware reliability loss (FLARE loss) to improve predictive performance and reliability under class imbalance. Experiments were conducted on a multi-wavelength solar image dataset covering a full 11-year solar activity cycle. As a result, our method outperformed baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the true skill statistic, which are standard metrics in terms of the performance and reliability.         ",
    "url": "https://arxiv.org/abs/2509.09988",
    "authors": [
      "Yusuke Takagi",
      "Shunya Nagashima",
      "Komei Sugiura"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Solar and Stellar Astrophysics (astro-ph.SR)"
    ]
  },
  {
    "id": "arXiv:2509.09989",
    "title": "rCamInspector: Building Reliability and Trust on IoT (Spy) Camera Detection using XAI",
    "abstract": "           The classification of network traffic using machine learning (ML) models is one of the primary mechanisms to address the security issues in IoT networks and/or IoT devices. However, the ML models often act as black-boxes that create a roadblock to take critical decision based on the model output. To address this problem, we design and develop a system, called rCamInspector, that employs Explainable AI (XAI) to provide reliable and trustworthy explanations to model output. rCamInspector adopts two classifiers, Flow Classifier - categorizes a flow into one of four classes, IoTCam, Conf, Share and Others, and SmartCam Classifier - classifies an IoTCam flow into one of six classes, Netatmo, Spy Clock, Canary, D3D, Ezviz, V380 Spy Bulb; both are IP address and transport port agnostic. rCamInspector is evaluated using 38GB of network traffic and our results show that XGB achieves the highest accuracy of 92% and 99% in the Flow and SmartCam classifiers respectively among eight supervised ML models. We analytically show that the traditional mutual information (MI) based feature importance cannot provide enough reliability on the model output of XGB in either classifiers. Using SHAP and LIME, we show that a separate set of features can be picked up to explain a correct prediction of XGB. For example, the feature Init Bwd Win Byts turns out to have the highest SHAP values to support the correct prediction of both IoTCam in Flow Classifier and Netatmo class in SmartCam Classifier. To evaluate the faithfulness of the explainers on our dataset, we show that both SHAP and LIME have a consistency of more than 0.7 and a sufficiency of 1.0. Comparing with existing works, we show that rCamInspector achieves a better accuracy (99%), precision (99%), and false negative rate (0.7%).         ",
    "url": "https://arxiv.org/abs/2509.09989",
    "authors": [
      "Priyanka Rushikesh Chaudhary",
      "Manan Gupta",
      "Jabez Christopher",
      "Putrevu Venkata Sai Charan",
      "Rajib Ranjan Maiti"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.10000",
    "title": "Neural Scaling Laws for Deep Regression",
    "abstract": "           Neural scaling laws--power-law relationships between generalization errors and characteristics of deep learning models--are vital tools for developing reliable models while managing limited resources. Although the success of large language models highlights the importance of these laws, their application to deep regression models remains largely unexplored. Here, we empirically investigate neural scaling laws in deep regression using a parameter estimation model for twisted van der Waals magnets. We observe power-law relationships between the loss and both training dataset size and model capacity across a wide range of values, employing various architectures--including fully connected networks, residual networks, and vision transformers. Furthermore, the scaling exponents governing these relationships range from 1 to 2, with specific values depending on the regressed parameters and model details. The consistent scaling behaviors and their large scaling exponents suggest that the performance of deep regression models can improve substantially with increasing data size.         ",
    "url": "https://arxiv.org/abs/2509.10000",
    "authors": [
      "Tilen Cadez",
      "Kyoung-Min Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Other Condensed Matter (cond-mat.other)"
    ]
  },
  {
    "id": "arXiv:2509.10004",
    "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes",
    "abstract": "           Unsupervised hallucination detection aims to identify hallucinated content generated by large language models (LLMs) without relying on labeled data. While unsupervised methods have gained popularity by eliminating labor-intensive human annotations, they frequently rely on proxy signals unrelated to factual correctness. This misalignment biases detection probes toward superficial or non-truth-related aspects, limiting generalizability across datasets and scenarios. To overcome these limitations, we propose IRIS, an unsupervised hallucination detection framework, leveraging internal representations intrinsic to factual correctness. IRIS prompts the LLM to carefully verify the truthfulness of a given statement, and obtain its contextualized embedding as informative features for training. Meanwhile, the uncertainty of each response is considered a soft pseudolabel for truthfulness. Experimental results demonstrate that IRIS consistently outperforms existing unsupervised methods. Our approach is fully unsupervised, computationally low cost, and works well even with few training data, making it suitable for real-time detection.         ",
    "url": "https://arxiv.org/abs/2509.10004",
    "authors": [
      "Ponhvoan Srey",
      "Xiaobao Wu",
      "Anh Tuan Luu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.10018",
    "title": "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method",
    "abstract": "           With the rapid advancement of Large Language Model (LLM), LLM-based agents exhibit exceptional abilities in understanding and generating natural language, facilitating human-like collaboration and information transmission in LLM-based Multi-Agent System (MAS). High-performance LLMs are often hosted on remote servers in public spaces. When tasks involve privacy data, MAS cannot securely utilize these LLMs without implementing privacy-preserving mechanisms. To address this challenge, we propose a General Anonymizing Multi-Agent system (GAMA), which divides the agents' workspace into private and public spaces and protects privacy through the anonymizing mechanism. In the private space, agents handle sensitive data, while in the public space, only anonymized data is utilized. GAMA incorporates two key modules to mitigate semantic loss caused by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The results demonstrate that GAMA has superior performance compared to the state-of-the-art models. To further assess its privacy-preserving capabilities, we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy Preservation. The final results highlight GAMA's exceptional effectiveness in both task processing and privacy preservation.         ",
    "url": "https://arxiv.org/abs/2509.10018",
    "authors": [
      "Hailong Yang",
      "Renhuo Zhao",
      "Guanjin Wang",
      "Zhaohong Deng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.10033",
    "title": "Sparse Coding Representation of 2-way Data",
    "abstract": "           Sparse dictionary coding represents signals as linear combinations of a few dictionary atoms. It has been applied to images, time series, graph signals and multi-way spatio-temporal data by jointly employing temporal and spatial dictionaries. Data-agnostic analytical dictionaries, such as the discrete Fourier transform, wavelets and graph Fourier, have seen wide adoption due to efficient implementations and good practical performance. On the other hand, dictionaries learned from data offer sparser and more accurate solutions but require learning of both the dictionaries and the coding coefficients. This becomes especially challenging for multi-dictionary scenarios since encoding coefficients correspond to all atom combinations from the dictionaries. To address this challenge, we propose a low-rank coding model for 2-dictionary scenarios and study its data complexity. Namely, we establish a bound on the number of samples needed to learn dictionaries that generalize to unseen samples from the same distribution. We propose a convex relaxation solution, called AODL, whose exact solution we show also solves the original problem. We then solve this relaxation via alternating optimization between the sparse coding matrices and the learned dictionaries, which we prove to be convergent. We demonstrate its quality for data reconstruction and missing value imputation in both synthetic and real-world datasets. For a fixed reconstruction quality, AODL learns up to 90\\% sparser solutions compared to non-low-rank and analytical (fixed) dictionary baselines. In addition, the learned dictionaries reveal interpretable insights into patterns present within the samples used for training.         ",
    "url": "https://arxiv.org/abs/2509.10033",
    "authors": [
      "Boya Ma",
      "Abram Magner",
      "Maxwell McNeil",
      "Petko Bogdanov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10034",
    "title": "Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability",
    "abstract": "           We present a formal and constructive theory showing that probabilistic finite automata (PFAs) can be exactly simulated using symbolic feedforward neural networks. Our architecture represents state distributions as vectors and transitions as stochastic matrices, enabling probabilistic state propagation via matrix-vector products. This yields a parallel, interpretable, and differentiable simulation of PFA dynamics using soft updates-without recurrence. We formally characterize probabilistic subset construction, $\\varepsilon$-closure, and exact simulation via layered symbolic computation, and prove equivalence between PFAs and specific classes of neural networks. We further show that these symbolic simulators are not only expressive but learnable: trained with standard gradient descent-based optimization on labeled sequence data, they recover the exact behavior of ground-truth PFAs. This learnability, formalized in Proposition 5.1, is the crux of this work. Our results unify probabilistic automata theory with neural architectures under a rigorous algebraic framework, bridging the gap between symbolic computation and deep learning.         ",
    "url": "https://arxiv.org/abs/2509.10034",
    "authors": [
      "Sahil Rajesh Dhayalkar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10035",
    "title": "Linguistic trajectories of bipolar disorder on social media",
    "abstract": "           Language provides valuable markers of affective disorders such as bipolar disorder (BD), yet clinical assessments remain limited in scale. In response, analyses of social media (SM) language have gained prominence due to their high temporal resolution and longitudinal scope. Here, we introduce a method to determine the timing of users' diagnoses and apply it to study language trajectories from 3 years before to 21 years after BD diagnosis - contrasted with uses reporting unipolar depression (UD) and non-affected users (HC). We show that BD diagnosis is accompanied by pervasive linguistic alterations reflecting mood disturbance, psychiatric comorbidity, substance abuse, hospitalization, medical comorbidities, unusual thought content, and disorganized thought. We further observe recurring mood-related language changes across two decades after the diagnosis, with a pronounced 12-month periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence suggests an increased periodicity in users estimated to be female. In sum, our findings provide evidence for language alterations in the acute and chronic phase of BD. This validates and extends recent efforts leveraging SM for scalable monitoring of mental health.         ",
    "url": "https://arxiv.org/abs/2509.10035",
    "authors": [
      "Laurin Plank",
      "Armin Zlomuzica"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.10036",
    "title": "Approximate Graph Propagation Revisited: Dynamic Parameterized Queries, Tighter Bounds and Dynamic Updates",
    "abstract": "           We revisit Approximate Graph Propagation (AGP), a unified framework which captures various graph propagation tasks, such as PageRank, feature propagation in Graph Neural Networks (GNNs), and graph-based Retrieval-Augmented Generation (RAG). Our work focuses on the settings of dynamic graphs and dynamic parameterized queries, where the underlying graphs evolve over time (updated by edge insertions or deletions) and the input query parameters are specified on the fly to fit application needs. Our first contribution is an interesting observation that the SOTA solution, AGP-Static, can be adapted to support dynamic parameterized queries; however several challenges remain unresolved. Firstly, the query time complexity of AGP-Static is based on an assumption of using an optimal algorithm for subset sampling in its query algorithm. Unfortunately, back to that time, such an algorithm did not exist; without such an optimal algorithm, an extra $O(\\log^2 n)$ factor is required in the query complexity, where $n$ is the number of vertices in the graphs. Secondly, AGP-Static performs poorly on dynamic graphs, taking $O(n\\log n)$ time to process each update. To address these challenges, we propose a new algorithm, AGP-Static++, which is simpler yet reduces roughly a factor of $O(\\log^2 n)$ in the query complexity while preserving the approximation guarantees of AGP-Static. However, AGP-Static++ still requires $O(n)$ time to process each update. To better support dynamic graphs, we further propose AGP-Dynamic, which achieves $O(1)$ amortized time per update, significantly improving the aforementioned $O(n)$ per-update bound, while still preserving the query complexity and approximation guarantees. Last, our comprehensive experiments validate the theoretical improvements: compared to the baselines, our algorithm achieves speedups of up to $177\\times$ on update time and $10\\times$ on query efficiency.         ",
    "url": "https://arxiv.org/abs/2509.10036",
    "authors": [
      "Zhuowei Zhao",
      "Zhuo Zhang",
      "Hanzhi Wang",
      "Junhao Gan",
      "Zhifeng Bao",
      "Jianzhong Qi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2509.10054",
    "title": "XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph",
    "abstract": "           The rapid advancement of Large Language Models (LLMs) has significantly enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans with complex, real-world tasks. However, MAS still face challenges in effective task planning when handling highly complex tasks with uncertainty, often resulting in misleading or incorrect outputs that hinder task execution. To address this, we propose XAgents, a unified multi-agent cooperative framework built on a multipolar task processing graph and IF-THEN rules. XAgents uses the multipolar task processing graph to enable dynamic task planning and handle task uncertainty. During subtask processing, it integrates domain-specific IF-THEN rules to constrain agent behaviors, while global rules enhance inter-agent collaboration. We evaluate the performance of XAgents across three distinct datasets, demonstrating that it consistently surpasses state-of-the-art single-agent and multi-agent approaches in both knowledge-typed and logic-typed question-answering tasks. The codes for XAgents are available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2509.10054",
    "authors": [
      "Hailong Yang",
      "Mingxian Gu",
      "Jianqi Wang",
      "Guanjin Wang",
      "Zhaohong Deng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.10062",
    "title": "A Note on Constructive Canonical Splitter Strategies in Nowhere Dense Graph Classes",
    "abstract": "           The radius-$r$ splitter game is played on a graph $G$ between two players: Splitter and Connector. In each round, Connector selects a vertex $v$, and the current game arena is restricted to the radius-$r$ neighborhood of $v$. Then Splitter removes a vertex from this restricted subgraph. The game ends, and Splitter wins, when the arena becomes empty. Splitter aims to end the game as quickly as possible, while Connector tries to prolong it for as long as possible. The splitter game was introduced by Grohe, Kreutzer and Siebertz to characterize nowhere dense graph classes. They showed that a class $\\mathscr{C}$ of graphs is nowhere dense if and only if for every radius $r$ there exists a number $\\ell$ such that Splitter has a strategy on every $G\\in \\mathscr{C}$ to win the radius-$r$ splitter game in at most $\\ell$ rounds. It was recently proved by Ohlmann et al. that there are only a bounded number of possible Splitter moves that are progressing, that is, moves that lead to an arena where Splitter can win in one less round. The proof of Ohlmann et al. is based on the compactness theorem and does not give a constructive bound on the number of progressing moves. In this work, we give a simple constructive proof, showing that if Splitter can force a win in the radius-$r$ game in $k$ rounds, then there are at most $(2r+1)^{\\,2^{k-1}-1}$ progressing moves.         ",
    "url": "https://arxiv.org/abs/2509.10062",
    "authors": [
      "Janne Fuchser",
      "Nikolas M\u00e4hlmann",
      "Sebastian Siebertz"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Discrete Mathematics (cs.DM)",
      "Logic (math.LO)"
    ]
  },
  {
    "id": "arXiv:2509.10077",
    "title": "Predictive Spike Timing Enables Distributed Shortest Path Computation in Spiking Neural Networks",
    "abstract": "           Efficient planning and sequence selection are central to intelligence, yet current approaches remain largely incompatible with biological computation. Classical graph algorithms like Dijkstra's or A* require global state and biologically implausible operations such as backtracing, while reinforcement learning methods rely on slow gradient-based policy updates that appear inconsistent with rapid behavioral adaptation observed in natural systems. We propose a biologically plausible algorithm for shortest-path computation that operates through local spike-based message-passing with realistic processing delays. The algorithm exploits spike-timing coincidences to identify nodes on optimal paths: Neurons that receive inhibitory-excitatory message pairs earlier than predicted reduce their response delays, creating a temporal compression that propagates backwards from target to source. Through analytical proof and simulations on random spatial networks, we demonstrate that the algorithm converges and discovers all shortest paths using purely timing-based mechanisms. By showing how short-term timing dynamics alone can compute shortest paths, this work provides new insights into how biological networks might solve complex computational problems through purely local computation and relative spike-time prediction. These findings open new directions for understanding distributed computation in biological and artificial systems, with possible implications for computational neuroscience, AI, reinforcement learning, and neuromorphic systems.         ",
    "url": "https://arxiv.org/abs/2509.10077",
    "authors": [
      "Simen Storesund",
      "Kristian Valset Aars",
      "Robin Dietrich",
      "Nicolai Waniek"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10080",
    "title": "BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals",
    "abstract": "           In autonomous driving, trajectory prediction is essential for ensuring safe and efficient navigation. To improve prediction accuracy, recent approaches often rely on pre-built high-definition (HD) maps or real-time local map construction modules to incorporate static environmental information. However, pre-built HD maps are limited to specific regions and cannot adapt to transient changes. In addition, local map construction modules, which recognize only predefined elements, may fail to capture critical scene details or introduce errors that degrade prediction performance. To overcome these limitations, we propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory prediction framework that operates directly in the bird's-eye view (BEV) space utilizing real-time sensor data without relying on any pre-built maps. The BEVTraj leverages deformable attention to efficiently extract relevant context from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate Proposal (SGCP) module, which enables full end-to-end prediction without requiring any post-processing steps. Extensive experiments demonstrate that the BEVTraj achieves performance comparable to state-of-the-art HD map-based models while offering greater flexibility by eliminating the dependency on pre-built maps. The source code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.10080",
    "authors": [
      "Minsang Kong",
      "Myeongjun Kim",
      "Sang Gu Kang",
      "Sang Hun Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.10089",
    "title": "KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework",
    "abstract": "           We introduce a novel symbolic regression framework, namely KAN-SR, built on Kolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach. Symbolic regression searches for mathematical equations that best fit a given dataset and is commonly solved with genetic programming approaches. We show that by using deep learning techniques, more specific KANs, and combining them with simplification strategies such as translational symmetries and separabilities, we are able to recover ground-truth equations of the Feynman Symbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we show that by combining the proposed framework with neural controlled differential equations, we are able to model the dynamics of an in-silico bioprocess system precisely, opening the door for the dynamic modeling of other engineering systems.         ",
    "url": "https://arxiv.org/abs/2509.10089",
    "authors": [
      "Marco Andrea B\u00fchler",
      "Gonzalo Guill\u00e9n-Gos\u00e1lbez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10099",
    "title": "Generating Energy-Efficient Code via Large-Language Models -- Where are we now?",
    "abstract": "           Context. The rise of Large Language Models (LLMs) has led to their widespread adoption in development pipelines. Goal. We empirically assess the energy efficiency of Python code generated by LLMs against human-written code and code developed by a Green software expert. Method. We test 363 solutions to 9 coding problems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting techniques, and comparing them to human-developed solutions. Energy consumption is measured on three different hardware platforms: a server, a PC, and a Raspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16% more energy-efficient on the server and 3% on the Raspberry Pi, while LLMs outperform human developers by 25% on the PC. Prompting does not consistently lead to energy savings, where the most energy-efficient prompts vary by hardware platform. The code developed by a Green software expert is consistently more energy-efficient by at least 17% to 30% against all LLMs on all hardware platforms. Conclusions. Even though LLMs exhibit relatively good code generation capabilities, no LLM-generated code was more energy-efficient than that of an experienced Green software developer, suggesting that as of today there is still a great need of human expertise for developing energy-efficient Python code.         ",
    "url": "https://arxiv.org/abs/2509.10099",
    "authors": [
      "Radu Apsan",
      "Vincenzo Stoico",
      "Michel Albonico",
      "Rudra Dhar",
      "Karthik Vaidhyanathan",
      "Ivano Malavolta"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.10110",
    "title": "Neural network-based singularity detection and applications",
    "abstract": "           We present a method for constructing a special type of shallow neural network that learns univariate meromorphic functions with pole-type singularities. Our method is based on using a finite set of Laurent coefficients as input information, which we compute by FFT, employing values of the investigated function on some contour $\\Gamma$ in the complex plane. The primary components of our methodology are the following: (1) the adaptive construction of rational polynomial activation functions, (2) a novel backpropagation-free method for determining the weights and biases of the hidden layer, and (3) the computation of the weights and biases of the output layer through least-squares fitting. Breaking with the idea of \"safe\" rational activation functions, we introduce a rational activation function as a meromorphic function with a single pole situated within the domain of investigation. Employing the weights and biases of the hidden layer, we then scale and shift the pole of the activation function to find the estimated locations of the singularities; this implies that the number of neurons in the hidden layer is determined by the number of singularities of the function that is being approximated. While the weights and biases of the hidden layer are tuned so as to capture the singularities, the least-squares fitting for the computation of weights and biases of the output layer ensures approximation of the function in the rest of the domain. Through the use of Laurent-Pad\u00e9 rational approximation concepts, we prove locally uniform convergence of our method. We illustrate the effectiveness of our method through numerical experiments, including the construction of extensions of the time-dependent solutions of nonlinear autonomous PDEs into the complex plane, and study the dynamics of their singularities.         ",
    "url": "https://arxiv.org/abs/2509.10110",
    "authors": [
      "Nadiia Derevianko",
      "Ioannis G. Kevrekidis",
      "Felix Dietrich"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2509.10112",
    "title": "Evolution of Coordination Through Institutional Incentives: An Evolutionary Game Theory Approach",
    "abstract": "           There is a broad recognition that commitment-based mechanisms can promote coordination and cooperative behaviours in both biological populations and self-organised multi-agent systems by making individuals' intentions explicit prior to engagement. Yet their effectiveness depends on sustained compliance supported by institutions, especially in one-off interactions. Despite advances in quantitative studies of cooperation and commitment, most applied analyses and policy debates remain largely qualitative, with limited attention to the allocation of scarce institutional resources between enhancing participation and ensuring commitment compliance. Herein, we develop an evolutionary game-theoretic model that explicitly examines the strategic distribution of a limited budget for institutional incentives, namely rewards or punishments, aimed at these two critical objectives within pre-commitment frameworks. Our findings reveal that a reward-based incentive approach consistently yields greater coordination success than a punishment-based approach, with optimal outcomes arising when resources are appropriately distributed between participation promotion and compliance assurance. These findings offer novel insights for designing institutional incentives to promote broad, coordinated adoption of new technologies.         ",
    "url": "https://arxiv.org/abs/2509.10112",
    "authors": [
      "Ndidi Bianca Ogbo",
      "Zhao Song",
      "Anh Han"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2509.10118",
    "title": "Scalable Synthesis and Verification of String Stable Neural Certificates for Interconnected Systems",
    "abstract": "           Ensuring string stability is critical for the safety and efficiency of large-scale interconnected systems. Although learning-based controllers (e.g., those based on reinforcement learning) have demonstrated strong performance in complex control scenarios, their black-box nature hinders formal guarantees of string stability. To address this gap, we propose a novel verification and synthesis framework that integrates discrete-time scalable input-to-state stability (sISS) with neural network verification to formally guarantee string stability in interconnected systems. Our contributions are four-fold. First, we establish a formal framework for synthesizing and robustly verifying discrete-time scalable input-to-state stability (sISS) certificates for neural network-based interconnected systems. Specifically, our approach extends the notion of sISS to discrete-time settings, constructs neural sISS certificates, and introduces a verification procedure that ensures string stability while explicitly accounting for discrepancies between the true dynamics and their neural approximations. Second, we establish theoretical foundations and algorithms to scale the training and verification pipeline to large-scale interconnected systems. Third, we extend the framework to handle systems with external control inputs, thereby allowing the joint synthesis and verification of neural certificates and controllers. Fourth, we validate our approach in scenarios of mixed-autonomy platoons, drone formations, and microgrids. Numerical simulations show that the proposed framework not only guarantees sISS with minimal degradation in control performance but also efficiently trains and verifies controllers for large-scale interconnected systems under specific practical conditions.         ",
    "url": "https://arxiv.org/abs/2509.10118",
    "authors": [
      "Jingyuan Zhou",
      "Haoze Wu",
      "Haokun Yu",
      "Kaidi Yang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2509.10127",
    "title": "Population-Aligned Persona Generation for LLM-based Social Simulation",
    "abstract": "           Recent advances in large language models (LLMs) have enabled human-like social simulations at unprecedented scale and fidelity, offering new opportunities for computational social science. A key challenge, however, is the construction of persona sets that authentically represent the diversity and distribution of real-world populations. Most existing LLM-based social simulation studies focus primarily on designing agentic frameworks and simulation environments, often overlooking the complexities of persona generation and the potential biases introduced by unrepresentative persona sets. In this paper, we propose a systematic framework for synthesizing high-quality, population-aligned persona sets for LLM-driven social simulation. Our approach begins by leveraging LLMs to generate narrative personas from long-term social media data, followed by rigorous quality assessment to filter out low-fidelity profiles. We then apply importance sampling to achieve global alignment with reference psychometric distributions, such as the Big Five personality traits. To address the needs of specific simulation contexts, we further introduce a task-specific module that adapts the globally aligned persona set to targeted subpopulations. Extensive experiments demonstrate that our method significantly reduces population-level bias and enables accurate, flexible social simulation for a wide range of research and policy applications.         ",
    "url": "https://arxiv.org/abs/2509.10127",
    "authors": [
      "Zhengyu Hu",
      "Zheyuan Xiao",
      "Max Xiong",
      "Yuxuan Lei",
      "Tianfu Wang",
      "Jianxun Lian",
      "Kaize Ding",
      "Ziang Xiao",
      "Nicholas Jing Yuan",
      "Xing Xie"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10140",
    "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization",
    "abstract": "           Vector quantization (VQ) is a key component in discrete tokenizers for image generation, but its training is often unstable due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, which lead to suboptimal reconstruction performance and low codebook usage. In this work, we analyze these fundamental challenges and provide a simple yet effective solution. To maintain high codebook usage in VQ networks (VQN) during learning annealing and codebook size expansion, we propose VQBridge, a robust, scalable, and efficient projector based on the map function method. VQBridge optimizes code vectors through a compress-process-recover pipeline, enabling stable and effective codebook training. By combining VQBridge with learning annealing, our VQN achieves full (100%) codebook usage across diverse codebook configurations, which we refer to as FVQ (FullVQ). Through extensive experiments, we demonstrate that FVQ is effective, scalable, and generalizable: it attains 100% codebook usage even with a 262k-codebook, achieves state-of-the-art reconstruction performance, consistently improves with larger codebooks, higher vector channels, or longer training, and remains effective across different VQ variants. Moreover, when integrated with LlamaGen, FVQ significantly enhances image generation performance, surpassing visual autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID, highlighting the importance of high-quality tokenizers for strong autoregressive image generation.         ",
    "url": "https://arxiv.org/abs/2509.10140",
    "authors": [
      "Yifan Chang",
      "Jie Qin",
      "Limeng Qiao",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Lin Ma",
      "Xingang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.10156",
    "title": "LayerLock: Non-collapsing Representation Learning with Progressive Freezing",
    "abstract": "           We introduce LayerLock, a simple yet effective approach for self-supervised visual representation learning, that gradually transitions from pixel to latent prediction through progressive layer freezing. First, we make the observation that during training of video masked-autoencoding (MAE) models, ViT layers converge in the order of their depth: shallower layers converge early, deeper layers converge late. We then show that this observation can be exploited to accelerate standard MAE by progressively freezing the model according to an explicit schedule, throughout training. Furthermore, this same schedule can be used in a simple and scalable approach to latent prediction that does not suffer from \"representation collapse\". We apply our proposed approach, LayerLock, to large models of up to 4B parameters with results surpassing those of non-latent masked prediction on the 4DS perception suite.         ",
    "url": "https://arxiv.org/abs/2509.10156",
    "authors": [
      "Goker Erdogan",
      "Nikhil Parthasarathy",
      "Catalin Ionescu",
      "Drew Hudson",
      "Alexander Lerchner",
      "Andrew Zisserman",
      "Mehdi Sajjadi",
      "Joao Carreira"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.10162",
    "title": "Online Robust Planning under Model Uncertainty: A Sample-Based Approach",
    "abstract": "           Online planning in Markov Decision Processes (MDPs) enables agents to make sequential decisions by simulating future trajectories from the current state, making it well-suited for large-scale or dynamic environments. Sample-based methods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely adopted for their ability to approximate optimal actions using a generative model. However, in practical settings, the generative model is often learned from limited data, introducing approximation errors that can degrade performance or lead to unsafe behaviors. To address these challenges, Robust MDPs (RMDPs) offer a principled framework for planning under model uncertainty, yet existing approaches are typically computationally intensive and not suited for real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the first online planning algorithm for RMDPs with finite-sample theoretical performance guarantees. Unlike Sparse Sampling, which estimates the nominal value function, RSS computes a robust value function by leveraging the efficiency and theoretical properties of Sample Average Approximation (SAA), enabling tractable robust policy computation in online settings. RSS is applicable to infinite or continuous state spaces, and its sample and computational complexities are independent of the state space size. We provide theoretical performance guarantees and empirically show that RSS outperforms standard Sparse Sampling in environments with uncertain dynamics.         ",
    "url": "https://arxiv.org/abs/2509.10162",
    "authors": [
      "Tamir Shazman",
      "Idan Lev-Yehudi",
      "Ron Benchetit",
      "Vadim Indelman"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.10163",
    "title": "Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks",
    "abstract": "           As sixth-generation (6G) networks move toward ultra-dense, intelligent edge environments, efficient resource management under stringent privacy, mobility, and energy constraints becomes critical. This paper introduces a novel Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that incorporates cross-layer orchestration of both the MAC layer and application layer for energy-efficient, privacy-preserving, and real-time resource management across heterogeneous edge devices. Each agent uses a Deep Recurrent Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum access, and CPU energy adaptation based on local observations (e.g., queue length, energy, CPU usage, and mobility). To protect privacy, we introduce a secure aggregation protocol based on elliptic curve Diffie Hellman key exchange, which ensures accurate model updates without exposing raw data to semi-honest adversaries. We formulate the resource management problem as a partially observable multi-agent Markov decision process (POMMDP) with a multi-objective reward function that jointly optimizes latency, energy efficiency, spectral efficiency, fairness, and reliability under 6G-specific service requirements such as URLLC, eMBB, and mMTC. Simulation results demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines in task success rate, latency, energy efficiency, and fairness, while ensuring robust privacy protection and scalability in dynamic, resource-constrained 6G edge networks.         ",
    "url": "https://arxiv.org/abs/2509.10163",
    "authors": [
      "Francisco Javier Esono Nkulu Andong",
      "Qi Min"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2509.10164",
    "title": "A Symmetry-Integrated Approach to Surface Code Decoding",
    "abstract": "           Quantum error correction, which utilizes logical qubits that are encoded as redundant multiple physical qubits to find and correct errors in physical qubits, is indispensable for practical quantum computing. Surface code is considered to be a promising encoding method with a high error threshold that is defined by stabilizer generators. However, previous methods have suffered from the problem that the decoder acquires solely the error probability distribution because of the non-uniqueness of correct prediction obtained from the input. To circumvent this problem, we propose a technique to reoptimize the decoder model by approximating syndrome measurements with a continuous function that is mathematically interpolated by neural network. We evaluated the improvement in accuracy of a multilayer perceptron based decoder for code distances of 5 and 7 as well as for decoders based on convolutional and recurrent neural networks and transformers for a code distance of 5. In all cases, the reoptimized decoder gave better accuracy than the original models, demonstrating the universal effectiveness of the proposed method that is independent of code distance or network architecture. These results suggest that re-framing the problem of surface code decoding into a regression problem that can be tackled by deep learning is a useful strategy.         ",
    "url": "https://arxiv.org/abs/2509.10164",
    "authors": [
      "Hoshitaro Ohnishi",
      "Hideo Mukai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2509.10173",
    "title": "Secure and Scalable Rerouting in LEO Satellite Networks",
    "abstract": "           Resilient routing in large-scale Low Earth Orbit (LEO) satellite networks remains a key challenge due to frequent and unpredictable link and node failures, potentially in response to cybersecurity breaches. While prior work has explored rerouting strategies with various levels of network awareness, their relative tradeoffs under dynamic failure conditions remain underexplored. In this work, we extend the Deep Space Network Simulator (DSNS) to systematically compare three rerouting paradigms, each differing in the scope of failure knowledge available to each node. We compare local neighbor-based, segment-based and global-knowledge-based rerouting as well as a naive source routing solution that is unaware of failures. Our main goal is to evaluate how the breadth of failure awareness impacts routing performance and resilience under failures, both random and targeted. We measure delivery ratio, latency, rerouting overhead, and loop occurrence. Our findings show the potential of segment-based rerouting to achieve a favorable tradeoff between local responsiveness and global coordination, offering resilience benefits with minimal overhead--insights that can inform future fault-tolerant satellite network design.         ",
    "url": "https://arxiv.org/abs/2509.10173",
    "authors": [
      "Lyubomir Yanev",
      "Pietro Ronchetti",
      "Joshua Smailes",
      "Martin Strohmeier"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2509.10182",
    "title": "On arc-density of pushably $3$-critical oriented graphs",
    "abstract": "           An oriented graph $\\overrightarrow{G}$ is pushably $k$-critical if it is not pushably $k$-colorable, but every proper subgraph of $\\overrightarrow{G}$ is. The main result of this article is that every pushably $3$-critical oriented graph on $n$ vertices, but for four exceptions, has at least $\\frac{15n+2}{13}$ arcs, and that this bound is tight. As an application of this result, we show that the class of oriented graphs with maximum average degree strictly less than $\\frac{30}{13}$ and girth at least $5$, which includes all oriented planar and projective planar graphs with girth at least $15$, have pushable chromatic number at most $3$. Moreover, we provide an exhaustive list of pushably $3$-critical graphs with maximum average degree equal to $\\frac{30}{13}$ and a pushably $3$-critical orientation of a $4$-cycle to prove the tightness of our bound with respect to both maximum average degree and girth. We also show that these classes of oriented graphs admit a homomorphism to an oriented planar graph on six vertices (an orientation of $K_{2,2,2}$) which (tightly) improves a result due to Borodin \\textit{et al.} [Discrete Mathematics 1998]. Furthermore, for these classes of oriented graphs, we prove that the $2$-dipath $L(p,q)$ and the oriented $L(p,q)$ spans are upper bounded by $2p+3q$ for all $q \\leq p$. All these implications improve previously known results.         ",
    "url": "https://arxiv.org/abs/2509.10182",
    "authors": [
      "Tapas Das",
      "Pavan P D",
      "Sagnik Sen",
      "S Taruni"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2509.10186",
    "title": "P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context",
    "abstract": "           We present a scalable framework for learning deterministic and probabilistic neural surrogates for high-resolution 3D physics simulations. We introduce a hybrid CNN-Transformer backbone architecture targeted for 3D physics simulations, which significantly outperforms existing architectures in terms of speed and accuracy. Our proposed network can be pretrained on small patches of the simulation domain, which can be fused to obtain a global solution, optionally guided via a fast and scalable sequence-to-sequence model to include long-range dependencies. This setup allows for training large-scale models with reduced memory and compute requirements for high-resolution datasets. We evaluate our backbone architecture against a large set of baseline methods with the objective to simultaneously learn the dynamics of 14 different types of PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate the versatility of our network by training it as a diffusion model to produce probabilistic samples of highly turbulent 3D channel flows across varying Reynolds numbers, accurately capturing the underlying flow statistics.         ",
    "url": "https://arxiv.org/abs/2509.10186",
    "authors": [
      "Benjamin Holzschuh",
      "Georg Kohl",
      "Florian Redinger",
      "Nils Thuerey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10188",
    "title": "Constant Time with Minimal Preprocessing, a Robust and Extensive Complexity Class",
    "abstract": "           In this paper, we study the class $\\mathtt{cstPP}$ of operations $\\mathtt{op}: \\mathbb{N}^k\\to\\mathbb{N}$, of any fixed arity $k\\ge 1$, satisfying the following property: for each fixed integer $d\\ge 1$, there exists an algorithm for a RAM machine which, for any input integer $N\\ge 2$, - pre-computes some tables in $O(N)$ time, - then reads $k$ operands $x_1,\\ldots,x_k<N^d$ and computes $\\mathtt{op}(x_1,\\dots,x_k)$ in constant time. We show that the $\\mathtt{cstPP}$ class is robust and extensive and satisfies several closure properties. It is invariant depending on whether the set of primitive operations of the RAM is $\\{+\\}$, or $\\{+,-,\\times,\\mathtt{div},\\mathtt{mod}\\}$, or any set of operations in $\\mathtt{cstPP}$ provided it includes $+$. We prove that the $\\mathtt{cstPP}$ class is closed under composition and, for fast-growing functions, is closed under inverse. We also show that in the definition of $\\mathtt{cstPP}$ the constant-time procedure can be reduced to a single return instruction. Finally, we establish that linear preprocessing time is not essential in the definition of the $\\mathtt{cstPP}$ class: this class is not modified if the preprocessing time is increased to $O(N^c)$, for any fixed $c>1$, or conversely, is reduced to $N^{\\varepsilon}$, for any positive $\\varepsilon<1$ (provided the set of primitive operation includes $+$, $\\mathtt{div}$ and $\\mathtt{mod}$). To complete the picture, we demonstrate that the $\\mathtt{cstPP}$ class degenerates if the preprocessing time reduces to $N^{o(1)}$.         ",
    "url": "https://arxiv.org/abs/2509.10188",
    "authors": [
      "\u00c9tienne Grandjean",
      "Louis Jachiet"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2509.10206",
    "title": "Investigating Feature Attribution for 5G Network Intrusion Detection",
    "abstract": "           With the rise of fifth-generation (5G) networks in critical applications, it is urgent to move from detection of malicious activity to systems capable of providing a reliable verdict suitable for mitigation. In this regard, understanding and interpreting machine learning (ML) models' security alerts is crucial for enabling actionable incident response orchestration. Explainable Artificial Intelligence (XAI) techniques are expected to enhance trust by providing insights into why alerts are raised. A dominant approach statistically associates feature sets that can be correlated to a given alert. This paper starts by questioning whether such attribution is relevant for future generation communication systems, and investigates its merits in comparison with an approach based on logical explanations. We extensively study two methods, SHAP and VoTE-XAI, by analyzing their interpretations of alerts generated by an XGBoost model in three different use cases with several 5G communication attacks. We identify three metrics for assessing explanations: sparsity, how concise they are; stability, how consistent they are across samples from the same attack type; and efficiency, how fast an explanation is generated. As an example, in a 5G network with 92 features, 6 were deemed important by VoTE-XAI for a Denial of Service (DoS) variant, ICMPFlood, while SHAP identified over 20. More importantly, we found a significant divergence between features selected by SHAP and VoTE-XAI. However, none of the top-ranked features selected by SHAP were missed by VoTE-XAI. When it comes to efficiency of providing interpretations, we found that VoTE-XAI is significantly more responsive, e.g. it provides a single explanation in under 0.002 seconds, in a high-dimensional setting (478 features).         ",
    "url": "https://arxiv.org/abs/2509.10206",
    "authors": [
      "Federica Uccello",
      "Simin Nadjm-Tehrani"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10213",
    "title": "Dynamic Vulnerability Patching for Heterogeneous Embedded Systems Using Stack Frame Reconstruction",
    "abstract": "           Existing dynamic vulnerability patching techniques are not well-suited for embedded devices, especially mission-critical ones such as medical equipment, as they have limited computational power and memory but uninterrupted service requirements. Those devices often lack sufficient idle memory for dynamic patching, and the diverse architectures of embedded systems further complicate the creation of patch triggers that are compatible across various system kernels and hardware platforms. To address these challenges, we propose a hot patching framework called StackPatch that facilitates patch development based on stack frame reconstruction. StackPatch introduces different triggering strategies to update programs stored in memory units. We leverage the exception-handling mechanisms commonly available in embedded processors to enhance StackPatch's adaptability across different processor architectures for control flow redirection. We evaluated StackPatch on embedded devices featuring three major microcontroller (MCU) architectures: ARM, RISC-V, and Xtensa. In the experiments, we used StackPatch to successfully fix 102 publicly disclosed vulnerabilities in real-time operating systems (RTOS). We applied patching to medical devices, soft programmable logic controllers (PLCs), and network services, with StackPatch consistently completing each vulnerability remediation in less than 260 MCU clock cycles.         ",
    "url": "https://arxiv.org/abs/2509.10213",
    "authors": [
      "Ming Zhou",
      "Xupu Hu",
      "Zhihao Wang",
      "Haining Wang",
      "Hui Wen",
      "Limin Sun",
      "Peng Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.10214",
    "title": "Friend or Foe? Identifying Anomalous Peers in Moneros P2P Network",
    "abstract": "           Monero, the leading privacy-focused cryptocurrency, relies on a peer-to-peer (P2P) network to propagate transactions and blocks. Growing evidence suggests that non-standard nodes exist in the network, posing as honest nodes but are perhaps intended for monitoring the network and spying on other nodes. However, our understanding of the detection and analysis of anomalous peer behavior remains limited. This paper presents a first comprehensive study of anomalous behavior in Monero's P2P network. To this end, we collected and analyzed over 240 hours of network traffic captured from five distinct vantage points worldwide. We further present a formal framework which allows us to analytically define and classify anomalous patterns in P2P cryptocurrency networks. Our detection methodology, implemented as an offline analysis, provides a foundation for real-time monitoring systems. Our analysis reveals the presence of non-standard peers in the network where approximately 14.74% (13.19%) of (reachable) peers in the network exhibit non-standard behavior. These peers exhibit distinct behavioral patterns that might suggest multiple concurrent attacks, pointing to substantial shortcomings in Monero's privacy guarantees and network decentralization. To support reproducibility and enable network operators to protect themselves, we release our examination pipeline to identify and block suspicious peers based on newly captured network traffic.         ",
    "url": "https://arxiv.org/abs/2509.10214",
    "authors": [
      "Yannik Kopyciok",
      "Stefan Schmid",
      "Friedhelm Victor"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2509.10240",
    "title": "Cooperative Base Station Assignment and Resource Allocation for 6G ISAC Network",
    "abstract": "           In the upcoming 6G networks, integrated sensing and communications (ISAC) will be able to provide a performance boost in both perception and wireless connectivity. This paper considers a multiple base station (BS) architecture to support the comprehensive services of data transmission and multi-target sensing. In this context, a cooperative BS assignment and resource allocation (CBARA) strategy is proposed in this paper, aiming at jointly optimizing the communication and sensing (C&S) performance. The posterior Cramer-Rao lower bound and the achievable rate with respect to transmit power and bandwidth are derived and utilized as optimization criteria for the CBARA scheme. We develop a heuristic alternating optimization algorithm to obtain an effective sub-optimal solution for the non-convex optimization problem caused by multiple coupled variables. Numerical results show the effectiveness of the proposed solution, which achieves a performance improvement of 117% in communication rate and 40% in sensing accuracy, compared to the classic scheme.         ",
    "url": "https://arxiv.org/abs/2509.10240",
    "authors": [
      "Jiajia Liao",
      "Luping Xiang",
      "Shida Zhong",
      "Lixia Xiao",
      "Haochen Liu",
      "Kun Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2509.10248",
    "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications",
    "abstract": "           The ongoing intense discussion on rising LLM usage in the scientific peer-review process has recently been mingled by reports of authors using hidden prompt injections to manipulate review scores. Since the existence of such \"attacks\" - although seen by some commentators as \"self-defense\" - would have a great impact on the further debate, this paper investigates the practicability and technical success of the described manipulations. Our systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide range of LLMs shows two distinct results: I) very simple prompt injections are indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews are generally biased toward acceptance (>95% in many models). Both results have great impact on the ongoing discussions on LLM usage in peer-review.         ",
    "url": "https://arxiv.org/abs/2509.10248",
    "authors": [
      "Janis Keuper"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10250",
    "title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection",
    "abstract": "           With generative models becoming increasingly sophisticated and diverse, detecting AI-generated images has become increasingly challenging. While existing AI-genereted Image detectors achieve promising performance on in-distribution generated images, their generalization to unseen generative models remains limited. This limitation is largely attributed to their reliance on generation-specific artifacts, such as stylistic priors and compression patterns. To address these limitations, we propose GAMMA, a novel training framework designed to reduce domain bias and enhance semantic alignment. GAMMA introduces diverse manipulation strategies, such as inpainting-based manipulation and semantics-preserving perturbations, to ensure consistency between manipulated and authentic content. We employ multi-task supervision with dual segmentation heads and a classification head, enabling pixel-level source attribution across diverse generative domains. In addition, a reverse cross-attention mechanism is introduced to allow the segmentation heads to guide and correct biased representations in the classification branch. Our method achieves state-of-the-art generalization performance on the GenImage benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on newly released generative model such as GPT-4o.         ",
    "url": "https://arxiv.org/abs/2509.10250",
    "authors": [
      "Haozhen Yan",
      "Yan Hong",
      "Suning Lang",
      "Jiahui Zhan",
      "Yikun Ji",
      "Yujie Gao",
      "Jun Lan",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Jianfu Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.10252",
    "title": "ExDoS: Expert-Guided Dual-Focus Cross-Modal Distillation for Smart Contract Vulnerability Detection",
    "abstract": "           The success of smart contracts has made them a target for attacks, but their closed-source nature often forces vulnerability detection to work on bytecode, which is inherently more challenging than source-code-based analysis. While recent studies try to align source and bytecode embeddings during training to transfer knowledge, current methods rely on graph-level alignment that obscures fine-grained structural and semantic correlations between the two modalities. Moreover, the absence of precise vulnerability patterns and granular annotations in bytecode leads to depriving the model of crucial supervisory signals for learning discriminant features. We propose ExDoS to transfer rich semantic knowledge from source code to bytecode, effectively supplementing the source code prior in practical settings. Specifically, we construct semantic graphs from source code and control-flow graphs from bytecode. To address obscured local signals in graph-level contract embeddings, we propose a Dual-Attention Graph Network introducing a novel node attention aggregation module to enhance local pattern capture in graph embeddings. Furthermore, by summarizing existing source code vulnerability patterns and designing a corresponding set of bytecode-level patterns for each, we construct the first dataset of vulnerability pattern annotations aligned with source code definitions to facilitate fine-grained cross-modal alignment and the capture of function-level vulnerability signals. Finally, we propose a dual-focus objective for our cross-modal distillation framework, comprising: a Global Semantic Distillation Loss for transferring graph-level knowledge and a Local Semantic Distillation Loss enabling expert-guided, fine-grained vulnerability-specific distillation. Experiments on real-world contracts demonstrate that our method achieves consistent F1-score improvements (3\\%--6\\%) over strong baselines.         ",
    "url": "https://arxiv.org/abs/2509.10252",
    "authors": [
      "Yifan Jia",
      "Ye Tian",
      "Yanbin Wang",
      "Jianguo Sun",
      "Haitao Xu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.10257",
    "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI",
    "abstract": "           Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce motion artifacts caused by fetal movement. However, these stacks are typically low resolution, may suffer from motion corruption, and do not adequately capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to address these limitations by combining slice-to-volume registration and super-resolution techniques to generate high-resolution (HR) 3D volumes. While several SRR methods have been proposed, their comparative performance - particularly in pathological cases - and their influence on downstream volumetric analysis and diagnostic tasks remain underexplored. In this study, we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to 140 fetal brain MRI scans, including both healthy controls (HC) and pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was segmented using the BoUNTi algorithm to extract volumes of nine principal brain structures. We evaluated visual quality, SRR success rates, volumetric measurement agreement, and diagnostic classification performance. NeSVoR demonstrated the highest and most consistent reconstruction success rate (>90%) across both HC and PC groups. Although significant differences in volumetric estimates were observed between SRR methods, classification performance for VM was not affected by the choice of SRR method. These findings highlight NeSVoR's robustness and the resilience of diagnostic performance despite SRR-induced volumetric variability.         ",
    "url": "https://arxiv.org/abs/2509.10257",
    "authors": [
      "Ema Masterl",
      "Tina Vipotnik Vesnaver",
      "\u017diga \u0160piclin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.10272",
    "title": "TubeBEND: A Real-World Dataset for Geometry Prediction in Rotary Draw Bending",
    "abstract": "           This paper presents TubeBEND, a real-world dataset comprising 318 rotary tube bending processes, which were collected and sorted by experts from various fields to evaluate machine learning and signal analysis methods. The dataset addresses the industrial challenge of predicting the geometry of a first-stage bend, which can be beneficial for designing machine clamping molds for the second-stage bend in two-stage rotary draw bending. Some geometry criteria, such as the tube's final bent angle (or springback) and its cross-sectional deformation, are being recorded in this dataset. This dataset gives us the possibility to build and test machine learning models that can predict the geometry and help the machine operators with a better machine setup to optimize the tube's springback and deformation. Moreover, by recording some process parameters, such as tool movements and forces or torques applied to them, we deliver detailed information about their impacts on the final tube geometry. The focus of our work is to discover solutions that can replace traditional methods, such as trial-and-error or simulation-based predictions, by including experimental process variables in ML algorithms. Our dataset is publicly available at this https URL and this https URL as a benchmark to improve data-driven methods in this field.         ",
    "url": "https://arxiv.org/abs/2509.10272",
    "authors": [
      "Zeyneddin Oz",
      "Jonas Knoche",
      "Alireza Yazdani",
      "Bernd Engel",
      "Kristof Van Laerhoven"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2509.10273",
    "title": "Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning",
    "abstract": "           Ionic liquids (ILs) have emerged as versatile replacements for traditional solvents because their physicochemical properties can be precisely tailored to various applications. However, accurately predicting key thermophysical properties remains challenging due to the vast chemical design space and the limited availability of experimental data. In this study, we present a data-driven transfer learning framework that leverages a neural recommender system (NRS) to enable reliable property prediction for ILs using sparse experimental datasets. The approach involves a two-stage process: first, pre-training NRS models on COSMO-RS-based simulated data at fixed temperature and pressure to learn property-specific structural embeddings for cations and anions; and second, fine-tuning simple feedforward neural networks using these embeddings with experimental data at varying temperatures and pressures. In this work, five essential IL properties are considered: density, viscosity, surface tension, heat capacity, and melting point. The framework supports both within-property and cross-property knowledge transfer. Notably, pre-trained models for density, viscosity, and heat capacity are used to fine-tune models for all five target properties, achieving improved performance by a substantial margin for four of them. The model exhibits robust extrapolation to previously unseen ILs. Moreover, the final trained models enable property prediction for over 700,000 IL combinations, offering a scalable solution for IL screening in process design. This work highlights the effectiveness of combining simulated data and transfer learning to overcome sparsity in the experimental data.         ",
    "url": "https://arxiv.org/abs/2509.10273",
    "authors": [
      "Sahil Sethi",
      "Kai Sundmacher",
      "Caroline Ganzer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10280",
    "title": "Large-scale Aerial Reconfigurable Intelligent Surface-aided Robust Anti-jamming Transmission",
    "abstract": "           Aerial reconfigurable intelligent surfaces (ARIS), deployed on unmanned aerial vehicles (UAVs), could enhance anti-jamming communication performance by dynamically configuring channel conditions and establishing reliable air-ground links. However, large-scale ARIS faces critical deployment challenges due to the prohibitive computational complexity of conventional discrete optimization methods and sophisticated jamming threats. In this paper, we introduce a mean field modeling approach to design the spatial configuration of ARIS by a continuous density function, thus bypassing high-dimensional combinatorial optimization. We consider an adaptive jammer which adjusts its position and beamforming to minimize the sum-rate. A key finding reveals that the jammer's optimal strategy is governed by a proximity-directivity trade-off between reducing path loss and enhancing spatial focusing. To combat the jamming, we propose a robust anti-jamming transmission framework that jointly optimizes the BS beamforming, the ARIS reflection, and the ARIS spatial distribution to maximize the worst-case sum-rate. By leveraging variational optimization and Riemannian manifold methods, we efficiently solve the functional optimization problems. Our analysis further unveils that the optimal ARIS deployment follows a spatial water-filling principle, concentrating resources in high-gain regions while avoiding interference-prone areas. Simulation results demonstrate that the proposed framework remarkably improves the sum-rate. Furthermore, the computational complexity of the proposed algorithm is independent of the number of UAVs, validating its effectiveness for scalable ARIS-assisted anti-jamming communications.         ",
    "url": "https://arxiv.org/abs/2509.10280",
    "authors": [
      "Junshan Luo",
      "Shilian Wang",
      "Boxiang He"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2509.10282",
    "title": "MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection",
    "abstract": "           Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects without relying on labeled training data, making it especially valuable in scenarios constrained by data scarcity, privacy, or high annotation cost. However, most existing methods focus exclusively on point clouds, neglecting the rich semantic cues available from complementary modalities such as RGB images and texts priors. This paper introduces MCL-AD, a novel framework that leverages multimodal collaboration learning across point clouds, RGB images, and texts semantics to achieve superior zero-shot 3D anomaly detection. Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that enhances the intra-modal representation capability and inter-modal collaborative learning by introducing an object-agnostic decoupled text prompt and a multimodal contrastive loss. In addition, a collaborative modulation mechanism (CMM) is proposed to fully leverage the complementary representations of point clouds and RGB images by jointly modulating the RGB image-guided and point cloud-guided branches. Extensive experiments demonstrate that the proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D anomaly detection.         ",
    "url": "https://arxiv.org/abs/2509.10282",
    "authors": [
      "Gang Li",
      "Tianjiao Chen",
      "Mingle Zhou",
      "Min Li",
      "Delong Han",
      "Jin Wan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10284",
    "title": "A Holistic Architecture for Monitoring and Optimization of Robust Multi-Agent Path Finding Plan Execution",
    "abstract": "           The goal of Multi-Agent Path Finding (MAPF) is to find a set of paths for a fleet of agents moving in a shared environment such that the agents reach their goals without colliding with each other. In practice, some of the robots executing the plan may get delayed, which can introduce collision risk. Although robust execution methods are used to ensure safety even in the presence of delays, the delays may still have a significant impact on the duration of the execution. At some point, the accumulated delays may become significant enough that instead of continuing with the execution of the original plan, even if it was optimal, there may now exist an alternate plan which will lead to a shorter execution. However, the problem is how to decide when to search for the alternate plan, since it is a costly procedure. In this paper, we propose a holistic architecture for robust execution of MAPF plans, its monitoring and optimization. We exploit a robust execution method called Action Dependency Graph to maintain an estimate of the expected execution duration during the plan's execution. This estimate is used to predict the potential that finding an alternate plan would lead to shorter execution. We empirically evaluate the architecture in experiments in a real-time simulator which we designed to mimic our real-life demonstrator of an autonomous warehouse robotic fleet.         ",
    "url": "https://arxiv.org/abs/2509.10284",
    "authors": [
      "David Zahr\u00e1dka",
      "Denisa Mu\u017e\u00edkov\u00e1",
      "David Woller",
      "Miroslav Kulich",
      "Ji\u0159\u00ed \u0160vancara",
      "Roman Bart\u00e1k"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.10287",
    "title": "URL2Graph++: Unified Semantic-Structural-Character Learning for Malicious URL Detection",
    "abstract": "           Malicious URL detection remains a major challenge in cybersecurity, primarily due to two factors: (1) the exponential growth of the Internet has led to an immense diversity of URLs, making generalized detection increasingly difficult; and (2) attackers are increasingly employing sophisticated obfuscation techniques to evade detection. We advocate that addressing these challenges fundamentally requires: (1) obtaining semantic understanding to improve generalization across vast and diverse URL sets, and (2) accurately modeling contextual relationships within the structural composition of URLs. In this paper, we propose a novel malicious URL detection method combining multi-granularity graph learning with semantic embedding to jointly capture semantic, character-level, and structural features for robust URL analysis. To model internal dependencies within URLs, we first construct dual-granularity URL graphs at both subword and character levels, where nodes represent URL tokens/characters and edges encode co-occurrence relationships. To obtain fine-grained embeddings, we initialize node representations using a character-level convolutional network. The two graphs are then processed through jointly trained Graph Convolutional Networks to learn consistent graph-level representations, enabling the model to capture complementary structural features that reflect co-occurrence patterns and character-level dependencies. Furthermore, we employ BERT to derive semantic representations of URLs for semantically aware understanding. Finally, we introduce a gated dynamic fusion network to combine the semantically enriched BERT representations with the jointly optimized graph vectors, further enhancing detection performance. We extensively evaluate our method across multiple challenging dimensions. Results show our method exceeds SOTA performance, including against large language models.         ",
    "url": "https://arxiv.org/abs/2509.10287",
    "authors": [
      "Ye Tian",
      "Yifan Jia",
      "Yanbin Wang",
      "Jianguo Sun",
      "Zhiquan Liu",
      "Xiaowen Ling"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.10298",
    "title": "Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks",
    "abstract": "           Deep neural networks and Vision Transformers achieve state-of-the-art performance in computer vision but are highly vulnerable to adversarial perturbations. Standard defenses often incur high computational cost or lack formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath) method, where drop probabilities increase with depth to control the effective Lipschitz constant of the network. This approach regularizes deeper layers, improving robustness while preserving clean accuracy and reducing computation. Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent schedule maintains near-baseline clean accuracy, enhances robustness under FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to baseline and linear DropPath schedules.         ",
    "url": "https://arxiv.org/abs/2509.10298",
    "authors": [
      "Laith Nayal",
      "Mahmoud Mousatat",
      "Bader Rasheed"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.10305",
    "title": "GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning",
    "abstract": "           In dynamic and uncertain environments, robotic path planning demands accurate spatiotemporal environment understanding combined with robust decision-making under partial observability. However, current deep reinforcement learning-based path planning methods face two fundamental limitations: (1) insufficient modeling of multi-scale temporal dependencies, resulting in suboptimal adaptability in dynamic scenarios, and (2) inefficient exploration-exploitation balance, leading to degraded path quality. To address these challenges, we propose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic Path Planning. The framework comprises two key modules: (i) the Spatiotemporal Perception module, which hierarchically extracts multi-granularity spatial features and multi-scale temporal dependencies ranging from instantaneous to extended time horizons, thereby improving perception accuracy in dynamic environments; and (ii) the Adaptive Policy Optimization module, which balances exploration and exploitation during training while optimizing for smoothness and collision probability through constrained policy updates. Experiments in dynamic environments demonstrate that GundamQ achieves a 15.3\\% improvement in success rate and a 21.7\\% increase in overall path quality, significantly outperforming existing state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2509.10305",
    "authors": [
      "Yutong Shen",
      "Ruizhe Xia",
      "Bokai Yan",
      "Shunqi zhang",
      "Pengrui Xiang",
      "Sicheng He",
      "Yixin Xu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.10308",
    "title": "GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction",
    "abstract": "           In the aftermath of disasters, many institutions worldwide face challenges in continually monitoring changes in disaster risk, limiting the ability of key decision-makers to assess progress towards the UN Sendai Framework for Disaster Risk Reduction 2015-2030. While numerous efforts have substantially advanced the large-scale modeling of hazard and exposure through Earth observation and data-driven methods, progress remains limited in modeling another equally important yet challenging element of the risk equation: physical vulnerability. To address this gap, we introduce Graph Categorical Structured Variational Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for modeling physical vulnerability by integrating deep learning, graph representation, and categorical probabilistic inference, using time-series satellite-derived datasets and prior expert belief systems. We introduce a weakly supervised first-order transition matrix that reflects the changes in the spatiotemporal distribution of physical vulnerability in two disaster-stricken and socioeconomically disadvantaged areas: (1) the cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the mudslide-affected city of Freetown in Sierra Leone. Our work reveals post-disaster regional dynamics in physical vulnerability, offering valuable insights into localized spatiotemporal auditing and sustainable strategies for post-disaster risk reduction.         ",
    "url": "https://arxiv.org/abs/2509.10308",
    "authors": [
      "Joshua Dimasaka",
      "Christian Gei\u00df",
      "Robert Muir-Wood",
      "Emily So"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10333",
    "title": "Revealing Higher-Order Interactions in Complex Networks: A U.S. Diplomacy Case Study",
    "abstract": "           Although diplomatic communication has long been examined in the social sciences, its network structure remains underexplored. Using the U.S. diplomatic cables released by WikiLeaks in 2010 as a case study, we adopt a network-science perspective. We represent diplomatic interactions as a hypergraph and develop a general, random-walk-based pipeline to evaluate this representation against traditional pairwise graphs. We further evaluate the pipeline on legislative co-sponsorship and organizational email data, finding improvements and empirical evidence that clarifies when hypergraph modeling is preferable to pairwise graphs. Overall, hypergraphs paired with appropriately specified random-walk dynamics more faithfully capture higher-order, group-based interactions, yielding a richer structural account of diplomacy and superior performance on interaction-prediction tasks that enables inferring new diplomatic relationships from existing patterns.         ",
    "url": "https://arxiv.org/abs/2509.10333",
    "authors": [
      "Arthur Rondeau",
      "Didier Wernli",
      "Roland Bouffanais"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2509.10338",
    "title": "Trusted Repeater Placement in QKD-enabled Optical Networks",
    "abstract": "           Quantum Key Distribution (QKD) provides information-theoretic security, but is limited by distance in optical networks, thereby requiring repeater nodes to extend coverage. Existing works usually assume all repeater nodes and associated Key Management Servers (KMSs) to be Trusted Repeater Nodes (TRNs), while ignoring risks from software exploits and insider threats. In this paper, we propose a reliability-aware TRN placement framework for metro optical networks, which assigns each node a trust score and integrates it into the Dijkstra algorithm via weighted links. We then rank the nodes using a composite score, which is a weighted combination of betweenness centrality and eigenvector centrality to enable a secure and scalable TRN deployment. Simulation results on a reference topology show that our method covers 10.77% more shortest paths compared to traditional metrics like degree centrality, using the same number (around eight) of TRNs, making it suitable for TRN selection to maximize secure connectivity.         ",
    "url": "https://arxiv.org/abs/2509.10338",
    "authors": [
      "Arup Kumar Marik",
      "Basabdatta Palit",
      "Sadananda Behera"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2509.10359",
    "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention",
    "abstract": "           Recent advances in text-based image editing have enabled fine-grained manipulation of visual content guided by natural language. However, such methods are susceptible to adversarial attacks. In this work, we propose a novel attack that targets the visual component of editing methods. We introduce Attention Attack, which disrupts the cross-attention between a textual prompt and the visual representation of the image by using an automatically generated caption of the source image as a proxy for the edit prompt. This breaks the alignment between the contents of the image and their textual description, without requiring knowledge of the editing method or the editing prompt. Reflecting on the reliability of existing metrics for immunization success, we propose two novel evaluation strategies: Caption Similarity, which quantifies semantic consistency between original and adversarial edits, and semantic Intersection over Union (IoU), which measures spatial layout disruption via segmentation masks. Experiments conducted on the TEDBench++ benchmark demonstrate that our attack significantly degrades editing performance while remaining imperceptible.         ",
    "url": "https://arxiv.org/abs/2509.10359",
    "authors": [
      "Matteo Trippodo",
      "Federico Becattini",
      "Lorenzo Seidenari"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.10401",
    "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems",
    "abstract": "           Failure attribution in multi-agent systems -- pinpointing the exact step where a decisive error occurs -- is a critical yet unsolved challenge. Current methods treat this as a pattern recognition task over long conversation logs, leading to critically low step-level accuracy (below 17\\%), which renders them impractical for debugging complex systems. Their core weakness is a fundamental inability to perform robust counterfactual reasoning: to determine if correcting a single action would have actually averted the task failure. To bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P) Scaffolding, a novel agent framework that transforms failure attribution from pattern recognition into a structured causal inference task. A2P explicitly guides a large language model through a formal three-step reasoning process within a single inference pass: (1) Abduction, to infer the hidden root causes behind an agent's actions; (2) Action, to define a minimal corrective intervention; and (3) Prediction, to simulate the subsequent trajectory and verify if the intervention resolves the failure. This structured approach leverages the holistic context of the entire conversation while imposing a rigorous causal logic on the model's analysis. Our extensive experiments on the Who\\&When benchmark demonstrate its efficacy. On the Algorithm-Generated dataset, A2P achieves 47.46\\% step-level accuracy, a 2.85$\\times$ improvement over the 16.67\\% of the baseline. On the more complex Hand-Crafted dataset, it achieves 29.31\\% step accuracy, a 2.43$\\times$ improvement over the baseline's 12.07\\%. By reframing the problem through a causal lens, A2P Scaffolding provides a robust, verifiable, and significantly more accurate solution for automated failure attribution.         ",
    "url": "https://arxiv.org/abs/2509.10401",
    "authors": [
      "Alva West",
      "Yixuan Weng",
      "Minjun Zhu",
      "Zhen Lin",
      "Yue Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.10402",
    "title": "Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality",
    "abstract": "           Large Language Models (LLMs) are becoming integral to modern software development workflows, assisting developers with code generation, API explanation, and iterative problem-solving through natural language conversations. Despite widespread adoption, there is limited understanding of how developers interact with LLMs in practice and how these conversational dynamics influence task outcomes, code quality, and software engineering workflows. To address this, we leverage CodeChat, a large dataset comprising 82,845 real-world developer-LLM conversations, containing 368,506 code snippets generated across over 20 programming languages, derived from the WildChat dataset. We find that LLM responses are substantially longer than developer prompts, with a median token-length ratio of 14:1. Multi-turn conversations account for 68% of the dataset and often evolve due to shifting requirements, incomplete prompts, or clarification requests. Topic analysis identifies web design (9.6% of conversations) and neural network training (8.7% of conversations) as the most frequent LLM-assisted tasks. Evaluation across five languages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and language-specific issues in LLM-generated code: generated Python and JavaScript code often include undefined variables (83.4% and 75.3% of code snippets, respectively); Java code lacks required comments (75.9%); C++ code frequently omits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a conversation, syntax and import errors persist across turns; however, documentation quality in Java improves by up to 14.7%, and import handling in Python improves by 3.7% over 5 turns. Prompts that point out mistakes in code generated in prior turns and explicitly request a fix are most effective for resolving errors.         ",
    "url": "https://arxiv.org/abs/2509.10402",
    "authors": [
      "Suzhen Zhong",
      "Ying Zou",
      "Bram Adams"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.10405",
    "title": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States",
    "abstract": "           We introduce a model for monocular RGB relative pose estimation of a ground robot that trains from scratch without pose labels nor prior knowledge about the robot's shape or appearance. At training time, we assume: (i) a robot fitted with multiple LEDs, whose states are independent and known at each frame; (ii) knowledge of the approximate viewing direction of each LED; and (iii) availability of a calibration image with a known target distance, to address the ambiguity of monocular depth estimation. Training data is collected by a pair of robots moving randomly without needing external infrastructure or human supervision. Our model trains on the task of predicting from an image the state of each LED on the robot. In doing so, it learns to predict the position of the robot in the image, its distance, and its relative bearing. At inference time, the state of the LEDs is unknown, can be arbitrary, and does not affect the pose estimation performance. Quantitative experiments indicate that our approach: is competitive with SoA approaches that require supervision from pose labels or a CAD model of the robot; generalizes to different domains; and handles multi-robot pose estimation.         ",
    "url": "https://arxiv.org/abs/2509.10405",
    "authors": [
      "Nicholas Carlotti",
      "Mirko Nava",
      "Alessandro Giusti"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.10419",
    "title": "Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining",
    "abstract": "           Ensuring the resilience of computer-based railways is increasingly crucial to account for uncertainties and changes due to the growing complexity and criticality of those systems. Although their software relies on strict verification and validation processes following well-established best-practices and certification standards, anomalies can still occur at run-time due to residual faults, system and environmental modifications that were unknown at design-time, or other emergent cyber-threat scenarios. This paper explores run-time control-flow anomaly detection using process mining to enhance the resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European Train Control System Level 2). Process mining allows learning the actual control flow of the system from its execution traces, thus enabling run-time monitoring through online conformance checking. In addition, anomaly localization is performed through unsupervised machine learning to link relevant deviations to critical system components. We test our approach on a reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its capability to detect and localize anomalies with high accuracy, efficiency, and explainability.         ",
    "url": "https://arxiv.org/abs/2509.10419",
    "authors": [
      "Francesco Vitale",
      "Tommaso Zoppi",
      "Francesco Flammini",
      "Nicola Mazzocca"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10426",
    "title": "DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training",
    "abstract": "           Trajectory prediction is a critical component of autonomous driving, essential for ensuring both safety and efficiency on the road. However, traditional approaches often struggle with the scarcity of labeled data and exhibit suboptimal performance in multi-agent prediction scenarios. To address these challenges, we introduce a disentangled context-aware pre-training framework for multi-agent motion prediction, named DECAMP. Unlike existing methods that entangle representation learning with pretext tasks, our framework decouples behavior pattern learning from latent feature reconstruction, prioritizing interpretable dynamics and thereby enhancing scene representation for downstream prediction. Additionally, our framework incorporates context-aware representation learning alongside collaborative spatial-motion pretext tasks, which enables joint optimization of structural and intentional reasoning while capturing the underlying dynamic intentions. Our experiments on the Argoverse 2 benchmark showcase the superior performance of our method, and the results attained underscore its effectiveness in multi-agent motion forecasting. To the best of our knowledge, this is the first context autoencoder framework for multi-agent motion forecasting in autonomous driving. The code and models will be made publicly available.         ",
    "url": "https://arxiv.org/abs/2509.10426",
    "authors": [
      "Jianxin Shi",
      "Zengqi Peng",
      "Xiaolong Chen",
      "Tianyu Wo",
      "Jun Ma"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2509.10446",
    "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL",
    "abstract": "           Augmenting large language models (LLMs) with browsing tools substantially improves their potential as deep search agents to solve complex, real-world tasks. Yet, open LLMs still perform poorly in such settings due to limited long-horizon reasoning capacity with browsing tools and the lack of sufficiently difficult supervised data. To address these challenges, we present DeepDive to advance deep search agents. First, we propose a strategy to automatically synthesize complex, difficult, and hard-to-find questions from open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement learning (RL) to enhance LLMs' long-horizon reasoning with deep search. Experiments show that DeepDive-32B achieves a new open-source competitive result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL training improves deep search ability and significantly contributes to the performance improvements across multiple benchmarks. We observe that DeepDive enables test-time scaling of tool calls and parallel sampling. All datasets, models, and code are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.10446",
    "authors": [
      "Rui Lu",
      "Zhenyu Hou",
      "Zihan Wang",
      "Hanchen Zhang",
      "Xiao Liu",
      "Yujiang Li",
      "Shi Feng",
      "Jie Tang",
      "Yuxiao Dong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.10453",
    "title": "SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets",
    "abstract": "           Alzheimer's disease is a progressive, neurodegenerative disorder that causes memory loss and cognitive decline. While there has been extensive research in applying deep learning models to Alzheimer's prediction tasks, these models remain limited by lack of available labeled data, poor generalization across datasets, and inflexibility to varying numbers of input scans and time intervals between scans. In this study, we adapt three state-of-the-art temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis, and add novel extensions designed to handle variable-length inputs and learn robust spatial features. We aggregate four publicly available datasets comprising 3,161 patients for pre-training, and show the performance of our model across multiple Alzheimer's prediction tasks including diagnosis classification, conversion detection, and future conversion prediction. Importantly, our SSL model implemented with temporal order prediction and contrastive learning outperforms supervised learning on six out of seven downstream tasks. It demonstrates adaptability and generalizability across tasks and number of input images with varying time intervals, highlighting its capacity for robust performance across clinical applications. We release our code and model publicly at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.10453",
    "authors": [
      "Emily Kaczmarek",
      "Justin Szeto",
      "Brennan Nichyporuk",
      "Tal Arbel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10454",
    "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation",
    "abstract": "           In this paper, we propose a training-free framework for vision-and-language navigation (VLN). Existing zero-shot VLN methods are mainly designed for discrete environments or involve unsupervised training in continuous simulator environments, which makes it challenging to generalize and deploy them in real-world scenarios. To achieve a training-free framework in continuous environments, our framework formulates navigation guidance as graph constraint optimization by decomposing instructions into explicit spatial constraints. The constraint-driven paradigm decodes spatial semantics through constraint solving, enabling zero-shot adaptation to unseen environments. Specifically, we construct a spatial constraint library covering all types of spatial relationship mentioned in VLN instructions. The human instruction is decomposed into a directed acyclic graph, with waypoint nodes, object nodes and edges, which are used as queries to retrieve the library to build the graph constraints. The graph constraint optimization is solved by the constraint solver to determine the positions of waypoints, obtaining the robot's navigation path and final goal. To handle cases of no solution or multiple solutions, we construct a navigation tree and the backtracking mechanism. Extensive experiments on standard benchmarks demonstrate significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods. We further conduct real-world experiments to show that our framework can effectively generalize to new environments and instruction sets, paving the way for a more robust and autonomous navigation framework.         ",
    "url": "https://arxiv.org/abs/2509.10454",
    "authors": [
      "Hang Yin",
      "Haoyu Wei",
      "Xiuwei Xu",
      "Wenxuan Guo",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.09719",
    "title": "Spectral Bottleneck in Deep Neural Networks: Noise is All You Need",
    "abstract": "           Deep neural networks are known to exhibit a spectral learning bias, wherein low-frequency components are learned early in training, while high-frequency modes emerge more gradually in later epochs. However, when the target signal lacks low-frequency components and is dominated by broadband high frequencies, training suffers from a 'spectral bottleneck', and the model fails to reconstruct the entire signal, including the frequency components that lie within the network's representational capacity. We examine such a scenario in the context of implicit neural representations (INRs) with sinusoidal representation networks (SIRENs), focusing on the challenge of fitting high-frequency-dominant signals that are susceptible to spectral bottleneck. To effectively fit any target signal irrespective of it's frequency content, we propose a generalized target-aware 'weight perturbation scheme' (WINNER - weight initialization with noise for neural representations) for network initialization. The scheme perturbs uniformly initialized weights with Gaussian noise, where the noise scales are adaptively determined by the spectral centroid of the target signal. We show that the noise scales can provide control over the spectra of network activations and the eigenbasis of the empirical neural tangent kernel. This method not only addresses the spectral bottleneck but also yields faster convergence and with improved representation accuracy, outperforming state-of-the-art approaches in audio fitting and achieving notable gains in image fitting and denoising tasks. Beyond signal reconstruction, our approach opens new directions for adaptive weight initialization strategies in computer vision and scientific machine learning.         ",
    "url": "https://arxiv.org/abs/2509.09719",
    "authors": [
      "Hemanth Chandravamsi",
      "Dhanush V. Shenoy",
      "Itay Zinn",
      "Shimon Pisnoy",
      "Steven H. Frankel"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2509.09894",
    "title": "Accelerating 3D Photoacoustic Computed Tomography with End-to-End Physics-Aware Neural Operators",
    "abstract": "           Photoacoustic computed tomography (PACT) combines optical contrast with ultrasonic resolution, achieving deep-tissue imaging beyond the optical diffusion limit. While three-dimensional PACT systems enable high-resolution volumetric imaging for applications spanning transcranial to breast imaging, current implementations require dense transducer arrays and prolonged acquisition times, limiting clinical translation. We introduce Pano (PACT imaging neural operator), an end-to-end physics-aware model that directly learns the inverse acoustic mapping from sensor measurements to volumetric reconstructions. Unlike existing approaches (e.g. universal back-projection algorithm), Pano learns both physics and data priors while also being agnostic to the input data resolution. Pano employs spherical discrete-continuous convolutions to preserve hemispherical sensor geometry, incorporates Helmholtz equation constraints to ensure physical consistency and operates resolutionindependently across varying sensor configurations. We demonstrate the robustness and efficiency of Pano in reconstructing high-quality images from both simulated and real experimental data, achieving consistent performance even with significantly reduced transducer counts and limited-angle acquisition configurations. The framework maintains reconstruction fidelity across diverse sparse sampling patterns while enabling real-time volumetric imaging capabilities. This advancement establishes a practical pathway for making 3D PACT more accessible and feasible for both preclinical research and clinical applications, substantially reducing hardware requirements without compromising image reconstruction quality.         ",
    "url": "https://arxiv.org/abs/2509.09894",
    "authors": [
      "Jiayun Wang",
      "Yousuf Aborahama",
      "Arya Khokhar",
      "Yang Zhang",
      "Chuwei Wang",
      "Karteekeya Sastry",
      "Julius Berner",
      "Yilin Luo",
      "Boris Bonev",
      "Zongyi Li",
      "Kamyar Azizzadenesheli",
      "Lihong V. Wang",
      "Anima Anandkumar"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.09972",
    "title": "Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms",
    "abstract": "           This study addresses the escalating threat of branched broomrape (Phelipanche ramosa) to California's tomato industry, which supplies over 90 percent of U.S. processing tomatoes. The parasite's largely underground life cycle makes early detection difficult, while conventional chemical controls are costly, environmentally harmful, and often ineffective. To address this, we combined drone-based multispectral imagery with Long Short-Term Memory (LSTM) deep learning networks, using the Synthetic Minority Over-sampling Technique (SMOTE) to handle class imbalance. Research was conducted on a known broomrape-infested tomato farm in Woodland, Yolo County, CA, across five key growth stages determined by growing degree days (GDD). Multispectral images were processed to isolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with 79.09 percent overall accuracy and 70.36 percent recall without integrating later stages. Incorporating sequential growth stages with LSTM improved detection substantially. The best-performing scenario, which integrated all growth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy and 95.37 percent recall. These results demonstrate the strong potential of temporal multispectral analysis and LSTM networks for early broomrape detection. While further real-world data collection is needed for practical deployment, this study shows that UAV-based multispectral sensing coupled with deep learning could provide a powerful precision agriculture tool to reduce losses and improve sustainability in tomato production.         ",
    "url": "https://arxiv.org/abs/2509.09972",
    "authors": [
      "Mohammadreza Narimani",
      "Alireza Pourreza",
      "Ali Moghimi",
      "Mohsen Mesgaran",
      "Parastoo Farajpoor",
      "Hamid Jafarbiglu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10031",
    "title": "Unified Learnable 2D Convolutional Feature Extraction for ASR",
    "abstract": "           Neural front-ends represent a promising approach to feature extraction for automatic speech recognition (ASR) systems as they enable to learn specifically tailored features for different tasks. Yet, many of the existing techniques remain heavily influenced by classical methods. While this inductive bias may ease the system design, our work aims to develop a more generic front-end for feature extraction. Furthermore, we seek to unify the front-end architecture contrasting with existing approaches that apply a composition of several layer topologies originating from different sources. The experiments systematically show how to reduce the influence of existing techniques to achieve a generic front-end. The resulting 2D convolutional front-end is parameter-efficient and suitable for a scenario with limited computational resources unlike large models pre-trained on unlabeled audio. The results demonstrate that this generic unified approach is not only feasible but also matches the performance of existing supervised learnable feature extractors.         ",
    "url": "https://arxiv.org/abs/2509.10031",
    "authors": [
      "Peter Vieting",
      "Benedikt Hilmes",
      "Ralf Schl\u00fcter",
      "Hermann Ney"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2509.10070",
    "title": "Toward Minimum Graphic Parity Networks",
    "abstract": "           Quantum circuits composed of CNOT and $R_z$ are fundamental building blocks of many quantum algorithms, so optimizing the synthesis of such quantum circuits is crucial. We address this problem from a theoretical perspective by studying the graphic parity network synthesis problem. A graphic parity network for a graph $G$ is a quantum circuit composed solely of CNOT gates where each edge of $G$ is represented in the circuit, and the final state of the wires matches the original input. We aim to synthesize graphic parity networks with the minimum number of gates, specifically for quantum algorithms addressing combinatorial optimization problems with Ising formulations. We demonstrate that a graphic parity network for a connected graph with $n$ vertices and $m$ edges requires at least $m+n-1$ gates. This lower bound can be improved to $m+\\Omega(m) = m+\\Omega(n^{1.5})$ when the shortest cycle in the graph has a length of at least five. We complement this result with a simple randomized algorithm that synthesizes a graphic parity network with expected $m + O(n^{1.5}\\sqrt{\\log n})$ gates. Additionally, we begin exploring connected graphs that allow for graphic parity networks with exactly $m+n-1$ gates. We conjecture that all such graphs belong to a newly defined graph class. Furthermore, we present a linear-time algorithm for synthesizing minimum graphic parity networks for graphs within this class. However, this graph class is not closed under taking induced subgraphs, and we show that recognizing it is $\\textsf{NP}$-complete, which is complemented with a fixed-parameter tractable algorithm parameterized by the treewidth.         ",
    "url": "https://arxiv.org/abs/2509.10070",
    "authors": [
      "Yixin Cao",
      "Yiren Lu",
      "Junhong Nie",
      "Xiaoming Sun",
      "Guojing Tian"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2509.10337",
    "title": "Why does your graph neural network fail on some graphs? Insights from exact generalisation error",
    "abstract": "           Graph Neural Networks (GNNs) are widely used in learning on graph-structured data, yet a principled understanding of why they succeed or fail remains elusive. While prior works have examined architectural limitations such as over-smoothing and over-squashing, these do not explain what enables GNNs to extract meaningful representations or why performance varies drastically between similar architectures. These questions are related to the role of generalisation: the ability of a model to make accurate predictions on unlabelled data. Although several works have derived generalisation error bounds for GNNs, these are typically loose, restricted to a single architecture, and offer limited insight into what governs generalisation in practice. In this work, we take a different approach by deriving the exact generalisation error for GNNs in a transductive fixed-design setting through the lens of signal processing. From this viewpoint, GNNs can be interpreted as graph filter operators that act on node features via the graph structure. By focusing on linear GNNs while allowing non-linearity in the graph filters, we derive the first exact generalisation error for a broad range of GNNs, including convolutional, PageRank-based, and attention-based models. The exact characterisation of the generalisation error reveals that only the aligned information between node features and graph structure contributes to generalisation. Furthermore, we quantify the effect of homophily on generalisation. Our work provides a framework that explains when and why GNNs can effectively leverage structural and feature information, offering practical guidance for model selection.         ",
    "url": "https://arxiv.org/abs/2509.10337",
    "authors": [
      "Nil Ayday",
      "Mahalakshmi Sabanayagam",
      "Debarghya Ghoshdastidar"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10378",
    "title": "Matrix-free Neural Preconditioner for the Dirac Operator in Lattice Gauge Theory",
    "abstract": "           Linear systems arise in generating samples and in calculating observables in lattice quantum chromodynamics~(QCD). Solving the Hermitian positive definite systems, which are sparse but ill-conditioned, involves using iterative methods, such as Conjugate Gradient (CG), which are time-consuming and computationally expensive. Preconditioners can effectively accelerate this process, with the state-of-the-art being multigrid preconditioners. However, constructing useful preconditioners can be challenging, adding additional computational overhead, especially in large linear systems. We propose a framework, leveraging operator learning techniques, to construct linear maps as effective preconditioners. The method in this work does not rely on explicit matrices from either the original linear systems or the produced preconditioners, allowing efficient model training and application in the CG solver. In the context of the Schwinger model U(1) gauge theory in 1+1 spacetime dimensions with two degenerate-mass fermions), this preconditioning scheme effectively decreases the condition number of the linear systems and approximately halves the number of iterations required for convergence in relevant parameter ranges. We further demonstrate the framework learns a general mapping dependent on the lattice structure which leads to zero-shot learning ability for the Dirac operators constructed from gauge field configurations of different sizes.         ",
    "url": "https://arxiv.org/abs/2509.10378",
    "authors": [
      "Yixuan Sun",
      "Srinivas Eswar",
      "Yin Lin",
      "William Detmold",
      "Phiala Shanahan",
      "Xiaoye Li",
      "Yang Liu",
      "Prasanna Balaprakash"
    ],
    "subjectives": [
      "High Energy Physics - Lattice (hep-lat)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.08327",
    "title": "Analyzing the Impact of Adversarial Examples on Explainable Machine Learning",
    "abstract": "           Adversarial attacks are a type of attack on machine learning models where an attacker deliberately modifies the inputs to cause the model to make incorrect predictions. Adversarial attacks can have serious consequences, particularly in applications such as autonomous vehicles, medical diagnosis, and security systems. Work on the vulnerability of deep learning models to adversarial attacks has shown that it is very easy to make samples that make a model predict things that it doesn't want to. In this work, we analyze the impact of model interpretability due to adversarial attacks on text classification problems. We develop an ML-based classification model for text data. Then, we introduce the adversarial perturbations on the text data to understand the classification performance after the attack. Subsequently, we analyze and interpret the model's explainability before and after the attack         ",
    "url": "https://arxiv.org/abs/2307.08327",
    "authors": [
      "Prathyusha Devabhakthini",
      "Sasmita Parida",
      "Raj Mani Shukla",
      "Suvendu Chandan Nayak",
      "Tapadhir Das"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.05675",
    "title": "Is Adversarial Training with Compressed Datasets Effective?",
    "abstract": "           Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset. This synthetic dataset aims to retain the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset. Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of adversarial robustness. In this work, we investigate the impact of adversarial robustness on models trained with compressed datasets. We show that the compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models. As a solution to improve dataset compression efficiency and adversarial robustness simultaneously, we present a robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset. The proposed method is (1) provably robust by minimizing the generalized adversarial loss, (2) more effective than DC methods when applying adversarial training over MFC, (3) obtained by a one-time computation and is applicable for any model.         ",
    "url": "https://arxiv.org/abs/2402.05675",
    "authors": [
      "Tong Chen",
      "Raghavendra Selvan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.01158",
    "title": "Interpretable Data-driven Anomaly Detection in Industrial Processes with ExIFFI",
    "abstract": "           Anomaly Detection (AD) is crucial in industrial settings to streamline operations by detecting underlying issues. Conventional methods merely label observations as normal or anomalous, lacking crucial insights. In Industry 5.0, interpretable outcomes become desirable to enable users to understand the rational under model decisions. This paper presents the first industrial application of ExIFFI, a recent approach for fast, efficient explanations for the Extended Isolation Forest (EIF) (AD) method. ExIFFI is tested on three industrial datasets, demonstrating superior explanation effectiveness and computational efficiency compared to other state-of-the-art explainable AD models.         ",
    "url": "https://arxiv.org/abs/2405.01158",
    "authors": [
      "Davide Frizzo",
      "Francesco Borsatti",
      "Alessio Arcudi",
      "Antonio De Moliner",
      "Roberto Oboe",
      "Gian Antonio Susto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.01284",
    "title": "Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot Learning: A General Framework",
    "abstract": "           Generalized Zero-Shot Learning (GZSL) is a challenging task requiring accurate classification of both seen and unseen classes. Within this domain, Audio-visual GZSL emerges as an extremely exciting yet difficult task, given the inclusion of both visual and acoustic features as multi-modal inputs. Existing efforts in this field mostly utilize either embedding-based or generative-based methods. However, generative training is difficult and unstable, while embedding-based methods often encounter domain shift problem. Thus, we find it promising to integrate both methods into a unified framework to leverage their advantages while mitigating their respective disadvantages. Our study introduces a general framework employing out-of-distribution (OOD) detection, aiming to harness the strengths of both approaches. We first employ generative adversarial networks to synthesize unseen features, enabling the training of an OOD detector alongside classifiers for seen and unseen classes. This detector determines whether a test feature belongs to seen or unseen classes, followed by classification utilizing separate classifiers for each feature type. We test our framework on three popular audio-visual datasets and observe a significant improvement comparing to existing state-of-the-art works. Codes can be found in this https URL.         ",
    "url": "https://arxiv.org/abs/2408.01284",
    "authors": [
      "Liuyuan Wen"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2408.16115",
    "title": "Uncertainty Modeling in Graph Neural Networks via Stochastic Differential Equations",
    "abstract": "           We propose a novel Stochastic Differential Equation (SDE) framework to address the problem of learning uncertainty-aware representations for graph-structured data. While Graph Neural Ordinary Differential Equations (GNODEs) have shown promise in learning node representations, they lack the ability to quantify uncertainty. To address this, we introduce Latent Graph Neural Stochastic Differential Equations (LGNSDE), which enhance GNODE by embedding randomness through a Bayesian prior-posterior mechanism for epistemic uncertainty and Brownian motion for aleatoric uncertainty. By leveraging the existence and uniqueness of solutions to graph-based SDEs, we prove that the variance of the latent space bounds the variance of model outputs, thereby providing theoretically sensible guarantees for the uncertainty estimates. Furthermore, we show mathematically that LGNSDEs are robust to small perturbations in the input, maintaining stability over time. Empirical results across several benchmarks demonstrate that our framework is competitive in out-of-distribution detection, robustness to noise, and active learning, underscoring the ability of LGNSDEs to quantify uncertainty reliably. Code is available at \\href{this https URL}{\\texttt{this http URL}}.         ",
    "url": "https://arxiv.org/abs/2408.16115",
    "authors": [
      "Richard Bergna",
      "Sergio Calvo-Ordo\u00f1ez",
      "Felix L. Opolka",
      "Pietro Li\u00f2",
      "Jose Miguel Hernandez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2409.20138",
    "title": "Constraint Guided Model Quantization of Neural Networks",
    "abstract": "           Deploying neural networks on the edge has become increasingly important as deep learning is being applied in an increasing amount of applications. At the edge computing hardware typically has limited resources disallowing to run neural networks with high complexity. To reduce the complexity of neural networks a wide range of quantization methods have been proposed in recent years. This work proposes Constraint Guided Model Quantization (CGMQ), which is a quantization aware training algorithm that uses an upper bound on the computational resources and reduces the bit-widths of the parameters of the neural network. CGMQ does not require the tuning of a hyperparameter to result in a mixed precision neural network that satisfies the predefined computational cost constraint, while prior work does. It is shown on MNIST and CIFAR10 that the performance of CGMQ is competitive with state-of-the-art quantization aware training algorithms, while guaranteeing the satisfaction of an upper bound on the computational complexity defined by the computational resources of the on edge hardware.         ",
    "url": "https://arxiv.org/abs/2409.20138",
    "authors": [
      "Quinten Van Baelen",
      "Peter Karsmakers"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.09590",
    "title": "Bayesian Sheaf Neural Networks",
    "abstract": "           Equipping graph neural networks with a convolution operation defined in terms of a cellular sheaf offers advantages for learning expressive representations of heterophilic graph data. The most flexible approach to constructing the sheaf is to learn it as part of the network as a function of the node features. However, this leaves the network potentially overly sensitive to the learned sheaf. As a counter-measure, we propose a variational approach to learning cellular sheaves within sheaf neural networks, yielding an architecture we refer to as a Bayesian sheaf neural network. As part of this work, we define a novel family of reparameterizable probability distributions on the rotation group $SO(n)$ using the Cayley transform. We evaluate the Bayesian sheaf neural network on several graph datasets, and show that our Bayesian sheaf models achieve leading performance compared to baseline models and are less sensitive to the choice of hyperparameters under limited training data settings.         ",
    "url": "https://arxiv.org/abs/2410.09590",
    "authors": [
      "Patrick Gillespie",
      "Layal Bou Hamdan",
      "Ioannis Schizas",
      "David L. Boothe",
      "Vasileios Maroulas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2410.21471",
    "title": "AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion models",
    "abstract": "           Recent advances in diffusion models have significantly enhanced the quality of image synthesis, yet they have also introduced serious safety concerns, particularly the generation of Not Safe for Work (NSFW) content. Previous research has demonstrated that adversarial prompts can be used to generate NSFW content. However, such adversarial text prompts are often easily detectable by text-based filters, limiting their efficacy. In this paper, we expose a previously overlooked vulnerability: adversarial image attacks targeting Image-to-Image (I2I) diffusion models. We propose AdvI2I, a novel framework that manipulates input images to induce diffusion models to generate NSFW content. By optimizing a generator to craft adversarial images, AdvI2I circumvents existing defense mechanisms, such as Safe Latent Diffusion (SLD), without altering the text prompts. Furthermore, we introduce AdvI2I-Adaptive, an enhanced version that adapts to potential countermeasures and minimizes the resemblance between adversarial images and NSFW concept embeddings, making the attack more resilient against defenses. Through extensive experiments, we demonstrate that both AdvI2I and AdvI2I-Adaptive can effectively bypass current safeguards, highlighting the urgent need for stronger security measures to address the misuse of I2I diffusion models.         ",
    "url": "https://arxiv.org/abs/2410.21471",
    "authors": [
      "Yaopei Zeng",
      "Yuanpu Cao",
      "Bochuan Cao",
      "Yurui Chang",
      "Jinghui Chen",
      "Lu Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.09852",
    "title": "InterFormer: Effective Heterogeneous Interaction Learning for Click-Through Rate Prediction",
    "abstract": "           Click-through rate (CTR) prediction, which predicts the probability of a user clicking an ad, is a fundamental task in recommender systems. The emergence of heterogeneous information, such as user profile and behavior sequences, depicts user interests from different aspects. A mutually beneficial integration of heterogeneous information is the cornerstone towards the success of CTR prediction. However, most of the existing methods suffer from two fundamental limitations, including (1) insufficient inter-mode interaction due to the unidirectional information flow between modes, and (2) aggressive information aggregation caused by early summarization, resulting in excessive information loss. To address the above limitations, we propose a novel module named InterFormer to learn heterogeneous information interaction in an interleaving style. To achieve better interaction learning, InterFormer enables bidirectional information flow for mutually beneficial learning across different modes. To avoid aggressive information aggregation, we retain complete information in each data mode and use a separate bridging arch for effective information selection and summarization. Our proposed InterFormer achieves state-of-the-art performance on three public datasets and a large-scale industrial dataset.         ",
    "url": "https://arxiv.org/abs/2411.09852",
    "authors": [
      "Zhichen Zeng",
      "Xiaolong Liu",
      "Mengyue Hang",
      "Xiaoyi Liu",
      "Qinghai Zhou",
      "Chaofei Yang",
      "Yiqun Liu",
      "Yichen Ruan",
      "Laming Chen",
      "Yuxin Chen",
      "Yujia Hao",
      "Jiaqi Xu",
      "Jade Nie",
      "Xi Liu",
      "Buyun Zhang",
      "Wei Wen",
      "Siyang Yuan",
      "Hang Yin",
      "Xin Zhang",
      "Kai Wang",
      "Wen-Yen Chen",
      "Yiping Han",
      "Huayu Li",
      "Chunzhi Yang",
      "Bo Long",
      "Philip S. Yu",
      "Hanghang Tong",
      "Jiyan Yang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.12039",
    "title": "Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection",
    "abstract": "           Despite their remarkable success, large language models (LLMs) have shown limited ability on safety-critical code tasks such as vulnerability detection. Typically, static analysis (SA) tools, like CodeQL, CodeGuru Security, etc., are used for vulnerability detection. SA relies on predefined, manually-crafted rules for flagging various vulnerabilities. Thus, effectiveness of SA in detecting vulnerabilities depends on human experts and is known to report high error rates. In this study we investigate whether LLM prompting can be an effective alternative to these static analyzers in the partial code setting. We propose prompting strategies that integrate natural language instructions of vulnerabilities with contrastive chain-of-thought reasoning, augmented using contrastive samples from a synthetic dataset. Our findings demonstrate that security-aware prompting techniques can be effective alternatives to the laborious, hand-crafted rules of static analyzers, which often result in high false negative rates in the partial code setting. When leveraging SOTA reasoning models such as DeepSeek-R1, each of our prompting strategies exceeds the static analyzer baseline, with the best strategies improving accuracy by as much as 31.6%, F1-scores by 71.7%, pairwise accuracies by 60.4%, and reducing FNR by as much as 37.6%.         ",
    "url": "https://arxiv.org/abs/2412.12039",
    "authors": [
      "Ira Ceka",
      "Feitong Qiao",
      "Anik Dey",
      "Aastha Valecha",
      "Gail Kaiser",
      "Baishakhi Ray"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2412.14482",
    "title": "Embedding high-resolution touch across robotic hands enables adaptive human-like grasping",
    "abstract": "           Developing robotic hands that adapt to real-world dynamics remains a fundamental challenge in robotics and machine intelligence. Despite significant advances in replicating human hand kinematics and control algorithms, robotic systems still struggle to match human capabilities in dynamic environments, primarily due to inadequate tactile feedback. To bridge this gap, we present F-TAC Hand, a biomimetic hand featuring high-resolution tactile sensing (0.1mm spatial resolution) across 70% of its surface area. Through optimized hand design, we overcome traditional challenges in integrating high-resolution tactile sensors while preserving the full range of motion. The hand, powered by our generative algorithm that synthesizes human-like hand configurations, demonstrates robust grasping capabilities in dynamic real-world conditions. Extensive evaluation across 600 real-world trials demonstrates that this tactile-embodied system significantly outperforms non-tactile-informed alternatives in complex manipulation tasks (p<0.0001). These results provide empirical evidence for the critical role of rich tactile embodiment in developing advanced robotic intelligence, offering new perspectives on the relationship between physical sensing capabilities and intelligent behavior.         ",
    "url": "https://arxiv.org/abs/2412.14482",
    "authors": [
      "Zihang Zhao",
      "Wanlin Li",
      "Yuyang Li",
      "Tengyu Liu",
      "Boren Li",
      "Meng Wang",
      "Kai Du",
      "Hangxin Liu",
      "Yixin Zhu",
      "Qining Wang",
      "Kaspar Althoefer",
      "Song-Chun Zhu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2501.13223",
    "title": "Data Matters Most: Auditing Social Bias in Contrastive Vision Language Models",
    "abstract": "           Vision-language models (VLMs) deliver strong zero-shot recognition but frequently inherit social biases from their training data. We systematically disentangle three design factors -- model size, training-data scale, and training-data source -- by comparing CLIP and OpenCLIP, two models that share an identical contrastive objective yet differ in encoder width and in the image-text corpora on which they are pre-trained (400M proprietary pairs vs. 400M/2B LAION). Across balanced face-analysis benchmarks, enlarging the encoder reduces gender skew in CLIP but amplifies both gender and racial skew in OpenCLIP; increasing the LAION corpus from 400M to 2B further increases OpenCLIP bias. At matched model and data budgets, substituting proprietary data with LAION improves gender fairness while increasing racial skew, underscoring data source as the primary driver of bias patterns. We also evaluate three post-hoc, test-time debiasing strategies -- Bias Prompts, Prompt Array, and SANER. Debiasing reduces but does not eliminate harm, and its effectiveness is source- and size-dependent: Bias Prompts most effectively reduce gender skew in CLIP at smaller model sizes, whereas Prompt Array and SANER more reliably reduce racial skew in OpenCLIP; scaling LAION reconfigures which method is most fair. Taken together, these findings challenge the assumption that bigger models or datasets are automatically fairer and foreground training data source as the key determinant of both bias and mitigation efficacy. We release code and evaluation scripts to enable transparent, reproducible auditing of future VLMs.         ",
    "url": "https://arxiv.org/abs/2501.13223",
    "authors": [
      "Zahraa Al Sahili",
      "Ioannis Patras",
      "Matthew Purver"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.08987",
    "title": "Neural Force Field: Few-shot Learning of Generalized Physical Reasoning",
    "abstract": "           Physical reasoning is a remarkable human ability that enables rapid learning and generalization from limited experience. Current AI models, despite extensive training, still struggle to achieve similar generalization, especially in Out-of-distribution (OOD) settings. This limitation stems from their inability to abstract core physical principles from observations. A key challenge is developing representations that can efficiently learn and generalize physical dynamics from minimal data. Here we present Neural Force Field (NFF), a framework extending Neural Ordinary Differential Equation (NODE) to learn complex object interactions through force field representations, which can be efficiently integrated through an Ordinary Differential Equation (ODE) solver to predict object trajectories. Unlike existing approaches that rely on discrete latent spaces, NFF captures fundamental physical concepts such as gravity, support, and collision in continuous explicit force fields. Experiments on three challenging physical reasoning tasks demonstrate that NFF, trained with only a few examples, achieves strong generalization to unseen scenarios. This physics-grounded representation enables efficient forward-backward planning and rapid adaptation through interactive refinement. Our work suggests that incorporating physics-inspired representations into learning systems can help bridge the gap between artificial and human physical reasoning capabilities.         ",
    "url": "https://arxiv.org/abs/2502.08987",
    "authors": [
      "Shiqian Li",
      "Ruihong Shen",
      "Yaoyu Tao",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.10990",
    "title": "FinMTEB: Finance Massive Text Embedding Benchmark",
    "abstract": "           Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, Fin-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including Fin-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.         ",
    "url": "https://arxiv.org/abs/2502.10990",
    "authors": [
      "Yixuan Tang",
      "Yi Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2502.13818",
    "title": "Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge",
    "abstract": "           Estimating the construction year of buildings is critical for advancing sustainability, as older structures often lack energy-efficient features. Sustainable urban planning relies on accurate building age data to reduce energy consumption and mitigate climate change. In this work, we introduce MapYourCity, a novel multi-modal benchmark dataset comprising top-view Very High Resolution (VHR) imagery, multi-spectral Earth Observation (EO) data from the Copernicus Sentinel-2 satellite constellation, and co-localized street-view images across various European cities. Each building is labeled with its construction epoch, and the task is formulated as a seven-class classification problem covering periods from 1900 to the present. To advance research in EO generalization and multi-modal learning, we organized a community-driven data challenge in 2024, hosted by ESA $\\Phi$-lab, which ran for four months and attracted wide participation. This paper presents the Top-4 performing models from the challenge and their evaluation results. We assess model generalization on cities excluded from training to prevent data leakage, and evaluate performance under missing modality scenarios, particularly when street-view data is unavailable. Results demonstrate that building age estimation is both feasible and effective, even in previously unseen cities and when relying solely on top-view satellite imagery (i.e. with VHR and Sentinel-2 images). The MapYourCity dataset thus provides a valuable resource for developing scalable, real-world solutions in sustainable urban analytics.         ",
    "url": "https://arxiv.org/abs/2502.13818",
    "authors": [
      "Nikolaos Dionelis",
      "Alessandra Feliciotti",
      "Mattia Marconcini",
      "Devis Peressutti",
      "Nika Oman Kadunc",
      "JaeWan Park",
      "Hagai Raja Sinulingga",
      "Steve Andreas Immanuel",
      "Ba Tran",
      "Caroline Arnold",
      "Nicolas Long\u00e9p\u00e9"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.16446",
    "title": "Auxiliary Discrminator Sequence Generative Adversarial Networks (ADSeqGAN) for Few Sample Molecule Generation",
    "abstract": "           In this work, we introduce Auxiliary Discriminator Sequence Generative Adversarial Networks (ADSeqGAN), a novel approach for molecular generation in small-sample datasets. Traditional generative models often struggle with limited training data, particularly in drug discovery, where molecular datasets for specific therapeutic targets, such as nucleic acids binders and central nervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by integrating an auxiliary random forest classifier as an additional discriminator into the GAN framework, significantly improves molecular generation quality and class specificity. Our method incorporates pretrained generator and Wasserstein distance to enhance training stability and diversity. We evaluate ADSeqGAN across three representative cases. First, on nucleic acid- and protein-targeting molecules, ADSeqGAN shows superior capability in generating nucleic acid binders compared to baseline models. Second, through oversampling, it markedly improves CNS drug generation, achieving higher yields than traditional de novo models. Third, in cannabinoid receptor type 1 (CB1) ligand design, ADSeqGAN generates novel druglike molecules, with 32.8\\% predicted actives surpassing hit rates of CB1-focused and general-purpose libraries when assessed by a target-specific LRIP-SF scoring function. Overall, ADSeqGAN offers a versatile framework for molecular design in data-scarce scenarios, with demonstrated applications in nucleic acid binders, CNS drugs, and CB1 ligands.         ",
    "url": "https://arxiv.org/abs/2502.16446",
    "authors": [
      "Haocheng Tang",
      "Jing Long",
      "Beihong Ji",
      "Junmei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2503.16913",
    "title": "FGIT: Fault-Guided Fine-Tuning for Code Generation",
    "abstract": "           Modern instruction-tuned large language models (LLMs) have made remarkable progress in code generation. However, these LLMs fine-tuned with standard supervised fine-tuning (SFT) sometimes generate plausible-looking but functionally incorrect code variants. This issue likely stems from the limitation of standard SFT, which treats all tokens equally during optimization and fails to emphasize the error-sensitive segments-specific code differences between correct implementations and similar incorrect variants. To address this problem, we propose Fault-Guided Fine-Tuning (FGIT), a novel fine-tuning technique that enhances LLMs' code generation by (1) extracting multi-granularity (line/token-level) differences between correct and incorrect yet similar implementations to identify error-sensitive segments, and (2) dynamically prioritizing those segments during training via dynamic loss weighting. Through extensive experiments on seven LLMs across three widely-used benchmarks, our method achieves an average relative improvement of 6.9% on pass@1 with some enhanced 6.7B LLMs outperforming closed-source models, e.g., GPT-3.5-Turbo. Furthermore, our fine-tuning technique demonstrates strong generalization with performance improvements ranging from 3.8% to 19.1% across diverse instruction-tuned LLMs, and our ablation studies confirm the contributions of different granularities of differences and hyperparameters.         ",
    "url": "https://arxiv.org/abs/2503.16913",
    "authors": [
      "Lishui Fan",
      "Zhongxin Liu",
      "Haoye Wang",
      "Lingfeng Bao",
      "Xin Xia",
      "Shanping Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2503.18978",
    "title": "Cluster Synchronization via Graph Laplacian Eigenvectors",
    "abstract": "           Almost equitable partitions (AEPs) have been linked to cluster synchronization in oscillatory systems, highlighting the importance of structure in collective network dynamics. We provide a general spectral framework that formalizes this connection, showing how eigenvectors associated with AEPs span a subspace of the Laplacian spectrum that governs partition-induced synchronization behavior. This offers a principled reduction of network dynamics, allowing clustered states to be understood in terms of quotient graph projections. Our approach clarifies the conditions under which transient hierarchical clustering and multi-frequency synchronization emerge, and connects these dynamical phenomena directly to network symmetry and community structure. In doing so, we bridge a critical gap between static topology and dynamic behavior-namely, the lack of a spectral method for analyzing synchronization in networks that exhibit exact or approximate structural regularity. Perfect AEPs are rare in real-world networks since most have some degree of irregularity or noise. We define a relaxation of an AEP we call a quasi-equitable partition at level $\\delta$ ($\\delta-$QEP). $\\delta-$QEPs can preserve many of the clustering-relevant properties of AEPs while tolerating structural imperfections and noise. This extension enables us to describe synchronization behavior in more realistic scenarios, where ideal symmetries are rarely present. Our findings have important implications for understanding synchronization patterns in real-world networks, from neural circuits to power grids.         ",
    "url": "https://arxiv.org/abs/2503.18978",
    "authors": [
      "Tobias Timofeyev",
      "Alice Patania"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Combinatorics (math.CO)",
      "Spectral Theory (math.SP)"
    ]
  },
  {
    "id": "arXiv:2505.07841",
    "title": "Task-Oriented Multimodal Token Transmission in Resource-Constrained Multiuser Networks",
    "abstract": "           Despite the promising paradigm enabled by integrating semantic communication (SemCom) with multimodal large models (MLMs) for transmitting and utilizing multimodal data, efficiently fusing and exploiting cross-modal information still remain challenging. Moreover, widely adopted transformer-based architectures inevitably produce excessively long token embeddings for transmission, which result in higher bandwidth consumption, increased power usage, and greater latency, rendering them impractical in resource-constrained networks. In this letter, we propose a task-oriented multimodal token transmission scheme for efficient multimodal information fusion and utilization. To improve inter-modal consistency and task-relevant token transmission, we design a two-stage training algotithm which involves cross-modal alignment followed by task-oriented fine-tuning. Meanwhile, token compression is performed using a sliding window pooling operation to conserve limited communication resources. To balance the trade-off between latency reduction and performance degradation caused by compression, we formulate a weighted-sum optimization problem over latency and inference performance. We jointly optimizes bandwidth, power allocation, and token length across users by using an alternating optimization method. Simulation results demonstrate that the proposed algorithm outperforms the baseline under different bandwidth and power budgets. Moreover, the two-stage training algorithm achieves higher accuracy across various signal-to-noise ratios than the method without cross-modal alignment.         ",
    "url": "https://arxiv.org/abs/2505.07841",
    "authors": [
      "Junhe Zhang",
      "Wanli Ni",
      "Pengwei Wang",
      "Dongyu Wang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.09583",
    "title": "Beyond Likes: How Normative Feedback Complements Engagement Signals on Social Media",
    "abstract": "           Many online platforms incorporate engagement signals, such as likes, into their interface design to boost engagement. However, these signals can unintentionally elevate content that may not support normatively desirable behavior, especially when toxic content correlates strongly with popularity indicators. In this study, we propose structured prosocial feedback as a complementary signal, which highlights content quality based on normative criteria. We design and implement an LLM-based feedback system, which evaluates user comments based on principles from positive psychology, such as individual well-being. A pre-registered user study then examines how existing peer-based (popularity) and the new expert-based feedback interact to shape users' reposting behavior in a social media setting. Results show that peer feedback increases conformity to popularity cues, while expert feedback shifts choices toward normatively higher-quality content. This illustrates the added value of normative cues and underscores the potential benefits of incorporating such signals into platform feedback systems to foster healthier online environments.         ",
    "url": "https://arxiv.org/abs/2505.09583",
    "authors": [
      "Yuchen Wu",
      "Mingduo Zhao",
      "John Canny"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2505.13451",
    "title": "Echo State and Band-pass Networks with aqueous memristors: leaky reservoir computing with a leaky substrate",
    "abstract": "           Recurrent Neural Networks (RNN) are extensively employed for processing sequential data such as time series. Reservoir computing (RC) has drawn attention as an RNN framework due to its fixed network that does not require training, making it an attractive platform for hardware based machine learning. We establish an explicit correspondence between the well-established mathematical RC implementations of Echo State Networks and Band-pass Networks with Leaky Integrator nodes on the one hand and a physical circuit containing iontronic simple volatile memristors on the other. These aqueous iontronic devices employ ion transport through water as signal carriers, and feature a voltage-dependent (memory) conductance. The activation function and the dynamics of the Leaky Integrator nodes naturally materialise as the (dynamic) conductance properties of iontronic memristors, while a simple fixed local current-to-voltage update rule at the memristor terminals facilitates the relevant matrix coupling between nodes. We process various time series, including pressure data from simulated airways during breathing that can be directly fed into the network due to the intrinsic responsiveness of iontronic devices to applied pressures. We accomplish this by employing established physical equations of motion of iontronic memristors for the internal dynamics of the circuit.         ",
    "url": "https://arxiv.org/abs/2505.13451",
    "authors": [
      "T.M. Kamsma",
      "J.J. Teijema",
      "R. van Roij",
      "C. Spitoni"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Soft Condensed Matter (cond-mat.soft)"
    ]
  },
  {
    "id": "arXiv:2506.05394",
    "title": "Attacking Attention of Foundation Models Disrupts Downstream Tasks",
    "abstract": "           Foundation models represent the most prominent and recent paradigm shift in artificial intelligence. Foundation models are large models, trained on broad data that deliver high accuracy in many downstream tasks, often without fine-tuning. For this reason, models such as CLIP , DINO or Vision Transfomers (ViT), are becoming the bedrock of many industrial AI-powered applications. However, the reliance on pre-trained foundation models also introduces significant security concerns, as these models are vulnerable to adversarial attacks. Such attacks involve deliberately crafted inputs designed to deceive AI systems, jeopardizing their reliability. This paper studies the vulnerabilities of vision foundation models, focusing specifically on CLIP and ViTs, and explores the transferability of adversarial attacks to downstream tasks. We introduce a novel attack, targeting the structure of transformer-based architectures in a task-agnostic fashion. We demonstrate the effectiveness of our attack on several downstream tasks: classification, captioning, image/text retrieval, segmentation and depth estimation. Code available at:this https URL ",
    "url": "https://arxiv.org/abs/2506.05394",
    "authors": [
      "Hondamunige Prasanna Silva",
      "Federico Becattini",
      "Lorenzo Seidenari"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.09174",
    "title": "Multivariate Long-term Time Series Forecasting with Fourier Neural Filter",
    "abstract": "           Multivariate long-term time series forecasting has been suffering from the challenge of capturing both temporal dependencies within variables and spatial correlations across variables simultaneously. Current approaches predominantly repurpose backbones from natural language processing or computer vision (e.g., Transformers), which fail to adequately address the unique properties of time series (e.g., periodicity). The research community lacks a dedicated backbone with temporal-specific inductive biases, instead relying on domain-agnostic backbones supplemented with auxiliary techniques (e.g., signal decomposition). We introduce FNF as the backbone and DBD as the architecture to provide excellent learning capabilities and optimal learning pathways for spatio-temporal modeling, respectively. Our theoretical analysis proves that FNF unifies local time-domain and global frequency-domain information processing within a single backbone that extends naturally to spatial modeling, while information bottleneck theory demonstrates that DBD provides superior gradient flow and representation capacity compared to existing unified or sequential architectures. Our empirical evaluation across 11 public benchmark datasets spanning five domains (energy, meteorology, transportation, environment, and nature) confirms state-of-the-art performance with consistent hyperparameter settings. Notably, our approach achieves these results without any auxiliary techniques, suggesting that properly designed neural architectures can capture the inherent properties of time series, potentially transforming time series modeling in scientific and industrial applications.         ",
    "url": "https://arxiv.org/abs/2506.09174",
    "authors": [
      "Chenheng Xu",
      "Dan Wu",
      "Yixin Zhu",
      "Ying Nian Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.14391",
    "title": "HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control",
    "abstract": "           Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.         ",
    "url": "https://arxiv.org/abs/2506.14391",
    "authors": [
      "Yaqiao Zhu",
      "Hongkai Wen",
      "Geyong Min",
      "Man Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.19366",
    "title": "Fractality of Wireless Mesh Networks: Dimensional Effects on Network Performance",
    "abstract": "           Wireless mesh networks (WMNs) depend on the spatial distribution of nodes, which directly influences connectivity, routing efficiency, and overall network performance. Conventional models typically assume uniform or random node placement, which inadequately represent the complex, hierarchical spatial patterns observed in practical deployments. In this study, we present a novel algorithm that constructs WMN topologies with tunable fractal dimensions, allowing precise control over spatial self-similarity. By systematically varying the fractal dimension, the algorithm generates network layouts spanning a continuum of spatial complexities, ranging from sparse fragmented clusters to dense, cohesive structures. Through NS-3 simulations, Key performance metrics including throughput, latency, jitter, and packet delivery ratio were evaluated across a range of fractal dimensions. Comparative evaluations against classical random, small-world, scale-free, grid and hierarchical tree networks models reveal that high-dimensional fractal topologies achieve enhanced resilience and throughput under equivalent conditions. These findings demonstrate the potential of fractal geometry as a design paradigm for scalable and efficient WMN architectures.         ",
    "url": "https://arxiv.org/abs/2506.19366",
    "authors": [
      "Marat Zaidyn",
      "Sayat Akhtanov",
      "Dana Turlykozhayeva",
      "Symbat Temesheva",
      "Almat Akhmetali",
      "Alisher Skabylov",
      "Nurzhan Ussipov"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2507.01607",
    "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems",
    "abstract": "           The widespread deployment of Deep Learning-based Face Recognition Systems raises multiple security concerns. While prior research has identified backdoor vulnerabilities on isolated components, Backdoor Attacks on real-world, unconstrained pipelines remain underexplored. This paper presents the first comprehensive system-level analysis of Backdoor Attacks targeting Face Recognition Systems and provides three contributions. We first show that face feature extractors trained with large margin metric learning losses are susceptible to Backdoor Attacks. By analyzing 20 pipeline configurations and 15 attack scenarios, we then reveal that a single backdoor can compromise an entire Face Recognition System. Finally, we propose effective best practices and countermeasures for stakeholders.         ",
    "url": "https://arxiv.org/abs/2507.01607",
    "authors": [
      "Quentin Le Roux",
      "Yannick Teglia",
      "Teddy Furon",
      "Philippe Loubet-Moundi",
      "Eric Bourbao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.05674",
    "title": "Integrating Diffusion-based Multi-task Learning with Online Reinforcement Learning for Robust Quadruped Robot Control",
    "abstract": "           Recent research has highlighted the powerful capabilities of imitation learning in robotics. Leveraging generative models, particularly diffusion models, these approaches offer notable advantages such as strong multi-task generalization, effective language conditioning, and high sample efficiency. While their application has been successful in manipulation tasks, their use in legged locomotion remains relatively underexplored, mainly due to compounding errors that affect stability and difficulties in task transition under limited data. Online reinforcement learning (RL) has demonstrated promising results in legged robot control in the past years, providing valuable insights to address these challenges. In this work, we propose DMLoco, a diffusion-based framework for quadruped robots that integrates multi-task pretraining with online PPO finetuning to enable language-conditioned control and robust task transitions. Our approach first pretrains the policy on a diverse multi-task dataset using diffusion models, enabling language-guided execution of various skills. Then, it finetunes the policy in simulation to ensure robustness and stable task transition during real-world deployment. By utilizing Denoising Diffusion Implicit Models (DDIM) for efficient sampling and TensorRT for optimized deployment, our policy runs onboard at 50Hz, offering a scalable and efficient solution for adaptive, language-guided locomotion on resource-constrained robotic platforms.         ",
    "url": "https://arxiv.org/abs/2507.05674",
    "authors": [
      "Xinyao Qin",
      "Xiaoteng Ma",
      "Yang Qi",
      "Qihan Liu",
      "Chuanyi Xue",
      "Ning Gui",
      "Qinyu Dong",
      "Jun Yang",
      "Bin Liang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.07373",
    "title": "Atherosclerosis through Hierarchical Explainable Neural Network Analysis",
    "abstract": "           In this work, we study the problem pertaining to personalized classification of subclinical atherosclerosis by developing a hierarchical graph neural network framework to leverage two characteristic modalities of a patient: clinical features within the context of the cohort, and molecular data unique to individual patients. Current graph-based methods for disease classification detect patient-specific molecular fingerprints, but lack consistency and comprehension regarding cohort-wide features, which are an essential requirement for understanding pathogenic phenotypes across diverse atherosclerotic trajectories. Furthermore, understanding patient subtypes often considers clinical feature similarity in isolation, without integration of shared pathogenic interdependencies among patients. To address these challenges, we introduce ATHENA: Atherosclerosis Through Hierarchical Explainable Neural Network Analysis, which constructs a novel hierarchical network representation through integrated modality learning; subsequently, it optimizes learned patient-specific molecular fingerprints that reflect individual omics data, enforcing consistency with cohort-wide patterns. With a primary clinical dataset of 391 patients, we demonstrate that this heterogeneous alignment of clinical features with molecular interaction patterns has significantly boosted subclinical atherosclerosis classification performance across various baselines by up to 13% in area under the receiver operating curve (AUC) and 20% in F1 score. Taken together, ATHENA enables mechanistically-informed patient subtype discovery through explainable AI (XAI)-driven subnetwork clustering; this novel integration framework strengthens personalized intervention strategies, thereby improving the prediction of atherosclerotic disease progression and management of their clinical actionable outcomes.         ",
    "url": "https://arxiv.org/abs/2507.07373",
    "authors": [
      "Irsyad Adam",
      "Steven Swee",
      "Erika Yilin",
      "Ethan Ji",
      "William Speier",
      "Dean Wang",
      "Alex Bui",
      "Wei Wang",
      "Karol Watson",
      "Peipei Ping"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.18293",
    "title": "Leveraging Data Augmentation and Siamese Learning for Predictive Process Monitoring",
    "abstract": "           Predictive Process Monitoring (PPM) enables forecasting future events or outcomes of ongoing business process instances based on event logs. However, deep learning PPM approaches are often limited by the low variability and small size of real-world event logs. To address this, we introduce SiamSA-PPM, a novel self-supervised learning framework that combines Siamese learning with Statistical Augmentation for Predictive Process Monitoring. It employs three novel statistically grounded transformation methods that leverage control-flow semantics and frequent behavioral patterns to generate realistic, semantically valid new trace variants. These augmented views are used within a Siamese learning setup to learn generalizable representations of process prefixes without the need for labeled supervision. Extensive experiments on real-life event logs demonstrate that SiamSA-PPM achieves competitive or superior performance compared to the SOTA in both next activity and final outcome prediction tasks. Our results further show that statistical augmentation significantly outperforms random transformations and improves variability in the data, highlighting SiamSA-PPM as a promising direction for training data enrichment in process prediction.         ",
    "url": "https://arxiv.org/abs/2507.18293",
    "authors": [
      "Sjoerd van Straten",
      "Alessandro Padella",
      "Marwan Hassani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02204",
    "title": "TacMan-Turbo: Proactive Tactile Control for Robust and Efficient Articulated Object Manipulation",
    "abstract": "           Adept manipulation of articulated objects is essential for robots to operate successfully in human environments. Such manipulation requires both effectiveness -- reliable operation despite uncertain object structures -- and efficiency -- swift execution with minimal redundant steps and smooth actions. Existing approaches struggle to achieve both objectives simultaneously: methods relying on predefined kinematic models lack effectiveness when encountering structural variations, while tactile-informed approaches achieve robust manipulation without kinematic priors but compromise efficiency through reactive, step-by-step exploration-compensation cycles. This paper introduces TacMan-Turbo, a novel proactive tactile control framework for articulated object manipulation that resolves this fundamental trade-off. Unlike previous approaches that treat tactile contact deviations merely as error signals requiring compensation, our method interprets these deviations as rich sources of local kinematic information. This new perspective enables our controller to predict optimal future interactions and make proactive adjustments, significantly enhancing manipulation efficiency. In comprehensive evaluations across 200 diverse simulated articulated objects and real-world experiments, our approach maintains a 100% success rate while significantly outperforming the previous tactile-informed method in time efficiency, action efficiency, and trajectory smoothness (all p-values < 0.0001). These results demonstrate that the long-standing trade-off between effectiveness and efficiency in articulated object manipulation can be successfully resolved without relying on prior kinematic knowledge.         ",
    "url": "https://arxiv.org/abs/2508.02204",
    "authors": [
      "Zihang Zhao",
      "Zhenghao Qi",
      "Yuyang Li",
      "Leiyao Cui",
      "Zhi Han",
      "Lecheng Ruan",
      "Yixin Zhu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.03703",
    "title": "Privacy Risks of LLM-Empowered Recommender Systems: An Inversion Attack Perspective",
    "abstract": "           The large language model (LLM) powered recommendation paradigm has been proposed to address the limitations of traditional recommender systems, which often struggle to handle cold start users or items with new IDs. Despite its effectiveness, this study uncovers that LLM empowered recommender systems are vulnerable to reconstruction attacks that can expose both system and user privacy. To examine this threat, we present the first systematic study on inversion attacks targeting LLM empowered recommender systems, where adversaries attempt to reconstruct original prompts that contain personal preferences, interaction histories, and demographic attributes by exploiting the output logits of recommendation models. We reproduce the vec2text framework and optimize it using our proposed method called Similarity Guided Refinement, enabling more accurate reconstruction of textual prompts from model generated logits. Extensive experiments across two domains (movies and books) and two representative LLM based recommendation models demonstrate that our method achieves high fidelity reconstructions. Specifically, we can recover nearly 65 percent of the user interacted items and correctly infer age and gender in 87 percent of the cases. The experiments also reveal that privacy leakage is largely insensitive to the victim model's performance but highly dependent on domain consistency and prompt complexity. These findings expose critical privacy vulnerabilities in LLM empowered recommender systems.         ",
    "url": "https://arxiv.org/abs/2508.03703",
    "authors": [
      "Yubo Wang",
      "Min Tang",
      "Nuo Shen",
      "Shujie Cui",
      "Weiqing Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.05524",
    "title": "GASP: A Gradient-Aware Shortest Path Algorithm for Boundary-Confined Visualization of 2-Manifold Reeb Graphs",
    "abstract": "           Reeb graphs are an important tool for abstracting and representing the topological structure of a function defined on a manifold. We have identified three properties for faithfully representing Reeb graphs in a visualization: they should be constrained to the boundary, compact, and aligned with the function gradient. Existing algorithms for drawing Reeb graphs are agnostic to or violate these properties. In this paper, we introduce an algorithm to generate Reeb graph visualizations, called GASP, that is cognizant of these properties, thereby producing visualizations that are more representative of the underlying data. To demonstrate the improvements, the resulting Reeb graphs are evaluated both qualitatively and quantitatively against the geometric barycenter algorithm, using its implementation available in the Topology ToolKit (TTK), a widely adopted tool for calculating and visualizing Reeb graphs.         ",
    "url": "https://arxiv.org/abs/2508.05524",
    "authors": [
      "Sefat E. Rahman",
      "Tushar M. Athawale",
      "Paul Rosen"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computational Geometry (cs.CG)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2508.08765",
    "title": "Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation",
    "abstract": "           The growing presence of AI-generated videos on social networks poses new challenges for deepfake detection, as detectors trained under controlled conditions often fail to generalize to real-world scenarios. A key factor behind this gap is the aggressive, proprietary compression applied by platforms like YouTube and Facebook, which launder low-level forensic cues. However, replicating these transformations at scale is difficult due to API limitations and data-sharing constraints. For these reasons, we propose a first framework that emulates the video sharing pipelines of social networks by estimating compression and resizing parameters from a small set of uploaded videos. These parameters enable a local emulator capable of reproducing platform-specific artifacts on large datasets without direct API access. Experiments on FaceForensics++ videos shared via social networks demonstrate that our emulated data closely matches the degradation patterns of real uploads. Furthermore, detectors fine-tuned on emulated videos achieve comparable performance to those trained on actual shared media. Our approach offers a scalable and practical solution for bridging the gap between lab-based training and real-world deployment of deepfake detectors, particularly in the underexplored domain of compressed video content.         ",
    "url": "https://arxiv.org/abs/2508.08765",
    "authors": [
      "Andrea Montibeller",
      "Dasara Shullani",
      "Daniele Baracchi",
      "Alessandro Piva",
      "Giulia Boato"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.09337",
    "title": "Decoding Neural Emotion Patterns through Large Language Model Embeddings",
    "abstract": "           Understanding how emotional expression in language relates to brain function is a challenge in computational neuroscience and affective computing. Traditional neuroimaging is costly and lab-bound, but abundant digital text offers new avenues for emotion-brain mapping. Prior work has largely examined neuroimaging-based emotion localization or computational text analysis separately, with little integration. We propose a computational framework that maps textual emotional content to anatomically defined brain regions without requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate high-dimensional semantic representations, apply dimensionality reduction and clustering to identify emotional groups, and map them to 18 brain regions linked to emotional processing. Three experiments were conducted: i) analyzing conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to compare mapping patterns, ii) applying the method to the GoEmotions dataset and iii) comparing human-written text with large language model (LLM) responses to assess differences in inferred brain activation. Emotional intensity was scored via lexical analysis. Results showed neuroanatomically plausible mappings with high spatial specificity. Depressed subjects exhibited greater limbic engagement tied to negative affect. Discrete emotions were successfully differentiated. LLM-generated text matched humans in basic emotion distribution but lacked nuanced activation in empathy and self-referential regions (medial prefrontal and posterior cingulate cortex). This cost-effective, scalable approach enables large-scale analysis of naturalistic language, distinguishes between clinical populations, and offers a brain-based benchmark for evaluating AI emotional expression.         ",
    "url": "https://arxiv.org/abs/2508.09337",
    "authors": [
      "Gideon Vos",
      "Maryam Ebrahimpour",
      "Liza van Eijk",
      "Zoltan Sarnyai",
      "Mostafa Rahimi Azghadi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.21135",
    "title": "HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection",
    "abstract": "           Detecting hidden or partially concealed objects remains a fundamental challenge in multimodal environments, where factors like occlusion, camouflage, and lighting variations significantly hinder performance. Traditional RGB-based detection methods often fail under such adverse conditions, motivating the need for more robust, modality-agnostic approaches. In this work, we present HiddenObject, a fusion framework that integrates RGB, thermal, and depth data using a Mamba-based fusion mechanism. Our method captures complementary signals across modalities, enabling enhanced detection of obscured or camouflaged targets. Specifically, the proposed approach identifies modality-specific features and fuses them in a unified representation that generalizes well across challenging scenarios. We validate HiddenObject across multiple benchmark datasets, demonstrating state-of-the-art or competitive performance compared to existing methods. These results highlight the efficacy of our fusion design and expose key limitations in current unimodal and na\u00efve fusion strategies. More broadly, our findings suggest that Mamba-based fusion architectures can significantly advance the field of multimodal object detection, especially under visually degraded or complex conditions.         ",
    "url": "https://arxiv.org/abs/2508.21135",
    "authors": [
      "Harris Song",
      "Tuan-Anh Vu",
      "Sanjith Menon",
      "Sriram Narasimhan",
      "M. Khalid Jawed"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.01328",
    "title": "Can Large Language Models Master Complex Card Games?",
    "abstract": "           Complex games have long been an important benchmark for testing the progress of artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have defeated top human players in Go and Chess, garnering widespread societal attention towards artificial intelligence. Concurrently, large language models (LLMs) have exhibited remarkable capabilities across various tasks, raising the question of whether LLMs can achieve similar success in complex games. In this paper, we explore the potential of LLMs in mastering complex card games. We systematically assess the learning capabilities of LLMs across eight diverse card games, evaluating the impact of fine-tuning on high-quality gameplay data, and examining the models' ability to retain general capabilities while mastering these games. Our findings indicate that: (1) LLMs can approach the performance of strong game AIs through supervised fine-tuning on high-quality data, (2) LLMs can master multiple complex card games simultaneously, with performance augmentation for games with similar rules and conflicts for dissimilar ones, and (3) LLMs experience a decline in general capabilities when mastering complex games, but this decline can be mitigated by integrating a certain amount of general instruction data. The evaluation results demonstrate strong learning ability and versatility of LLMs.         ",
    "url": "https://arxiv.org/abs/2509.01328",
    "authors": [
      "Wei Wang",
      "Felix Henry",
      "Junzhe Chen",
      "Dan Zhang",
      "Shiyu Huang",
      "Evgeny Kharlamov",
      "Jie Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2509.03108",
    "title": "Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods",
    "abstract": "           Face recognition systems are robust against environmental changes and noise, and thus may be vulnerable to illegal authentication attempts using user face photos, such as spoofing attacks. To prevent such spoofing attacks, it is crucial to discriminate whether the input image is a live user image or a spoofed image prior to the face recognition process. Most existing spoofing attack detection methods utilize deep learning, which necessitates a substantial amount of training data. Consequently, if malicious data is injected into a portion of the training dataset, a specific spoofing attack may be erroneously classified as live, leading to false positives. In this paper, we propose a novel backdoor poisoning attack method to demonstrate the latent threat of backdoor poisoning within face anti-spoofing detection. The proposed method enables certain spoofing attacks to bypass detection by embedding features extracted from the spoofing attack's face image into a live face image without inducing any perceptible visual alterations. Through experiments conducted on public datasets, we demonstrate that the proposed method constitutes a realistic threat to existing spoofing attack detection systems.         ",
    "url": "https://arxiv.org/abs/2509.03108",
    "authors": [
      "Shota Iwamatsu",
      "Koichi Ito",
      "Takafumi Aoki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.03951",
    "title": "ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD Detection",
    "abstract": "           The introduction of negative labels (NLs) has proven effective in enhancing Out-of-Distribution (OOD) detection. However, existing methods often lack an understanding of OOD images, making it difficult to construct an accurate negative space. In addition, the presence of false negative labels significantly degrades their near-OOD performance. To address these issues, we propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the understanding and reasoning capabilities of multimodal large language models (MLLMs). Specifically, we identify images likely to be OOD samples as negative images and prompt the MLLM to describe these images, generating expressive negative sentences that precisely characterize the OOD distribution and enhance far-OOD detection. For the near-OOD setting, where OOD samples resemble the in-distribution (ID) subset, we first identify the subset of ID classes that are visually similar to negative images and then leverage the reasoning capability of MLLMs to generate visually similar negative labels tailored to this subset, effectively reducing false negatives and improving near-OOD detection. To balance these two types of negative textual spaces, we design an adaptive weighted score that enables the method to handle different OOD task settings (near-OOD and far-OOD) without relying on task-specific prior knowledge, making it highly adaptable in open environments. On the ImageNet benchmark, our ANTS significantly reduces the FPR95 by 4.2\\%, establishing a new state-of-the-art. Furthermore, our method is training-free and zero-shot, enabling high scalability.         ",
    "url": "https://arxiv.org/abs/2509.03951",
    "authors": [
      "Wenjie Zhu",
      "Yabin Zhang",
      "Xin Jin",
      "Wenjun Zeng",
      "Lei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.05002",
    "title": "Graph Reconstruction with a Connected Components Oracle",
    "abstract": "           In the Graph Reconstruction (GR) problem, the goal is to recover a hidden graph by utilizing some oracle that provides limited access to the structure of the graph. The interest is in characterizing how strong different oracles are when the complexity of an algorithm is measured in the number of performed queries. We study a novel oracle that returns the set of connected components (CC) on the subgraph induced by the queried subset of vertices. Our main contributions are as follows: 1. For a hidden graph with $n$ vertices, $m$ edges, maximum degree $\\Delta$, and treewidth $k$, GR can be solved in $O(\\min\\{m / \\log m, \\Delta^2, k^2\\} \\cdot \\log n)$ CC queries by an adaptive randomized algorithm. 2. For a hidden graph with $n$ vertices and degeneracy $d$, GR can be solved in $O(d^2 \\log^2 n)$ CC queries by an adaptive randomized algorithm. 3. For a hidden graph with $n$ vertices, $m$ edges, maximum degree $\\Delta$, and treewidth $k$, no algorithm can solve GR in $o(\\min\\{m, \\Delta^2, k^2\\})$ CC queries.         ",
    "url": "https://arxiv.org/abs/2509.05002",
    "authors": [
      "Juha Harviainen",
      "Pekka Parviainen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2509.06591",
    "title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising",
    "abstract": "           Low-dose computed tomography (LDCT) and positron emission tomography (PET) have emerged as safer alternatives to conventional imaging modalities by significantly reducing radiation exposure. However, this reduction often results in increased noise and artifacts, which can compromise diagnostic accuracy. Consequently, denoising for LDCT/PET has become a vital area of research aimed at enhancing image quality while maintaining radiation safety. In this study, we introduce a novel Hybrid Swin Attention Network (HSANet), which incorporates Efficient Global Attention (EGA) modules and a hybrid upsampling module. The EGA modules enhance both spatial and channel-wise interaction, improving the network's capacity to capture relevant features, while the hybrid upsampling module mitigates the risk of overfitting to noise. We validate the proposed approach using a publicly available LDCT/PET dataset. Experimental results demonstrate that HSANet achieves superior denoising performance compared to existing methods, while maintaining a lightweight model size suitable for deployment on GPUs with standard memory configurations. This makes our approach highly practical for real-world clinical applications.         ",
    "url": "https://arxiv.org/abs/2509.06591",
    "authors": [
      "Yichao Liu",
      "Hengzhi Xue",
      "YueYang Teng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.08440",
    "title": "Augmenting Neural Networks-based Model Approximators in Robotic Force-tracking Tasks",
    "abstract": "           As robotics gains popularity, interaction control becomes crucial for ensuring force tracking in manipulator-based tasks. Typically, traditional interaction controllers either require extensive tuning, or demand expert knowledge of the environment, which is often impractical in real-world applications. This work proposes a novel control strategy leveraging Neural Networks (NNs) to enhance the force-tracking behavior of a Direct Force Controller (DFC). Unlike similar previous approaches, it accounts for the manipulator's tangential velocity, a critical factor in force exertion, especially during fast motions. The method employs an ensemble of feedforward NNs to predict contact forces, then exploits the prediction to solve an optimization problem and generate an optimal residual action, which is added to the DFC output and applied to an impedance controller. The proposed Velocity-augmented Artificial intelligence Interaction Controller for Ambiguous Models (VAICAM) is validated in the Gazebo simulator on a Franka Emika Panda robot. Against a vast set of trajectories, VAICAM achieves superior performance compared to two baseline controllers.         ",
    "url": "https://arxiv.org/abs/2509.08440",
    "authors": [
      "Kevin Saad",
      "Vincenzo Petrone",
      "Enrico Ferrentino",
      "Pasquale Chiacchio",
      "Francesco Braghin",
      "Loris Roveda"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2509.08863",
    "title": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation",
    "abstract": "           LLMs have made substantial progress in task automation and natural language understanding. However, without expertise in GIS, they continue to encounter limitations. To address these issues, we propose GeoJSON Agents-a multi-agent LLM architecture. This framework transforms natural language tasks into structured GeoJSON operation commands and processes spatial data using two widely adopted LLM enhancement techniques: Function Calling and Code Generation. The architecture consists of three components-task parsing, agent collaboration, and result integration-aimed at enhancing both the performance and scalability of GIS automation. The Planner agent interprets natural language tasks into structured GeoJSON commands. Then, specialized Worker agents collaborate according to assigned roles to perform spatial data processing and analysis, either by invoking predefined function APIs or by dynamically generating and executing Python-based spatial analysis code. Finally, the system integrates the outputs from multiple execution rounds into reusable, standards-compliant GeoJSON files. To systematically evaluate the performance of the two approaches, we constructed a benchmark dataset of 70 tasks with varying complexity and conducted experiments using OpenAI's GPT-4o as the core model. Results indicate that the Function Calling-based GeoJSON Agent achieved an accuracy of 85.71%, while the Code Generation-based agent reached 97.14%, both significantly outperforming the best-performing general-purpose model (48.57%). Further analysis reveals that the Code Generation provides greater flexibility, whereas the Function Calling approach offers more stable execution. This study is the first to introduce an LLM multi-agent framework for GeoJSON data and to compare the strengths and limitations of two mainstream LLM enhancement methods, offering new perspectives for improving GeoAI system performance.         ",
    "url": "https://arxiv.org/abs/2509.08863",
    "authors": [
      "Qianqian Luo",
      "Liuchang Xu",
      "Qingming Lin",
      "Sensen Wu",
      "Ruichen Mao",
      "Chao Wang",
      "Hailin Feng",
      "Bo Huang",
      "Zhenhong Du"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2509.08926",
    "title": "Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures",
    "abstract": "           Object re-identification (Re-ID) methods are highly sensitive to label noise, which typically leads to significant performance degradation. We address this challenge by reframing Re-ID as a supervised image similarity task and adopting a Siamese network architecture trained to capture discriminative pairwise relationships. Central to our approach is a novel statistical outlier detection (OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier Detection), which models the distribution of cosine similarities between embedding pairs using a two-component Beta distribution mixture model. We establish a novel identifiability result for mixtures of two Beta distributions, ensuring that our learning task is well-posed. The proposed OD step complements the Re-ID architecture combining binary cross-entropy, contrastive, and cosine embedding losses that jointly optimize feature-level similarity this http URL demonstrate the effectiveness of Beta-SOD in de-noising and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance compared to the state-of-the-art methods across various noise levels (10-30\\%), demonstrating both robustness and broad applicability in noisy Re-ID scenarios. The implementation of Beta-SOD is available at: this http URL ",
    "url": "https://arxiv.org/abs/2509.08926",
    "authors": [
      "Waqar Ahmad",
      "Evan Murphy",
      "Vladimir A. Krylov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2509.08934",
    "title": "SFD-Mamba2Net: Structure-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation",
    "abstract": "           Background: Coronary Artery Disease (CAD) is one of the leading causes of death worldwide. Invasive Coronary Angiography (ICA), regarded as the gold standard for CAD diagnosis, necessitates precise vessel segmentation and stenosis detection. However, ICA images are typically characterized by low contrast, high noise levels, and complex, fine-grained vascular structures, which pose significant challenges to the clinical adoption of existing segmentation and detection methods. Objective: This study aims to improve the accuracy of coronary artery segmentation and stenosis detection in ICA images by integrating multi-scale structural priors, state-space-based long-range dependency modeling, and frequency-domain detail enhancement strategies. Methods: We propose SFD-Mamba2Net, an end-to-end framework tailored for ICA-based vascular segmentation and stenosis detection. In the encoder, a Curvature-Aware Structural Enhancement (CASE) module is embedded to leverage multi-scale responses for highlighting slender tubular vascular structures, suppressing background interference, and directing attention toward vascular regions. In the decoder, we introduce a Progressive High-Frequency Perception (PHFP) module that employs multi-level wavelet decomposition to progressively refine high-frequency details while integrating low-frequency global structures. Results and Conclusions: SFD-Mamba2Net consistently outperformed state-of-the-art methods across eight segmentation metrics, and achieved the highest true positive rate and positive predictive value in stenosis detection.         ",
    "url": "https://arxiv.org/abs/2509.08934",
    "authors": [
      "Nan Mu",
      "Ruiqi Song",
      "Zhihui Xu",
      "Jingfeng Jiang",
      "Chen Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.09501",
    "title": "Region-Wise Correspondence Prediction between Manga Line Art Images",
    "abstract": "           Understanding region-wise correspondence between manga line art images is a fundamental task in manga processing, enabling downstream applications such as automatic line art colorization and in-between frame generation. However, this task remains largely unexplored, especially in realistic scenarios without pre-existing segmentation or annotations. In this paper, we introduce a novel and practical task: predicting region-wise correspondence between raw manga line art images without any pre-existing labels or masks. To tackle this problem, we divide each line art image into a set of patches and propose a Transformer-based framework that learns patch-level similarities within and across images. We then apply edge-aware clustering and a region matching algorithm to convert patch-level predictions into coherent region-level correspondences. To support training and evaluation, we develop an automatic annotation pipeline and manually refine a subset of the data to construct benchmark datasets. Experiments on multiple datasets demonstrate that our method achieves high patch-level accuracy (e.g., 96.34%) and generates consistent region-level correspondences, highlighting its potential for real-world manga applications.         ",
    "url": "https://arxiv.org/abs/2509.09501",
    "authors": [
      "Yingxuan Li",
      "Jiafeng Mao",
      "Qianru Qiu",
      "Yusuke Matsui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.09550",
    "title": "Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates",
    "abstract": "           Neural Audio Codecs (NACs) have become increasingly adopted in speech processing tasks due to their excellent rate-distortion performance and compatibility with Large Language Models (LLMs) as discrete feature representations for audio generation. While most existing codecs rely on Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has recently emerged as a compelling alternative that simplifies training and natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC, and show that FSQ encodes baked-in redundancy which produces an encoding which is robust when transmitted through noisy channels. First, through an encoder distillation experiment, we show that two different encoders can learn to encode identical audio into vastly different code sequences whilst maintaining comparable reconstruction quality with the same quantizer and decoder. Second, we demonstrate that FSQ has vastly superior bit-level perturbation robustness by comparing the performance of RVQ and FSQ codecs when simulating the transmission of code sequences through a noisy channel.         ",
    "url": "https://arxiv.org/abs/2509.09550",
    "authors": [
      "Harry Julian",
      "Rachel Beeson",
      "Lohith Konathala",
      "Johanna Ulin",
      "Jiameng Gao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.14484",
    "title": "PL-Net: Progressive Learning Network for Medical Image Segmentation",
    "abstract": "           In recent years, deep convolutional neural network-based segmentation methods have achieved state-of-the-art performance for many medical analysis tasks. However, most of these approaches rely on optimizing the U-Net structure or adding new functional modules, which overlooks the complementation and fusion of coarse-grained and fine-grained semantic information. To address these issues, we propose a 2D medical image segmentation framework called Progressive Learning Network (PL-Net), which comprises Internal Progressive Learning (IPL) and External Progressive Learning (EPL). PL-Net offers the following advantages: (1) IPL divides feature extraction into two steps, allowing for the mixing of different size receptive fields and capturing semantic information from coarse to fine granularity without introducing additional parameters; (2) EPL divides the training process into two stages to optimize parameters and facilitate the fusion of coarse-grained information in the first stage and fine-grained information in the second stage. We conducted comprehensive evaluations of our proposed method on five medical image segmentation datasets, and the experimental results demonstrate that PL-Net achieves competitive segmentation performance. It is worth noting that PL-Net does not introduce any additional learnable parameters compared to other U-Net variants.         ",
    "url": "https://arxiv.org/abs/2110.14484",
    "authors": [
      "Kunpeng Mao",
      "Ruoyu Li",
      "Junlong Cheng",
      "Danmei Huang",
      "Zhiping Song",
      "ZeKui Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.21194",
    "title": "Prior shift estimation for positive unlabeled data through the lens of kernel embedding",
    "abstract": "           We study estimation of a class prior for unlabeled target samples which possibly differs from that of source population. Moreover, it is assumed that the source data is partially observable: only samples from the positive class and from the whole population are available (PU learning scenario). We introduce a novel direct estimator of a class prior which avoids estimation of posterior probabilities in both populations and has a simple geometric interpretation. It is based on a distribution matching technique together with kernel embedding in a Reproducing Kernel Hilbert Space and is obtained as an explicit solution to an optimisation task. We establish its asymptotic consistency as well as an explicit non-asymptotic bound on its deviation from the unknown prior, which is calculable in practice. We study finite sample behaviour for synthetic and real data and show that the proposal works consistently on par or better than its competitors.         ",
    "url": "https://arxiv.org/abs/2502.21194",
    "authors": [
      "Jan Mielniczuk",
      "Wojciech Rejchel",
      "Pawe\u0142 Teisseyre"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.05367",
    "title": "Semi-Supervised Learning for Dose Prediction in Targeted Radionuclide: A Synthetic Data Study",
    "abstract": "           Targeted Radionuclide Therapy (TRT) is a modern strategy in radiation oncology that aims to administer a potent radiation dose specifically to cancer cells using cancer-targeting radiopharmaceuticals. Accurate radiation dose estimation tailored to individual patients is crucial. Deep learning, particularly with pre-therapy imaging, holds promise for personalizing TRT doses. However, current methods require large time series of SPECT imaging, which is hardly achievable in routine clinical practice, and thus raises issues of data availability. Our objective is to develop a semi-supervised learning (SSL) solution to personalize dosimetry using pre-therapy images. The aim is to develop an approach that achieves accurate results when PET/CT images are available, but are associated with only a few post-therapy dosimetry data provided by SPECT images. In this work, we introduce an SSL method using a pseudo-label generation approach for regression tasks inspired by the FixMatch framework. The feasibility of the proposed solution was preliminarily evaluated through an in-silico study using synthetic data and Monte Carlo simulation. Experimental results for organ dose prediction yielded promising outcomes, showing that the use of pseudo-labeled data provides better accuracy compared to using only labeled data.         ",
    "url": "https://arxiv.org/abs/2503.05367",
    "authors": [
      "Jing Zhang",
      "Alexandre Bousse",
      "Chi-Hieu Pham",
      "Kuangyu Shi",
      "Julien Bert"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.23408",
    "title": "Quantum-Assisted Machine Learning Models for Enhanced Weather Prediction",
    "abstract": "           Quantum Machine Learning (QML) presents as a revolutionary approach to weather forecasting by using quantum computing to improve predictive modeling capabilities. In this study, we apply QML models, including Quantum Gated Recurrent Units (QGRUs), Quantum Neural Networks (QNNs), Quantum Long Short-Term Memory(QLSTM), Variational Quantum Circuits(VQCs), and Quantum Support Vector Machines(QSVMs), to analyze meteorological time-series data from the ERA5 dataset. Our methodology includes preprocessing meteorological features, implementing QML architectures for both classification and regression tasks. The results demonstrate that QML models can achieve reasonable accuracy in both prediction and classification tasks, particularly in binary classification. However, challenges such as quantum hardware limitations and noise affect scalability and generalization. This research provides insights into the feasibility of QML for weather prediction, paving the way for further exploration of hybrid quantum-classical frameworks to enhance meteorological forecasting.         ",
    "url": "https://arxiv.org/abs/2503.23408",
    "authors": [
      "Saiyam Sakhuja",
      "Shivanshu Siyanwal",
      "Abhishek Tiwari",
      "Britant",
      "Savita Kashyap"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.08159",
    "title": "Self-Optimizing Machine Learning Potential Assisted Automated Workflow for Highly Efficient Complex Systems Material Design",
    "abstract": "           Machine learning interatomic potentials have revolutionized complex materials design by enabling rapid exploration of material configurational spaces via crystal structure prediction with ab initio accuracy. However, critical challenges persist in ensuring robust generalization to unknown structures and minimizing the requirement for substantial expert knowledge and time-consuming manual interventions. Here, we propose an automated crystal structure prediction framework built upon the attention-coupled neural networks potential to address these limitations. The generalizability of the potential is achieved by sampling regions across the local minima of the potential energy surface, where the self-evolving pipeline autonomously refines the potential iteratively while minimizing human intervention. The workflow is validated on Mg-Ca-H ternary and Be-P-N-O quaternary systems by exploring nearly 10 million configurations, demonstrating substantial speedup compared to first-principles calculations. These results underscore the effectiveness of our approach in accelerating the exploration and discovery of complex multi-component functional materials.         ",
    "url": "https://arxiv.org/abs/2505.08159",
    "authors": [
      "Jiaxiang Li",
      "Junwei Feng",
      "Jie Luo",
      "Bowen Jiang",
      "Xiangyu Zheng",
      "Qigang Song",
      "Jian Lv",
      "Keith Butler",
      "Hanyu Liu",
      "Congwei Xie",
      "Yu Xie",
      "Yanming Ma"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.04779",
    "title": "Constructive Universal Approximation and Sure Convergence for Multi-Layer Neural Networks",
    "abstract": "           We propose o1Neuro, a new neural network model built on sparse indicator activation neurons, with two key statistical properties. (1) Constructive universal approximation: At the population level, a deep o1Neuro can approximate any measurable function of $\\boldsymbol{X}$, while a shallow o1Neuro suffices for additive models with two-way interaction components, including XOR and univariate terms, assuming $\\boldsymbol{X} \\in [0,1]^p$ has bounded density. Combined with prior work showing that a single-hidden-layer non-sparse network is a universal approximator, this highlights a trade-off between activation sparsity and network depth in approximation capability. (2) Sure convergence: At the sample level, the optimization of o1Neuro reaches an optimal model with probability approaching one after sufficiently many update rounds, and we provide an example showing that the required number of updates is well bounded under linear data-generating models. Empirically, o1Neuro is compared with XGBoost, Random Forests, and TabNet for learning complex regression functions with interactions, demonstrating superior predictive performance on several benchmark datasets from OpenML and the UCI Machine Learning Repository with $n = 10000$, as well as on synthetic datasets with $100 \\le n \\le 20000$.         ",
    "url": "https://arxiv.org/abs/2507.04779",
    "authors": [
      "Chien-Ming Chi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  }
]