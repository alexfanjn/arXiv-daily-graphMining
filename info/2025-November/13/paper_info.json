[
  {
    "id": "arXiv:2511.08593",
    "title": "Knowledge Graph Analysis of Legal Understanding and Violations in LLMs",
    "abstract": "           The rise of Large Language Models (LLMs) offers transformative potential for interpreting complex legal frameworks, such as Title 18 Section 175 of the US Code, which governs biological weapons. These systems hold promise for advancing legal analysis and compliance monitoring in sensitive domains. However, this capability comes with a troubling contradiction: while LLMs can analyze and interpret laws, they also demonstrate alarming vulnerabilities in generating unsafe outputs, such as actionable steps for bioweapon creation, despite their safeguards. To address this challenge, we propose a methodology that integrates knowledge graph construction with Retrieval-Augmented Generation (RAG) to systematically evaluate LLMs' understanding of this law, their capacity to assess legal intent (mens rea), and their potential for unsafe applications. Through structured experiments, we assess their accuracy in identifying legal violations, generating prohibited instructions, and detecting unlawful intent in bioweapons-related scenarios. Our findings reveal significant limitations in LLMs' reasoning and safety mechanisms, but they also point the way forward. By combining enhanced safety protocols with more robust legal reasoning frameworks, this research lays the groundwork for developing LLMs that can ethically and securely assist in sensitive legal domains - ensuring they act as protectors of the law rather than inadvertent enablers of its violation.         ",
    "url": "https://arxiv.org/abs/2511.08593",
    "authors": [
      "Abha Jha",
      "Abel Salinas",
      "Fred Morstatter"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2511.08596",
    "title": "What About the Scene with the Hitler Reference? HAUNT: A Framework to Probe LLMs' Self-consistency Via Adversarial Nudge",
    "abstract": "           Hallucinations pose a critical challenge to the real-world deployment of large language models (LLMs) in high-stakes domains. In this paper, we present a framework for stress testing factual fidelity in LLMs in the presence of adversarial nudge. Our framework consists of three steps. In the first step, we instruct the LLM to produce sets of truths and lies consistent with the closed domain in question. In the next step, we instruct the LLM to verify the same set of assertions as truths and lies consistent with the same closed domain. In the final step, we test the robustness of the LLM against the lies generated (and verified) by itself. Our extensive evaluation, conducted using five widely known proprietary LLMs across two closed domains of popular movies and novels, reveals a wide range of susceptibility to adversarial nudges: \\texttt{Claude} exhibits strong resilience, \\texttt{GPT} and \\texttt{Grok} demonstrate moderate resilience, while \\texttt{Gemini} and \\texttt{DeepSeek} show weak resilience. Considering that a large population is increasingly using LLMs for information seeking, our findings raise alarm.         ",
    "url": "https://arxiv.org/abs/2511.08596",
    "authors": [
      "Arka Dutta",
      "Sujan Dutta",
      "Rijul Magu",
      "Soumyajit Datta",
      "Munmun De Choudhury",
      "Ashiqur R. KhudaBukhsh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2511.08615",
    "title": "A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking",
    "abstract": "           Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\\sim$90\\% detection and tracking accuracy, as well as successfully tracks $\\sim$80\\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.         ",
    "url": "https://arxiv.org/abs/2511.08615",
    "authors": [
      "Kosta Dakic",
      "Kanchana Thilakarathna",
      "Rodrigo N. Calheiros",
      "Teng Joon Lim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2511.08628",
    "title": "Learning Topology-Driven Multi-Subspace Fusion for Grassmannian Deep Network",
    "abstract": "           Grassmannian manifold offers a powerful carrier for geometric representation learning by modelling high-dimensional data as low-dimensional subspaces. However, existing approaches predominantly rely on static single-subspace representations, neglecting the dynamic interplay between multiple subspaces critical for capturing complex geometric structures. To address this limitation, we propose a topology-driven multi-subspace fusion framework that enables adaptive subspace collaboration on the Grassmannian. Our solution introduces two key innovations: (1) Inspired by the Kolmogorov-Arnold representation theorem, an adaptive multi-subspace modelling mechanism is proposed that dynamically selects and weights task-relevant subspaces via topological convergence analysis, and (2) a multi-subspace interaction block that fuses heterogeneous geometric representations through Fr\u00e9chet mean optimisation on the manifold. Theoretically, we establish the convergence guarantees of adaptive subspaces under a projection metric topology, ensuring stable gradient-based optimisation. Practically, we integrate Riemannian batch normalisation and mutual information regularisation to enhance discriminability and robustness. Extensive experiments on 3D action recognition (HDM05, FPHA), EEG classification (MAMEM-SSVEPII), and graph tasks demonstrate state-of-the-art performance. Our work not only advances geometric deep learning but also successfully adapts the proven multi-channel interaction philosophy of Euclidean networks to non-Euclidean domains, achieving superior discriminability and interpretability.         ",
    "url": "https://arxiv.org/abs/2511.08628",
    "authors": [
      "Xuan Yu",
      "Tianyang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.08634",
    "title": "CADIC: Continual Anomaly Detection Based on Incremental Coreset",
    "abstract": "           The primary objective of Continual Anomaly Detection (CAD) is to learn the normal patterns of new tasks under dynamic data distribution assumptions while mitigating catastrophic forgetting. Existing embedding-based CAD approaches continuously update a memory bank with new embeddings to adapt to sequential tasks. However, these methods require constructing class-specific sub-memory banks for each task, which restricts their flexibility and scalability. To address this limitation, we propose a novel CAD framework where all tasks share a unified memory bank. During training, the method incrementally updates embeddings within a fixed-size coreset, enabling continuous knowledge acquisition from sequential tasks without task-specific memory fragmentation. In the inference phase, anomaly scores are computed via a nearest-neighbor matching mechanism, achieving state-of-the-art detection accuracy. We validate the method through comprehensive experiments on MVTec AD and Visa datasets. Results show that our approach outperforms existing baselines, achieving average image-level AUROC scores of 0.972 (MVTec AD) and 0.891 (Visa). Notably, on a real-world electronic paper dataset, it demonstrates 100% accuracy in anomaly sample detection, confirming its robustness in practical scenarios. The implementation will be open-sourced on GitHub.         ",
    "url": "https://arxiv.org/abs/2511.08634",
    "authors": [
      "Gen Yang",
      "Zhipeng Deng",
      "Junfeng Man"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.08651",
    "title": "RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation",
    "abstract": "           Dynamic Scene Graph Generation (DSGG) models how object relations evolve over time in videos. However, existing methods are trained only on annotated object pairs and lack guidance for non-related pairs, making it difficult to identify meaningful relations during inference. In this paper, we propose Relation Scoring Network (RS-Net), a modular framework that scores the contextual importance of object pairs using both spatial interactions and long-range temporal context. RS-Net consists of a spatial context encoder with learnable context tokens and a temporal encoder that aggregates video-level information. The resulting relation scores are integrated into a unified triplet scoring mechanism to enhance relation prediction. RS-Net can be easily integrated into existing DSGG models without architectural changes. Experiments on the Action Genome dataset show that RS-Net consistently improves both Recall and Precision across diverse baselines, with notable gains in mean Recall, highlighting its ability to address the long-tailed distribution of relations. Despite the increased number of parameters, RS-Net maintains competitive efficiency, achieving superior performance over state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2511.08651",
    "authors": [
      "Hae-Won Jo",
      "Yeong-Jun Cho"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.08655",
    "title": "Learning the Basis: A Kolmogorov-Arnold Network Approach Embedding Green's Function Priors",
    "abstract": "           The Method of Moments (MoM) is constrained by the usage of static, geometry-defined basis functions, such as the Rao-Wilton-Glisson (RWG) basis. This letter reframes electromagnetic modeling around a learnable basis representation rather than solving for the coefficients over a fixed basis. We first show that the RWG basis is essentially a static and piecewise-linear realization of the Kolmogorov-Arnold representation theorem. Inspired by this insight, we propose PhyKAN, a physics-informed Kolmogorov-Arnold Network (KAN) that generalizes RWG into a learnable and adaptive basis family. Derived from the EFIE, PhyKAN integrates a local KAN branch with a global branch embedded with Green's function priors to preserve physical consistency. It is demonstrated that, across canonical geometries, PhyKAN achieves sub-0.01 reconstruction errors as well as accurate, unsupervised radar cross section predictions, offering an interpretable, physics-consistent bridge between classical solvers and modern neural network models for electromagnetic modeling.         ",
    "url": "https://arxiv.org/abs/2511.08655",
    "authors": [
      "Rui Zhu",
      "Yuexing Peng",
      "George C. Alexandropoulos",
      "Wenbo Wang",
      "Wei Xiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.08666",
    "title": "Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding",
    "abstract": "           We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.         ",
    "url": "https://arxiv.org/abs/2511.08666",
    "authors": [
      "Joseph Fioresi",
      "Ishan Rajendrakumar Dave",
      "Mubarak Shah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.08697",
    "title": "PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation",
    "abstract": "           Accurate and efficient simulations of physical phenomena governed by partial differential equations (PDEs) are important for scientific and engineering progress. While traditional numerical solvers are powerful, they are often computationally expensive. Recently, data-driven methods have emerged as alternatives, but they frequently suffer from error accumulation and limited physical consistency, especially in multiphysics and complex geometries. To address these challenges, we propose PEGNet, a Physics-Embedded Graph Network that incorporates PDE-guided message passing to redesign the graph neural network architecture. By embedding key PDE dynamics like convection, viscosity, and diffusion into distinct message functions, the model naturally integrates physical constraints into its forward propagation, producing more stable and physically consistent solutions. Additionally, a hierarchical architecture is employed to capture multi-scale features, and physical regularization is integrated into the loss function to further enforce adherence to governing physics. We evaluated PEGNet on benchmarks, including custom datasets for respiratory airflow and drug delivery, showing significant improvements in long-term prediction accuracy and physical consistency over existing methods. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.08697",
    "authors": [
      "Can Yang",
      "Zhenzhong Wang",
      "Junyuan Liu",
      "Yunpeng Gong",
      "Min Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.08708",
    "title": "Stabilizing Direct Training of Spiking Neural Networks: Membrane Potential Initialization and Threshold-robust Surrogate Gradient",
    "abstract": "           Recent advancements in the direct training of Spiking Neural Networks (SNNs) have demonstrated high-quality outputs even at early timesteps, paving the way for novel energy-efficient AI paradigms. However, the inherent non-linearity and temporal dependencies in SNNs introduce persistent challenges, such as temporal covariate shift (TCS) and unstable gradient flow with learnable neuron thresholds. In this paper, we present two key innovations: MP-Init (Membrane Potential Initialization) and TrSG (Threshold-robust Surrogate Gradient). MP-Init addresses TCS by aligning the initial membrane potential with its stationary distribution, while TrSG stabilizes gradient flow with respect to threshold voltage during training. Extensive experiments validate our approach, achieving state-of-the-art accuracy on both static and dynamic image datasets. The code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2511.08708",
    "authors": [
      "Hyunho Kook",
      "Byeongho Yu",
      "Jeong Min Oh",
      "Eunhyeok Park"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.08733",
    "title": "Gromov-Wasserstein Graph Coarsening",
    "abstract": "           We study the problem of graph coarsening within the Gromov-Wasserstein geometry. Specifically, we propose two algorithms that leverage a novel representation of the distortion induced by merging pairs of nodes. The first method, termed Greedy Pair Coarsening (GPC), iteratively merges pairs of nodes that locally minimize a measure of distortion until the desired size is achieved. The second method, termed $k$-means Greedy Pair Coarsening (KGPC), leverages clustering based on pairwise distortion metrics to directly merge clusters of nodes. We provide conditions guaranteeing optimal coarsening for our methods and validate their performance on six large-scale datasets and a downstream clustering task. Results show that the proposed methods outperform existing approaches on a wide range of parameters and scenarios.         ",
    "url": "https://arxiv.org/abs/2511.08733",
    "authors": [
      "Carlos A. Taveras",
      "Santiago Segarra",
      "C\u00e9sar A. Uribe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.08749",
    "title": "Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning",
    "abstract": "           Reinforcement learning has traditionally focused on a singular objective: learning policies that select actions to maximize reward. We challenge this paradigm by asking: what if we explicitly architected RL systems as inference engines that can answer diverse queries about their environment? In deterministic settings, trained agents implicitly encode rich knowledge about reachability, distances, values, and dynamics - yet current architectures are not designed to expose this information efficiently. We introduce Query Conditioned Deterministic Inference Networks (QDIN), a unified architecture that treats different types of queries (policy, reachability, paths, comparisons) as first-class citizens, with specialized neural modules optimized for each inference pattern. Our key empirical finding reveals a fundamental decoupling: inference accuracy can reach near-perfect levels (99% reachability IoU) even when control performance remains suboptimal (31% return), suggesting that the representations needed for accurate world knowledge differ from those required for optimal control. Experiments demonstrate that query specialized architectures outperform both unified models and post-hoc extraction methods, while maintaining competitive control performance. This work establishes a research agenda for RL systems designed from inception as queryable knowledge bases, with implications for interpretability, verification, and human-AI collaboration.         ",
    "url": "https://arxiv.org/abs/2511.08749",
    "authors": [
      "Mehrdad Zakershahrak"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.08752",
    "title": "Information-Driven Fault Detection and Identification for Multi-Agent Spacecraft Systems: Collaborative On-Orbit Inspection Mission",
    "abstract": "           This work presents a global-to-local, task-aware fault detection and identification (FDI) framework for multi-spacecraft systems conducting collaborative inspection missions in low Earth orbit. The inspection task is represented by a global information-driven cost functional that integrates the sensor model, spacecraft poses, and mission-level information-gain objectives. This formulation links guidance, control, and FDI by using the same cost function to drive both global task allocation and local sensing or motion decisions. Fault detection is achieved through comparisons between expected and observed task metrics, while higher-order cost-gradient measures enable the identification of faults among sensors, actuators, and state estimators. An adaptive thresholding mechanism captures the time-varying inspection geometry and dynamic mission conditions. Simulation results for representative multi-spacecraft inspection scenarios demonstrate the reliability of fault localization and classification under uncertainty, providing a unified, information-driven foundation for resilient autonomous inspection architectures.         ",
    "url": "https://arxiv.org/abs/2511.08752",
    "authors": [
      "Akshita Gupta",
      "Arna Bhardwaj",
      "Yashwanth Kumar Nakka",
      "Changrak Choi",
      "Amir Rahmani"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2511.08753",
    "title": "Fourier Neural Operators for Structural Dynamics Models: Challenges, Limitations and Advantages of Using a Spectrogram Loss",
    "abstract": "           Fourier Neural Operators (FNOs) have emerged as promising surrogates for partial differential equation solvers. In this work, we extensively tested FNOs on a variety of systems with non-linear and non-stationary properties, using a wide range of forcing functions to isolate failure mechanisms. FNOs stand out in modeling linear systems, regardless of complexity, while achieving near-perfect energy preservation and accurate spectral representation for linear dynamics. However, they fail on non-linear systems, where the failure manifests as artificial energy dissipation and manipulated frequency content. This limitation persists regardless of training dataset size, and we discuss the root cause through discretization error analysis. Comparison with LSTM as the baseline shows FNOs are superior for both linear and non-linear systems, independent of the training dataset size. We develop a spectrogram-based loss function that combines time-domain Mean Squared Error (MSE) with frequency-domain magnitude and phase errors, addressing the low-frequency bias of FNOs. This frequency-aware training eliminates artificial dissipation in linear systems and enhances the energy ratios of non-linear systems. The IEA 15MW turbine model validates our findings. Despite hundreds of degrees of freedom, FNO predictions remain accurate because the turbine behaves in a predominantly linear regime. Our findings establish that system non-linearity, rather than dimensionality or complexity, determines the success of FNO. These results provide clear guidelines for practitioners and challenge assumptions about FNOs' universality.         ",
    "url": "https://arxiv.org/abs/2511.08753",
    "authors": [
      "Rad Haghi",
      "Bipin Gaikwad",
      "Abani Patra"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2511.08762",
    "title": "Reservoir Computing-Based Detection for Molecular Communications",
    "abstract": "           Diffusion-based Molecular Communication (MC) is inherently challenged by severe inter-symbol interference (ISI). This is significantly amplified in mobile scenarios, where the channel impulse response (CIR) becomes time-varying and stochastic. Obtaining accurate Channel State Information (CSI) for traditional model-based detection is intractable in such dynamic environments. While deep learning (DL) offers adaptability, its complexity is unsuitable for resource-constrained micro/nanodevices. This paper proposes a low-complexity Reservoir Computing (RC) based detector. The RC architecture utilizes a fixed, recurrent non-linear reservoir to project the time-varying received signal into a high-dimensional state space. This effectively transforms the complex temporal detection problem into a simple linear classification task, capturing ISI dynamics without explicit channel modeling or complex retraining. Evaluated in a realistic 3D mobile MC simulation environment (Smoldyn), our RC detector significantly outperforms classical detectors and achieves superior performance compared to complex ML methods (LSTM, CNN, MLP) under severe ISI. Importantly, RC achieves this with significantly fewer trainable parameters (e.g., 300 vs. up to 264k for MLP) and ultra-low latency inference (approx. 1 $\\mu$s per symbol).         ",
    "url": "https://arxiv.org/abs/2511.08762",
    "authors": [
      "Abdulkadir Bilge",
      "Eren Akyol",
      "Murat Kuscu"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2511.08801",
    "title": "A Simple Analysis of Ranking in General Graphs",
    "abstract": "           We provide a simple combinatorial analysis of the Ranking algorithm, originally introduced in the seminal work by Karp, Vazirani, and Vazirani [KVV90], demonstrating that it achieves a $(1/2 + c)$-approximate matching for general graphs for $c \\geq 0.005$.         ",
    "url": "https://arxiv.org/abs/2511.08801",
    "authors": [
      "Mahsa Derakhshan",
      "Mohammad Roghani",
      "Mohammad Saneian",
      "Tao Yu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2511.08809",
    "title": "Adaptive graph Kolmogorov-Arnold network for 3D human pose estimation",
    "abstract": "           Graph convolutional network (GCN)-based methods have shown strong performance in 3D human pose estimation by leveraging the natural graph structure of the human skeleton. However, their local receptive field limits their ability to capture long-range dependencies essential for handling occlusions and depth ambiguities. They also exhibit spectral bias, which prioritizes low-frequency components while struggling to model high-frequency details. In this paper, we introduce PoseKAN, an adaptive graph Kolmogorov-Arnold Network (KAN), framework that extends KANs to graph-based learning for 2D-to-3D pose lifting from a single image. Unlike GCNs that use fixed activation functions, KANs employ learnable functions on graph edges, allowing data-driven, adaptive feature transformations. This enhances the model's adaptability and expressiveness, making it more expressive in learning complex pose variations. Our model employs multi-hop feature aggregation, ensuring the body joints can leverage information from both local and distant neighbors, leading to improved spatial awareness. It also incorporates residual PoseKAN blocks for deeper feature refinement, and a global response normalization for improved feature selectivity and contrast. Extensive experiments on benchmark datasets demonstrate the competitive performance of our model against state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2511.08809",
    "authors": [
      "Abu Taib Mohammed Shahjahan",
      "A. Ben Hamza"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.08810",
    "title": "SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph",
    "abstract": "           Adversarial attacks expose a fundamental vulnerability in modern deep vision models by exploiting their dependence on dense, pixel-level representations that are highly sensitive to imperceptible perturbations. Traditional defense strategies typically operate within this fragile pixel domain, lacking mechanisms to incorporate inherently robust visual features. In this work, we introduce SIFT-Graph, a multimodal defense framework that enhances the robustness of traditional vision models by aggregating structurally meaningful features extracted from raw images using both handcrafted and learned modalities. Specifically, we integrate Scale-Invariant Feature Transform keypoints with a Graph Attention Network to capture scale and rotation invariant local structures that are resilient to perturbations. These robust feature embeddings are then fused with traditional vision model, such as Vision Transformer and Convolutional Neural Network, to form a unified, structure-aware and perturbation defensive model. Preliminary results demonstrate that our method effectively improves the visual model robustness against gradient-based white box adversarial attacks, while incurring only a marginal drop in clean accuracy.         ",
    "url": "https://arxiv.org/abs/2511.08810",
    "authors": [
      "Jingjie He",
      "Weijie Liang",
      "Zihan Shan",
      "Matthew Caesar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.08825",
    "title": "Neural Value Iteration",
    "abstract": "           The value function of a POMDP exhibits the piecewise-linear-convex (PWLC) property and can be represented as a finite set of hyperplanes, known as $\\alpha$-vectors. Most state-of-the-art POMDP solvers (offline planners) follow the point-based value iteration scheme, which performs Bellman backups on $\\alpha$-vectors at reachable belief points until convergence. However, since each $\\alpha$-vector is $|S|$-dimensional, these methods quickly become intractable for large-scale problems due to the prohibitive computational cost of Bellman backups. In this work, we demonstrate that the PWLC property allows a POMDP's value function to be alternatively represented as a finite set of neural networks. This insight enables a novel POMDP planning algorithm called \\emph{Neural Value Iteration}, which combines the generalization capability of neural networks with the classical value iteration framework. Our approach achieves near-optimal solutions even in extremely large POMDPs that are intractable for existing offline solvers.         ",
    "url": "https://arxiv.org/abs/2511.08825",
    "authors": [
      "Yang You",
      "Ufuk \u00c7ak\u0131r",
      "Alex Schutz",
      "Robert Skilton",
      "Nick Hawes"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.08846",
    "title": "On topological descriptors for graph products",
    "abstract": "           Topological descriptors have been increasingly utilized for capturing multiscale structural information in relational data. In this work, we consider various filtrations on the (box) product of graphs and the effect on their outputs on the topological descriptors - the Euler characteristic (EC) and persistent homology (PH). In particular, we establish a complete characterization of the expressive power of EC on general color-based filtrations. We also show that the PH descriptors of (virtual) graph products contain strictly more information than the computation on individual graphs, whereas EC does not. Additionally, we provide algorithms to compute the PH diagrams of the product of vertex- and edge-level filtrations on the graph product. We also substantiate our theoretical analysis with empirical investigations on runtime analysis, expressivity, and graph classification performance. Overall, this work paves way for powerful graph persistent descriptors via product filtrations. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.08846",
    "authors": [
      "Mattie Ji",
      "Amauri H. Souza",
      "Vikas Garg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Algebraic Topology (math.AT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2511.08851",
    "title": "Learning-based Radio Link Failure Prediction Based on Measurement Dataset in Railway Environments",
    "abstract": "           In this paper, a measurement-driven framework is proposed for early radio link failure (RLF) prediction in 5G non-standalone (NSA) railway environments. Using 10 Hz metro-train traces with serving and neighbor-cell indicators, we benchmark six models, namely CNN, LSTM, XGBoost, Anomaly Transformer, PatchTST, and TimesNet, under varied observation windows and prediction horizons. When the observation window is three seconds, TimesNet attains the highest F1 score with a three-second prediction horizon, while CNN provides a favorable accuracy-latency tradeoff with a two-second horizon, enabling proactive actions such as redundancy and adaptive handovers. The results indicate that deep temporal models can anticipate reliability degradations several seconds in advance using lightweight features available on commercial devices, offering a practical path to early-warning control in 5G-based railway systems.         ",
    "url": "https://arxiv.org/abs/2511.08851",
    "authors": [
      "Po-Heng Chou",
      "Da-Chih Lin",
      "Hung-Yu Wei",
      "Walid Saad",
      "Yu Tsao"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2511.08853",
    "title": "Rethinking Graph Super-resolution: Dual Frameworks for Topological Fidelity",
    "abstract": "           Graph super-resolution, the task of inferring high-resolution (HR) graphs from low-resolution (LR) counterparts, is an underexplored yet crucial research direction that circumvents the need for costly data acquisition. This makes it especially desirable for resource-constrained fields such as the medical domain. While recent GNN-based approaches show promise, they suffer from two key limitations: (1) matrix-based node super-resolution that disregards graph structure and lacks permutation invariance; and (2) reliance on node representations to infer edge weights, which limits scalability and expressivity. In this work, we propose two GNN-agnostic frameworks to address these issues. First, Bi-SR introduces a bipartite graph connecting LR and HR nodes to enable structure-aware node super-resolution that preserves topology and permutation invariance. Second, DEFEND learns edge representations by mapping HR edges to nodes of a dual graph, allowing edge inference via standard node-based GNNs. We evaluate both frameworks on a real-world brain connectome dataset, where they achieve state-of-the-art performance across seven topological measures. To support generalization, we introduce twelve new simulated datasets that capture diverse topologies and LR-HR relationships. These enable comprehensive benchmarking of graph super-resolution methods.         ",
    "url": "https://arxiv.org/abs/2511.08853",
    "authors": [
      "Pragya Singh",
      "Islem Rekik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2511.08867",
    "title": "Conformal Prediction for Multi-Source Detection on a Network",
    "abstract": "           Detecting the origin of information or infection spread in networks is a fundamental challenge with applications in misinformation tracking, epidemiology, and beyond. We study the multi-source detection problem: given snapshot observations of node infection status on a graph, estimate the set of source nodes that initiated the propagation. Existing methods either lack statistical guarantees or are limited to specific diffusion models and assumptions. We propose a novel conformal prediction framework that provides statistically valid recall guarantees for source set detection, independent of the underlying diffusion process or data distribution. Our approach introduces principled score functions to quantify the alignment between predicted probabilities and true sources, and leverages a calibration set to construct prediction sets with user-specified recall and coverage levels. The method is applicable to both single- and multi-source scenarios, supports general network diffusion dynamics, and is computationally efficient for large graphs. Empirical results demonstrate that our method achieves rigorous coverage with competitive accuracy, outperforming existing baselines in both reliability and this http URL code is available online.         ",
    "url": "https://arxiv.org/abs/2511.08867",
    "authors": [
      "Xingchao Jian",
      "Purui Zhang",
      "Lan Tian",
      "Feng Ji",
      "Wenfei Liang",
      "Wee Peng Tay",
      "Bihan Wen",
      "Felix Krahmer"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.08888",
    "title": "Weaver: Kronecker Product Approximations of Spatiotemporal Attention for Traffic Network Forecasting",
    "abstract": "           Spatiotemporal forecasting on transportation networks is a complex task that requires understanding how traffic nodes interact within a dynamic, evolving system dictated by traffic flow dynamics and social behavioral patterns. The importance of transportation networks and ITS for modern mobility and commerce necessitates forecasting models that are not only accurate but also interpretable, efficient, and robust under structural or temporal perturbations. Recent approaches, particularly Transformer-based architectures, have improved predictive performance but often at the cost of high computational overhead and diminished architectural interpretability. In this work, we introduce Weaver, a novel attention-based model that applies Kronecker product approximations (KPA) to decompose the PN X PN spatiotemporal attention of O(P^2N^2) complexity into local P X P temporal and N X N spatial attention maps. This Kronecker attention map enables our Parallel-Kronecker Matrix-Vector product (P2-KMV) for efficient spatiotemporal message passing with O(P^2N + N^2P) complexity. To capture real-world traffic dynamics, we address the importance of negative edges in modeling traffic behavior by introducing Valence Attention using the continuous Tanimoto coefficient (CTC), which provides properties conducive to precise latent graph generation and training stability. To fully utilize the model's learning capacity, we introduce the Traffic Phase Dictionary for self-conditioning. Evaluations on PEMS-BAY and METR-LA show that Weaver achieves competitive performance across model categories while training more efficiently.         ",
    "url": "https://arxiv.org/abs/2511.08888",
    "authors": [
      "Christopher Cheong",
      "Gary Davis",
      "Seongjin Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.08900",
    "title": "An Improved Dual-Attention Transformer-LSTM for Small-Sample Prediction of Modal Frequency and Actual Anchor Radius in Micro Hemispherical Resonator Design",
    "abstract": "           The high-temperature glassblowing-fabricated micro hemispherical resonator (MHR) exhibits high symmetry and high Q-value for precision inertial navigation. However, MHR design entails a comprehensive evaluation of multiple possible configurations and demands extremely time-consuming simulation of key parameters combination. To address this problem, this paper proposed a rapid prediction method of modal frequency and actual anchor radius of designed MHR using an improved Transformer-LSTM (Long Short-Term Memory) model for rapid design sizing. High-temperature-induced softening deformation at the anchor point reduces the actual anchor radius below the designed value. By varying key parameters such as resonator height, anchor radius and edge thickness, finite element glassblowing simulation and modal analyse were conducted to obtain the first six modal frequencies and actual anchor radius. To address regression prediction challenges with limited data, dual multi-head self-attention (MHSA) mechanisms replaced the transformer's standard Feed Forward Network, to improve hidden information capture for high-accuracy predictions of modal frequencies and anchor radius. By checking fabricating feasibility of anchor radius and allowing rapid modal characteristics evaluation without interference, ablation and comparative experiments validated the method's superiority, as an effective support of MHR design. Design optimization experiments demonstrate a prediction accuracy of 96.35%, with computational time reduced to 1/48,000 of traditional finite element methods, significantly improving design efficiency. This study offers a new paradigm for intelligent Micro-Electro-Mechanical System (MEMS) device design under complex process conditions.         ",
    "url": "https://arxiv.org/abs/2511.08900",
    "authors": [
      "Yuyi Yao",
      "Gongliu Yang",
      "Runzhuo Xu",
      "Yongqiang Tu",
      "Haozhou Mo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2511.08904",
    "title": "Consistency Change Detection Framework for Unsupervised Remote Sensing Change Detection",
    "abstract": "           Unsupervised remote sensing change detection aims to monitor and analyze changes from multi-temporal remote sensing images in the same geometric region at different times, without the need for labeled training data. Previous unsupervised methods attempt to achieve style transfer across multi-temporal remote sensing images through reconstruction by a generator network, and then capture the unreconstructable areas as the changed regions. However, it often leads to poor performance due to generator overfitting. In this paper, we propose a novel Consistency Change Detection Framework (CCDF) to address this challenge. Specifically, we introduce a Cycle Consistency (CC) module to reduce the overfitting issues in the generator-based reconstruction. Additionally, we propose a Semantic Consistency (SC) module to enable detail reconstruction. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches.         ",
    "url": "https://arxiv.org/abs/2511.08904",
    "authors": [
      "Yating Liu",
      "Yan Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.08908",
    "title": "HitoMi-Cam: A Shape-Agnostic Person Detection Method Using the Spectral Characteristics of Clothing",
    "abstract": "           While convolutional neural network (CNN)-based object detection is widely used, it exhibits a shape dependency that degrades performance for postures not included in the training data. Building upon our previous simulation study published in this journal, this study implements and evaluates the spectral-based approach on physical hardware to address this limitation. Specifically, this paper introduces HitoMi-Cam, a lightweight and shape-agnostic person detection method that uses the spectral reflectance properties of clothing. The author implemented the system on a resource-constrained edge device without a GPU to assess its practical viability. The results indicate that a processing speed of 23.2 frames per second (fps) (253x190 pixels) is achievable, suggesting that the method can be used for real-time applications. In a simulated search and rescue scenario where the performance of CNNs declines, HitoMi-Cam achieved an average precision (AP) of 93.5%, surpassing that of the compared CNN models (best AP of 53.8%). Throughout all evaluation scenarios, the occurrence of false positives remained minimal. This study positions the HitoMi-Cam method not as a replacement for CNN-based detectors but as a complementary tool under specific conditions. The results indicate that spectral-based person detection can be a viable option for real-time operation on edge devices in real-world environments where shapes are unpredictable, such as disaster rescue.         ",
    "url": "https://arxiv.org/abs/2511.08908",
    "authors": [
      "Shuji Ono"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.08912",
    "title": "A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction",
    "abstract": "           In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.         ",
    "url": "https://arxiv.org/abs/2511.08912",
    "authors": [
      "Jinyu Zhang",
      "Lijun Han",
      "Feng Jian",
      "Lingxi Zhang",
      "Hesheng Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2511.08919",
    "title": "Iterative Ricci-Foster Curvature Flow with GMM-Based Edge Pruning: A Novel Approach to Community Detection",
    "abstract": "           Community detection in complex networks is a fundamental problem, open to new approaches in various scientific settings. We introduce a novel community detection method, based on Ricci flow on graphs. Our technique iteratively updates edge weights (their metric lengths) according to their (combinatorial) Foster version of Ricci curvature computed from effective resistance distance between the nodes. The latter computation is known to be done by pseudo-inverting the graph Laplacian matrix. At that, our approach is alternative to one based on Ollivier-Ricci geometric flow for community detection on graphs, significantly outperforming it in terms of computation time. In our proposed method, iterations of Foster-Ricci flow that highlight network regions of different curvature -- are followed by a Gaussian Mixture Model (GMM) separation heuristic. That allows to classify edges into ''strong'' (intra-community) and ''weak'' (inter-community) groups, followed by a systematic pruning of the former to isolate communities. We benchmark our algorithm on synthetic networks generated from the Stochastic Block Model (SBM), evaluating performance with the Adjusted Rand Index (ARI). Our results demonstrate that proposed framework robustly recovers the planted community structure of SBM-s, establishing Ricci-Foster Flow with GMM-clustering as a principled and computationally effective new tool for network analysis, tested against alternative Ricci-Ollivier flow coupled with spectral clustering.         ",
    "url": "https://arxiv.org/abs/2511.08919",
    "authors": [
      "Arsenii Onuchin",
      "Konstantin Sorokin",
      "Maxim Beketov",
      "Liubov Tupikina"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2511.08937",
    "title": "Boosting Adversarial Transferability via Ensemble Non-Attention",
    "abstract": "           Ensemble attacks integrate the outputs of surrogate models with diverse architectures, which can be combined with various gradient-based attacks to improve adversarial transferability. However, previous work shows unsatisfactory attack performance when transferring across heterogeneous model architectures. The main reason is that the gradient update directions of heterogeneous surrogate models differ widely, making it hard to reduce the gradient variance of ensemble models while making the best of individual model. To tackle this challenge, we design a novel ensemble attack, NAMEA, which for the first time integrates the gradients from the non-attention areas of ensemble models into the iterative gradient optimization process. Our design is inspired by the observation that the attention areas of heterogeneous models vary sharply, thus the non-attention areas of ViTs are likely to be the focus of CNNs and vice versa. Therefore, we merge the gradients respectively from the attention and non-attention areas of ensemble models so as to fuse the transfer information of CNNs and ViTs. Specifically, we pioneer a new way of decoupling the gradients of non-attention areas from those of attention areas, while merging gradients by meta-learning. Empirical evaluations on ImageNet dataset indicate that NAMEA outperforms AdaEA and SMER, the state-of-the-art ensemble attacks by an average of 15.0% and 9.6%, respectively. This work is the first attempt to explore the power of ensemble non-attention in boosting cross-architecture transferability, providing new insights into launching ensemble attacks.         ",
    "url": "https://arxiv.org/abs/2511.08937",
    "authors": [
      "Yipeng Zou",
      "Qin Liu",
      "Jie Wu",
      "Yu Peng",
      "Guo Chen",
      "Hui Zhou",
      "Guanghui Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.08938",
    "title": "Neural B-frame Video Compression with Bi-directional Reference Harmonization",
    "abstract": "           Neural video compression (NVC) has made significant progress in recent years, while neural B-frame video compression (NBVC) remains underexplored compared to P-frame compression. NBVC can adopt bi-directional reference frames for better compression performance. However, NBVC's hierarchical coding may complicate continuous temporal prediction, especially at some hierarchical levels with a large frame span, which could cause the contribution of the two reference frames to be unbalanced. To optimize reference information utilization, we propose a novel NBVC method, termed Bi-directional Reference Harmonization Video Compression (BRHVC), with the proposed Bi-directional Motion Converge (BMC) and Bi-directional Contextual Fusion (BCF). BMC converges multiple optical flows in motion compression, leading to more accurate motion compensation on a larger scale. Then BCF explicitly models the weights of reference contexts under the guidance of motion compensation accuracy. With more efficient motions and contexts, BRHVC can effectively harmonize bi-directional references. Experimental results indicate that our BRHVC outperforms previous state-of-the-art NVC methods, even surpassing the traditional coding, VTM-RA (under random access configuration), on the HEVC datasets. The source code is released at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.08938",
    "authors": [
      "Yuxi Liu",
      "Dengchao Jin",
      "Shuai Huo",
      "Jiawen Gu",
      "Chao Zhou",
      "Huihui Bai",
      "Ming Lu",
      "Zhan Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.08940",
    "title": "QIBONN: A Quantum-Inspired Bilevel Optimizer for Neural Networks on Tabular Classification",
    "abstract": "           Hyperparameter optimization (HPO) for neural networks on tabular data is critical to a wide range of applications, yet it remains challenging due to large, non-convex search spaces and the cost of exhaustive tuning. We introduce the Quantum-Inspired Bilevel Optimizer for Neural Networks (QIBONN), a bilevel framework that encodes feature selection, architectural hyperparameters, and regularization in a unified qubit-based representation. By combining deterministic quantum-inspired rotations with stochastic qubit mutations guided by a global attractor, QIBONN balances exploration and exploitation under a fixed evaluation budget. We conduct systematic experiments under single-qubit bit-flip noise (0.1\\%--1\\%) emulated by an IBM-Q backend. Results on 13 real-world datasets indicate that QIBONN is competitive with established methods, including classical tree-based methods and both classical/quantum-inspired HPO algorithms under the same tuning budget.         ",
    "url": "https://arxiv.org/abs/2511.08940",
    "authors": [
      "Pedro Chumpitaz-Flores",
      "My Duong",
      "Ying Mao",
      "Kaixun Hua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2511.08944",
    "title": "Robust Backdoor Removal by Reconstructing Trigger-Activated Changes in Latent Representation",
    "abstract": "           Backdoor attacks pose a critical threat to machine learning models, causing them to behave normally on clean data but misclassify poisoned data into a poisoned class. Existing defenses often attempt to identify and remove backdoor neurons based on Trigger-Activated Changes (TAC) which is the activation differences between clean and poisoned data. These methods suffer from low precision in identifying true backdoor neurons due to inaccurate estimation of TAC values. In this work, we propose a novel backdoor removal method by accurately reconstructing TAC values in the latent representation. Specifically, we formulate the minimal perturbation that forces clean data to be classified into a specific class as a convex quadratic optimization problem, whose optimal solution serves as a surrogate for TAC. We then identify the poisoned class by detecting statistically small $L^2$ norms of perturbations and leverage the perturbation of the poisoned class in fine-tuning to remove backdoors. Experiments on CIFAR-10, GTSRB, and TinyImageNet demonstrated that our approach consistently achieves superior backdoor suppression with high clean accuracy across different attack types, datasets, and architectures, outperforming existing defense methods.         ",
    "url": "https://arxiv.org/abs/2511.08944",
    "authors": [
      "Kazuki Iwahana",
      "Yusuke Yamasaki",
      "Akira Ito",
      "Takayuki Miura",
      "Toshiki Shibahara"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2511.08949",
    "title": "EVADE: LLM-Based Explanation Generation and Validation for Error Detection in NLI",
    "abstract": "           High-quality datasets are critical for training and evaluating reliable NLP models. In tasks like natural language inference (NLI), human label variation (HLV) arises when multiple labels are valid for the same instance, making it difficult to separate annotation errors from plausible variation. An earlier framework VARIERR (Weber-Genzel et al., 2024) asks multiple annotators to explain their label decisions in the first round and flag errors via validity judgments in the second round. However, conducting two rounds of manual annotation is costly and may limit the coverage of plausible labels or explanations. Our study proposes a new framework, EVADE, for generating and validating explanations to detect errors using large language models (LLMs). We perform a comprehensive analysis comparing human- and LLM-detected errors for NLI across distribution comparison, validation overlap, and impact on model fine-tuning. Our experiments demonstrate that LLM validation refines generated explanation distributions to more closely align with human annotations, and that removing LLM-detected errors from training data yields improvements in fine-tuning performance than removing errors identified by human annotators. This highlights the potential to scale error detection, reducing human effort while improving dataset quality under label variation.         ",
    "url": "https://arxiv.org/abs/2511.08949",
    "authors": [
      "Longfei Zuo",
      "Barbara Plank",
      "Siyao Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2511.08982",
    "title": "Heterogeneous Graph Neural Networks for Assumption-Based Argumentation",
    "abstract": "           Assumption-Based Argumentation (ABA) is a powerful structured argumentation formalism, but exact computation of extensions under stable semantics is intractable for large frameworks. We present the first Graph Neural Network (GNN) approach to approximate credulous acceptance in ABA. To leverage GNNs, we model ABA frameworks via a dependency graph representation encoding assumptions, claims and rules as nodes, with heterogeneous edge labels distinguishing support, derive and attack relations. We propose two GNN architectures - ABAGCN and ABAGAT - that stack residual heterogeneous convolution or attention layers, respectively, to learn node embeddings. Our models are trained on the ICCMA 2023 benchmark, augmented with synthetic ABAFs, with hyperparameters optimised via Bayesian search. Empirically, both ABAGCN and ABAGAT outperform a state-of-the-art GNN baseline that we adapt from the abstract argumentation literature, achieving a node-level F1 score of up to 0.71 on the ICCMA instances. Finally, we develop a sound polynomial time extension-reconstruction algorithm driven by our predictor: it reconstructs stable extensions with F1 above 0.85 on small ABAFs and maintains an F1 of about 0.58 on large frameworks. Our work opens new avenues for scalable approximate reasoning in structured argumentation.         ",
    "url": "https://arxiv.org/abs/2511.08982",
    "authors": [
      "Preesha Gehlot",
      "Anna Rapberger",
      "Fabrizio Russo",
      "Francesca Toni"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.08987",
    "title": "WDT-MD: Wavelet Diffusion Transformers for Microaneurysm Detection in Fundus Images",
    "abstract": "           Microaneurysms (MAs), the earliest pathognomonic signs of Diabetic Retinopathy (DR), present as sub-60 $\\mu m$ lesions in fundus images with highly variable photometric and morphological characteristics, rendering manual screening not only labor-intensive but inherently error-prone. While diffusion-based anomaly detection has emerged as a promising approach for automated MA screening, its clinical application is hindered by three fundamental limitations. First, these models often fall prey to \"identity mapping\", where they inadvertently replicate the input image. Second, they struggle to distinguish MAs from other anomalies, leading to high false positives. Third, their suboptimal reconstruction of normal features hampers overall performance. To address these challenges, we propose a Wavelet Diffusion Transformer framework for MA Detection (WDT-MD), which features three key innovations: a noise-encoded image conditioning mechanism to avoid \"identity mapping\" by perturbing image conditions during training; pseudo-normal pattern synthesis via inpainting to introduce pixel-level supervision, enabling discrimination between MAs and other anomalies; and a wavelet diffusion Transformer architecture that combines the global modeling capability of diffusion Transformers with multi-scale wavelet analysis to enhance reconstruction of normal retinal features. Comprehensive experiments on the IDRiD and e-ophtha MA datasets demonstrate that WDT-MD outperforms state-of-the-art methods in both pixel-level and image-level MA detection. This advancement holds significant promise for improving early DR screening.         ",
    "url": "https://arxiv.org/abs/2511.08987",
    "authors": [
      "Yifei Sun",
      "Yuzhi He",
      "Junhao Jia",
      "Jinhong Wang",
      "Ruiquan Ge",
      "Changmiao Wang",
      "Hongxia Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.08992",
    "title": "Learning to Control PDEs with Differentiable Predictive Control and Time-Integrated Neural Operators",
    "abstract": "           We present an end-to-end learning to control framework for partial differential equations (PDEs). Our approach integrates Time-Integrated Deep Operator Networks (TI-DeepONets) as differentiable PDE surrogate models within the Differentiable Predictive Control (DPC)-a self-supervised learning framework for constrained neural control policies. The TI-DeepONet architecture learns temporal derivatives and couples them with numerical integrators, thus preserving the temporal causality of infinite-dimensional PDEs while reducing error accumulation in long-horizon predictions. Within DPC, we leverage automatic differentiation to compute policy gradients by backpropagating the expectations of optimal control loss through the learned TI-DeepONet, enabling efficient offline optimization of neural policies without the need for online optimization or supervisory controllers. We empirically demonstrate that the proposed method learns feasible parametric policies across diverse PDE systems, including the heat, the nonlinear Burgers', and the reaction-diffusion equations. The learned policies achieve target tracking, constraint satisfaction, and curvature minimization objectives, while generalizing across distributions of initial conditions and problem parameters. These results highlight the promise of combining operator learning with DPC for scalable, model-based self-supervised learning in PDE-constrained optimal control.         ",
    "url": "https://arxiv.org/abs/2511.08992",
    "authors": [
      "Dibakar Roy Sarkar",
      "J\u00e1n Drgo\u0148a",
      "Somdatta Goswami"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2511.08997",
    "title": "T-Rex-Omni: Integrating Negative Visual Prompt in Generic Object Detection",
    "abstract": "           Object detection methods have evolved from closed-set to open-set paradigms over the years. Current open-set object detectors, however, remain constrained by their exclusive reliance on positive indicators based on given prompts like text descriptions or visual exemplars. This positive-only paradigm experiences consistent vulnerability to visually similar but semantically different distractors. We propose T-Rex-Omni, a novel framework that addresses this limitation by incorporating negative visual prompts to negate hard negative distractors. Specifically, we first introduce a unified visual prompt encoder that jointly processes positive and negative visual prompts. Next, a training-free Negating Negative Computing (NNC) module is proposed to dynamically suppress negative responses during the probability computing stage. To further boost performance through fine-tuning, our Negating Negative Hinge (NNH) loss enforces discriminative margins between positive and negative embeddings. T-Rex-Omni supports flexible deployment in both positive-only and joint positive-negative inference modes, accommodating either user-specified or automatically generated negative examples. Extensive experiments demonstrate remarkable zero-shot detection performance, significantly narrowing the performance gap between visual-prompted and text-prompted methods while showing particular strength in long-tailed scenarios (51.2 AP_r on LVIS-minival). This work establishes negative prompts as a crucial new dimension for advancing open-set visual recognition systems.         ",
    "url": "https://arxiv.org/abs/2511.08997",
    "authors": [
      "Jiazhou Zhou",
      "Qing Jiang",
      "Kanghao Chen",
      "Lutao Jiang",
      "Yuanhuiyi Lyu",
      "Ying-Cong Chen",
      "Lei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.09001",
    "title": "Contextual Graph Embeddings: Accounting for Data Characteristics in Heterogeneous Data Integration",
    "abstract": "           As organizations continue to access diverse datasets, the demand for effective data integration has increased. Key tasks in this process, such as schema matching and entity resolution, are essential but often require significant effort. Although previous studies have aimed to automate these tasks, the influence of dataset characteristics on the matching effectiveness has not been thoroughly examined, and combinations of different methods remain limited. This study introduces a contextual graph embedding technique that integrates structural details from tabular data and contextual elements such as column descriptions and external knowledge. Tests conducted on datasets with varying properties such as domain specificity, data size, missing rate, and overlap rate showed that our approach consistently surpassed existing graph-based methods, especially in difficult scenarios such those with a high proportion of numerical values or significant missing data. However, we identified specific failure cases, such as columns that were semantically similar but distinct, which remains a challenge for our method. The study highlights two main insights: (i) contextual embeddings enhance the matching reliability, and (ii) dataset characteristics significantly affect the integration outcomes. These contributions can advance the development of practical data integration systems that can support real-world enterprise applications.         ",
    "url": "https://arxiv.org/abs/2511.09001",
    "authors": [
      "Yuka Haruki",
      "Shigeru Ishikura",
      "Kazuya Demachi",
      "Teruaki Hayashi"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2511.09016",
    "title": "Assumed Density Filtering and Smoothing with Neural Network Surrogate Models",
    "abstract": "           The Kalman filter and Rauch-Tung-Striebel (RTS) smoother are optimal for state estimation in linear dynamic systems. With nonlinear systems, the challenge consists in how to propagate uncertainty through the state transitions and output function. For the case of a neural network model, we enable accurate uncertainty propagation using a recent state-of-the-art analytic formula for computing the mean and covariance of a deep neural network with Gaussian input. We argue that cross entropy is a more appropriate performance metric than RMSE for evaluating the accuracy of filters and smoothers. We demonstrate the superiority of our method for state estimation on a stochastic Lorenz system and a Wiener system, and find that our method enables more optimal linear quadratic regulation when the state estimate is used for feedback.         ",
    "url": "https://arxiv.org/abs/2511.09016",
    "authors": [
      "Simon Kuang",
      "Xinfan Lin"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09025",
    "title": "FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks",
    "abstract": "           Large Language Models (LLMs) have impressive data fusion and reasoning capabilities for autonomous driving (AD). However, training LLMs for AD faces significant challenges including high computation transmission costs, and privacy concerns associated with sensitive driving data. Federated Learning (FL) is promising for enabling autonomous vehicles (AVs) to collaboratively train models without sharing raw data. We present Federated LLM-based Autonomous Driving (FLAD), an FL framework that leverages distributed multimodal sensory data across AVs in heterogeneous environment. FLAD has three key innovations: (1) a cloud-edge-vehicle collaborative architecture that reduces communication delay and preserving data privacy; (2) an intelligent parallelized collaborative training with a communication scheduling mechanism that optimizes training efficiency, leveraging end-devices otherwise having insufficient resources for model training; and (3) a knowledge distillation method that personalizes LLM according to heterogeneous edge data. In addition, we prototype FLAD in a testbed with NVIDIA Jetsons, overcoming practical implementation challenges including CPU/GPU memory sharing in resource-constrained devices, dynamic model partitions, and fault-tolerant this http URL experimental evaluation demonstrates that FLAD achieves superior end-to-end AD performance while efficiently utilizing distributed vehicular resources, opening up new possibilities for future collaborative AD model training and knowledge sharing.         ",
    "url": "https://arxiv.org/abs/2511.09025",
    "authors": [
      "Tianao Xiang",
      "Mingjian Zhi",
      "Yuanguo Bi",
      "Lin Cai",
      "Yuhao Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09036",
    "title": "FedSDWC: Federated Synergistic Dual-Representation Weak Causal Learning for OOD",
    "abstract": "           Amid growing demands for data privacy and advances in computational infrastructure, federated learning (FL) has emerged as a prominent distributed learning paradigm. Nevertheless, differences in data distribution (such as covariate and semantic shifts) severely affect its reliability in real-world deployments. To address this issue, we propose FedSDWC, a causal inference method that integrates both invariant and variant features. FedSDWC infers causal semantic representations by modeling the weak causal influence between invariant and variant features, effectively overcoming the limitations of existing invariant learning methods in accurately capturing invariant features and directly constructing causal representations. This approach significantly enhances FL's ability to generalize and detect OOD data. Theoretically, we derive FedSDWC's generalization error bound under specific conditions and, for the first time, establish its relationship with client prior distributions. Moreover, extensive experiments conducted on multiple benchmark datasets validate the superior performance of FedSDWC in handling covariate and semantic shifts. For example, FedSDWC outperforms FedICON, the next best baseline, by an average of 3.04% on CIFAR-10 and 8.11% on CIFAR-100.         ",
    "url": "https://arxiv.org/abs/2511.09036",
    "authors": [
      "Zhenyuan Huang",
      "Hui Zhang",
      "Wenzhong Tang",
      "Haijun Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09039",
    "title": "Fairness-Aware Few-Shot Learning for Audio-Visual Stress Detection",
    "abstract": "           Fairness in AI-driven stress detection is critical for equitable mental healthcare, yet existing models frequently exhibit gender bias, particularly in data-scarce scenarios. To address this, we propose FairM2S, a fairness-aware meta-learning framework for stress detection leveraging audio-visual data. FairM2S integrates Equalized Odds constraints during both meta-training and adaptation phases, employing adversarial gradient masking and fairness-constrained meta-updates to effectively mitigate bias. Evaluated against five state-of-the-art baselines, FairM2S achieves 78.1% accuracy while reducing the Equal Opportunity to 0.06, demonstrating substantial fairness gains. We also release SAVSD, a smartphone-captured dataset with gender annotations, designed to support fairness research in low-resource, real-world contexts. Together, these contributions position FairM2S as a state-of-the-art approach for equitable and scalable few-shot stress detection in mental health AI. We release our dataset and FairM2S publicly with this paper.         ",
    "url": "https://arxiv.org/abs/2511.09039",
    "authors": [
      "Anushka Sanjay Shelke",
      "Aditya Sneh",
      "Arya Adyasha",
      "Haroon R. Lone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2511.09042",
    "title": "GeoGNN: Quantifying and Mitigating Semantic Drift in Text-Attributed Graphs",
    "abstract": "           Graph neural networks (GNNs) on text--attributed graphs (TAGs) typically encode node texts using pretrained language models (PLMs) and propagate these embeddings through linear neighborhood aggregation. However, the representation spaces of modern PLMs are highly non--linear and geometrically structured, where textual embeddings reside on curved semantic manifolds rather than flat Euclidean spaces. Linear aggregation on such manifolds inevitably distorts geometry and causes semantic drift--a phenomenon where aggregated representations deviate from the intrinsic manifold, losing semantic fidelity and expressive power. To quantitatively investigate this problem, this work introduces a local PCA--based metric that measures the degree of semantic drift and provides the first quantitative framework to analyze how different aggregation mechanisms affect manifold structure. Building upon these insights, we propose Geodesic Aggregation, a manifold--aware mechanism that aggregates neighbor information along geodesics via log--exp mappings on the unit sphere, ensuring that representations remain faithful to the semantic manifold during message passing. We further develop GeoGNN, a practical instantiation that integrates spherical attention with manifold interpolation. Extensive experiments across four benchmark datasets and multiple text encoders show that GeoGNN substantially mitigates semantic drift and consistently outperforms strong baselines, establishing the importance of manifold--aware aggregation in text--attributed graph learning.         ",
    "url": "https://arxiv.org/abs/2511.09042",
    "authors": [
      "Liangwei Yang",
      "Jing Ma",
      "Jianguo Zhang",
      "Zhiwei Liu",
      "Jielin Qiu",
      "Shirley Kokane",
      "Shiyu Wang",
      "Haolin Chen",
      "Rithesh Murthy",
      "Ming Zhu",
      "Huan Wang",
      "Weiran Yao",
      "Caiming Xiong",
      "Shelby Heinecke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09045",
    "title": "USF-Net: A Unified Spatiotemporal Fusion Network for Ground-Based Remote Sensing Cloud Image Sequence Extrapolation",
    "abstract": "           Ground-based remote sensing cloud image sequence extrapolation is a key research area in the development of photovoltaic power systems. However, existing approaches exhibit several limitations:(1)they primarily rely on static kernels to augment feature information, lacking adaptive mechanisms to extract features at varying resolutions dynamically;(2)temporal guidance is insufficient, leading to suboptimal modeling of long-range spatiotemporal dependencies; and(3)the quadratic computational cost of attention mechanisms is often overlooked, limiting efficiency in practical deployment. To address these challenges, we propose USF-Net, a Unified Spatiotemporal Fusion Network that integrates adaptive large-kernel convolutions and a low-complexity attention mechanism, combining temporal flow information within an encoder-decoder framework. Specifically, the encoder employs three basic layers to extract features. Followed by the USTM, which comprises:(1)a SiB equipped with a SSM that dynamically captures multi-scale contextual information, and(2)a TiB featuring a TAM that effectively models long-range temporal dependencies while maintaining computational efficiency. In addition, a DSM with a TGM is introduced to enable unified modeling of temporally guided spatiotemporal dependencies. On the decoder side, a DUM is employed to address the common \"ghosting effect.\" It utilizes the initial temporal state as an attention operator to preserve critical motion signatures. As a key contribution, we also introduce and release the ASI-CIS dataset. Extensive experiments on ASI-CIS demonstrate that USF-Net significantly outperforms state-of-the-art methods, establishing a superior balance between prediction accuracy and computational efficiency for ground-based cloud extrapolation. The dataset and source code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.09045",
    "authors": [
      "Penghui Niu",
      "Taotao Cai",
      "Jiashuai She",
      "Yajuan Zhang",
      "Junhua Gua",
      "Ping Zhanga",
      "Jungong Hane",
      "Jianxin Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.09048",
    "title": "Guaranteeing Conservation of Integrals with Projection in Physics-Informed Neural Networks",
    "abstract": "           We propose a novel projection method that guarantees the conservation of integral quantities in Physics-Informed Neural Networks (PINNs). While the soft constraint that PINNs use to enforce the structure of partial differential equations (PDEs) enables necessary flexibility during training, it also permits the discovered solution to violate physical laws. To address this, we introduce a projection method that guarantees the conservation of the linear and quadratic integrals, both separately and jointly. We derived the projection formulae by solving constrained non-linear optimization problems and found that our PINN modified with the projection, which we call PINN-Proj, reduced the error in the conservation of these quantities by three to four orders of magnitude compared to the soft constraint and marginally reduced the PDE solution error. We also found evidence that the projection improved convergence through improving the conditioning of the loss landscape. Our method holds promise as a general framework to guarantee the conservation of any integral quantity in a PINN if a tractable solution exists.         ",
    "url": "https://arxiv.org/abs/2511.09048",
    "authors": [
      "Anthony Baez",
      "Wang Zhang",
      "Ziwen Ma",
      "Lam Nguyen",
      "Subhro Das",
      "Luca Daniel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09064",
    "title": "Diversifying Counterattacks: Orthogonal Exploration for Robust CLIP Inference",
    "abstract": "           Vision-language pre-training models (VLPs) demonstrate strong multimodal understanding and zero-shot generalization, yet remain vulnerable to adversarial examples, raising concerns about their reliability. Recent work, Test-Time Counterattack (TTC), improves robustness by generating perturbations that maximize the embedding deviation of adversarial inputs using PGD, pushing them away from their adversarial representations. However, due to the fundamental difference in optimization objectives between adversarial attacks and counterattacks, generating counterattacks solely based on gradients with respect to the adversarial input confines the search to a narrow space. As a result, the counterattacks could overfit limited adversarial patterns and lack the diversity to fully neutralize a broad range of perturbations. In this work, we argue that enhancing the diversity and coverage of counterattacks is crucial to improving adversarial robustness in test-time defense. Accordingly, we propose Directional Orthogonal Counterattack (DOC), which augments counterattack optimization by incorporating orthogonal gradient directions and momentum-based updates. This design expands the exploration of the counterattack space and increases the diversity of perturbations, which facilitates the discovery of more generalizable counterattacks and ultimately improves the ability to neutralize adversarial perturbations. Meanwhile, we present a directional sensitivity score based on averaged cosine similarity to boost DOC by improving example discrimination and adaptively modulating the counterattack strength. Extensive experiments on 16 datasets demonstrate that DOC improves adversarial robustness under various attacks while maintaining competitive clean accuracy. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.09064",
    "authors": [
      "Chengze Jiang",
      "Minjing Dong",
      "Xinli Shi",
      "Jie Gui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.09068",
    "title": "Toward an Intrusion Detection System for a Virtualization Framework in Edge Computing",
    "abstract": "           Edge computing pushes computation closer to data sources, but it also expands the attack surface on resource-constrained devices. This work explores the deployment of the Lightweight Deep Anomaly Detection for Network Traffic (LDPI) integrated as an isolated service within a virtualization framework that provides security by separation. LDPI, adopting a Deep Learning approach, achieved strong training performance, reaching AUC 0.999 (5-fold mean) across the evaluated packet-window settings (n, l), with high F1 at conservative operating points. We deploy LDPI on a laptop-class edge node and evaluate its overhead and performance in two scenarios: (i) comparing it with representative signature-based IDSes (Suricata and Snort) deployed on the same framework under identical workloads, and (ii) while detecting network flooding attacks.         ",
    "url": "https://arxiv.org/abs/2511.09068",
    "authors": [
      "Everton de Matos",
      "Hazaa Alameri",
      "Willian Tessaro Lunardi",
      "Martin Andreoni",
      "Eduardo Viegas"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2511.09087",
    "title": "Tele-LLM-Hub: Building Context-Aware Multi-Agent LLM Systems for Telecom Networks",
    "abstract": "           This paper introduces Tele-LLM-Hub, a user friendly low-code solution for rapid prototyping and deployment of context aware multi-agent (MA) Large Language Model (LLM) systems tailored for 5G and beyond. As telecom wireless networks become increasingly complex, intelligent LLM applications must share a domainspecific understanding of network state. We propose TeleMCP, the Telecom Model Context Protocol, to enable structured and context-rich communication between agents in telecom environments. Tele-LLM-Hub actualizes TeleMCP through a low-code interface that supports agent creation, workflow composition, and interaction with software stacks such as srsRAN. Key components include a direct chat interface, a repository of pre-built systems, an Agent Maker leveraging finetuning with our RANSTRUCT framework, and an MA-Maker for composing MA workflows. The goal of Tele-LLM-Hub is to democratize the design of contextaware MA systems and accelerate innovation in next-generation wireless networks.         ",
    "url": "https://arxiv.org/abs/2511.09087",
    "authors": [
      "Vijay K Shah",
      "Cong Shen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09088",
    "title": "Improving Sustainability of Adversarial Examples in Class-Incremental Learning",
    "abstract": "           Current adversarial examples (AEs) are typically designed for static models. However, with the wide application of Class-Incremental Learning (CIL), models are no longer static and need to be updated with new data distributed and labeled differently from the old ones. As a result, existing AEs often fail after CIL updates due to significant domain drift. In this paper, we propose SAE to enhance the sustainability of AEs against CIL. The core idea of SAE is to enhance the robustness of AE semantics against domain drift by making them more similar to the target class while distinguishing them from all other classes. Achieving this is challenging, as relying solely on the initial CIL model to optimize AE semantics often leads to overfitting. To resolve the problem, we propose a Semantic Correction Module. This module encourages the AE semantics to be generalized, based on a visual-language model capable of producing universal semantics. Additionally, it incorporates the CIL model to correct the optimization direction of the AE semantics, guiding them closer to the target class. To further reduce fluctuations in AE semantics, we propose a Filtering-and-Augmentation Module, which first identifies non-target examples with target-class semantics in the latent space and then augments them to foster more stable semantics. Comprehensive experiments demonstrate that SAE outperforms baselines by an average of 31.28% when updated with a 9-fold increase in the number of classes.         ",
    "url": "https://arxiv.org/abs/2511.09088",
    "authors": [
      "Taifeng Liu",
      "Xinjing Liu",
      "Liangqiu Dong",
      "Yang Liu",
      "Yilong Yang",
      "Zhuo Ma"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09091",
    "title": "APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots",
    "abstract": "           Learning natural, animal-like locomotion from demonstrations has become a core paradigm in legged robotics. Despite the recent advancements in motion tracking, most existing methods demand extensive tuning and rely on reference data during deployment, limiting adaptability. We present APEX (Action Priors enable Efficient Exploration), a plug-and-play extension to state-of-the-art motion tracking algorithms that eliminates any dependence on reference data during deployment, improves sample efficiency, and reduces parameter tuning effort. APEX integrates expert demonstrations directly into reinforcement learning (RL) by incorporating decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is combined with a multi-critic framework that balances task performance with motion style. Moreover, APEX enables a single policy to learn diverse motions and transfer reference-like styles across different terrains and velocities, while remaining robust to variations in reward design. We validate the effectiveness of our method through extensive experiments in both simulation and on a Unitree Go2 robot. By leveraging demonstrations to guide exploration during RL training, without imposing explicit bias toward them, APEX enables legged robots to learn with greater stability, efficiency, and generalization. We believe this approach paves the way for guidance-driven RL to boost natural skill acquisition in a wide array of robotic tasks, from locomotion to manipulation. Website and code: this https URL.         ",
    "url": "https://arxiv.org/abs/2511.09091",
    "authors": [
      "Shivam Sood",
      "Laukik Nakhwa",
      "Sun Ge",
      "Yuhong Cao",
      "Jin Cheng",
      "Fatemah Zargarbashi",
      "Taerim Yoon",
      "Sungjoon Choi",
      "Stelian Coros",
      "Guillaume Sartoretti"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2511.09094",
    "title": "RIS-based Communication Enhancement and Location Privacy Protection in UAV Networks",
    "abstract": "           With the explosive advancement of unmanned aerial vehicles (UAVs), the security of efficient UAV networks has become increasingly critical. Owing to the open nature of its communication environment, illegitimate malicious UAVs (MUs) can infer the position of the source UAV (SU) by analyzing received signals, thus compromising the SU location privacy. To protect the SU location privacy while ensuring efficient communication with legitimate receiving UAVs (RUs), we propose an Active Reconfigurable Intelligent Surface (ARIS)-assisted covert communication scheme based on virtual partitioning and artificial noise (AN). Specifically, we design a novel ARIS architecture integrated with an AN module. This architecture dynamically partitions its reflecting elements into multiple sub-regions: one subset is optimized to enhance the communication rate between the SU and RUs, while the other subset generates AN to interfere with the localization of the SU by MUs. We first derive the Cram\u00e9r-Rao Lower Bound (CRLB) for the localization with received signal strength (RSS), based on which, we establish a joint optimization framework for communication enhancement and localization interference. Subsequently, we derive and validate the optimal ARIS partitioning and power allocation under average channel conditions. Finally, tailored optimization methods are proposed for the reflection precoding and AN design of the two partitions. Simulation results validate that, compared to baseline schemes, the proposed scheme significantly increases the localization error of the SU by MUs while maintaining efficient communication between the SU and RUs, thereby effectively protecting the SU location privacy.         ",
    "url": "https://arxiv.org/abs/2511.09094",
    "authors": [
      "Ziqi Chen",
      "Jun Du",
      "Chunxiao Jiang",
      "Tony Q. S. Quek",
      "Zhu Han"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2511.09105",
    "title": "Cost-Minimized Label-Flipping Poisoning Attack to LLM Alignment",
    "abstract": "           Large language models (LLMs) are increasingly deployed in real-world systems, making it critical to understand their vulnerabilities. While data poisoning attacks during RLHF/DPO alignment have been studied empirically, their theoretical foundations remain unclear. We investigate the minimum-cost poisoning attack required to steer an LLM's policy toward an attacker's target by flipping preference labels during RLHF/DPO, without altering the compared outputs. We formulate this as a convex optimization problem with linear constraints, deriving lower and upper bounds on the minimum attack cost. As a byproduct of this theoretical analysis, we show that any existing label-flipping attack can be post-processed via our proposed method to reduce the number of label flips required while preserving the intended poisoning effect. Empirical results demonstrate that this cost-minimization post-processing can significantly reduce poisoning costs over baselines, particularly when the reward model's feature dimension is small relative to the dataset size. These findings highlight fundamental vulnerabilities in RLHF/DPO pipelines and provide tools to evaluate their robustness against low-cost poisoning attacks.         ",
    "url": "https://arxiv.org/abs/2511.09105",
    "authors": [
      "Shigeki Kusaka",
      "Keita Saito",
      "Mikoto Kudo",
      "Takumi Tanabe",
      "Akifumi Wachi",
      "Youhei Akimoto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09114",
    "title": "Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks",
    "abstract": "           Recent advances in deep reinforcement learning for autonomous cyber defence have resulted in agents that can successfully defend simulated computer networks against cyber-attacks. However, many of these agents would need retraining to defend networks with differing topology or size, making them poorly suited to real-world networks where topology and size can vary over time. In this research we introduce a novel set of Topological Extensions for Reinforcement Learning Agents (TERLA) that provide generalisability for the defence of networks with differing topology and size, without the need for retraining. Our approach involves the use of heterogeneous graph neural network layers to produce a fixed-size latent embedding representing the observed network state. This representation learning stage is coupled with a reduced, fixed-size, semantically meaningful and interpretable action space. We apply TERLA to a standard deep reinforcement learning Proximal Policy Optimisation (PPO) agent model, and to reduce the sim-to-real gap, conduct our research using Cyber Autonomy Gym for Experimentation (CAGE) Challenge 4. This Cyber Operations Research Gym environment has many of the features of a real-world network, such as realistic Intrusion Detection System (IDS) events and multiple agents defending network segments of differing topology and size. TERLA agents retain the defensive performance of vanilla PPO agents whilst showing improved action efficiency. Generalisability has been demonstrated by showing that all TERLA agents have the same network-agnostic neural network architecture, and by deploying a single TERLA agent multiple times to defend network segments with differing topology and size, showing improved defensive performance and efficiency.         ",
    "url": "https://arxiv.org/abs/2511.09114",
    "authors": [
      "Tim Dudman",
      "Martyn Bull"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2511.09117",
    "title": "DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization",
    "abstract": "           Kuzushiji, a pre-modern Japanese cursive script, can currently be read and understood by only a few thousand trained experts in Japan. With the rapid development of deep learning, researchers have begun applying Optical Character Recognition (OCR) techniques to transcribe Kuzushiji into modern Japanese. Although existing OCR methods perform well on clean pre-modern Japanese documents written in Kuzushiji, they often fail to consider various types of noise, such as document degradation and seals, which significantly affect recognition accuracy. To the best of our knowledge, no existing dataset specifically addresses these challenges. To address this gap, we introduce the Degraded Kuzushiji Documents with Seals (DKDS) dataset as a new benchmark for related tasks. We describe the dataset construction process, which required the assistance of a trained Kuzushiji expert, and define two benchmark tracks: (1) text and seal detection and (2) document binarization. For the text and seal detection track, we provide baseline results using multiple versions of the You Only Look Once (YOLO) models for detecting Kuzushiji characters and seals. For the document binarization track, we present baseline results from traditional binarization algorithms, traditional algorithms combined with K-means clustering, and Generative Adversarial Network (GAN)-based methods. The DKDS dataset and the implementation code for baseline methods are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.09117",
    "authors": [
      "Rui-Yang Ju",
      "Kohei Yamashita",
      "Hirotaka Kameko",
      "Shinsuke Mori"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.09122",
    "title": "Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation",
    "abstract": "           Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.         ",
    "url": "https://arxiv.org/abs/2511.09122",
    "authors": [
      "Joschka Kersting",
      "Michael Rummel",
      "Gesa Benndorf"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09131",
    "title": "FsimNNs: An Open-Source Graph Neural Network Platform for SEU Simulation-based Fault Injection",
    "abstract": "           Simulation-based fault injection is a widely adopted methodology for assessing circuit vulnerability to Single Event Upsets (SEUs); however, its computational cost grows significantly with circuit complexity. To address this limitation, this work introduces an open-source platform that exploits Spatio-Temporal Graph Neural Networks (STGNNs) to accelerate SEU fault simulation. The platform includes three STGNN architectures incorporating advanced components such as Atrous Spatial Pyramid Pooling (ASPP) and attention mechanisms, thereby improving spatio-temporal feature extraction. In addition, SEU fault simulation datasets are constructed from six open-source circuits with varying levels of complexity, providing a comprehensive benchmark for performance evaluation. The predictive capability of the STGNN models is analyzed and compared on these datasets. Moreover, to further investigate the efficiency of the approach, we evaluate the predictive capability of STGNNs across multiple test cases and discuss their generalization capability. The developed platform and datasets are released as open-source to support reproducibility and further research on this https URL.         ",
    "url": "https://arxiv.org/abs/2511.09131",
    "authors": [
      "Li Lu",
      "Jianan Wen",
      "Milos Krstic"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2511.09132",
    "title": "A Spanning-Tree-Based Algorithm for Planar Graph Dismantling",
    "abstract": "           In spatially embedded networks such as transportation and power grids, understanding how edge removals affect connectivity is crucial for robustness analysis. This paper studies a planar graph dismantling problem under an edge-budget constraint. We propose a spanning-tree-skeleton dual-path framework that first samples multiple uniform spanning trees to capture network backbones and then adaptively selects between two complementary paths according to the budget. The small-budget path estimates a dismantlable subgraph fraction using a logarithmic density feature, while the large-budget path predicts the optimal partition count through a slope-based model. Experiments on random planar graphs demonstrate near-linear runtime scaling, consistent reductions in the largest connected component ratio, and clear budget-fragmentation trends. The method provides an interpretable and efficient approach for planar-network robustness analysis.         ",
    "url": "https://arxiv.org/abs/2511.09132",
    "authors": [
      "Fangchen You"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2511.09139",
    "title": "MACEval: A Multi-Agent Continual Evaluation Network for Large Models",
    "abstract": "           Hundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a \\Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.         ",
    "url": "https://arxiv.org/abs/2511.09139",
    "authors": [
      "Zijian Chen",
      "Yuze Sun",
      "Yuan Tian",
      "Wenjun Zhang",
      "Guangtao Zhai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.09146",
    "title": "DoPE: Denoising Rotary Position Embedding",
    "abstract": "           Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (DoPE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DoPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization. Our project page is Project: this https URL ",
    "url": "https://arxiv.org/abs/2511.09146",
    "authors": [
      "Jing Xiong",
      "Liyang Fan",
      "Hui Shen",
      "Zunhai Su",
      "Min Yang",
      "Lingpeng Kong",
      "Ngai Wong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2511.09148",
    "title": "LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls",
    "abstract": "           Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.         ",
    "url": "https://arxiv.org/abs/2511.09148",
    "authors": [
      "Kangning Zhang",
      "Wenxiang Jiao",
      "Kounianhua Du",
      "Yuan Lu",
      "Weiwen Liu",
      "Weinan Zhang",
      "Lei Zhang",
      "Yong Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09152",
    "title": "Steering Opinion Dynamics in Signed Time-Varying Networks via External Control Input",
    "abstract": "           This paper studies targeted opinion formation in multi-agent systems evolving over signed, time-varying directed graphs. The dynamics of each agent's state follow a Laplacian-based update rule driven by both cooperative and antagonistic interactions in the presence of exogenous factors. We formulate these exogenous factors as external control inputs and establish a suitable controller design methodology enabling collective opinion to converge to any desired steady-state configuration, superseding the natural emergent clustering or polarization behavior imposed by persistently structurally balanced influential root nodes. Our approach leverages upper Dini derivative analysis and Gr\u00f6nwall-type inequalities to establish exponential convergence for opinion magnitude towards the desired steady state configuration on networks with uniform quasi-strong $\\delta$-connectivity. Finally, the theoretical results are validated through extensive numerical simulations.         ",
    "url": "https://arxiv.org/abs/2511.09152",
    "authors": [
      "Swati Priya",
      "Twinkle Tripathy"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2511.09179",
    "title": "A Hybrid Search for Complex Table Question Answering in Securities Report",
    "abstract": "           Recently, Large Language Models (LLMs) are gaining increased attention in the domain of Table Question Answering (TQA), particularly for extracting information from tables in documents. However, directly entering entire tables as long text into LLMs often leads to incorrect answers because most LLMs cannot inherently capture complex table structures. In this paper, we propose a cell extraction method for TQA without manual identification, even for complex table headers. Our approach estimates table headers by computing similarities between a given question and individual cells via a hybrid retrieval mechanism that integrates a language model and TF-IDF. We then select as the answer the cells at the intersection of the most relevant row and column. Furthermore, the language model is trained using contrastive learning on a small dataset of question-header pairs to enhance performance. We evaluated our approach in the TQA dataset from the U4 shared task at NTCIR-18. The experimental results show that our pipeline achieves an accuracy of 74.6\\%, outperforming existing LLMs such as GPT-4o mini~(63.9\\%). In the future, although we used traditional encoder models for retrieval in this study, we plan to incorporate more efficient text-search models to improve performance and narrow the gap with human evaluation results.         ",
    "url": "https://arxiv.org/abs/2511.09179",
    "authors": [
      "Daiki Shirafuji",
      "Koji Tanaka",
      "Tatsuhiko Saito"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09192",
    "title": "Runtime Safety and Reach-avoid Prediction of Stochastic Systems via Observation-aware Barrier Functions",
    "abstract": "           Stochastic dynamical systems have emerged as fundamental models across numerous application domains, providing powerful mathematical representations for capturing uncertain system behavior. In this paper, we address the problem of runtime safety and reach-avoid probability prediction for discrete-time stochastic systems with online observations, i.e., estimating the probability that the system satisfies a given safety or reach-avoid specification. Unlike traditional approaches that rely solely on offline models, we propose a framework that incorporates real-time observations to dynamically refine probability estimates for safety and reach-avoid events. By introducing observation-aware barrier functions, our method adaptively updates probability bounds as new observations are collected, combining efficient offline computation with online backward iteration. This approach enables rigorous and responsive prediction of safety and reach-avoid probabilities under uncertainty. In addition to the theoretical guarantees, experimental results on benchmark systems demonstrate the practical effectiveness of the proposed method.         ",
    "url": "https://arxiv.org/abs/2511.09192",
    "authors": [
      "Shenghua Feng",
      "Jie An",
      "Fanjiang Xu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2511.09200",
    "title": "Sure! Here's a short and concise title for your paper: \"Contamination in Generated Text Detection Benchmarks\"",
    "abstract": "           Large language models are increasingly used for many applications. To prevent illicit use, it is desirable to be able to detect AI-generated text. Training and evaluation of such detectors critically depend on suitable benchmark datasets. Several groups took on the tedious work of collecting, curating, and publishing large and diverse datasets for this task. However, it remains an open challenge to ensure high quality in all relevant aspects of such a dataset. For example, the DetectRL benchmark exhibits relatively simple patterns of AI-generation in 98.5% of the Claude-LLM data. These patterns may include introductory words such as \"Sure! Here is the academic article abstract:\", or instances where the LLM rejects the prompted task. In this work, we demonstrate that detectors trained on such data use such patterns as shortcuts, which facilitates spoofing attacks on the trained detectors. We consequently reprocessed the DetectRL dataset with several cleansing operations. Experiments show that such data cleansing makes direct attacks more difficult. The reprocessed dataset is publicly available.         ",
    "url": "https://arxiv.org/abs/2511.09200",
    "authors": [
      "Philipp Dingfelder",
      "Christian Riess"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09209",
    "title": "CoCo-MILP: Inter-Variable Contrastive and Intra-Constraint Competitive MILP Solution Prediction",
    "abstract": "           Mixed-Integer Linear Programming (MILP) is a cornerstone of combinatorial optimization, yet solving large-scale instances remains a significant computational challenge. Recently, Graph Neural Networks (GNNs) have shown promise in accelerating MILP solvers by predicting high-quality solutions. However, we identify that existing methods misalign with the intrinsic structure of MILP problems at two levels. At the leaning objective level, the Binary Cross-Entropy (BCE) loss treats variables independently, neglecting their relative priority and yielding plausible logits. At the model architecture level, standard GNN message passing inherently smooths the representations across variables, missing the natural competitive relationships within constraints. To address these challenges, we propose CoCo-MILP, which explicitly models inter-variable Contrast and intra-constraint Competition for advanced MILP solution prediction. At the objective level, CoCo-MILP introduces the Inter-Variable Contrastive Loss (VCL), which explicitly maximizes the embedding margin between variables assigned one versus zero. At the architectural level, we design an Intra-Constraint Competitive GNN layer that, instead of homogenizing features, learns to differentiate representations of competing variables within a constraint, capturing their exclusionary nature. Experimental results on standard benchmarks demonstrate that CoCo-MILP significantly outperforms existing learning-based approaches, reducing the solution gap by up to 68.12% compared to traditional solvers. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.09209",
    "authors": [
      "Tianle Pu",
      "Jianing Li",
      "Yingying Gao",
      "Shixuan Liu",
      "Zijie Geng",
      "Haoyang Liu",
      "Chao Chen",
      "Changjun Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09211",
    "title": "Parameter-Free Clustering via Self-Supervised Consensus Maximization (Extended Version)",
    "abstract": "           Clustering is a fundamental task in unsupervised learning, but most existing methods heavily rely on hyperparameters such as the number of clusters or other sensitive settings, limiting their applicability in real-world scenarios. To address this long-standing challenge, we propose a novel and fully parameter-free clustering framework via Self-supervised Consensus Maximization, named SCMax. Our framework performs hierarchical agglomerative clustering and cluster evaluation in a single, integrated process. At each step of agglomeration, it creates a new, structure-aware data representation through a self-supervised learning task guided by the current clustering structure. We then introduce a nearest neighbor consensus score, which measures the agreement between the nearest neighbor-based merge decisions suggested by the original representation and the self-supervised one. The moment at which consensus maximization occurs can serve as a criterion for determining the optimal number of clusters. Extensive experiments on multiple datasets demonstrate that the proposed framework outperforms existing clustering approaches designed for scenarios with an unknown number of clusters.         ",
    "url": "https://arxiv.org/abs/2511.09211",
    "authors": [
      "Lijun Zhang",
      "Suyuan Liu",
      "Siwei Wang",
      "Shengju Yu",
      "Xueling Zhu",
      "Miaomiao Li",
      "Xinwang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09212",
    "title": "Leveraging Self-Paced Learning for Software Vulnerability Detection",
    "abstract": "           Software vulnerabilities are major risks to software systems. Recently, researchers have proposed many deep learning approaches to detect software vulnerabilities. However, their accuracy is limited in practice. One of the main causes is low-quality training data (i.e., source code). To this end, we propose a new approach: SPLVD (Self-Paced Learning for Software Vulnerability Detection). SPLVD dynamically selects source code for model training based on the stage of training, which simulates the human learning process progressing from easy to hard. SPLVD has a data selector that is specifically designed for the vulnerability detection task, which enables it to prioritize the learning of easy source code. Before each training epoch, SPLVD uses the data selector to recalculate the difficulty of the source code, select new training source code, and update the data selector. When evaluating SPLVD, we first use three benchmark datasets with over 239K source code in which 25K are vulnerable for standard evaluations. Experimental results demonstrate that SPLVD achieves the highest F1 of 89.2%, 68.7%, and 43.5%, respectively, outperforming the state-of-the-art approaches. Then we collect projects from OpenHarmony, a new ecosystem that has not been learned by general LLMs, to evaluate SPLVD further. SPLVD achieves the highest precision of 90.9%, demonstrating its practical effectiveness.         ",
    "url": "https://arxiv.org/abs/2511.09212",
    "authors": [
      "Zeru Cheng",
      "Yanjing Yang",
      "He Zhang",
      "Lanxin Yang",
      "Jinghao Hu",
      "Jinwei Xu",
      "Bohan Liu",
      "Haifeng Shen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2511.09223",
    "title": "AILINKPREVIEWER: Enhancing Code Reviews with LLM-Powered Link Previews",
    "abstract": "           Code review is a key practice in software engineering, where developers evaluate code changes to ensure quality and maintainability. Links to issues and external resources are often included in Pull Requests (PRs) to provide additional context, yet they are typically discarded in automated tasks such as PR summarization and code review comment generation. This limits the richness of information available to reviewers and increases cognitive load by forcing context-switching. To address this gap, we present AILINKPREVIEWER, a tool that leverages Large Language Models (LLMs) to generate previews of links in PRs using PR metadata, including titles, descriptions, comments, and link body content. We analyzed 50 engineered GitHub repositories and compared three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. The results in metrics such as BLEU, BERTScore, and compression ratio show that contextual summaries consistently outperform other methods. However, in a user study with seven participants, most preferred non-contextual summaries, suggesting a trade-off between metric performance and perceived usability. These findings demonstrate the potential of LLM-powered link previews to enhance code review efficiency and to provide richer context for developers and automation in software engineering. The video demo is available at this https URL, and the tool and its source code can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.09223",
    "authors": [
      "Panya Trakoolgerntong",
      "Tao Xiao",
      "Masanari Kondo",
      "Chaiyong Ragkhitwetsagul",
      "Morakot Choetkiertikul",
      "Pattaraporn Sangaroonsilp",
      "Yasutaka Kamei"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2511.09235",
    "title": "Investigation of resonance between HVDC-MMC link and AC network",
    "abstract": "           HVDC networks offer several advantages over traditional HVAC systems, particularly for long-distance power transmission and integration of renewable energy sources, such as reduced losses and enhanced stability and control, but also increase the risk of oscillations. This study investigates electrical resonant phenomena associated with HVDC stations through numerical EMT simulations. The findings indicate that electrical resonance is primarily pronounced in weak networks with long cables, as confirmed by the Nyquist criterion applied to frequency responses. Two real cases were successfully simulated in the time domain by introducing network changes, such as temporary faults and alterations in network's power strength, to activate the identified resonances. Notably, in a strong network with short cables, electrical resonance occurred alongside interactions between the network and the converter's protection system. The analysis of voltage waveforms revealed that the amplitude of the induced resonant harmonic dissipates quickly, indicating sufficient damping in the network configuration. Furthermore, the study confirmed the network's sensitivity to changes in converter parameters modeled using available MMC model.         ",
    "url": "https://arxiv.org/abs/2511.09235",
    "authors": [
      "Iva Radecic",
      "Bozidar Filipovic-Grcic",
      "Paul Akiki",
      "Alain Xemard",
      "Bruno Jurisic"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2511.09247",
    "title": "MedFuse: Multiplicative Embedding Fusion For Irregular Clinical Time Series",
    "abstract": "           Clinical time series derived from electronic health records (EHRs) are inherently irregular, with asynchronous sampling, missing values, and heterogeneous feature dynamics. While numerical laboratory measurements are highly informative, existing embedding strategies usually combine feature identity and value embeddings through additive operations, which constrains their ability to capture value-dependent feature interactions. We propose MedFuse, a framework for irregular clinical time series centered on the MuFuse (Multiplicative Embedding Fusion) module. MuFuse fuses value and feature embeddings through multiplicative modulation, preserving feature-specific information while modeling higher-order dependencies across features. Experiments on three real-world datasets covering both intensive and chronic care show that MedFuse consistently outperforms state-of-the-art baselines on key predictive tasks. Analysis of the learned representations further demonstrates that multiplicative fusion enhances expressiveness and supports cross-dataset pretraining. These results establish MedFuse as a generalizable approach for modeling irregular clinical time series.         ",
    "url": "https://arxiv.org/abs/2511.09247",
    "authors": [
      "Yi-Hsien Hsieh",
      "Ta-Jung Chien",
      "Chun-Kai Huang",
      "Shao-Hua Sun",
      "Che Lin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09252",
    "title": "Unveiling Hidden Threats: Using Fractal Triggers to Boost Stealthiness of Distributed Backdoor Attacks in Federated Learning",
    "abstract": "           Traditional distributed backdoor attacks (DBA) in federated learning improve stealthiness by decomposing global triggers into sub-triggers, which however requires more poisoned data to maintian the attck strength and hence increases the exposure risk. To overcome this defect, This paper proposes a novel method, namely Fractal-Triggerred Distributed Backdoor Attack (FTDBA), which leverages the self-similarity of fractals to enhance the feature strength of sub-triggers and hence significantly reduce the required poisoning volume for the same attack strength. To address the detectability of fractal structures in the frequency and gradient domains, we introduce a dynamic angular perturbation mechanism that adaptively adjusts perturbation intensity across the training phases to balance efficiency and stealthiness. Experiments show that FTDBA achieves a 92.3\\% attack success rate with only 62.4\\% of the poisoning volume required by traditional DBA methods, while reducing the detection rate by 22.8\\% and KL divergence by 41.2\\%. This study presents a low-exposure, high-efficiency paradigm for federated backdoor attacks and expands the application of fractal features in adversarial sample generation.         ",
    "url": "https://arxiv.org/abs/2511.09252",
    "authors": [
      "Jian Wang",
      "Hong Shen",
      "Chan-Tong Lam"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09266",
    "title": "SecTracer: A Framework for Uncovering the Root Causes of Network Intrusions via Security Provenance",
    "abstract": "           Modern enterprise networks comprise diverse and heterogeneous systems that support a wide range of services, making it challenging for administrators to track and analyze sophisticated attacks such as advanced persistent threats (APTs), which often exploit multiple vectors. To address this challenge, we introduce the concept of network-level security provenance, which enables the systematic establishment of causal relationships across hosts at the network level, facilitating the accurate identification of the root causes of security incidents. Building on this concept, we present SecTracer as a framework for a network-wide provenance analysis. SecTracer offers three main contributions: (i) comprehensive and efficient forensic data collection in enterprise networks via software-defined networking (SDN), (ii) reconstruction of attack histories through provenance graphs to provide a clear and interpretable view of intrusions, and (iii) proactive attack prediction using probabilistic models. We evaluated the effectiveness and efficiency of SecTracer through a real-world APT simulation, demonstrating its capability to enhance threat mitigation while introducing less than 1% network throughput overhead and negligible latency impact.         ",
    "url": "https://arxiv.org/abs/2511.09266",
    "authors": [
      "Seunghyeon Lee",
      "Hyunmin Seo",
      "Hwanjo Heo",
      "Anduo Wang",
      "Seungwon Shin",
      "Jinwoo Kim"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2511.09268",
    "title": "Decoding the Configuration of AI Coding Agents: Insights from Claude Code Projects",
    "abstract": "           Agentic code assistants are a new generation of AI systems capable of performing end-to-end software engineering tasks. While these systems promise unprecedented productivity gains, their behavior and effectiveness depend heavily on configuration files that define architectural constraints, coding practices, and tool usage policies. However, little is known about the structure and content of these configuration artifacts. This paper presents an empirical study of the configuration ecosystem of Claude Code, one of the most widely used agentic coding systems. We collected and analyzed 328 configuration files from public Claude Code projects to identify (i) the software engineering concerns and practices they specify and (ii) how these concerns co-occur within individual files. The results highlight the importance of defining a wide range of concerns and practices in agent configuration files, with particular emphasis on specifying the architecture the agent should follow.         ",
    "url": "https://arxiv.org/abs/2511.09268",
    "authors": [
      "Helio Victor F. Santos",
      "Vitor Costa",
      "Joao Eduardo Montandon",
      "Marco Tulio Valente"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2511.09269",
    "title": "Robust Estimation and Control for Heterogeneous Multi-agent Systems Based on Decentralized k-hop Prescribed Performance Observers",
    "abstract": "           We propose decentralized k-hop Prescribed Performance State and Input Observers for heterogeneous multi-agent systems subject to bounded external disturbances. In the proposed input/state observer, each agent estimates the state and input of agents located two or more hops away using only local information exchanged with 1-hop neighbors, while guaranteeing that transient estimation errors satisfy predefined performance bounds. Conditions are established under which the input observer can be omitted, allowing the state observer convergence to be independent of the input estimates. Theoretical analysis demonstrates that if a closed-loop controller with full state knowledge achieves the control objective and the estimation-based closed-loop system is set-Input to State Stable (set-ISS) with respect to the goal set, then the estimated states can be used to achieve the system objective with an arbitrarily small worst-case error governed by the accuracy of the states estimates. Simulation results are provided to validate the proposed approach.         ",
    "url": "https://arxiv.org/abs/2511.09269",
    "authors": [
      "Tommaso Zaccherini",
      "Siyuan Liu",
      "Dimos V. Dimarogonas"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2511.09294",
    "title": "GuardFed: A Trustworthy Federated Learning Framework Against Dual-Facet Attacks",
    "abstract": "           Federated learning (FL) enables privacy-preserving collaborative model training but remains vulnerable to adversarial behaviors that compromise model utility or fairness across sensitive groups. While extensive studies have examined attacks targeting either objective, strategies that simultaneously degrade both utility and fairness remain largely unexplored. To bridge this gap, we introduce the Dual-Facet Attack (DFA), a novel threat model that concurrently undermines predictive accuracy and group fairness. Two variants, Synchronous DFA (S-DFA) and Split DFA (Sp-DFA), are further proposed to capture distinct real-world collusion scenarios. Experimental results show that existing robust FL defenses, including hybrid aggregation schemes, fail to resist DFAs effectively. To counter these threats, we propose GuardFed, a self-adaptive defense framework that maintains a fairness-aware reference model using a small amount of clean server data augmented with synthetic samples. In each training round, GuardFed computes a dual-perspective trust score for every client by jointly evaluating its utility deviation and fairness degradation, thereby enabling selective aggregation of trustworthy updates. Extensive experiments on real-world datasets demonstrate that GuardFed consistently preserves both accuracy and fairness under diverse non-IID and adversarial conditions, achieving state-of-the-art performance compared with existing robust FL methods.         ",
    "url": "https://arxiv.org/abs/2511.09294",
    "authors": [
      "Yanli Li",
      "Yanan Zhou",
      "Zhongliang Guo",
      "Nan Yang",
      "Yuning Zhang",
      "Huaming Chen",
      "Dong Yuan",
      "Weiping Ding",
      "Witold Pedrycz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09297",
    "title": "On the complexity of freezing automata networks of bounded pathwidth",
    "abstract": "           An automata network is a graph of entities, each holding a state from a finite set and evolving according to a local update rule which depends only on its neighbors in the network's graph. It is freezing if there is an order on the states such that the state evolution of any node is non-decreasing in any orbit. They are commonly used to model epidemic propagation, diffusion phenomena like bootstrap percolation or cristal growth. Previous works have established that, under the hypothesis that the network graph is of bounded treewidth, many problems that can be captured by trace specifications at individual nodes admit efficient algorithms. In this paper we study the even more restricted case of a network of bounded pathwidth and show two hardness results that somehow illustrate the complexity of freezing dynamics under such a strong graph constraint. First, we show that the trace specification checking problem is NL-complete. Second, we show that deciding first order properties of the orbits augmented with a reachability predicate is NP-hard.         ",
    "url": "https://arxiv.org/abs/2511.09297",
    "authors": [
      "Eric Goles",
      "Pedro Montealegre",
      "Mart\u00edn R\u00edos-Wilson",
      "Guillaume Theyssier"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2511.09299",
    "title": "Efficiently Transforming Neural Networks into Decision Trees: A Path to Ground Truth Explanations with RENTT",
    "abstract": "           Although neural networks are a powerful tool, their widespread use is hindered by the opacity of their decisions and their black-box nature, which result in a lack of trustworthiness. To alleviate this problem, methods in the field of explainable Artificial Intelligence try to unveil how such automated decisions are made. But explainable AI methods are often plagued by missing faithfulness/correctness, meaning that they sometimes provide explanations that do not align with the neural network's decision and logic. Recently, transformations to decision trees have been proposed to overcome such problems. Unfortunately, they typically lack exactness, scalability, or interpretability as the size of the neural network grows. Thus, we generalize these previous results, especially by considering convolutional neural networks, recurrent neural networks, non-ReLU activation functions, and bias terms. Our findings are accompanied by rigorous proofs and we present a novel algorithm RENTT (Runtime Efficient Network to Tree Transformation) designed to compute an exact equivalent decision tree representation of neural networks in a manner that is both runtime and memory efficient. The resulting decision trees are multivariate and thus, possibly too complex to understand. To alleviate this problem, we also provide a method to calculate the ground truth feature importance for neural networks via the equivalent decision trees - for entire models (global), specific input regions (regional), or single decisions (local). All theoretical results are supported by detailed numerical experiments that emphasize two key aspects: the computational efficiency and scalability of our algorithm, and that only RENTT succeeds in uncovering ground truth explanations compared to conventional approximation methods like LIME and SHAP. All code is available at this https URL .         ",
    "url": "https://arxiv.org/abs/2511.09299",
    "authors": [
      "Helena Monke",
      "Benjamin Fresz",
      "Marco Bernreuther",
      "Yilin Chen",
      "Marco F. Huber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09315",
    "title": "A Tensor Residual Circuit Neural Network Factorized with Matrix Product Operation",
    "abstract": "           It is challenging to reduce the complexity of neural networks while maintaining their generalization ability and robustness, especially for practical applications. Conventional solutions for this problem incorporate quantum-inspired neural networks with Kronecker products and hybrid tensor neural networks with MPO factorization and fully-connected layers. Nonetheless, the generalization power and robustness of the fully-connected layers are not as outstanding as circuit models in quantum computing. In this paper, we propose a novel tensor circuit neural network (TCNN) that takes advantage of the characteristics of tensor neural networks and residual circuit models to achieve generalization ability and robustness with low complexity. The proposed activation operation and parallelism of the circuit in complex number field improves its non-linearity and efficiency for feature learning. Moreover, since the feature information exists in the parameters in both the real and imaginary parts in TCNN, an information fusion layer is proposed for merging features stored in those parameters to enhance the generalization capability. Experimental results confirm that TCNN showcases more outstanding generalization and robustness with its average accuracies on various datasets 2\\%-3\\% higher than those of the state-of-the-art compared models. More significantly, while other models fail to learn features under noise parameter attacking, TCNN still showcases prominent learning capability owing to its ability to prevent gradient explosion. Furthermore, it is comparable to the compared models on the number of trainable parameters and the CPU running time. An ablation study also indicates the advantage of the activation operation, the parallelism architecture and the information fusion layer.         ",
    "url": "https://arxiv.org/abs/2511.09315",
    "authors": [
      "Andi Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09316",
    "title": "AdaptDel: Adaptable Deletion Rate Randomized Smoothing for Certified Robustness",
    "abstract": "           We consider the problem of certified robustness for sequence classification against edit distance perturbations. Naturally occurring inputs of varying lengths (e.g., sentences in natural language processing tasks) present a challenge to current methods that employ fixed-rate deletion mechanisms and lead to suboptimal performance. To this end, we introduce AdaptDel methods with adaptable deletion rates that dynamically adjust based on input properties. We extend the theoretical framework of randomized smoothing to variable-rate deletion, ensuring sound certification with respect to edit distance. We achieve strong empirical results in natural language tasks, observing up to 30 orders of magnitude improvement to median cardinality of the certified region, over state-of-the-art certifications.         ",
    "url": "https://arxiv.org/abs/2511.09316",
    "authors": [
      "Zhuoqun Huang",
      "Neil G. Marchant",
      "Olga Ohrimenko",
      "Benjamin I. P. Rubinstein"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09329",
    "title": "Sim4IA-Bench: A User Simulation Benchmark Suite for Next Query and Utterance Prediction",
    "abstract": "           Validating user simulation is a difficult task due to the lack of established measures and benchmarks, which makes it challenging to assess whether a simulator accurately reflects real user behavior. As part of the Sim4IA Micro-Shared Task at the Sim4IA Workshop, SIGIR 2025, we present Sim4IA-Bench, a simulation benchmark suit for the prediction of the next queries and utterances, the first of its kind in the IR com- munity. Our dataset as part of the suite comprises 160 real-world search sessions from the CORE search engine. For 70 of these sessions, up to 62 simulator runs are available, divided into Task A and Task B, in which different approaches predicted users next search queries or utterances. Sim4IA-Bench provides a basis for evaluating and comparing user simu- lation approaches and for developing new measures of simulator validity. Although modest in size, the suite represents the first publicly available benchmark that links real search sessions with simulated next-query pre- dictions. In addition to serving as a testbed for next query prediction, it also enables exploratory studies on query reformulation behavior, intent drift, and interaction-aware retrieval evaluation. We also introduce a new measure for evaluating next-query predictions in this task. By making the suite publicly available, we aim to promote reproducible research and stimulate further work on realistic and explainable user simulation for information access: this https URL.         ",
    "url": "https://arxiv.org/abs/2511.09329",
    "authors": [
      "Andreas Konstantin Kruff",
      "Christin Katharina Kreutz",
      "Timo Breuer",
      "Philipp Schaer",
      "Krisztian Balog"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2511.09347",
    "title": "FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection",
    "abstract": "           Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.         ",
    "url": "https://arxiv.org/abs/2511.09347",
    "authors": [
      "Jiangyong Yu",
      "Changyong Shu",
      "Sifan Zhou",
      "Zichen Yu",
      "Xing Hu",
      "Yan Chen",
      "Dawei Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.09351",
    "title": "Quantum Meet-in-the-Middle Attacks on Key-Length Extension Constructions",
    "abstract": "           Key-length extension (KLE) techniques provide a general approach to enhancing the security of block ciphers by using longer keys. There are mainly two classes of KLE techniques, cascade encryption and XOR-cascade encryption. This paper presents several quantum meet-in-the-middle (MITM) attacks against two specific KLE constructions. For the two-key triple encryption (2kTE), we propose two quantum MITM attacks under the Q2 model. The first attack, leveraging the quantum claw-finding (QCF) algorithm, achieves a time complexity of $O(2^{2\\kappa/3})$ with $O(2^{2\\kappa/3})$ quantum random access memory (QRAM). The second attack, based on Grover's algorithm, achieves a time complexity of $O(2^{\\kappa/2})$ with $O(2^\\kappa)$ QRAM. The latter complexity is nearly identical to Grover-based brute-force attack on the underlying block cipher, indicating that 2kTE does not enhance security under the Q2 model when sufficient QRAM resources are available. For the 3XOR-cascade encryption (3XCE), we propose a quantum MITM attack applicable to the Q1 model. This attack requires no QRAM and has a time complexity of $O(2^{(\\kappa+n)/2})$ ($\\kappa$ and $n$ are the key length and block length of the underlying block cipher, respectively.), achieving a quadratic speedup over classical MITM attack. Furthermore, we extend the quantum MITM attack to quantum sieve-in-the-middle (SITM) attack, which is applicable for more constructions. We present a general quantum SITM framework for the construction $ELE=E^2\\circ L\\circ E^1$ and provide specific attack schemes for three different forms of the middle layer $L$. The quantum SITM attack technique can be further applied to a broader range of quantum cryptanalysis scenarios.         ",
    "url": "https://arxiv.org/abs/2511.09351",
    "authors": [
      "Min Liang",
      "Ruihao Gao",
      "Jiali Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2511.09352",
    "title": "Spatio-Temporal Context Learning with Temporal Difference Convolution for Moving Infrared Small Target Detection",
    "abstract": "           Moving infrared small target detection (IRSTD) plays a critical role in practical applications, such as surveillance of unmanned aerial vehicles (UAVs) and UAV-based search system. Moving IRSTD still remains highly challenging due to weak target features and complex background interference. Accurate spatio-temporal feature modeling is crucial for moving target detection, typically achieved through either temporal differences or spatio-temporal (3D) convolutions. Temporal difference can explicitly leverage motion cues but exhibits limited capability in extracting spatial features, whereas 3D convolution effectively represents spatio-temporal features yet lacks explicit awareness of motion dynamics along the temporal dimension. In this paper, we propose a novel moving IRSTD network (TDCNet), which effectively extracts and enhances spatio-temporal features for accurate target detection. Specifically, we introduce a novel temporal difference convolution (TDC) re-parameterization module that comprises three parallel TDC blocks designed to capture contextual dependencies across different temporal ranges. Each TDC block fuses temporal difference and 3D convolution into a unified spatio-temporal convolution representation. This re-parameterized module can effectively capture multi-scale motion contextual features while suppressing pseudo-motion clutter in complex backgrounds, significantly improving detection performance. Moreover, we propose a TDC-guided spatio-temporal attention mechanism that performs cross-attention between the spatio-temporal features from the TDC-based backbone and a parallel 3D backbone. This mechanism models their global semantic dependencies to refine the current frame's features. Extensive experiments on IRSTD-UAV and public infrared datasets demonstrate that our TDCNet achieves state-of-the-art detection performance in moving target detection.         ",
    "url": "https://arxiv.org/abs/2511.09352",
    "authors": [
      "Houzhang Fang",
      "Shukai Guo",
      "Qiuhuan Chen",
      "Yi Chang",
      "Luxin Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.09388",
    "title": "Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition",
    "abstract": "           Recognizing unseen skeleton action categories remains highly challenging due to the absence of corresponding skeletal priors. Existing approaches generally follow an \"align-then-classify\" paradigm but face two fundamental issues, i.e., (i) fragile point-to-point alignment arising from imperfect semantics, and (ii) rigid classifiers restricted by static decision boundaries and coarse-grained anchors. To address these issues, we propose a novel method for zero-shot skeleton action recognition, termed $\\texttt{$\\textbf{Flora}$}$, which builds upon $\\textbf{F}$lexib$\\textbf{L}$e neighb$\\textbf{O}$r-aware semantic attunement and open-form dist$\\textbf{R}$ibution-aware flow cl$\\textbf{A}$ssifier. Specifically, we flexibly attune textual semantics by incorporating neighboring inter-class contextual cues to form direction-aware regional semantics, coupled with a cross-modal geometric consistency objective that ensures stable and robust point-to-region alignment. Furthermore, we employ noise-free flow matching to bridge the modality distribution gap between semantic and skeleton latent embeddings, while a condition-free contrastive regularization enhances discriminability, leading to a distribution-aware classifier with fine-grained decision boundaries achieved through token-level velocity predictions. Extensive experiments on three benchmark datasets validate the effectiveness of our method, showing particularly impressive performance even when trained with only 10\\% of the seen data. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.09388",
    "authors": [
      "Yang Chen",
      "Miaoge Li",
      "Zhijie Rao",
      "Deze Zeng",
      "Song Guo",
      "Jingcai Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.09400",
    "title": "Abstract Gradient Training: A Unified Certification Framework for Data Poisoning, Unlearning, and Differential Privacy",
    "abstract": "           The impact of inference-time data perturbation (e.g., adversarial attacks) has been extensively studied in machine learning, leading to well-established certification techniques for adversarial robustness. In contrast, certifying models against training data perturbations remains a relatively under-explored area. These perturbations can arise in three critical contexts: adversarial data poisoning, where an adversary manipulates training samples to corrupt model performance; machine unlearning, which requires certifying model behavior under the removal of specific training data; and differential privacy, where guarantees must be given with respect to substituting individual data points. This work introduces Abstract Gradient Training (AGT), a unified framework for certifying robustness of a given model and training procedure to training data perturbations, including bounded perturbations, the removal of data points, and the addition of new samples. By bounding the reachable set of parameters, i.e., establishing provable parameter-space bounds, AGT provides a formal approach to analyzing the behavior of models trained via first-order optimization methods.         ",
    "url": "https://arxiv.org/abs/2511.09400",
    "authors": [
      "Philip Sosnin",
      "Matthew Wicker",
      "Josh Collyer",
      "Calvin Tsay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09404",
    "title": "Spatio-Temporal Graph Unlearning",
    "abstract": "           Spatio-temporal graphs are widely used in modeling complex dynamic processes such as traffic forecasting, molecular dynamics, and healthcare monitoring. Recently, stringent privacy regulations such as GDPR and CCPA have introduced significant new challenges for existing spatio-temporal graph models, requiring complete unlearning of unauthorized data. Since each node in a spatio-temporal graph diffuses information globally across both spatial and temporal dimensions, existing unlearning methods primarily designed for static graphs and localized data removal cannot efficiently erase a single node without incurring costs nearly equivalent to full model retraining. Therefore, an effective approach for complete spatio-temporal graph unlearning is a pressing need. To address this, we propose CallosumNet, a divide-and-conquer spatio-temporal graph unlearning framework inspired by the corpus callosum structure that facilitates communication between the brain's two hemispheres. CallosumNet incorporates two novel techniques: (1) Enhanced Subgraph Construction (ESC), which adaptively constructs multiple localized subgraphs based on several factors, including biologically-inspired virtual ganglions; and (2) Global Ganglion Bridging (GGB), which reconstructs global spatio-temporal dependencies from these localized subgraphs, effectively restoring the full graph representation. Empirical results on four diverse real-world datasets show that CallosumNet achieves complete unlearning with only 1%-2% relative MAE loss compared to the gold model, significantly outperforming state-of-the-art baselines. Ablation studies verify the effectiveness of both proposed techniques.         ",
    "url": "https://arxiv.org/abs/2511.09404",
    "authors": [
      "Qiming Guo",
      "Wenbo Sun",
      "Wenlu Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09438",
    "title": "LLM-Guided Dynamic-UMAP for Personalized Federated Graph Learning",
    "abstract": "           We propose a method that uses large language models to assist graph machine learning under personalization and privacy constraints. The approach combines data augmentation for sparse graphs, prompt and instruction tuning to adapt foundation models to graph tasks, and in-context learning to supply few-shot graph reasoning signals. These signals parameterize a Dynamic UMAP manifold of client-specific graph embeddings inside a Bayesian variational objective for personalized federated learning. The method supports node classification and link prediction in low-resource settings and aligns language model latent representations with graph structure via a cross-modal regularizer. We outline a convergence argument for the variational aggregation procedure, describe a differential privacy threat model based on a moments accountant, and present applications to knowledge graph completion, recommendation-style link prediction, and citation and product graphs. We also discuss evaluation considerations for benchmarking LLM-assisted graph machine learning.         ",
    "url": "https://arxiv.org/abs/2511.09438",
    "authors": [
      "Sai Puppala",
      "Ismail Hossain",
      "Md Jahangir Alam",
      "Tanzim Ahad",
      "Sajedul Talukder"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09477",
    "title": "Latent Planning via Embedding Arithmetic: A Contrastive Approach to Strategic Reasoning",
    "abstract": "           Planning in high-dimensional decision spaces is increasingly being studied through the lens of learned representations. Rather than training policies or value heads, we investigate whether planning can be carried out directly in an evaluation-aligned embedding space. We introduce SOLIS, which learns such a space using supervised contrastive learning. In this representation, outcome similarity is captured by proximity, and a single global advantage vector orients the space from losing to winning regions. Candidate actions are then ranked according to their alignment with this direction, reducing planning to vector operations in latent space. We demonstrate this approach in chess, where SOLIS uses only a shallow search guided by the learned embedding to reach competitive strength under constrained conditions. More broadly, our results suggest that evaluation-aligned latent planning offers a lightweight alternative to traditional dynamics models or policy learning.         ",
    "url": "https://arxiv.org/abs/2511.09477",
    "authors": [
      "Andrew Hamara",
      "Greg Hamerly",
      "Pablo Rivas",
      "Andrew C. Freeman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09512",
    "title": "GenePheno: Interpretable Gene Knockout-Induced Phenotype Abnormality Prediction from Gene Sequences",
    "abstract": "           Exploring how genetic sequences shape phenotypes is a fundamental challenge in biology and a key step toward scalable, hypothesis-driven experimentation. The task is complicated by the large modality gap between sequences and phenotypes, as well as the pleiotropic nature of gene-phenotype relationships. Existing sequence-based efforts focus on the degree to which variants of specific genes alter a limited set of phenotypes, while general gene knockout induced phenotype abnormality prediction methods heavily rely on curated genetic information as inputs, which limits scalability and generalizability. As a result, the task of broadly predicting the presence of multiple phenotype abnormalities under gene knockout directly from gene sequences remains underexplored. We introduce GenePheno, the first interpretable multi-label prediction framework that predicts knockout induced phenotypic abnormalities from gene sequences. GenePheno employs a contrastive multi-label learning objective that captures inter-phenotype correlations, complemented by an exclusive regularization that enforces biological consistency. It further incorporates a gene function bottleneck layer, offering human interpretable concepts that reflect functional mechanisms behind phenotype formation. To support progress in this area, we curate four datasets with canonical gene sequences as input and multi-label phenotypic abnormalities induced by gene knockouts as targets. Across these datasets, GenePheno achieves state-of-the-art gene-centric Fmax and phenotype-centric AUC, and case studies demonstrate its ability to reveal gene functional mechanisms.         ",
    "url": "https://arxiv.org/abs/2511.09512",
    "authors": [
      "Jingquan Yan",
      "Yuwei Miao",
      "Lei Yu",
      "Yuzhi Guo",
      "Xue Xiao",
      "Lin Xu",
      "Junzhou Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09535",
    "title": "Robust and Diverse Multi-Agent Learning via Rational Policy Gradient",
    "abstract": "           Adversarial optimization algorithms that explicitly search for flaws in agents' policies have been successfully applied to finding robust and diverse policies in multi-agent settings. However, the success of adversarial optimization has been largely limited to zero-sum settings because its naive application in cooperative settings leads to a critical failure mode: agents are irrationally incentivized to self-sabotage, blocking the completion of tasks and halting further learning. To address this, we introduce Rationality-preserving Policy Optimization (RPO), a formalism for adversarial optimization that avoids self-sabotage by ensuring agents remain rational--that is, their policies are optimal with respect to some possible partner policy. To solve RPO, we develop Rational Policy Gradient (RPG), which trains agents to maximize their own reward in a modified version of the original game in which we use opponent shaping techniques to optimize the adversarial objective. RPG enables us to extend a variety of existing adversarial optimization algorithms that, no longer subject to the limitations of self-sabotage, can find adversarial examples, improve robustness and adaptability, and learn diverse policies. We empirically validate that our approach achieves strong performance in several popular cooperative and general-sum environments. Our project page can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.09535",
    "authors": [
      "Niklas Lauffer",
      "Ameesh Shah",
      "Micah Carroll",
      "Sanjit A. Seshia",
      "Stuart Russell",
      "Michael Dennis"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.09554",
    "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
    "abstract": "           Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at this https URL ",
    "url": "https://arxiv.org/abs/2511.09554",
    "authors": [
      "Isaac Robinson",
      "Peter Robicheaux",
      "Matvei Popov",
      "Deva Ramanan",
      "Neehar Peri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.09555",
    "title": "SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation",
    "abstract": "           Robotic manipulation requires precise spatial understanding to interact with objects in the real world. Point-based methods suffer from sparse sampling, leading to the loss of fine-grained semantics. Image-based methods typically feed RGB and depth into 2D backbones pre-trained on 3D auxiliary tasks, but their entangled semantics and geometry are sensitive to inherent depth noise in real-world that disrupts semantic understanding. Moreover, these methods focus on high-level geometry while overlooking low-level spatial cues essential for precise interaction. We propose SpatialActor, a disentangled framework for robust robotic manipulation that explicitly decouples semantics and geometry. The Semantic-guided Geometric Module adaptively fuses two complementary geometry from noisy depth and semantic-guided expert priors. Also, a Spatial Transformer leverages low-level spatial cues for accurate 2D-3D mapping and enables interaction among spatial features. We evaluate SpatialActor on multiple simulation and real-world scenarios across 50+ tasks. It achieves state-of-the-art performance with 87.4% on RLBench and improves by 13.9% to 19.4% under varying noisy conditions, showing strong robustness. Moreover, it significantly enhances few-shot generalization to new tasks and maintains robustness under various spatial perturbations. Project Page: this https URL ",
    "url": "https://arxiv.org/abs/2511.09555",
    "authors": [
      "Hao Shi",
      "Bin Xie",
      "Yingfei Liu",
      "Yang Yue",
      "Tiancai Wang",
      "Haoqiang Fan",
      "Xiangyu Zhang",
      "Gao Huang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.08606",
    "title": "Data-driven Feynman-Kac Discovery with Applications to Prediction and Data Generation",
    "abstract": "           In this paper, we propose a novel data-driven framework for discovering probabilistic laws underlying the Feynman-Kac formula. Specifically, we introduce the first stochastic SINDy method formulated under the risk-neutral probability measure to recover the backward stochastic differential equation (BSDE) from a single pair of stock and option trajectories. Unlike existing approaches to identifying stochastic differential equations-which typically require ergodicity-our framework leverages the risk-neutral measure, thereby eliminating the ergodicity assumption and enabling BSDE recovery from limited financial time series data. Using this algorithm, we are able not only to make forward-looking predictions but also to generate new synthetic data paths consistent with the underlying probabilistic law.         ",
    "url": "https://arxiv.org/abs/2511.08606",
    "authors": [
      "Qi Feng",
      "Guang Lin",
      "Purav Matlia",
      "Denny Serdarevic"
    ],
    "subjectives": [
      "Mathematical Finance (q-fin.MF)",
      "Artificial Intelligence (cs.AI)",
      "Computational Finance (q-fin.CP)"
    ]
  },
  {
    "id": "arXiv:2511.08625",
    "title": "Cross-Field Interface-Aware Neural Operators for Multiphase Flow Simulation",
    "abstract": "           Multiphase flow systems, with their complex dynamics, field discontinuities, and interphase interactions, pose significant computational challenges for traditional numerical solvers. While neural operators offer efficient alternatives, they often struggle to achieve high-resolution numerical accuracy in these systems. This limitation primarily stems from the inherent spatial heterogeneity and the scarcity of high-quality training data in multiphase flows. In this work, we propose the Interface Information-Aware Neural Operator (IANO), a novel framework that explicitly leverages interface information as a physical prior to enhance the prediction accuracy. The IANO architecture introduces two key components: 1) An interface-aware multiple function encoding mechanism jointly models multiple physical fields and interfaces, thus capturing the high-frequency physical features at the interface. 2) A geometry-aware positional encoding mechanism further establishes the relationship between interface information, physical variables, and spatial positions, enabling it to achieve pointwise super-resolution prediction even in the low-data regimes. Experimental results demonstrate that IANO outperforms baselines by $\\sim$10\\% in accuracy for multiphase flow simulations while maintaining robustness under data-scarce and noise-perturbed conditions.         ",
    "url": "https://arxiv.org/abs/2511.08625",
    "authors": [
      "ZhenZhong Wang",
      "Xin Zhang",
      "Jun Liao",
      "Min Jiang"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.08626",
    "title": "SAMora: Enhancing SAM through Hierarchical Self-Supervised Pre-Training for Medical Images",
    "abstract": "           The Segment Anything Model (SAM) has demonstrated significant potential in medical image segmentation. Yet, its performance is limited when only a small amount of labeled data is available, while there is abundant valuable yet often overlooked hierarchical information in medical data. To address this limitation, we draw inspiration from self-supervised learning and propose SAMora, an innovative framework that captures hierarchical medical knowledge by applying complementary self-supervised learning objectives at the image, patch, and pixel levels. To fully exploit the complementarity of hierarchical knowledge within LoRAs, we introduce HL-Attn, a hierarchical fusion module that integrates multi-scale features while maintaining their distinct characteristics. SAMora is compatible with various SAM variants, including SAM2, SAMed, and H-SAM. Experimental results on the Synapse, LA, and PROMISE12 datasets demonstrate that SAMora outperforms existing SAM variants. It achieves state-of-the-art performance in both few-shot and fully supervised settings while reducing fine-tuning epochs by 90%. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.08626",
    "authors": [
      "Shuhang Chen",
      "Hangjie Yuan",
      "Pengwei Liu",
      "Hanxue Gu",
      "Tao Feng",
      "Dong Ni"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.08642",
    "title": "Robust Multi-modal Task-oriented Communications with Redundancy-aware Representations",
    "abstract": "           Semantic communications for multi-modal data can transmit task-relevant information efficiently over noisy and bandwidth-limited channels. However, a key challenge is to simultaneously compress inter-modal redundancy and improve semantic reliability under channel distortion. To address the challenge, we propose a robust and efficient multi-modal task-oriented communication framework that integrates a two-stage variational information bottleneck (VIB) with mutual information (MI) redundancy minimization. In the first stage, we apply uni-modal VIB to compress each modality separately, i.e., text, audio, and video, while preserving task-specific features. To enhance efficiency, an MI minimization module with adversarial training is then used to suppress cross-modal dependencies and to promote complementarity rather than redundancy. In the second stage, a multi-modal VIB is further used to compress the fused representation and to enhance robustness against channel distortion. Experimental results on multi-modal emotion recognition tasks demonstrate that the proposed framework significantly outperforms existing baselines in accuracy and reliability, particularly under low signal-to-noise ratio regimes. Our work provides a principled framework that jointly optimizes modality-specific compression, inter-modal redundancy, and communication reliability.         ",
    "url": "https://arxiv.org/abs/2511.08642",
    "authors": [
      "Jingwen Fu",
      "Ming Xiao",
      "Zhonghao Lyu",
      "Mikael Skoglund",
      "Celimuge Wu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2511.08645",
    "title": "Fluence Map Prediction with Deep Learning: A Transformer-based Approach",
    "abstract": "           Accurate fluence map prediction is essential in intensity-modulated radiation therapy (IMRT) to maximize tumor coverage while minimizing dose to healthy tissues. Conventional optimization is time-consuming and dependent on planner expertise. This study presents a deep learning framework that accelerates fluence map generation while maintaining clinical quality. An end-to-end 3D Swin-UNETR network was trained to predict nine-beam fluence maps directly from volumetric CT images and anatomical contours using 99 prostate IMRT cases (79 for training and 20 for testing). The transformer-based model employs hierarchical self-attention to capture both local anatomical structures and long-range spatial dependencies. Predicted fluence maps were imported into the Eclipse Treatment Planning System for dose recalculation, and model performance was evaluated using beam-wise fluence correlation, spatial gamma analysis, and dose-volume histogram (DVH) metrics. The proposed model achieved an average R^2 of 0.95 +/- 0.02, MAE of 0.035 +/- 0.008, and gamma passing rate of 85 +/- 10 percent (3 percent / 3 mm) on the test set, with no significant differences observed in DVH parameters between predicted and clinical plans. The Swin-UNETR framework enables fully automated, inverse-free fluence map prediction directly from anatomical inputs, enhancing spatial coherence, accuracy, and efficiency while offering a scalable and consistent solution for automated IMRT plan generation.         ",
    "url": "https://arxiv.org/abs/2511.08645",
    "authors": [
      "Ujunwa Mgboh",
      "Rafi Sultan",
      "Dongxiao Zhu",
      "Joshua Kim"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.08648",
    "title": "Compact Artificial Neural Network Models for Predicting Protein Residue - RNA Base Binding",
    "abstract": "           Large Artificial Neural Network (ANN) models have demonstrated success in various domains, including general text and image generation, drug discovery, and protein-RNA (ribonucleic acid) binding tasks. However, these models typically demand substantial computational resources, time, and data for effective training. Given that such extensive resources are often inaccessible to many researchers and that life sciences data sets are frequently limited, we investigated whether small ANN models could achieve acceptable accuracy in protein-RNA prediction. We experimented with shallow feed-forward ANNs comprising two hidden layers and various non-linearities. These models did not utilize explicit structural information; instead, a sliding window approach was employed to implicitly consider the context of neighboring residues and bases. We explored different training techniques to address the issue of highly unbalanced data. Among the seven most popular non-linearities for feed-forward ANNs, only three: Rectified Linear Unit (ReLU), Gated Linear Unit (GLU), and Hyperbolic Tangent (Tanh) yielded converging models. Common re-balancing techniques, such as under- and over-sampling of training sets, proved ineffective, whereas increasing the volume of training data and using model ensembles significantly improved performance. The optimal context window size, balancing both false negative and false positive errors, was found to be approximately 30 residues and bases. Our findings indicate that high-accuracy protein-RNA binding prediction is achievable using computing hardware accessible to most educational and research institutions.         ",
    "url": "https://arxiv.org/abs/2511.08648",
    "authors": [
      "Stanislav Selitskiy"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.08658",
    "title": "\"It Looks All the Same to Me\": Cross-index Training for Long-term Financial Series Prediction",
    "abstract": "           We investigate a number of Artificial Neural Network architectures (well-known and more ``exotic'') in application to the long-term financial time-series forecasts of indexes on different global markets. The particular area of interest of this research is to examine the correlation of these indexes' behaviour in terms of Machine Learning algorithms cross-training. Would training an algorithm on an index from one global market produce similar or even better accuracy when such a model is applied for predicting another index from a different market? The demonstrated predominately positive answer to this question is another argument in favour of the long-debated Efficient Market Hypothesis of Eugene Fama.         ",
    "url": "https://arxiv.org/abs/2511.08658",
    "authors": [
      "Stanislav Selitskiy"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.08768",
    "title": "WATSON-Net: Vetting, Validation, and Analysis of Transits from Space Observations with Neural Networks",
    "abstract": "           Context. As the number of detected transiting exoplanet candidates continues to grow, the need for robust and scalable automated tools to prioritize or validate them has become increasingly critical. Among the most promising solutions, deep learning models offer the ability to interpret complex diagnostic metrics traditionally used in the vetting process. Aims. In this work, we present WATSON-Net, a new open-source neural network classifier and data preparation package designed to compete with current state-of-the-art tools for vetting and validation of transiting exoplanet signals from space-based missions. Methods. Trained on Kepler Q1-Q17 DR25 data using 10-fold cross-validation, WATSON-Net produces ten independent models, each evaluated on dedicated validation and test sets. The ten models are calibrated and prepared to be extensible for TESS data by standardizing the input pipeline, allowing for performance assessment across different space missions. Results. For Kepler targets, WATSON-Net achieves a recall-at-precision of 0.99 (R@P0.99) of 0.903, ranking second, with only the ExoMiner network performing better (R@P0.99 = 0.936). For TESS signals, WATSON-Net emerges as the best-performing non-fine-tuned machine learning classifier, achieving a precision of 0.93 and a recall of 0.76 on a test set comprising confirmed planets and false positives. Both the model and its data preparation tools are publicly available in the dearwatson Python package, fully open-source and integrated into the vetting engine of the SHERLOCK pipeline.         ",
    "url": "https://arxiv.org/abs/2511.08768",
    "authors": [
      "M. D\u00e9vora-Pajares",
      "F.J. Pozuelos",
      "J.C. Su\u00e1rez",
      "M. Gonz\u00e1lez-Penedo",
      "C. Dafonte"
    ],
    "subjectives": [
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.08955",
    "title": "MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction",
    "abstract": "           Simulating microstructure evolution (MicroEvo) is vital for materials design but demands high numerical accuracy, efficiency, and physical fidelity. Although recent studies on deep learning (DL) offer a promising alternative to traditional solvers, the field lacks standardized benchmarks. Existing studies are flawed due to a lack of comparing specialized MicroEvo DL models with state-of-the-art spatio-temporal architectures, an overemphasis on numerical accuracy over physical fidelity, and a failure to analyze error propagation over time. To address these gaps, we introduce MicroEvoEval, the first comprehensive benchmark for image-based microstructure evolution prediction. We evaluate 14 models, encompassing both domain-specific and general-purpose architectures, across four representative MicroEvo tasks with datasets specifically structured for both short- and long-term assessment. Our multi-faceted evaluation framework goes beyond numerical accuracy and computational cost, incorporating a curated set of structure-preserving metrics to assess physical fidelity. Our extensive evaluations yield several key insights. Notably, we find that modern architectures (e.g., VMamba), not only achieve superior long-term stability and physical fidelity but also operate with an order-of-magnitude greater computational efficiency. The results highlight the necessity of holistic evaluation and identify these modern architectures as a highly promising direction for developing efficient and reliable surrogate models in data-driven materials science.         ",
    "url": "https://arxiv.org/abs/2511.08955",
    "authors": [
      "Qinyi Zhang",
      "Duanyu Feng",
      "Ronghui Han",
      "Yangshuai Wang",
      "Hao Wang"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.08991",
    "title": "Robust Sampling for Active Statistical Inference",
    "abstract": "           Active statistical inference is a new method for inference with AI-assisted data collection. Given a budget on the number of labeled data points that can be collected and assuming access to an AI predictive model, the basic idea is to improve estimation accuracy by prioritizing the collection of labels where the model is most uncertain. The drawback, however, is that inaccurate uncertainty estimates can make active sampling produce highly noisy results, potentially worse than those from naive uniform sampling. In this work, we present robust sampling strategies for active statistical inference. Robust sampling ensures that the resulting estimator is never worse than the estimator using uniform sampling. Furthermore, with reliable uncertainty estimates, the estimator usually outperforms standard active inference. This is achieved by optimally interpolating between uniform and active sampling, depending on the quality of the uncertainty scores, and by using ideas from robust optimization. We demonstrate the utility of the method on a series of real datasets from computational social science and survey research.         ",
    "url": "https://arxiv.org/abs/2511.08991",
    "authors": [
      "Puheng Li",
      "Tijana Zrnic",
      "Emmanuel Cand\u00e8s"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.08994",
    "title": "Generalisable prediction model of surgical case duration: multicentre development and temporal validation",
    "abstract": "           Background: Accurate prediction of surgical case duration underpins operating room (OR) scheduling, yet existing models often depend on site- or surgeon-specific inputs and rarely undergo external validation, limiting generalisability. Methods: We undertook a retrospective multicentre study using routinely collected perioperative data from two general hospitals in Japan (development: 1 January 2021-31 December 2023; temporal test: 1 January-31 December 2024). Elective weekday procedures with American Society of Anesthesiologists (ASA) Physical Status 1-4 were included. Pre-specified preoperative predictors comprised surgical context (year, month, weekday, scheduled duration, general anaesthesia indicator, body position) and patient factors (sex, age, body mass index, allergy, infection, comorbidity, ASA). Missing data were addressed by multiple imputation by chained equations. Four learners (elastic-net, generalised additive models, random forest, gradient-boosted trees) were tuned within internal-external cross-validation (IECV; leave-one-cluster-out by centre-year) and combined by stacked generalisation to predict log-transformed duration. Results: We analysed 63,206 procedures (development 45,647; temporal test 17,559). Cluster-specific and pooled errors and calibrations from IECV are provided with consistent performance across centres and years. In the 2024 temporal test cohort, calibration was good (intercept 0.423, 95%CI 0.372 to 0.474; slope 0.921, 95%CI 0.911 to 0.932). Conclusions: A stacked machine-learning model using only widely available preoperative variables achieved accurate, well-calibrated predictions in temporal external validation, supporting transportability across sites and over time. Such general-purpose tools may improve OR scheduling without relying on idiosyncratic inputs.         ",
    "url": "https://arxiv.org/abs/2511.08994",
    "authors": [
      "Daijiro Kabata",
      "Mari Ito",
      "Tokito Koga",
      "Kazuma Yunoki"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09002",
    "title": "Convergence and Stability Analysis of Self-Consuming Generative Models with Heterogeneous Human Curation",
    "abstract": "           Self-consuming generative models have received significant attention over the last few years. In this paper, we study a self-consuming generative model with heterogeneous preferences that is a generalization of the model in Ferbach et al. (2024). The model is retrained round by round using real data and its previous-round synthetic outputs. The asymptotic behavior of the retraining dynamics is investigated across four regimes using different techniques including the nonlinear Perron--Frobenius theory. Our analyses improve upon that of Ferbach et al. (2024) and provide convergence results in settings where the well-known Banach contraction mapping arguments do not apply. Stability and non-stability results regarding the retraining dynamics are also given.         ",
    "url": "https://arxiv.org/abs/2511.09002",
    "authors": [
      "Hongru Zhao",
      "Jinwen Fu",
      "Tuan Pham"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09186",
    "title": "Scalable Mixed-Integer Optimization with Neural Constraints via Dual Decomposition",
    "abstract": "           Embedding deep neural networks (NNs) into mixed-integer programs (MIPs) is attractive for decision making with learned constraints, yet state-of-the-art monolithic linearisations blow up in size and quickly become intractable. In this paper, we introduce a novel dual-decomposition framework that relaxes the single coupling equality u=x with an augmented Lagrange multiplier and splits the problem into a vanilla MIP and a constrained NN block. Each part is tackled by the solver that suits it best-branch and cut for the MIP subproblem, first-order optimisation for the NN subproblem-so the model remains modular, the number of integer variables never grows with network depth, and the per-iteration cost scales only linearly with the NN size. On the public \\textsc{SurrogateLIB} benchmark, our method proves \\textbf{scalable}, \\textbf{modular}, and \\textbf{adaptable}: it runs \\(120\\times\\) faster than an exact Big-M formulation on the largest test case; the NN sub-solver can be swapped from a log-barrier interior step to a projected-gradient routine with no code changes and identical objective value; and swapping the MLP for an LSTM backbone still completes the full optimisation in 47s without any bespoke adaptation.         ",
    "url": "https://arxiv.org/abs/2511.09186",
    "authors": [
      "Shuli Zeng",
      "Sijia Zhang",
      "Feng Wu",
      "Shaojie Tang",
      "Xiang-Yang Li"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09242",
    "title": "Robust Least-Squares Optimization for Data-Driven Predictive Control: A Geometric Approach",
    "abstract": "           The paper studies a geometrically robust least-squares problem that extends classical and norm-based robust formulations. Rather than minimizing residual error for fixed or perturbed data, we interpret least-squares as enforcing approximate subspace inclusion between measured and true data spaces. The uncertainty in this geometric relation is modeled as a metric ball on the Grassmannian manifold, leading to a min-max problem over Euclidean and manifold variables. The inner maximization admits a closed-form solution, enabling an efficient algorithm with a transparent geometric interpretation. Applied to robust finite-horizon linear-quadratic tracking in data-enabled predictive control, the method improves upon existing robust least-squares formulations, achieving stronger robustness and favorable scaling under small uncertainty.         ",
    "url": "https://arxiv.org/abs/2511.09242",
    "authors": [
      "Shreyas Bharadwaj",
      "Bamdev Mishra",
      "Cyrus Mostajeran",
      "Alberto Padoan",
      "Jeremy Coulson",
      "Ravi N. Banavar"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09427",
    "title": "Adversarially and Distributionally Robust Virtual Energy Storage Systems via the Scenario Approach",
    "abstract": "           We propose an optimization model where a parking lot manager (PLM) can aggregate parked EV batteries to provide virtual energy storage services that are provably robust under uncertain EV departures and state-of-charge caps. Our formulation yields a data-driven convex optimization problem where a prosumer community agrees on a contract with the PLM for the provision of storage services over a finite horizon. Leveraging recent results in the scenario approach, we certify out-of-sample constraint safety. Furthermore, we enable a tunable profit-risk trade-off through scenario relaxation and extend our model to account for robustness to adversarial perturbations and distributional shifts over Wasserstein-based ambiguity sets. All the approaches are accompanied by tight finite-sample certificates. Numerical studies demonstrate the out-of-sample and out-of-distribution constraint satisfaction of our proposed model compared to the developed theoretical guarantees, showing their effectiveness and potential in robust and efficient virtual energy services.         ",
    "url": "https://arxiv.org/abs/2511.09427",
    "authors": [
      "Georgios Pantazis",
      "Nicola Mignoni",
      "Raffaele Carli",
      "Mariagrazia Dotoli",
      "Sergio Grammatico"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2104.13395",
    "title": "ACDC: The Adverse Conditions Dataset with Correspondences for Robust Semantic Driving Scene Perception",
    "abstract": "           Level-5 driving automation requires a robust visual perception system that can parse input images under any condition. However, existing driving datasets for dense semantic perception are either dominated by images captured under normal conditions or are small in scale. To address this, we introduce ACDC, the Adverse Conditions Dataset with Correspondences for training and testing methods for diverse semantic perception tasks on adverse visual conditions. ACDC consists of a large set of 8012 images, half of which (4006) are equally distributed between four common adverse conditions: fog, nighttime, rain, and snow. Each adverse-condition image comes with a high-quality pixel-level panoptic annotation, a corresponding image of the same scene under normal conditions, and a binary mask that distinguishes between intra-image regions of clear and uncertain semantic content. 1503 of the corresponding normal-condition images feature panoptic annotations, raising the total annotated images to 5509. ACDC supports the standard tasks of semantic segmentation, object detection, instance segmentation, and panoptic segmentation, as well as the newly introduced uncertainty-aware semantic segmentation. A detailed empirical study demonstrates the challenges that the adverse domains of ACDC pose to state-of-the-art supervised and unsupervised approaches and indicates the value of our dataset in steering future progress in the field. Our dataset and benchmark are publicly available at this https URL ",
    "url": "https://arxiv.org/abs/2104.13395",
    "authors": [
      "Christos Sakaridis",
      "Haoran Wang",
      "Ke Li",
      "Ren\u00e9 Zurbr\u00fcgg",
      "Arpit Jadon",
      "Wim Abbeloos",
      "Daniel Olmeda Reino",
      "Luc Van Gool",
      "Dengxin Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2107.06236",
    "title": "An FPT algorithm for the embeddability of graphs into two-dimensional simplicial complexes",
    "abstract": "           We consider the embeddability problem of a graph G into a two-dimensional simplicial complex C: Given G and C, decide whether G admits a topological embedding into C. The problem is NP-hard, even in the restricted case where C is homeomorphic to a surface. We prove that the problem is fixed-parameter tractable in the size of the two-dimensional complex, by providing an O(2^{poly(c)}.n^2)-time algorithm. If G embeds into C, we can compute a representation of an embedding in the same amount of time. Moreover, we show that several known problems reduce to this one, such as the crossing number and the planarity number problems, and, under some conditions, the embedding extension problem. Our approach is to reduce to the case where G has bounded branchwidth via an irrelevant vertex method, and to apply dynamic programming. We do not rely on any component of the existing linear-time algorithms for embedding graphs on a fixed surface, but only on algorithms from graph minor theory. However, by combining our results with a linear-time algorithm for embedding graphs on surfaces and with a very recent result for the irrelevant vertex method, we can decide whether G embeds into C in f(c).O(n) time, for some function f.         ",
    "url": "https://arxiv.org/abs/2107.06236",
    "authors": [
      "\u00c9ric Colin de Verdi\u00e8re",
      "Thomas Magnard"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2310.12017",
    "title": "Exploring the Adversarial Robustness of Face Forgery Detection with Decision-based Black-box Attacks",
    "abstract": "           Face forgery generation technologies generate vivid faces, which have raised public concerns about security and privacy. Many intelligent systems, such as electronic payment and identity verification, rely on face forgery detection. Although face forgery detection has successfully distinguished fake faces, recent studies have demonstrated that face forgery detectors are very vulnerable to adversarial examples. Meanwhile, existing attacks rely on network architectures or training datasets instead of the predicted labels, which leads to a gap in attacking deployed applications. To narrow this gap, we first explore the decision-based attacks on face forgery detection. We identify challenges in directly applying existing decision-based attacks, such as perturbation initialization failure and reduced image quality. To overcome these issues, we propose cross-task perturbation to handle initialization failures by utilizing the high correlation of face features on different tasks. Additionally, inspired by the use of frequency cues in face forgery detection, we introduce the frequency decision-based attack. This attack involves adding perturbations in the frequency domain while constraining visual quality in the spatial domain. Finally, extensive experiments demonstrate that our method achieves state-of-the-art attack performance on FaceForensics++, CelebDF, and industrial APIs, with high query efficiency and guaranteed image quality. Further, the fake faces by our method can pass face forgery detection and face recognition, which exposes the security problems of face forgery detectors.         ",
    "url": "https://arxiv.org/abs/2310.12017",
    "authors": [
      "Zhaoyu Chen",
      "Bo Li",
      "Kaixun Jiang",
      "Shuang Wu",
      "Shouhong Ding",
      "Wenqiang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2310.12815",
    "title": "Formalizing and Benchmarking Prompt Injection Attacks and Defenses",
    "abstract": "           A prompt injection attack aims to inject malicious instruction/data into the input of an LLM-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 LLMs and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses. To facilitate research on this topic, we make our platform public at this https URL.         ",
    "url": "https://arxiv.org/abs/2310.12815",
    "authors": [
      "Yupei Liu",
      "Yuqi Jia",
      "Runpeng Geng",
      "Jinyuan Jia",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.13393",
    "title": "Causal Graph Dynamics and Kan Extensions",
    "abstract": "           On the one side, the formalism of Global Transformations comes with the claim of capturing any transformation of space that is local, synchronous and deterministic. The claim has been proven for different classes of models such as mesh refinements from computer graphics, Lindenmayer systems from morphogenesis modeling and cellular automata from biological, physical and parallel computation modeling. The Global Transformation formalism achieves this by using category theory for its genericity, and more precisely the notion of Kan extension to determine the global behaviors based on the local ones. On the other side, Causal Graph Dynamics describe the transformation of port graphs in a synchronous and deterministic way and has not yet being tackled. In this paper, we show the precise sense in which the claim of Global Transformations holds for them as well. This is done by showing different ways in which they can be expressed as Kan extensions, each of them highlighting different features of Causal Graph Dynamics. Along the way, this work uncovers the interesting class of Monotonic Causal Graph Dynamics and their universality among General Causal Graph Dynamics.         ",
    "url": "https://arxiv.org/abs/2403.13393",
    "authors": [
      "Luidnel Maignan",
      "Antoine Spicher"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Discrete Mathematics (cs.DM)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2404.10776",
    "title": "Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback",
    "abstract": "           Learning from human feedback plays an important role in aligning generative models, such as large language models (LLM). However, the effectiveness of this approach can be influenced by adversaries, who may intentionally provide misleading preferences to manipulate the output in an undesirable or harmful direction. To tackle this challenge, we study a specific model within this problem domain--contextual dueling bandits with adversarial feedback, where the true preference label can be flipped by an adversary. We propose an algorithm, namely robust contextual dueling bandits, which is based on uncertainty-weighted maximum likelihood estimation. Our algorithm achieves an $\\tilde O(d\\sqrt{T}/\\kappa+dC/\\kappa)$ regret bound, where $T$ is the number of rounds, $d$ is the dimension of the context, $\\kappa$ is the lower bound of the derivative of the link function, and $ 0 \\le C \\le T$ is the total number of adversarial feedback. We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without ($C=0$) adversarial feedback. Our work is the first to achieve nearly minimax optimal regret for dueling bandits in the presence of adversarial preference feedback. Additionally, for the sigmoid link function, we develop a novel algorithm that takes into account the effect of local derivatives in maximum likelihood estimation (MLE) analysis through a refined method for estimating the link function's derivative. This method helps us to eliminate the $\\kappa$ dependence in the leading term with respect to $T$, which reduces the exponential dependence on the parameter radius $B$ to a polynomial dependence. We conduct experiments to evaluate our proposed algorithm against various types of adversarial feedback. Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback.         ",
    "url": "https://arxiv.org/abs/2404.10776",
    "authors": [
      "Qiwei Di",
      "Jiafan He",
      "Quanquan Gu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.19117",
    "title": "Coexistence of eMBB+ and mMTC+ in Uplink Cell-Free Massive MIMO Networks",
    "abstract": "           This paper tackles the problem of designing proper uplink multiple access schemes for coexistence between enhanced mobile broadband+ (eMBB+) users and massive machine-type communications+ (mMTC+) devices in a terminal-centric cell-free massive MIMO system. Specifically, the use of a time-frequency spreading technique for the mMTC+ devices has been proposed. Coupled with the assumption of imperfect channel knowledge, closed-form bounds of the achievable (ergodic) rate for the two data services are derived. Using suitable power control mechanisms, we show it is possible to efficiently multiplex eMBB+ and mMTC+ traffic in the same time-frequency resource grid. Numerical experiments reveal interesting trade-offs in the selection of the spreading gain and the number of serving access points within the system. Results also demonstrate that the performance of the mMTC+ devices is slightly affected by the presence of the eMBB+ users. Overall, our approach can endow good quality of service to both 6G cornerstones at once.         ",
    "url": "https://arxiv.org/abs/2404.19117",
    "authors": [
      "Sergi Liesegang",
      "Stefano Buzzi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2405.18770",
    "title": "Multimodal Adversarial Defense for Vision-Language Models by Leveraging One-To-Many Relationships",
    "abstract": "           Pre-trained vision-language (VL) models are highly vulnerable to adversarial attacks. However, existing defense methods primarily focus on image classification, overlooking two key aspects of VL tasks: multimodal attacks, where both image and text can be perturbed, and the one-to-many relationship of images and texts, where a single image can correspond to multiple textual descriptions and vice versa (1:N and N:1). This work is the first to explore defense strategies against multimodal attacks in VL tasks, whereas prior VL defense methods focus on vision robustness. We propose multimodal adversarial training (MAT), which incorporates adversarial perturbations in both image and text modalities during training, significantly outperforming existing unimodal defenses. Furthermore, we discover that MAT is limited by deterministic one-to-one (1:1) image-text pairs in VL training data. To address this, we conduct a comprehensive study on leveraging one-to-many relationships to enhance robustness, investigating diverse augmentation techniques. Our analysis shows that, for a more effective defense, augmented image-text pairs should be well-aligned, diverse, yet avoid distribution shift -- conditions overlooked by prior research. This work pioneers defense strategies against multimodal attacks, providing insights for building robust VLMs from both optimization and data perspectives.         ",
    "url": "https://arxiv.org/abs/2405.18770",
    "authors": [
      "Futa Waseda",
      "Antonio Tejero-de-Pablos",
      "Isao Echizen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2406.12163",
    "title": "Discussion Graph Semantics of First-Order Logic with Equality for Reasoning about Discussion and Argumentation",
    "abstract": "           We make three contributions. First, we formulate a discussion-graph semantics for first-order logic with equality, enabling reasoning about discussion and argumentation in AI more generally than before. This addresses the current lack of a formal reasoning framework capable of handling diverse discussion and argumentation models. Second, we generalise Dung's notion of extensions to cases where two or more graph nodes in an argumentation framework are equivalent. Third, we connect these two contributions by showing that the generalised extensions are first-order characterisable within the proposed discussion-graph semantics. Propositional characterisability of all Dung's extensions is an immediate consequence. We furthermore show that the set of all generalised extensions (acceptability semantics), too, are first-order characterisable. Propositional characterisability of all Dung's acceptability semantics is an immediate consequence.         ",
    "url": "https://arxiv.org/abs/2406.12163",
    "authors": [
      "Ryuta Arisaka"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2407.04326",
    "title": "LMSeg: An end-to-end geometric message-passing network on barycentric dual graphs for large-scale landscape mesh segmentation",
    "abstract": "           Semantic segmentation of large-scale 3D landscape meshes is critical for geospatial analysis in complex environments, yet existing approaches face persistent challenges of scalability, end-to-end trainability, and accurate segmentation of small and irregular objects. To address these issues, we introduce the BudjBim Wall (BBW) dataset, a large-scale annotated mesh dataset derived from high-resolution LiDAR scans of the UNESCO World Heritage-listed Budj Bim cultural landscape in Victoria, Australia. The BBW dataset captures historic dry-stone wall structures that are difficult to detect under vegetation occlusion, supporting research in underrepresented cultural heritage contexts. Building on this dataset, we propose LMSeg, a deep graph message-passing network for semantic segmentation of large-scale meshes. LMSeg employs a barycentric dual graph representation of mesh faces and introduces the Geometry Aggregation+ (GA+) module, a learnable softmax-based operator that adaptively combines neighborhood features and captures high-frequency geometric variations. A hierarchical-local dual pooling integrates hierarchical and local geometric aggregation to balance global context with fine-detail preservation. Experiments on three large-scale benchmarks (SUM, H3D, and BBW) show that LMSeg achieves 75.1% mIoU on SUM, 78.4% O.A. on H3D, and 62.4% mIoU on BBW, using only 2.4M lightweight parameters. In particular, LMSeg demonstrates accurate segmentation across both urban and natural scenes-capturing small-object classes such as vehicles and high vegetation in complex city environments, while also reliably detecting dry-stone walls in dense, occluded rural landscapes. Together, the BBW dataset and LMSeg provide a practical and extensible method for advancing 3D mesh segmentation in cultural heritage, environmental monitoring, and urban applications.         ",
    "url": "https://arxiv.org/abs/2407.04326",
    "authors": [
      "Zexian Huang",
      "Kourosh Khoshelham",
      "Martin Tomko"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.06849",
    "title": "TeVAE: A Variational Autoencoder Approach for Discrete Online Anomaly Detection in Variable-state Multivariate Time-series Data",
    "abstract": "           As attention to recorded data grows in the realm of automotive testing and manual evaluation reaches its limits, there is a growing need for automatic online anomaly detection. This real-world data is complex in many ways and requires the modelling of testee behaviour. To address this, we propose a temporal variational autoencoder (TeVAE) that can detect anomalies with minimal false positives when trained on unlabelled data. Our approach also avoids the bypass phenomenon and introduces a new method to remap individual windows to a continuous time series. Furthermore, we propose metrics to evaluate the detection delay and root-cause capability of our approach and present results from experiments on a real-world industrial data set. When properly configured, TeVAE flags anomalies only 6% of the time wrongly and detects 65% of anomalies present. It also has the potential to perform well with a smaller training and validation subset but requires a more sophisticated threshold estimation method.         ",
    "url": "https://arxiv.org/abs/2407.06849",
    "authors": [
      "Lucas Correia",
      "Jan-Christoph Goos",
      "Philipp Klein",
      "Thomas B\u00e4ck",
      "Anna V. Kononova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2408.05745",
    "title": "Improving Adversarial Transferability with Neighbourhood Gradient Information",
    "abstract": "           Deep neural networks (DNNs) are known to be susceptible to adversarial examples, leading to significant performance degradation. In black-box attack scenarios, a considerable attack performance gap between the surrogate model and the target model persists. This work focuses on enhancing the transferability of adversarial examples to narrow this performance gap. We observe that the gradient information around the clean image, i.e., Neighbourhood Gradient Information (NGI), can offer high this http URL on this insight, we introduce NGI-Attack, incorporating Example Backtracking and Multiplex Mask strategies to exploit this gradient information and enhance transferability. Specifically, we first adopt Example Backtracking to accumulate Neighbourhood Gradient Information as the initial momentum term. Then, we utilize Multiplex Mask to form a multi-way attack strategy that forces the network to focus on non-discriminative regions, which can obtain richer gradient information during only a few iterations. Extensive experiments demonstrate that our approach significantly enhances adversarial transferability. Especially, when attacking numerous defense models, we achieve an average attack success rate of 95.2%. Notably, our method can seamlessly integrate with any off-the-shelf algorithm, enhancing their attack performance without incurring extra time costs.         ",
    "url": "https://arxiv.org/abs/2408.05745",
    "authors": [
      "Haijing Guo",
      "Jiafeng Wang",
      "Zhaoyu Chen",
      "Kaixun Jiang",
      "Lingyi Hong",
      "Pinxue Guo",
      "Jinglun Li",
      "Wenqiang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2409.00449",
    "title": "LangPose: Language-Aligned Motion for Robust 3D Human Pose Estimation",
    "abstract": "           2D-to-3D human pose lifting is an ill-posed problem due to depth ambiguity and occlusion. Existing methods relying on spatial and temporal consistency alone are insufficient to resolve these problems especially in the presence of significant occlusions or high dynamic actions. Semantic information, however, offers a complementary signal that can help disambiguate such cases. To this end, we propose LangPose, a framework that leverages action knowledge by aligning motion embeddings with text embeddings of fine-grained action labels. LangPose operates in two stages: pretraining and fine-tuning. In the pretraining stage, the model simultaneously learns to recognize actions and reconstruct 3D poses from masked and noisy 2D poses. During the fine-tuning stage, the model is further refined using real-world 3D human pose estimation datasets without action labels. Additionally, our framework incorporates masked body parts and masked time windows in motion modeling, encouraging the model to leverage semantic information when spatial and temporal consistency is unreliable. Experiments demonstrate the effectiveness of LangPose, achieving SOTA level performance in 3D pose estimation on public datasets, including Human3.6M and MPI-INF-3DHP. Specifically, LangPose achieves an MPJPE of 36.7mm on Human3.6M with detected 2D poses as input and 15.5mm on MPI-INF-3DHP with ground-truth 2D poses as input.         ",
    "url": "https://arxiv.org/abs/2409.00449",
    "authors": [
      "Longyun Liao",
      "Rong Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.06998",
    "title": "Mixture of Scope Experts at Test: Generalizing Deeper Graph Neural Networks with Shallow Variants",
    "abstract": "           Heterophilous graphs, where dissimilar nodes tend to connect, pose a challenge for graph neural networks (GNNs). Increasing the GNN depth can expand the scope (i.e., receptive field), potentially finding homophily from the higher-order neighborhoods. However, GNNs suffer from performance degradation as depth increases. Despite having better expressivity, state-of-the-art deeper GNNs achieve only marginal improvements compared to their shallow variants. Through theoretical and empirical analysis, we systematically demonstrate a shift in GNN generalization preferences across nodes with different homophily levels as depth increases. This creates a disparity in generalization patterns between GNN models with varying depth. Based on these findings, we propose to improve deeper GNN generalization while maintaining high expressivity by Mixture of scope experts at test (Moscat). Experimental results show that Moscat works flexibly with various GNNs across a wide range of datasets while significantly improving accuracy. Our code is available at (this https URL).         ",
    "url": "https://arxiv.org/abs/2409.06998",
    "authors": [
      "Gangda Deng",
      "Hongkuan Zhou",
      "Rajgopal Kannan",
      "Viktor Prasanna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2410.16100",
    "title": "ExDBN: Learning Dynamic Bayesian Networks using Extended Mixed-Integer Programming Formulations",
    "abstract": "           Causal learning from data has received much attention recently. Bayesian networks can be used to capture causal relationships. There, one recovers a weighted directed acyclic graph in which random variables are represented by vertices, and the weights associated with each edge represent the strengths of the causal relationships between them. This concept is extended to capture dynamic effects by introducing a dependency on past data, which may be captured by the structural equation model. This formalism is utilized in the present contribution to propose a score-based learning algorithm. A mixed-integer quadratic program is formulated and an algorithmic solution proposed, in which the pre-generation of exponentially many acyclicity constraints is avoided by utilizing the so-called branch-and-cut (``lazy constraint'') method. Comparing the novel approach to the state-of-the-art, we show that the proposed approach turns out to produce more accurate results when applied to small and medium-sized synthetic instances containing up to 80 time series. Lastly, two interesting applications in bioscience and finance, to which the method is directly applied, further stress the importance of developing highly accurate, globally convergent solvers that can handle instances of modest size.         ",
    "url": "https://arxiv.org/abs/2410.16100",
    "authors": [
      "Pavel Rytir",
      "Ales Wodecki",
      "Georgios Korpas",
      "Jakub Marecek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.15206",
    "title": "Conditional Distribution Learning for Graph Classification",
    "abstract": "           Leveraging the diversity and quantity of data provided by various graph-structured data augmentations while preserving intrinsic semantic information is challenging. Additionally, successive layers in graph neural network (GNN) tend to produce more similar node embeddings, while graph contrastive learning aims to increase the dissimilarity between negative pairs of node embeddings. This inevitably results in a conflict between the message-passing mechanism (MPM) of GNNs and the contrastive learning (CL) of negative pairs via intraviews. In this paper, we propose a conditional distribution learning (CDL) method that learns graph representations from graph-structured data for semisupervised graph classification. Specifically, we present an end-to-end graph representation learning model to align the conditional distributions of weakly and strongly augmented features over the original features. This alignment enables the CDL model to effectively preserve intrinsic semantic information when both weak and strong augmentations are applied to graph-structured data. To avoid the conflict between the MPM and the CL of negative pairs, positive pairs of node representations are retained for measuring the similarity between the original features and the corresponding weakly augmented features. Extensive experiments with several benchmark graph datasets demonstrate the effectiveness of the proposed CDL method.         ",
    "url": "https://arxiv.org/abs/2411.15206",
    "authors": [
      "Jie Chen",
      "Hua Mao",
      "Chuanbin Liu",
      "Zhu Wang",
      "Xi Peng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.16754",
    "title": "The Visual Counter Turing Test (VCT2): A Benchmark for Evaluating AI-Generated Image Detection and the Visual AI Index (VAI)",
    "abstract": "           The rapid progress and widespread availability of text-to-image (T2I) generative models have heightened concerns about the misuse of AI-generated visuals, particularly in the context of misinformation campaigns. Existing AI-generated image detection (AGID) methods often overfit to known generators and falter on outputs from newer or unseen models. We introduce the Visual Counter Turing Test (VCT2), a comprehensive benchmark of 166,000 images, comprising both real and synthetic prompt-image pairs produced by six state-of-the-art T2I systems: Stable Diffusion 2.1, SDXL, SD3 Medium, SD3.5 Large, DALL.E 3, and Midjourney 6. We curate two distinct subsets: COCOAI, featuring structured captions from MS COCO, and TwitterAI, containing narrative-style tweets from The New York Times. Under a unified zero-shot evaluation, we benchmark 17 leading AGID models and observe alarmingly low detection accuracy, 58% on COCOAI and 58.34% on TwitterAI. To transcend binary classification, we propose the Visual AI Index (VAI), an interpretable, prompt-agnostic realism metric based on twelve low-level visual features, enabling us to quantify and rank the perceptual quality of generated outputs with greater nuance. Correlation analysis reveals a moderate inverse relationship between VAI and detection accuracy: Pearson of -0.532 on COCOAI and -0.503 on TwitterAI, suggesting that more visually realistic images tend to be harder to detect, a trend observed consistently across generators. We release COCOAI, TwitterAI, and all codes to catalyze future advances in generalized AGID and perceptual realism assessment.         ",
    "url": "https://arxiv.org/abs/2411.16754",
    "authors": [
      "Nasrin Imanpour",
      "Abhilekh Borah",
      "Shashwat Bajpai",
      "Subhankar Ghosh",
      "Sainath Reddy Sankepally",
      "Hasnat Md Abdullah",
      "Nishoak Kosaraju",
      "Shreyas Dixit",
      "Ashhar Aziz",
      "Shwetangshu Biswas",
      "Vinija Jain",
      "Aman Chadha",
      "Song Wang",
      "Amit Sheth",
      "Amitava Das"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.18235",
    "title": "Certified Training with Branch-and-Bound for Lyapunov-stable Neural Control",
    "abstract": "           We study the problem of learning verifiably Lyapunov-stable neural controllers that provably satisfy the Lyapunov asymptotic stability condition within a region-of-attraction (ROA). Unlike previous works that adopted counterexample-guided training without considering the computation of verification in training, we introduce Certified Training with Branch-and-Bound (CT-BaB), a new certified training framework that optimizes certified bounds, thereby reducing the discrepancy between training and test-time verification that also computes certified bounds. To achieve a relatively global guarantee on an entire input region-of-interest, we propose a training-time BaB technique that maintains a dynamic training dataset and adaptively splits hard input subregions into smaller ones, to tighten certified bounds and ease the training. Meanwhile, subregions created by the training-time BaB also inform test-time verification, for a more efficient training-aware verification. We demonstrate that CT-BaB yields verification-friendly models that can be more efficiently verified at test time while achieving stronger verifiable guarantees with larger ROA. On the largest output-feedback 2D Quadrotor system experimented, CT-BaB reduces verification time by over 11X relative to the previous state-of-the-art baseline while achieving 164X larger ROA.         ",
    "url": "https://arxiv.org/abs/2411.18235",
    "authors": [
      "Zhouxing Shi",
      "Haoyu Li",
      "Cho-Jui Hsieh",
      "Huan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.19461",
    "title": "Robust Bayesian Scene Reconstruction with Retrieval-Augmented Priors for Precise Grasping and Planning",
    "abstract": "           Constructing 3D representations of object geometry is critical for many robotics tasks, particularly manipulation problems. These representations must be built from potentially noisy partial observations. In this work, we focus on the problem of reconstructing a multi-object scene from a single RGBD image using a fixed camera. Traditional scene representation methods generally cannot infer the geometry of unobserved regions of the objects in the image. Attempts have been made to leverage deep learning to train on a dataset of known objects and representations, and then generalize to new observations. However, this can be brittle to noisy real-world observations and objects not contained in the dataset, and do not provide well-calibrated reconstruction confidences. We propose BRRP, a reconstruction method that leverages preexisting mesh datasets to build an informative prior during robust probabilistic reconstruction. We introduce the concept of a retrieval-augmented prior, where we retrieve relevant components of our prior distribution from a database of objects during inference. The resulting prior enables estimation of the geometry of occluded portions of the in-scene objects. Our method produces a distribution over object shape that can be used for reconstruction and measuring uncertainty. We evaluate our method in both simulated scenes and in the real world. We demonstrate the robustness of our method against deep learning-only approaches while being more accurate than a method without an informative prior. Through real-world experiments, we particularly highlight the capability of BRRP to enable successful dexterous manipulation in clutter.         ",
    "url": "https://arxiv.org/abs/2411.19461",
    "authors": [
      "Herbert Wright",
      "Weiming Zhi",
      "Martin Matak",
      "Matthew Johnson-Roberson",
      "Tucker Hermans"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2412.02054",
    "title": "Redundant Queries in DETR-Based 3D Detection Methods: Unnecessary and Prunable",
    "abstract": "           Query-based models are extensively used in 3D object detection tasks, with a wide range of pre-trained checkpoints readily available online. However, despite their popularity, these models often require an excessive number of object queries, far surpassing the actual number of objects to detect. The redundant queries result in unnecessary computational and memory costs. In this paper, we find that not all queries contribute equally -- a significant portion of queries have a much smaller impact compared to others. Based on this observation, we propose an embarrassingly simple approach called \\bd{G}radually \\bd{P}runing \\bd{Q}ueries (GPQ), which prunes queries incrementally based on their classification scores. It is straightforward to implement in any query-based method, as it can be seamlessly integrated as a fine-tuning step using an existing checkpoint after training. With GPQ, users can easily generate multiple models with fewer queries, starting from a checkpoint with an excessive number of queries. Experiments on various advanced 3D detectors show that GPQ effectively reduces redundant queries while maintaining performance. Using our method, model inference on desktop GPUs can be accelerated by up to 1.31x. Moreover, after deployment on edge devices, it achieves up to a 67.86\\% reduction in FLOPs and a 76.38\\% decrease in inference time. The code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.02054",
    "authors": [
      "Lizhen Xu",
      "Zehao Wu",
      "Wenzhao Qiu",
      "Shanmin Pang",
      "Xiuxiu Bai",
      "Kuizhi Mei",
      "Jianru Xue"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.04697",
    "title": "Privacy-Preserving Retrieval-Augmented Generation with Differential Privacy",
    "abstract": "           With the recent remarkable advancement of large language models (LLMs), there has been a growing interest in utilizing them in the domains with highly sensitive data that lies outside their training data. For this purpose, retrieval-augmented generation (RAG) is particularly effective -- it assists LLMs by directly providing relevant information from the external knowledge sources. However, without extra privacy safeguards, RAG outputs risk leaking sensitive information from the external data source. In this work, we explore RAG under differential privacy (DP), a formal guarantee of data privacy. The main challenge with differentially private RAG is how to generate long accurate answers within a moderate privacy budget. We address this by proposing an algorithm that smartly spends privacy budget only for the tokens that require the sensitive information and uses the non-private LLM for other tokens. Our extensive empirical evaluations reveal that our algorithm outperforms the non-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$ across different models and datasets.         ",
    "url": "https://arxiv.org/abs/2412.04697",
    "authors": [
      "Tatsuki Koga",
      "Ruihan Wu",
      "Zhiyuan Zhang",
      "Kamalika Chaudhuri"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2412.06819",
    "title": "A Physics-Constrained Neural Differential Equation Framework for Data-Driven Snowpack Simulation",
    "abstract": "           This paper presents a physics-constrained neural differential equation framework for parameterization, and employs it to model the time evolution of seasonal snow depth given hydrometeorological forcings. When trained on data from multiple SNOTEL sites, the parameterization predicts daily snow depth with under 9% median error and Nash Sutcliffe Efficiencies over 0.94 across a wide variety of snow climates. The parameterization also generalizes to new sites not seen during training, which is not often true for calibrated snow models. Requiring the parameterization to predict snow water equivalent in addition to snow depth only increases error to ~12%. The structure of the approach guarantees the satisfaction of physical constraints, enables these constraints during model training, and allows modeling at different temporal resolutions without additional retraining of the parameterization. These benefits hold potential in climate modeling, and could extend to other dynamical systems with physical constraints.         ",
    "url": "https://arxiv.org/abs/2412.06819",
    "authors": [
      "Andrew Charbonneau",
      "Katherine Deck",
      "Tapio Schneider"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2412.13918",
    "title": "Localized RETE for Incremental Graph Queries with Nested Graph Conditions",
    "abstract": "           The growing size of graph-based modeling artifacts in model-driven engineering calls for techniques that enable efficient execution of graph queries. Incremental approaches based on the RETE algorithm provide an adequate solution in many scenarios, but are generally designed to search for query results over the entire graph. However, in certain situations, a user may only be interested in query results for a subgraph, for instance when a developer is working on a large model of which only a part is loaded into their workspace. In this case, the global execution semantics can result in significant computational overhead. To mitigate the outlined shortcoming, in this article we propose an extension of the RETE approach that enables local, yet fully incremental execution of graph queries, while still guaranteeing completeness of results with respect to the relevant subgraph. We empirically evaluate the presented approach via experiments inspired by a scenario from software development and with queries and data from an independent social network benchmark. The experimental results indicate that the proposed technique can significantly improve performance regarding memory consumption and execution time in favorable cases, but may incur a noticeable overhead in unfavorable cases.         ",
    "url": "https://arxiv.org/abs/2412.13918",
    "authors": [
      "Matthias Barkowsky",
      "Holger Giese"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2501.14230",
    "title": "GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm",
    "abstract": "           Deep neural networks are highly vulnerable to adversarial examples, which are inputs with small, carefully crafted perturbations that cause misclassification--making adversarial attacks a critical tool for evaluating robustness. Existing black-box methods typically entail a trade-off between precision and flexibility: pixel-sparse attacks (e.g., single- or few-pixel attacks) provide fine-grained control but lack adaptability, whereas patch- or frequency-based attacks improve efficiency or transferability, but at the cost of producing larger and less precise perturbations. We present GreedyPixel, a fine-grained black-box attack method that performs brute-force-style, per-pixel greedy optimization guided by a surrogate-derived priority map and refined by means of query feedback. It evaluates each coordinate directly without any gradient information, guaranteeing monotonic loss reduction and convergence to a coordinate-wise optimum, while also yielding near white-box-level precision and pixel-wise sparsity and perceptual quality. On the CIFAR-10 and ImageNet datasets, spanning convolutional neural networks (CNNs) and Transformer models, GreedyPixel achieved state-of-the-art success rates with visually imperceptible perturbations, effectively bridging the gap between black-box practicality and white-box performance. The implementation is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2501.14230",
    "authors": [
      "Hanrui Wang",
      "Ching-Chun Chang",
      "Chun-Shien Lu",
      "Christopher Leckie",
      "Isao Echizen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.15282",
    "title": "AutoG: Towards automatic graph construction from tabular data",
    "abstract": "           Recent years have witnessed significant advancements in graph machine learning (GML), with its applications spanning numerous domains. However, the focus of GML has predominantly been on developing powerful models, often overlooking a crucial initial step: constructing suitable graphs from common data formats, such as tabular data. This construction process is fundamental to applying graph-based models, yet it remains largely understudied and lacks formalization. Our research aims to address this gap by formalizing the graph construction problem and proposing an effective solution. We identify two critical challenges to achieve this goal: 1. The absence of dedicated datasets to formalize and evaluate the effectiveness of graph construction methods, and 2. Existing automatic construction methods can only be applied to some specific cases, while tedious human engineering is required to generate high-quality graphs. To tackle these challenges, we present a two-fold contribution. First, we introduce a set of datasets to formalize and evaluate graph construction methods. Second, we propose an LLM-based solution, AutoG, automatically generating high-quality graph schemas without human intervention. The experimental results demonstrate that the quality of constructed graphs is critical to downstream task performance, and AutoG can generate high-quality graphs that rival those produced by human experts. Our code can be accessible from this https URL.         ",
    "url": "https://arxiv.org/abs/2501.15282",
    "authors": [
      "Zhikai Chen",
      "Han Xie",
      "Jian Zhang",
      "Xiang song",
      "Jiliang Tang",
      "Huzefa Rangwala",
      "George Karypis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.05109",
    "title": "Graph Contrastive Learning for Connectome Classification",
    "abstract": "           With recent advancements in non-invasive techniques for measuring brain activity, such as magnetic resonance imaging (MRI), the study of structural and functional brain networks through graph signal processing (GSP) has gained notable prominence. GSP stands as a key tool in unraveling the interplay between the brain's function and structure, enabling the analysis of graphs defined by the connections between regions of interest -- referred to as connectomes in this context. Our work represents a further step in this direction by exploring supervised contrastive learning methods within the realm of graph representation learning. The main objective of this approach is to generate subject-level (i.e., graph-level) vector representations that bring together subjects sharing the same label while separating those with different labels. These connectome embeddings are derived from a graph neural network Encoder-Decoder architecture, which jointly considers structural and functional connectivity. By leveraging data augmentation techniques, the proposed framework achieves state-of-the-art performance in a gender classification task using Human Connectome Project data. More broadly, our connectome-centric methodological advances support the promising prospect of using GSP to discover more about brain function, with potential impact to understanding heterogeneity in the neurodegeneration for precision medicine and diagnosis.         ",
    "url": "https://arxiv.org/abs/2502.05109",
    "authors": [
      "Mart\u00edn Schmidt",
      "Sara Silva",
      "Federico Larroca",
      "Gonzalo Mateos",
      "Pablo Mus\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.12003",
    "title": "Improved Wildfire Spread Prediction with Time-Series Data and the WSTS+ Benchmark",
    "abstract": "           Recent research has demonstrated the potential of deep neural networks (DNNs) to accurately predict wildfire spread on a given day based upon high-dimensional explanatory data from a single preceding day, or from a time series of T preceding days. For the first time, we investigate a large number of existing data-driven wildfire modeling strategies under controlled conditions, revealing the best modeling strategies and resulting in models that achieve state-of-the-art (SOTA) accuracy for both single-day and multi-day input scenarios, as evaluated on a large public benchmark for next-day wildfire spread, termed the WildfireSpreadTS (WSTS) benchmark. Consistent with prior work, we found that models using time-series input obtained the best overall accuracy, suggesting this is an important future area of research. Furthermore, we create a new benchmark, WSTS+, by incorporating four additional years of historical wildfire data into the WSTS benchmark. Our benchmark doubles the number of unique years of historical data, expands its geographic scope, and, to our knowledge, represents the largest public benchmark for time-series-based wildfire spread prediction.         ",
    "url": "https://arxiv.org/abs/2502.12003",
    "authors": [
      "Saad Lahrichi",
      "Jake Bova",
      "Jesse Johnson",
      "Jordan Malof"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.14389",
    "title": "Leveraging Small LLMs for Argument Mining in Education: Argument Component Identification, Classification, and Assessment",
    "abstract": "           Argument mining algorithms analyze the argumentative structure of essays, making them a valuable tool for enhancing education by providing targeted feedback on the students' argumentation skills. While current methods often use encoder or encoder-decoder deep learning architectures, decoder-only models remain largely unexplored, offering a promising research direction. This paper proposes leveraging open-source, small Large Language Models (LLMs) for argument mining through few-shot prompting and fine-tuning. These models' small size and open-source nature ensure accessibility, privacy, and computational efficiency, enabling schools and educators to adopt and deploy them locally. Specifically, we perform three tasks: segmentation of student essays into arguments, classification of the arguments by type, and assessment of their quality. We empirically evaluate the models on the Feedback Prize - Predicting Effective Arguments dataset of grade 6-12 students essays and demonstrate how fine-tuned small LLMs outperform baseline methods in segmenting the essays and determining the argument types while few-shot prompting yields comparable performance to that of the baselines in assessing quality. This work highlights the educational potential of small, open-source LLMs to provide real-time, personalized feedback, enhancing independent learning and writing skills while ensuring low computational cost and privacy.         ",
    "url": "https://arxiv.org/abs/2502.14389",
    "authors": [
      "Lucile Favero",
      "Juan Antonio P\u00e9rez-Ortiz",
      "Tanja K\u00e4ser",
      "Nuria Oliver"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.01783",
    "title": "vS-Graphs: Tightly Coupling Visual SLAM and 3D Scene Graphs Exploiting Hierarchical Scene Understanding",
    "abstract": "           Current Visual Simultaneous Localization and Mapping (VSLAM) systems often struggle to create maps that are both semantically rich and easily interpretable. While incorporating semantic scene knowledge aids in building richer maps with contextual associations among mapped objects, representing them in structured formats, such as scene graphs, has not been widely addressed, resulting in complex map comprehension and limited scalability. This paper introduces vS-Graphs, a novel real-time VSLAM framework that integrates vision-based scene understanding with map reconstruction and comprehensible graph-based representation. The framework infers structural elements (i.e., rooms and floors) from detected building components (i.e., walls and ground surfaces) and incorporates them into optimizable 3D scene graphs. This solution enhances the reconstructed map's semantic richness, comprehensibility, and localization accuracy. Extensive experiments on standard benchmarks and real-world datasets demonstrate that vS-Graphs achieves an average of 15.22% accuracy gain across all tested datasets compared to state-of-the-art VSLAM methods. Furthermore, the proposed framework achieves environment-driven semantic entity detection accuracy comparable to that of precise LiDAR-based frameworks, using only visual features. The code is publicly available at this https URL and is actively being improved. Moreover, a web page containing more media and evaluation outcomes is available on this https URL.         ",
    "url": "https://arxiv.org/abs/2503.01783",
    "authors": [
      "Ali Tourani",
      "Saad Ejaz",
      "Hriday Bavle",
      "Miguel Fernandez-Cortizas",
      "David Morilla-Cabello",
      "Jose Luis Sanchez-Lopez",
      "Holger Voos"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.07199",
    "title": "How Well Can Differential Privacy Be Audited in One Run?",
    "abstract": "           Recent methods for auditing the privacy of machine learning algorithms have improved computational efficiency by simultaneously intervening on multiple training examples in a single training run. Steinke et al. (2024) prove that one-run auditing indeed lower bounds the true privacy parameter of the audited algorithm, and give impressive empirical results. Their work leaves open the question of how precisely one-run auditing can uncover the true privacy parameter of an algorithm, and how that precision depends on the audited algorithm. In this work, we characterize the maximum achievable efficacy of one-run auditing and show that the key barrier to its efficacy is interference between the observable effects of different data elements. We present new conceptual approaches to minimize this barrier, towards improving the performance of one-run auditing of real machine learning algorithms.         ",
    "url": "https://arxiv.org/abs/2503.07199",
    "authors": [
      "Amit Keinan",
      "Moshe Shenfeld",
      "Katrina Ligett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.19037",
    "title": "Evolutionary Policy Optimization",
    "abstract": "           On-policy reinforcement learning (RL) algorithms are widely used for their strong asymptotic performance and training stability, but they struggle to scale with larger batch sizes, as additional parallel environments yield redundant data due to limited policy-induced diversity. In contrast, Evolutionary Algorithms (EAs) scale naturally and encourage exploration via randomized population-based search, but are often sample-inefficient. We propose Evolutionary Policy Optimization (EPO), a hybrid algorithm that combines the scalability and diversity of EAs with the performance and stability of policy gradients. EPO maintains a population of agents conditioned on latent variables, shares actor-critic network parameters for coherence and memory efficiency, and aggregates diverse experiences into a master agent. Across tasks in dexterous manipulation, legged locomotion, and classic control, EPO outperforms state-of-the-art baselines in sample efficiency, asymptotic performance, and scalability.         ",
    "url": "https://arxiv.org/abs/2503.19037",
    "authors": [
      "Jianren Wang",
      "Yifan Su",
      "Abhinav Gupta",
      "Deepak Pathak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.22454",
    "title": "A Causal Framework to Measure and Mitigate Non-binary Treatment Discrimination",
    "abstract": "           Fairness studies of algorithmic decision-making systems often simplify complex decision processes, such as bail or loan approvals, into binary classification tasks. However, these approaches overlook that such decisions are not inherently binary (e.g., approve or not approve bail or loan); they also involve non-binary treatment decisions (e.g., bail conditions or loan terms) that can influence the downstream outcomes (e.g., loan repayment or reoffending). In this paper, we argue that non-binary treatment decisions are integral to the decision process and controlled by decision-makers and, therefore, should be central to fairness analyses in algorithmic decision-making. We propose a causal framework that extends fairness analyses and explicitly distinguishes between decision-subjects' covariates and the treatment decisions. This specification allows decision-makers to use our framework to (i) measure treatment disparity and its downstream effects in historical data and, using counterfactual reasoning, (ii) mitigate the impact of past unfair treatment decisions when automating decision-making. We use our framework to empirically analyze four widely used loan approval datasets to reveal potential disparity in non-binary treatment decisions and their discriminatory impact on outcomes, highlighting the need to incorporate treatment decisions in fairness assessments. Moreover, by intervening in treatment decisions, we show that our framework effectively mitigates treatment discrimination from historical data to ensure fair risk score estimation and (non-binary) decision-making processes that benefit all stakeholders.         ",
    "url": "https://arxiv.org/abs/2503.22454",
    "authors": [
      "Ayan Majumdar",
      "Deborah D. Kanubala",
      "Kavya Gupta",
      "Isabel Valera"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.00758",
    "title": "TAMIS: Tailored Membership Inference Attacks on Synthetic Data",
    "abstract": "           Membership Inference Attacks (MIA) enable to empirically assess the privacy of a machine learning algorithm. In this paper, we propose TAMIS, a novel MIA against differentially-private synthetic data generation methods that rely on graphical models. This attack builds upon MAMA-MIA, a recently-published state-of-the-art method. It lowers its computational cost and requires less attacker knowledge. Our attack is the product of a two-fold improvement. First, we recover the graphical model having generated a synthetic dataset by using solely that dataset, rather than shadow-modeling over an auxiliary one. This proves less costly and more performant. Second, we introduce a more mathematically-grounded attack score, that provides a natural threshold for binary predictions. In our experiments, TAMIS achieves better or similar performance as MAMA-MIA on replicas of the SNAKE challenge.         ",
    "url": "https://arxiv.org/abs/2504.00758",
    "authors": [
      "Paul Andrey",
      "Batiste Le Bars",
      "Marc Tommasi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2504.03406",
    "title": "Rapid Mixing on Random Regular Graphs beyond Uniqueness",
    "abstract": "           The hardcore model is a fundamental probabilistic model extensively studied in statistical physics, probability theory, and computer science. For graphs of maximum degree $\\Delta$, a well-known computational phase transition occurs at the tree-uniqueness threshold $\\lambda_c(\\Delta) = \\frac{(\\Delta-1)^{\\Delta-1}}{(\\Delta-2)^\\Delta}$, where the mixing behavior of the Glauber dynamics (a simple Markov chain) undergoes a sharp transition. It is conjectured that random regular graphs exhibit different mixing behavior, with the slowdown occurring far beyond the uniqueness threshold. We confirm this conjecture by showing that, for the hardcore model on random $\\Delta$-regular graphs, the Glauber dynamics mixes rapidly with high probability when $\\lambda = O(1/\\sqrt{\\Delta})$, which is significantly beyond the uniqueness threshold $\\lambda_c(\\Delta) \\approx e/\\Delta$. Our result establishes a sharp distinction between the hardcore model on worst-case and beyond-worst-case instances, showing that the worst-case and average-case complexities of sampling and counting are fundamentally different. This result of rapid mixing on random instances follows from a new criterion we establish for rapid mixing of Glauber dynamics for any distribution supported on a downward closed set family. Our criterion is simple, general, and easy to check. In addition to proving new mixing conditions for the hardcore model, we also establish improved mixing time bounds for sampling uniform matchings or $b$ matchings on graphs, the random cluster model on matroids with $q \\in [0,1)$, and the determinantal point process. Our proof of this new criterion for rapid mixing combines and generalizes several recent tools in a novel way, including a trickle down theorem for field dynamics, spectral/entropic stability, and a new comparison result between field dynamics and Glauber dynamics.         ",
    "url": "https://arxiv.org/abs/2504.03406",
    "authors": [
      "Xiaoyu Chen",
      "Zejia Chen",
      "Zongchen Chen",
      "Yitong Yin",
      "Xinyuan Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2504.05478",
    "title": "GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph Databases",
    "abstract": "           Large language models have shown remarkable language processing and reasoning ability but are prone to hallucinate when asked about private data. Retrieval-augmented generation (RAG) retrieves relevant data that fit into an LLM's context window and prompts the LLM for an answer. GraphRAG extends this approach to structured Knowledge Graphs (KGs) and questions regarding entities multiple hops away. The majority of recent GraphRAG methods either overlook the retrieval step or have ad hoc retrieval processes that are abstract or inefficient. This prevents them from being adopted when the KGs are stored in graph databases supporting graph query languages. In this work, we present GraphRAFT, a retrieve-and-reason framework that finetunes LLMs to generate provably correct Cypher queries to retrieve high-quality subgraph contexts and produce accurate answers. Our method is the first such solution that can be taken off-the-shelf and used on KGs stored in native graph DBs. Benchmarks suggest that our method is sample-efficient and scales with the availability of training data. Our method achieves significantly better results than all state-of-the-art models across all four standard metrics on two challenging Q&As on large text-attributed KGs.         ",
    "url": "https://arxiv.org/abs/2504.05478",
    "authors": [
      "Alfred Clemedtson",
      "Borun Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2504.09185",
    "title": "Repetitive Contrastive Learning Enhances Mamba's Selectivity in Time Series Prediction",
    "abstract": "           Long sequence prediction is a key challenge in time series forecasting. While Mamba-based models have shown strong performance due to their sequence selection capabilities, they still struggle with insufficient focus on critical time steps and incomplete noise suppression, caused by limited selective abilities. To address this, we introduce Repetitive Contrastive Learning (RCL), a token-level contrastive pretraining framework aimed at enhancing Mamba's selective capabilities. RCL pretrains a single Mamba block to strengthen its selective abilities and then transfers these pretrained parameters to initialize Mamba blocks in various backbone models, improving their temporal prediction performance. RCL uses sequence augmentation with Gaussian noise and applies inter-sequence and intra-sequence contrastive learning to help the Mamba module prioritize information-rich time steps while ignoring noisy ones. Extensive experiments show that RCL consistently boosts the performance of backbone models, surpassing existing methods and achieving state-of-the-art results. Additionally, we propose two metrics to quantify Mamba's selective capabilities, providing theoretical, qualitative, and quantitative evidence for the improvements brought by RCL.         ",
    "url": "https://arxiv.org/abs/2504.09185",
    "authors": [
      "Wenbo Yan",
      "Hanzhong Cao",
      "Ying Tan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.11358",
    "title": "DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks",
    "abstract": "           LLM-integrated applications and agents are vulnerable to prompt injection attacks, where an attacker injects prompts into their inputs to induce attacker-desired outputs. A detection method aims to determine whether a given input is contaminated by an injected prompt. However, existing detection methods have limited effectiveness against state-of-the-art attacks, let alone adaptive ones. In this work, we propose DataSentinel, a game-theoretic method to detect prompt injection attacks. Specifically, DataSentinel fine-tunes an LLM to detect inputs contaminated with injected prompts that are strategically adapted to evade detection. We formulate this as a minimax optimization problem, with the objective of fine-tuning the LLM to detect strong adaptive attacks. Furthermore, we propose a gradient-based method to solve the minimax optimization problem by alternating between the inner max and outer min problems. Our evaluation results on multiple benchmark datasets and LLMs show that DataSentinel effectively detects both existing and adaptive prompt injection attacks.         ",
    "url": "https://arxiv.org/abs/2504.11358",
    "authors": [
      "Yupei Liu",
      "Yuqi Jia",
      "Jinyuan Jia",
      "Dawn Song",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.13713",
    "title": "SLAM&Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM",
    "abstract": "           Models and methods originally developed for Novel View Synthesis and Scene Rendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are increasingly being adopted as representations in Simultaneous Localization and Mapping (SLAM). However, existing datasets fail to include the specific challenges of both fields, such as sequential operations and, in many settings, multi-modality in SLAM or generalization across viewpoints and illumination conditions in neural rendering. Additionally, the data are often collected using sensors which are handheld or mounted on drones or mobile robots, which complicates the accurate reproduction of sensor motions. To bridge these gaps, we introduce SLAM&Render, a novel dataset designed to benchmark methods in the intersection between SLAM, Novel View Rendering and Gaussian Splatting. Recorded with a robot manipulator, it uniquely includes 40 sequences with time-synchronized RGB-D images, IMU readings, robot kinematic data, and ground-truth pose streams. By releasing robot kinematic data, the dataset also enables the assessment of recent integrations of SLAM paradigms within robotic applications. The dataset features five setups with consumer and industrial objects under four controlled lighting conditions, each with separate training and test trajectories. All sequences are static with different levels of object rearrangements and occlusions. Our experimental results, obtained with several baselines from the literature, validate SLAM&Render as a relevant benchmark for this emerging research area.         ",
    "url": "https://arxiv.org/abs/2504.13713",
    "authors": [
      "Samuel Cerezo",
      "Gaetano Meli",
      "Tom\u00e1s Berriel Martins",
      "Kirill Safronov",
      "Javier Civera"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.18906",
    "title": "Sim-to-Real: An Unsupervised Noise Layer for Screen-Camera Watermarking Robustness",
    "abstract": "           Unauthorized screen capturing and dissemination pose severe security threats such as data leakage and information theft. Several studies propose robust watermarking methods to track the copyright of Screen-Camera (SC) images, facilitating post-hoc certification against infringement. These techniques typically employ heuristic mathematical modeling or supervised neural network fitting as the noise layer, to enhance watermarking robustness against SC. However, both strategies cannot fundamentally achieve an effective approximation of SC noise. Mathematical simulation suffers from biased approximations due to the incomplete decomposition of the noise and the absence of interdependence among the noise components. Supervised networks require paired data to train the noise-fitting model, and it is difficult for the model to learn all the features of the noise. To address the above issues, we propose Simulation-to-Real (S2R). Specifically, an unsupervised noise layer employs unpaired data to learn the discrepancy between the modeled simulated noise distribution and the real-world SC noise distribution, rather than directly learning the mapping from sharp images to real-world images. Learning this transformation from simulation to reality is inherently simpler, as it primarily involves bridging the gap in noise distributions, instead of the complex task of reconstructing fine-grained image details. Extensive experimental results validate the efficacy of the proposed method, demonstrating superior watermark robustness and generalization compared to state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2504.18906",
    "authors": [
      "Yufeng Wu",
      "Xin Liao",
      "Baowei Wang",
      "Han Fang",
      "Xiaoshuai Wu",
      "Mingyue Chen",
      "Guiling Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.19574",
    "title": "DG-DETR: Toward Domain Generalized Detection Transformer",
    "abstract": "           End-to-end Transformer-based detectors (DETRs) have demonstrated strong detection performance. However, domain generalization (DG) research has primarily focused on convolutional neural network (CNN)-based detectors, while paying little attention to enhancing the robustness of DETRs. In this letter, we introduce a Domain Generalized DEtection TRansformer (DG-DETR), a simple, effective, and plug-and-play method that improves out-of-distribution (OOD) robustness for DETRs. Specifically, we propose a novel domain-agnostic query selection strategy that removes domain-induced biases from object queries via orthogonal projection onto the instance-specific style space. Additionally, we leverage a wavelet decomposition to disentangle features into domain-invariant and domain-specific components, enabling synthesis of diverse latent styles while preserving the semantic features of objects. Experimental results validate the effectiveness of DG-DETR. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2504.19574",
    "authors": [
      "Seongmin Hwang",
      "Daeyoung Han",
      "Moongu Jeon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.04116",
    "title": "RFNNS: Robust Fixed Neural Network Steganography with Universal Text-to-Image Models",
    "abstract": "           With the rapid development of generative AI, image steganography has garnered widespread attention due to its unique concealment. Recent studies have demonstrated the practical advantages of Fixed Neural Network Steganography (FNNS), notably its ability to achieve stable information embedding and extraction without any additional network training. However, the stego images generated by FNNS still exhibit noticeable distortion and limited robustness. These drawbacks compromise the security of the embedded information and restrict the practical applicability of the method. To address these limitations, we propose Robust Fixed Neural Network Steganography (RFNNS). Specifically, a texture-aware localization technique selectively embeds perturbations carrying secret information into regions of complex textures, effectively preserving visual quality. Additionally, a robust steganographic perturbation generation (RSPG) strategy is designed to enhance the decoding accuracy, even under common and unknown attacks. These robust perturbations are combined with AI-generated cover images to produce stego images. Experimental results demonstrate that RFNNS significantly improves robustness compared to state-of-the-art FNNS methods, achieving an average increase in SSIM of 23\\% for recovered secret images under common attacks. Furthermore, the LPIPS value of recovered secrets images against previously unknown attacks achieved by RFNNS was reduced to 39\\% of the SOTA method, underscoring its practical value for covert communication.         ",
    "url": "https://arxiv.org/abs/2505.04116",
    "authors": [
      "Yu Cheng",
      "Jiuan Zhou",
      "Jiawei Chen",
      "Zhaoxia Yin",
      "Xinpeng Zhang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2505.09074",
    "title": "Trends in Motion Prediction Toward Deployable and Generalizable Autonomy: A Revisit and Perspectives",
    "abstract": "           Motion prediction, recently popularized as world models, refers to the anticipation of future agent states or scene evolution, which is rooted in human cognition, bridging perception and decision-making. It enables intelligent systems, such as robots and self-driving cars, to act safely in dynamic, human-involved environments, and informs broader time-series reasoning challenges. With advances in methods, representations, and datasets, the field has seen rapid progress, reflected in quickly evolving benchmark results. Yet, when state-of-the-art methods are deployed in the real world, they often struggle to generalize to open-world conditions and fall short of deployment standards. This reveals a gap between research benchmarks, which are often idealized or ill-posed, and real-world complexity. To address this gap, this survey revisits the generalization and deployability of motion prediction models, with an emphasis on applications of robotics, autonomous driving, and human motion. We first offer a comprehensive taxonomy of motion prediction methods, covering representations, modeling strategies, application domains, and evaluation protocols. We then study two key challenges: (1) how to push motion prediction models to be deployable to realistic deployment standards, where motion prediction does not act in a vacuum, but functions as one module of closed-loop autonomy stacks - it takes input localization and perception, and informs downstream planning and control. 2) How to generalize motion prediction models from limited seen scenarios/datasets to the open-world settings. Throughout the paper, we highlight critical open challenges to guide future work, aiming to recalibrate the community's efforts, fostering progress that is not only measurable but also meaningful for real-world applications. The project webpage can be found here this https URL.         ",
    "url": "https://arxiv.org/abs/2505.09074",
    "authors": [
      "Letian Wang",
      "Marc-Antoine Lavoie",
      "Sandro Papais",
      "Barza Nisar",
      "Yuxiao Chen",
      "Wenhao Ding",
      "Boris Ivanovic",
      "Hao Shao",
      "Abulikemu Abuduweili",
      "Evan Cook",
      "Yang Zhou",
      "Peter Karkus",
      "Jiachen Li",
      "Changliu Liu",
      "Marco Pavone",
      "Steven Waslander"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.11122",
    "title": "Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic Factor Mining",
    "abstract": "           Alpha factor mining is pivotal in quantitative investment for identifying predictive signals from complex financial data. While traditional formulaic alpha mining relies on human expertise, contemporary automated methods, such as those based on genetic programming or reinforcement learning, often struggle with search inefficiency or yield alpha factors that are difficult to interpret. This paper introduces a novel framework that integrates Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS) to overcome these limitations. Our framework leverages the LLM's instruction-following and reasoning capability to iteratively generate and refine symbolic alpha formulas within an MCTS-driven exploration. A key innovation is the guidance of MCTS exploration by rich, quantitative feedback from financial backtesting of each candidate factor, enabling efficient navigation of the vast search space. Furthermore, a frequent subtree avoidance mechanism is introduced to enhance search diversity and prevent formulaic homogenization, further improving performance. Experimental results on real-world stock market data demonstrate that our LLM-based framework outperforms existing methods by mining alphas with superior predictive accuracy and trading performance. The resulting formulas are also more amenable to human interpretation, establishing a more effective and efficient paradigm for formulaic alpha mining.         ",
    "url": "https://arxiv.org/abs/2505.11122",
    "authors": [
      "Yu Shi",
      "Yitong Duan",
      "Jian Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.17330",
    "title": "FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding",
    "abstract": "           In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG's capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance. Code : this https URL ",
    "url": "https://arxiv.org/abs/2505.17330",
    "authors": [
      "Amit Agarwal",
      "Srikant Panda",
      "Kulbhushan Pachauri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.23015",
    "title": "Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models",
    "abstract": "           Stealthy data poisoning during fine-tuning can backdoor large language models (LLMs), threatening downstream safety. Existing detectors either use classifier-style probability signals--ill-suited to generation--or rely on rewriting, which can degrade quality and even introduce new triggers. We address the practical need to efficiently remove poisoned examples before or during fine-tuning. We observe a robust signal in the response space: after applying TF-IDF to model responses, poisoned examples form compact clusters (driven by consistent malicious outputs), while clean examples remain dispersed. We leverage this with RFTC--Reference-Filtration + TF-IDF Clustering. RFTC first compares each example's response with that of a reference model and flags those with large deviations as suspicious; it then performs TF-IDF clustering on the suspicious set and identifies true poisoned examples using intra-class distance. On two machine translation datasets and one QA dataset, RFTC outperforms prior detectors in both detection accuracy and the downstream performance of the fine-tuned models. Ablations with different reference models further validate the effectiveness and robustness of Reference-Filtration.         ",
    "url": "https://arxiv.org/abs/2505.23015",
    "authors": [
      "Jinwen Chen",
      "Hainan Zhang",
      "Fei Sun",
      "Qinnan Zhang",
      "Sijia Wen",
      "Ziwei Wang",
      "Zhiming Zheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.06151",
    "title": "Joint-GCG: Unified Gradient-Based Poisoning Attacks on Retrieval-Augmented Generation Systems",
    "abstract": "           Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by retrieving relevant documents from external corpora before generating responses. This approach significantly expands LLM capabilities by leveraging vast, up-to-date external knowledge. However, this reliance on external knowledge makes RAG systems vulnerable to corpus poisoning attacks that manipulate generated outputs via poisoned document injection. Existing poisoning attack strategies typically treat the retrieval and generation stages as disjointed, limiting their effectiveness. We propose Joint-GCG, the first framework to unify gradient-based attacks across both retriever and generator models through three innovations: (1) Cross-Vocabulary Projection for aligning embedding spaces, (2) Gradient Tokenization Alignment for synchronizing token-level gradient signals, and (3) Adaptive Weighted Fusion for dynamically balancing attacking objectives. Evaluations demonstrate that Joint-GCG achieves at most 25% and an average of 5% higher attack success rate than previous methods across multiple retrievers and generators. While optimized under a white-box assumption, the generated poisons show unprecedented transferability to unseen models. Joint-GCG's innovative unification of gradient-based attacks across retrieval and generation stages fundamentally reshapes our understanding of vulnerabilities within RAG systems. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.06151",
    "authors": [
      "Haowei Wang",
      "Rupeng Zhang",
      "Junjie Wang",
      "Mingyang Li",
      "Yuekai Huang",
      "Dandan Wang",
      "Qing Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.09544",
    "title": "STOAT: Spatial-Temporal Probabilistic Causal Inference Network",
    "abstract": "           Spatial-temporal causal time series (STC-TS) involve region-specific temporal observations driven by causally relevant covariates and interconnected across geographic or network-based spaces. Existing methods often model spatial and temporal dynamics independently and overlook causality-driven probabilistic forecasting, limiting their predictive power. To address this, we propose STOAT (Spatial-Temporal Probabilistic Causal Inference Network), a novel framework for probabilistic forecasting in STC-TS. The proposed method extends a causal inference approach by incorporating a spatial relation matrix that encodes interregional dependencies (e.g. proximity or connectivity), enabling spatially informed causal effect estimation. The resulting latent series are processed by deep probabilistic models to estimate the parameters of the distributions, enabling calibrated uncertainty modeling. We further explore multiple output distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture region-specific variability. Experiments on COVID-19 data across six countries demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics, particularly in regions with strong spatial dependencies. By bridging causal inference and geospatial probabilistic forecasting, STOAT offers a generalizable framework for complex spatial-temporal tasks, such as epidemic management.         ",
    "url": "https://arxiv.org/abs/2506.09544",
    "authors": [
      "Yang Yang",
      "Du Yin",
      "Hao Xue",
      "Flora Salim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.14951",
    "title": "Flat Channels to Infinity in Neural Loss Landscapes",
    "abstract": "           The loss landscapes of neural networks contain minima and saddle points that may be connected in flat regions or appear in isolation. We identify and characterize a special structure in the loss landscape: channels along which the loss decreases extremely slowly, while the output weights of at least two neurons, $a_i$ and $a_j$, diverge to $\\pm$infinity, and their input weight vectors, $\\mathbf{w_i}$ and $\\mathbf{w_j}$, become equal to each other. At convergence, the two neurons implement a gated linear unit: $a_i\\sigma(\\mathbf{w_i} \\cdot \\mathbf{x}) + a_j\\sigma(\\mathbf{w_j} \\cdot \\mathbf{x}) \\rightarrow \\sigma(\\mathbf{w} \\cdot \\mathbf{x}) + (\\mathbf{v} \\cdot \\mathbf{x}) \\sigma'(\\mathbf{w} \\cdot \\mathbf{x})$. Geometrically, these channels to infinity are asymptotically parallel to symmetry-induced lines of critical points. Gradient flow solvers, and related optimization methods like SGD or ADAM, reach the channels with high probability in diverse regression settings, but without careful inspection they look like flat local minima with finite parameter values. Our characterization provides a comprehensive picture of these quasi-flat regions in terms of gradient dynamics, geometry, and functional interpretation. The emergence of gated linear units at the end of the channels highlights a surprising aspect of the computational capabilities of fully connected layers.         ",
    "url": "https://arxiv.org/abs/2506.14951",
    "authors": [
      "Flavio Martinelli",
      "Alexander Van Meegen",
      "Berfin \u015eim\u015fek",
      "Wulfram Gerstner",
      "Johanni Brea"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2506.17004",
    "title": "A Synthetic Benchmark for Collaborative 3D Semantic Occupancy Prediction in V2X Autonomous Driving",
    "abstract": "           3D semantic occupancy prediction is an emerging perception paradigm in autonomous driving, providing a voxel-level representation of both geometric details and semantic categories. However, the perception capability of a single vehicle is inherently constrained by occlusion, restricted sensor range, and narrow viewpoints. To address these limitations, collaborative perception enables the exchange of complementary information, thereby enhancing the completeness and accuracy. In the absence of a dedicated dataset for collaborative 3D semantic occupancy prediction, we augment an existing collaborative perception dataset by replaying it in CARLA with a high-resolution semantic voxel sensor to provide dense and comprehensive occupancy annotations. In addition, we establish benchmarks with varying prediction ranges designed to systematically assess the impact of spatial extent on collaborative prediction. We further develop a baseline model that performs inter-agent feature fusion via spatial alignment and attention aggregation. Experimental results demonstrate that our baseline model consistently outperforms single-agent models, with increasing gains observed as the prediction range expands.         ",
    "url": "https://arxiv.org/abs/2506.17004",
    "authors": [
      "Hanlin Wu",
      "Pengfei Lin",
      "Ehsan Javanmardi",
      "Naren Bao",
      "Bo Qian",
      "Hao Si",
      "Manabu Tsukada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.04323",
    "title": "DMAT: An End-to-End Framework for Joint Atmospheric Turbulence Mitigation and Object Detection",
    "abstract": "           Atmospheric Turbulence (AT) degrades the clarity and accuracy of surveillance imagery, posing challenges not only for visualization quality but also for object classification and scene tracking. Deep learning-based methods have been proposed to improve visual quality, but spatio-temporal distortions remain a significant issue. Although deep learning-based object detection performs well under normal conditions, it struggles to operate effectively on sequences distorted by atmospheric turbulence. In this paper, we propose a novel framework that learns to compensate for distorted features while simultaneously improving visualization and object detection. This end-to-end training strategy leverages and exchanges knowledge of low-level distorted features in the AT mitigator with semantic features extracted in the object detector. Specifically, in the AT mitigator a 3D Mamba-based structure is used to handle the spatio-temporal displacements and blurring caused by turbulence. Optimization is achieved through back-propagation in both the AT mitigator and object detector. Our proposed DMAT outperforms state-of-the-art AT mitigation and object detection systems up to a 15% improvement on datasets corrupted by generated turbulence.         ",
    "url": "https://arxiv.org/abs/2507.04323",
    "authors": [
      "Paul Hill",
      "Zhiming Liu",
      "Alin Achim",
      "Dave Bull",
      "Nantheera Anantrasirichai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.07850",
    "title": "Identifying the Smallest Adversarial Load Perturbation that Renders DC-OPF Infeasible",
    "abstract": "           What is the globally smallest load perturbation that renders DC-OPF infeasible? Reliably identifying such \"adversarial attack\" perturbations has useful applications in a variety of emerging grid-related contexts, including machine learning performance verification, cybersecurity, and operational robustness of power systems dominated by stochastic renewable energy resources. In this paper, we formulate the inherently nonconvex adversarial attack problem by applying a parameterized version of Farkas' lemma to a perturbed set of DC-OPF equations. Since the resulting formulation is very hard to globally optimize, we also propose a parameterized generation control policy which, when applied to the primal DC-OPF problem, provides solvability guarantees. Together, these nonconvex problems provide guaranteed upper and lower bounds on adversarial attack size; by combining them into a single optimization problem, we can efficiently \"squeeze\" these bounds towards a common global solution. We apply these methods on a range of small- to medium-sized test cases from PGLib, benchmarking our results against the best adversarial attack lower bounds provided by Gurobi 12.0's spatial Branch and Bound solver.         ",
    "url": "https://arxiv.org/abs/2507.07850",
    "authors": [
      "Samuel Chevalier",
      "William A. Wheeler"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.08802",
    "title": "The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?",
    "abstract": "           The concept of causal abstraction got recently popularised to demystify the opaque decision-making processes of machine learning models; in short, a neural network can be abstracted as a higher-level algorithm if there exists a function which allows us to map between them. Notably, most interpretability papers implement these maps as linear functions, motivated by the linear representation hypothesis: the idea that features are encoded linearly in a model's representations. However, this linearity constraint is not required by the definition of causal abstraction. In this work, we critically examine the concept of causal abstraction by considering arbitrarily powerful alignment maps. In particular, we prove that under reasonable assumptions, any neural network can be mapped to any algorithm, rendering this unrestricted notion of causal abstraction trivial and uninformative. We complement these theoretical findings with empirical evidence, demonstrating that it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task; e.g., on an experiment using randomly initialised language models, our alignment maps reach 100\\% interchange-intervention accuracy on the indirect object identification task. This raises the non-linear representation dilemma: if we lift the linearity constraint imposed to alignment maps in causal abstraction analyses, we are left with no principled way to balance the inherent trade-off between these maps' complexity and accuracy. Together, these results suggest an answer to our title's question: causal abstraction is not enough for mechanistic interpretability, as it becomes vacuous without assumptions about how models encode information. Studying the connection between this information-encoding assumption and causal abstraction should lead to exciting future work.         ",
    "url": "https://arxiv.org/abs/2507.08802",
    "authors": [
      "Denis Sutter",
      "Julian Minder",
      "Thomas Hofmann",
      "Tiago Pimentel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12482",
    "title": "Kodezi Chronos: A Debugging-First Language Model for Repository-Scale Code Understanding",
    "abstract": "           Large Language Models (LLMs) have advanced code generation and software automation but remain constrained by inference-time context and lack structured reasoning over code, leaving debugging largely unsolved. While Claude 4.5 Sonnet and Claude 4.1 Opus exceed 70% on code synthesis benchmarks, they achieve under 15% on real debugging tasks. We introduce Kodezi Chronos, a language model purpose-built for debugging that integrates Adaptive Graph-Guided Retrieval to traverse codebases up to 10 million lines, Persistent Debug Memory trained on over 15 million sessions, and a seven-layer fix-test-refine architecture. On 5,000 real-world scenarios, Chronos achieves 67.3% fix accuracy compared to 14.2% and 13.8% for Claude 4.1 Opus and GPT-4.1, respectively. On SWE-bench Lite, Chronos reaches a state-of-the-art 80.33% resolution rate (241 of 300), outperforming the next best system by 20 points and achieving repository-specific highs of 96.1% on Sympy and 90.4% on Django. Chronos reduces debugging time by 40% and iterations by 65%, resolving complex multi-file and cross-repository bugs. It remains limited on hardware-dependent and dynamic language errors. Chronos will be available in Kodezi OS in Q4 2025 and via API in Q1 2026.         ",
    "url": "https://arxiv.org/abs/2507.12482",
    "authors": [
      "Ishraq Khan",
      "Assad Chowdary",
      "Sharoz Haseeb",
      "Urvish Patel",
      "Yousuf Zaii"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.12707",
    "title": "Splittable Spanning Trees and Balanced Forests in Dense Random Graphs",
    "abstract": "           We consider the probability that a spanning tree chosen uniformly at random from a graph can be partitioned into a fixed number $k$ of trees of equal size by removing $k-1$ edges. In that case, the spanning tree is called {\\em splittable}. Splittable spanning trees are useful in algorithms for sampling {\\em balanced forests}, forests whose components are of equal size, and for sampling partitions of a graph into components of equal size, with applications in redistricting, network algorithms, and image decomposition. Cannon et al.~recently showed that spanning trees on grid and grid-like graphs on $n$ vertices are splittable into $k$ equal sized components with probability at least $n^{-2k}$, leading to the first rigorous sampling algorithm for balanced forests in any class of graphs. Focusing on the complementary case of dense random graphs, we show that random spanning trees have inverse polynomial probability of being splittable; specifically, a random spanning tree is splittable with probability at least $n^{(-k/2)}$ for both the $G(n,p)$ and $G(n,m)$ models when $p = \\Omega(1/\\log n)$, giving the first dense class of graphs where partitions of equal size can be sampled efficiently. In addition, we present an infinite family of graphs with properties that have been conjectured to ensure splittability (i.e., Hamiltonian subgraphs of the triangular lattice) and prove that random spanning trees are not splittable with more than exponentially small probability. As a consequence, we show that a family of widely-used Markov chain algorithms for sampling equal-size partitions will fail on this family of graphs if their state spaces are restricted to equal-size partitions. Moreover, we show these algorithms will be inefficient if their state spaces are generalized to include any unbalanced partitions, suggesting barriers for sampling balanced partitions in sparse graphs.         ",
    "url": "https://arxiv.org/abs/2507.12707",
    "authors": [
      "David Gillman",
      "Jacob Platnick",
      "Dana Randall"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2507.13397",
    "title": "Trustworthy Pedestrian Trajectory Prediction via Pattern-Aware Interaction Modeling",
    "abstract": "           Accurate and reliable pedestrian trajectory prediction is critical for the application of intelligent applications, yet achieving trustworthy prediction remains highly challenging due to the complexity of interactions among pedestrians. Previous methods often adopt black-box modeling of pedestrian interactions. Despite their strong performance, such opaque modeling limits the reliability of predictions in real-world deployments. To address this issue, we propose InSyn (Interaction-Synchronization Network), a novel Transformer-based model that explicitly captures diverse interaction patterns (e.g., walking in sync or conflicting) while effectively modeling direction-sensitive social behaviors. Additionally, we introduce a training strategy, termed Seq-Start of Seq (SSOS), designed to alleviate the common issue of initial-step divergence in numerical time-series prediction. Experiments on the ETH and UCY datasets demonstrate that our model not only outperforms recent black-box baselines in prediction accuracy, especially under high-density scenarios, but also provides transparent interaction modeling, as shown in the case study. Furthermore, the SSOS strategy proves to be effective in improving sequential prediction performance, reducing the initial-step prediction error by approximately 6.58%. Code is avaliable at this https URL ",
    "url": "https://arxiv.org/abs/2507.13397",
    "authors": [
      "Kaiyuan Zhai",
      "Juan Chen",
      "Chao Wang",
      "Zeyi Xu",
      "Guoming Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.15483",
    "title": "Advancing Lunar Communication through Inter-domain Space Networks and Dynamic Orchestration",
    "abstract": "           The resurgent era of lunar exploration is defined by a strategic shift from temporary visits to a sustained international and commercial presence, resulting in an unprecedented demand for a robust and continuously available communication infrastructure. The conventional direct-to-Earth communication architecture relies on limited and oversubscribed deep space networks, which are further challenged by the radiative environment and insufficient visibility in certain areas of the cislunar domain. We address these issues by proposing a foundational move toward inter-domain space network cooperation by introducing architectures based on near space networks. They can directly service lunar surface users or, via cislunar relays, by forming a resilient and multi-layered communication backbone. First, we establish a unified link analysis framework incorporating frequently disregarded environmental factors, such as the Moon's variable illumination, to provide a high-fidelity performance evaluation. Second, we assess architectures' reliability based on the outage risk, essential for quantifying the operational robustness of communication links. Finally, to manage the inherent dynamism of architectures, we propose an inter-domain space digital twin: a dynamic decision-making engine that performs real-time analysis to autonomously select the best communication path, ensuring high and stable reliability while simultaneously optimizing power consumption. Overall, our paper provides a holistic architectural and conceptual management framework, emphasizing the necessity of lunar communications to support a permanent human and economic foothold on the Moon.         ",
    "url": "https://arxiv.org/abs/2507.15483",
    "authors": [
      "Selen Gecgel Cetin",
      "Baris Donmez",
      "Gunes Karabulut Kurt"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2507.16476",
    "title": "Survival Modeling from Whole Slide Images via Patch-Level Graph Clustering and Mixture Density Experts",
    "abstract": "           We propose a modular framework for predicting cancer specific survival directly from whole slide pathology images (WSIs). The framework consists of four key stages designed to capture prognostic and morphological heterogeneity. First, a Quantile Based Patch Filtering module selects prognostically informative tissue regions through quantile thresholding. Second, Graph Regularized Patch Clustering models phenotype level variations using a k nearest neighbor graph that enforces spatial and morphological coherence. Third, Hierarchical Feature Aggregation learns both intra and inter cluster dependencies to represent multiscale tumor organization. Finally, an Expert Guided Mixture Density Model estimates complex survival distributions via Gaussian mixtures, enabling fine grained risk prediction. Evaluated on TCGA LUAD, TCGA KIRC, and TCGA BRCA cohorts, our model achieves concordance indices of 0.653 ,0.719 ,and 0.733 respectively, surpassing existing state of the art approaches in survival prediction from WSIs.         ",
    "url": "https://arxiv.org/abs/2507.16476",
    "authors": [
      "Ardhendu Sekhar",
      "Vasu Soni",
      "Keshav Aske",
      "Garima Jain",
      "Pranav Jeevan",
      "Amit Sethi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02671",
    "title": "Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on Vision-Language Models",
    "abstract": "           For CLIP-based prompt tuning, introducing more data as additional knowledge for enhancing fine-tuning process is proved to be an effective approach. Existing data amplification strategies for prompt tuning typically rely on external knowledge (e.g., large language models or pre-structured knowledge bases), resulting in higher costs for data collection and processing, while generally ignoring further utilization of features in image modality. To address this, we propose Augmentation-driven Prompt Tuning (AugPT), a self-contained distillation-based prompt tuning approach using only internal augmentation on raw dataset to better exploit known features. Specifically, AugPT employs self-supervised augmentation on unlabeled images in the training set, and introduces a novel gating mechanism based on consensus test, reusing the pre-trained prompt tuning backbone model to spontaneously filter noisy samples, further enhancing the quality of augmented views. Extensive experiments validate that AugPT simultaneously enhances model performance and generalization capability without using appended external knowledge. The code of AugPT is available at: this https URL .         ",
    "url": "https://arxiv.org/abs/2508.02671",
    "authors": [
      "Haoyang Li",
      "Liang Wang",
      "Chao Wang",
      "Siyu Zhou",
      "Jing Jiang",
      "Yan Peng",
      "Guodong Long"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.03136",
    "title": "Distributionally Robust Markov Games with Average Reward",
    "abstract": "           We study distributionally robust Markov games (DR-MGs) with the average-reward criterion, a crucial framework for multi-agent decision-making under uncertainty over extended horizons. We first establish a connection between the best-response policies and the optimal policies for the induced single-agent problems. Under a standard irreducible assumption, we derive a correspondence between the optimal policies and the solutions of the robust Bellman equation, and derive the existence of stationary Nash Equilibrium (NE) based on these results. We also study a more general weakly communicating setting. We construct a set-valued map and show its value is a subset of the best-response policies, convex and upper hemi-continuous, which imply the existence of NE. We then introduce Robust Nash-Iteration, and provide convergence guarantees. Finally, we connect average-reward NE to discounted robust equilibria, showing approximation as the discount factor approaches one. Our studies provide comprehensive theoretical and algorithmic foundation for decision-making in complex, uncertain, and long-running multi-player environments.         ",
    "url": "https://arxiv.org/abs/2508.03136",
    "authors": [
      "Zachary Roch",
      "Yue Wang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2508.03213",
    "title": "The Power of Many: Synergistic Unification of Diverse Augmentations for Efficient Adversarial Robustness",
    "abstract": "           Adversarial perturbations pose a significant threat to deep learning models. Adversarial Training (AT), the predominant defense method, faces challenges of high computational costs and a degradation in standard performance. While data augmentation offers an alternative path, existing techniques either yield limited robustness gains or incur substantial training overhead. Therefore, developing a defense mechanism that is both highly efficient and strongly robust is of paramount this http URL this work, we first conduct a systematic analysis of existing augmentation techniques, revealing that the synergy among diverse strategies -- rather than any single method -- is crucial for enhancing robustness. Based on this insight, we propose the Universal Adversarial Augmenter (UAA) framework, which is characterized by its plug-and-play nature and training efficiency. UAA decouples the expensive perturbation generation process from model training by pre-computing a universal transformation offline, which is then used to efficiently generate unique adversarial perturbations for each sample during this http URL experiments conducted on multiple benchmarks validate the effectiveness of UAA. The results demonstrate that UAA establishes a new state-of-the-art (SOTA) for data-augmentation-based adversarial defense strategies , without requiring the online generation of adversarial examples during training. This framework provides a practical and efficient pathway for building robust models,Our code is available in the supplementary materials.         ",
    "url": "https://arxiv.org/abs/2508.03213",
    "authors": [
      "Wang Yu-Hang",
      "Shiwei Li",
      "Jianxiang Liao",
      "Li Bohan",
      "Jian Liu",
      "Wenfei Yin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.04001",
    "title": "ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval",
    "abstract": "           Conversational search aims to satisfy users' complex information needs via multiple-turn interactions. The key challenge lies in revealing real users' search intent from the context-dependent queries. Previous studies achieve conversational search by fine-tuning a conversational dense retriever with relevance judgments between pairs of context-dependent queries and documents. However, this training paradigm encounters data scarcity issues. To this end, we propose ConvMix, a mixed-criteria framework to augment conversational dense retrieval, which covers more aspects than existing data augmentation frameworks. We design a two-sided relevance judgment augmentation schema in a scalable manner via the aid of large language models. Besides, we integrate the framework with quality control mechanisms to obtain semantically diverse samples and near-distribution supervisions to combine various annotated data. Experimental results on five widely used benchmarks show that the conversational dense retriever trained by our ConvMix framework outperforms previous baseline methods, which demonstrates our superior effectiveness.         ",
    "url": "https://arxiv.org/abs/2508.04001",
    "authors": [
      "Fengran Mo",
      "Jinghan Zhang",
      "Yuchen Hui",
      "Jia Ao Sun",
      "Zhichao Xu",
      "Zhan Su",
      "Jian-Yun Nie"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.08543",
    "title": "M3-Net: A Cost-Effective Graph-Free MLP-Based Model for Traffic Prediction",
    "abstract": "           Achieving accurate traffic prediction is a fundamental but crucial task in the development of current intelligent transportation this http URL of the mainstream methods that have made breakthroughs in traffic prediction rely on spatio-temporal graph neural networks, spatio-temporal attention mechanisms, etc. The main challenges of the existing deep learning approaches are that they either depend on a complete traffic network structure or require intricate model designs to capture complex spatio-temporal dependencies. These limitations pose significant challenges for the efficient deployment and operation of deep learning models on large-scale datasets. To address these challenges, we propose a cost-effective graph-free Multilayer Perceptron (MLP) based model M3-Net for traffic prediction. Our proposed model not only employs time series and spatio-temporal embeddings for efficient feature processing but also first introduces a novel MLP-Mixer architecture with a mixture of experts (MoE) mechanism. Extensive experiments conducted on multiple real datasets demonstrate the superiority of the proposed model in terms of prediction performance and lightweight this http URL code is available at this https URL ",
    "url": "https://arxiv.org/abs/2508.08543",
    "authors": [
      "Guangyin Jin",
      "Sicong Lai",
      "Xiaoshuai Hao",
      "Mingtao Zhang",
      "Jinlei Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.12495",
    "title": "Mitigating Hallucinations in Large Language Models via Causal Reasoning",
    "abstract": "           Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.12495",
    "authors": [
      "Yuangang Li",
      "Yiqing Shen",
      "Yi Nian",
      "Jiechao Gao",
      "Ziyi Wang",
      "Chenxiao Yu",
      "Shawn Li",
      "Jie Wang",
      "Xiyang Hu",
      "Yue Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.10837",
    "title": "Exploring the Paradigm Shift from Grounding to Skolemization for Complex Query Answering on Knowledge Graphs",
    "abstract": "           Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs), typically formalized as reasoning with Existential First-Order predicate logic with one free variable (EFO\\textsubscript{1}), faces a fundamental tradeoff between logic fidelity and computational efficiency. This work establishes a Grounding-Skolemization dichotomy to systematically analyze this challenge and motivate a paradigm shift in CQA. While Grounding-based methods inherently suffer from combinatorial explosion, most Skolemization-based methods neglect to explicitly model Skolem functions and compromise logical consistency. To address these limitations, we propose the Logic-constrained Vector Symbolic Architecture (LVSA), a neuro-symbolic framework that unifies a differentiable Skolemization module and a neural negator, as well as a logical constraint-driven optimization protocol to harmonize geometric and logical requirements. Theoretically, LVSA guarantees universality for all EFO\\textsubscript{1} queries with low computational complexity. Empirically, it outperforms state-of-the-art Skolemization-based methods and reduces inference costs by orders of magnitude compared to Grounding-based baselines.         ",
    "url": "https://arxiv.org/abs/2509.10837",
    "authors": [
      "Yuyin Lu",
      "Hegang Chen",
      "Shanrui Xie",
      "Yanghui Rao",
      "Haoran Xie",
      "Fu Lee Wang",
      "Qing Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.11745",
    "title": "Removal Attack and Defense on AI-generated Content Latent-based Watermarking",
    "abstract": "           Digital watermarks can be embedded into AI-generated content (AIGC) by initializing the generation process with starting points sampled from a secret distribution. When combined with pseudorandom error-correcting codes, such watermarked outputs can remain indistinguishable from unwatermarked objects, while maintaining robustness under whitenoise. In this paper, we go beyond indistinguishability and investigate security under removal attacks. We demonstrate that indistinguishability alone does not necessarily guarantee resistance to adversarial removal. Specifically, we propose a novel attack that exploits boundary information leaked by the locations of watermarked objects. This attack significantly reduces the distortion required to remove watermarks -- by up to a factor of $15 \\times$ compared to a baseline whitenoise attack under certain settings. To mitigate such attacks, we introduce a defense mechanism that applies a secret transformation to hide the boundary, and prove that the secret transformation effectively rendering any attacker's perturbations equivalent to those of a naive whitenoise adversary. Our empirical evaluations, conducted on multiple versions of Stable Diffusion, validate the effectiveness of both the attack and the proposed defense, highlighting the importance of addressing boundary leakage in latent-based watermarking schemes.         ",
    "url": "https://arxiv.org/abs/2509.11745",
    "authors": [
      "De Zhang Lee",
      "Han Fang",
      "Hanyi Wang",
      "Ee-Chien Chang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.21389",
    "title": "Towards Adapting Federated & Quantum Machine Learning for Network Intrusion Detection: A Survey",
    "abstract": "           This survey explores the integration of Federated Learning (FL) with Network Intrusion Detection Systems (NIDS), with particular emphasis on deep learning and quantum machine learning approaches. FL enables collaborative model training across distributed devices while preserving data privacy-a critical requirement in network security contexts where sensitive traffic data cannot be centralized. Our comprehensive analysis systematically examines the full spectrum of FL architectures, deployment strategies, communication protocols, and aggregation methods specifically tailored for intrusion detection. We provide an in-depth investigation of privacy-preserving techniques, model compression approaches, and attack-specific federated solutions for threats including DDoS, MITM, and botnet attacks. The survey further delivers a pioneering exploration of Quantum FL (QFL), discussing quantum feature encoding, quantum machine learning algorithms, and quantum-specific aggregation methods that promise exponential speedups for complex pattern recognition in network traffic. Through rigorous comparative analysis of classical and quantum approaches, identification of research gaps, and evaluation of real-world deployments, we outline a concrete roadmap for industrial adoption and future research directions. This work serves as an authoritative reference for researchers and practitioners seeking to enhance privacy, efficiency, and robustness of federated intrusion detection systems in increasingly complex network environments, while preparing for the quantum-enhanced cybersecurity landscape of tomorrow.         ",
    "url": "https://arxiv.org/abs/2509.21389",
    "authors": [
      "Devashish Chaudhary",
      "Sutharshan Rajasegarar",
      "Shiva Raj Pokhrel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.23084",
    "title": "Sparse Graph Reconstruction and Seriation for Large-Scale Image Stacks",
    "abstract": "           We study recovering a 1D order from a noisy, locally sampled pairwise comparison matrix under a tight query budget. We recast the task as reconstructing a sparse, noisy line graph and present, to our knowledge, the first method that provably builds a sparse graph containing all edges needed for exact seriation using only O(N(log N + K)) oracle queries, which is near-linear in N for fixed window K. The approach is parallelizable and supports both binary and bounded-noise distance oracles. Our five-stage pipeline consists of: (i) a random-hook Boruvka step to connect components via short-range edges in O(N log N) queries; (ii) iterative condensation to bound graph diameter; (iii) a double-sweep BFS to obtain a provisional global order; (iv) fixed-window densification around that order; and (v) a greedy SuperChain that assembles the final permutation. Under a simple top-1 margin and bounded relative noise we prove exact recovery; empirically, SuperChain still succeeds when only about 2N/3 of true adjacencies are present. On wafer-scale serial-section EM, our method outperforms spectral, MST, and TSP baselines with far fewer comparisons, and is applicable to other locally structured sequencing tasks such as temporal snapshot ordering, archaeological seriation, and playlist/tour construction.         ",
    "url": "https://arxiv.org/abs/2509.23084",
    "authors": [
      "Fuming Yang",
      "Yaron Meirovitch",
      "Jeff W. Lichtman"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2510.09659",
    "title": "Heterogeneous Point Set Transformers for Segmentation of Multiple View Particle Detectors",
    "abstract": "           NOvA is a long-baseline neutrino oscillation experiment that detects neutrino particles from the NuMI beam at Fermilab. Before data from this experiment can be used in analyses, raw hits in the detector must be matched to their source particles, and the type of each particle must be identified. This task has commonly been done using a mix of traditional clustering approaches and convolutional neural networks (CNNs). Due to the construction of the detector, the data is presented as two sparse 2D images: an XZ and a YZ view of the detector, rather than a 3D representation. We propose a point set neural network that operates on the sparse matrices with an operation that mixes information from both views. Our model uses less than 10% of the memory required using previous methods while achieving a 96.8% AUC score, a higher score than obtained when both views are processed independently (85.4%).         ",
    "url": "https://arxiv.org/abs/2510.09659",
    "authors": [
      "Edgar E. Robles",
      "Dikshant Sagar",
      "Alejandro Yankelevich",
      "Jianming Bian",
      "Pierre Baldi",
      "NOvA Collaboration"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)"
    ]
  },
  {
    "id": "arXiv:2510.16088",
    "title": "Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch",
    "abstract": "           Quantization of neural networks provides benefits of inference in less compute and memory requirements. Previous work in quantization lack two important aspects which this work provides. First almost all previous work in quantization used a non-differentiable approach and for learning; the derivative is usually set manually in backpropogation which make the learning ability of algorithm questionable, our approach is not just differentiable, we also provide proof of convergence of our approach to the optimal neural network. Second previous work in shift/logrithmic quantization either have avoided activation quantization along with weight quantization or achieved less accuracy. Learning logrithmic quantize values of form $2^n$ requires the quantization function can scale to more than 1 bit quantization which is another benifit of our quantization that it provides $n$ bits quantization as well. Our approach when tested with image classification task using imagenet dataset, resnet18 and weight quantization only achieves less than 1 percent accuracy compared to full precision accuracy while taking only 15 epochs to train using shift bit quantization and achieves comparable to SOTA approaches accuracy in both weight and activation quantization using shift bit quantization in 15 training epochs with slightly higher(only higher cpu instructions) inference cost compared to 1 bit quantization(without logrithmic quantization) and not requiring any higher precision multiplication.         ",
    "url": "https://arxiv.org/abs/2510.16088",
    "authors": [
      "Zia Badar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2510.17644",
    "title": "Mapping Hidden Heritage: Self-supervised Pre-training for Archaeological Stone Wall Mapping in Historic Landscapes Using High-Resolution DEM Derivatives",
    "abstract": "           Historic dry-stone walls hold significant cultural and environmental importance, serving as historical markers and contributing to ecosystem preservation and wildfire management during dry seasons in Australia. However, many of these stone structures in remote or vegetated landscapes remain undocumented due to limited accessibility and the high cost of manual mapping. Deep learning-based segmentation offers a scalable approach for automated mapping of such features, but challenges remain: the visual occlusion of low-lying walls by dense vegetation and the scarcity of labeled training data. This study presents DINO-CV, a self-supervised cross-view pre-training framework based on knowledge distillation, designed for accurate mapping of dry-stone walls using high-resolution Digital Elevation Models (DEMs) derived from airborne LiDAR. By learning invariant structural representations across multiple DEM-derived views, specifically Multi-directional Hillshade (MHS) and Visualization for Archaeological Topography (VAT), DINO-CV addresses both occlusion and data scarcity challenges. Applied to the Budj Bim Cultural Landscape (Victoria, Australia), a UNESCO World Heritage site, the approach achieves a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains 63.8% mIoU when fine-tuned with only 10% labeled data. These results demonstrate the potential of self-supervised learning on high-resolution DEM derivatives for large-scale, automated mapping of cultural heritage features in complex and vegetated environments. Beyond archaeology, this approach offers a scalable solution for environmental monitoring and heritage preservation across inaccessible or environmentally sensitive regions.         ",
    "url": "https://arxiv.org/abs/2510.17644",
    "authors": [
      "Zexian Huang",
      "Mashnoon Islam",
      "Brian Armstrong",
      "Billy Bell",
      "Kourosh Khoshelham",
      "Martin Tomko"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.21945",
    "title": "Generalization Bounds for Rank-sparse Neural Networks",
    "abstract": "           It has been recently observed in much of the literature that neural networks exhibit a bottleneck rank property: for larger depths, the activation and weights of neural networks trained with gradient-based methods tend to be of approximately low rank. In fact, the rank of the activations of each layer converges to a fixed value referred to as the ``bottleneck rank'', which is the minimum rank required to represent the training data. This perspective is in line with the observation that regularizing linear networks (without activations) with weight decay is equivalent to minimizing the Schatten $p$ quasi norm of the neural network. In this paper we investigate the implications of this phenomenon for generalization. More specifically, we prove generalization bounds for neural networks which exploit the approximate low rank structure of the weight matrices if present. The final results rely on the Schatten $p$ quasi norms of the weight matrices: for small $p$, the bounds exhibit a sample complexity $ \\widetilde{O}(WrL^2)$ where $W$ and $L$ are the width and depth of the neural network respectively and where $r$ is the rank of the weight matrices. As $p$ increases, the bound behaves more like a norm-based bound instead.         ",
    "url": "https://arxiv.org/abs/2510.21945",
    "authors": [
      "Antoine Ledent",
      "Rodrigo Alves",
      "Yunwen Lei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.00456",
    "title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations",
    "abstract": "           This study proposes a weakly supervised deep learning framework for pneumonia classification and localization from chest X-rays, utilizing Grad-CAM explanations. Instead of costly pixel-level annotations, our approach uses image-level labels to generate clinically meaningful heatmaps that highlight regions affected by pneumonia. We evaluate seven pre-trained architectures and the Vision Transformer under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high accuracy (96-98%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V2 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed models focus on clinically relevant lung regions, supporting the use of interpretable AI for radiological diagnostics. This work highlights the potential of weakly supervised, explainable models that enhance the transparency of pneumonia screening and clinical trust in AI-assisted screening.         ",
    "url": "https://arxiv.org/abs/2511.00456",
    "authors": [
      "Kiran Shahi",
      "Anup Bagale"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.04029",
    "title": "Faithful Contouring: Near-Lossless 3D Voxel Representation Free from Iso-surface",
    "abstract": "           Accurate and efficient voxelized representations of 3D meshes are the foundation of 3D reconstruction and generation. However, existing representations based on iso-surface heavily rely on water-tightening or rendering optimization, which inevitably compromise geometric fidelity. We propose Faithful Contouring, a sparse voxelized representation that supports 2048+ resolutions for arbitrary meshes, requiring neither converting meshes to field functions nor extracting the isosurface during remeshing. It achieves near-lossless fidelity by preserving sharpness and internal structures, even for challenging cases with complex geometry and topology. The proposed method also shows flexibility for texturing, manipulation, and editing. Beyond representation, we design a dual-mode autoencoder for Faithful Contouring, enabling scalable and detail-preserving shape reconstruction. Extensive experiments show that Faithful Contouring surpasses existing methods in accuracy and efficiency for both representation and reconstruction. For direct representation, it achieves distance errors at the $10^{-5}$ level; for mesh reconstruction, it yields a 93\\% reduction in Chamfer Distance and a 35\\% improvement in F-score over strong baselines, confirming superior fidelity as a representation for 3D learning tasks.         ",
    "url": "https://arxiv.org/abs/2511.04029",
    "authors": [
      "Yihao Luo",
      "Xianglong He",
      "Chuanyu Pan",
      "Yiwen Chen",
      "Jiaqi Wu",
      "Yangguang Li",
      "Wanli Ouyang",
      "Yuanming Hu",
      "Guang Yang",
      "ChoonHwai Yap"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2511.04472",
    "title": "Exploiting Data Structures for Bypassing and Crashing Anti-Malware Solutions via Telemetry Complexity Attacks",
    "abstract": "           Anti-malware systems rely on sandboxes, hooks, and telemetry pipelines, including collection agents, serializers, and database backends, to monitor program and system behavior. We show that these data-handling components constitute an exploitable attack surface that can lead to denial-of-analysis (DoA) states without disabling sensors or requiring elevated privileges. As a result, we present Telemetry Complexity Attacks (TCAs), a new class of vulnerabilities that exploit fundamental mismatches between unbounded collection mechanisms and bounded processing capabilities. Our method recursively spawns child processes to generate specially crafted, deeply nested, and oversized telemetry that stresses serialization and storage boundaries, as well as visualization layers, for example, JSON/BSON depth and size limits. Depending on the product, this leads to various inconsistent results, such as truncated or missing behavioral reports, rejected database inserts, serializer recursion and size errors, and unresponsive dashboards. In the latter cases, depending on the solution, the malware under test is either not recorded and/or not presented to the analysts. Therefore, instead of evading sensors, we break the pipeline that stores the data captured by the sensors. We evaluate our technique against twelve commercial and open-source malware analysis platforms and endpoint detection and response (EDR) solutions. Seven products fail in different stages of the telemetry pipeline; two vendors assigned CVE identifiers (CVE-2025-61301 and CVE-2025-61303), and others issued patches or configuration changes. We discuss root causes and propose mitigation strategies to prevent DoA attacks triggered by adversarial telemetry.         ",
    "url": "https://arxiv.org/abs/2511.04472",
    "authors": [
      "Evgenios Gkritsis",
      "Constantinos Patsakis",
      "George Stergiopoulos"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2511.04494",
    "title": "Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks",
    "abstract": "           Neural networks are widely used for image-related tasks but typically demand considerable computing power. Once a network has been trained, however, its memory- and compute-footprint can be reduced by compression. In this work, we focus on compression through tensorization and low-rank representations. Whereas classical approaches search for a low-rank approximation by minimizing an isotropic norm such as the Frobenius norm in weight-space, we use data-informed norms that measure the error in function space. Concretely, we minimize the change in the layer's output distribution, which can be expressed as $\\lVert (W - \\widetilde{W}) \\Sigma^{1/2}\\rVert_F$ where $\\Sigma^{1/2}$ is the square root of the covariance matrix of the layer's input and $W$, $\\widetilde{W}$ are the original and compressed weights. We propose new alternating least square algorithms for the two most common tensor decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike conventional compression pipelines, which almost always require post-compression fine-tuning, our data-informed approach often achieves competitive accuracy without any fine-tuning. We further show that the same covariance-based norm can be transferred from one dataset to another with only a minor accuracy drop, enabling compression even when the original training dataset is unavailable. Experiments on several CNN architectures (ResNet-18/50, and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100) confirm the advantages of the proposed method.         ",
    "url": "https://arxiv.org/abs/2511.04494",
    "authors": [
      "Alper Kalle",
      "Theo Rudkiewicz",
      "Mohamed-Oumar Ouerfelli",
      "Mohamed Tamaazousti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.04864",
    "title": "Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction",
    "abstract": "           Recovering high-quality surfaces from irregular point cloud is ill-posed unless strong geometric priors are available. We introduce an implicit self-prior approach that distills a shape-specific prior directly from the input point cloud itself and embeds it within an implicit neural representation. This is achieved by jointly training a small dictionary of learnable embeddings with an implicit distance field; at every query location, the field attends to the dictionary via cross-attention, enabling the network to capture and reuse repeating structures and long-range correlations inherent to the shape. Optimized solely with self-supervised point cloud reconstruction losses, our approach requires no external training data. To effectively integrate this learned prior while preserving input fidelity, the trained field is then sampled to extract densely distributed points and analytic normals via automatic differentiation. We integrate the resulting dense point cloud and corresponding normals into a robust implicit moving least squares (RIMLS) formulation. We show this hybrid strategy preserves fine geometric details in the input data, while leveraging the learned prior to regularize sparse regions. Experiments show that our method outperforms both classical and learning-based approaches in generating high-fidelity surfaces with superior detail preservation and robustness to common data degradations.         ",
    "url": "https://arxiv.org/abs/2511.04864",
    "authors": [
      "Kyle Fogarty",
      "Chenyue Cai",
      "Jing Yang",
      "Zhilin Guo",
      "Cengiz \u00d6ztireli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.04914",
    "title": "MERaLiON-SER: Robust Speech Emotion Recognition Model for English and SEA Languages",
    "abstract": "           We present MERaLiON-SER, a robust speech emotion recognition model designed for English and Southeast Asian languages. The model is trained using a hybrid objective combining weighted categorical cross-entropy and Concordance Correlation Coefficient (CCC) losses for joint discrete and dimensional emotion modelling. This dual approach enables the model to capture both the distinct categories of emotion (like happy or angry) and the fine-grained, such as arousal (intensity), valence (positivity/negativity), and dominance (sense of control), leading to a more comprehensive and robust representation of human affect. Extensive evaluations across multilingual Singaporean languages (English, Chinese, Malay, and Tamil ) and other public benchmarks show that MERaLiON-SER consistently surpasses both open-source speech encoders and large Audio-LLMs. These results underscore the importance of specialised speech-only models for accurate paralinguistic understanding and cross-lingual generalisation. Furthermore, the proposed framework provides a foundation for integrating emotion-aware perception into future agentic audio systems, enabling more empathetic and contextually adaptive multimodal reasoning.         ",
    "url": "https://arxiv.org/abs/2511.04914",
    "authors": [
      "Hardik B. Sailor",
      "Aw Ai Ti",
      "Chen Fang Yih Nancy",
      "Chiu Ying Lay",
      "Ding Yang",
      "He Yingxu",
      "Jiang Ridong",
      "Li Jingtao",
      "Liao Jingyi",
      "Liu Zhuohan",
      "Lu Yanfeng",
      "Ma Yi",
      "Manas Gupta",
      "Muhammad Huzaifah Bin Md Shahrin",
      "Nabilah Binte Md Johan",
      "Nattadaporn Lertcheva",
      "Pan Chunlei",
      "Pham Minh Duc",
      "Siti Maryam Binte Ahmad Subaidi",
      "Siti Umairah Binte Mohammad Salleh",
      "Sun Shuo",
      "Tarun Kumar Vangani",
      "Wang Qiongqiong",
      "Won Cheng Yi Lewis",
      "Wong Heng Meng Jeremy",
      "Wu Jinyang",
      "Zhang Huayun",
      "Zhang Longyin",
      "Zou Xunlong"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.05694",
    "title": "Distributionally Robust Self Paced Curriculum Reinforcement Learning",
    "abstract": "           A central challenge in reinforcement learning is that policies trained in controlled environments often fail under distribution shifts at deployment into real-world environments. Distributionally Robust Reinforcement Learning (DRRL) addresses this by optimizing for worst-case performance within an uncertainty set defined by a robustness budget $\\epsilon$. However, fixing $\\epsilon$ results in a tradeoff between performance and robustness: small values yield high nominal performance but weak robustness, while large values can result in instability and overly conservative policies. We propose Distributionally Robust Self-Paced Curriculum Reinforcement Learning (DR-SPCRL), a method that overcomes this limitation by treating $\\epsilon$ as a continuous curriculum. DR-SPCRL adaptively schedules the robustness budget according to the agent's progress, enabling a balance between nominal and robust performance. Empirical results across multiple environments demonstrate that DR-SPCRL not only stabilizes training but also achieves a superior robustness-performance trade-off, yielding an average 11.8\\% increase in episodic return under varying perturbations compared to fixed or heuristic scheduling strategies, and achieving approximately 1.9$\\times$ the performance of the corresponding nominal RL algorithms.         ",
    "url": "https://arxiv.org/abs/2511.05694",
    "authors": [
      "Anirudh Satheesh",
      "Keenan Powell",
      "Vaneet Aggarwal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.06298",
    "title": "SFFR: Spatial-Frequency Feature Reconstruction for Multispectral Aerial Object Detection",
    "abstract": "           Recent multispectral object detection methods have primarily focused on spatial-domain feature fusion based on CNNs or Transformers, while the potential of frequency-domain feature remains underexplored. In this work, we propose a novel Spatial and Frequency Feature Reconstruction method (SFFR) method, which leverages the spatial-frequency feature representation mechanisms of the Kolmogorov-Arnold Network (KAN) to reconstruct complementary representations in both spatial and frequency domains prior to feature fusion. The core components of SFFR are the proposed Frequency Component Exchange KAN (FCEKAN) module and Multi-Scale Gaussian KAN (MSGKAN) module. The FCEKAN introduces an innovative selective frequency component exchange strategy that effectively enhances the complementarity and consistency of cross-modal features based on the frequency feature of RGB and IR images. The MSGKAN module demonstrates excellent nonlinear feature modeling capability in the spatial domain. By leveraging multi-scale Gaussian basis functions, it effectively captures the feature variations caused by scale changes at different UAV flight altitudes, significantly enhancing the model's adaptability and robustness to scale variations. It is experimentally validated that our proposed FCEKAN and MSGKAN modules are complementary and can effectively capture the frequency and spatial semantic features respectively for better feature fusion. Extensive experiments on the SeaDroneSee, DroneVehicle and DVTOD datasets demonstrate the superior performance and significant advantages of the proposed method in UAV multispectral object perception task. Code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.06298",
    "authors": [
      "Xin Zuo",
      "Yuchen Qu",
      "Haibo Zhan",
      "Jifeng Shen",
      "Wankou Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.06942",
    "title": "HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection",
    "abstract": "           To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at this https URL.         ",
    "url": "https://arxiv.org/abs/2511.06942",
    "authors": [
      "Fangqi Dai",
      "Xingjian Jiang",
      "Zizhuang Deng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2511.07242",
    "title": "Privacy on the Fly: A Predictive Adversarial Transformation Network for Mobile Sensor Data",
    "abstract": "           Mobile motion sensors such as accelerometers and gyroscopes are now ubiquitously accessible by third-party apps via standard APIs. While enabling rich functionalities like activity recognition and step counting, this openness has also enabled unregulated inference of sensitive user traits, such as gender, age, and even identity, without user consent. Existing privacy-preserving techniques, such as GAN-based obfuscation or differential privacy, typically require access to the full input sequence, introducing latency that is incompatible with real-time scenarios. Worse, they tend to distort temporal and semantic patterns, degrading the utility of the data for benign tasks like activity recognition. To address these limitations, we propose the Predictive Adversarial Transformation Network (PATN), a real-time privacy-preserving framework that leverages historical signals to generate adversarial perturbations proactively. The perturbations are applied immediately upon data acquisition, enabling continuous protection without disrupting application functionality. Experiments on two datasets demonstrate that PATN substantially degrades the performance of privacy inference models, achieving Attack Success Rate (ASR) of 40.11% and 44.65% (reducing inference accuracy to near-random) and increasing the Equal Error Rate (EER) from 8.30% and 7.56% to 41.65% and 46.22%. On ASR, PATN outperforms baseline methods by 16.16% and 31.96%, respectively.         ",
    "url": "https://arxiv.org/abs/2511.07242",
    "authors": [
      "Tianle Song",
      "Chenhao Lin",
      "Yang Cao",
      "Zhengyu Zhao",
      "Jiahao Sun",
      "Chong Zhang",
      "Le Yang",
      "Chao Shen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2511.07935",
    "title": "DiffRegCD: Integrated Registration and Change Detection with Diffusion Features",
    "abstract": "           Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.         ",
    "url": "https://arxiv.org/abs/2511.07935",
    "authors": [
      "Seyedehanita Madani",
      "Rama Chellappa",
      "Vishal M. Patel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.08033",
    "title": "Nash-equilibrium Seeking Algorithm for Power-Allocation Games on Networks of International Relations",
    "abstract": "           In the field of international security, understanding the strategic interactions between countries within a networked context is crucial. Our previous research has introduced a ``games-on-signed graphs'' framework~\\cite{LiMorse2022} to analyze these interactions. While the framework is intended to be basic and general, there is much left to be explored, particularly in capturing the complexity of strategic scenarios in international relations. Our paper aims to fill this gap in two key ways. First, we modify the existing preference axioms to allow for a more nuanced understanding of how countries pursue self-survival, defense of allies, and offense toward adversaries. Second, we introduce a novel algorithm that proves the existence of a pure-strategy Nash equilibrium for these revised games. To validate our model, we employ historical data from the year 1940 as the game input and predict countries' survivability. Our contributions thus extend the real-world applicability of the original framework, offering a more comprehensive view of strategic interactions in a networked security environment.         ",
    "url": "https://arxiv.org/abs/2511.08033",
    "authors": [
      "Chuanzhe Zhang",
      "Yuke Li",
      "Wenjun Mei"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2511.08160",
    "title": "Dividing Indivisible Items for the Benefit of All: It is Hard to Be Fair Without Social Awareness",
    "abstract": "           In standard fair division models, we assume that all agents are selfish. However, in many scenarios, division of resources has a direct impact on the whole group or even society. Therefore, we study fair allocations of indivisible items that, at the same time, maximize social impact. In this model, each agent is associated with two additive functions that define their value and social impact for each item. The goal is to allocate items so that the social impact is maximized while maintaining some fairness criterion. We reveal that the complexity of the problem heavily depends on whether the agents are socially aware, i.e., they take into consideration the social impact functions. For socially unaware agents, we prove that the problem is NP-hard for a variety of fairness notions, and that it is tractable only for very restricted cases, e.g., if, for every agent, the valuation equals social impact and it is binary. On the other hand, social awareness allows for fair allocations that maximize social impact, and such allocations can be computed in polynomial time. Interestingly, the problem becomes again intractable as soon as the definition of social awareness is relaxed.         ",
    "url": "https://arxiv.org/abs/2511.08160",
    "authors": [
      "Argyris Deligkas",
      "Eduard Eiben",
      "Tiger-Lily Goldsmith",
      "Du\u0161an Knop",
      "\u0160imon Schierreich"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2511.08344",
    "title": "SASG-DA: Sparse-Aware Semantic-Guided Diffusion Augmentation For Myoelectric Gesture Recognition",
    "abstract": "           Surface electromyography (sEMG)-based gesture recognition plays a critical role in human-machine interaction (HMI), particularly for rehabilitation and prosthetic control. However, sEMG-based systems often suffer from the scarcity of informative training data, leading to overfitting and poor generalization in deep learning models. Data augmentation offers a promising approach to increasing the size and diversity of training data, where faithfulness and diversity are two critical factors to effectiveness. However, promoting untargeted diversity can result in redundant samples with limited utility. To address these challenges, we propose a novel diffusion-based data augmentation approach, Sparse-Aware Semantic-Guided Diffusion Augmentation (SASG-DA). To enhance generation faithfulness, we introduce the Semantic Representation Guidance (SRG) mechanism by leveraging fine-grained, task-aware semantic representations as generation conditions. To enable flexible and diverse sample generation, we propose a Gaussian Modeling Semantic Sampling (GMSS) strategy, which models the semantic representation distribution and allows stochastic sampling to produce both faithful and diverse samples. To enhance targeted diversity, we further introduce a Sparse-Aware Semantic Sampling (SASS) strategy to explicitly explore underrepresented regions, improving distribution coverage and sample utility. Extensive experiments on benchmark sEMG datasets, Ninapro DB2, DB4, and DB7, demonstrate that SASG-DA significantly outperforms existing augmentation methods. Overall, our proposed data augmentation approach effectively mitigates overfitting and improves recognition performance and generalization by offering both faithful and diverse samples.         ",
    "url": "https://arxiv.org/abs/2511.08344",
    "authors": [
      "Chen Liu",
      "Can Han",
      "Weishi Xu",
      "Yaqi Wang",
      "Dahong Qian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2511.08453",
    "title": "Measuring Value Expressions in Social Media Posts",
    "abstract": "           The value alignment of sociotechnical systems has become a central debate but progress in this direction requires the measurement of the expressions of values. While the rise of large-language models offer new possible opportunities for measuring expressions of human values (e.g., humility or equality) in social media data, there remain both conceptual and practical challenges in operationalizing value expression in social media posts: what value system and operationalization is most applicable, and how do we actually measure them? In this paper, we draw on the Schwartz value system as a broadly encompassing and theoretically grounded set of basic human values, and introduce a framework for measuring Schwartz value expressions in social media posts at scale. We collect 32,370 ground truth value expression annotations from N=1,079 people on 5,211 social media posts representative of real users' feeds. We observe low levels of inter-rater agreement between people, and low agreement between human raters and LLM-based methods. Drawing on theories of interpretivism - that different people will have different subjective experiences of the same situation - we argue that value expression is (partially) in the eye of the beholder. In response, we construct a personalization architecture for classifying value expressions. We find that a system that explicitly models these differences yields predicted value expressions that people agree with more than they agree with other people. These results contribute new methods and understanding for the measurement of human values in social media data.         ",
    "url": "https://arxiv.org/abs/2511.08453",
    "authors": [
      "Ziv Epstein",
      "Farnaz Jahanbakhsh",
      "Tiziano Piccardi",
      "Isabel Gallegos",
      "Dora Zhao",
      "Johan Ugander",
      "Michael Bernstein"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2511.08544",
    "title": "LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics",
    "abstract": "           Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&D. We present a comprehensive theory of JEPAs and instantiate it in {\\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\\href{this https URL}{GitHub repo}).         ",
    "url": "https://arxiv.org/abs/2511.08544",
    "authors": [
      "Randall Balestriero",
      "Yann LeCun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2511.08558",
    "title": "Hyperdimensional Decoding of Spiking Neural Networks",
    "abstract": "           This work presents a novel spiking neural network (SNN) decoding method, combining SNNs with Hyperdimensional computing (HDC). The goal is to create a decoding method with high accuracy, high noise robustness, low latency and low energy usage. Compared to analogous architectures decoded with existing approaches, the presented SNN-HDC model attains generally better classification accuracy, lower classification latency and lower estimated energy consumption on multiple test cases from literature. The SNN-HDC achieved estimated energy consumption reductions ranging from 1.24x to 3.67x on the DvsGesture dataset and from 1.38x to 2.27x on the SL-Animals-DVS dataset. The presented decoding method can also efficiently identify unknown classes it has not been trained on. In the DvsGesture dataset the SNN-HDC model can identify 100% of samples from an unseen/untrained class. Given the numerous benefits shown and discussed in this paper, this decoding method represents a very compelling alternative to both rate and latency decoding.         ",
    "url": "https://arxiv.org/abs/2511.08558",
    "authors": [
      "Cedrick Kinavuidi",
      "Luca Peres",
      "Oliver Rhodes"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.11093",
    "title": "Simulating Non-Markovian Open Quantum Dynamics with Neural Quantum States",
    "abstract": "           Reducing computational scaling for simulating non-Markovian dissipative dynamics using artificial neural networks is both a major focus and formidable challenge in open quantum systems. To enable neural quantum states (NQSs), we encode environmental memory in dissipatons (quasiparticles with characteristic lifetimes), yielding the dissipaton-embedded quantum master equation (DQME). The resulting NQS-DQME framework achieves compact representation of many-body correlations and non-Markovian memory. Benchmarking against numerically exact hierarchical equations of motion confirms NQS-DQME maintains comparable accuracy while enhancing scalability and interpretability. This methodology opens new paths to explore non-Markovian open quantum dynamics in previously intractable systems.         ",
    "url": "https://arxiv.org/abs/2404.11093",
    "authors": [
      "Long Cao",
      "Liwei Ge",
      "Daochi Zhang",
      "Xiang Li",
      "Yao Wang",
      "Rui-Xue Xu",
      "YiJing Yan",
      "Xiao Zheng"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.20586",
    "title": "Testing and Improving the Robustness of Amortized Bayesian Inference for Cognitive Models",
    "abstract": "           Contaminant observations and outliers often cause problems when estimating the parameters of cognitive models, which are statistical models representing cognitive processes. In this study, we test and improve the robustness of parameter estimation using amortized Bayesian inference (ABI) with neural networks. To this end, we conduct systematic analyses on a toy example and analyze both synthetic and real data using a popular cognitive model, the Drift Diffusion Models (DDM). First, we study the sensitivity of ABI to contaminants with tools from robust statistics: the empirical influence function and the breakdown point. Next, we propose a data augmentation or noise injection approach that incorporates a contamination distribution into the data-generating process during training. We examine several candidate distributions and evaluate their performance and cost in terms of accuracy and efficiency loss relative to a standard estimator. Introducing contaminants from a Cauchy distribution during training considerably increases the robustness of the neural density estimator as measured by bounded influence functions and a much higher breakdown point. Overall, the proposed method is straightforward and practical to implement and has a broad applicability in fields where outlier detection or removal is challenging.         ",
    "url": "https://arxiv.org/abs/2412.20586",
    "authors": [
      "Yufei Wu",
      "Stefan T. Radev",
      "Francis Tuerlinckx"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2506.06318",
    "title": "MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes",
    "abstract": "           MEMS gyroscopes play a critical role in inertial navigation and motion control applications but typically suffer from a fundamental trade-off between measurement range and noise performance. Existing hardware-based solutions aimed at mitigating this issue introduce additional complexity, cost, and scalability challenges. Deep-learning methods primarily focus on noise reduction and typically require precisely aligned ground-truth signals, making them difficult to deploy in practical scenarios and leaving the fundamental trade-off unresolved. To address these challenges, we introduce Mixture of Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework specifically designed for simultaneous over-range signal reconstruction and noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing saturated segments; and a Denoise Expert (DE), utilizing dual-branch complementary masking combined with FFT-guided augmentation for robust noise reduction. A lightweight gating module dynamically routes input segments to the appropriate expert. Furthermore, existing evaluation lack a comprehensive standard for assessing multi-dimensional signal enhancement. To bridge this gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source benchmarking platform comprising the GyroPeak-100 dataset and a unified evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our proposed ISEBench, demonstrating that our framework significantly extends the measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by 98.4%, and achieves state-of-the-art performance, effectively addressing the long-standing trade-off in inertial sensing.         ",
    "url": "https://arxiv.org/abs/2506.06318",
    "authors": [
      "Feiyang Pan",
      "Shenghe Zheng",
      "Chunyan Yin",
      "Guangbin Dou"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.22552",
    "title": "Probing forced responses and causality in data-driven climate emulators: conceptual limitations and the role of reduced-order models",
    "abstract": "           A central challenge in climate science and applied mathematics is developing data-driven models of multiscale systems that capture both stationary statistics and responses to external perturbations. Current neural climate emulators aim to resolve the atmosphere-ocean system in all its complexity but often struggle to reproduce forced responses, limiting their use in causal studies such as Green's function experiments. To investigate the origin of these limitations, we first examine a simplified dynamical system that retains key features of climate variability. We interpret the results through linear response theory, providing a rigorous framework to evaluate neural models beyond stationary statistics and probe causal mechanisms. We argue that the ability of multiscale systems' emulators to reproduce perturbed statistics depends critically on (i) identifying an appropriate coarse-grained representation and (ii) careful parameterizations of unresolved processes. For low-frequency climate dynamics, these insights highlight reduced-order models, tailored to specific processes and scales, as valuable alternatives to general-purpose emulators. We next consider a real-world application, developing a neural model to investigate the joint variability of the surface temperature field and radiative fluxes. The model infers a multiplicative noise process directly from data, largely reproduces the system's probability distribution, and enables causal studies through forced responses. We discuss its limitations and outline directions for future work. These results expose key challenges in data-driven modeling of multiscale physical systems and underscore the value of coarse-grained, stochastic approaches, with response theory as a principled framework to guide model design.         ",
    "url": "https://arxiv.org/abs/2506.22552",
    "authors": [
      "Fabrizio Falasca"
    ],
    "subjectives": [
      "Chaotic Dynamics (nlin.CD)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2507.19648",
    "title": "Decoding street network morphologies and their correlation to travel mode choice",
    "abstract": "           Urban morphology has long been recognized as a factor shaping human mobility, yet comparative and formal classifications of urban form across metropolitan areas remain limited. Building on theoretical principles of urban structure and advances in unsupervised learning, we systematically classified the built environment of nine U.S. metropolitan areas using structural indicators such as density, connectivity, and spatial configuration. The resulting morphological types were linked to mobility patterns through descriptive statistics, marginal effects estimation, and post hoc statistical testing. Here we show that distinct urban forms are systematically associated with different mobility behaviors, such as reticular morphologies being linked to significantly higher public transport use (marginal effect = 0.49) and reduced car dependence (-0.41), while organic forms are associated with increased car usage (0.44), and substantial declines in public transport (-0.47) and active mobility (-0.30). These effects are statistically robust (p < 1e-19), highlighting that the spatial configuration of urban areas plays a fundamental role in shaping transportation choices. Our findings extend previous work by offering a reproducible framework for classifying urban form and demonstrate the added value of morphological analysis in comparative urban research. These results suggest that urban form should be treated as a key variable in mobility planning and provide empirical support for incorporating spatial typologies into sustainable urban policy design.         ",
    "url": "https://arxiv.org/abs/2507.19648",
    "authors": [
      "Juan Fernando Riascos-Goyes",
      "Michael Lowry",
      "Nicol\u00e1s Guar\u00edn-Zapata",
      "Juan P. Ospina"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.22787",
    "title": "Amorphous Solid Model of Vectorial Hopfield Neural Networks",
    "abstract": "           We introduce a three-dimensional vectorial extension of the Hopfield associative-memory model in which each neuron is a unit vector on $S^2$ and synaptic couplings are $3\\times 3$ blocks generated through a vectorial Hebbian rule. The resulting block-structured operator is mathematically analogous to the Hessian of amorphous solids and induces a rigid energy landscape with deep minima for stored patterns. Simulations and spectral analysis demonstrate that the vectorial network substantially outperforms the classical binary Hopfield model: it exhibits a memory capacity that grows linearly with connectivity, a persistent spectral gap separating stored patterns from noise, and significantly enlarged basins of attraction. These results show that geometric constraints combined with amorphous-solid-inspired structure yield associative memories with markedly enhanced stability and retrieval performance.         ",
    "url": "https://arxiv.org/abs/2507.22787",
    "authors": [
      "F. Gallavotti",
      "A. Zaccone"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2507.22992",
    "title": "A Compact Framework for Analyzing Asynchronous Entanglement Distribution in Quantum Networks",
    "abstract": "           This work introduces a compact framework for analyzing asynchronous entanglement distribution protocols under realistic error models. We focus on two contemporary protocols: sequential, where entanglement is established one node at a time, and parallel, where all nodes attempt to generate entanglement simultaneously. We derive an analytical expression for the fidelity of distributed entangled states, showing that the fidelity depends only on the total time all qubits spend in memory, rather than the individual memory times for each qubit. This result distills the complex dynamics of entanglement distribution into a compact accessible form, providing an scalable tool for evaluating protocol efficiency. Using this lightweight framework, we analyze the performance of parallel and sequential protocols, demonstrating that parallel distribution consistently outperforms sequential and highlighting the potential of parallel protocols for practical quantum network implementations.         ",
    "url": "https://arxiv.org/abs/2507.22992",
    "authors": [
      "Emma Hughes",
      "William Munizzi",
      "Prineha Narang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2511.00773",
    "title": "Stochastic representation of solutions for the parabolic Cauchy problem with variable exponent coefficients",
    "abstract": "           In this work, we prove existence and uniqueness of a bounded viscosity solution for the Cauchy problem of degenerate parabolic equations with variable exponent coefficients. We construct the solution directly using the stochastic representation, then verify it satisfies the Cauchy problem. The corresponding SDE, on the other hand, allows the drift and diffusion coefficients to respond nonlinearly to the current state through the state-dependent variable exponents, and thus, extends the expressive power of classical SDEs to better capture complex dynamics. To validate our theoretical framework, we conduct comprehensive numerical experiments comparing finite difference solutions (Crank-Nicolson on logarithmic grids) with Monte Carlo simulations of the SDE.         ",
    "url": "https://arxiv.org/abs/2511.00773",
    "authors": [
      "Mustafa Avci"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2511.05452",
    "title": "Self-adaptive weighting and sampling for physics-informed neural networks",
    "abstract": "           Physics-informed deep learning has emerged as a promising framework for solving partial differential equations (PDEs). Nevertheless, training these models on complex problems remains challenging, often leading to limited accuracy and efficiency. In this work, we introduce a hybrid adaptive sampling and weighting method to enhance the performance of physics-informed neural networks (PINNs). The adaptive sampling component identifies training points in regions where the solution exhibits rapid variation, while the adaptive weighting component balances the convergence rate across training points. Numerical experiments show that applying only adaptive sampling or only adaptive weighting is insufficient to consistently achieve accurate predictions, particularly when training points are scarce. Since each method emphasizes different aspects of the solution, their effectiveness is problem dependent. By combining both strategies, the proposed framework consistently improves prediction accuracy and training efficiency, offering a more robust approach for solving PDEs with PINNs.         ",
    "url": "https://arxiv.org/abs/2511.05452",
    "authors": [
      "Wenqian Chen",
      "Amanda Howard",
      "Panos Stinis"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  }
]